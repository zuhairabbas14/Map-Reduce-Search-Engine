{"id": "5180", "url": "https://en.wikipedia.org/wiki?curid=5180", "title": "Chemistry", "text": "Chemistry\n\nChemistry is a branch of physical science that studies the composition, structure, properties and change of matter. Chemistry includes topics such as the properties of individual atoms, how atoms form chemical bonds to create chemical compounds, the interactions of substances through intermolecular forces that give matter its general properties, and the interactions between substances through chemical reactions to form different substances.\n\nChemistry is sometimes called the central science because it bridges other natural sciences, including physics, geology and biology. For the differences between chemistry and physics see comparison of chemistry and physics.\n\nThe history of chemistry can be traced to alchemy, which had been practiced for several millennia in various parts of the world.\n\nThe word \"chemistry\" comes from \"alchemy,\" which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism and medicine. It is often seen as linked to the quest to turn lead or another common starting material into gold, though in ancient times the study encompassed many of the questions of modern chemistry being defined as the study of the composition of waters, movement, growth, embodying, disembodying, drawing the spirits from bodies and bonding the spirits within bodies by the early 4th century Greek-Egyptian alchemist Zosimos. An alchemist was called a 'chemist' in popular speech, and later the suffix \"-ry\" was added to this to describe the art of the chemist as \"chemistry\".\n\nThe modern word \"alchemy\" in turn is derived from the Arabic word \"al-kīmīā\" (الکیمیاء). In origin, the term is borrowed from the Greek χημία or χημεία. This may have Egyptian origins since \"al-kīmīā\" is derived from the Greek χημία, which is in turn derived from the word Chemi or Kimi, which is the ancient name of Egypt in Egyptian. Alternately, \"al-kīmīā\" may derive from χημεία, meaning \"cast together\".\n\nIn retrospect, the definition of chemistry has changed over time, as new discoveries and theories add to the functionality of the science. The term \"chymistry\", in the view of noted scientist Robert Boyle in 1661, meant the subject of the material principles of mixed bodies. In 1663 the chemist Christopher Glaser described \"chymistry\" as a scientific art, by which one learns to dissolve bodies, and draw from them the different substances on their composition, and how to unite them again, and exalt them to a higher perfection.\n\nThe 1730 definition of the word \"chemistry\", as used by Georg Ernst Stahl, meant the art of resolving mixed, compound, or aggregate bodies into their principles; and of composing such bodies from those principles. In 1837, Jean-Baptiste Dumas considered the word \"chemistry\" to refer to the science concerned with the laws and effects of molecular forces. This definition further evolved until, in 1947, it came to mean the science of substances: their structure, their properties, and the reactions that change them into other substances - a characterization accepted by Linus Pauling. More recently, in 1998, Professor Raymond Chang broadened the definition of \"chemistry\" to mean the study of matter and the changes it undergoes.\n\nEarly civilizations, such as the Egyptians Babylonians, Indians amassed practical knowledge concerning the arts of metallurgy, pottery and dyes, but didn't develop a systematic theory.\n\nA basic chemical hypothesis first emerged in Classical Greece with the theory of four elements as propounded definitively by Aristotle stating that fire, air, earth and water were the fundamental elements from which everything is formed as a combination. Greek atomism dates back to 440 BC, arising in works by philosophers such as Democritus and Epicurus. In 50 BC, the Roman philosopher Lucretius expanded upon the theory in his book \"De rerum natura\" (On The Nature of Things). Unlike modern concepts of science, Greek atomism was purely philosophical in nature, with little concern for empirical observations and no concern for chemical experiments.\n\nIn the Hellenistic world the art of alchemy first proliferated, mingling magic and occultism into the study of natural substances with the ultimate goal of transmuting elements into gold and discovering the elixir of eternal life. Work, particularly the development of distillation, continued in the early Byzantine period with the most famous practitioner being the 4th century Greek-Egyptian Zosimos of Panopolis. Alchemy continued to be developed and practised throughout the Arab world after the Muslim conquests, and from there, and from the Byzantine remnants, diffused into medieval and Renaissance Europe through Latin translations. Some influential Muslim chemists, Abū al-Rayhān al-Bīrūnī, Avicenna and Al-Kindi refuted the theories of alchemy, particularly the theory of the transmutation of metals; and al-Tusi described a version of the conservation of mass, noting that a body of matter is able to change but is not able to disappear.\n\nThe development of the modern scientific method was slow and arduous, but an early scientific method for chemistry began emerging among early Muslim chemists, beginning with the 9th century Persian or Arabian chemist Jābir ibn Hayyān (known as \"Geber\" in Europe), who is sometimes referred to as \"the father of chemistry\". He introduced a systematic and experimental approach to scientific research based in the laboratory, in contrast to the ancient Greek and Egyptian alchemists whose works were largely allegorical and often unintelligble. Under the influence of the new empirical methods propounded by Sir Francis Bacon and others, a group of chemists at Oxford, Robert Boyle, Robert Hooke and John Mayow began to reshape the old alchemical traditions into a scientific discipline. Boyle in particular is regarded as the founding father of chemistry due to his most important work, the classic chemistry text \"The Sceptical Chymist\" where the differentiation is made between the claims of alchemy and the empirical scientific discoveries of the new chemistry. He formulated Boyle's law, rejected the classical \"four elements\" and proposed a mechanistic alternative of atoms and chemical reactions that could be subject to rigorous experiment.\nThe theory of phlogiston (a substance at the root of all combustion) was propounded by the German Georg Ernst Stahl in the early 18th century and was only overturned by the end of the century by the French chemist Antoine Lavoisier, the chemical analogue of Newton in physics; who did more than any other to establish the new science on proper theoretical footing, by elucidating the principle of conservation of mass and developing a new system of chemical nomenclature used to this day.\n\nBefore his work, though, many important discoveries had been made, specifically relating to the nature of 'air' which was discovered to be composed of many different gases. The Scottish chemist Joseph Black (the first experimental chemist) and the Dutchman J. B. van Helmont discovered carbon dioxide, or what Black called 'fixed air' in 1754; Henry Cavendish discovered hydrogen and elucidated its properties and Joseph Priestley and, independently, Carl Wilhelm Scheele isolated pure oxygen.\n\nEnglish scientist John Dalton proposed the modern theory of atoms; that all substances are composed of indivisible 'atoms' of matter and that different atoms have varying atomic weights.\n\nThe development of the electrochemical theory of chemical combinations occurred in the early 19th century as the result of the work of two scientists in particular, J. J. Berzelius and Humphry Davy, made possible by the prior invention of the voltaic pile by Alessandro Volta. Davy discovered nine new elements including the alkali metals by extracting them from their oxides with electric current.\n\nBritish William Prout first proposed ordering all the elements by their atomic weight as all atoms had a weight that was an exact multiple of the atomic weight of hydrogen. J. A. R. Newlands devised an early table of elements, which was then developed into the modern periodic table of elements in the 1860s by Dmitri Mendeleev and independently by several other scientists including Julius Lothar Meyer. The inert gases, later called the noble gases were discovered by William Ramsay in collaboration with Lord Rayleigh at the end of the century, thereby filling in the basic structure of the table.\n\nOrganic chemistry was developed by Justus von Liebig and others, following Friedrich Wöhler's synthesis of urea which proved that living organisms were, in theory, reducible to chemistry. Other crucial 19th century advances were; an understanding of valence bonding (Edward Frankland in 1852) and the application of thermodynamics to chemistry (J. W. Gibbs and Svante Arrhenius in the 1870s).\n\nAt the turn of the twentieth century the theoretical underpinnings of chemistry were finally understood due to a series of remarkable discoveries that succeeded in probing and discovering the very nature of the internal structure of atoms. In 1897, J. J. Thomson of Cambridge University discovered the electron and soon after the French scientist Becquerel as well as the couple Pierre and Marie Curie investigated the phenomenon of radioactivity. In a series of pioneering scattering experiments Ernest Rutherford at the University of Manchester discovered the internal structure of the atom and the existence of the proton, classified and explained the different types of radioactivity and successfully transmuted the first element by bombarding nitrogen with alpha particles.\n\nHis work on atomic structure was improved on by his students, the Danish physicist Niels Bohr and Henry Moseley. The electronic theory of chemical bonds and molecular orbitals was developed by the American scientists Linus Pauling and Gilbert N. Lewis.\n\nThe year 2011 was declared by the United Nations as the International Year of Chemistry. It was an initiative of the International Union of Pure and Applied Chemistry, and of the United Nations Educational, Scientific, and Cultural Organization and involves chemical societies, academics, and institutions worldwide and relied on individual initiatives to organize local and regional activities.\n\nThe current model of atomic structure is the quantum mechanical model. Traditional chemistry starts with the study of elementary particles, atoms, molecules, substances, metals, crystals and other aggregates of matter. This matter can be studied in solid, liquid, or gas states, in isolation or in combination. The interactions, reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory.\n\nThe chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it.\n\nA chemical reaction is a transformation of some substances into one or more different substances. The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws.\n\nEnergy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists. Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are:\n\nIn chemistry, matter is defined as anything that has rest mass and volume (it takes up space) and is made up of particles. The particles that make up matter have rest mass as well - not all particles have rest mass, such as the photon. Matter can be a pure chemical substance or a mixture of substances.\n\nThe atom is the basic unit of chemistry. It consists of a dense core called the atomic nucleus surrounded by a space called the electron cloud. The nucleus is made up of positively charged protons and uncharged neutrons (together called nucleons), while the electron cloud consists of negatively charged electrons which orbit the nucleus. In a neutral atom, the negatively charged electrons balance out the positive charge of the protons. The nucleus is dense; the mass of a nucleon is appromixately 1,836 times that of an electron, yet the radius of an atom is about 10,000 times that of its nucleus.\n\nThe atom is also the smallest entity that can be envisaged to retain the chemical properties of the element, such as electronegativity, ionization potential, preferred oxidation state(s), coordination number, and preferred types of bonds to form (e.g., metallic, ionic, covalent).\n\nA chemical element is a pure substance which is composed of a single type of atom, characterized by its particular number of protons in the nuclei of its atoms, known as the atomic number and represented by the symbol \"Z\". The mass number is the sum of the number of protons and neutrons in a nucleus. Although all the nuclei of all atoms belonging to one element will have the same atomic number, they may not necessarily have the same mass number; atoms of an element which have different mass numbers are known as isotopes. For example, all atoms with 6 protons in their nuclei are atoms of the chemical element carbon, but atoms of carbon may have mass numbers of 12 or 13.\n\nThe standard presentation of the chemical elements is in the periodic table, which orders elements by atomic number. The periodic table is arranged in groups, or columns, and periods, or rows. The periodic table is useful in identifying periodic trends.\n\nA \"compound\" is a pure chemical substance composed of more than one element. The properties of a compound bear little similarity to those of its elements. The standard nomenclature of compounds is set by the International Union of Pure and Applied Chemistry (IUPAC). Organic compounds are named according to the organic nomenclature system. The names for Inorganic compounds are created according to the inorganic nomenclature system. When a compound has more than one component, then they are divided into two classes, the electropositive and the electronegative components. In addition the Chemical Abstracts Service has devised a method to index chemical substances. In this scheme each chemical substance is identifiable by a number known as its CAS registry number.\n\nA \"molecule\" is the smallest indivisible portion of a pure chemical substance that has its unique set of chemical properties, that is, its potential to undergo a certain set of chemical reactions with other substances. However, this definition only works well for substances that are composed of molecules, which is not true of many substances (see below). Molecules are typically a set of atoms bound together by covalent bonds, such that the structure is electrically neutral and all valence electrons are paired with other electrons either in bonds or in lone pairs.\n\nThus, molecules exist as electrically neutral units, unlike ions. When this rule is broken, giving the \"molecule\" a charge, the result is sometimes named a molecular ion or a polyatomic ion. However, the discrete and separate nature of the molecular concept usually requires that molecular ions be present only in well-separated form, such as a directed beam in a vacuum in a mass spectrometer. Charged polyatomic collections residing in solids (for example, common sulfate or nitrate ions) are generally not considered \"molecules\" in chemistry. Some molecules contain one or more unpaired electrons, creating radicals. Most radicals are comparatively reactive, but some, such as nitric oxide (NO) can be stable.\nThe \"inert\" or noble gas elements (helium, neon, argon, krypton, xenon and radon) are composed of lone atoms as their smallest discrete unit, but the other isolated chemical elements consist of either molecules or networks of atoms bonded to each other in some way. Identifiable molecules compose familiar substances such as water, air, and many organic compounds like alcohol, sugar, gasoline, and the various pharmaceuticals.\n\nHowever, not all substances or chemical compounds consist of discrete molecules, and indeed most of the solid substances that make up the solid crust, mantle, and core of the Earth are chemical compounds without molecules. These other types of substances, such as ionic compounds and network solids, are organized in such a way as to lack the existence of identifiable molecules \"per se\". Instead, these substances are discussed in terms of formula units or unit cells as the smallest repeating structure within the substance. Examples of such substances are mineral salts (such as table salt), solids like carbon and diamond, metals, and familiar silica and silicate minerals such as quartz and granite.\n\nOne of the main characteristics of a molecule is its geometry often called its structure. While the structure of diatomic, triatomic or tetra atomic molecules may be trivial, (linear, angular pyramidal etc.) the structure of polyatomic molecules, that are constituted of more than six atoms (of several elements) can be crucial for its chemical nature.\n\nA chemical substance is a kind of matter with a definite composition and set of properties. A collection of substances is called a mixture. Examples of mixtures are air and alloys.\n\nThe mole is a unit of measurement that denotes an amount of substance (also called chemical amount). The mole is defined as the number of atoms found in exactly 0.012 kilogram (or 12 grams) of carbon-12, where the carbon-12 atoms are unbound, at rest and in their ground state. The number of entities per mole is known as the Avogadro constant, and is determined empirically to be approximately 6.022 mol. Molar concentration is the amount of a particular substance per volume of solution, and is commonly reported in moldm.\n\nIn addition to the specific chemical properties that distinguish different chemical classifications, chemicals can exist in several phases. For the most part, the chemical classifications are independent of these bulk phase classifications; however, some more exotic phases are incompatible with certain chemical properties. A \"phase\" is a set of states of a chemical system that have similar bulk structural properties, over a range of conditions, such as pressure or temperature.\n\nPhysical properties, such as density and refractive index tend to fall within values characteristic of the phase. The phase of matter is defined by the \"phase transition\", which is when energy put into or taken out of the system goes into rearranging the structure of the system, instead of changing the bulk conditions.\n\nSometimes the distinction between phases can be continuous instead of having a discrete boundary, in this case the matter is considered to be in a supercritical state. When three states meet based on the conditions, it is known as a triple point and since this is invariant, it is a convenient way to define a set of conditions.\n\nThe most familiar examples of phases are solids, liquids, and gases. Many substances exhibit multiple solid phases. For example, there are three phases of solid iron (alpha, gamma, and delta) that vary based on temperature and pressure. A principal difference between solid phases is the crystal structure, or arrangement, of the atoms. Another phase commonly encountered in the study of chemistry is the \"aqueous\" phase, which is the state of substances dissolved in aqueous solution (that is, in water).\n\nLess familiar phases include plasmas, Bose–Einstein condensates and fermionic condensates and the paramagnetic and ferromagnetic phases of magnetic materials. While most familiar phases deal with three-dimensional systems, it is also possible to define analogs in two-dimensional systems, which has received attention for its relevance to systems in biology.\n\nAtoms sticking together in molecules or crystals are said to be bonded with one another. A chemical bond may be visualized as the multipole balance between the positive charges in the nuclei and the negative charges oscillating about them. More than simple attraction and repulsion, the energies and distributions characterize the availability of an electron to bond to another atom.\n\nA chemical bond can be a covalent bond, an ionic bond, a hydrogen bond or just because of Van der Waals force. Each of these kinds of bonds is ascribed to some potential. These potentials create the interactions which hold atoms together in molecules or crystals. In many simple compounds, valence bond theory, the Valence Shell Electron Pair Repulsion model (VSEPR), and the concept of oxidation number can be used to explain molecular structure and composition.\n\nAn ionic bond is formed when a metal loses one or more of its electrons, becoming a positively charged cation, and the electrons are then gained by the non-metal atom, becoming a negatively charged anion. The two oppositely charged ions attract one another, and the ionic bond is the electrostatic force of attraction between them. For example, sodium (Na), a metal, loses one electron to become an Na cation while chlorine (Cl), a non-metal, gains this electron to become Cl. The ions are held together due to electrostatic attraction, and that compound sodium chloride (NaCl), or common table salt, is formed.\nIn a covalent bond, one or more pairs of valence electrons are shared by two atoms: the resulting electrically neutral group of bonded atoms is termed a molecule. Atoms will share valence electrons in such a way as to create a noble gas electron configuration (eight electrons in their outermost shell) for each atom. Atoms that tend to combine in such a way that they each have eight electrons in their valence shell are said to follow the octet rule. However, some elements like hydrogen and lithium need only two electrons in their outermost shell to attain this stable configuration; these atoms are said to follow the \"duet rule\", and in this way they are reaching the electron configuration of the noble gas helium, which has two electrons in its outer shell.\n\nSimilarly, theories from classical physics can be used to predict many ionic structures. With more complicated compounds, such as metal complexes, valence bond theory is less applicable and alternative approaches, such as the molecular orbital theory, are generally used. See diagram on electronic orbitals.\n\nIn the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structures, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants.\n\nA reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. A reaction is said to be exothermic if the reaction releases heat to the surroundings; in the case of endothermic reactions, the reaction absorbs heat from the surroundings.\n\nChemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The \"speed\" of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor formula_1 - that is the probability of a molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.\nThe activation energy necessary for a chemical reaction to occur can be in the form of heat, light, electricity or mechanical force in the form of ultrasound.\n\nA related concept free energy, which also incorporates entropy considerations, is a very useful means for predicting the feasibility of a reaction and determining the state of equilibrium of a chemical reaction, in chemical thermodynamics. A reaction is feasible only if the total change in the Gibbs free energy is negative, formula_2; if it is equal to zero the chemical reaction is said to be at equilibrium.\n\nThere exist only limited possible states of energy for electrons, atoms and molecules. These are determined by the rules of quantum mechanics, which require quantization of energy of a bound system. The atoms/molecules in a higher energy state are said to be excited. The molecules/atoms of substance in an excited energy state are often much more reactive; that is, more amenable to chemical reactions.\n\nThe phase of a substance is invariably determined by its energy and the energy of its surroundings. When the intermolecular forces of a substance are such that the energy of the surroundings is not sufficient to overcome them, it occurs in a more ordered phase like liquid or solid as is the case with water (HO); a liquid at room temperature because its molecules are bound by hydrogen bonds. Whereas hydrogen sulfide (HS) is a gas at room temperature and standard pressure, as its molecules are bound by weaker dipole-dipole interactions.\n\nThe transfer of energy from one chemical substance to another depends on the \"size\" of energy quanta emitted from one substance. However, heat energy is often transferred more easily from almost any substance to another because the phonons responsible for vibrational and rotational energy levels in a substance have much less energy than photons invoked for the electronic energy transfer. Thus, because vibrational and rotational energy levels are more closely spaced than electronic energy levels, heat is more easily transferred between substances relative to light or other forms of electronic energy. For example, ultraviolet electromagnetic radiation is not transferred with as much efficacy from one substance to another as thermal or electrical energy.\n\nThe existence of characteristic energy levels for different chemical substances is useful for their identification by the analysis of spectral lines. Different kinds of spectra are often used in chemical spectroscopy, e.g. IR, microwave, NMR, ESR, etc. Spectroscopy is also used to identify the composition of remote objects - like stars and distant galaxies - by analyzing their radiation spectra.\nThe term chemical energy is often used to indicate the potential of a chemical substance to undergo a transformation through a chemical reaction or to transform other chemical substances.\n\nWhen a chemical substance is transformed as a result of its interaction with another substance or with energy, a chemical reaction is said to have occurred. A \"chemical reaction\" is therefore a concept related to the \"reaction\" of a substance when it comes in close contact with another, whether as a mixture or a solution; exposure to some form of energy, or both. It results in some energy exchange between the constituents of the reaction as well as with the system environment, which may be designed vessels—often laboratory glassware.\n\nChemical reactions can result in the formation or dissociation of molecules, that is, molecules breaking apart to form two or more smaller molecules, or rearrangement of atoms within or across molecules. Chemical reactions usually involve the making or breaking of chemical bonds. Oxidation, reduction, dissociation, acid-base neutralization and molecular rearrangement are some of the commonly used kinds of chemical reactions.\n\nA chemical reaction can be symbolically depicted through a chemical equation. While in a non-nuclear chemical reaction the number and kind of atoms on both sides of the equation are equal, for a nuclear reaction this holds true only for the nuclear particles viz. protons and neutrons.\n\nThe sequence of steps in which the reorganization of chemical bonds may be taking place in the course of a chemical reaction is called its mechanism. A chemical reaction can be envisioned to take place in a number of steps, each of which may have a different speed. Many reaction intermediates with variable stability can thus be envisaged during the course of a reaction. Reaction mechanisms are proposed to explain the kinetics and the relative product mix of a reaction. Many physical chemists specialize in exploring and proposing the mechanisms of various chemical reactions. Several empirical rules, like the Woodward–Hoffmann rules often come in handy while proposing a mechanism for a chemical reaction.\n\nAccording to the IUPAC gold book, a chemical reaction is \"a process that results in the interconversion of chemical species.\" Accordingly, a chemical reaction may be an elementary reaction or a stepwise reaction. An additional caveat is made, in that this definition includes cases where the interconversion of conformers is experimentally observable. Such detectable chemical reactions normally involve sets of molecular entities as indicated by this definition, but it is often conceptually convenient to use the term also for changes involving single molecular entities (i.e. 'microscopic chemical events').\n\nAn \"ion\" is a charged species, an atom or a molecule, that has lost or gained one or more electrons. When an atom loses an electron and thus has more protons than electrons, the atom is a positively charged ion or cation. When an atom gains an electron and thus has more electrons than protons, the atom is a negatively charged ion or anion. Cations and anions can form a crystalline lattice of neutral salts, such as the Na and Cl ions forming sodium chloride, or NaCl. Examples of polyatomic ions that do not split up during acid-base reactions are hydroxide (OH) and phosphate (PO).\n\nPlasma is composed of gaseous matter that has been completely ionized, usually through high temperature.\n\nA substance can often be classified as an acid or a base. There are several different theories which explain acid-base behavior. The simplest is Arrhenius theory, which states than an acid is a substance that produces hydronium ions when it is dissolved in water, and a base is one that produces hydroxide ions when dissolved in water. According to Brønsted–Lowry acid–base theory, acids are substances that donate a positive hydrogen ion to another substance in a chemical reaction; by extension, a base is the substance which receives that hydrogen ion.\n\nA third common theory is Lewis acid-base theory, which is based on the formation of new chemical bonds. Lewis theory explains that an acid is a substance which is capable of accepting a pair of electrons from another substance during the process of bond formation, while a base is a substance which can provide a pair of electrons to form a new bond. According to this theory, the crucial things being exchanged are charges. There are several other ways in which a substance may be classified as an acid or a base, as is evident in the history of this concept.\n\nAcid strength is commonly measured by two methods. One measurement, based on the Arrhenius definition of acidity, is pH, which is a measurement of the hydronium ion concentration in a solution, as expressed on a negative logarithmic scale. Thus, solutions that have a low pH have a high hydronium ion concentration, and can be said to be more acidic. The other measurement, based on the Brønsted–Lowry definition, is the acid dissociation constant (K), which measures the relative ability of a substance to act as an acid under the Brønsted–Lowry definition of an acid. That is, substances with a higher K are more likely to donate hydrogen ions in chemical reactions than those with lower K values.\n\nRedox (\"red\"uction-\"ox\"idation) reactions include all chemical reactions in which atoms have their oxidation state changed by either gaining electrons (reduction) or losing electrons (oxidation). Substances that have the ability to oxidize other substances are said to be oxidative and are known as oxidizing agents, oxidants or oxidizers. An oxidant removes electrons from another substance. Similarly, substances that have the ability to reduce other substances are said to be reductive and are known as reducing agents, reductants, or reducers.\n\nA reductant transfers electrons to another substance, and is thus oxidized itself. And because it \"donates\" electrons it is also called an electron donor. Oxidation and reduction properly refer to a change in oxidation number—the actual transfer of electrons may never occur. Thus, oxidation is better defined as an increase in oxidation number, and reduction as a decrease in oxidation number.\n\nAlthough the concept of equilibrium is widely used across sciences, in the context of chemistry, it arises whenever a number of different states of the chemical composition are possible, as for example, in a mixture of several chemical compounds that can react with one another, or when a substance can be present in more than one kind of phase.\n\nA system of chemical substances at equilibrium, even though having an unchanging composition, is most often not static; molecules of the substances continue to react with one another thus giving rise to a dynamic equilibrium. Thus the concept describes the state in which the parameters such as chemical composition remain unchanged over time.\n\nChemical reactions are governed by certain laws, which have become fundamental concepts in chemistry. Some of them are:\n\nChemistry is typically divided into several major sub-disciplines. There are also several main cross-disciplinary and more specialized fields of chemistry.\n\nOther disciplines within chemistry are traditionally grouped by the type of matter being studied or the kind of study. These include inorganic chemistry, the study of inorganic matter; organic chemistry, the study of organic (carbon-based) matter; biochemistry, the study of substances found in biological organisms; physical chemistry, the study of chemical processes using physical concepts such as thermodynamics and quantum mechanics; and analytical chemistry, the analysis of material samples to gain an understanding of their chemical composition and structure. Many more specialized disciplines have emerged in recent years, e.g. neurochemistry the chemical study of the nervous system (see subdisciplines).\n\nOther fields include agrochemistry, astrochemistry (and cosmochemistry), atmospheric chemistry, chemical engineering, chemical biology, chemo-informatics, electrochemistry, environmental chemistry, femtochemistry, flavor chemistry, flow chemistry, geochemistry, green chemistry, histochemistry, history of chemistry, hydrogenation chemistry, immunochemistry, marine chemistry, materials science, mathematical chemistry, mechanochemistry, medicinal chemistry, molecular biology, molecular mechanics, nanotechnology, natural product chemistry, oenology, organometallic chemistry, petrochemistry, pharmacology, photochemistry, physical organic chemistry, phytochemistry, polymer chemistry, radiochemistry, solid-state chemistry, sonochemistry, supramolecular chemistry, surface chemistry, synthetic chemistry, thermochemistry, and many others.\n\nThe chemical industry represents an important economic activity worldwide. The global top 50 chemical producers in 2013 had sales of US$980.5 billion with a profit margin of 10.3%.\n\n\n\n\n\n\n"}
{"id": "5184", "url": "https://en.wikipedia.org/wiki?curid=5184", "title": "Cytoplasm", "text": "Cytoplasm\n\nIn cell biology, the cytoplasm is the material within a living cell, excluding the cell nucleus. It comprises cytosol (the gel-like substance enclosed within the cell membrane) and the organelles – the cell's internal sub-structures. All of the contents of the cells of prokaryotic organisms (such as bacteria, which lack a cell nucleus) are contained within the cytoplasm. Within the cells of eukaryotic organisms the contents of the cell nucleus are separated from the cytoplasm, and are then called the nucleoplasm. The cytoplasm is about 80% water and usually colorless.\n\nThe submicroscopic ground cell substance or cytoplasmatic matrix which remains after exclusion the cell organelles and particles is groundplasm. It is the hyaloplasm of light microscopy, and high complex, polyphasic system in which all of resolvable cytoplasmic elements of are suspended, including the larger organelles such as the ribosomes, mitochondria, the plant plastids, lipid droplets, and vacuoles.\n\nIt is within the cytoplasm that most cellular activities occur, such as many metabolic pathways including glycolysis, and processes such as cell division. The concentrated inner area is called the endoplasm and the outer layer is called the cell cortex or the ectoplasm.\n\nMovement of calcium ions in and out of the cytoplasm is a signaling activity for metabolic processes.\n\nIn plants, movement of the cytoplasm around vacuoles is known as cytoplasmic streaming.\n\nThe term was introduced by Rudolf von Kölliker in 1863, originally as a synonym for protoplasm, but later it has come to mean the cell substance and organelles outside the nucleus.\n\nThere has been certain disagreement on the definition of cytoplasm, as some authors prefer to exclude from it some organelles, especially the vacuoles and sometimes the plastids.\n\nThe physical properties of the cytoplasm have been contested in recent years. It remains uncertain how the varied components of the cytoplasm interact to allow movement of particles and organelles while maintaining the cell’s structure. The flow of cytoplasmic components plays an important role in many cellular functions which are dependent on the permeability of the cytoplasm. An example of such function is cell signalling, a process which is dependent on the manner in which signaling molecules are allowed to diffuse across the cell. While small signaling molecules like calcium ions are able to diffuse with ease, larger molecules and subcellular structures often require aid in moving through the cytoplasm. The irregular dynamics of such particles have given rise to various theories on the nature of the cytoplasm.\n\nThere has long been evidence that the cytoplasm behaves like a sol-gel. It is thought that the component molecules and structures of the cytoplasm behave at times like a disordered colloidal solution (sol) and at other times like an integrated network, forming a solid mass (gel). This theory thus proposes that the cytoplasm exists in distinct fluid and solid phases depending on the level of interaction between cytoplasmic components, which may explain the differential dynamics of different particles observed moving through the cytoplasm.\n\nRecently it has been proposed that the cytoplasm behaves like a glass-forming liquid approaching the glass transition. In this theory, the greater the concentration of cytoplasmic components, the less the cytoplasm behaves like a liquid and the more it behaves as a solid glass, freezing larger cytoplasmic components in place (it is thought that the cell's metabolic activity is able to fluidize the cytoplasm to allow the movement of such larger cytoplasmic components). A cell's ability to vitrify in the absence of metabolic activity, as in dormant periods, may be beneficial as a defence strategy. A solid glass cytoplasm would freeze subcellular structures in place, preventing damage, while allowing the transmission of very small proteins and metabolites, helping to kickstart growth upon the cell's revival from dormancy.\n\nThere has been research examining the motion of cytoplasmic particles independent of the nature of the cytoplasm. In such an alternative approach, the aggregate random forces within the cell caused by motor proteins explain the non-Brownian motion of cytoplasmic constituents.\n\nThe three major elements of the cytoplasm are the cytosol, organelles and inclusions.\n\nThe cytosol is the portion of the cytoplasm not contained within membrane-bound organelles. Cytosol makes up about 70% of the cell volume and is a complex mixture of cytoskeleton filaments, dissolved molecules, and water. The cytosol's filaments include the protein filaments such as actin filaments and microtubules that make up the cytoskeleton, as well as soluble proteins and small structures such as ribosomes, proteasomes, and the mysterious vault complexes. The inner, granular and more fluid portion of the cytoplasm is referred to as endoplasm.\n\nDue to this network of fibres and high concentrations of dissolved macromolecules, such as proteins, an effect called macromolecular crowding occurs and the cytosol does not act as an ideal solution. This crowding effect alters how the components of the cytosol interact with each other.\n\nOrganelles (literally \"little organs\"), are usually membrane-bound structures inside the cell that have specific functions. Some major organelles that are suspended in the cytosol are the mitochondria, the endoplasmic reticulum, the Golgi apparatus, vacuoles, lysosomes, and in plant cells, chloroplasts.\n\nThe inclusions are small particles of insoluble substances suspended in the cytosol. A huge range of inclusions exist in different cell types, and range from crystals of calcium oxalate or silicon dioxide in plants, to granules of energy-storage materials such as starch, glycogen, or polyhydroxybutyrate. A particularly widespread example are lipid droplets, which are spherical droplets composed of lipids and proteins that are used in both prokaryotes and eukaryotes as a way of storing lipids such as fatty acids and sterols. Lipid droplets make up much of the volume of adipocytes, which are specialized lipid-storage cells, but they are also found in a range of other cell types.\n\nThe cytoplasm, mitochondria and most organelles are contributions to the cell from the maternal gamete. Contrary to the older information that disregards any notion of the cytoplasm being active, new research has shown it to be in control of movement and flow of nutrients in and out of the cell by viscoplastic behavior and a measure of the reciprocal rate of bond breakage within the cytoplasmic network.\n\nThe material properties of the cytoplasm remain an ongoing investigation. Recent measurements using force spectrum microscopy reveal that the cytoplasm can be likened to an elastic solid, rather than a viscoelastic fluid.\n\n"}
{"id": "5185", "url": "https://en.wikipedia.org/wiki?curid=5185", "title": "Christ (title)", "text": "Christ (title)\n\nIn Christianity, the Christ (Greek word Χριστός, \"Christós\", meaning \"the anointed one\") is a title for the saviour and redeemer who would bring salvation to the Jewish people and mankind. Christians believe that Jesus is the Jewish messiah called Christ in both the Hebrew Bible and the Christian Old Testament. \"Christ\", used by Christians as both a name and a title, is synonymous with Jesus.\n\nThe role of the Christ in Christianity originated from the concept of the messiah in Judaism. Though the conceptions of the messiah in each religion are similar, for the most part they are distinct from one another due to the split of early Christianity and Judaism in the 1st century.\n\nThough the original followers of Jesus believed Jesus to be the Jewish messiah, e.g. in the Confession of Peter, before the crucifixion and resurrection, Jesus was usually referred to as \"Jesus of Nazareth\" or \"Jesus, son of Joseph\". Jesus came to be called \"Jesus Christ\" (meaning \"Jesus the \"Khristós\"\", i.e. \"Jesus the Messiah\" or \"Jesus the Anointed\") by his followers after his crucifixion and resurrection. Christians believe that the messianic prophecies were fulfilled in his mission, death, and resurrection. The Pauline epistles, the earliest texts of the New Testament, often refer to Jesus as \"Christ Jesus\" or \"Christ\". The word \"Christ\" was originally a title, but later became part of the name \"Jesus Christ\". It is, however, still also used as a title, in the reciprocal use \"Christ Jesus\", meaning \"the Messiah Jesus\", and independently as \"the Christ\".\n\nThe followers of Jesus became known as Christians (as in Acts ) because they believed Jesus to be the \"Khristós\" or \"Mashiach\" prophesied in the Hebrew Bible. Jesus was not, and is not, accepted in Judaism as a Jewish messiah, and the concept of a divine messiah was always rejected by Judaism as idolatry. Religious Jews still await their messiah's first coming and the Messianic prophecies of Jewish tradition to be accomplished. Religious Christians believe in the Second Coming of Christ, and they await the rest of Christian messianic prophecies to be fulfilled. One of those prophecies, distinctive in both the Jewish and Christian concept of the messiah, is that a Jewish king from the Davidic line, who will be \"anointed\" with holy anointing oil, will be king of God's kingdom on earth, and rule the Jewish people and mankind during the Messianic Age and World to come. Muslims accept Jesus () as \"al-Masih\", the messiah in Islam, but don't believe that the messiah is divine or the Son of God, but do believe he will come again.\n\nThe area of Christian theology called Christology is primarily concerned with the nature and person of Jesus Christ as recorded in the canonical gospels and the letters of the New Testament.\n\nThe word \"Christ\" (and similar spellings) appears in English and in most European languages. English-speakers now often use \"Christ\" as if it were a name, one part of the name \"Jesus Christ\", though it was originally a title (\"the Messiah\"). Its usage in \"Christ Jesus\" emphasizes its nature as a title. Compare the usage \"the Christ\".\n\nThe spelling \"Christ\" in English became standardized in the 18th century, when, in the spirit of the Enlightenment, the spelling of certain words changed to fit their Greek or Latin origins. Prior to this, scribes writing in Old and Middle English usually used the spelling \"Crist\" - the \"i\" being pronounced either as , preserved in the names of churches such as St Katherine Cree, or as a short , preserved in the modern pronunciation of \"Christmas\". The spelling \"Christ\" in English is attested from the 14th century.\n\nIn modern and ancient usage, even in secular terminology, \"Christ\" usually refers to Jesus, based on the centuries-old tradition of such usage. Since the Apostolic Age, the[...] use of the definite article before the word Christ and its gradual development into a proper name show the Christians identified the bearer with the promised Messias of the Jews.\n\nAt the time of Jesus, there was no single form of Second Temple Judaism, and there were significant political, social, and religious differences among the various Jews groups. However, for centuries the Jews had used the term \"moshiach\" (\"anointed\") to refer to their expected deliverer. A large number of Old Testament passages were regarded as messianic by the Jews, many more than are commonly considered messianic by Christians, and different groups of Jews assigned varying degrees of significance to them.\n\nThe Greek word \"messias\" appears only twice in the Septuagint of the promised prince (; ). This title was used when a name was wanted for the promised one who was to be at once King and Savior. The New Testament states that the long-awaited messiah had come and describes this savior as \"the Christ\". In , the apostle Peter said, in what has become a famous proclamation of faith among Christians since the first century, \"You are the Christ, the Son of the living God.\"\n\nMark (\"The beginning of the gospel of Jesus Christ, the Son of God\") identifies Jesus as both Christ and the Son of God. The divinity is re-affirmed in . Thereafter, Mark never applies the term \"Christ\" to Jesus as a name. uses Christ as a name and Matthew explains it again with: \"Jesus, who is called Christ\". In the Gospel of John, Jesus referred to himself as the Son of God far more frequently than in the Synoptic Gospels.\n\nThe use of the definite article before the word \"Christ\" and its gradual development into a proper name show that the Christians identified Jesus with the promised messiah of the Jews who fulfilled all the Messianic predictions in a fuller and a higher sense than had been given them by the rabbis.\n\nThe Gospels of Mark and Matthew begin by calling Jesus both Christ and the Son of God, but these are two distinct attributions. They develop in the New Testament along separate paths and have distinct theological implications. The development of both titles involves \"the precursor\" John the Baptist. At the time in Roman Judaea, the Jews had been awaiting the \"messiah\", and many people were wondering who it would be. When John the Baptist appeared and began preaching, he attracted disciples who assumed that he would be announced as the messiah, or \"the one\" that they had been awaiting. But the title Son of God was not attributed to John.\n\nThe first instance of Jesus being called the Son of God appears during his baptism by John the Baptist. In the narrative, a voice from heaven called Jesus \"My Son\". John the Baptist was in prison in the Messengers from John the Baptist episode ( and Luke ), and two of his disciples ask Jesus a question on his behalf: \"Are you the one to come after me or shall we wait for another?\"—indicating that John doubted the identity of Jesus as Christ at that time (see also Rejection of Jesus).\n\nIn Martha told Jesus, \"you are the Christ, the Son of God, who is coming into the world\", signifying that both titles were generally accepted (yet considered distinct) among the followers of Jesus before the raising of Lazarus.\n\nExplicit claims of Jesus being the messiah are found in the canonical gospels in the Confession of Peter (e.g. ) and the words of Jesus before his judges at his trial before the Sanhedrin. These incidents involve far more than a mere Messianic claim; taken in their setting, they constitute a claim to be the Son of God.\n\nIn the trial of Jesus before the Sanhedrin and Pontius Pilate, it might appear from the narratives of Matthew and Luke that Jesus at first refused a direct reply to the high priest's question: \"Art thou the Christ?\", where his answer is given merely as \"\"su eipas\"\" (\"thou hast said it\"). The Gospel of Mark, however, states the answer as \"\"ego eimi\"\" (\"I am\"), and there are instances from Jewish literature in which the expression \"thou hast said it\" is equivalent to \"you are right\". The Messianic claim was less significant than the claim to divinity, which caused the high priest's horrified accusation of blasphemy and the subsequent call for the death sentence. Before Pilate, on the other hand, it was merely the assertion of his royal dignity which gave grounds for his condemnation.\n\nThe word \"Christ\" is closely associated with Jesus in the Pauline epistles, which suggests that there was no need for the early Christians to claim that Jesus is Christ because it was considered widely accepted among them. Hence Paul can use the term \"Khristós\" with no confusion as to whom it refers, and he can use expressions such as \"in Christ\" to refer to the followers of Jesus, as in and . Paul proclaimed him as the Last Adam, who restored through obedience what Adam lost through disobedience. The Pauline epistles are a source of some key Christological connections; e.g., relates the love of Christ to the knowledge of Christ, and considers the love of Christ as a necessity for knowing him.\n\nThere are also implicit claims to him being the Christ in the words and actions of Jesus. Episodes in the life of Jesus and statements about what he accomplished during his public ministry are found throughout the New Testament. Trinitarianism summarily claims: \"Jesus Christ was fully God and fully man in one person, and will be so forever.\"\n\nThere are distinct, and differing, views among Christians regarding the existence of Christ before his conception. A key passage in the New Testament is where John 1:17 specifically mentions that \"grace and truth came through Jesus Christ.\" Those who believe in the Trinity, consider Christ a pre-existent divine hypostasis called the Logos or the Word. Other, non-Trinitarian views, question the aspect of personal pre-existence or question the aspect of divinity, or both. An example is the Orthodox Gnomic view, which asserts that Christ was, in fact, not a pre-existent divine being.\n\nThe concept of Christ as Logos derives from : \"In the beginning was the Word, and the Word was with God, and the Word was God.\" In the original Greek, \"Logos\" (λόγος) is used for \"Word,\" and is often used untranslated. In the Christology of the Logos, Christ is viewed as the Incarnation of the \"Divine Logos\", i.e. The Word.\n\nIn the 2nd century, with his theory of \"recapitulation\", Irenaeus connected \"Christ the Creator\" with \"Christ the Savior\", relying on (\"when the times reach their fulfillment – to bring unity to all things in heaven and on earth under Christ\") to gather together and \"wrap up\" the cycle of the Nativity and Resurrection of Christ.\n\nIn Apostle Paul viewed the Nativity of Jesus as an event of cosmic significance which changed the nature of the world by paving the way for salvation.\n\nChristian teachings present the Love of Christ as a basis for his sacrificial act that brought forth salvation. In Jesus explains that his sacrifice was performed so: \"that the world may know that I love the Father, and as the Father gave me commandment, even so I do.\" then states that: \"Christ also loved the church, and gave himself up for it\".\n\nIn the 2nd century, Irenaeus expressed his views of salvation in terms of the imitation of Christ and his theory of \"recapitulation\". For Irenaeus the imitation of Christ is based on God's plan of salvation, which involved Christ as the \"Last Adam\" He viewed the incarnation as the way in which Christ repaired the damage done by Adam's disobedience. For Irenaeus, salvation was achieved by Christ restoring humanity to the image of God, and he saw the Christian imitation of Christ as a key component on the path to salvation. For Irenaeus Christ succeeded on every point on which Adam failed. Irenaeus drew a number of parallels, e.g. just as in the fall of Adam resulted from the fruit of a tree, Irenaeus saw redemption and salvation as the fruit of another tree: the cross of crucifixion.\n\nFollowing in the Pauline tradition, in the 5th century Augustine of Hippo viewed Christ as the mediator of the New Covenant between God and man and as the conqueror over sin. He viewed Christ as the cause and reason for the reconciliation of man with God after the fall of Adam, and he saw in Christ the path to Christian salvation. Augustine believed that salvation is available to those who are worthy of it, through faith in Christ.\n\nIn the 13th century Thomas Aquinas aimed to recapture the teachings of the Church Fathers on the role of the Holy Trinity in the economy of salvation. In Aquinas' view angels and humans were created for salvation from the very beginning. For Aquinas the Passion of Christ poured out the grace of salvation and all its virtues unto humanity.\n\nMartin Luther distinguished the history of salvation between the Old and the New Testament, and saw a new dimension to salvation with the arrival of Christ.\n\nThe focus on human history was an important element of the biblically grounded 16th-century theology of John Calvin. Calvin considered the first coming of Christ as the key turning point in human history. He viewed Christ as \"the one through whom salvation began\" and he saw the completion of Christ's plan of salvation as his death and Resurrection.\n\nThe use of \"Χ,\" derived from Chi, the Greek alphabet initial, as an abbreviation for Christ (most commonly in the abbreviation \"Χmas\") is often misinterpreted as a modern secularization of the term. Thus understood, the centuries-old English word Χmas, is actually a shortened form of CHmas, which is, itself, a shortened form for Christmas. Christians are sometimes referred to as \"Xians,\" with the 'X' replacing 'Christ.\n\nA very early Christogram is the \"Chi Rho\" symbol formed by superimposing the first two Greek letters in Christ ( ), chi = ch and rho = r, to produce ☧.\n\n\n"}
{"id": "5187", "url": "https://en.wikipedia.org/wiki?curid=5187", "title": "Capital", "text": "Capital\n\nCapital may refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5188", "url": "https://en.wikipedia.org/wiki?curid=5188", "title": "Central Europe", "text": "Central Europe\n\nCentral Europe lies between Eastern Europe and Western Europe. The concept of Central Europe is based on a common historical, social and cultural identity. Central Europe is going through a phase of \"strategic awakening\", with initiatives such as the CEI, Centrope and the Visegrád Four. While the region's economy shows high disparities with regard to income, all Central European countries are listed by the Human Development Index as very highly developed.\n\nElements of unity for Western and Central Europe were Roman Catholicism and Latin. Eastern Europe, which remained Eastern Orthodox Christian, was the area of Byzantine cultural influence; after the schism (1054), The area developed cultural unity and resistance to the Western world (Catholic and Protestant) within the framework of Slavonic language and the Cyrillic alphabet.\nAccording to Hungarian historian Jenő Szűcs, foundations of Central European history at the first millennium were in close connection with Western European development. He explained that between the 11th and 15th centuries not only Christianization and its cultural consequences were implemented, but well-defined social features emerged in Central Europe based on Western characteristics. The keyword of Western social development after millennium was the spread of liberties and autonomies in Western Europe. These phenomena appeared in the middle of the 13th century in Central European countries. There were self-governments of towns, counties and parliaments.\n\nIn 1335 under the rule of the King Charles I of Hungary, the castle of Visegrád, the seat of the Hungarian monarchs was the scene of the royal summit of the Kings of Poland, Bohemia and Hungary. They agreed to cooperate closely in the field of politics and commerce, inspiring their late successors to launch a successful Central European initiative.\n\nIn the Middle Ages, countries in Central Europe adopted Magdeburg rights.\n\nBefore 1870, the industrialization that had developed in Western and Central Europe and the United States did not extend in any significant way to the rest of the world. Even in Eastern Europe, industrialization lagged far behind. Russia, for example, remained largely rural and agricultural, and its autocratic rulers kept the peasants in serfdom.\nThe concept of Central Europe was already known at the beginning of the 19th century, but its real life began in the 20th century and immediately became an object of intensive interest. However, the very first concept mixed science, politics and economy – it was strictly connected with intensively growing German economy and its aspirations to dominate a part of European continent called \"Mitteleuropa\". The German term denoting Central Europe was so fashionable that other languages started referring to it when indicating territories from Rhine to Vistula, or even Dnieper, and from the Baltic Sea to the Balkans. An example of that-time vision of Central Europe may be seen in J. Partsch’s book of 1903.\n\nOn 21 January 1904, \"Mitteleuropäischer Wirtschaftsverein\" (Central European Economic Association) was established in Berlin with economic integration of Germany and Austria–Hungary (with eventual extension to Switzerland, Belgium and the Netherlands) as its main aim. Another time, the term Central Europe became connected to the German plans of political, economic and cultural domination. The \"bible\" of the concept was Friedrich Naumann’s book \"Mitteleuropa\" in which he called for an economic federation to be established after the war. Naumann's idea was that the federation would have at its centre Germany and the Austro-Hungarian Empire but would also include all European nations outside the Anglo-French alliance, on one side, and Russia, on the other. The concept failed after the German defeat in World War I and the dissolution of Austria–Hungary. The revival of the idea may be observed during the Hitler era.\n\nAccording to Emmanuel de Martonne, in 1927 the Central European countries included: Austria, Czechoslovakia, Germany, Hungary, Poland, Romania and Switzerland. Italy and Yugoslavia are not considered by the author to be Central European because they are located mostly outside Central Europe. The author use both Human and Physical Geographical features to define Central Europe.\n\nThe interwar period (1918–1939) brought new geopolitical system and economic and political problems, and the concept of Central Europe took a different character. The centre of interest was moved to its eastern part – the countries that have (re)appeared on the map of Europe: Czechoslovakia, Hungary and Poland. Central Europe ceased to be the area of German aspiration to lead or dominate and became a territory of various integration movements aiming at resolving political, economic and national problems of \"new\" states, being a way to face German and Soviet pressures. However, the conflict of interests was too big and neither Little Entente nor Intermarium (\"Międzymorze\") ideas succeeded.\n\nThe interwar period brought new elements to the concept of Central Europe. Before World War I, it embraced mainly German states (Germany, Austria), non-German territories being an area of intended German penetration and domination – German leadership position was to be the natural result of economic dominance. After the war, the Eastern part of Central Europe was placed at the centre of the concept. At that time the scientists took interest in the idea: the International Historical Congress in Brussels in 1923 was committed to Central Europe, and the 1933 Congress continued the discussions.\n\nHungarian scholar Magda Adam wrote in her study \"Versailles System and Central Europe\" (2006): \"Today we know that the bane of Central Europe was the Little Entente, military alliance of Czechoslovakia, Romania and Kingdom of Serbs, Croats and Slovenes (later Yugoslavia), created in 1921 not for Central Europe's cooperation nor to fight German expansion, but in a wrong perceived notion that a completely powerless Hungary must be kept down\".\n\nThe avant-garde movements of Central Europe were an essential part of modernism’s evolution, reaching its peak throughout the continent during the 1920s. The \"Sourcebook of Central European avantgards\" (Los Angeles County Museum of Art) contains primary documents of the avant-gardes in Austria, Czechoslovakia, Germany, Hungary, Poland, Romania and Yugoslavia from 1910 to 1930. The manifestos and magazines of Western European radical art circles are well known to Western scholars and are being taught at primary universities of their kind in the western world.\n\nThe German term \"Mitteleuropa\" (or alternatively its literal translation into English, \"Middle Europe\") is an ambiguous German concept. It is sometimes used in English to refer to an area somewhat larger than most conceptions of 'Central Europe'; it refers to territories under Germanic cultural hegemony until World War I (encompassing Austria–Hungary and Germany in their pre-war formations but usually excluding the Baltic countries north of East Prussia). According to Fritz Fischer \"Mitteleuropa\" was a scheme in the era of the Reich of 1871–1918 by which the old imperial elites had allegedly sought to build a system of German economic, military and political domination from the northern seas to the Near East and from the Low Countries through the steppes of Russia to the Caucasus. Later on, professor Fritz Epstein argued the threat of a Slavic \"Drang nach Westen\" (Western expansion) had been a major factor in the emergence of a \"Mitteleuropa\" ideology before the Reich of 1871 ever came into being.\n\nIn Germany the connotation was also sometimes linked to the pre-war German provinces east of the Oder-Neisse line which were lost as the result of World War II, annexed by People's Republic of Poland and the Soviet Union, and ethnically cleansed of Germans by communist authorities and forces (\"see expulsion of Germans after World War II\") due to Yalta Conference and Potsdam Conference decisions. In this view Bohemia and Moravia, with its dual Western Slavic and Germanic heritage, combined with the historic element of the \"Sudetenland\", is a core region illustrating the problems and features of the entire Central European region.\n\nThe term \"Mitteleuropa\" conjures up negative historical associations among some elder people, although the Germans have not played an exclusively negative role in the region. Most Central European Jews embraced the enlightened German humanistic culture of the 19th century. German-speaking Jews from turn of the 20th century Vienna, Budapest and Prague became representatives of what many consider to be Central European culture at its best, though the Nazi version of \"Mitteleuropa\" destroyed this kind of culture instead. However, the term \"Mitteleuropa\" is now widely used again in German education and media without negative meaning, especially since the end of communism. In fact, many people from the new states of Germany do not identify themselves as being part of Western Europe and therefore prefer the term \"Mitteleuropa\".\n\nFollowing World War II, large parts of Europe that were culturally and historically Western became part of the Eastern bloc. Czech author Milan Kundera (emigrant to France) thus wrote in 1984 about the \"Tragedy of Central Europe\" in the New York Review of Books. Consequently, the English term \"Central Europe\" was increasingly applied only to the westernmost former Warsaw Pact countries (East Germany, Poland, Czechoslovakia, Hungary) to specify them as communist states that were culturally tied to Western Europe. This usage continued after the end of the Warsaw Pact when these countries started to undergo transition.\n\nThe post-World War II period brought blocking of the research on Central Europe in the Eastern Bloc countries, as its every result proved the dissimilarity of Central Europe, which was inconsistent with the Stalinist doctrine. On the other hand, the topic became popular in Western Europe and the United States, much of the research being carried out by immigrants from Central Europe. At the end of the communism, publicists and historians in Central Europe, especially anti-communist opposition, came back to their research.\n\nAccording to Karl A. Sinnhuber (\"Central Europe: Mitteleuropa: Europe Centrale: An Analysis of a Geographical Term\") most Central European states were unable to preserve their political independence and became Soviet Satellite Europe. Besides Austria, only marginal Central European states of Finland and Yugoslavia did preserve their political sovereignty to a certain degree, being left out from any military alliances in Europe.\n\nAccording to \"Meyers Enzyklopädisches Lexikon\", Central Europe is a part of Europe composed by the surface of the Austria, Belgium, Czechoslovakia, Germany, Hungary, Luxembourg, Netherlands, Poland, Romania and Switzerland, and northern marginal regions of Italy and Yugoslavia (northern states – Croatia, Vojvodina and Slovenia), as well as northeastern France.\n\nRather than a physical entity, Central Europe is a concept of shared history which contrasts with that of the surrounding regions. The issue of how to name and define the Central European region is subject to debates. Very often, the definition depends on the nationality and historical perspective of its author.\n\nMain propositions, gathered by Jerzy Kłoczowski, include:\n\n\nAccording to Ronald Tiersky, the 1991 summit held in Visegrád, Hungary and attended by the Polish, Hungarian and Czechoslovak presidents was hailed at the time as a major breakthrough in Central European cooperation, but the Visegrád Group became a vehicle for coordinating Central Europe's road to the European Union, while development of closer ties within the region languished.\n\nPeter J. Katzenstein described Central Europe as a way station in a Europeanization process that marks the transformation process of the Visegrád Group countries in different, though comparable ways. According to him, in Germany's contemporary public discourse \"Central European identity\" refers to the civilizational divide between Roman Catholicism and Eastern Orthodoxy. He says there's no precise, uncontestable way to decide whether the Baltic states, Serbia, Croatia, Slovenia, Romania, and Bulgaria are parts of Central Europe or not.\n\nLonnie R. Johnson points out criteria to distinguish Central Europe from Western, Eastern and Southeast Europe:\n\nHe also thinks that Central Europe is a dynamic historical concept, not a static spatial one. For example, Lithuania, a fair share of Belarus and western Ukraine are in Eastern Europe today, but years ago they were in Polish–Lithuanian Commonwealth. Johnson's study on Central Europe received acclaim and positive reviews in the scientific community. However, according to Romanian researcher Maria Bucur this very ambitious project suffers from the weaknesses imposed by its scope (almost 1600 years of history).\n\n\"The Columbia Encyclopedia\" defines Central Europe as: Germany, Switzerland, Liechtenstein, Austria, Poland, the Czech Republic, Slovakia, and Hungary. The World Factbook uses the same definition adding Slovenia too. Encarta Encyclopedia and Encyclopædia Britannica do not clearly define the region, but Encarta places the same countries into Central Europe in its individual articles on countries, adding Slovenia in \"south central Europe\".\n\nThe German Encyclopaedia \"Meyers Grosses Taschenlexikon\" (), 1999, defines Central Europe as the central part of Europe with no precise borders to the East and West. The term is mostly used to denominate the territory between the Schelde to Vistula and from the Danube to the Moravian Gate. Usually the countries considered to be Central European are Austria, Croatia, the Czech Republic, Germany, Hungary, Poland, Slovakia, Slovenia, Switzerland; in the broader sense Romania too, occasionally also Belgium, the Netherlands, and Luxembourg.\n\nThe comprehension of the concept of \"Central Europe\" is an ongoing source of controversy, though the Visegrád Group constituents are almost always included as \"de facto\" C.E. countries. Although views on which countries belong to Central Europe are vastly varied, according to many sources (see section Current views on Central Europe) the region includes the states listed in the sections below.\n\nDepending on context, Central European countries are sometimes grouped as Eastern or Western European countries, collectively or individually but some place them in Eastern Europe instead: for instance Austria can be referred to as Central European, as well as Eastern European or Western European.\n\nSome sources also add neighbouring countries for historical reasons (the former Austro-Hungarian and German Empires, and modern Baltic states), or based on geographical and/or cultural reasons:\n\n\nThe Baltic states, geographically located in Northern Europe, have been considered part of Central Europe in the German tradition of the term, \"Mitteleuropa\". Benelux countries are generally considered a part of Western Europe, rather than Central Europe. Nevertheless, they are occasionally mentioned in the Central European context due to cultural, historical and linguistic ties.\n\nThe following states or some of their regions may sometimes be included in Central Europe:\n\nGeography defines Central Europe's natural borders with the neighbouring regions to the North across the Baltic Sea namely the Northern Europe (or Scandinavia), and to the South across the Alps, the Apennine peninsula (or Italy), and the Balkan peninsula across the Soča-Krka-Sava-Danube line. The borders to Western Europe and Eastern Europe are geographically less defined and for this reason the cultural and historical boundaries migrate more easily West-East than South-North. The Rhine river which runs South-North through Western Germany is an exception.\n\nSouthwards, the Pannonian Plain is bounded by the rivers Sava and Danube- and their respective floodplains. The Pannonian Plain stretches over the following countries: Austria, Croatia, Hungary, Romania, Serbia, Slovakia and Slovenia, and touches borders of Bosnia and Herzegovina (Republika Srpska) and Ukraine (\"peri- Pannonian states\").\n\nAs southeastern division of the Eastern Alps, the Dinaric Alps extend for 650 kilometres along the coast of the Adriatic Sea (northwest-southeast), from the Julian Alps in the northwest down to the Šar-Korab massif, north-south. According to the Freie Universitaet Berlin, this mountain chain is classified as South Central European.\n\nThe Central European flora region stretches from Central France (the Massif Central) to Central Romania (Carpathians) and Southern Scandinavia.\n\nAt times, the term \"Central Europe\" denotes a geographic definition as the Danube region in the heart of the continent, including the language and culture areas which are today included in the states of Croatia, the Czech Republic, Hungary, Poland, Serbia, Slovakia, Slovenia and usually also Austria and Germany, but \"never\" Russia and other countries of the former Soviet Union towards the Ural mountains.\n\n\nCentral Europe is one of continent's most populous regions. It includes countries of varied sizes, ranging from tiny Liechtenstein to Germany, the largest European country by population (that is entirely placed in Europe). Demographic figures for countries entirely located within notion of Central Europe (\"the core countries\") number around 165 million people, out of which around 82 million are residents of Germany. Other populations include: Poland with around 38.5 million residents, Czech Republic at 10.5 million, Hungary at 10 million, Austria with 8.5 million, Switzerland with its 8 million inhabitants, Slovakia at 5.4 million, Croatia with its 4.3 million residents, Slovenia at 2 million (2014 estimate) and Liechtenstein at a bit less than 40,000.\nIf the countries which are occasionally included in Central Europe were counted in, partially or in whole – Romania (20 million), Lithuania (2.9 million), Latvia (2 million), Estonia (1.3 million) – it would contribute to the rise of between 25–35 million, depending on whether regional or integral approach was used. If smaller, western and eastern historical parts of Central Europe would be included in the demographic corpus, further 20 million people of different nationalities would also be added in the overall count, it would surpass the 200 million people figure.\n\nCurrently, the members of the Eurozone include Austria, Germany, Luxembourg, Slovakia, and Slovenia. Croatia, the Czech Republic, Hungary and Poland use their currencies (Croatian kuna, Czech koruna, Hungarian forint, Polish złoty), but are obliged to adopt the Euro. Switzerland uses its own currency, Swiss franc.\n\nCountries in descending order of Human Development Index (2014 data):\n\nThe index of globalization in Central European countries (2015 data):\n\n\nLegatum Prosperity Index demonstrates an average and high level of prosperity in Central Europe (2016 data):\n\nMost countries in Central Europe score tend to score above the average in the Corruption Perceptions Index (2015 data):\n\nAccording to the Bribe Payers Index, released yearly since 1995 by the Berlin-based NGO Transparency International, Germany and Switzerland, the only two Central European countries examined in the study, were respectively ranked 2nd and 4th in 2011.\n\nIndustrialisation occurred early in Central Europe. That caused construction of rail and other types of infrastructure.\n\nCentral Europe contains the continent's earliest railway systems, whose greatest expansion was recorded in Austro-Hungarian and German territories between 1860-1870s. By the mid-19th century Berlin, Vienna, and Buda/Pest were focal points for network lines connecting industrial areas of Saxony, Silesia, Bohemia, Moravia and Lower Austria with the Baltic (Kiel, Szczecin) and Adriatic (Rijeka, Trieste). Rail infrastructure in Central Europe remains the densest in the world. Railway density, with total length of lines operated (km) per 1,000 km2, is the highest in the Czech Republic (198.6), Poland (121.0), Slovenia (108.0), Germany (105.5), Hungary (98.7), Slovakia (73.9) and Croatia (72.5). when compared with most of Europe and the rest of the world.\n\nBefore the first railroads appeared in the 1840s, river transport constituted the main means of communication and trade. Earliest canals included Plauen Canal (1745), Finow Canal, and also Bega Canal (1710) which connected Timișoara to Novi Sad and Belgrade via Danube. The most significant achievement in this regard was the facilitation of navigability on Danube from the Black sea to Ulm in the 19th century.\n\nCompared to most of Europe, the economies of Austria, Croatia, the Czech Republic, Germany, Hungary, Poland, Slovakia, Slovenia and Switzerland tend to demonstrate high complexity. Industrialisation has reached Central Europe relatively early: Luxembourg and Germany by 1860, the Czech Republic, Poland, Slovakia and Switzerland by 1870, Austria, Croatia, Hungary, Liechtenstein, Romania, Serbia and Slovenia by 1880.\n\nCentral European countries are some of the most significant food producers in the world. Germany is the world's largest hops producer with 34.27% share in 2010, third producer of rye and barley, 5th rapeseed producer, sixth largest milk producer, and fifth largest potato producer. Poland is the world's largest triticale producer, second largest producer of raspberry, currant, third largest of rye, the fifth apple and buckwheat producer, and seventh largest producer of potatoes. The Czech Republic is world's fourth largest hops producer and 8th producer of triticale. Hungary is world's fifth hops and seventh largest triticale producer. Serbia is world's second largest producer of plums and second largest of raspberries. Slovenia is world's sixth hops producer.\n\nCentral European business has a regional organisation, Central European Business Association (CEBA), founded in 1996 in New York as a non-profit organization dedicated to promoting business opportunities within Central Europe and supporting the advancement of professionals in America with a Central European background.\n\nCentral European countries, especially Austria, Croatia, Germany and Switzerland are some of the most competitive tourism destinations. Poland is presently a major destination for outsourcing.\n\nKraków, Warsaw, and Wroclaw, Poland; Prague and Brno, Czech Republic; Budapest, Hungary; Bucharest, Romania; Bratislava, Slovakia; Ljubljana, Slovenia and Zagreb, Croatia are among the world's top 100 outsourcing destinations.\n\nCentral European countries are very literate. All of them have the literacy rate of 96% or over (for both sexes):\nLanguages taught as the first language in Central Europe are: Croatian, Czech, French, German, Hungarian, Italian, Polish, Romansh, Slovak and Slovenian. The most popular language taught at schools in Central Europe as foreign languages are: English, French and German.\n\nProficiency in English is ranked as high or moderate, according to the EF English Proficiency Index:\n\nOther languages, also popular (spoken by over 5% as a second language):\n\nStudent performance has varied across Central Europe, according to the Programme for International Student Assessment. In the last study, countries scored medium, below or over the average scores in three fields studied.\n\nIn maths:\n\n\nIn the sciences:\n\n\nIn reading:\n\n\nThe first university east of France and north of the Alps was the Charles University in Prague established in 1347 or 1348 by Charles IV, Holy Roman Emperor and modeled on the University of Paris, with the full number of faculties (law, medicine, philosophy and theology). The list of Central Europe's oldest universities in continuous operation, established by 1500, include (by their dates of foundation):\n\nThe Central European University (CEU) is a graduate-level, English-language university promoting a distinctively Central European perspective. It was established in 1991 by the Hungarian philanthropist George Soros, who has provided an endowment of US$880 million, making the university one of the wealthiest in Europe. In the academic year 2013/2014, the CEU had 1,381 students from 93 countries and 388 faculty members from 58 countries.\n\nCentral European Exchange Program for University Studies (CEEPUS) is an international exchange program for students and teachers teaching or studying in participating countries. Its current members include (year it joined for the first time in brackets):\n\nResearch centres of Central European literature include Harvard (Cambridge, MA), Purdue University\n\nCentral European architecture has been shaped by major European styles including but not limited to: Brick Gothic, Rococo, Secession (art) and Modern architecture. Six Central European countries are amongst those countries with higher numbers of World Heritage Sites:\n\nCentral European countries are mostly Roman Catholic (Austria, Croatia, Liechtenstein, Luxembourg, Poland, Slovakia, Slovenia) or mixed Catholic and Protestant, (Germany, Hungary and Switzerland). Large Protestant groups include Lutheran and Calvinist. Significant populations of Eastern Catholicism and Old Catholicism are also prevalent throughout Central Europe. Central Europe has been a centre of Protestantism in the past; however, it has been mostly eradicated by the Counterreformation. The Czech Republic (Bohemia) was historically the first Protestant country, then violently recatholised, and now overwhelmingly non-religious, nevertheless the largest number of religious people are Catholic (10.3%). Romania and Serbia are mostly Eastern Orthodox with significant Protestant and Catholic minorities.\n\nBefore the Holocaust (1941–45), there was also a sizeable Ashkenazi Jewish community in the region, numbering approximately 16.7 million people.\n\nIn some of these countries, there is a number of atheists, undeclared and non-religious people: the Czech Republic (non-religious 34.2% and undeclared 45.2%), Germany (non-religious 38%), Slovenia (atheist 30.2%), Luxembourg (25% non-religious), Switzerland (20.1%), Hungary (27.2% undeclared, 16.7% \"non-religious\" and 1.5% atheists), Slovakia (atheists and non-religious 13.4%, \"not specified\" 10.6%) Austria (19.7% of \"other or none\"), Liechtenstein (10.6% with no religion), Croatia (4%) and Poland (3% of non-believers/agnostics and 1% of undeclared).\n\nCentral European cuisine has evolved through centuries due to social and political change. Most countries share many dishes. The most popular dishes typical to Central Europe are sausages and cheeses, where the earliest evidence of cheesemaking in the archaeological record dates back to 5,500 BCE (Kujawy, Poland). Other foods widely associated with Central Europe are goulash and beer. List of countries by beer consumption per capita is led by the Czech Republic, followed by Germany and Austria. Poland comes 5th, Croatia 7th and Slovenia 13th.\n\nHuman rights have a long tradition in Central Europe. In 1222 Hungary defined for the first time the rights of the nobility in its \"Golden Bull\". In 1264 the Statute of Kalisz and the General Charter of Jewish Liberties introduced numerous rights for the Jews in Poland, granting them de facto autonomy. In 1783 for the first time, Poland forbid corporal punishment of children in schools. In the same year, a German state of Baden banned slavery.\n\nOn the other hand, there were also major regressions, such as \"Nihil novi\" in Poland in 1505 which forbade peasants from leaving their land without permission from their feudal lord.\n\nGenerally, the countries in the region are progressive on the issue of human rights: death penalty is illegal in all of them, corporal punishment is outlawed in most of them and people of both genders can vote in elections. Nevertheless, Central European countries struggle to adopt new generations of human rights, such as same-sex marriage. Austria, the Czech Republic, Germany, and Poland also have a history of participation in the CIA's extraordinary rendition and detention program, according to the Open Society Foundation.\n\nRegional writing tradition revolves around the turbulent history of the region, as well as its cultural diversity, and its existence is sometimes challenged.\n\nSpecific courses on Central European literature are taught at Stanford University, Harvard University and Jagiellonian University The as well as cultural magazines dedicated to regional literature.\n\nAngelus Central European Literature Award is an award worth 150,000.00 PLN (about $50,000 or £30,000) for writers originating from the region.\n\nThere is a whole spectrum of media active in the region: newspapers, television and internet channels, radio channels, internet websites etc.\nCentral European media are regarded as free, according to the Press Freedom Index. Some of the top scoring countries are in Central Europe include:\n\nThere is a number of Central European Sport events and leagues. They include:\n\nFootball is one of the most popular sports. Countries of Central Europe had many great national teams throughout history and hosted several major competitions.\nYugoslavia hosted UEFA Euro 1976 before the competition expanded to 8 teams and Germany (at that times as West Germany) hosted UEFA Euro 1988. Recently, 2008 and 2012 UEFA European Championships were held in Austria & Switzerland and Poland & Ukraine respectively.\nGermany hosted 2 FIFA World Cups (1974 and 2006) and are the current champions (as of 2014).\n\nCentral Europe is a birthplace of regional political organisations:\n\nCentral Europe is a home to some of world's oldest democracies. However, most of them have been impacted by totalitarian rule, particularly Nazism (Germany, Austria, other occupied countries) and Communism. Most of Central Europe have been occupied and later allied with the USSR, often against their will through forged referendum (e.g., Polish people's referendum in 1946) or force (northeast Germany, Poland, Hungary et alia). Nevertheless, these experiences have been dealt in most of them. Most of Central European countries score very highly in the Democracy Index:\n\nIn spite of its turbulent history, Central Europe is currently one of world's safest regions. Most Central European countries are in top 20%:\n\nThe time zone used in most parts of the European Union is a standard time which is 1 hour ahead of Coordinated Universal Time. It is commonly called Central European Time because it has been first adopted in central Europe (by year):\n\nCentral Europe is mentioned in 35th episode of Lovejoy, entitled \"The Prague Sun\", filmed in 1992. While walking over the famous Charles Bridge, the main character, Lovejoy says: \" I've never been to Prague before. Well, it is one of the great unspoiled cities in Central Europe. Notice: I said: \"Central\", not \"Eastern\"! The Czechs are a bit funny about that, they think of Eastern Europeans as \"turnip heads\".\"\n\nWes Anderson's Oscar-winning film The Grand Budapest Hotel is regarded as a fictionalised celebration of the 1930s in Central Europe and region's musical tastes\n\n\n\n"}
{"id": "5192", "url": "https://en.wikipedia.org/wiki?curid=5192", "title": "Geography of Canada", "text": "Geography of Canada\n\nThe geography of Canada describes the geographic features of Canada, the world's second largest country in total area.\n\nSituated in northern North America (constituting 41% of the continent's area), Canada spans a vast, diverse territory between the North Pacific Ocean to the west and the North Atlantic Ocean to the east and the Arctic Ocean to the north (hence the country's motto \"From sea to sea\"), with the United States to the south (contiguous United States) and northwest (Alaska). Greenland is to the northeast; off the southern coast of Newfoundland lies Saint-Pierre and Miquelon, an overseas collectivity of France. Since 1925, Canada has claimed the portion of the Arctic between 60°W and 141°W longitude to the North Pole; however, this claim is contested. While the magnetic North Pole lies within the Canadian Arctic territorial claim as of 2011, recent measurements indicate it is moving towards Siberia.\n\nCovering (land: ; freshwater: ), Canada is slightly less than three-fifths as large as Russia and slightly smaller than Europe. In total area, Canada is slightly larger than both the U.S. and China; however, Canada ranks fourth in land area (i.e. total area minus the area of lakes and rivers)—China is and the U.S. is \n\nThe population of Canada, some 35,749,600 as of April 2015, is concentrated in the south in proximity to its border with the contiguous U.S.; with a population density of 3.5 people per square kilometre (9.1/sq mi), it is one of the most sparsely populated countries in the world. The northernmost settlement in Canada—and in the world—is Canadian Forces Station (CFS) Alert (just north of Alert, Nunavut) on the northern tip of Ellesmere Island at 82°30′N 62°19′W, just from the North Pole.\n\nCanada has a diverse climate. The climate varies from temperate on the west coast of British Columbia to a subarctic climate in the north. Extreme northern Canada can have snow for most of the year with a Polar climate. Landlocked areas tend to have a warm summer continental climate zone with the exception of Southwestern Ontario which has a hot summer humid continental climate. Parts of Western Canada have a semi-arid climate, and parts of Vancouver Island can even be classified as cool summer Mediterranean climate. Temperature extremes in Canada range from 45.0 °C (113 °F) in Midale and Yellow Grass, Saskatchewan on July 5, 1937 to −63.0 °C (−81.4 °F) in Snag, Yukon on Monday, February 3, 1947.\n\nCanada covers 9,984,670 km (3,855,103 sq. miles) and a panoply of various geoclimatic regions. There are 8 main regions. Canada also encompasses vast maritime terrain, with the world's longest coastline of . The physical geography of Canada is widely varied. Boreal forests prevail throughout the country, ice is prominent in northerly Arctic regions and through the Rocky Mountains, and the relatively flat Canadian Prairies in the southwest facilitate productive agriculture. The Great Lakes feed the St. Lawrence River (in the southeast) where lowlands host much of Canada's population.\n\nThe Appalachian mountain range extends from Alabama through the Gaspé Peninsula and the Atlantic Provinces, creating rolling hills indented by river valleys. It also runs through parts of southern Quebec.\n\nThe Appalachian mountains (more specifically the Notre Dame and Long Range Mountains) are an old and eroded range of mountains, approximately 380 million years in age. Notable mountains in the Appalachians include Mount Jacques-Cartier (Quebec, and Mount Carleton (New Brunswick, ). Parts of the Appalachians are home to a rich endemic flora and fauna and are considered to have been nunataks during the last glaciation era.\n\nThe southern parts of Quebec and Ontario, in the section of the Great Lakes (bordered entirely by Ontario on the Canadian side) and St. Lawrence basin (often called St. Lawrence Lowlands), is another particularly rich sedimentary plain. Prior to its colonization and heavy urban sprawl of the 20th century, this Eastern Great Lakes lowland forests area was home to large mixed forests covering a mostly flat area of land between the Appalachian Mountains and the Canadian Shield. Most of this forest has been cut down through agriculture and logging operations, but the remaining forests are for the most part heavily protected. In this part of Canada the Gulf of St. Lawrence is one of the world's largest estuary (see Gulf of St. Lawrence lowland forests).\n\nWhile the relief of these lowlands is particularly flat and regular, a group of batholites known as the Monteregian Hills are spread along a mostly regular line across the area. The most notable are Montreal's Mount Royal and Mont Saint-Hilaire. These hills are known for a great richness in precious minerals.\n\nThe northeastern part of Alberta, northern parts of Saskatchewan, Manitoba, Ontario, and Quebec, as well as most of Labrador (the mainland portions of the province of Newfoundland and Labrador), are located on a vast rock base known as the Canadian Shield. The Shield mostly consists of eroded hilly terrain and contains many lakes and important rivers used for hydroelectric production, particularly in northern Quebec and Ontario. The shield also encloses an area of wetlands, the Hudson Bay lowlands. Some particular regions of the Shield are referred to as mountain ranges, including the Torngat and Laurentian Mountains.\n\nThe Shield cannot support intensive agriculture, although there is subsistence agriculture and small dairy farms in many of the river valleys and around the abundant lakes, particularly in the southern regions. Boreal forest covers much of the shield, with a mix of conifers that provide valuable timber resources in areas such as the Central Canadian Shield forests ecoregion that covers much of Northern Ontario. The region is known for its extensive mineral reserves.\n\nThe Canadian Shield is known for its vast minerals, such as emeralds, diamonds and copper. The Canadian shield is also called the mineral house.\n\nThe Canadian Prairies are part of a vast sedimentary plain covering much of Alberta, southern Saskatchewan, and southwestern Manitoba, as well as much of the region between the Rocky Mountains and the Great Slave and Great Bear lakes in Northwest Territories. The plains generally describes the expanses of (largely flat) arable agricultural land which sustain extensive grain farming operations in the southern part of the provinces. Despite this, some areas such as the Cypress Hills and the Alberta Badlands are quite hilly and the prairie provinces contain large areas of forest such as the Mid-Continental Canadian forests. The size is roughly ~.\n\nThe Canadian Cordillera, contiguous with the American cordillera, is bounded by the Rocky Mountains to the east and the Pacific Ocean to the west.\n\nThe Canadian Rockies are part of a major continental divide that extends north and south through western North America and western South America. The Columbia and the Fraser Rivers have their headwaters in the Canadian Rockies and are the second and third largest rivers respectively to drain to the west coast of North America. To the west of their headwaters, across the Rocky Mountain Trench, is a second belt of mountains, the Columbia Mountains, comprising the Selkirk, Purcell, Monashee and Cariboo Mountains sub-ranges.\n\nImmediately west of the Columbia Mountains is a large and rugged Interior Plateau, encompassing the Chilcotin and Cariboo regions in central British Columbia (the Fraser Plateau), the Nechako Plateau further north, and also the Thompson Plateau in the south. The Peace River Valley in northeastern British Columbia is Canada's most northerly agricultural region, although it is part of the Prairies. The dry, temperate climate of the Okanagan Valley in south central British Columbia provides ideal conditions for fruit growing and a flourishing wine industry; the semi-arid belt of the Southern Interior also includes the Fraser Canyon, and Thompson, Nicola, Similkameen, Shuswap and Boundary regions and fruit-growing is common in these areas also, and also in the West Kootenay. Between the plateau and the coast is the province's largest mountain range, the Coast Mountains. The Coast Mountains contain some of the largest temperate-latitude icefields in the world.\n\nOn the south coast of British Columbia, Vancouver Island is separated from the mainland by the continuous Juan de Fuca, Georgia, and Johnstone Straits. Those straits include a large number of islands, notably the Gulf Islands and Discovery Islands. North, near the Alaskan border, Haida Gwaii lies across Hecate Strait from the North Coast region and to its north, across Dixon Entrance from Southeast Alaska. Other than in the plateau regions of the Interior and its many river valleys, most of British Columbia is coniferous forest. The only temperate rain forests in Canada are found along the Pacific Coast in the Coast Mountains, on Vancouver Island, and on Haida Gwaii, and in the Cariboo Mountains on the eastern flank of the Plateau.\n\nThe Western Cordillera continues northwards past the Liard River in northernmost British Columbia to include the Mackenzie and Selwyn Ranges which lie in the far western Northwest Territories and the eastern Yukon Territory. West of them is the large Yukon Plateau and, west of that, the Yukon Ranges and Saint Elias Mountains, which include Canada's and British Columbia's highest summits, Mount Saint Elias in the Kluane region and Mount Fairweather in the Tatshenshini-Alsek region. The headwaters of the Yukon River, the largest and longest of the rivers on the Pacific Slope, lie in northern British Columbia at Atlin and Teslin Lakes.\n\nWestern Canada has many volcanoes and is part of the Pacific Ring of Fire, a system of volcanoes found around the margins of the Pacific Ocean. There are over 200 young volcanic centres that stretch northward from the Cascade Range to Yukon. They are grouped into five volcanic belts with different volcano types and tectonic settings. The Northern Cordilleran Volcanic Province was formed by faulting, cracking, rifting, and the interaction between the Pacific Plate and the North American plate. The Garibaldi Volcanic Belt was formed by subduction of the Juan de Fuca Plate beneath the North American Plate. The Anahim Volcanic Belt was formed as a result of the North American Plate sliding westward over the Anahim hotspot. The Chilcotin Group is believed to have formed as a result of back-arc extension behind the Cascadia subduction zone. The Wrangell Volcanic Field formed as a result of subduction of the Pacific Plate beneath the North American Plate at the easternmost end of the Aleutian Trench.\n\nVolcanism has also occurred in the Canadian Shield. It contains over 150 volcanic belts (now deformed and eroded down to nearly flat plains) that range from 600 million to 2.8 billion years old. Many of Canada's major ore deposits are associated with Precambrian volcanoes. There are pillow lavas in the Northwest Territories that are about 2.6 billion years old and are preserved in the Cameron River Volcanic Belt. The pillow lavas in rocks over 2 billion years old in the Canadian Shield signify that great oceanic volcanoes existed during the early stages of the formation of the Earth's crust. Ancient volcanoes play an important role in estimating Canada's mineral potential. Many of the volcanic belts bear ore deposits that are related to the volcanism.\n\nWhile the largest part of the Canadian Arctic is composed of seemingly endless permafrost and tundra north of the tree line, it encompasses geological regions of varying types: the Arctic Cordillera (with the British Empire Range and the United States Range on Ellesmere Island) contains the northernmost mountain system in the world. The Arctic Lowlands and Hudson Bay lowlands comprise a substantial part of the geographic region often designated as the Canadian Shield (in contrast to the sole geologic area). The ground in the Arctic is mostly composed of permafrost, making construction difficult and often hazardous, and agriculture virtually impossible.\n\nThe Arctic, when defined as everything north of the tree line, covers most of Nunavut and the northernmost parts of Northwest Territories, Yukon, Manitoba, Ontario, Quebec, and Labrador.\n\nCanada holds vast reserves of water: its rivers discharge nearly 9% of the world's renewable water supply, it contains a quarter of the world's wetlands, and it has the third largest amount of glaciers (after Antarctica and Greenland). Because of extensive glaciation, Canada hosts more than two million lakes: of those that are entirely within Canada, more than 31,000 are between in area, while 563 are larger than .\n\nCanada’s two longest rivers are the Mackenzie, which empties into the Arctic Ocean and drains a large part of northwestern Canada, and the St. Lawrence, which drains the Great Lakes and empties into the Gulf of St. Lawrence. The Mackenzie is over in length while the St. Lawrence is over in length. Rounding out the ten longest rivers within Canada are the Nelson, Churchill, Peace, Fraser, North Saskatchewan, Ottawa, Athabasca and Yukon rivers.\n\nThe Atlantic watershed drains the entirety of the Atlantic provinces (parts of the Quebec-Labrador border are fixed at the Atlantic Ocean-Arctic Ocean continental divide), most of inhabited Quebec and large parts of southern Ontario. It is mostly drained by the economically important St. Lawrence River and its tributaries, notably the Saguenay, Manicouagan and Ottawa rivers. The Great Lakes and Lake Nipigon are also drained by the St. Lawrence. The Churchill River and Saint John River are other important elements of the Atlantic watershed in Canada.\n\nThe Hudson Bay watershed drains over a third of Canada. It covers Manitoba, northern Ontario and Quebec, most of Saskatchewan, southern Alberta, southwestern Nunavut and the southern half of Baffin Island. This basin is most important in fighting drought in the prairies and producing hydroelectricity, especially in Manitoba, northern Ontario and Quebec. Major elements of this watershed include Lake Winnipeg, Nelson River, the North Saskatchewan and South Saskatchewan Rivers, Assiniboine River, and Nettilling Lake on Baffin Island. Wollaston Lake lies on the boundary between the Hudson Bay and Arctic Ocean watersheds and drains into both. It is the largest lake in the world that naturally drains in two directions.\n\nThe continental divide in the Rockies separates the Pacific watershed in British Columbia and Yukon from the Arctic and Hudson Bay watersheds. This watershed irrigates the agriculturally important areas of inner British Columbia (such as the Okanagan and Kootenay valleys), and is used to produce hydroelectricity. Major elements are the Yukon, Columbia and Fraser rivers.\n\nThe northern parts of Alberta, Manitoba and British Columbia, most of Northwest Territories and Nunavut, and parts of Yukon are drained by the Arctic watershed. This watershed has been little used for hydroelectricity, with the exception of the Mackenzie River, the longest river in Canada. The Peace, Athabasca and Liard Rivers, as well as Great Bear Lake and Great Slave Lake (respectively the largest and second largest lakes wholly enclosed by Canada) are significant elements of the Arctic watershed. Each of these elements eventually merges with the Mackenzie, thereby draining the vast majority of the Arctic watershed.\n\nThe southernmost part of Alberta drains into the Gulf of Mexico through the Milk River and its tributaries. The Milk River originates in the Rocky Mountains of Montana, then flows into Alberta, then returns into the United States, where it is drained by the Missouri River. A small area of southwestern Saskatchewan is drained by Battle Creek, which empties into the Milk River.\n\nCanada has produced a Biodiversity Action Plan in response to the 1992 international accord; the plan addresses conservation of endangered species and certain habitats. The main biomes of Canada are:\n\n\nCanada is divided into ten provinces and three territories. According to Statistics Canada, 72.0 percent of the population is concentrated within of the nation's southern border with the United States, 70.0% live south of the 49th parallel, and over 60 percent of the population lives along the Great Lakes and St. Lawrence River between Windsor, Ontario and Quebec City. This leaves the vast majority of Canada's territory as sparsely populated wilderness; Canada's population density is 3.5 people/km (9.1/mi), among the lowest in the world. Despite this, 79.7 percent of Canada's population resides in urban areas, where population densities are increasing.\n\nCanada shares with the U.S. the world's longest undefended border at ; are with Alaska. The Danish island dependency of Greenland lies to Canada's northeast, separated from the Canadian Arctic islands by Baffin Bay and Davis Strait. The French islands of Saint-Pierre and Miquelon lie off the southern coast of Newfoundland in the Gulf of St. Lawrence and have a maritime territorial enclave within Canada's Exclusive Economic Zone. Canada also shares a land border with Denmark, as maps released in December 2006 show that the agreed upon boundaries run through the middle of Hans Island.\n\nCanada's geographic proximity to the United States has historically bound the two countries together in the political world as well. Canada's position between the Soviet Union (now Russia) and the U.S. was strategically important during the Cold War since the route over the North Pole and Canada was the fastest route by air between the two countries and the most direct route for intercontinental ballistic missiles. Since the end of the Cold War, there has been growing speculation that Canada's Arctic maritime claims may become increasingly important if global warming melts the ice enough to open the Northwest Passage.\n\nSimilarly, the disputed—and tiny—Hans Island (with Denmark), in the Nares Strait between Ellesmere Island and northern Greenland, may be a flashpoint for challenges to overall claims of Canadian sovereignty in the Arctic.\n\nCanada's abundance of natural resources is reflected in their continued importance in the economy of Canada. Major resource-based industries are fisheries, forestry, agriculture, petroleum products and mining.\n\nThe fisheries industry has historically been one of Canada's strongest. Unmatched cod stocks on the Grand Banks off Newfoundland launched this industry in the 16th century. Today these stocks are nearly depleted, and their conservation has become a preoccupation of the Atlantic Provinces. On the West Coast, tuna stocks are now restricted. The less depleted (but still greatly diminished) salmon population continues to drive a strong fisheries industry. Canada claims of territorial sea, a contiguous zone of , an exclusive economic zone of and a continental shelf of or to the edge of the continental margin.\n\nForestry has long been a major industry in Canada. Forest products contribute one fifth of the nation's exports. The provinces with the largest forestry industries are British Columbia, Ontario and Quebec. Fifty-four percent of Canada's land area is covered in forest. The boreal forests account for four-fifths of Canada's forestland.\nFive per cent of Canada's land area is arable, none of which is for permanent crops. Three per cent of Canada's land area is covered by permanent pastures. Canada has 7,200 square kilometres (2,800 mi) of irrigated land (1993 estimate). Agricultural regions in Canada include the Canadian Prairies, the Lower Mainland and various regions within the Interior of British Columbia, the St. Lawrence Basin and the Canadian Maritimes. Main crops in Canada include flax, oats, wheat, maize, barley, sugar beets and rye in the prairies; flax and maize in Western Ontario; Oats and potatoes in the Maritimes. Fruit and vegetables are grown primarily in the Annapolis Valley of Nova Scotia, Southwestern Ontario, the Golden Horseshoe region of Ontario, along the south coast of Georgian Bay and in the Okanagan Valley of British Columbia. Cattle and sheep are raised in the valleys and plateaus of British Columbia. Cattle, sheep and hogs are raised on the prairies, cattle and hogs in Western Ontario, sheep and hogs in Quebec, and sheep in the Maritimes. There are significant dairy regions in central Nova Scotia, southern New Brunswick, the St. Lawrence Valley, northeastern Ontario, southwestern Ontario, the Red River valley of Manitoba and the valleys in the British Columbia Interior, on Vancouver Island and in the Lower Mainland.\n\nFossil fuels are a more recently developed resource in Canada, with oil and gas being extracted from deposits in the Western Canadian Sedimentary Basin since the mid 1900s. While Canada's crude oil deposits are fewer, technological developments in recent decades have opened up oil production in Alberta's Oil Sands to the point where Canada now has some of the largest reserves of oil in the world. In other forms, Canadian industry has a long history of extracting large coal and natural gas reserves.\n\nCanada's mineral resources are diverse and extensive. Across the Canadian Shield and in the north there are large iron, nickel, zinc, copper, gold, lead, molybdenum, and uranium reserves. Large diamond concentrations have been recently developed in the Arctic, making Canada one of the world's largest producers. Throughout the Shield there are many mining towns extracting these minerals. The largest, and best known, is Sudbury, Ontario. Sudbury is an exception to the normal process of forming minerals in the Shield since there is significant evidence that the Sudbury Basin is an ancient meteorite impact crater. The nearby, but less known Temagami Magnetic Anomaly has striking similarities to the Sudbury Basin. Its magnetic anomalies are very similar to the Sudbury Basin, and so it could be a second metal-rich impact crater. The Shield is also covered by vast boreal forests that support an important logging industry.\n\nCanada's many rivers have afforded extensive development of hydroelectric power. Extensively developed in British Columbia, Ontario, Quebec and Labrador, the many dams have long provided a clean, dependable source of energy.\n\nContinuous permafrost in the north is a serious obstacle to development. Cyclonic storms form east of the Rocky Mountains, a result of the mixing of air masses from the Arctic, Pacific, and North American interior, and produce most of the country's rain and snow east of the mountains.\n\nAir pollution and resulting acid rain severely affects lakes and damages forests. Metal smelting, coal-burning utilities, and vehicle emissions impact agricultural and forest productivity. And ocean waters are becoming contaminated from agricultural, industrial, mining, and forestry activities.\n\nGlobal climate change and the warming of the polar region will likely cause significant changes to the environment, including loss of the polar bear, the exploration for resource then the extraction of these resources and an alternative transport route to the Panama Canal through the Northwest Passage.\n\nThe northernmost point within the boundaries of Canada is Cape Columbia, Ellesmere Island, Nunavut . The northernmost point of the Canadian mainland is Zenith Point on Boothia Peninsula, Nunavut .\n\nThe southernmost point is Middle Island, in Lake Erie, Ontario (41°41′N, 82°40′W); the southernmost water point lies just south of the island, on the Ontario–Ohio border (41°40′35″N). The southernmost point of the Canadian mainland is Point Pelee, Ontario .\n\nThe westernmost point is Boundary Peak 187 (60°18′22.929″N, 141°00′7.128″W) at the southern end of the Yukon–Alaska border which is roughly following 141°W but leans very slightly east as it goes North .\n\nThe easternmost point is Cape Spear, Newfoundland (47°31′N, 52°37′W) . The easternmost point of the Canadian mainland is Elijah Point, Cape St. Charles, Labrador (52°13′N, 55°37′W) .\n\nThe lowest point is sea level at 0 m, whilst the highest point is Mount Logan, Yukon, at 5,959 m / 19,550 ft .\n\nThe Canadian pole of inaccessibility is allegedly near Jackfish River, Alberta (Latitude: 59°2′ 60 N, Longitude: 112°49′ 60 W).\n\nThe furthest straight-line distance that can be travelled to Canadian points of land is between the northwest tip of Ivvavik National Park (at Clarence Lagoon) and Cripple Cove, NL (near Cape Race) at a distance of .\n\n\n"}
{"id": "5193", "url": "https://en.wikipedia.org/wiki?curid=5193", "title": "Demographics of Canada", "text": "Demographics of Canada\n\nThis article is about the demographic features of the population of Canada, including population density, ethnicity, religious affiliations and other aspects of the population, the People of Canada.\n\nThe Canada 2016 Census had a total population count of 35,151,728 individuals, making up approximately 0.5% of the world's total population. Estimates have the population over 36 million as of July 2016.\n\nAccording to OECD/World Bank, the population in Canada increased from 1990 to 2008 with 5.6 million and 20.4% growth in population, compared to 21.7% growth in the United States and 31.2% growth in Mexico. According to the OECD/World Bank population statistics, between 1990–2008 the world population growth was 27%, a total of 1,423 million people.\n\nDerived from: Statistics Canada – (table) Population and Dwelling Counts, for Canada, Provinces and Territories, 2011 and 2006 Censuses – 100% Data\n\n(p)=January–September 2016\n\n\nAge characteristics:\nNet migration rate: 5.65 migrant(s)/1,000 population (2013 est.)\n\nUrbanization:\nSex ratio:\n\nMaternal mortality rate: 12 deaths/100,000 live births (2010 est.)\n\nLife expectancy:\n\n\nMedian age by province and territory, 2011\n\nTotal: 40.6\n\nIn the 2006 census, Canadians could identify as being of one or more ethnicities. Percentages therefore add up to more than 100%. The most common response was \"Canadian\". As data is completely self-reported, and reporting individuals may have varying definitions of \"Ethnic origin\" (or may not know their ethnic origin), these figures should not be considered an exact record of the relative prevalence of different ethno-cultural ancestries but rather how Canadians self-identify.\n\nStatistics Canada projects that, by 2031, about 28% of the population will be foreign-born. The number of people belonging to visible minority groups will double, and make up the majority of the population in Toronto and Vancouver.\n\nCounting both single and multiple responses, the most commonly identified ethnic origins were (2011):\nData from the same subject matter, though from 2001, is also grouped more geographically by Statistics Canada as follows:\n\nThe most common ethnic origins per province are as follows (total responses; only percentages 10% or higher shown; ordered by percentage of \"Canadian\"):\n\nBold indicates either that this response is dominant within this province, or that this province has the highest ratio (percentage) of this response among provinces.\n\nStatistics Canada identifies visible minorities in accordance with the \"Employment Equity Act\". Statistics Canada states the \"\"Employment Equity Act\" defines visible minorities as 'persons, other than Aboriginal peoples, who are non-Caucasian in race or non-white in colour.'\"\n\nAll statistics are from the Canada 2011 Census.\n\n\"Note: Inuit, other Aboriginal and mixed Aboriginal groups are not listed as their own, but they are all accounted for in total Aboriginal\"\n\nAll statistics are from the Canada 2011 Census.\n\nLanguage used most often at work:\n\nLanguages by language used most often at home:\n\nLanguages by mother tongue:\n\nStatistics Canada (StatCan) grouped responses to the 2011 National Household Survey (NHS) question on religion into nine core religious categories – Buddhist, Christian, Hindu, Jewish, Muslim, Sikh, Traditional (Aboriginal) Spirituality, other religions and no religious affiliation. Among these, of Canadians were self-identified as Christians in 2011. The second and third-largest categories were of Canadians with no religious affiliation at and Canadian Muslims at .\n\nWithin the 2011 NHS results, StatCan further subcategorized Christianity in nine groups of its own – Anglican, Baptist, Catholic, Christian Orthodox, Lutheran, Pentecostal, Presbyterian, United Church and Other Christian. Among these, of Canadians were self-identified as Catholic in 2011. The second and third-largest ungrouped subcategories of Christian Canadians were United at and Anglican at , while of Christians were grouped into the Other Christian subcategory comprising numerous denominations.\n\nOf the 3,036,785 or of Canadians identified as Other Christians:\n\n\n"}
{"id": "5194", "url": "https://en.wikipedia.org/wiki?curid=5194", "title": "Politics of Canada", "text": "Politics of Canada\n\nThe politics of Canada function within a framework of parliamentary democracy and a federal system of parliamentary government with strong democratic traditions. Canada is a constitutional monarchy, in which the Monarch is head of state. The country has a multi-party system in which many of its legislative practices derive from the unwritten conventions of and precedents set by the United Kingdom's Westminster Parliament. However, Canada has evolved variations: party discipline in Canada is stronger than in the United Kingdom and more parliamentary votes are considered motions of confidence, which tends to diminish the role of non-Cabinet Members of Parliament, (MPs). Such members, in the government caucus, and junior or lower-profile members of opposition caucuses, are known as backbenchers. Backbenchers can, however, exert their influence by sitting in parliamentary committees, like the Public Accounts Committee or the National-Defence Committee.\n\nThe two dominant political parties in Canada have historically been the Liberal Party of Canada and Conservative Party of Canada (or its predecessors), however, the social-democratic New Democratic Party (NDP) has risen to prominence, and even threatened to upset the two other established parties during the 2011 election and the 2015 election. Smaller parties like the Quebec nationalist Bloc Québécois, and the Green Party of Canada have also been able to exert their own influence over the political process. Far-right and Far-left politics has never been a prominent force in Canadian society.\n\nCanada's governmental structure was originally established by the British Parliament through the British North America Act (now known as the Constitution Act, 1867), but the federal model and division of powers were devised by Canadian politicians. Particularly after World War I, citizens of the self-governing Dominions, such as Canada, began to develop a strong sense of identity, and, in the Balfour Declaration of 1926, the British government expressed its intent to grant full autonomy to these regions.\n\nThus in 1931, the British Parliament passed the Statute of Westminster, giving legal recognition to the autonomy of Canada and other Dominions. Following this, Canadian politicians were unable to obtain consensus on a process for amending the constitution until 1982, meaning amendments to Canada's constitution continued to require the approval of the British parliament until that date. Similarly, the Judicial Committee of the Privy Council in Britain continued to make the final decision on criminal appeals until 1933 and on civil appeals until 1949.\n\n\n\n\n\n\n\n\n\n\n\n\nThe bicameral Parliament of Canada consists of three parts: the monarch, the Senate, and the House of Commons.\n\nCurrently, the Senate, which is frequently described as providing \"regional\" representation, has 105 members appointed by the Governor General on the advice of the Prime Minister to serve until age 75. It was created with equal representation from each of Ontario, Quebec, the Maritime region and the Western Provinces. However, it is currently the product of various specific exceptions, additions and compromises, meaning that regional equality is not observed, nor is representation-by-population. The normal number of senators can be exceeded by the monarch on the advice of the Prime Minister, as long as the additional senators are distributed equally with regard to region (up to a total of eight additional Senators). This power of additional appointment has only been used once, when Prime Minister Brian Mulroney petitioned Queen Elizabeth II to add eight seats to the Senate so as to ensure the passage of the Goods and Services Tax legislation.\n\nThe House of Commons currently has 338 members elected in single-member districts in a plurality voting system (first past the post), meaning that members must attain only a plurality (the most votes of any candidate) rather than a majority (50 percent plus one). The electoral districts are also known as ridings.\n\nMandates cannot exceed five years; an election must occur by the end of this time. This fixed mandate has been exceeded only once, when Prime Minister Robert Borden perceived the need to do so during World War I. The size of the House and apportionment of seats to each province is revised after every census, conducted every five years, and is based on population changes and approximately on representation-by-population.\n\nCanadians vote for their local Member of Parliament (MP) only. The party leaders are elected prior to the general elections by party memberships. Parties elect their leaders in run-off elections to ensure that the winner receives more than 50% of the votes. Normally the party leader stands as a candidate to be an MP during an election.\n\nThe election of a local MP gives a seat to one of the several political parties. The party that gets the most seats normally forms the government, with that party's leader becoming prime minister. The Prime Minister is not directly elected by the general population, although the Prime Minister is almost always directly elected as an MP within his or her constituency.\n\nCanada's parliamentary system empowers political parties and their party leaders. Where one party gets a majority of the seats in the House of Commons, that party is said to have a \"majority government.\" Through party discipline, the party leader, who is only elected in one riding, exercises a great deal of control over the cabinet and the parliament.\n\nA minority government situation occurs when the party that holds the most seats in the House of Commons holds less seats than the opposition parties combined. In this scenario the party leader whose party has the most seats in the House is selected by the Governor General to lead the government, however, to create stability, the leader chosen must have the support of the majority of the House, meaning they need the support of at least one other party.\n\nIn Canada, the provinces are considered co-sovereign; sovereignty of the provinces is passed on, not by the Governor General or the Canadian parliament, but through the Crown itself. This means that the Crown is \"divided\" into 11 legal jurisdictions; into 11 \"Crowns\" – one federal and ten provincial.\n\nFederal-provincial (or intergovernmental, formerly Dominion-provincial) relations is a regular issue in Canadian politics: Quebec wishes to preserve and strengthen its distinctive nature, western provinces desire more control over their abundant natural resources, especially energy reserves; industrialized Central Canada is concerned with its manufacturing base, and the Atlantic provinces strive to escape from being less affluent than the rest of the country.\n\nIn order to ensure that social programs such as health care and education are funded consistently throughout Canada, the \"have-not\" (poorer) provinces receive a proportionately greater share of federal \"transfer (equalization) payments\" than the richer, or \"have\", provinces do; this has been somewhat controversial. The richer provinces often favour freezing transfer payments, or rebalancing the system in their favour, based on the claim that they already pay more in taxes than they receive in federal government services, and the poorer provinces often favour an increase on the basis that the amount of money they receive is not sufficient for their existing needs.\n\nParticularly in the past decade, some scholars have argued that the federal government's exercise of its unlimited constitutional spending power has contributed to strained federal-provincial relations. This power, which allows the federal government to spend the revenue it raises in any way that it pleases, allows it to overstep the constitutional division of powers by creating programs that encroach on areas of provincial jurisdiction. The federal spending power is not expressly set out in the Constitution Act, 1867; however, in the words of the Court of Appeal for Ontario the power \"can be inferred\" from s. 91(1A), \"the public debt and property\".\n\nA prime example of an exercise of the spending power is the Canada Health Act, which is a conditional grant of money to the provinces. Regulation of health services is, under the Constitution, a provincial responsibility. However, by making the funding available to the provinces under the Canada Health Act contingent upon delivery of services according to federal standards, the federal government has the ability to influence health care delivery. This spending power, coupled with Supreme Court rulings – such as Reference re Canada Assistance Plan (B.C.) – that have held that funding delivered under the spending power can be reduced unilaterally at any time, has contributed to strained federal-provincial relations.\n\nExcept for three short-lived transitional or minority governments, prime ministers from Quebec led Canada continuously from 1968 to early 2006. Québécois led both Liberal and Progressive Conservative governments in this period.\n\nMonarchs, governors general, and prime ministers are now expected to be at least functional, if not fluent, in both English and French. In selecting leaders, political parties give preference to candidates who are fluently bilingual.\n\nAlso, by law, three of the nine positions on the Supreme Court of Canada must be held by judges from Quebec. This representation makes sure that at least three judges have sufficient experience with the civil law system to treat cases involving Quebec laws.\n\nCanada has a long and storied history of secessionist movements (see Secessionist movements of Canada). National unity has been a major issue in Canada since the forced union of Upper and Lower Canada in 1840.\n\nThe predominant and lingering issue concerning Canadian national unity has been the ongoing conflict between the French-speaking majority in Quebec and the English-speaking majority in the rest of Canada. Quebec's continued demands for recognition of its \"distinct society\" through special political status has led to attempts for constitutional reform, most notably with the failed attempts to amend the constitution through the Meech Lake Accord and the Charlottetown Accord (the latter of which was rejected through a national referendum).\n\nSince the Quiet Revolution, sovereigntist sentiments in Quebec have been variably stoked by the patriation of the Canadian constitution in 1982 (without Quebec's consent) and by the failed attempts at constitutional reform. Two provincial referenda, in 1980 and 1995, rejected proposals for sovereignty with majorities of 60% and 50.6% respectively. Given the narrow federalist victory in 1995, a reference was made by the Chrétien government to the Supreme Court of Canada in 1998 regarding the legality of unilateral provincial secession. The court decided that a unilateral declaration of secession would be unconstitutional. This resulted in the passage of the Clarity Act in 2000.\n\nThe Bloc Québécois, a sovereigntist party which runs candidates exclusively in Quebec, was started by a group of MPs who left the Progressive Conservative (PC) party (along with several disaffected Liberal MPs), and first put forward candidates in the 1993 federal election. With the collapse of the PCs in that election, the Bloc and Liberals were seen as the only two viable parties in Quebec. Thus, prior to the 2006 election, any gain by one party came at the expense of the other, regardless of whether national unity was really at issue. The Bloc, then, benefited (with a significant increase in seat total) from the impressions of corruption that surrounded the Liberal Party in the leadup to the 2004 election. However, the newly unified Conservative party re-emerged as a viable party in Quebec by winning 10 seats in the 2006 election. In the 2011 election, the New Democratic Party succeeded in winning 59 of Quebec's 75 seats, successfully reducing the number of seats of every other party substantially. The NDP surge nearly destroyed the Bloc, reducing them to 4 seats, far below the minimum requirement of 12 seats for Official party status.\n\nNewfoundland and Labrador is also a problem regarding national unity. As the Dominion of Newfoundland was a self-governing country equal to Canada until 1949, there are large, though uncoordinated, feelings of Newfoundland nationalism and anti-Canadian sentiment among much of the population. This is due in part to the perception of chronic federal mismanagement of the fisheries, forced resettlement away from isolated settlements in the 1960s, the government of Quebec still drawing inaccurate political maps whereby they take parts of Labrador, and to the perception that mainland Canadians look down upon Newfoundlanders. In 2004, the Newfoundland and Labrador First Party contested provincial elections and in 2008 in federal ridings within the province. In 2004, then-premier Danny Williams ordered all federal flags removed from government buildings as a result of lost offshore revenues to equalization clawbacks. On December 23, 2004, premier Williams made this statement to reporters in St. John's, \n\nWestern alienation is another national-unity-related concept that enters into Canadian politics. Residents of the four western provinces, particularly Alberta, have often been unhappy with a lack of influence and a perceived lack of understanding when residents of Central Canada consider \"national\" issues. While this is seen to play itself out through many avenues (media, commerce, and so on.), in politics, it has given rise to a number of political parties whose base constituency is in western Canada. These include the United Farmers of Alberta, who first won federal seats in 1917, the Progressives (1921), the Social Credit Party (1935), the Co-operative Commonwealth Federation (1935), the Reconstruction Party (1935), New Democracy (1940) and most recently the Reform Party (1989).\n\nThe Reform Party's slogan \"The West Wants In\" was echoed by commentators when, after a successful merger with the PCs, the successor party to both parties, the Conservative Party won the 2006 election. Led by Stephen Harper, who is an MP from Alberta, the electoral victory was said to have made \"The West IS In\" a reality. However, regardless of specific electoral successes or failures, the concept of western alienation continues to be important in Canadian politics, particularly on a provincial level, where opposing the federal government is a common tactic for provincial politicians. For example, in 2001, a group of prominent Albertans produced the Alberta Agenda, urging Alberta to take steps to make full use of its constitutional powers, much as Quebec has done.\n\nCanada is considered by most sources to be a very stable democracy. In 2006 \"The Economist\" ranked Canada the third most democratic nation in its Democracy Index, ahead of all other nations in the Americas and ahead of every nation more populous than itself. In 2008, Canada was ranked World No. 11 and again ahead of all countries more populous and No. 1 for the Americas. (In 2008, the United States was ranked World No. 18, Uruguay World No. 23, and Costa Rica World No. 27.)\n\nThe Liberal Party of Canada, under the leadership of Paul Martin, won a minority victory in the June 2004 general elections. In December 2003, Martin had succeeded fellow Liberal Jean Chrétien, who had, in 2000, become the first Prime Minister to lead three consecutive majority governments since 1945. However, in 2004 the Liberals lost seats in Parliament, going from 172 of 301 Parliamentary seats to 135 of 308, and from 40.9% to 36.7% in the popular vote. The Canadian Alliance, which did well in western Canada in the 2000 election, but was unable to make significant inroads in the East, merged with the Progressive Conservative Party to form the Conservative Party of Canada in late 2003.\n\nThey proved to be moderately successful in the 2004 campaign, gaining seats from a combined Alliance-PC total of 78 in 2000 to 99 in 2004. However, the new Conservatives lost in popular vote, going from 37.7% in 2000 down to 29.6%. In 2006 the Conservatives, led by Stephen Harper, won a minority government with 124 seats. They improved their percentage from 2004, garnering 36.3% of the vote. During this election, the Conservatives also made major breakthroughs in Quebec. They gained 10 seats here, whereas in 2004 they had no seats.\n\nIn the 2011 election, the Conservatives won a majority government with 167 seats. For the first time, the NDP became the Official Opposition, with 102 seats; the Liberals came in third with 34 seats. This was the first election in which the Green Party won a seat, that of leader Elizabeth May; the Bloc won 4 seats, losing Official Party status.\n\nThe Liberal Party, after dominating Canadian politics since the 1920s, was in decline in early years of the 21st century. As Lang (2010) concluded, they lost their majority in Parliament in the 2004 election, were defeated in 2006, and in 2008 became little more than a \"rump\", falling to their lowest seat count in decades and a mere 26% of the popular vote. Furthermore, said Lang (a Liberal himself), its prospects \"are as bleak as they have ever been.\" In the 2011 election, the Liberals suffered a crushing defeat, managing to secure only 18.9% of the vote share and only 34 seats. As a result, the Liberals lost their status as official opposition to the NDP.\n\nIn explaining those trends, Behiels (2010) synthesized major studies and reported that \"a great many journalists, political advisors, and politicians argue that a new political party paradigm is emerging\" She claimed they saw a new power configuration based on a right-wing political party capable of sharply changing the traditional role of the state (federal and provincial) in the twenty-first-century. Behiels said that unlike Brian Mulroney, who tried but failed to challenge the long-term dominance of the Liberals, Harper's attempt had proven to be more determined, systematic and successful.\n\nMany commentators thought it signalled a major realignment. The \"Economist\" said, \"the election represents the biggest realignment of Canadian politics since 1993.\" Lawrence Martin, commentator for the \"Globe and Mail\" said, \"Harper has completed a remarkable reconstruction of a Canadian political landscape that endured for more than a century. The realignment saw both old parties of the moderate middle, the Progressive Conservatives and the Liberals, either eliminated or marginalized.\" \"Maclean's\" said, the election marked \"an unprecedented realignment of Canadian politics\" as \"the Conservatives are now in a position to replace the Liberals as the natural governing party in Canada.\"\n\nDespite the grim outlook and poor early poll numbers, when the 2015 election was held, the Liberals under Justin Trudeau had an unprecedented comeback and the realignment was proved only temporary. Gaining 148 seats, they won a majority government for the first time since 2000. The \"Toronto Star\" claimed the comeback was \"headed straight for the history books\" and that Harpers name would \"forever be joined with that of his Liberal nemesis in Canada’s electoral annals\". Spencer McKay for the \"National Post\" suggested that \"maybe we’ve witnessed a revival of Canada’s 'natural governing party'\".\n\nFunding changes were made to ensure greater reliance on personal contributions. Personal donations to federal parties and campaigns benefit from tax credits, although the amount of tax relief depends on the amount given. Also only people paying income taxes receive any benefit from this.\n\nA good part of the reasoning behind the change in funding was that union or business funding should not be allowed to have as much impact on federal election funding as these are not contributions from citizens and are not evenly spread out between parties. They are still allowed to contribute to the election but only in a minor fashion. The new rules stated that a party had to receive 2% of the vote nationwide in order to receive the general federal funding for parties. Each vote garnered a certain dollar amount for a party (approximately $1.75) in future funding. For the initial disbursement, approximations were made based on previous elections. The NDP received more votes than expected (its national share of the vote went up) while the new Conservative Party of Canada received fewer votes than had been estimated and was asked to refund the difference. Quebec was the first province to implement a similar system of funding many years before the changes to funding of federal parties.\n\nFederal funds are disbursed quarterly to parties, beginning at the start of 2005. For the moment, this disbursement delay leaves the NDP and the Green Party in a better position to fight an election, since they rely more on individual contributors than federal funds. The Green Party now receives federal funds, since it for the first time received a sufficient share of the vote in the 2004 election.\n\nIn 2007, news emerged of a funding loophole that \"could cumulatively exceed the legal limit by more than $60,000,\" through anonymous recurrent donations of $200 to every riding of a party from corporations or unions. At the time, for each individual, the legal annual donation limit was $1,100 for each party, $1,100 combined total for each party's associations, and in an election year, an additional $1,100 combined total for each party's candidates. All three limits increase on 1 April every year based on the inflation rate. \n\n\n\nLeaders debates in Canada consist of two debates, one English and one French, both produced by a consortium of Canada's five major television broadcasters (CBC/SRC, CTV, Global and TVA) and usually consist of the leaders of all parties with representation in the House of Commons.\n\nThese debates air on the networks of the producing consortium as well as the public affairs and parliamentary channel CPAC and the American public affairs network C-SPAN.\n\nThe highest court in Canada is the Supreme Court of Canada and is the final court of appeal in the Canadian justice system. The court is composed of nine judges: eight Puisne Justices and the Chief Justice of Canada. Justices of the Supreme Court of Canada are appointed by the Governor-in-Council. The Supreme Court Act limits eligibility for appointment to persons who have been judges of a superior court, or members of the bar for ten or more years. Members of the bar or superior judge of Quebec, by law, must hold three of the nine positions on the Supreme Court of Canada.\n\nThe Canadian government operates the public service using departments, smaller agencies (for example, commissions, tribunals, and boards), and crown corporations. There are two types of departments: central agencies such as Finance, Privy Council Office, and Treasury Board Secretariat have an organizing and oversight role for the entire public service; line departments are departments which perform tasks in a specific area or field, such as the departments of Agriculture, Environment, or Defence.\n\nScholar Peter Aucoin, writing about the Canadian Westminster system, has raised concerns in the early 2000s about the centralization of power; an increased number, role and influence of partisan-political staff; personal-politicization of appointments to the senior public service; and, the assumption that the public service is promiscuously partisan for the government of the day.\n\nIn 1967 Canada established a point-based system to determine if immigrants should be eligible to enter the country, using meritorious qualities such as the applicant's ability to speak both French and English, their level of education, and other details that may be expected of a natural born Canadian. This system was considered ground-breaking at the time since prior systems were slanted on the basis of ethnicity. However, many foreign nationals still found it challenging to secure work after immigrating, resulting in a higher unemployment rate among immigrants. After winning power in 2006, the conservative party has sought to curb this issue by placing weight on whether or not the applicant has a standing job offer in Canada. The change has been a source of some contention as opponents argue that businesses use this change to suppress wages, with corporate owners leveraging the knowledge that an immigrant should hold a job to successfully complete the immigration process.\n\n"}
{"id": "5195", "url": "https://en.wikipedia.org/wiki?curid=5195", "title": "Economy of Canada", "text": "Economy of Canada\n\nCanada has the 10th (nominal) or 17th-largest (PPP) economy in the world (measured in US dollars at market exchange rates), is one of the world's wealthiest nations, and is a member of the Organization for Economic Co-operation and Development (OECD) and Group of Seven (G7). As with other developed nations, the Canadian economy is dominated by the service industry, which employs about three quarters of Canadians. Canada is unusual among developed countries in the importance of the primary sector, with the logging and oil industries being two of Canada's most important. Canada also has a sizable manufacturing sector, based in Central Canada, with the automobile industry and aircraft industry being especially important. With a long coastline, Canada has the 8th largest commercial fishing and seafood industry in the world. Canada is one of the global leaders of the entertainment software industry.\n\nWith the exception of a few island nations in the Caribbean, Canada is the only major parliamentary democracy in the western hemisphere. As a result, Canada has developed its own social and political institutions, distinct from most other countries in the world. Though the Canadian economy is closely integrated with the American economy, it has developed unique economic institutions.\n\nThe Canadian economic system generally combines elements of private enterprise and public enterprise. Many aspects of public enterprise, most notably the development of an extensive social welfare system to redress social and economic inequities, were adopted after the end of World War Two in 1945.\n\nCanada has a private to public (Crown) property ratio of 60:40 and one of the highest levels of economic freedom in the world. Today Canada closely resembles the U.S. in its market-oriented economic system and pattern of production. According to the Forbes Global 2000 list of the world's largest companies in 2008, Canada has 69 companies in the list, ranking 5th next to France.\n\nInternational trade makes up a large part of the Canadian economy, particularly of its natural resources. In 2009, agriculture, energy, forestry and mining exports accounted for about 58% of Canada's total exports. Machinery, equipment, automotive products and other manufactures accounted for a further 38% of exports in 2009. In 2009, exports accounted for approximately 30% of Canada's GDP. The United States is by far its largest trading partner, accounting for about 73% of exports and 63% of imports as of 2009. Canada's combined exports and imports ranked 8th among all nations in 2006.\n\nApproximately 4% of Canadians are directly employed in primary resource fields, and they account for 6.2% of GDP. They are still paramount in many parts of the country. Many, if not most, towns in northern Canada, where agriculture is difficult, exist because of a nearby mine or source of timber. Canada is a world leader in the production of many natural resources such as gold, nickel, uranium, diamonds, lead, and in recent years, crude petroleum, which, with the world's second-largest oil reserves, is taking an increasingly prominent position in natural resources extraction. Several of Canada's largest companies are based in natural resource industries, such as EnCana, Cameco, Goldcorp, and Barrick Gold. The vast majority of these products are exported, mainly to the United States. There are also many secondary and service industries that are directly linked to primary ones. For instance one of Canada's largest manufacturing industries is the pulp and paper sector, which is directly linked to the logging business.\n\nThe reliance on natural resources has several effects on the Canadian economy and Canadian society. While manufacturing and service industries are easy to standardize, natural resources vary greatly by region. This ensures that differing economic structures developed in each region of Canada, contributing to Canada's strong regionalism. At the same time the vast majority of these resources are exported, integrating Canada closely into the international economy. Howlett and Ramesh argue that the inherent instability of such industries also contributes to greater government intervention in the economy, to reduce the social impact of market changes.\n\nNatural resource industries also raise important questions of sustainability. Despite many decades as a leading producer, there is little risk of depletion. Large discoveries continue to be made, such as the massive nickel find at Voisey's Bay. Moreover, the far north remains largely undeveloped as producers await higher prices or new technologies as many operations in this region are not yet cost effective. In recent decades Canadians have become less willing to accept the environmental destruction associated with exploiting natural resources. High wages and Aboriginal land claims have also curbed expansion. Instead many Canadian companies have focused their exploration, exploitation and expansion activities overseas where prices are lower and governments more amenable. Canadian companies are increasingly playing important roles in Latin America, Southeast Asia, and Africa.\n\nThe depletion of renewable resources has raised concerns in recent years. After decades of escalating overutilization the cod fishery all but collapsed in the 1990s, and the Pacific salmon industry also suffered greatly. The logging industry, after many years of activism, has in recent years moved to a more sustainable model, or to other countries.\n\nProductivity measures are key indicators of economic performance and a key source of economic growth and competitiveness. The Organisation for Economic Co-operation and Development (OECD) The OECD Compendium of Productivity Indicators, published annually, presents a broad overview of productivity levels and growth in member nations, highlighting key measurement issues. It analyses the role of \"productivity as the main driver of economic growth and convergence\" and the \"contributions of labour, capital and MFP in driving economic growth.\" According to the definition above \"MFP is often interpreted as the contribution to economic growth made by factors such as technical and organisational innovation\" (OECD 2008,11). Measures of productivity include Gross Domestic Product (GDP)(OECD 2008,11) and multifactor productivity.\n\nThe OECD provides data for example comparing labour productivity levels in the total economy of each member nation. In their 2011 report Canada's Gross Domestic Product (GDP) was $CDN 1,720,748 million.\n\nIn the International Monetary Fund's (IMF) quarterly World Economic Outlook released in April 2015, the IMF forecast that Canada’s real gross domestic product (GDP) would grow 2.2 percent. In the July World Economic Outlook the IMF forecast that Canada's real GDP would grow by 1.5 per cent in 2015.\n\nAccording to CTV News real estate accounts for half of all GDP growth.\n\nAnother productivity measure, used by the OECD, is the long-term trend in multifactor productivity (MFP) also known as total factor productivity (TFP). This indicator assesses an economy’s \"underlying productive capacity (\"potential output\"), itself an important measure of the growth possibilities of economies and of inflationary pressures.\" MFP measures the residual growth that cannot be explained by the rate of change in the services of labour, capital and intermediate outputs, and is often interpreted as the contribution to economic growth made by factors such as technical and organisational innovation. (OECD 2008,11)\n\nAccording to the OECD's annual economic survey of Canada in June 2012, Canada has experienced weak growth of multi-factor productivity (MFP) and has been declining further since 2002. One of the ways MFP growth is raised is by boosting innovation and Canada's innovation indicators such as business R&D and patenting rates were poor. Raising MFP growth, is \"needed to sustain rising living standards, especially as the population ages.\"\n\nThe Bank of Canada, a federal crown corporation, has the responsibility of Canada's monetary system. During the period that John Crow was Governor of the Bank of Canada—1987 to 1994— there was a worldwide recession and the bank rate rose to around 14% and unemployment topped 11%. In 1991, with Prime Minister Brian Mulroney in office, the federal government and the Bank of Canada announced a new inflation targeting monetary policy that has been the cornerstone of Canada's monetary and fiscal policy ever since. Although since that time inflation-targeting has been adopted by \"most advanced-world central banks\", in 1991 it was innovative and Canada was an early adopter when the then-Finance Minister Michael Wilson approved the Bank of Canada's first inflation-targeting in the 1991 federal budget. The inflation target was set at 2 per cent, which is the midpoint of an inflation range of 1 to 3 per cent. They established a set of inflation-reduction targets in order to keep inflation \"low, stable and predictable\" and to foster \"confidence in the value of money,\" contribute to Canada's sustained growth, employment gains and improved standard of living. Inflation is measured by the total consumer price index (CPI). In 2011 the Government of Canada and the Bank of Canada extended Canada's inflation-control target to December 31, 2016. The Bank of Canada uses three unconventional instruments to achieve the inflation target: \"a conditional statement on the future path of the policy rate,\" quantitative easing, and credit easing.\n\nAs a result, interest rates and inflation eventually came down along with the value of the Canadian dollar. From 1991 to 2011 the inflation-targeting regime kept \"price gains fairly reliable.\"\n\nFollowing the Financial crisis of 2007–08 the narrow focus of inflation-targeting as a means of providing stable growth in the Canadian economy, was questioned. By 2011, the then-Bank of Canada Governor Mark Carney argued that the central bank’s mandate would allow for a more flexible inflation-targeting in specific situations where he would consider taking longer \"than the typical six to eight quarters to return inflation to 2 per cent.\"\n\nThe central bank— the Bank of Canada— issues its rate announcement through its Monetary Policy Report which is released eight times a year. On July 15, 2015 the Bank of Canada announced that it was lowering its target for the overnight rate by another one-quarter percentage point, to 0.5 per cent \"to try to stimulate an economy that appears to have failed to rebound meaningfully from the oil shock woes that dragged it into decline in the first quarter.\" According to the Bank of Canada announcement, in the first quarter of 2015, the total Consumer price index (CPI) inflation was about 1 per cent. This reflects \"year-over-year price declines for consumer energy products.\" Core inflation in the first quarter of 2015 was about 2 per cent with an underlying trend in inflation at about 1.5 to 1.7 per cent.\n\nIn response to the Bank of Canada's July 15, 2015 rate adjustment, Prime Minister Stephen Harper explained that the Canadian economy was being dragged down by forces beyond Canadian borders such as global oil prices, the European debt crisis, and China's economic slowdown\" which has made the global economy \"fragile.\"\n\nThe Chinese stock market had lost about US$3 trillion of wealth by July 2015 when panicked investors sold stocks, which created declines in the commodities markets, which in turn negatively impacted resource-producing countries like Canada.\n\nThe Bank's main priority has been to keep inflation at a moderate level. As part of that strategy, interest rates were kept at a low level for almost seven years. Since September 2010, the key interest rate (overnight rate) was 0.5%. In mid 2017, inflation remained below the Bank's 2% target, (at 1.6%) mostly because of reductions in the cost of energy, food and automobiles; as well, the economy was in a continuing spurt with a predicted GDP growth of 2.8 percent by year end. Early on 12 July 2017, the bank issued a statement that the benchmark rate would be increased to 0.75%. \"The economy can handle very well this move we have today and of course you need to preface that with an acknowledgment that of course interest rates are still very low,\" Governor Stephen Poloz subsequently said. In its press release, the bank had confirmed that the rate would continue to be evaluated at least partly on the basis of inflation. \"Future adjustments to the target for the overnight rate will be guided by incoming data as they inform the bank's inflation outlook, keeping in mind continued uncertainty and financial system vulnerabilities.\" Poloz refused to speculate on the future of the economy but said, \"I don't doubt that interest rates will move higher, but there's no predetermined path in mind at this stage\".\n\nIn 2012, the Canadian economy had the following relative weighting by industry, as percentage value of GDP:\n\n\nThe service sector in Canada is vast and multifaceted, employing about three quarters of Canadians and accounting for 70% of GDP. The largest employer is the retail sector, employing almost 12% of Canadians. The retail industry is mainly concentrated in a small number of chain stores clustered together in shopping malls. In recent years, there has been an increase in the number of big-box stores, such as Wal-Mart (of the United States), Real Canadian Superstore, and Best Buy (of the United States). This has led to fewer workers in this sector and a migration of retail jobs to the suburbs.\n\nThe second largest portion of the service sector is the business service and hire only a slightly smaller percentage of the population. This includes the financial services, real estate, and communications industries. This portion of the economy has been rapidly growing in recent years. It is largely concentrated in the major urban centres, especially Toronto, Montreal and Vancouver (see Banking in Canada).\n\nThe education and health sectors are two of Canada's largest, but both are largely under the influence of the government. The health care industry has been quickly growing, and is the third largest in Canada. Its rapid growth has led to problems for governments who must find money to fund it.\n\nCanada has an important high tech industry, and a burgeoning film, television, and entertainment industry creating content for local and international consumption (see Media in Canada). Tourism is of ever increasing importance, with the vast majority of international visitors coming from the United States. Though the recent strength of the Canadian Dollar has hurt this sector, other nations such as China have increased tourism to Canada. Casino gaming is currently the fastest-growing component of the Canadian tourism industry, contributing $5 billion in profits for Canadian governments and employing 41,000 Canadians as of 2001.\n\nThe general pattern of development for wealthy nations was a transition from a primary industry based economy to a manufacturing based one, and then to a service based economy. Canada did not escape this pattern - at its (abnormally high World War II) peak in 1944, manufacturing accounted for 29% of GDP, declining to 15.6% in 2005. Canada has not suffered as greatly as most other rich, industrialized nations from the pains of the relative decline in the importance of manufacturing since the 1960s. A 2009 study by Statistics Canada also found that, while manufacturing declined as a relative percentage of GDP from 24.3% in the 1960s to 15.6% in 2005, manufacturing volumes between 1961 and 2005 kept pace with the overall growth in the volume index of GDP. Manufacturing in Canada was especially hit hard by the financial crisis of 2007–08. As of 2010, manufacturing accounts for 13% of Canada's GDP, a relative decline of more than 2% of GDP since 2005.\n\nCentral Canada is home to branch plants to all the major American and Japanese automobile makers and many parts factories owned by Canadian firms such as Magna International and Linamar Corporation. Central Canada today produces more vehicles each year than the neighbouring U.S. state of Michigan, the heart of the American automobile industry. Manufacturers have been attracted to Canada due to the highly educated population with lower labour costs than the United States. Canada's publicly funded health care system is also an important attraction, as companies are exempt from the high health insurance costs US firms pay, though they are offset by corporate health care taxes.\n\nMuch of the Canadian manufacturing industry consists of branch plants of United States firms, though there are some important domestic manufacturers, such as Bombardier Inc.. This has raised several concerns for Canadians. Branch plants provide mainly blue collar jobs, with research and executive positions confined to the United States.\n\nCanada is one of the few developed nations that is a net exporter of energy - in 2009 net exports of energy products amounted to 2.9% of GDP. Most important are the large oil and gas resources centred in Alberta and the Northern Territories, but also present in neighbouring British Columbia and Saskatchewan. The vast Athabasca oil sands give Canada the world's third largest reserves of oil after Saudi Arabia and Venezuela according to USGS. In British Columbia and Quebec, as well as Ontario, Saskatchewan, Manitoba and the Labrador region, hydroelectric power is an inexpensive and relatively environmentally friendly source of abundant energy. In part because of this, Canada is also one of the world's highest per capita consumers of energy. Cheap energy has enabled the creation of several important industries, such as the large aluminium industries in British Columbia and Quebec.\n\nHistorically, an important issue in Canadian politics is the interplay between the oil and energy industry in Western Canada and the industrial heartland of Southern Ontario. Foreign investment in Western oil projects has fueled Canada's rising dollar. This has raised the price of Ontario's manufacturing exports and made them less competitive, a problem similar to the decline of the manufacturing sector in the Netherlands. Also, Ontario has relatively fewer native sources of power. However, it is cheaper for Alberta to ship its oil to the western United States than to eastern Canada. The eastern Canadian ports thus import significant quantities of oil from overseas, and Ontario makes significant use of nuclear power.\n\nThe National Energy Policy of the early 1980s attempted to force Alberta to sell low priced oil to eastern Canada. This policy proved deeply divisive, and quickly lost its importance as oil prices collapsed in the mid-1980s. One of the most controversial sections of the Canada-United States Free Trade Agreement of 1988 was a promise that Canada would never charge the United States more for energy than fellow Canadians. In 2007 Canada's three biggest oil companies brought in record profits of $11.75 billion, up 10 percent from $10.72 billion in 2006.\n\nCanada is also one of the world's largest suppliers of agricultural products, particularly of wheat and other grains. Canada is a major exporter of agricultural products, to the United States and Asia. As with all other developed nations the proportion of the population and GDP devoted to agriculture fell dramatically over the 20th century.\n\nAs with other developed nations, the Canadian agriculture industry receives significant government subsidies and supports. However, Canada has been a strong supporter of reducing market influencing subsidies through the World Trade Organization. In 2000, Canada spent approximately CDN$4.6 billion on supports for the industry. Of this, $2.32 billion was classified under the WTO designation of \"green box\" support, meaning it did not directly influence the market, such as money for research or disaster relief. All but $848.2 million were subsidies worth less than 5% of the value of the crops they were provided for.\n\n\n\nCanada is negotiating bilateral FTAs with the following countries and trade blocs:\n\nCanada has been involved in negotiations to create the following regional trade blocks:\n\n\nCanada and the United States share a common trading relationship. Canada's job market continues to perform well along with the US, reaching a 30-year low in the unemployment rate in December 2006, following 14 consecutive years of employment growth.\n\nThe United States is by far Canada's largest trading partner, with more than $1.7 billion CAD in trade per day in 2005. In 2009, 73% of Canada's exports went to the United States, and 63% of Canada's imports were from the United States. Trade with Canada makes up 23% of the United States' exports and 17% of its imports. By comparison, in 2005 this was more than U.S. trade with all countries in the European Union combined, and well over twice U.S. trade with all the countries of Latin America combined. Just the two-way trade that crosses the Ambassador Bridge between Michigan and Ontario equals all U.S. exports to Japan. Canada's importance to the United States is not just a border-state phenomenon: Canada is the leading export market for 35 of 50 U.S. states, and is the United States' largest foreign supplier of energy.\n\nBilateral trade increased by 52% between 1989, when the U.S.-Canada Free Trade Agreement (FTA) went into effect, and 1994, when the North American Free Trade Agreement (NAFTA) superseded it. Trade has since increased by 40%. NAFTA continues the FTA's moves toward reducing trade barriers and establishing agreed-upon trade rules. It also resolves some long-standing bilateral irritants and liberalizes rules in several areas, including agriculture, services, energy, financial services, investment, and government procurement. NAFTA forms the largest trading area in the world, embracing the 405 million people of the three North American countries.\n\nThe largest component of U.S.-Canada trade is in the commodity sector.\n\nThe U.S. is Canada's largest agricultural export market, taking well over half of all Canadian food exports. Similarly, Canada is the largest market for U.S. agricultural goods, with nearly 20% of American food exports going to its northern neighbour. Nearly two-thirds of Canada's forest products, including pulp and paper, are exported to the United States; 72% of Canada's total newsprint production also is exported to the U.S.\n\nAt $73.6 billion in 2004, U.S.-Canada trade in energy is the largest U.S. energy trading relationship, with the overwhelming majority ($66.7 billion) being exports from Canada. The primary components of U.S. energy trade with Canada are petroleum, natural gas, and electricity. Canada is the United States' largest oil supplier and the fifth-largest energy producing country in the world. Canada provides about 16% of U.S. oil imports and 14% of total U.S. consumption of natural gas. The United States and Canada's national electricity grids are linked, and both countries share hydropower facilities on the western borders.\n\nWhile most of U.S.-Canada trade flows smoothly, there are occasionally bilateral trade disputes, particularly in the agricultural and cultural fields. Usually these issues are resolved through bilateral consultative forums or referral to World Trade Organization (WTO) or NAFTA dispute resolution. In May 1999, the U.S. and Canadian governments negotiated an agreement on magazines that provides increased access for the U.S. publishing industry to the Canadian market. The United States and Canada also have resolved several major issues involving fisheries. By common agreement, the two countries submitted a Gulf of Maine boundary dispute to the International Court of Justice in 1981; both accepted the court's 12 October 1984 ruling which demarcated the territorial sea boundary. A current issue between the United States and Canada is the ongoing softwood lumber dispute, as the U.S. alleges that Canada unfairly subsidizes its forestry industry.\n\nIn 1990, the United States and Canada signed a bilateral Fisheries Enforcement Agreement, which has served to deter illegal fishing activity and reduce the risk of injury during fisheries enforcement incidents. The U.S. and Canada signed a Pacific Salmon Agreement in June 1999 that settled differences over implementation of the 1985 Pacific Salmon Treaty for the next decade.\n\nCanada and the United States signed an aviation agreement during Bill Clinton's visit to Canada in February 1995, and air traffic between the two countries has increased dramatically as a result. The two countries also share in operation of the St. Lawrence Seaway, connecting the Great Lakes to the Atlantic Ocean.\n\nThe U.S. is Canada's largest foreign investor and the most popular destination for Canadian foreign investments; at the end of 2007, the stock of U.S. direct investment in Canada was estimated at $293 billion, while Canadian direct investment (stock) in the United States was valued at $213 billion. U.S. FDI accounts for 59.5% of total foreign direct investment in Canada while Canadian FDI in the U.S. accounts for 10% (5th largest foreign investor). US investments are primarily directed at Canada's mining and smelting industries, petroleum, chemicals, the manufacture of machinery and transportation equipment, and finance, while Canadian investment in the United States is concentrated in manufacturing, wholesale trade, real estate, petroleum, finance, and insurance and other services.\n\nThe OECD reports the Central Government Debt as percentage of the GDP. In 2000 Canada's was 40.9 percent, in 2007 it was 25.2 percent, in 2008 it was 28.6 percent and by 2010 it was 36.1 percent. The OECD reports \"net\" financial liabilities measure used by the OECD, reports the net number at 25.2%, as of 2008, making Canada’s total government debt burden as the lowest in the G8. The gross number was 68% in 2011.\n\nThe CIA World Factbook, updated weekly, measures financial liabilities by using gross general government debt, as opposed to net federal debt used by the OECD and the Canadian federal government. Gross general government debt includes both \"intragovernmental debt and the debt of public entities at the sub-national level.\" For example, the CIA measured Canada's public debt as 84.1% of GDP in 2012 and 87.4% of GDP in 2011 making it 22nd in the world.\n\nIn March 2015 the International Monetary Fund reported that Canada's high household debt was one of two vulnerable domestic areas in Canada's economy; the second is its overheated housing market.\n\nAccording to a July 2015 report by Laura Cooper, an economist with the RBC—the largest financial institution in Canada— \"outstanding household credit balances\" had reached $1.83 trillion. Canadian household credit growth had reached a peak in 2009 then plummeted to a cycle-low in late 2013. There was a quickened pace of growth in household debt in December 2012 and another in April and May 2015.\n\nAccording to the August 2013 third annual Ipsos Reid Debt Poll only 24 per cent of Canadians were debt free in 2013 compared to 26 per cent in 2012. The average personal non-mortgage debt in 2013 was $15,920 up from $13,141 in 2012. According to an IPSOS chart produced in 2013 debt levels increased \"a staggering 35 per cent\" in Western Canada compared to 10 per cent in Eastern Canada since 2012 even before the Alberta floods. In Alberta in 2013 household debt rose 63 per cent to $24,271 per household from 2012 after the 2013 Alberta floods. In 2013 the average personal debt load in British Columbia was \"up 38 per cent to $15,549;\" in \"Manitoba and Saskatchewan, up 32 per cent to $16,145;\" in Ontario, \"up 13 per cent to $17,416,\" in Quebec up \"3 per cent to $10,458;\" and in Atlantic Canada, \"up 12 per cent to $15,243.\"\n\nStatistics Canada announced in December 2014 that Canada's household debt-to-income ratio \"hit a record high in the third quarter of 2014, climbing to 162.6 percent from 161.5 percent in the second quarter.\" However \"household assets and net worth increased much faster than debt,\" with the national net worth at C$8.12 trillion in the third quarter of 2014, a increase of 2.8 percent from the second quarter. Also through the inflation-targeting policy of the Bank of Canada, interest rates are kept low improving the ability of households to service their debt. \"The debt-service ratio, or interest paid as a proportion of disposable income, fell to a record low 6.8 percent in the third quarter.\"\n\nBy 2015 according to the Globe and Mail, \"The total debt owed by all Canadians at the end of March 2015 was a record $1.8-trillion with mortgage debt making up $1.29-trillion.\"\n\nAccording to Philip Cross of the Fraser Institute, in May 2015, while the Canadian household debt-to-income ratio is similar to that in the US, however lending standards in Canada are tighter than those in the United States to protect against high-risk borrowers taking out unsustainable debt.\n\nHousehold debt, the amount of money that all adults in the household owe financial institutions, includes consumer debt and mortgage loans. Paul Krugman argued that by 2007 household debt in the United States, prior to the financial crisis, had reached 130 percent of household income. Krugman distinguished between the total domestic non-financial debt (public plus private) relative to GDP which is \"money we owe to ourselves\" and net foreign debt. Statistics Canada reported in March 2013 that \"credit-market debt such as mortgages rose to 165% of disposable income, compared with 164.7% in the prior three-month period\" in 2013 According to the IMF in 2012, \"Housing-related debt (mortgages) comprises about 70 percent of gross household debt in advanced economies. The remainder consists mainly of credit card debt and auto loans.\"\n\nAs shown in the table below—based on the RBC Economic and Financial Market Outlook March 11, 2016 report—in Canada in 2015—while business investments decreased—consumption, housing and government spending along with net exports contributed to a real GDP increase at a subpar 1.2% pace. In December 2015 export volumes reached $1.2 billion—the sixth time since 2010 with sales growing by such a large amount-evidence that the Canadian economy is transitioning. In November and December 2015, with the weakening in the Canadian dollar, manufacturing sales and exports increased and employment rose. Job losses in construction, mining, oil and gas were countered by gains in the service sector.\n\nBetween early December, 2015 and mid-January the price of oil unexpectedly dropped 24%. RBC economists argued that fear not fundamentals led the shift in financial conditions. Risk adverse investors contributed to a global double-digit decline in the first six weeks of 2016. In Canada, the US, UK and Euro-area yields on long-term government bonds reached an all-time low. As financial market volatility continued in March 2016 the Bank of Canada and Bank of England held their policy rate at 0.5%.\n\n\n\n"}
{"id": "5196", "url": "https://en.wikipedia.org/wiki?curid=5196", "title": "Telecommunications in Canada", "text": "Telecommunications in Canada\n\nPresent-day Telecommunications in Canada include telephone, radio, television, and internet usage. In the past, telecommunications included telegraphy available through Canadian Pacific and Canadian National.\nRadio broadcast stations: AM 245, FM 582, shortwave 6 (2004)\n\nITU prefixes: Letter combinations available for use in Canada as the first two letters of a television or radio station's call sign are CF, CG, CH, CI, CJ, CK, CY, CZ, VA, VB, VC, VD, VE, VF, VG, VO, VX, VY, XJ, XK, XL, XM, XN and XO. Only CF, CH, CI, CJ and CK are currently in common use, although four radio stations in St. John's, Newfoundland and Labrador retained call letters beginning with VO when Newfoundland joined Canadian Confederation in 1949. Stations owned by the Canadian Broadcasting Corporation use CB through a special agreement with the government of Chile. Some codes beginning with VE and VF are also in use to identify radio repeater transmitters.\n\nTelevision broadcast stations: 1456 (128 originating stations, 1328 re-transmitters) (2003)\n\nTelephones – main lines in use: 18.251 million (2009)\n\nTelephones – mobile cellular: 23.081 million (2009)\n\nTelephone system:\n\n\nThe history of telegraphy in Canada dates back to the Province of Canada. While the first telegraph company was the Toronto, Hamilton and Niagara Electro-Magnetic Telegraph Company, founded in 1846, it was the Montreal Telegraph Company, controlled by Hugh Allan and founded a year later, that dominated in Canada during the technology's early years.\n\nFollowing the 1852 Telegraph Act, Canada's first permanent transatlantic telegraph link was a submarine cable built in 1866 between Ireland and Newfoundland.Telegrams were sent through networks built by Canadian Pacific and Canadian National.\n\nIn 1868 Montreal Telegraph began facing competition from the newly established Dominion Telegraph Company. 1880 saw the Great North Western Telegraph Company established to connect Ontario and Manitoba but within a year it was taken over by Western Union, leading briefly to that company's control of almost all telegraphy in Canada. In 1882, Canadian Pacific transmitted its first commercial telegram over telegraph lines they had erected alongside its tracks, breaking Western Union's monopoly. Great North Western Telegraph, facing bankruptcy, was taken over in 1915 by Canadian Northern.\n\nBy the end of World War II, Canadians communicated by telephone, more than any other country. In 1967 the CP and CN networks were merged to form CNCP Telecommunications.\n\nAs of 1951, approximately 7000 messages were sent daily from the United States to Canada. An agreement with Western Union required that U.S. company to route messages in a specified ratio of 3:1, with three telegraphic messages transmitted to Canadian National for every message transmitted to Canadian Pacific. The agreement was complicated by the fact that some Canadian destinations were served by only one of the two networks.\n\nTelecommunications is overseen by the Canadian Radio-television and Telecommunications Commission under the Telecommunications Act and Radiocommunication Act\n\n\n"}
{"id": "5197", "url": "https://en.wikipedia.org/wiki?curid=5197", "title": "Transportation in Canada", "text": "Transportation in Canada\n\nTransportation in Canada, the world's second-largest country in total area, is dedicated to having an efficient, high-capacity multimodal transport spanning often vast distances between natural resource extraction sites, agricultural and urban areas. Canada's transportation system includes more than of roads, 10 major international airports, 300 smaller airports, of functioning railway track, and more than 300 commercial ports and harbours that provide access to the Pacific, Atlantic and Arctic oceans as well as the Great Lakes and the St. Lawrence Seaway. In 2005, the transportation sector made up 4.2% of Canada's GDP, compared to 3.7% for Canada's mining and oil and gas extraction industries.\n\nTransport Canada oversees and regulates most aspects of transportation within federal jurisdiction, including interprovincial transport. This primarily includes rail, air and maritime transportation. Transport Canada is under the direction of the federal government's Minister of Transport. The Transportation Safety Board of Canada is responsible for maintaining transportation safety in Canada by investigating accidents and making safety recommendations.\n\nThere is a total of of roads in Canada, of which are paved, including of expressways (the third-longest in the world, behind the Interstate Highway System of the United States and the China's National Trunk Highway System). As of 2008, were unpaved.\n\nIn 2009, there were 20,706,616 road vehicles registered in Canada, of which 96% were vehicles under , 2.4% were vehicles between tonnes and 1.6% were or greater. These vehicles travelled a total of 333.29 billion kilometres, of which 303.6 billion was for vehicles under , 8.3 billion was for vehicles between and 21.4 billion was for vehicles over . For the trucks, 88.9% of vehicle-kilometres were intra-province trips, 4.9% were inter-province, 2.8% were between Canada and the US and 3.4% made outside of Canada. For trucks over , 59.1% of vehicle-kilometres were intra-province trips, 20% inter-province trips, 13.8% Canada-US trips and 7.1% trips made outside of Canada.\n\nCanada's vehicles consumed a total of of gasoline and of diesel. Trucking generated 35% of the total GDP from transport, compared to 25% for rail, water and air combined (the remainder being generated by the industry's transit, pipeline, scenic and support activities). Hence roads are the dominant means of passenger and freight transport in Canada.\n\nRoads and highways were managed by provincial and municipal authorities until construction of the Northwest Highway System (the Alaska Highway) and the Trans-Canada Highway project initiation. The Alaska Highway of 1942 was constructed during World War II for military purposes connecting Fort St. John, British Columbia with Fairbanks, Alaska. The transcontinental highway, a joint national and provincial expenditure, was begun in 1949 under the initiation of the Trans Canada Highway Act on December 10, 1949. The highway was completed in 1962 at a total expenditure of $1.4 billion.\n\nInternationally, Canada has road links with both the lower 48 US states and Alaska. The Ministry of Transportation maintains the road network in Ontario and also employs Ministry of Transport Enforcement Officers for the purpose of administering the Canada Transportation Act and related regulations. The Department of Transportation in New Brunswick performs a similar task in that province as well.\n\nRegulations enacted in regards to Canada highways are the 1971 Motor Vehicle Safety Act and the 1990 Highway Traffic Act\n\nThe safety of Canada's roads is moderately good by international standards, and is improving both in terms of accidents per head of population and per billion vehicle kilometers.\n\nAir transportation made up 9% of the transport sector's GDP generation in 2005. Canada's largest air carrier and its flag carrier is Air Canada, which had 34 million customers in 2006 and, as of April 2010, operates 363 aircraft (including Air Canada Jazz). CHC Helicopter, the largest commercial helicopter operator in the world, is second with 142 aircraft and WestJet, a low-cost carrier formed in 1996, is third with 100 aircraft. Canada's airline industry saw significant change following the signing of the US-Canada open skies agreement in 1995, when the marketplace became less regulated and more competitive.\n\nThe Canadian Transportation Agency employs transportation enforcement officers to maintain aircraft safety standards, and conduct periodic aircraft inspections, of all air carriers. The Canadian Air Transport Security Authority is charged with the responsibility for the security of air traffic within Canada. In 1994 the National Airports Policy was enacted\n\nOf over 1,800 registered Canadian aerodromes, certified airports, heliports, and floatplane bases, 26 are specially designated under Canada's National Airports System (NAS): these include all airports that handle 200,000 or more passengers each year, as well as the principal airport serving each federal, provincial, and territorial capital. However, since the introduction of the policy only one, Iqaluit Airport, has been added and no airports have been removed despite dropping below 200,000 passengers. The Government of Canada, with the exception of the three territorial capitals, retains ownership of these airports and leases them to local authorities. The next tier consists of 64 regional/local airports formerly owned by the federal government, most of which have now been transferred to other owners (most often to municipalities).\n\nBelow is a table of Canada's ten biggest airports by passenger traffic in 2011.\n\nIn 2007, Canada had a total of of freight and passenger railway, of which is electrified. While intercity passenger transportation by rail is now very limited, freight transport by rail remains common. Total revenues of rail services in 2006 was $10.4 billion, of which only 2.8% was from passenger services. The Canadian National and Canadian Pacific Railway are Canada's two major freight railway companies, each having operations throughout North America. In 2007, 357 billion tonne-kilometres of freight were transported by rail, and 4.33 million passengers travelled 1.44 billion passenger-kilometres (an almost negligible amount compared to the 491 billion passenger-kilometres made in light road vehicles). 34,281 people were employed by the rail industry in the same year.\n\nNationwide passenger services are provided by the federal crown corporation Via Rail. Three Canadian cities have commuter rail services: in the Montreal area by AMT, in the Toronto area by GO Transit, and in the Vancouver area by West Coast Express. Smaller railways such as Ontario Northland, Rocky Mountaineer, and Algoma Central also run passenger trains to remote rural areas.\n\nIn Canada railways are served by standard gauge, , rails. See also track gauge in Canada.\n\nCanada has railway links with the lower 48 US States, but no connection with Alaska other than a train ferry service from Prince Rupert, British Columbia, although a line has been proposed. There are no other international rail connections.\n\nIn 2005, 139.2 million tonnes of cargo was loaded and unloaded at Canadian ports. The Port of Vancouver is the busiest port in Canada, moving 68 million tonnes or 15% of Canada's total in domestic and international shipping in 2003.\n\nTransport Canada oversees most of the regulatory functions related to marine registration, safety of large vessel, and port pilotage duties. Many of Canada's port facilities are in the process of being divested from federal responsibility to other agencies or municipalities.\n\nInland waterways comprise , including the St. Lawrence Seaway. Transport Canada enforces acts and regulations governing water transportation and safety.\n\nThe St. Lawrence waterway was at one time the world's greatest inland water navigation system. The main route canals of Canada are those of the St. Lawrence River and the Great Lakes. The others are subsidiary canals.\n\nThe National Harbours Board administered Halifax, Saint John, Chicoutimi, Trois-Rivières, Churchill, and Vancouver until 1983. At one time, over 300 harbours across Canada were supervised by the Department of Transport. A program of divestiture was implemented around the turn of the millennium, and as of 2014, 493 of the 549 sites identified for divestiture in 1995 have been sold or otherwise transferred, as indicated by a DoT list. The government maintains an active divestiture programme, and after divestiture Transport Canada oversees only 17 Canada Port Authorities for the 17 largest shipping ports.\n\n\nCanada's merchant marine comprised a \"total\" of 173 ships ( or over) or at the end of 2007.\n\nPipelines are part of the energy extraction and transportation network of Canada and are used to transport natural gas, natural gas liquids, crude oil, synthetic crude and other petroleum based products. Canada has of pipeline for transportation of crude and refined oil, and for liquefied petroleum gas.\n\nMost Canadian cities have public transport, if only a bus system. Three Canadian cities have rapid transit systems, four have light rail systems, and three have commuter rail systems (see below). In 2006, 11% of Canadians used public transportation to get to work. This compares to 80.0% that got to work using a car (72.3% by driving, 7.7% as a passenger), 6.4% that walked and 1.3% that rode a bike.\n\nThere are three rapid transit systems operating in Canada: the Montreal Metro, the Toronto Subway, and the Vancouver SkyTrain.\n\nThere is also an Airport Circulator, the LINK Train at Toronto Pearson International Airport. It operates 24 hours a day, 7 days a week and is wheelchair-accessible. It is free of cost.\n\nThree cities have light rail systems—the Calgary CTrain, the Edmonton Light Rail Transit, and the O-Train in Ottawa—and Toronto has an extensive streetcar system.\nCommuter trains serve the cities and surrounding areas of Montreal, Toronto and Vancouver:\nThe standard history covers the French regime, fur traders, the canals, and early roads, and gives extensive attention to the railways.\n\nAboriginal peoples in Canada walked. They also used canoes, kayaks, umiaks and Bull Boats, in addition to the snowshoe, toboggan and sled in winter. They had no wheeled vehicles, and no animals larger than dogs.\n\nEuropeans adopted canoes as they pushed deeper into the continent's interior, and were thus able to travel via the waterways that fed from the St. Lawrence River and Hudson Bay.\n\nIn the 19th century and early 20th century transportation relied on harnessing oxen to \"Red River ox carts\" or horse to waggon. Maritime transportation was via manual labour such as canoe or wind on sail. Water or land travel speeds was approximately .\n\nSettlement was along river routes. Agricultural commodities were perishable, and trade centres were within . Rural areas centred around villages, and they were approximately apart. The advent of steam railways and steamships connected resources and markets of vast distances in the late 19th century. Railways also connected city centres, in such a way that the traveller went by sleeper, railway hotel, to the cities. Crossing the country by train took four or five days, as it still does by car. People generally lived within of the downtown core thus the train could be used for inter-city travel and the tram for commuting.\n\nThe advent of the interstate or Trans-Canada Highway in Canada in 1963 established ribbon development, truck stops, and industrial corridors along throughways.\n\nThe Federal Department of Transport (established 2 November 1936) supervised railways, canals, harbours, marine and shipping, civil aviation, radio and meteorology. The Transportation Act of 1938 and the amended Railway Act, placed control and regulation of carriers in the hands of the Board of Transport commissioners for Canada. The Royal Commission on Transportation was formed 29 December 1948, to examine transportation services to all areas of Canada to eliminate economic or geographic disadvantages. The Commission also reviewed the Railway Act to provide uniform yet competitive freight-rates.\n\n\n\n"}
{"id": "5199", "url": "https://en.wikipedia.org/wiki?curid=5199", "title": "Canada–United States relations", "text": "Canada–United States relations\n\nRelations between Canada and the United States of America historically have been extensive, given a shared border and ever-increasing close cultural, economical ties and similarities. The shared historical and cultural heritage has resulted in one of the most stable and mutually beneficial international relationships in the world. For both countries, the level of trade with the other is at the top of the annual combined import-export total. Tourism and migration between the two nations have increased rapport, but border security was heightened after the terrorist attacks in the United States on September 11, 2001. The U.S. is ten times larger in population and has the dominant cultural and economic influence. Starting with the American Revolution, when anti-American Loyalists fled to Canada, a vocal element in Canada has warned against US dominance or annexation. The War of 1812 saw invasions across the border. In 1815, the war ended with the border unchanged and demilitarized, as were the Great Lakes. The British ceased aiding First Nation attacks on American territory, and the United States never again attempted to invade Canada. Apart from minor raids, it has remained peaceful.\n\nAs Britain decided to disengage, fears of an American takeover played a role in the formation of the Dominion of Canada (1867), and Canada's rejection of free trade (1911). Military collaboration was close during World War II and continued throughout the Cold War, bilaterally through NORAD and multilaterally through NATO. A very high volume of trade and migration continues between the two nations, as well as a heavy overlapping of popular and elite culture, a dynamic which has generated closer ties, especially after the signing of the Canada–United States Free Trade Agreement in 1988.\n\nCanada and the United States are the world's largest trading partners. The two nations have the world's longest shared border (), and also have significant interoperability within the defence sphere. Recent difficulties have included repeated trade disputes, environmental concerns, Canadian concern for the future of oil exports, and issues of illegal immigration and the threat of terrorism. Trade has continued to expand, especially following the 1988 FTA and North American Free Trade Agreement (NAFTA) in 1994 which has further merged the two economies.\nCo-operation on many fronts, such as the ease of the flow of goods, services, and people across borders are to be even more extended, as well as the establishment of joint border inspection agencies, relocation of U.S. food inspectors agents to Canadian plants and vice versa, greater sharing of intelligence, and harmonizing regulations on everything from food to manufactured goods, thus further increasing the American-Canadian assemblage.\n\nThe foreign policies of the neighbours have been closely aligned since the Cold War. Canada has disagreed with American policies regarding the Vietnam War, the status of Cuba, the Iraq War, Missile Defense, and the War on Terrorism. A diplomatic debate has been underway in recent years on whether the Northwest Passage is in international waters or under Canadian sovereignty.\n\nToday there are close cultural ties, many similar and identical traits and according to Gallup's annual public opinion polls, Canada has consistently been Americans' favorite nation, with 96% of Americans viewing Canada favorably in 2012. According to a 2013 BBC World Service Poll, 84% of Americans view their northern neighbor's influence positively, with only 5% expressing a negative view, the most favorable perception of Canada in the world. As of spring 2013, 64% of Canadians had a favorable view of the U.S. and 81% expressed confidence in then-US President Obama to do the right thing in international matters. According to the same poll, 30% viewed the U.S. negatively. Also, according to a 2014 BBC World Service Poll, 86% of Americans view Canada's influence positively, with only 5% expressing a negative view. However, according to the same poll, 43% of Canadians view U.S. influence positively, with 52% expressing a negative view. In addition, according to Spring 2017 Global Attitudes Survey, 43% of Canadians view U.S. positively, while 51% hold a negative view.\n\nLeaders of Canada and the United States from 1950\n\nBefore the British conquest of French Canada in 1760, there had been a series of wars between the British and the French which were fought out in the colonies as well as in Europe and the high seas. In general, the British heavily relied on American colonial militia units, while the French heavily relied on their First Nation allies. The Iroquois Nation were important allies of the British. Much of the fighting involved ambushes and small-scale warfare in the villages along the border between New England and Quebec. The New England colonies had a much larger population than Quebec, so major invasions came from south to north. The First Nation allies, only loosely controlled by the French, repeatedly raided New England villages to kidnap women and children, and torture and kill the men. Those who survived were brought up as Francophone Catholics. The tension along the border was exacerbated by religion, the French Catholics and English Protestants had a deep mutual distrust. There was a naval dimension as well, involving privateers attacking enemy merchant ships.\n\nEngland seized Quebec from 1629 to 1632, and Acadia in 1613 and again from 1654 to 1670; These territories were returned to France by the peace treaties. The major wars were (to use American names), King William's War (1689–1697); Queen Anne's War (1702–1713); King George's War (1744–1748), and the French and Indian War (1755–1763). In Canada, as in Europe, this era is known as the Seven Years' War.\n\nNew England soldiers and sailors were critical to the successful British campaign to capture the French fortress of Louisbourg in 1745, and (after it had been returned by treaty) to capture it again in 1758.\n\nFrom the 1750s to the 21st century, there has been extensive mingling of the Canadian and American populations, with large movements in both directions.\n\nNew England Yankees settled large parts of Nova Scotia before 1775, and were neutral during the American Revolution. At the end of the Revolution, about 75,000 Loyalists moved out of the new United States to Nova Scotia, New Brunswick, and the lands of Quebec, east and south of Montreal. From 1790 to 1812 many farmers moved from New York and New England into Ontario (mostly to Niagara, and the north shore of Lake Ontario). In the mid and late 19th century gold rushes attracted American prospectors, mostly to British Columbia after the Cariboo Gold Rush, Fraser Canyon Gold Rush, and later to the Yukon. In the early 20th century, the opening of land blocks in the Prairie Provinces attracted many farmers from the American Midwest. Many Mennonites immigrated from Pennsylvania and formed their own colonies. In the 1890s some Mormons went north to form communities in Alberta after The Church of Jesus Christ of Latter-day Saints rejected plural marriage. The 1960s saw the arrival of about 50,000 draft-dodgers who opposed the Vietnam War.\n\nIn the late 19th and early 20th centuries, about 900,000 French Canadians moved to the U.S., with 395,000 residents there in 1900. Two-thirds went to mill towns in New England, where they formed distinctive ethnic communities. By the late 20th century, they had abandoned the French language, but most kept the Catholic religion. About twice as many English Canadians came to the U.S., but they did not form distinctive ethnic settlements.\n\nCanada was a way-station through which immigrants from other lands stopped for a while, ultimately heading to the U.S. In 1851–1951, 7.1 million people arrived in Canada (mostly from Continental Europe), and 6.6 million left Canada, most of them to the U.S.\n\nAt the outset of the American Revolutionary War, the American revolutionaries hoped the French Canadians in Quebec and the Colonists in Nova Scotia would join their rebellion and they were pre-approved for joining the United States in the Articles of Confederation. When Canada was invaded, thousands joined the American cause and formed regiments that fought during the war; however most remained neutral and some joined the British effort. Britain advised the French Canadians that the British Empire already enshrined their rights in the Quebec Act, which the American colonies had viewed as one of the Intolerable Acts. The American invasion was a fiasco and Britain tightened its grip on its northern possessions; in 1777, a major British invasion into New York led to the surrender of the entire British army at Saratoga, and led France to enter the war as an ally of the U.S. The French Canadians largely ignored France's appeals for solidarity. After the war Canada became a refuge for about 75,000 Loyalists who either wanted to leave the U.S., or were compelled by Patriot reprisals to do so.\n\nAmong the original Loyalists there were 3,500 free blacks. Most went to Nova Scotia and in 1792, 1200 migrated to Sierra Leone. About 2000 black slaves were brought in by Loyalist owners; they remained slaves in Canada until the Empire abolished slavery in 1833. Before 1860, about 30,000–40,000 blacks entered Canada; many were already free and others were escaped slaves who came through the Underground Railroad.\n\nThe Treaty of Paris (1783), which ended the war, called for British forces to vacate all their forts south of the Great Lakes border. Britain refused to do so, citing failure of the United States to provide financial restitution for Loyalists who had lost property in the war. The Jay Treaty in 1795 with Great Britain resolved that lingering issue and the British departed the forts. Thomas Jefferson saw the nearby British imperial presence as a threat to the United States, and so he opposed the Jay Treaty, and it became one of the major political issues in the United States at the time. Thousands of Americans immigrated to Upper Canada (Ontario) from 1785 to 1812 to obtain cheaper land and better tax rates prevalent in that province; despite expectations that they would be loyal to the U.S. if a war broke out, in the event they were largely non-political.\n\nTensions mounted again after 1805, erupting into the War of 1812, when the Americans declared war on Britain. The Americans were angered by British harassment of U.S. ships on the high seas and seizure (\"Impressment\") of 6,000 sailors from American ships, severe restrictions against neutral American trade with France, and British support for hostile Indian tribes in Ohio and territories the U.S. had gained in 1783. American \"honor\" was an implicit issue. The Americans were outgunned by more than 10 to 1 by the Royal Navy, but could call on an army much larger than the British garrison in Canada, and so a land invasion of Canada was proposed as the only feasible, and most advantegous means of attacking the British Empire. Americans on the western frontier also hoped an invasion would bring an end to British support of Native American resistance to the westward expansion of the United States, typified by Tecumseh's coalition of tribes. Americans may also have wanted to annex Canada.\n\nOnce war broke out, the American strategy was to seize Canada—perhaps as a means of forcing concessions from the British Empire, or perhaps in order to annex it. There was some hope that settlers in western Canada—most of them recent immigrants from the U.S.—would welcome the chance to overthrow their British rulers. However, the American invasions were defeated primarily by British regulars with support from Native Americans and Upper Canada (Ontario) militia. Aided by the powerful Royal Navy, a series of British raids on the American coast were highly successful, culminating with an attack on Washington that resulted in the British burning of the White House, Capitol, and other public buildings. Major British invasions of New York in 1814 and Louisiana in 1814–15 were fiascoes, with the British retreating from New York and decisively defeated at the Battle of New Orleans. At the end of the war, Britain's American Indian allies had largely been defeated, and the Americans controlled a strip of Western Ontario centered on Fort Malden. However, Britain held much of Maine, and, with the support of their remaining American Indian allies, huge areas of the Old Northwest, including Wisconsin and much of Michigan and Illinois. With the surrender of Napoleon in 1814, Britain ended naval policies that angered Americans; with the defeat of the Indian tribes the threat to American expansion was ended. The upshot was both sides had asserted their honour, Canada was not annexed, and London and Washington had nothing more to fight over. The war was ended by the Treaty of Ghent, which took effect in February 1815. A series of postwar agreements further stabilized peaceful relations along the Canadian-US border. Canada reduced American immigration for fear of undue American influence, and built up the Anglican church as a counterweight to the largely American Methodist and Baptist churches.\n\nIn later years, Anglophone Canadians, especially in Ontario, viewed the War of 1812 as a heroic and successful resistance against invasion and as a victory that defined them as a people. The myth that the Canadian militia had defeated the invasion almost single-handed, known logically as the \"militia myth\", became highly prevalent after the war, having been propounded by John Strachan, Anglican Bishop of York. Meanwhile, the United States celebrated victory in its \"Second War of Independence,\" and war heroes such as Andrew Jackson and William Henry Harrison headed to the White House.\n\nIn the aftermath of the War of 1812, pro-imperial conservatives led by Anglican Bishop John Strachan took control in Ontario (\"Upper Canada\"), and promoted the Anglican religion as opposed to the more republican Methodist and Baptist churches. A small interlocking elite, known as the Family Compact took full political control. Democracy, as practiced in the US, was ridiculed. The policies had the desired effect of deterring immigration from United States. Revolts in favor of democracy in Ontario and Quebec (\"Lower Canada\") in 1837 were suppressed; many of the leaders fled to the US. The American policy was to largely ignore the rebellions, and indeed ignore Canada generally in favor of westward expansion of the American Frontier.\n\nAt the end of the American Civil War in 1865, Americans were angry at British support for the Confederacy. One result was toleration of Fenian efforts to use the U.S. as a base to attack Canada. More serious was the demand for a huge payment to cover the damages caused, on the notion that British involvement had lengthened the war. Senator Charles Sumner, the chairman of the Senate Foreign Relations Committee, originally wanted to ask for $2 billion, or alternatively the ceding of all of Canada to the United States. When American Secretary of State William H. Seward negotiated the Alaska Purchase with Russia in 1867, he intended it as the first step in a comprehensive plan to gain control of the entire northwest Pacific Coast. Seward was a firm believer in Manifest Destiny, primarily for its commercial advantages to the U.S. Seward expected British Columbia to seek annexation to the U.S. and thought Britain might accept this in exchange for the \"Alabama\" claims. Soon other elements endorsed annexation, Their plan was to annex British Columbia, Red River Colony (Manitoba), and Nova Scotia, in exchange for the dropping the damage claims. The idea reached a peak in the spring and summer of 1870, with American expansionists, Canadian separatists, and British anti-imperialists seemingly combining forces. The plan was dropped for multiple reasons. London continued to stall, American commercial and financial groups pressed Washington for a quick settlement of the dispute on a cash basis, growing Canadian nationalist sentiment in British Columbia called for staying inside the British Empire, Congress became preoccupied with Reconstruction, and most Americans showed little interest in territorial expansion. The \"Alabama Claims\" dispute went to international arbitration. In one of the first major cases of arbitration, the tribunal in 1872 supported the American claims and ordered Britain to pay $15.5 million. Britain paid and the episode ended in peaceful relations.\n\nCanada became a self-governing dominion in 1867 in internal affairs while Britain controlled diplomacy and defense policy. Prior to Confederation, there was an Oregon boundary dispute in which the Americans claimed the 54th degree latitude. That issue was resolved by splitting the disputed territory; the northern half became British Columbia, and the southern half the states of Washington and Oregon. Strained relations with America continued, however, due to a series of small-scale armed incursions named the Fenian raids by Irish-American Civil War veterans across the border from 1866 to 1871 in an attempt to trade Canada for Irish independence. The American government, angry at Canadian tolerance of Confederate raiders during the American Civil War, moved very slowly to disarm the Fenians. The British government, in charge of diplomatic relations, protested cautiously, as Anglo-American relations were tense. Much of the tension was relieved as the Fenians faded away and in 1872 by the settlement of the Alabama Claims, when Britain paid the U.S. $15.5 million for war losses caused by warships built in Britain and sold to the Confederacy.\n\nDisputes over ocean boundaries on Georges Bank and over fishing, whaling, and sealing rights in the Pacific were settled by international arbitration, setting an important precedent.\n\nAfter 1850, the pace of industrialization and urbanization was much faster in the United States, drawing a wide range of immigrants from the North. By 1870, 1/6 of all the people born in Canada had moved to the United States, with the highest concentrations in New England, which was the destination of Francophone emigrants from Quebec and Anglophone emigrants from the Maritimes. It was common for people to move back and forth across the border, such as seasonal lumberjacks, entrepreneurs looking for larger markets, and families looking for jobs in the textile mills that paid much higher wages than in Canada.\n\nThe southward migration slacked off after 1890, as Canadian industry began a growth spurt. By then, the American frontier was closing, and thousands of farmers looking for fresh land moved from the United States north into the Prairie Provinces. The net result of the flows were that in 1901 there were 128,000 American-born residents in Canada (3.5% of the Canadian population) and 1.18 million Canadian-born residents in the United States (1.6% of the U.S. population).\n\nA short-lived controversy was the Alaska boundary dispute, settled in favor of the United States in 1903. No one cared until a gold rush brought tens of thousands of men to Canada's Yukon, and they had to arrive through American ports. Canada needed its port and claimed that it had a legal right to a port near the present American town of Haines, Alaska. It would provide an all-Canadian route to the rich goldfields. The dispute was settled by arbitration, and the British delegate voted with the Americans—to the astonishment and disgust of Canadians who suddenly realized that Britain considered its relations with the United States paramount compared to those with Canada. The arbitrartion validated the status quo, but made Canada angry at Britain.\n\n1907 saw a minor controversy over USS \"Nashville\" sailing into the Great Lakes via Canada without Canadian permission. To head off future embarrassments, in 1909 the two sides signed the International Boundary Waters Treaty and the International Joint Commission was established to manage the Great Lakes and keep them disarmed. It was amended in World War II to allow the building and training of warships.\n\nAnti-Americanism reached a shrill peak in 1911 in Canada. The Liberal government in 1911 negotiated a Reciprocity treaty with the U.S. that would lower trade barriers. Canadian manufacturing interests were alarmed that free trade would allow the bigger and more efficient American factories to take their markets. The Conservatives made it a central campaign issue in the 1911 election, warning that it would be a \"sell out\" to the United States with economic annexation a special danger. Conservative slogan was \"No truck or trade with the Yankees\", as they appealed to Canadian nationalism and nostalgia for the British Empire to win a major victory.\n\nCanada demanded and received permission from London to send its own delegation to the Versailles Peace Talks in 1919, with the proviso that it sign the treaty under the British Empire. Canada subsequently took responsibility for its own foreign and military affairs in the 1920s. Its first ambassador to the United States, Vincent Massey, was named in 1927. The United States first ambassador to Canada was William Phillips. Canada became an active member of the British Commonwealth, the League of Nations, and the World Court, none of which included the U.S.\n\nIn July 1923, as part of his Pacific Northwest tour and a week before his death, US President Warren Harding visited Vancouver, making him the first head of state of the United States to visit Canada. The then Premier of British Columbia, John Oliver, and then mayor of Vancouver, Charles Tisdall, hosted a lunch in his honor at the Hotel Vancouver. Over 50,000 people heard Harding speak in Stanley Park. A monument to Harding designed by Charles Marega was unveiled in Stanley Park in 1925.\n\nRelations with the United States were cordial until 1930, when Canada vehemently protested the new Smoot–Hawley Tariff Act by which the U.S. raised tariffs (taxes) on products imported from Canada. Canada retaliated with higher tariffs of its own against American products, and moved toward more trade within the British Commonwealth. U.S.–Canadian trade fell 75% as the Great Depression dragged both countries down.\n\nDown to the 1920s the war and naval departments of both nations designed hypothetical war game scenarios with the other as an enemy. These were primarily exercises; the departments were never told to get ready for a real war. In 1921, Canada developed Defence Scheme No. 1 for an attack on American cities and for forestalling invasion by the United States until Imperial reinforcements arrived. Through the later 1920s and 1930s, the United States Army War College developed a plan for a war with the British Empire waged largely on North American territory, in War Plan Red.\n\nHerbert Hoover meeting in 1927 with British Ambassador Sir Esme Howard agreed on the \"absurdity of contemplating the possibility of war between the United States and the British Empire.\"\n\nIn 1938, as the roots of World War II were set in motion, U.S. President Franklin Roosevelt gave a public speech at Queens University in Kingston, Ontario, declaring that the United States would not sit idly by if another power tried to dominate Canada. Diplomats saw it as a clear warning to Germany not to attack Canada.\n\nThe two nations cooperated closely in World War II, as both nations saw new levels of prosperity and a determination to defeat the Axis powers. Prime Minister William Lyon Mackenzie King and President Franklin D. Roosevelt were determined not to repeat the mistakes of their predecessors. They met in August 1940 at Ogdensburg, issuing a declaration calling for close cooperation, and formed the Permanent Joint Board on Defense (PJBD).\n\nKing sought to raise Canada's international visibility by hosting the August 1943 Quadrant conference in Quebec on military and political strategy; he was a gracious host but was kept out of the important meetings by Winston Churchill and Roosevelt.\n\nCanada allowed the construction of the Alaska Highway and participated in the building of the atomic bomb. 49,000 Americans joined the RCAF (Canadian) or RAF (British) air forces through the Clayton Knight Committee, which had Roosevelt's permission to recruit in the U.S. in 1940–42.\n\nAmerican attempts in the mid-1930s to integrate British Columbia into a united West Coast military command had aroused Canadian opposition. Fearing a Japanese invasion of Canada's vulnerable coast, American officials urged the creation of a united military command for an eastern Pacific Ocean theater of war. Canadian leaders feared American imperialism and the loss of autonomy more than a Japanese invasion. In 1941, Canadians successfully argued within the PJBD for mutual cooperation rather than unified command for the West Coast.\n\nThe United States built large military bases in Newfoundland, at the time, a British dominion. The American involvement ended the depression and brought new prosperity; Newfoundland's business community sought closer ties with the United States as expressed by the Economic Union Party. Ottawa took notice and wanted Newfoundland to join Canada, which it did after hotly contested referenda. There was little demand in the United States for the acquisition of Newfoundland, so the United States did not protest the British decision not to allow an American option on the Newfoundland referendum.\n\nFollowing co-operation in the two World Wars, Canada and the United States lost much of their previous animosity. As Britain's influence as a global imperial power declined, Canada and the United States became extremely close partners. Canada was a close ally of the United States during the Cold War.\n\nThe United States had become Canada's largest market, and after the war the Canadian economy became dependent on smooth trade flows with the United States so much that in 1971 when the United States enacted the \"Nixon Shock\" economic policies (including a 10% tariff on all imports) it put the Canadian government into a panic. This led in a large part to the articulation of Prime Minister Trudeau's \"Third Option\" policy of diversifying Canada's trade and downgrading the importance of Canada – United States relations. In a 1972 speech in Ottawa, Nixon declared the \"special relationship\" between Canada and the United States dead.\n\nThe main issues in Canada–U.S. relations in the 1990s focused on the NAFTA agreement, which was signed in 1994. It created a common market that by 2014 was worth $19 trillion, encompassed 470 million people, and had created millions of jobs. Wilson says, \"Few dispute that NAFTA has produced\nlarge and measurable gains for Canadian consumers, workers, and businesses.\" However, he adds, \"NAFTA has fallen well short of expectations.\"\n\nSince the arrival of the Loyalists as refugees from the American Revolution in the 1780s, historians have identified a constant theme of Canadian fear of the United States and of \"Americanization\" or a cultural takeover. In the War of 1812, for example, the enthusiastic response by French militia to defend Lower Canada reflected, according to Heidler and Heidler (2004), \"the fear of Americanization.\" Scholars have traced this attitude over time in Ontario and Quebec.\n\nCanadian intellectuals who wrote about the U.S. in the first half of the 20th century identified America as the world center of modernity, and deplored it. Imperialists (who admired the British Empire) explained that Canadians had narrowly escaped American conquest with its rejection of tradition, its worship of \"progress\" and technology, and its mass culture; they explained that Canada was much better because of its commitment to orderly government and societal harmony. There were a few ardent defenders of the nation to the south, notably liberal and socialist intellectuals such as F. R. Scott and Jean-Charles Harvey (1891–1967).\n\nLooking at television, Collins (1990) finds that it is in English Canada that fear of cultural Americanization is most powerful, for there the attractions of the U.S. are strongest. Meren (2009) argues that after 1945, the emergence of Quebec nationalism and the desire to preserve French-Canadian cultural heritage led to growing anxiety regarding American cultural imperialism and Americanization. In 2006 surveys showed that 60 percent of Quebecers had a fear of Americanization, while other surveys showed they preferred their current situation to that of the Americans in the realms of health care, quality of life as seniors, environmental quality, poverty, educational system, racism and standard of living. While agreeing that job opportunities are greater in America, 89 percent disagreed with the notion that they would rather be in the United States, and they were more likely to feel closer to English Canadians than to Americans. However, there is evidence that the elites and Quebec are much less fearful of Americanization, and much more open to economic integration than the general public.\n\nThe history has been traced in detail by a leading Canadian historian J.L. Granatstein in \"Yankee Go Home: Canadians and Anti-Americanism\" (1997). Current studies report the phenomenon persists. Two scholars report, \"Anti-Americanism is alive and well in Canada today, strengthened by, among other things, disputes related to NAFTA, American involvement in the Middle East, and the ever-increasing Americanization of Canadian culture.\" Jamie Glazov writes, \"More than anything else, Diefenbaker became the tragic victim of Canadian anti-Americanism, a sentiment the prime minister had fully embraced by 1962. [He was] unable to imagine himself (or his foreign policy) without enemies.\" Historian J. M. Bumsted says, \"In its most extreme form, Canadian suspicion of the United States has led to outbreaks of overt anti-Americanism, usually spilling over against American residents in Canada.\" John R. Wennersten writes, \"But at the heart of Canadian anti-Americanism lies a cultural bitterness that takes an American expatriate unaware. Canadians fear the American media's influence on their culture and talk critically about how Americans are exporting a culture of violence in its television programming and movies.\" However Kim Nossal points out that the Canadian variety is much milder than anti-Americanism in some other countries. By contrast Americans show very little knowledge or interest one way or the other regarding Canadian affairs. Canadian historian Frank Underhill, quoting Canadian playwright Merrill Denison summed it up: \"Americans are benevolently ignorant about Canada, whereas Canadians are malevolently informed about the United States.\"\n\nThe executive of each country is represented differently. The President of the United States serves as both the head of state and head of government, and his \"administration\" is the executive, while the Prime Minister of Canada is head of government only, and his or her \"government\" or \"ministry\" directs the executive.\n\nRelations between Brian Mulroney and Ronald Reagan were famously close. This relationship resulted in negotiations for the Canada–United States Free Trade Agreement, and the U.S.–Canada Air Quality Agreement to reduce acid-rain-causing emissions, both major policy goals of Mulroney, that would be finalized under the presidency of George H. W. Bush.\n\nAlthough Jean Chrétien was wary of appearing too close to the President, personally, he and Bill Clinton were known to be golfing partners. Their governments had many small trade quarrels over the Canadian content of American magazines, softwood lumber, and so on, but on the whole were quite friendly. Both leaders had run on reforming or abolishing NAFTA, but the agreement went ahead with the addition of environmental and labor side agreements. Crucially, the Clinton administration lent rhetorical support to Canadian unity during the 1995 referendum in Quebec on separation from Canada.\n\nRelations between Chrétien and George W. Bush were strained throughout their overlapping times in office. After the September 11 attacks terror attacks, Jean Chrétien publicly mused that U.S. foreign policy might be part of the \"root causes\" of terrorism. Some Americans criticized his \"smug moralism\", and Chrétien's public refusal to support the 2003 Iraq war was met drew responses in the United States, especially among conservatives.\n\nStephen Harper and George W. Bush were thought to share warm personal relations and also close ties between their administrations. Because Bush was so unpopular among liberals in Canada (particularly in the media), this was underplayed by the Harper government.\n\nShortly after being congratulated by Bush for his victory in February 2006, Harper rebuked U.S. ambassador to Canada David Wilkins for criticizing the Conservatives' plans to assert Canada's sovereignty over the Arctic Ocean waters with military force.\n\nPresident Barack Obama's first international trip was to Canada on February 19, 2009, thereby sending a strong message of peace and cooperation. With the exception of Canadian lobbying against \"Buy American\" provisions in the U.S. stimulus package, relations between the two administrations were smooth.\n\nThey also held friendly bets on hockey games during the Winter Olympic season. In the 2010 Winter Olympics hosted by Canada in Vancouver, Canada defeated the US in both gold medal matches, entitling Stephen Harper to receive a case of Molson Canadian beer from Barack Obama; in reverse, if Canada had lost, Harper would have provided a case of Yuengling beer to Obama. During the 2014 Winter Olympics, alongside U.S. Secretary of State John Kerry & Minister of Foreign Affairs John Baird, Stephen Harper was given a case of Samuel Adams beer by Obama for the Canadian gold medal victory over the US in women's hockey, and the semi-final victory over the US in men's hockey.\n\nOn February 4, 2011, Harper and Obama issued a \"Declaration on a Shared Vision for Perimeter Security and Economic Competitiveness\" and announced the creation of the Canada–United States Regulatory Cooperation Council (RCC) \"to increase regulatory transparency and coordination between the two countries.\"\n\nHealth Canada and the United States Food and Drug Administration (FDA) under the RCC mandate, undertook the \"first of its kind\" initiative by selecting \"as its first area of alignment common cold indications for certain over-the-counter antihistamine ingredients (GC 2013-01-10).\"\n\nOn December 7, 2011, Harper flew to Washington, met with Obama and signed an agreement to implement the joint action plans that had been developed since the initial meeting in February. The plans called on both countries to spend more on border infrastructure, share more information on people who cross the border, and acknowledge more of each other's safety and security inspection on third-country traffic. An editorial in \"The Globe and Mail\" praised the agreement for giving Canada the ability to track whether failed refugee claimants have left Canada via the U.S. and for eliminating \"duplicated baggage screenings on connecting flights\". The agreement is not a legally binding treaty, and relies on the political will and ability of the executives of both governments to implement the terms of the agreement. These types of executive agreements are routine—on both sides of the Canada–U.S. border.\n\nPresident Barack Obama and Prime Minister Justin Trudeau first met formally at the APEC summit meeting in Manila, Philippines in November 2015, nearly a week after the latter was sworn into the office. Both leaders expressed eagerness for increased cooperation and coordination between the two countries during the course of Trudeau's government with Trudeau promising an \"enhanced Canada–U.S. partnership\".\n\nOn November 6, 2015, Obama announced the U.S. State Department's rejection of the proposed Keystone XL pipeline, the fourth phase of the Keystone oil pipeline system running between Canada and the United States, to which Trudeau expressed disappointment but said that the rejection would not damage Canada–U.S. relations and would instead provide a \"fresh start\" to strengthening ties through cooperation and coordination, saying that \"the Canada–U.S. relationship is much bigger than any one project.\" Obama has since praised Trudeau's efforts to prioritize the reduction of climate change, calling it \"extraordinarily helpful\" to establish a worldwide consensus on addressing the issue.\n\nAlthough Trudeau has told Obama his plans to withdraw Canada's McDonnell Douglas CF-18 Hornet jets assisting in the American-led intervention against ISIL, Trudeau said that Canada will still \"do more than its part\" in combating the terrorist group by increasing the number of Canadian special forces members training and fighting on ground in Iraq and Syria.\n\nTrudeau visited the White House for an official visit and state dinner on March 10, 2016. Trudeau and Obama were reported to have shared warm personal relations during the visit, making humorous remarks about which country was better at hockey and which country had better beer. Obama complimented Trudeau's 2015 election campaign for its \"message of hope and change\" and \"positive and optimistic vision\". Obama and Trudeau also held \"productive\" discussions on climate change and relations between the two countries, and Trudeau invited Obama to speak in the Canadian parliament in Ottawa later in the year.\n\nFollowing the victory of Donald Trump in the 2016 U.S. presidential election, Trudeau congratulated him and invited him to visit Canada at the \"earliest opportunity.\" Prime Minister Trudeau and President Trump formally met for the first time at the White House on February 13, 2017, nearly a month after Trump was sworn into the office. Trump has ruffled relations with Canada with tariffs on softwood lumber. Diafiltered Milk has also been brought up by Trump as an area that needs to be negotiated. Trump is expected to renegotiate NAFTA with Canada.\n\nThe Canadian military, like forces of other NATO countries, fought alongside the United States in most major conflicts since World War II, including the Korean War, the Gulf War, the Kosovo War, and most recently the war in Afghanistan. The main exceptions to this were the Canadian government's opposition to the Vietnam War and the Iraq War, which caused some brief diplomatic tensions. Despite these issues, military relations have remained close.\n\nAmerican defense arrangements with Canada are more extensive than with any other country. The Permanent Joint Board of Defense, established in 1940, provides policy-level consultation on bilateral defense matters. The United States and Canada share North Atlantic Treaty Organization (NATO) mutual security commitments. In addition, American and Canadian military forces have cooperated since 1958 on continental air defense within the framework of the North American Aerospace Defense Command (NORAD). Canadian forces have provided indirect support for the American invasion of Iraq that began in 2003. Moreover, interoperability with the American armed forces has been a guiding principle of Canadian military force structuring and doctrine since the end of the Cold War. Canadian navy frigates, for instance, integrate seamlessly into American carrier battle groups.\n\nIn commemoration of the 200th Anniversary of the War of 1812 ambassadors from Canada and the US, and naval officers from both countries gathered at the Pritzker Military Library on August 17, 2012, for a panel discussion on Canada-US relations with emphasis on national security-related matters. Also as part of the commemoration, the navies of both countries sailed together throughout the Great Lakes region.\n\nCanada's elite JTF2 unit joined American special forces in Afghanistan shortly after the al-Qaida attacks on September 11, 2001. Canadian forces joined the multinational coalition in Operation Anaconda in January 2002. On April 18, 2002, an American pilot bombed Canadian forces involved in a training exercise, killing four and wounding eight Canadians. A joint American-Canadian inquiry determined the cause of the incident to be pilot error, in which the pilot interpreted ground fire as an attack; the pilot ignored orders that he felt were \"second-guessing\" his field tactical decision. Canadian forces assumed a six-month command rotation of the International Security Assistance Force in 2003; in 2005, Canadians assumed operational command of the multi-national Brigade in Kandahar, with 2,300 troops, and supervises the Provincial Reconstruction Team in Kandahar, where al-Qaida forces are most active. Canada has also deployed naval forces in the Persian Gulf since 1991 in support of the UN Gulf Multinational Interdiction Force.\n\nThe Canadian Embassy in Washington, DC maintains a public relations website named CanadianAlly.com, which is intended \"to give American citizens a better sense of the scope of Canada's role in North American and Global Security and the War on Terror\".\n\nThe New Democratic Party and some recent Liberal leadership candidates have expressed opposition to Canada's expanded role in the Afghan conflict on the ground that it is inconsistent with Canada's historic role (since the Second World War) of peacekeeping operations.\n\nAccording to contemporary polls, 71% of Canadians were opposed to the 2003 invasion of Iraq. Many Canadians, and the former Liberal Cabinet headed by Paul Martin (as well as many Americans such as Bill Clinton and Barack Obama), made a policy distinction between conflicts in Afghanistan and Iraq, unlike the Bush Doctrine, which linked these together in a \"Global war on terror\".\n\nCanada has been involved in international responses to the threats from Daesh/ISIS/ISIL in Syria and Iraq, and is a member of the Global Coalition to Counter Daesh. In October 2016, Foreign Affairs Minister Dion and National Defence Minister Sajjan meet U.S. special envoy for this coalition. The Americans thanked Canada \"for the role of Canadian Armed Forces (CAF) in providing training and assistance to Iraqi security forces, as well as the CAF's role in improving essential capacity-building capabilities with regional forces.\"\n\nCanada and the United States have the world's largest trading relationship, with huge quantities of goods and people flowing across the border each year. Since the 1987 Canada–United States Free Trade Agreement, there have been no tariffs on most goods passed between the two countries.\n\nIn the course of the softwood lumber dispute, the U.S. has placed tariffs on Canadian softwood lumber because of what it argues is an unfair Canadian government subsidy, a claim which Canada disputes. The dispute has cycled through several agreements and arbitration cases. Other notable disputes include the Canadian Wheat Board, and Canadian cultural \"restrictions\" on magazines and television (See CRTC, CBC, and National Film Board of Canada). Canadians have been criticized about such things as the ban on beef since a case of Mad Cow disease was discovered in 2003 in cows from the United States (and a few subsequent cases) and the high American agricultural subsidies. Concerns in Canada also run high over aspects of the North American Free Trade Agreement (NAFTA) such as Chapter 11.\n\nA principal instrument of this cooperation is the International Joint Commission (IJC), established as part of the Boundary Waters Treaty of 1909 to resolve differences and promote international cooperation on boundary waters. The Great Lakes Water Quality Agreement of 1972 is another historic example of joint cooperation in controlling trans-border water pollution. However, there have been some disputes. Most recently, the Devil's Lake Outlet, a project instituted by North Dakota, has angered Manitobans who fear that their water may soon become polluted as a result of this project.\n\nBeginning in 1986 the Canadian government of Brian Mulroney began pressing the Reagan administration for an \"Acid Rain Treaty\" in order to do something about U.S. industrial air pollution causing acid rain in Canada. The Reagan administration was hesitant, and questioned the science behind Mulroney's claims. However, Mulroney was able to prevail. The product was the signing and ratification of the Air Quality Agreement of 1991 by the first Bush administration. Under that treaty, the two governments consult semi-annually on trans-border air pollution, which has demonstrably reduced acid rain, and they have since signed an annex to the treaty dealing with ground level ozone in 2000. Despite this, trans-border air pollution remains an issue, particularly in the Great Lakes-St. Lawrence watershed during the summer. The main source of this trans-border pollution results from coal-fired power stations, most of them located in the Midwestern United States. As part of the negotiations to create NAFTA, Canada and the U.S. signed, along with Mexico, the North American Agreement On Environmental Cooperation which created the Commission for Environmental Cooperation which monitors environmental issues across the continent, publishing the North American Environmental Atlas as one aspect of its monitoring duties.\n\nCurrently neither of the countries' governments support the Kyoto Protocol, which set out time scheduled curbing of greenhouse gas emissions. Unlike the United States, Canada has ratified the agreement. Yet after ratification, due to internal political conflict within Canada, the Canadian government does not enforce the Kyoto Protocol, and has received criticism from environmental groups and from other governments for its climate change positions. In January 2011, the Canadian minister of the environment, Peter Kent, explicitly stated that the policy of his government with regards to greenhouse gas emissions reductions is to wait for the United States to act first, and then try to harmonize with that action – a position that has been condemned by environmentalists and Canadian nationalists, and as well as scientists and government think-tanks.\n\nThe United States and Britain, had a long-standing dispute about the rights of Americans fishing in the waters near Newfoundland. Before 1776, there was no question that American fishermen, mostly from Massachusetts, had rights to use the waters off Newfoundland. In the peace treaty negotiations of 1783, the Americans insisted on a statement of these rights. However, France, an American ally, disputed the American position because France had its own specified rights in the area and wanted them to be exclusive. The Treaty of Paris (1783) gave the Americans not rights, but rather \"liberties\" to fish within the territorial waters of British North America and to dry fish on certain coasts.\n\nAfter the War of 1812, the Convention of 1818 between the United States and Britain specified exactly what liberties were involved. Canadian and Newfoundland fishermen contested these liberties in the 1830s and 1840s. The Canadian–American Reciprocity Treaty of 1854, and the Treaty of Washington of 1871 spelled-out the liberties in more detail. However the Treaty of Washington expired in 1885, and there was a continuous round of disputes over jurisdictions and liberties. Britain and the United States sent the issue to the Permanent Court of Arbitration in The Hague in 1909. It produced a compromise settlement that permanently ended the problems.\n\nIn 2003 the American government became concerned when members of the Canadian government announced plans to decriminalize marijuana. David Murray, an assistant to U.S. Drug Czar John P. Walters, said in a CBC interview that, \"We would have to respond. We would be forced to respond.\" However the election of the Conservative Party in early 2006 halted the liberalization of marijuana laws for the foreseeable future.\n\nA 2007 joint report by American and Canadian officials on cross-border drug smuggling indicated that, despite their best efforts, \"drug trafficking still occurs in significant quantities in both directions across the border. The principal illicit substances smuggled across our shared border are MDMA (\"Ecstasy\"), cocaine, and marijuana.\" The report indicated that Canada was a major producer of \"Ecstasy\" and marijuana for the U.S. market, while the U.S. was a transit country for cocaine entering Canada.\n\nPresidents and prime ministers typically make formal or informal statements that indicate the diplomatic policy of their administration. Diplomats and journalists at the time—and historians since—dissect the nuances and tone to detect the warmth or coolness of the relationship.\n\nCanada's first Prime Minister also said:\n\n\nUnited States President George W. Bush was \"deeply disliked\" by a majority of Canadians according to the \"Arizona Daily Sun\". A 2004 poll found that more than two thirds of Canadians favoured Democrat John Kerry over Bush in the 2004 presidential election, with Bush's lowest approval ratings in Canada being in the province of Quebec where just 11% of the population supported him. Canadian public opinion of Barack Obama was significantly more positive. A 2012 poll found that 65% of Canadians would vote for Obama in the 2012 presidential election \"if they could\" while only 9% of Canadians would vote for his Republican opponent Mitt Romney. The same study found that 61% of Canadians felt that the Obama administration had been \"good\" for America, while only 12% felt it had been \"bad\". Similarly, a Pew Research poll conducted in June 2016 found that 83% of Canadians were \"confident in Obama to do the right thing regarding world affairs\". The study also found that a majority of members of all three major Canadian political parties supported Obama, and also found that Obama had slightly higher approval ratings in Canada in 2012 than he did in 2008. John Ibbitson of \"The Globe and Mail\" stated in 2012 that Canadians generally supported Democratic presidents over Republican presidents, citing how President Richard Nixon was \"never liked\" in Canada and that Canadians generally did not approve of Prime Minister Brian Mulroney's friendship with President Ronald Reagan.\n\nA January 2017 poll found that 66% of Canadians \"disapproved\" of Donald Trump, with 23% approving of him and 11% being \"unsure\". The poll also found that only 18% of Canadians believed Trump's presidency would have a positive impact on Canada, while 63% believed it would have a negative effect.\n\nThese include maritime boundary disputes:\n\nTerritorial land disputes:\n\nand disputes over the international status of the:\n\nA long-simmering dispute between Canada and the U.S. involves the issue of Canadian sovereignty over the Northwest Passage (the sea passages in the Arctic). Canada's assertion that the Northwest Passage represents internal (territorial) waters has been challenged by other countries, especially the U.S., which argue that these waters constitute an international strait (international waters). Canadians were alarmed when Americans drove the reinforced oil tanker through the Northwest Passage in 1969, followed by the icebreaker Polar Sea in 1985, which actually resulted in a minor diplomatic incident. In 1970, the Canadian parliament enacted the Arctic Waters Pollution Prevention Act, which asserts Canadian regulatory control over pollution within a 100-mile zone. In response, the United States in 1970 stated, \"We cannot accept the assertion of a Canadian claim that the Arctic waters are internal waters of Canada. ... Such acceptance would jeopardize the freedom of navigation essential for United States naval activities worldwide.\" A compromise of sorts was reached in 1988, by an agreement on \"Arctic Cooperation,\" which pledges that voyages of American icebreakers \"will be undertaken with the consent of the Government of Canada.\" However the agreement did not alter either country's basic legal position. Paul Cellucci, the American ambassador to Canada, in 2005 suggested to Washington that it should recognize the straits as belonging to Canada. His advice was rejected and Harper took opposite positions. The U.S. opposes Harper's proposed plan to deploy military icebreakers in the Arctic to detect interlopers and assert Canadian sovereignty over those waters.\n\nCanada and the United States both hold membership in a number of multinational organizations such as:\n\nCanada's chief diplomatic mission to the United States is the Canadian Embassy in Washington, D.C.. It is further supported by many consulates located through United States.\nThe Canadian Government maintains consulates-general in several major U.S. cities including: Atlanta, Boston, Chicago, Dallas, Denver, Detroit, Los Angeles, Miami, Minneapolis, New York City, San Francisco and Seattle. Canadian consular services are also available in Honolulu at the consulate of Australia through the Canada–Australia Consular Services Sharing Agreement.\n\nThere are also Canadian trade offices located in Houston, Palo Alto and San Diego.\n\nThe United States's chief diplomatic mission to Canada is the United States Embassy in Ottawa. It is further supported by many consulates located throughout Canada.\nThe U.S government maintains consulates-general in several major Canadian cities including:\nCalgary, Halifax, Montreal, Quebec City, Toronto, Vancouver and Winnipeg.\n\nThe United States also maintains Virtual Presence Posts (VPP) in the: Northwest Territories, Nunavut, Southwestern Ontario and Yukon.\n\n\n\n"}
{"id": "5211", "url": "https://en.wikipedia.org/wiki?curid=5211", "title": "Christianity", "text": "Christianity\n\nChristianity is an Abrahamic monotheistic religion based on the life and teachings of Jesus Christ, who serves as the focal point of the Christian faith. It is the world's largest religion, with over 2.4 billion followers, or 33% of the global population, known as Christians. Christians make up a majority of the population in 158 countries and territories. They believe that Jesus is the Son of God and the savior of humanity whose coming as the Messiah (the Christ) was prophesied in the Old Testament.\n\nChristian theology is summarized in creeds such as the Apostles' Creed and Nicene Creed. These professions of faith state that Jesus suffered, died, was buried, descended into hell, and rose from the dead, in order to grant eternal life to those who believe in him and trust in him for the remission of their sins. The creeds further maintain that Jesus physically ascended into heaven, where he reigns with God the Father in the unity of the Holy Spirit, and that he will return to judge the living and the dead and grant eternal life to his followers. His incarnation, earthly ministry, crucifixion and resurrection are often referred to as \"the gospel\", meaning \"good news\". The term \"gospel\" also refers to written accounts of Jesus' life and teaching, four of which—Matthew, Mark, Luke, and John—are considered canonical and included in the Christian Bible.\n\nChristianity began as a Second Temple Judaic sect in the mid-1st century. Originating in Judea, it quickly spread to Europe, Syria, Mesopotamia, Asia Minor, Transcaucasia, Egypt, Ethiopia and India, and by the end of the 4th century had become the official state church of the Roman Empire. Following the Age of Discovery, Christianity spread to the Americas, Australasia, sub-Saharan Africa and the rest of the world through missionary work and colonization. Christianity has played a prominent role in the shaping of Western civilization.\n\nThroughout its history, Christianity has weathered schisms and theological disputes that have resulted in many distinct churches and denominations. Worldwide, the three largest branches of Christianity are the Catholic Church, the Eastern Orthodox Church and the various denominations of Protestantism. The Catholic and Eastern Orthodox churches broke communion with each other in the East–West Schism of 1054; Protestantism came into existence in the Reformation of the 16th century, splitting from the Catholic Church.\nThere are many important differences of interpretation and opinion of the Bible and sacred tradition on which Christianity is based. Because of these irreconcilable differences in theology and a lack of consensus on the core tenets of Christianity, Catholics, Protestants and Orthodox often deny that members of certain other branches are Christians.\n\nConcise doctrinal statements or confessions of religious beliefs are known as creeds (from Latin \"credo\", meaning \"I believe\"). They began as baptismal formulae and were later expanded during the Christological controversies of the 4th and 5th centuries to become statements of faith.\n\nMany evangelical Protestants reject creeds as definitive statements of faith, even while agreeing with some or all of the substance of the creeds. The Baptists have been non-creedal \"in that they have not sought to establish binding authoritative confessions of faith on one another.\" Also rejecting creeds are groups with roots in the Restoration Movement, such as the Christian Church (Disciples of Christ), the Evangelical Christian Church in Canada and the Churches of Christ.\nThe Apostles' Creed is the most widely accepted statement of the articles of Christian faith. It is used by a number of Christian denominations for both liturgical and catechetical purposes, most visibly by liturgical churches of Western Christian tradition, including the Latin Church of the Catholic Church, Lutheranism, Anglicanism and Western Rite Orthodoxy. It is also used by Presbyterians, Methodists and Congregationalists. This particular creed was developed between the 2nd and 9th centuries. Its central doctrines are those of the Trinity and God the Creator. Each of the doctrines found in this creed can be traced to statements current in the apostolic period. The creed was apparently used as a summary of Christian doctrine for baptismal candidates in the churches of Rome.\n\nIts main points include:\n\n\nThe Nicene Creed was formulated, largely in response to Arianism, at the Councils of Nicaea and Constantinople in 325 and 381 respectively and ratified as the universal creed of Christendom by the First Council of Ephesus in 431.\n\nThe Chalcedonian Definition, or Creed of Chalcedon, developed at the Council of Chalcedon in 451, though rejected by the Oriental Orthodox churches, taught Christ \"to be acknowledged in two natures, inconfusedly, unchangeably, indivisibly, inseparably\": one divine and one human, and that both natures, while perfect in themselves, are nevertheless also perfectly united into one person.\n\nThe Athanasian Creed, received in the Western Church as having the same status as the Nicene and Chalcedonian, says: \"We worship one God in Trinity, and Trinity in Unity; neither confounding the Persons nor dividing the Substance.\"\n\nMost Christians (Roman Catholic, Eastern Orthodox, Oriental Orthodox and Protestant alike) accept the use of creeds, and subscribe to at least one of the creeds mentioned above.\n\nThe central tenet of Christianity is the belief in Jesus as the Son of God and the Messiah (Christ). Christians believe that Jesus, as the Messiah, was anointed by God as savior of humanity, and hold that Jesus' coming was the fulfillment of messianic prophecies of the Old Testament. The Christian concept of the Messiah differs significantly from the contemporary Jewish concept. The core Christian belief is that through belief in and acceptance of the death and resurrection of Jesus, sinful humans can be reconciled to God and thereby are offered salvation and the promise of eternal life.\n\nWhile there have been many theological disputes over the nature of Jesus over the earliest centuries of Christian history, generally Christians believe that Jesus is God incarnate and \"true God and true man\" (or both fully divine and fully human). Jesus, having become fully human, suffered the pains and temptations of a mortal man, but did not sin. As fully God, he rose to life again. According to the New Testament, he rose from the dead, ascended to heaven, is seated at the right hand of the Father and will ultimately return to fulfill the rest of Messianic prophecy, including the resurrection of the dead, the Last Judgment and final establishment of the Kingdom of God.\n\nAccording to the canonical gospels of Matthew and Luke, Jesus was conceived by the Holy Spirit and born from the Virgin Mary. Little of Jesus' childhood is recorded in the canonical gospels, although infancy gospels were popular in antiquity. In comparison, his adulthood, especially the week before his death, is well documented in the gospels contained within the New Testament, because that part of his life is believed to be most important. The biblical accounts of Jesus' ministry include: his baptism, miracles, preaching, teaching and deeds.\n\nChristians consider the resurrection of Jesus to be the cornerstone of their faith (see 1 Corinthians 15) and the most important event in history. Among Christian beliefs, the death and resurrection of Jesus are two core events on which much of Christian doctrine and theology is based. According to the New Testament, Jesus was crucified, died a physical death, was buried within a tomb and rose from the dead three days later. \n\nThe New Testament mentions several resurrection appearances of Jesus on different occasions to his twelve apostles and disciples, including \"more than five hundred brethren at once\", before Jesus' Ascension to heaven. Jesus' death and resurrection are commemorated by Christians in all worship services, with special emphasis during Holy Week which includes Good Friday and Easter Sunday.\n\nThe death and resurrection of Jesus are usually considered the most important events in Christian theology, partly because they demonstrate that Jesus has power over life and death and therefore has the authority and power to give people eternal life.\n\nChristian churches accept and teach the New Testament account of the resurrection of Jesus with very few exceptions. Some modern scholars use the belief of Jesus' followers in the resurrection as a point of departure for establishing the continuity of the historical Jesus and the proclamation of the early church. Some liberal Christians do not accept a literal bodily resurrection, seeing the story as richly symbolic and spiritually nourishing myth. Arguments over death and resurrection claims occur at many religious debates and interfaith dialogues. Paul the Apostle, an early Christian convert and missionary, wrote, \"If Christ was not raised, then all our preaching is useless, and your trust in God is useless.\" \n\nPaul the Apostle, like Jews and Roman pagans of his time, believed that sacrifice can bring about new kinship ties, purity and eternal life. For Paul, the necessary sacrifice was the death of Jesus: Gentiles who are \"Christ's\" are, like Israel, descendants of Abraham and \"heirs according to the promise\". The God who raised Jesus from the dead would also give new life to the \"mortal bodies\" of Gentile Christians, who had become with Israel the \"children of God\" and were therefore no longer \"in the flesh\". \n\nModern Christian churches tend to be much more concerned with how humanity can be saved from a universal condition of sin and death than the question of how both Jews and Gentiles can be in God's family. According to both Catholic and Protestant doctrine, salvation comes by Jesus' substitutionary death and resurrection. The Catholic Church teaches that salvation does not occur without faithfulness on the part of Christians; converts must live in accordance with principles of love and ordinarily must be baptized. Martin Luther taught that baptism was necessary for salvation, but modern Lutherans and other Protestants tend to teach that salvation is a gift that comes to an individual by God's grace, sometimes defined as \"unmerited favor\", even apart from baptism.\n\nChristians differ in their views on the extent to which individuals' salvation is pre-ordained by God. Reformed theology places distinctive emphasis on grace by teaching that individuals are completely incapable of self-redemption, but that sanctifying grace is irresistible. In contrast Catholics, Orthodox Christians and Arminian Protestants believe that the exercise of free will is necessary to have faith in Jesus.\n\n\"Trinity\" refers to the teaching that the one God comprises three distinct, eternally co-existing persons; the \"Father\", the \"Son\" (incarnate in Jesus Christ) and the \"Holy Spirit\". Together, these three persons are sometimes called the Godhead, although there is no single term in use in Scripture to denote the unified Godhead. In the words of the Athanasian Creed, an early statement of Christian belief, \"the Father is God, the Son is God and the Holy Spirit is God, and yet there are not three Gods but one God\". They are distinct from another: the Father has no source, the Son is begotten of the Father and the Spirit proceeds from the Father. Though distinct, the three persons cannot be divided from one another in being or in operation. While some Christians also believe that God appeared as the Father in the Old Testament, it is agreed that he appeared as the Son in the New Testament, and will still continue to manifest as the Holy Spirit in the present. But still, God still existed as three persons in each of these times. However, traditionally there is a belief that it was the Son who appeared in the Old Testament because, for example, when the Trinity is depicted in art, the Son typically has the distinctive appearance, a cruciform halo identifying Christ, and in depictions of the Garden of Eden this looks forward to an Incarnation yet to occur. In some Early Christian sarcophagi the Logos is distinguished with a beard, \"which allows him to appear ancient, even preexistent.\"\n\nThe Trinity is an essential doctrine of mainstream Christianity. From earlier than the times of the Nicene Creed, 325, Christianity advocated the triune mystery-nature of God as a normative profession of faith. According to Roger E. Olson and Christopher Hall, through prayer, meditation, study and practice, the Christian community concluded \"that God must exist as both a unity and trinity\", codifying this in ecumenical council at the end of the 4th century.\nAccording to this doctrine, God is not divided in the sense that each person has a third of the whole; rather, each person is considered to be fully God (see Perichoresis). The distinction lies in their relations, the Father being unbegotten; the Son being begotten of the Father; and the Holy Spirit proceeding from the Father and (in Western Christian theology) from the Son. Regardless of this apparent difference, the three \"persons\" are each eternal and omnipotent. Other Christian religions including Unitarian Universalism, Jehovah's Witnesses, Mormonism and others do not share those views on the Trinity.\n\nThe Latin word \"trias\", from which \"trinity\" is derived, is first seen in the works of Theophilus of Antioch. He wrote of \"the Trinity of God (the Father), His Word (the Son) and His Wisdom (Holy Spirit)\". The term may have been in use before this time. Afterwards it appears in Tertullian. In the following century the word was in general use. It is found in many passages of Origen.\n\n\"Trinitarianism\" denotes those Christians who believe in the concept of the Trinity. Almost all Christian denominations and churches hold Trinitarian beliefs. Although the words \"Trinity\" and \"Triune\" do not appear in the Bible, theologians beginning in the 3rd century developed the term and concept to facilitate comprehension of the New Testament teachings of God as being Father, Son and Holy Spirit. Since that time, Christian theologians have been careful to emphasize that Trinity does not imply that there are three gods (the antitrinitarian heresy of Tritheism), nor that each hypostasis of the Trinity is one-third of an infinite God (partialism), nor that the Son and the Holy Spirit are beings created by and subordinate to the Father (Arianism). Rather, the Trinity is defined as one God in three Persons.\n\n\"Nontrinitarianism\" (or \"antitrinitarianism\") refers to theology that rejects the doctrine of the Trinity. Various nontrinitarian views, such as adoptionism or modalism, existed in early Christianity, leading to the disputes about Christology. Nontrinitarianism later appeared again in the Gnosticism of the Cathars in the 11th through 13th centuries, among groups with Unitarian theology in the Protestant Reformation of the 16th century, in the 18th-century Enlightenment and in some groups arising during the Second Great Awakening of the 19th century.\n\nChristianity, like other religions, has adherents whose beliefs and biblical interpretations vary. Christianity regards the biblical canon, the Old Testament and the New Testament, as the inspired word of God. The traditional view of inspiration is that God worked through human authors so that what they produced was what God wished to communicate. The Greek word referring to inspiration in is \"theopneustos\", which literally means \"God-breathed\".\n\nSome believe that divine inspiration makes our present Bibles inerrant. Others claim inerrancy for the Bible in its original manuscripts, although none of those are extant. Still others maintain that only a particular translation is inerrant, such as the King James Version. Another closely related view is Biblical infallibility or limited inerrancy, which affirms that the Bible is free of error as a guide to salvation, but may include errors on matters such as history, geography or science.\n\nThe books of the Bible accepted by the Orthodox, Catholic and Protestant churches vary somewhat, with Jews accepting only the Hebrew Bible as canonical; there is however substantial overlap. These variations are a reflection of the range of traditions, and of the councils that have convened on the subject. Every version of the Old Testament always includes the books of the Tanakh, the canon of the Hebrew Bible. The Catholic and Orthodox canons, in addition to the Tanakh, also include the Deuterocanonical Books as part of the Old Testament. These books appear in the Septuagint, but are regarded by Protestants to be apocryphal. However, they are considered to be important historical documents which help to inform the understanding of words, grammar and syntax used in the historical period of their conception. Some versions of the Bible include a separate Apocrypha section between the Old Testament and the New Testament. The New Testament, originally written in Koine Greek, contains 27 books which are agreed upon by all churches.\n\nModern scholarship has raised many issues with the Bible. While the Authorized King James Version is held to by many because of its striking English prose, in fact it was translated from the Erasmus Greek Bible which in turn \"was based on a single 12th Century manuscript that is one of the worst manuscripts we have available to us\". Much scholarship in the past several hundred years has gone into comparing different manuscripts in order to reconstruct the original text. Another issue is that several books are considered to be forgeries. The injunction that women \"be silent and submissive\" in 1 Timothy 2 is thought by many to be a forgery by a follower of Paul, a similar phrase in 1 Corinthians 14, which is thought to be by Paul, appears in different places in different manuscripts and is thought to originally be a margin note by a copyist. Other verses in 1 Corinthians, such as 1 Corinthians 11:2–16 where women are instructed to wear a covering over their hair \"when they pray or prophesies\", contradict this verse.\n\nA final issue with the Bible is the way in which books were selected for inclusion in the New Testament. Other Gospels have now been recovered, such as those found near Nag Hammadi in 1945, and while some of these texts are quite different from what Christians have been used to, it should be understood that some of this newly recovered Gospel material is quite possibly contemporaneous with, or even earlier than, the New Testament Gospels. The core of the Gospel of Thomas, in particular, may date from as early as AD 50 (although some major scholars contest this early dating), and if so would provide an insight into the earliest gospel texts that underlie the canonical Gospels, texts that are mentioned in Luke 1:1–2. The Gospel of Thomas contains much that is familiar from the canonical Gospels—verse 113, for example (\"The Father's Kingdom is spread out upon the earth, but people do not see it\"), is reminiscent of Luke 17:20–21—and the Gospel of John, with a terminology and approach that is suggestive of what was later termed \"Gnosticism\", has recently been seen as a possible response to the Gospel of Thomas, a text that is commonly labelled \"proto-Gnostic\". Scholarship, then, is currently exploring the relationship in the Early Church between mystical speculation and experience on the one hand and the search for church order on the other, by analyzing new-found texts, by subjecting canonical texts to further scrutiny, and by an examination of the passage of New Testament texts to canonical status.\n\nIn antiquity, two schools of exegesis developed in Alexandria and Antioch. Alexandrine interpretation, exemplified by Origen, tended to read Scripture allegorically, while Antiochene interpretation adhered to the literal sense, holding that other meanings (called \"theoria\") could only be accepted if based on the literal meaning.\n\nCatholic theology distinguishes two senses of scripture: the literal and the spiritual.\n\nThe \"literal\" sense of understanding scripture is the meaning conveyed by the words of Scripture. The \"spiritual\" sense is further subdivided into:\n\nRegarding exegesis, following the rules of sound interpretation, Catholic theology holds:\n\nSome Protestant interpreters make use of typology.\n\nThe end of things, whether the end of an individual life, the end of the age, or the end of the world, broadly speaking is Christian eschatology; the study of the destiny of humans as it is revealed in the Bible. The major issues in Christian eschatology are the Tribulation, death and the afterlife, the Rapture, the Second Coming of Jesus, Resurrection of the Dead, Heaven and Hell, Millennialism, the Last Judgment, the end of the world and the New Heavens and New Earth.\n\nChristians believe that the second coming of Christ will occur at the end of time after a period of severe persecution (the Great Tribulation). All who have died will be resurrected bodily from the dead for the Last Judgment. Jesus will fully establish the Kingdom of God in fulfillment of scriptural prophecies.\n\nMost Christians believe that human beings experience divine judgment and are rewarded either with eternal life or eternal damnation. This includes the general judgement at the resurrection of the dead as well as the belief (held by Roman Catholics, Orthodox and most Protestants) in a judgment particular to the individual soul upon physical death.\n\nIn Roman Catholicism, those who die in a state of grace, i.e., without any mortal sin separating them from God, but are still imperfectly purified from the effects of sin, undergo purification through the intermediate state of purgatory to achieve the holiness necessary for entrance into God's presence. Those who have attained this goal are called \"saints\" (Latin \"sanctus\", \"holy\").\n\nSome Christian groups, such as Seventh-day Adventists, hold to mortalism, the belief that the human soul is not naturally immortal, and is unconscious during the intermediate state between bodily death and resurrection. These Christians also hold to Annihilationism, the belief that subsequent to the final judgement, the wicked will cease to exist rather than suffer everlasting torment. Jehovah's Witnesses hold to a similar view.\n\nJustin Martyr described 2nd-century Christian liturgy in his \"First Apology\" (c. 150) to Emperor Antoninus Pius, and his description remains relevant to the basic structure of Christian liturgical worship:And on the day called Sunday, all who live in cities or in the country gather together to one place, and the memoirs of the apostles or the writings of the prophets are read, as long as time permits; then, when the reader has ceased, the president verbally instructs, and exhorts to the imitation of these good things. Then we all rise together and pray, and, as we before said, when our prayer is ended, bread and wine and water are brought, and the president in like manner offers prayers and thanksgivings, according to his ability, and the people assent, saying Amen; and there is a distribution to each, and a participation of that over which thanks have been given, and to those who are absent a portion is sent by the deacons. And they who are well to do, and willing, give what each thinks fit; and what is collected is deposited with the president, who succours the orphans and widows and those who, through sickness or any other cause, are in want, and those who are in bonds and the strangers sojourning among us, and in a word takes care of all who are in need.Thus, as Justin described, Christians assemble for communal worship on Sunday, the day of the resurrection, though other liturgical practices often occur outside this setting. Scripture readings are drawn from the Old and New Testaments, but especially the gospel accounts. Often these are arranged on an annual cycle, using a book called a lectionary. Instruction is given based on these readings, called a sermon, or homily. There are a variety of congregational prayers, including thanksgiving, confession and intercession, which occur throughout the service and take a variety of forms including recited, responsive, silent, or sung. The Lord's Prayer, or Our Father, is regularly prayed.\nSome groups depart from this traditional liturgical structure. A division is often made between \"High\" church services, characterized by greater solemnity and ritual, and \"Low\" services, but even within these two categories there is great diversity in forms of worship. Seventh-day Adventists meet on Saturday, while others do not meet on a weekly basis. Charismatic or Pentecostal congregations may spontaneously feel led by the Holy Spirit to action rather than follow a formal order of service, including spontaneous prayer. Quakers sit quietly until moved by the Holy Spirit to speak.\n\nSome evangelical services resemble concerts with rock and pop music, dancing and use of multimedia. For groups which do not recognize a priesthood distinct from ordinary believers the services are generally led by a minister, preacher, or pastor. Still others may lack any formal leaders, either in principle or by local necessity. Some churches use only a cappella music, either on principle (for example, many Churches of Christ object to the use of instruments in worship) or by tradition (as in Orthodoxy).\n\nNearly all forms of churchmanship celebrate the Eucharist (Holy Communion), which consists of a consecrated meal. It is reenacted in accordance with Jesus' instruction at the Last Supper that his followers do in remembrance of him as when he gave his disciples bread, saying, \"This is my body\", and gave them wine saying, \"This is my blood\". Some Christian denominations practice closed communion. They offer communion to those who are already united in that denomination or sometimes individual church. Catholics restrict participation to their members who are not in a state of mortal sin. Most other churches practice open communion since they view communion as a means to unity, rather than an end, and invite all believing Christians to participate.\n\nWorship can be varied for special events like baptisms or weddings in the service or significant feast days. In the early church, Christians and those yet to complete initiation would separate for the Eucharistic part of the worship. In many churches today, adults and children will separate for all or some of the service to receive age-appropriate teaching. Such children's worship is often called Sunday school or Sabbath school (Sunday schools are often held before rather than during services).\n\nIn Christian belief and practice, a \"sacrament\" is a rite, instituted by Christ, that confers grace, constituting a sacred mystery. The term is derived from the Latin word \"sacramentum\", which was used to translate the Greek word for \"mystery\". Views concerning both which rites are sacramental, and what it means for an act to be a sacrament, vary among Christian denominations and traditions.\n\nThe most conventional functional definition of a sacrament is that it is an outward sign, instituted by Christ, that conveys an inward, spiritual grace through Christ. The two most widely accepted sacraments are Baptism and the Eucharist (or Holy Communion), however, the majority of Christians also recognize five additional sacraments: Confirmation (Chrismation in the Orthodox tradition), Holy orders (ordination), Penance (or Confession), Anointing of the Sick and Matrimony (see Christian views on marriage).\n\nTaken together, these are the Seven Sacraments as recognized by churches in the High Church tradition—notably Roman Catholic, Eastern Orthodox, Oriental Orthodox, Independent Catholic, Old Catholic, many Anglicans and some Lutherans. Most other denominations and traditions typically affirm only Baptism and Eucharist as sacraments, while some Protestant groups, such as the Quakers, reject sacramental theology. Christian denominations, such as Baptists, which believe these rites do not communicate grace, prefer to call Baptism and Holy Communion \"ordinances\" rather than sacraments.\n\nIn addition to this, the Church of the East has two additional sacraments in place of the traditional sacraments of Matrimony and the Anointing of the Sick. These include Holy Leaven (Melka) and the sign of the cross.\n\nRoman Catholics, Anglicans, Eastern Christians and traditional Protestant communities frame worship around the liturgical year. The liturgical cycle divides the year into a series of seasons, each with their theological emphases, and modes of prayer, which can be signified by different ways of decorating churches, colours of paraments and vestments for clergy, scriptural readings, themes for preaching and even different traditions and practices often observed personally or in the home.\n\nWestern Christian liturgical calendars are based on the cycle of the Roman Rite of the Catholic Church, and Eastern Christians use analogous calendars based on the cycle of their respective rites. Calendars set aside holy days, such as solemnities which commemorate an event in the life of Jesus, Mary or the saints, and periods of fasting, such as Lent and other pious events such as memoria or lesser festivals commemorating saints. Christian groups that do not follow a liturgical tradition often retain certain celebrations, such as Christmas, Easter and Pentecost: these are the celebrations of Christ's birth, resurrection and the descent of the Holy Spirit upon the Church, respectively. A few denominations make no use of a liturgical calendar.\n\nChristianity has not generally practiced aniconism, or the avoidance or prohibition of types of images, even if the early Jewish Christians sects, as well as some modern denominations, preferred to some extent not to use figures in their symbols, by invoking the Decalogue's prohibition of idolatry.\n\nThe cross, which is today one of the most widely recognized symbols in the world, was used as a Christian symbol from the earliest times. Tertullian, in his book \"De Corona\", tells how it was already a tradition for Christians to trace repeatedly on their foreheads the sign of the cross. Although the cross was known to the early Christians, the crucifix did not appear in use until the 5th century.\n\nAmong the symbols employed by the primitive Christians, that of the fish or Ichthys seems to have ranked first in importance. From monumental sources such as tombs it is known that the symbolic fish was familiar to Christians from the earliest times. The fish was depicted as a Christian symbol in the first decades of the 2nd century. Its popularity among Christians was due principally, it would seem, to the famous acrostic consisting of the initial letters of five Greek words forming the word for fish (Ichthys), which words briefly but clearly described the character of Christ and the claim to worship of believers: \"Iesous Christos Theou Yios Soter\" (Ίησοῦς Χριστός, Θεοῦ Υἱός, Σωτήρ), meaning, \"Jesus Christ, Son of God, Savior\".\n\nOther major Christian symbols include the chi-rho monogram, the dove (symbolic of the Holy Spirit), the sacrificial lamb (symbolic of Christ's sacrifice), the vine (symbolizing the necessary connectedness of the Christian with Christ) and many others. These all derive from writings found in the New Testament.\n\nBaptism is the ritual act, with the use of water, by which a person is admitted to membership of the Church. Beliefs on baptism vary among denominations. Differences occur firstly on whether the act has any spiritual significance. Some, such as the Catholic and Eastern Orthodox churches, as well as Lutherans and Anglicans, hold to the doctrine of baptismal regeneration, which affirms that baptism creates or strengthens a person's faith, and is intimately linked to salvation. Others view baptism as a purely symbolic act, an external public declaration of the inward change which has taken place in the person, but not as spiritually efficacious. Secondly, there are differences of opinion on the methodology of the act. These methods are: by \"immersion\"; if immersion is total, by \"submersion\"; by affusion (pouring); and by aspersion (sprinkling). Those who hold the first view may also adhere to the tradition of infant baptism; the Orthodox Churches all practice infant baptism and always baptize by total immersion repeated three times in the name of the Father, the Son and the Holy Spirit. The Catholic Church also practices infant baptism, usually by affusion, and utilizing the Trinitarian formula.\n\nJesus' teaching on prayer in the Sermon on the Mount displays a distinct lack of interest in the external aspects of prayer. A concern with the techniques of prayer is condemned as 'pagan', and instead a simple trust in God's fatherly goodness is encouraged. Elsewhere in the New Testament this same freedom of access to God is also emphasized. This confident position should be understood in light of Christian belief in the unique relationship between the believer and Christ through the indwelling of the Holy Spirit.\n\nIn subsequent Christian traditions, certain physical gestures are emphasized, including medieval gestures such as genuflection or making the sign of the cross. Kneeling, bowing and prostrations (see also poklon) are often practiced in more traditional branches of Christianity. Frequently in Western Christianity the hands are placed palms together and forward as in the feudal commendation ceremony. At other times the older orans posture may be used, with palms up and elbows in.\n\n\"Intercessory prayer\" is prayer offered for the benefit of other people. There are many intercessory prayers recorded in the Bible, including prayers of the Apostle Peter on behalf of sick persons and by prophets of the Old Testament in favor of other people. In the Epistle of James, no distinction is made between the intercessory prayer offered by ordinary believers and the prominent Old Testament prophet Elijah. The effectiveness of prayer in Christianity derives from the power of God rather than the status of the one praying.\n\nThe ancient church, in both Eastern Christianity and Western Christianity, developed a tradition of asking for the intercession of (deceased) saints, and this remains the practice of most Eastern Orthodox, Oriental Orthodox, Roman Catholic, and some Anglican churches. Churches of the Protestant Reformation, however, rejected prayer to the saints, largely on the basis of the sole mediatorship of Christ. The reformer Huldrych Zwingli admitted that he had offered prayers to the saints until his reading of the Bible convinced him that this was idolatrous.\n\nAccording to the Catechism of the Catholic Church: \"Prayer is the raising of one's mind and heart to God or the requesting of good things from God.\" The Book of Common Prayer in the Anglican tradition is a guide which provides a set order for church services, containing set prayers, scripture readings, and hymns or sung Psalms.\n\nChristianity began as a Jewish sect in the Levant of the middle east in the mid-1st century. Other than Second Temple Judaism, the primary religious influences of early Christianity are Zoroastrianism and Gnosticism. John Bowker states that Christian ideas such as \"angels, the end of the world, a final judgment, the resurrection and heaven and hell received form and substance from ... Zoroastrian beliefs\". Its earliest development took place under the leadership of the remaining Twelve Apostles, particularly Saint Peter, and Paul the Apostle, followed by the early bishops, whom Christians consider the successors of the Apostles.\n\nAccording to the Christian scriptures, Christians were from the beginning subject to persecution by some Jewish and Roman religious authorities, who disagreed with the apostles' teachings (See Split of early Christianity and Judaism). This involved punishments, including death, for Christians such as Stephen and James, son of Zebedee. Larger-scale persecutions followed at the hands of the authorities of the Roman Empire, first in the year 64, when Emperor Nero blamed them for the Great Fire of Rome. According to Church tradition, it was under Nero's persecution that early Church leaders Peter and Paul of Tarsus were each martyred in Rome.\n\nFurther widespread persecutions of the Church occurred under nine subsequent Roman emperors, most intensely under Decius and Diocletian. From the year 150, Christian teachers began to produce theological and apologetic works aimed at defending the faith. These authors are known as the Church Fathers, and study of them is called Patristics. Notable early Fathers include Ignatius of Antioch, Polycarp, Justin Martyr, Irenaeus, Tertullian, Clement of Alexandria and Origen. However, Armenia is considered the first nation to accept Christianity in AD 301.\n\nKing Trdat IV made Christianity the state religion in Armenia between 301 and 314, it was not an entirely new religion in Armenia. It penetrated into the country from at least the third century, but may have been present even earlier.\n\nState persecution ceased in the 4th century, when Constantine I issued an edict of toleration in 313. On 27 February 380, Emperor Theodosius I enacted a law establishing Nicene Christianity as the state church of the Roman Empire. From at least the 4th century, Christianity has played a prominent role in the shaping of Western civilization.\n\nConstantine was also instrumental in the convocation of the First Council of Nicaea in 325, which sought to address the Arian heresy and formulated the Nicene Creed, which is still used by the Catholic Church, Eastern Orthodoxy, Anglican Communion and many Protestant churches. Nicaea was the first of a series of Ecumenical (worldwide) Councils which formally defined critical elements of the theology of the Church, notably concerning Christology. The Assyrian Church of the East did not accept the third and following Ecumenical Councils, and are still separate today.\n\nThe presence of Christianity in Africa began in the middle of the 1st century in Egypt, and by the end of the 2nd century in the region around Carthage. Mark the Evangelist started the Coptic Orthodox Church of Alexandria in about AD 43. Important Africans who influenced the early development of Christianity includes Tertullian, Clement of Alexandria, Origen of Alexandria, Cyprian, Athanasius and Augustine of Hippo. The later rise of Islam in North Africa reduced the size and numbers of Christian congregations, leaving only the Coptic Church in Egypt, the Ethiopian Orthodox Tewahedo Church in the Horn of Africa and the Nubian Church in the Sudan (Nobatia, Makuria and Alodia).\n\nIn terms of prosperity and cultural life, the Byzantine Empire was one of the peaks in Christian history and Orthodox civilization, and Constantinople remained the leading city of the Christian world in size, wealth and culture. There was a renewed interest in classical Greek philosophy, as well as an increase in literary output in vernacular Greek. Byzantine art and literature held a pre-eminent place in Europe, and the cultural impact of Byzantine art on the west during this period was enormous and of long lasting significance.\n\nWith the decline and fall of the Roman Empire in the west, the papacy became a political player, first visible in Pope Leo's diplomatic dealings with Huns and Vandals. The church also entered into a long period of missionary activity and expansion among the various tribes. While Arianists instituted the death penalty for practicing pagans (see Massacre of Verden as example), Catholicism also spread among the Germanic peoples, the Celtic and Slavic peoples, the Hungarians and the Baltic peoples. Christianity has been an important part of the shaping of Western civilization, at least since the 4th century.\n\nAround 500, St. Benedict set out his Monastic Rule, establishing a system of regulations for the foundation and running of monasteries. Monasticism became a powerful force throughout Europe, and gave rise to many early centers of learning, most famously in Ireland, Scotland and Gaul, contributing to the Carolingian Renaissance of the 9th century.\n\nIn the 7th century Muslims conquered Syria (including Jerusalem), North Africa and Spain. Part of the Muslims' success was due to the exhaustion of the Byzantine empire in its decades long conflict with Persia. Beginning in the 8th century, with the rise of Carolingian leaders, the papacy began to find greater political support in the Frankish Kingdom.\n\nThe Middle Ages brought about major changes within the church. Pope Gregory the Great dramatically reformed ecclesiastical structure and administration. In the early 8th century, iconoclasm became a divisive issue, when it was sponsored by the Byzantine emperors. The Second Ecumenical Council of Nicaea (787) finally pronounced in favor of icons. In the early 10th century, Western Christian monasticism was further rejuvenated through the leadership of the great Benedictine monastery of Cluny.\n\nHebraism, like Hellenism, has been an all-important factor in the development of Western Civilization; Judaism, as the precursor of Christianity, has indirectly had had much to do with shaping the ideals and morality of western nations since the Christian era.\n\nIn the west, from the 11th century onward, older cathedral schools developed into universities (see University of Oxford, University of Paris and University of Bologna.) The traditional medieval universities—evolved from Catholic and Protestant church schools—then established specialized academic structures for properly educating greater numbers of students as professionals. Prof. Walter Rüegg, editor of \"A History of the University in Europe\", reports that universities then only trained students to become clerics, lawyers, civil servants and physicians.\n\nOriginally teaching only theology, universities steadily added subjects including medicine, philosophy and law, becoming the direct ancestors of modern institutions of learning.\nThe university is generally regarded as an institution that has its origin in the Medieval Christian setting. Prior to the establishment of universities, European higher education took place for hundreds of years in Christian cathedral schools or monastic schools (\"Scholae monasticae\"), in which monks and nuns taught classes; evidence of these immediate forerunners of the later university at many places dates back to the 6th century AD.\n\nAccompanying the rise of the \"new towns\" throughout Europe, mendicant orders were founded, bringing the consecrated religious life out of the monastery and into the new urban setting. The two principal mendicant movements were the Franciscans and the Dominicans founded by St. Francis and St. Dominic respectively. Both orders made significant contributions to the development of the great universities of Europe. Another new order were the Cistercians, whose large isolated monasteries spearheaded the settlement of former wilderness areas. In this period church building and ecclesiastical architecture reached new heights, culminating in the orders of Romanesque and Gothic architecture and the building of the great European cathedrals.\n\nFrom 1095 under the pontificate of Urban II, the Crusades were launched. These were a series of military campaigns in the Holy Land and elsewhere, initiated in response to pleas from the Byzantine Emperor Alexios I for aid against Turkish expansion. The Crusades ultimately failed to stifle Islamic aggression and even contributed to Christian enmity with the sacking of Constantinople during the Fourth Crusade.\n\nOver a period stretching from the 7th to the 13th century, the Christian Church underwent gradual alienation, resulting in a schism dividing it into a so-called Latin or Western Christian branch, the Roman Catholic Church, and an Eastern, largely Greek, branch, the Orthodox Church. These two churches disagree on a number of administrative, liturgical and doctrinal issues, most notably papal primacy of jurisdiction. The Second Council of Lyon (1274) and the Council of Florence (1439) attempted to reunite the churches, but in both cases the Eastern Orthodox refused to implement the decisions and the two principal churches remain in schism to the present day. However, the Roman Catholic Church has achieved union with various smaller eastern churches.\n\nBeginning around 1184, following the crusade against the Cathar heresy, various institutions, broadly referred to as the Inquisition, were established with the aim of suppressing heresy and securing religious and doctrinal unity within Christianity through conversion and prosecution.\n\nThe 15th-century Renaissance brought about a renewed interest in ancient and classical learning. Another major schism, the Reformation, resulted in the splintering of the Western Christendom into several branches. Martin Luther in 1517 protested against the sale of indulgences and soon moved on to deny several key points of Roman Catholic doctrine.\n\nOther reformers like Zwingli, Calvin, Knox and Arminius further criticized Roman Catholic teaching and worship. These challenges developed into the movement called Protestantism, which repudiated the primacy of the pope, the role of tradition, the seven sacraments and other doctrines and practices. The Reformation in England began in 1534, when King Henry VIII had himself declared head of the Church of England. Beginning in 1536, the monasteries throughout England, Wales and Ireland were dissolved.\n\nThomas Müntzer, Andreas Karlstadt and other theologians perceived both the Roman Catholic Church and the confessions of the Magisterial Reformation as corrupted. Their activity brought about the Radical Reformation, which gave birth to various Anabaptist denominations.\nPartly in response to the Protestant Reformation, the Roman Catholic Church engaged in a substantial process of reform and renewal, known as the Counter-Reformation or Catholic Reform. The Council of Trent clarified and reasserted Roman Catholic doctrine. During the following centuries, competition between Roman Catholicism and Protestantism became deeply entangled with political struggles among European states.\n\nMeanwhile, the discovery of America by Christopher Columbus in 1492 brought about a new wave of missionary activity. Partly from missionary zeal, but under the impetus of colonial expansion by the European powers, Christianity spread to the Americas, Oceania, East Asia and sub-Saharan Africa.\n\nThroughout Europe, the divides caused by the Reformation led to outbreaks of religious violence and the establishment of separate state churches in Europe. Lutheranism spread into northern, central and eastern parts of present-day Germany, Livonia and Scandinavia. Anglicanism was established in England in 1534. Calvinism and its varieties (such as Presbyterianism) were introduced in Scotland, the Netherlands, Hungary, Switzerland and France. Arminianism gained followers in the Netherlands and Frisia. Ultimately, these differences led to the outbreak of conflicts in which religion played a key factor. The Thirty Years' War, the English Civil War and the French Wars of Religion are prominent examples. These events intensified the Christian debate on persecution and toleration.\n\nIn the era known as the Great Divergence, when in the West the Age of Enlightenment and the Scientific revolution brought about great societal changes, Christianity was confronted with various forms of skepticism and with certain modern political ideologies such as versions of socialism and liberalism. Events ranged from mere anti-clericalism to violent outbursts against Christianity such as the Dechristianisation during the French Revolution, the Spanish Civil War and certain Marxist movements, especially the Russian Revolution and the persecution of Christians in the Soviet Union under state atheism.\n\nEspecially pressing in Europe was the formation of nation states after the Napoleonic era. In all European countries, different Christian denominations found themselves in competition, to greater or lesser extents, with each other and with the state. Variables are the relative sizes of the denominations and the religious, political and ideological orientation of the state. Urs Altermatt of the University of Fribourg, looking specifically at Catholicisms in Europe, identifies four models for the European nations. In traditionally Catholic countries such as Belgium, Spain and to some extent Austria, religious and national communities are more or less identical. Cultural symbiosis and separation are found in Poland, Ireland and Switzerland, all countries with competing denominations. Competition is found in Germany, the Netherlands and again Switzerland, all countries with minority Catholic populations who to a greater or lesser extent did identify with the nation. Finally, separation between religion (again, specifically Catholicism) and the state is found to a great degree in France and Italy, countries where the state actively opposed itself to the authority of the Catholic Church.\n\nThe combined factors of the formation of nation states and ultramontanism, especially in Germany and the Netherlands but also in England (to a much lesser extent), often forced Catholic churches, organizations and believers to choose between the national demands of the state and the authority of the Church, specifically the papacy. This conflict came to a head in the First Vatican Council, and in Germany would lead directly to the Kulturkampf, where liberals and Protestants under the leadership of Bismarck managed to severely restrict Catholic expression and organization.\n\nChristian commitment in Europe dropped as modernity and secularism came into their own in Europe, particularly in the Czech Republic and Estonia, while religious commitments in America have been generally high in comparison to Europe. The late 20th century has shown the shift of Christian adherence to the Third World and southern hemisphere in general, with the western civilization no longer the chief standard bearer of Christianity.\n\nSome Europeans (including diaspora), Indigenous peoples of the Americas and natives of other continents have revived their respective peoples' historical folk religions. Approximately 7.1 to 10% of Arabs are Christians, most prevalent in Egypt, Syria and Lebanon.\n\nWith around 2.4 billion adherents, split into three main branches of Catholic, Protestant and Eastern Orthodox, Christianity is the world's largest religion. The Christian share of the world's population has stood at around 33% for the last hundred years, which says that one in three persons on earth are Christians. This masks a major shift in the demographics of Christianity; large increases in the developing world have been accompanied by substantial declines in the developed world, mainly in Europe and North America. According to a 2015 Pew Research Center study, within the next four decades, Christians will remain the world's largest religion; and by 2050, the Christian population is expected to exceed 3 billion.\n\nAs a percentage of Christians, the Catholic Church and Orthodoxy (both Eastern and Oriental) are declining, while Protestants and other Christians are on the rise. The so-called \"popular Protestantism\" is one of the fastest growing religious categories in the world.\n\nChristianity is the predominant religion in Europe, the Americas and Southern Africa. In Asia, it is the dominant religion in Georgia, Armenia, East Timor and the Philippines. However, it is declining in many areas including the Northern and Western United States, Oceania (Australia and New Zealand), northern Europe (including Great Britain, Scandinavia and other places), France, Germany, the Canadian provinces of Ontario, British Columbia and Quebec, and parts of Asia (especially the Middle East – due to the Christian emigration, South Korea, Taiwan, and Macau).\n\nThe Christian population is not decreasing in Brazil, the Southern United States and the province of Alberta, Canada, but the percentage is decreasing. In countries such as Australia and New Zealand, the Christian population are declining in both numbers and percentage.\n\nDespite the declining numbers, Christianity remains the dominant religion in the Western World, where 70% are Christians. A 2011 Pew Research Center survey found that 76.2% of Europeans, 73.3% in Oceania and about 86.0% in the Americas (90.0% in Latin America and 77.4% in North America) identified themselves as Christians. By 2010 about 157 countries and territories in the world had Christian majorities.\n\nHowever, there are many charismatic movements that have become well established over large parts of the world, especially Africa, Latin America and Asia. Since 1900, primarily due to conversion, Protestantism has spread rapidly in Africa, Asia, Oceania and Latin America. From 1960 to 2000, the global growth of the number of reported Evangelical Protestants grew three times the world's population rate, and twice that of Islam. St. Mary's University study estimated about 10.2 million Muslim convert to Christianity in 2015. as well a significant numbers of Muslims converts to Christianity in Afghanistan, Albania, Azerbaijan Algeria, Belgium, France, Germany, Iran, India, Indonesia, Malaysia, Morocco, Russia, Netherlands, Saudi Arabia, Tunisia, Turkey, Kazakhstan, Kyrgyzstan, Kosovo, United States, and Central Asia. It is also reported that Christianity is popular among people of different backgrounds in India (mostly Hindus), and Malaysia, Mongolia, Nigeria, Vietnam, Singapore, Indonesia, China, Japan, and South Korea.\n\nIn most countries in the developed world, church attendance among people who continue to identify themselves as Christians has been falling over the last few decades. Some sources view this simply as part of a drift away from traditional membership institutions, while others link it to signs of a decline in belief in the importance of religion in general. Europe's Christian population, though in decline, still constitutes the largest geographical component of the religion. According to data from the 2012 European Social Survey, around a third of European Christians say they attend services once a month or more, Conversely about more than two-thirds of Latin American Christians and according to the World Values Survey about 90% of African Christians (in Ghana, Nigeria, Rwanda, South Africa and Zimbabwe) said they attended church regularly.\n\nChristianity, in one form or another, is the sole state religion of the following nations: Argentina (Roman Catholic), Tuvalu (Reformed), Tonga (Methodist), Norway (Lutheran), Costa Rica (Roman Catholic), Kingdom of Denmark (Lutheran), England (Anglican), Georgia (Georgian Orthodox), Greece (Greek Orthodox), Iceland (Lutheran), Liechtenstein (Roman Catholic), Malta (Roman Catholic), Monaco (Roman Catholic), and Vatican City (Roman Catholic).\n\nThere are numerous other countries, such as Cyprus, which although do not have an established church, still give official recognition and support to a specific Christian denomination.\n\nThe three primary divisions of Christianity are Roman Catholicism, Eastern Orthodoxy and Protestantism. However, there are other Christian groups that do not fit neatly into one of these primary categories. The Nicene Creed is accepted as authoritative by most Christian denominations, including the Roman Catholic, Eastern Orthodox, Anglican and major Protestant churches.\n\nThere is a diversity of doctrines and practices among groups calling themselves Christian. These groups are sometimes classified under denominations, though for theological reasons many groups reject this classification system. A broader distinction that is sometimes drawn is between Eastern Christianity and Western Christianity, which has its origins in the East–West Schism (Great Schism) of the 11th century.\n\nIn addition to the Lutheran and Reformed (or Calvinist) branches of the Reformation, there is Anglicanism after the English Reformation. The Anabaptist tradition was largely ostracized by the other Protestant parties at the time, but has achieved a measure of affirmation in more recent history. Adventist, Baptist, Methodist, Pentecostal and other Protestant confessions arose in the following centuries.\n\nAs well as these modern divisions, there were many diverse Christian communities with wildly different Christologies, eschatologies, soteriologies and cosmologies that existed alongside the \"Early Church\" which is itself a projected concept to indicate which communities were \"proto-orthodox\", in that their views would become dominate. In many ways, the first three centuries of Christianity was significantly more diverse than the modern Church.\n\nThe Catholic Church consists of those particular Churches, headed by bishops, in communion with the Pope, the Bishop of Rome, as its highest authority in matters of faith, morality and Church governance. Like Eastern Orthodoxy, the Roman Catholic Church, through apostolic succession, traces its origins to the Christian community founded by Jesus Christ. Catholics maintain that the \"one, holy, catholic and apostolic church\" founded by Jesus subsists fully in the Roman Catholic Church, but also acknowledges other Christian churches and communities and works towards reconciliation among all Christians. The Catholic faith is detailed in the \"Catechism of the Catholic Church\".\n\nThe 2,834 sees are grouped into 24 particular autonomous Churches (the largest of which being the Latin Church), each with its own distinct traditions regarding the liturgy and the administering the sacraments. With more than 1.1 billion baptized members, the Catholic Church is the largest Christian church and represents over half of all Christians as well as one sixth of the world's population.\n\nVarious smaller communities, such as the Old Catholic and Independent Catholic Churches, include the word \"Catholic\" in their title, and share much in common with Roman Catholicism, but are no longer in communion with the Holy See.\n\nThe Eastern Orthodox Church consists of those churches in communion with the Patriarchal Sees of the East, such as the Ecumenical Patriarch of Constantinople. Like the Roman Catholic Church, the Eastern Orthodox Church also traces its heritage to the foundation of Christianity through apostolic succession and has an episcopal structure, though the autonomy of its component parts is emphasized, and most of them are national churches. A number of conflicts with Western Christianity over questions of doctrine and authority culminated in the Great Schism. Eastern Orthodoxy is the second largest single denomination in Christianity, with an estimated 225–300 million adherents.\n\nThe Oriental Orthodox churches (also called \"Old Oriental\" churches) are those eastern churches that recognize the first three ecumenical councils—Nicaea, Constantinople and Ephesus—but reject the dogmatic definitions of the Council of Chalcedon and instead espouse a Miaphysite christology. The Oriental Orthodox communion consists of six groups: Syriac Orthodox, Coptic Orthodox, Ethiopian Orthodox, Eritrean Orthodox, Malankara Orthodox Syrian Church (India) and Armenian Apostolic churches. These six churches, while being in communion with each other are completely independent hierarchically. These churches are generally not in communion with Eastern Orthodox Churches with whom they are in dialogue for erecting a communion.\n\nIn the 16th century, Martin Luther, and subsequently Huldrych Zwingli and John Calvin, inaugurated what has come to be called Protestantism. Luther's primary theological heirs are known as Lutherans. Zwingli and Calvin's heirs are far broader denominationally, and are broadly referred to as the Reformed tradition. The oldest Protestant groups separated from the Catholic Church in the Protestant Reformation, often followed by further divisions.\n\nIn the 18th century, for example, Methodism grew out of Anglican minister John Wesley's evangelical and revival movement. Several Pentecostal and non-denominational churches, which emphasize the cleansing power of the Holy Spirit, in turn grew out of Methodism. Because Methodists, Pentecostals and other evangelicals stress \"accepting Jesus as your personal Lord and Savior\", which comes from Wesley's emphasis of the New Birth, they often refer to themselves as being born-again.\n\nEstimates of the total number of Protestants are very uncertain, but it seems clear that Protestantism is the second largest major group of Christians after Roman Catholicism in number of followers (although the Eastern Orthodox Church is larger than any single Protestant denomination). Often that number is put at more than 800 million, corresponding to nearly 40% of world's Christians. The majority of Protestants are members of just a handful of denominational families, i.e. Adventists, Anglicans, Baptists, Reformed (Calvinists), Lutherans, Methodists and Pentecostals. Nondenominational, evangelical, charismatic, neo-charismatic, independent and other churches are on the rise, and constitute a significant part of Protestant Christianity.\n\nA special grouping are the Anglican churches descended from the Church of England and organized in the Anglican Communion. Some Anglican churches consider themselves both Protestant and Catholic. Some Anglicans consider their church a branch of the \"One Holy Catholic Church\" alongside of the Roman Catholic and Eastern Orthodox churches, a concept rejected by the Roman Catholic Church and some Eastern Orthodox.\n\nWhile Anglicans, Lutherans and the Reformed branches of Protestantism originated in the Magisterial Reformation, other Protestant groups such as the Anabaptists (mostly made-up of Amish, Mennonites, Hutterites and Schwarzenau Brethren/German Baptist groups), originated in the Radical Reformation and are distinguished by their belief in credobaptism.\n\nSome groups of individuals who hold basic Protestant tenets identify themselves simply as \"Christians\" or \"born-again Christians\". They typically distance themselves from the confessionalism and/or creedalism of other Christian communities by calling themselves \"non-denominational\" or \"evangelical\". Often founded by individual pastors, they have little affiliation with historic denominations.<ref name=\"Pew Forum on Religion & Public Life / U.S. Religious Landscape Survey\"></ref>\n\nThe Second Great Awakening, a period of religious revival that occurred in the United States during the early 1800s, saw the development of a number of unrelated churches. They generally saw themselves as restoring the original church of Jesus Christ rather than reforming one of the existing churches. A common belief held by Restorationists was that the other divisions of Christianity had introduced doctrinal defects into Christianity, which was known as the Great Apostasy. In Asia, Iglesia ni Cristo is a known restorationist religion that was established during the early 1900s.\n\nSome of the churches originating during this period are historically connected to early 19th-century camp meetings in the Midwest and Upstate New York. American Millennialism and Adventism, which arose from Evangelical Protestantism, influenced the Jehovah's Witnesses movement and, as a reaction specifically to William Miller, the Seventh-day Adventists. Others, including the Christian Church (Disciples of Christ), Evangelical Christian Church in Canada, Churches of Christ, and the Christian churches and churches of Christ, have their roots in the contemporaneous Stone-Campbell Restoration Movement, which was centered in Kentucky and Tennessee. Other groups originating in this time period include the Christadelphians and Latter Day Saint movement. While the churches originating in the Second Great Awakening have some superficial similarities, their doctrine and practices vary significantly.\n\nEsoteric Christians regard Christianity as a mystery religion, and profess the existence and possession of certain esoteric doctrines or practices, hidden from the public but accessible only to a narrow circle of \"enlightened\", \"initiated\", or highly educated people. Some of the esoteric Christian institutions include the Rosicrucian Fellowship, the Anthroposophical Society and the Martinism.\n\nMessianic Judaism (or Messianic Movement) is the name of a Christian movement comprising a number of streams, whose members may consider themselves Jewish. The movement originated in the 1960s and 1970s, and it blends elements of religious Jewish practice with evangelical Christianity. Messianic Judaism affirms Christian creeds such as the messiahship and divinity of \"Yeshua\" (the Hebrew name of Jesus) and the Triune Nature of God, while also adhering to some Jewish dietary laws and customs.\n\nWestern culture, throughout most of its history, has been nearly equivalent to Christian culture, and a large portion of the population of the Western hemisphere can be described as cultural Christians. The notion of \"Europe\" and the \"Western World\" has been intimately connected with the concept of \"Christianity and Christendom\" many even attribute Christianity for being the link that created a unified European identity.\n\nThough Western culture contained several polytheistic religions during its early years under the Greek and Roman empires, as the centralized Roman power waned, the dominance of the Catholic Church was the only consistent force in Europe. Until the Age of Enlightenment, Christian culture guided the course of philosophy, literature, art, music and science. Christian disciplines of the respective arts have subsequently developed into Christian philosophy, Christian art, Christian music, Christian literature etc.\n\nChristianity has had a significant impact on education as the church created the bases of the Western system of education, and was the sponsor of founding universities in the Western world; as the university is generally regarded as an institution that has its origin in the Medieval Christian setting. Historically, Christianity has often been a patron of science and medicine. It has been prolific in the foundation of schools, universities and hospitals, and many Catholic clergy; Jesuits in particular, have been active in the sciences throughout history and have made significant contributions to the development of science. Protestantism also has had an important influence on science. According to the Merton Thesis, there was a positive correlation between the rise of English Puritanism and German Pietism on the one hand and early experimental science on the other. The Civilizing influence of Christianity includes social welfare, founding hospitals, economics (as the Protestant work ethic), politics, architecture, literature, personal hygiene, and family life.\n\nEastern Christians (particularly Nestorian Christians) contributed to the Arab Islamic Civilization during the reign of the Ummayad and the Abbasid by translating works of Greek philosophers to Syriac and afterwards to Arabic. They also excelled in philosophy, science, theology and medicine. And many scholars of the House of Wisdom were of Christian background.\n\nChristians have made a myriad of contributions to human progress in a broad and diverse range of fields, including philosophy, science and technology, fine arts and architecture, politics, literatures, music, and business. According to \"100 Years of Nobel Prizes\" a review of Nobel prizes award between 1901 and 2000 reveals that (65.4%) of Nobel Prizes Laureates, have identified Christianity in its various forms as their religious preference.\n\n\"Postchristianity\" is the term for the decline of Christianity, particularly in Europe, Canada, Australia and to a minor degree the Southern Cone, in the 20th and 21st centuries, considered in terms of postmodernism. It refers to the loss of Christianity's monopoly on values and world view in historically Christian societies.\n\nCultural Christians are secular people with a Christian heritage who may not believe in the religious claims of Christianity, but who retain an affinity for the popular culture, art, music and so on related to it. Another frequent application of the term is to distinguish political groups in areas of mixed religious backgrounds.\n\nChristian groups and denominations have long expressed ideals of being reconciled, and in the 20th century, Christian ecumenism advanced in two ways. One way was greater cooperation between groups, such as the World Evangelical Alliance founded in 1846 in London or the Edinburgh Missionary Conference of Protestants in 1910, the Justice, Peace and Creation Commission of the World Council of Churches founded in 1948 by Protestant and Orthodox churches, and similar national councils like the National Council of Churches in Australia which includes Roman Catholics.\n\nThe other way was institutional union with United and uniting churches, a practice that can be traced back to unions between Lutherans and Calvinists in early 19th-century Germany. Congregationalist, Methodist and Presbyterian churches united in 1925 to form the United Church of Canada, and in 1977 to form the Uniting Church in Australia. The Church of South India was formed in 1947 by the union of Anglican, Baptist, Methodist, Congregationalist and Presbyterian churches.\n\nThe ecumenical, monastic Taizé Community is notable for being composed of more than one hundred brothers from Protestant and Catholic traditions. The community emphasizes the reconciliation of all denominations and its main church, located in Taizé, Saône-et-Loire, France, is named the \"Church of Reconciliation\". The community is internationally known, attracting over 100,000 young pilgrims annually.\nSteps towards reconciliation on a global level were taken in 1965 by the Roman Catholic and Orthodox churches mutually revoking the excommunications that marked their Great Schism in 1054; the Anglican Roman Catholic International Commission (ARCIC) working towards full communion between those churches since 1970; and some Lutheran and Roman Catholic churches signing the Joint Declaration on the Doctrine of Justification in 1999 to address conflicts at the root of the Protestant Reformation. In 2006, the World Methodist Council, representing all Methodist denominations, adopted the declaration.\n\nCriticism of Christianity and Christians goes back to the Apostolic Age, with the New Testament recording friction between the followers of Jesus and the Pharisees and scribes (e.g. and ). In the 2nd century, Christianity was criticized by the Jews on various grounds, e.g. that the prophecies of the Hebrew Bible could not have been fulfilled by Jesus, given that he did not have a successful life. Additionally a sacrifice to remove sins in advance, for everyone or as a human being, did not fit to the Jewish sacrifice ritual, furthermore God is said to judge people on their deeds instead of their beliefs. One of the first comprehensive attacks on Christianity came from the Greek philosopher Celsus, who wrote \"The True Word\", a polemic criticizing Christians as being unprofitable members of society.\n\nBy the 3rd century, criticism of Christianity had mounted, partly as a defense against it. Wild rumors about Christians were widely circulated, claiming that they were atheists and that, as part of their rituals, they devoured human infants and engaged in incestuous orgies. The Neoplatonist philosopher Porphyry wrote the fifteen-volume \"Adversus Christianos\" as a comprehensive attack on Christianity, in part building on the pre-Christian concepts of Plotinus.\n\nBy the 12th century, the Mishneh Torah (i.e., Rabbi Moses Maimonides) was criticizing Christianity on the grounds of idol worship, in that Christians attributed divinity to Jesus who had a physical body. In the 19th century, Nietzsche began to write a series of polemics on the \"unnatural\" teachings of Christianity (e.g. sexual abstinence), and continued his criticism of Christianity to the end of his life. In the 20th century, the philosopher Bertrand Russell expressed his criticism of Christianity in \"Why I Am Not a Christian\", formulating his rejection of Christianity in the setting of logical arguments.\n\nCriticism of Christianity continues to date, e.g. Jewish and Muslim theologians criticize the doctrine of the Trinity held by most Christians, stating that this doctrine in effect assumes that there are three Gods, running against the basic tenet of monotheism. New Testament scholar Robert M. Price has outlined the possibility that some Bible stories are based partly on myth in \"The Christ Myth Theory and its problems\".\n\nChristian apologetics aims to present a rational basis for Christianity. The word \"apologetic\" comes from the Greek word \"apologeomai\", meaning \"in defense of\". Christian apologetics has taken many forms over the centuries, starting with Paul the Apostle. The philosopher Thomas Aquinas presented five arguments for God's existence in the \"Summa Theologica\", while his \"Summa contra Gentiles\" was a major apologetic work. Another famous apologist, G. K. Chesterton, wrote in the early twentieth century about the benefits of religion and, specifically, Christianity. Famous for his use of paradox, Chesterton explained that while Christianity had the most mysteries, it was the most practical religion. He pointed to the advance of Christian civilizations as proof of its practicality. The physicist and priest John Polkinghorne, in his \"Questions of Truth\" discusses the subject of religion and science, a topic that other Christian apologists such as Ravi Zacharias, John Lennox and William Lane Craig have engaged, with the latter two men opining that the inflationary Big Bang model is evidence for the existence of God.\n\n\n\n"}
{"id": "5213", "url": "https://en.wikipedia.org/wiki?curid=5213", "title": "Computing", "text": "Computing\n\nComputing is any goal-oriented activity requiring, benefiting from, or creating a mathematical sequence of steps known as an algorithm — e.g. through computers. Computing includes designing, developing and building hardware and software systems; processing, structuring, and managing various kinds of information; doing scientific research on and with computers; making computer systems behave intelligently; and creating and using communications and entertainment media. The field of computing includes computer engineering, software engineering, computer science, information systems, and information technology.\n\nThe ACM \"Computing Curricula 2005\" defined \"computing\" as follows:\n\n\"In a general way, we can define computing to mean any goal-oriented activity requiring, benefiting from, or creating computers. Thus, computing includes designing and building hardware and software systems for a wide range of purposes; processing, structuring, and managing various kinds of information; doing scientific studies using computers; making computer systems behave intelligently; creating and using communications and entertainment media; finding and gathering information relevant to any particular purpose, and so on. The list is virtually endless, and the possibilities are vast.\"\n\nand it defines five sub-disciplines of the \"computing\" field: Computer Science, Computer Engineering, Information Systems, Information Technology, and Software Engineering.\n\nHowever, \"Computing Curricula 2005\" also recognizes that the meaning of \"computing\" depends on the context:\n\n\"Computing also has other meanings that are more specific, based on the context in which the term is used. For example, an information systems specialist will view computing somewhat differently from a software engineer. Regardless of the context, doing computing well can be complicated and difficult. Because society needs people to do computing well, we must think of computing not only as a profession but also as a discipline.\"\n\nThe term \"computing\" has sometimes been narrowly defined, as in a 1989 ACM report on \"Computing as a Discipline\":\n\n\"The discipline of computing is the systematic study of algorithmic\nprocesses that describe and transform information: their theory, analysis, design, efficiency, implementation, and application. The fundamental question underlying all computing is \"What can be (efficiently) automated?\"\n\nThe term \"computing\" is also synonymous with counting and calculating. In earlier times, it was used in reference to the action performed by mechanical computing machines, and before that, to human computers.\n\nThe history of computing is longer than the history of computing hardware and modern computing technology and includes the history of methods intended for pen and paper or for chalk and slate, with or without the aid of tables.\n\nComputing is intimately tied to the representation of numbers. But long before abstractions like \"the number\" arose, there were mathematical concepts to serve the purposes of civilization. These concepts include one-to-one correspondence (the basis of counting), comparison to a standard (used for measurement), and the \"3-4-5\" right triangle (a device for assuring a \"right angle\").\n\nThe earliest known tool for use in computation was the abacus, and it was thought to have been invented in Babylon circa 2400 BC. Its original style of usage was by lines drawn in sand with pebbles. Abaci, of a more modern design, are still used as calculation tools today. This was the first known computer and most advanced system of calculation known to date - preceding Greek methods by 2,000 years.\n\nThe first recorded idea of using digital electronics for computing was the 1931 paper \"The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena\" by C. E. Wynn-Williams. Claude Shannon's 1938 paper \"A Symbolic Analysis of Relay and Switching Circuits\" then introduced the idea of using electronics for Boolean algebraic operations.\n\nA computer is a machine that manipulates data according to a set of instructions called a computer program. The program has an executable form that the computer can use directly to execute the instructions. The same program in its human-readable source code form, enables a programmer to study and develop a sequence of steps known as an algorithm. Because the instructions can be carried out in different types of computers, a single set of source instructions converts to machine instructions according to the central processing unit type.\n\nThe execution process carries out the instructions in a computer program. Instructions express the computations performed by the computer. They trigger sequences of simple actions on the executing machine. Those actions produce effects according to the semantics of the instructions.\n\nComputer software or just \"software\", is a collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it. Software refers to one or more computer programs and data held in the storage of the computer for some purposes. In other words, software is a set of \"programs, procedures, algorithms\" and its \"documentation\" concerned with the operation of a data processing system. Program software performs the function of the program it implements, either by directly providing instructions to the computer hardware or by serving as input to another piece of software. The term was coined to contrast with the old term hardware (meaning physical devices). In contrast to hardware, software is intangible. Software is also sometimes used in a more narrow sense, meaning application software only.\n\nApplication software, also known as an \"application\" or an \"app\", is a computer software designed to help the user to perform specific tasks. Examples include enterprise software, accounting software, office suites, graphics software and media players. Many application programs deal principally with documents. Apps may be bundled with the computer and its system software, or may be published separately. Some users are satisfied with the bundled apps and need never install one.\n\nApplication software is contrasted with system software and middleware, which manage and integrate a computer's capabilities, but typically do not directly apply them in the performance of tasks that benefit the user. The system software serves the application, which in turn serves the user.\n\nApplication software applies the power of a particular computing platform or system software to a particular purpose. Some apps such as Microsoft Office are available in versions for several different platforms; others have narrower requirements and are thus called, for example, a Geography application for Windows or an Android application for education or Linux gaming. Sometimes a new and popular application arises that only runs on one platform, increasing the desirability of that platform. This is called a killer application.\n\nSystem software, or systems software, is computer software designed to operate and control the computer hardware and to provide a platform for running application software. System software includes operating systems, utility software, device drivers, window systems, and firmware. Frequently development tools such as compilers, linkers, and debuggers are classified as system software.\n\nA computer network, often simply referred to as a network, is a collection of hardware components and computers interconnected by communication channels that allow sharing of resources and information. Where at least one process in one device is able to send/receive data to/from at least one process residing in a remote device, then the two devices are said to be in a network.\n\nNetworks may be classified according to a wide variety of characteristics such as the medium used to transport the data, communications protocol used, scale, topology, and organizational scope.\n\nCommunications protocols define the rules and data formats for exchanging information in a computer network, and provide the basis for network programming. Well-known communications protocols are Ethernet, a hardware and Link Layer standard that is ubiquitous in local area networks, and the Internet Protocol Suite, which defines a set of protocols for internetworking, i.e. for data communication between multiple networks, as well as host-to-host data transfer, and application-specific data transmission formats.\n\nComputer networking is sometimes considered a sub-discipline of electrical engineering, telecommunications, computer science, information technology or computer engineering, since it relies upon the theoretical and practical application of these disciplines.\n\nThe Internet is a global system of interconnected computer networks that use the standard Internet protocol suite (TCP/IP) to serve billions of users that consists of millions of private, public, academic, business, and government networks, of local to global scope, that are linked by a broad array of electronic, wireless and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents of the World Wide Web (WWW) and the infrastructure to support email.\n\nComputer programming in general is the process of writing, testing, debugging, and maintaining the source code and documentation of computer programs. This source code is written in a programming language, which is an artificial language often more restrictive or demanding than natural languages, but easily translated by the computer. The purpose of programming is to invoke the desired behavior (customization) from the machine. The process of writing high quality source code requires knowledge of both the application's domain \"and\" the computer science domain. The highest-quality software is thus developed by a team of various domain experts, each person a specialist in some area of development. But the term \"programmer\" may apply to a range of program quality, from hacker to open source contributor to professional. And a single programmer could do most or all of the computer programming needed to generate the proof of concept to launch a new \"killer\" application.\n\nA programmer, computer programmer, or coder is a person who writes computer software. The term \"computer programmer\" can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. A programmer's primary computer language (C, C++, Java, Lisp, Python, etc.) is often prefixed to the above titles, and those who work in a web environment often prefix their titles with \"web\". The term \"programmer\" can be used to refer to a software developer, software engineer, computer scientist, or software analyst. However, members of these professions typically possess other software engineering skills, beyond programming.\n\nThe computer industry is made up of all of the businesses involved in developing computer software, designing computer hardware and computer networking infrastructures, the manufacture of computer components and the provision of information technology services including system administration and maintenance.\n\nThe software industry includes businesses engaged in development, maintenance and publication of software. The industry also includes software services, such as training, documentation, and consulting.\n\nComputer engineering is a discipline that integrates several fields of electrical engineering and computer science required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work, but also how they integrate into the larger picture.\n\nSoftware engineering (SE) is the application of a systematic, disciplined, quantifiable approach to the design, development, operation, and maintenance of software, and the study of these approaches; that is, the application of engineering to software. In layman's terms, it is the act of using insights to conceive, model and scale a solution to a problem. The first reference to the term is the 1968 NATO Software Engineering Conference and was meant to provoke thought regarding the perceived \"software crisis\" at the time. \"Software development\", a much used and more generic term, does not necessarily subsume the engineering paradigm. The generally accepted concepts of Software Engineering as an engineering discipline have been specified in the Guide to the Software Engineering Body of Knowledge (SWEBOK). The SWEBOK has become an internationally accepted standard ISO/IEC TR 19759:2005.\n\nComputer science or computing science (abbreviated CS or Comp Sci) is the scientific and practical approach to computation and its applications. A computer scientist specializes in the theory of computation and the design of computational systems.\n\nIts subfields can be divided into practical techniques for its implementation and application in computer systems and purely theoretical areas. Some, such as computational complexity theory, which studies fundamental properties of computational problems, are highly abstract, while others, such as computer graphics, emphasize real-world applications. Still others focus on the challenges in implementing computations. For example, programming language theory studies approaches to description of computations, while the study of computer programming itself investigates various aspects of the use of programming languages and complex systems, and human–computer interaction focuses on the challenges in making computers and computations useful, usable, and universally accessible to humans.\n\n\"Information systems (IS)\" is the study of complementary networks of hardware and software (see information technology) that people and organizations use to collect, filter, process, create, and distribute data. Computing Careers says on their website that \"A majority of IS programs are located in business schools; however, they may have different names such as management information systems, computer information systems, or business information systems. All IS degrees combine business and computing topics, but the emphasis between technical and organizational issues varies among programs. For example, programs differ substantially in the amount of programming required.\"\nThe study bridges business and computer science using the theoretical foundations of information and computation to study various business models and related algorithmic processes within a computer science discipline. Computer Information System(s) (CIS) is a field studying computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society while IS emphasizes functionality over design.\n\nInformation technology (IT) is the application of computers and telecommunications equipment to store, retrieve, transmit and manipulate data, often in the context of a business or other enterprise. The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several industries are associated with information technology, such as computer hardware, software, electronics, semiconductors, internet, telecom equipment, e-commerce and computer services.\n\nA system administrator, IT systems administrator, systems administrator, or sysadmin is a person employed to maintain and operate a computer system and/or network. The duties of a system administrator are wide-ranging, and vary widely from one organization to another. Sysadmins are usually charged with installing, supporting and maintaining servers or other computer systems, and planning for and responding to service outages and other problems. Other duties may include scripting or light programming, project management for systems-related projects, supervising or training computer operators, and being the consultant for computer problems beyond the knowledge of technical support staff.\n\nDNA-based computing and quantum computing are areas of active research in both hardware and software (such as the development of quantum algorithms). Potential infrastructure for future technologies includes DNA origami on photolithography and quantum antennae for transferring information between ion traps. By 2011, researchers had entangled 14 qubits. Fast digital circuits (including those based on Josephson junctions and rapid single flux quantum technology) are becoming more nearly realizable with the discovery of nanoscale superconductors.\n\nFiber-optic and photonic (optical) devices, which already have been used to transport data over long distances, have started being used by data centers, side by side with CPU and semiconductor memory components. This allows the separation of RAM from CPU by optical interconnects. IBM has created an integrated circuit with both electronic and optical information processing in one chip. This is denoted \"CMOS-integrated nanophotonics\" or (CINP). One benefit of optical interconnects is that motherboards which formerly required a certain kind of system on a chip (SoC) can now move formerly dedicated memory and network controllers off the motherboards, spreading the controllers out onto the rack. This allows standardization of backplane interconnects and motherboards for multiple types of SoCs, which allows more timely upgrades of CPUs.\n\n\n"}
{"id": "5215", "url": "https://en.wikipedia.org/wiki?curid=5215", "title": "Casino", "text": "Casino\n\nA casino is a facility which houses and accommodates certain types of gambling activities. The industry that deals in casinos is called the gaming industry. Casinos are most commonly built near or combined with hotels, restaurants, retail shopping, cruise ships or other tourist attractions. There is much debate over whether or not the social and economic consequences of casino gambling outweigh the initial revenue that may be generated. Some casinos are also known for hosting live entertainment events, such as stand-up comedy, concerts, and sporting events.\n\nThe term \"casino\" is a confusing linguistic false friend for translators.\n\n\"Casino\" is of Italian origin; the root \"casa\" (house) originally meant a small country villa, summerhouse, or social club. During the 19th century, the term \"casino\" came to include other public buildings where pleasurable activities took place; such edifices were usually built on the grounds of a larger Italian villa or palazzo, and were used to host civic town functions, including dancing, gambling, music listening, and sports; examples in Italy include Villa Farnese and Villa Giulia, and in the US the Newport Casino in Newport, Rhode Island. In modern-day Italian, the term \"casino\" designates a bordello (also called \"casa chiusa\", literally \"closed house\"), while the gambling house is spelled \"casinò\" with an accent.\n\nNot all casinos were used for gaming. The Catalina Casino, a famous landmark overlooking Avalon Harbor on Santa Catalina Island, California, has never been used for traditional games of chance, which were already outlawed in California by the time it was built. The Copenhagen Casino was a theatre, known for the mass public meetings often held in its hall during the 1848 Revolution, which made Denmark a constitutional monarchy. Until 1937, it was a well-known Danish theatre. The Hanko Casino in Hanko, Finland—one of that town's most conspicuous landmarks—was never used for gambling. Rather, it was a banquet hall for the Russian nobility which frequented this spa resort in the late 19th century and is now used as a restaurant.\n\nIn military and non-military usage in German and Spanish, a \"casino\" or \"kasino\" is an officers' mess. In Italian—the source-language of the word—a \"casino\" is either a brothel, a mess, or a noisy environment, while a gaming house is called a \"casinò\".\n\nThe precise origin of gambling is unknown. It is generally believed that gambling in some form or another has been seen in almost every society in history. From the Ancient Greeks and Romans to Napoleon's France and Elizabethan England, much of history is filled with stories of entertainment based on games of chance.\n\nThe first known European gambling house, not called a casino although meeting the modern definition, was the Ridotto, established in Venice, Italy in 1638 by the Great Council of Venice to provide controlled gambling during the carnival season. It was closed in 1774 as the city government felt it was impoverishing the local gentry.\n\nIn American history, early gambling establishments were known as saloons. The creation and importance of saloons was greatly influenced by four major cities: New Orleans, St. Louis, Chicago and San Francisco. It was in the saloons that travelers could find people to talk to, drink with, and often gamble with. During the early 20th century in America, gambling became outlawed and banned by state legislation and social reformers of the time. However, in 1931, gambling was legalized throughout the state of Nevada. America's first legalized casinos were set up in those places. In 1976 New Jersey allowed gambling in Atlantic City, now America's second largest gambling city.\n\nMost jurisdictions worldwide have a minimum gambling age (16 to 21 years of age in most countries which permit the operation of casinos).\n\nCustomers gamble by playing games of chance, in some cases with an element of skill, such as craps, roulette, baccarat, blackjack, and video poker. Most games played have mathematically determined odds that ensure the house has at all times an overall advantage over the players. This can be expressed more precisely by the notion of expected value, which is uniformly negative (from the player's perspective). This advantage is called the \"house edge\". In games such as poker where players play against each other, the house takes a commission called the rake. Casinos sometimes give out complimentary items or comps to gamblers.\n\n\"Payout\" is the percentage of funds (\"winnings\") returned to players.\n\nCasinos in the United States say that a player staking money won from the casino is \"playing with the house's money\".\n\nVideo Lottery Machines (slot machines) have become one of the most popular forms of gambling in casinos. investigative reports have started calling into question whether the modern-day slot-machine is addictive.\nCasino design—regarded as a psychological exercise—is an intricate process that involves optimising floor plan, décor and atmospherics to encourage consumer gambling.\n\nFactors influencing consumer gambling tendencies include sound, odour and lighting. Natasha Dow Schüll, an anthropologist at the Massachusetts Institute of Technology, highlights the audio directors at Silicon Gaming’s decision to make its slot machines resonate in, “the universally pleasant tone of C, sampling existing casino soundscapes to create a sound that would please but not clash”.\n\nDr. Alan Hirsch, founder of the Smell & Taste Treatment and Research Foundation in Chicago, studied the impact of certain scents on gamblers, discerning that a pleasant albeit unidentifiable odour released by Las Vegas slots machines generated approximately 50% more in daily revenue. He suggested that the scent acted as an aphrodisiac, facilitating a more aggressive form of gambling.\n\nCasino designer Roger Thomas is credited with implementing a successful, disruptive design for the Las Vegas Wynn Resorts’ casinos in 2008. He broke casino design convention by introducing natural sunlight and flora to appeal to a female demographic. Thomas inserted skylights and antique clocks, defying the commonplace notion that a casino should be a timeless space.\n\nThe following lists major casino markets in the world with casino revenue of over $1 billion USD as published in PricewaterhouseCoopers's report on \nthe outlook for the global casino market:\n\nAccording to Bloomberg, accumulated revenue of biggest casino operator companies worldwide amounted almost 55 billion US dollars as per 2011. SJM Holdings ltd. was the leading company in this field and earned 9.7 billion in 2011, followed by Las Vegas Sands Corp. (7.4 bn). The third biggest casino operator company (based on revenue) was Caesars Entertainment with revenue of 6.2 bn US dollar.\n\nWhile there are casinos in many places, a few places have become well-known specifically for gambling. Perhaps the place almost defined by its casino is Monte Carlo, but other places are known as gambling centers.\n\nMonte Carlo has a famous casino popular with well-off visitors and is a tourist attraction in its own right. A song and a film named \"The Man Who Broke the Bank at Monte Carlo\" need no explanation—they clearly refer to the casino.\n\nMonte Carlo's Casino has also been depicted in many books including Ben Mezrich's \"Busting Vegas\", where a group of Massachusetts Institute of Technology students beat the casino out of nearly $1 000 000. This book is based on real people and events; however, many of those events are contested by main character Semyon Dukach.\n\nThe casino has made Monte Carlo so well known for games of chance that mathematical methods for solving various problems using many quasi-random numbers—numbers with the statistical distribution of numbers generated by chance—are formally known as Monte Carlo methods. Monte Carlo was part of the plot in a few James Bond novels and films.\n\nThe former Portuguese colony of Macau, a special administrative region of China since 1999, is a popular destination for visitors who wish to gamble. This started in Portuguese times, when Macau was popular with visitors from nearby British Hong Kong where gambling was more closely regulated. The Venetian Macao is currently the largest casino in the world. Macau also surpassed Las Vegas as the largest gambling market in the world.\n\nSingapore is an up-and-coming destination for visitors wanting to gamble, although there are currently only two casinos (both foreign owned), in Singapore. The Marina Bay Sands is the most expensive standalone casino in the world, at a price of US$8 billion, and is among the world's ten most expensive buildings. The Resorts World Sentosa has the world's largest oceanarium.\n\nWith currently over 1,000 casinos, the United States has the largest number of casinos in the world. The number continues to grow steadily as more states seek to legalize casinos. 40 states now have some form of casino gambling. Relatively small places such as Las Vegas are best known for gambling; larger cities such as Chicago are not defined by their casinos in spite of the large turnover.\n\nThe Las Vegas Valley has the largest concentration of casinos in the United States. Based on revenue, Atlantic City, New Jersey ranks second, and the Chicago region third.\n\nTop American casino markets by revenue (2015 annual revenues):\n\n\nThe Nevada Gaming Control Board divides Clark County, which is coextensive with the Las Vegas metropolitan area, into seven market regions for reporting purposes.\n\nIndian gaming has been responsible for a rise in the number of casinos outside of Las Vegas and Atlantic City.\n\nGiven the large amounts of currency handled within a casino, both patrons and staff may be tempted to cheat and steal, in collusion or independently; most casinos have security measures to prevent this. Security cameras located throughout the casino are the most basic measure.\n\nModern casino security is usually divided between a physical security force and a specialized surveillance department. The physical security force usually patrols the casino and responds to calls for assistance and reports of suspicious or definite criminal activity. A specialized surveillance department operates the casino's closed circuit television system, known in the industry as the eye in the sky. Both of these specialized casino security departments work very closely with each other to ensure the safety of both guests and the casino's assets, and have been quite successful in preventing crime. Some casinos also have catwalks in the ceiling above the casino floor, which allow surveillance personnel to look directly down, through one way glass, on the activities at the tables and slot machines.\n\nWhen it opened in 1989, The Mirage was the first casino to use cameras full-time on all table games.\n\nIn addition to cameras and other technological measures, casinos also enforce security through rules of conduct and behavior; for example, players at card games are required to keep the cards they are holding in their hands visible at all times.\n\nOver the past few decades, casinos have developed many different marketing techniques for attracting and maintaining loyal patrons. Many casinos use a loyalty rewards program used to track players' spending habits and target their patrons more effectively, by sending mailings with free slot play and other promotions.\n\nOne area of controversy surrounding casinos is their relationship to crime rates. Economic studies that show a positive relationship between casinos and crime usually fail to consider the visiting population at risk when they calculate the crime rate in casino areas. Such studies thus count the crimes committed by visitors, but do not count visitors in the population measure, and this overstates the crime rates in casino areas. Part of the reason this methodology is used, despite it leading to an overstatement of crime rates is that reliable data on tourist count are often not available.\nIn a 2004 report by the US Department of Justice, researchers interviewed people who had been arrested in Las Vegas and Des Moines and found that the percentage of problem or pathological gamblers among the arrestees was three to five times higher than in the general population. According to some police reports, incidences of reported crime often double and triple in communities within three years of a casino opening.\n\n"}
{"id": "5216", "url": "https://en.wikipedia.org/wiki?curid=5216", "title": "Khmer language", "text": "Khmer language\n\nKhmer or Cambodian (natively , or more formally ) is the language of the Khmer people and the official language of Cambodia. With approximately 16 million speakers, it is the second most widely spoken Austroasiatic language (after Vietnamese). Khmer has been influenced considerably by Sanskrit and Pali, especially in the royal and religious registers, through Hinduism and Buddhism. The more colloquial registers have influenced, and have been influenced by, Thai, Lao, Vietnamese, and Cham, all of which, due to geographical proximity and long-term cultural contact, form a sprachbund in peninsular Southeast Asia. It is also the earliest recorded and earliest written language of the Mon–Khmer family, predating Mon and by a significant margin Vietnamese, due to Old Khmer being the language of the historical empires of Chenla, Angkor and, presumably, their earlier predecessor state, Funan.\n\nThe vast majority of Khmer speakers speak Central Khmer, the dialect of the central plain where the Khmer are most heavily concentrated. Within Cambodia, regional accents exist in remote areas but these are regarded as varieties of Central Khmer. Two exceptions are the speech of the capital, Phnom Penh, and that of the Khmer Khe in Stung Treng province, both of which differ sufficiently enough from Central Khmer to be considered separate dialects of Khmer. Outside of Cambodia, three distinct dialects are spoken by ethnic Khmers native to areas that were historically part of the Khmer Empire. The Northern Khmer dialect is spoken by over a million Khmers in the southern regions of Northeast Thailand and is treated by some linguists as a separate language. Khmer Krom, or Southern Khmer, is the first language of the Khmer of Vietnam while the Khmer living in the remote Cardamom mountains speak a very conservative dialect that still displays features of the Middle Khmer language.\n\nKhmer is primarily an analytic, isolating language. There are no inflections, conjugations or case endings. Instead, particles and auxiliary words are used to indicate grammatical relationships. General word order is subject–verb–object, and modifiers follow the word they modify. Classifiers appear after numbers when used to count nouns, though not always so consistently as in languages like Chinese. In spoken Khmer, topic-comment structure is common and the perceived social relation between participants determines which sets of vocabulary, such as pronouns and honorifics, are proper.\n\nKhmer differs from neighboring languages such as Thai, Burmese, Lao and Vietnamese in that it is not a tonal language. Words are stressed on the final syllable, hence many words conform to the typical Mon–Khmer pattern of a stressed syllable preceded by a minor syllable. The language has been written in the Khmer script, an abugida descended from the Brahmi script via the southern Indian Pallava script, since at least the seventh century. The script's form and use has evolved over the centuries; its modern features include subscripted versions of consonants used to write clusters and a division of consonants into two series with different inherent vowels. Approximately 79% of Cambodians are able to read Khmer.\n\nKhmer is a member of the Austroasiatic language family, the autochthonous family in an area that stretches from the Malay Peninsula through Southeast Asia to East India. Austroasiatic, which also includes Mon, Vietnamese and Munda, has been studied since 1856 and was first proposed as a language family in 1907. Despite the amount of research, there is still doubt about the internal relationship of the languages of Austroasiatic. Diffloth places Khmer in an eastern branch of the Mon-Khmer languages. In these classification schemes Khmer's closest genetic relatives are the Bahnaric and Pearic languages. More recent classifications doubt the validity of the Mon-Khmer sub-grouping and place the Khmer language as its own branch of Austroasiatic equidistant from the other 12 branches of the family.\n\nKhmer is spoken by some 13 million people in Cambodia, where it is the official language. It is also a second language for most of the minority groups and indigenous hill tribes there. Additionally there are a million speakers of Khmer native to southern Vietnam (1999 census) and 1.4 million in northeast Thailand (2006).\n\nKhmer dialects, although mutually intelligible, are sometimes quite marked. Notable variations are found in speakers from Phnom Penh (Cambodia's capital city), the rural Battambang area, the areas of Northeast Thailand adjacent to Cambodia such as Surin province, the Cardamom Mountains, and southern Vietnam. The dialects form a continuum running roughly north to south. Standard Cambodian Khmer is mutually intelligible with the others but a Khmer Krom speaker from Vietnam, for instance, may have great difficulty communicating with a Khmer native of Sisaket Province in Thailand.\n\nThe following is a classification scheme showing the development of the modern Khmer dialects.\n\nStandard Khmer, or Central Khmer, the language as taught in Cambodian schools and used by the media, is based on the dialect spoken throughout the Central Plain, a region encompassed by the northwest and central provinces.\n\nNorthern Khmer (called \"Khmer Surin\" in Khmer) refers to the dialects spoken by many in several border provinces of present-day northeast Thailand. After the fall of the Khmer Empire in the early 15th century, the Dongrek Mountains served as a natural border leaving the Khmer north of the mountains under the sphere of influence of the Kingdom of Lan Xang. The conquests of Cambodia by Naresuan the Great for Ayutthaya furthered their political and economic isolation from Cambodia proper, leading to a dialect that developed relatively independently from the midpoint of the Middle Khmer period. This has resulted in a distinct accent influenced by the surrounding tonal languages Lao and Thai, lexical differences, and phonemic differences in both vowels and distribution of consonants. Syllable-final , which has become silent in other dialects of Khmer, is still pronounced in Northern Khmer. Some linguists classify Northern Khmer as a separate but closely related language rather than a dialect.\n\nWestern Khmer, also called Cardamom Khmer or Chanthaburi Khmer, is spoken by a very small, isolated population in the Cardamom mountain range extending from western Cambodia into eastern Central Thailand. Although little studied, this variety is unique in that it maintains a definite system of vocal register that has all but disappeared in other dialects of modern Khmer.\n\nPhnom Penh Khmer is spoken in the capital and surrounding areas. This dialect is characterized by merging or complete elision of syllables, considered by speakers from other regions to be a \"relaxed\" pronunciation. For instance, \"Phnom Penh\" will sometimes be shortened to \"m'Penh\". Another characteristic of Phnom Penh speech is observed in words with an \"r\" either as an initial consonant or as the second member of a consonant cluster (as in the English word \"bread\"). The \"r\", trilled or flapped in other dialects, is either pronounced as a uvular trill or not pronounced at all. This alters the quality of any preceding consonant, causing a harder, more emphasized pronunciation. Another unique result is that the syllable is spoken with a low-rising or \"dipping\" tone much like the \"hỏi\" tone in Vietnamese. For example, some people pronounce ('fish') as : the is dropped and the vowel begins by dipping much lower in tone than standard speech and then rises, effectively doubling its length. Another example is the word ('study'), which is pronounced , with the uvular \"r\" and the same intonation described above.\n\nKhmer Krom or Southern Khmer is spoken by the indigenous Khmer population of the Mekong Delta, formerly controlled by the Khmer Empire but part of Vietnam since 1698. Khmers are persecuted by the Vietnamese government for using their native language and, since the 1950s, have been forced to take Vietnamese names. Consequently, very little research has been published regarding this dialect. It has been generally influenced by Vietnamese for three centuries and accordingly displays a pronounced accent, tendency toward monosyllablic words and lexical differences from Standard Khmer.\n\nKhmer Khe is spoken in the Se San, Srepok and Sekong river valleys of Sesan and Siem Pang districts in Stung Treng Province. Following the decline of Angkor, the Khmer abandoned their northern territories which were then settled by the Lao. In the 17th century, Chey Chetha XI led a Khmer force into Stung Treng to retake the area. The Khmer Khe living in this area of Stung Treng in modern times are presumed to be the descendants of this group. Their dialect is thought to resemble that of pre-modern Siem Reap.\n\nLinguistic study of the Khmer language divides its history into four periods one of which, the Old Khmer period, is subdivided into pre-Angkorian and Angkorian. Pre-Angkorian Khmer, the Old Khmer language from 600 CE through 800, is only known from words and phrases in Sanskrit texts of the era. Old Khmer (or Angkorian Khmer) is the language as it was spoken in the Khmer Empire from the 9th century until the weakening of the empire sometime in the 13th century. Old Khmer is attested by many primary sources and has been studied in depth by a few scholars, most notably Saveros Pou, Phillip Jenner and Heinz-Jürgen Pinnow. Following the end of the Khmer Empire the language lost the standardizing influence of being the language of government and accordingly underwent a turbulent period of change in morphology, phonology and lexicon. The language of this transition period, from about the 14th to 18th centuries, is referred to as Middle Khmer and saw borrowing from Thai, Lao and, to a lesser extent, Vietnamese. The changes during this period are so profound that the rules of Modern Khmer can not be applied to correctly understand Old Khmer. The language became recognizable as Modern Khmer, spoken from the 19th century till today.\n\nThe following table shows the conventionally accepted historical stages of Khmer.\n\nJust as modern Khmer was emerging from the transitional period represented by Middle Khmer, Cambodia fell under the influence of French colonialism. Thailand, which had for centuries claimed suzerainty over Cambodia and controlled succession to the Cambodian throne, began losing its influence on the language. In 1887 Cambodia was fully integrated into French Indochina which brought in a French-speaking aristocracy. This led to French becoming the language of higher education and the intellectual class. By 1907, the French had wrested over half of modern-day Cambodia, including the north and northwest where Thai had been the prestige language, back from Thai control and reintegrated it into the country.\n\nMany native scholars in the early 20th century, led by a monk named Chuon Nath, resisted the French and Thai influences on their language. Forming the government sponsored Cultural Committee to define and standardize the modern language, they championed Khmerization, purging of foreign elements, reviving affixation, and the use of Old Khmer roots and historical Pali and Sanskrit to coin new words for modern ideas. Opponents, led by Keng Vannsak, who embraced \"total Khmerization\" by denouncing the reversion to classical languages and favoring the use of contemporary colloquial Khmer for neologisms, and Ieu Koeus, who favored borrowing from Thai, were also influential. Koeus later joined the Cultural Committee and supported Nath. Nath's views and prolific work won out and he is credited with cultivating modern Khmer-language identity and culture, overseeing the translation of the entire Pali Buddhist canon into Khmer. He also created the modern Khmer language dictionary that is still in use today, thereby ensuring that Khmer would survive, and indeed flourish, during the French colonial period.\n\nThe phonological system described here is the inventory of sounds of the standard spoken language, represented using appropriate symbols from the International Phonetic Alphabet (IPA).\n\nThe voiceless plosives may occur with or without aspiration (as vs. , etc.); this difference is contrastive before a vowel. However, the aspirated sounds in that position may be analyzed as sequences of two phonemes: . This analysis is supported by the fact that infixes can be inserted between the stop and the aspiration; for example ('big') becomes ('size') with a nominalizing infix. When one of these plosives occurs initially before another consonant, aspiration is no longer contrastive and can be regarded as mere phonetic detail: slight aspiration is expected when the following consonant is not one of (or if the initial plosive is ).\n\nThe voiced plosives are pronounced as implosives by most speakers, but this feature is weak in educated speech, where they become .\n\nIn syllable-final position, and approach and respectively. The stops are unaspirated and have no audible release when occurring as syllable finals.\n\nIn addition, the consonants , , and occur occasionally in recent loan words in the speech of Cambodians familiar with French and other languages.\n\nVarious authors have proposed slightly different analyses of the Khmer vowel system. This may be in part because of the wide degree of variation in pronunciation between individual speakers, even within a dialectal region. The description below follows Huffman (1970). The number of vowel nuclei and their values vary between dialects; differences exist even between the Standard Khmer system and that of the Battambang dialect on which the standard is based.\n\nIn addition, there are diphthongs and triphthongs which are analyzed as a vowel nucleus plus a semivowel ( or ) coda because they can not be followed by a final consonant. These include: (with short monophthongs) , , , , ; (with long monophthongs) , ; (with long diphthongs) , , , , and .\n\nA Khmer syllable begins with a single consonant, or else with a cluster of two, or rarely three, consonants. The only possible clusters of three consonants at the start of a syllable are , and (with aspirated consonants analyzed as two-consonant sequences) . There are 85 possible two-consonant clusters (including [pʰ] etc. analyzed as /ph/ etc.). All the clusters are shown in the following table, phonetically, i.e. superscript ʰ can mark either contrastive or non-contrastive aspiration (see above).\n\nSlight vowel epenthesis occurs in the clusters consisting of a plosive followed by , in those beginning , and in the cluster .\n\nAfter the initial consonant or consonant cluster comes the syllabic nucleus, which is one of the vowels listed above. This vowel may end the syllable or may be followed by a coda, which is a single consonant. If the syllable is stressed and the vowel is short, there must be a final consonant. All consonant sounds except and the aspirates can appear as the coda (although final /r/ is heard in some dialects, most notably in Northern Khmer).\n\nA minor syllable (unstressed syllable preceding the main syllable of a word) has a structure of CV-, CrV-, CVN- or CrVN- (where C is a consonant, V a vowel, and N a nasal consonant). The vowels in such syllables are usually short; in conversation they may be reduced to , although in careful or formal speech, including on television and radio, they are clearly articulated. An example of such a word is ('person'), pronounced , or more casually .\n\nStress in Khmer falls on the final syllable of a word. Because of this predictable pattern, stress is non-phonemic in Khmer (it does not distinguish different meanings).\n\nMost Khmer words consist of either one or two syllables. In most native disyllabic words, the first syllable is a minor (fully unstressed) syllable. Such words have been described as \"sesquisyllabic\" (i.e. as having one-and-a-half syllables). There are also some disyllabic words in which the first syllable does not behave as a minor syllable, but takes secondary stress. Most such words are compounds, but some are single morphemes (generally loanwords). An example is ('language'), pronounced .\n\nWords with three or more syllables, if they are not compounds, are mostly loanwords, usually derived from Pali, Sanskrit, or more recently, French. They are nonetheless adapted to Khmer stress patterns. Primary stress falls on the final syllable, with secondary stress on every second syllable from the end. Thus in a three-syllable word, the first syllable has secondary stress; in a four-syllable word, the second syllable has secondary stress; in a five-syllable word, the first and third syllables have secondary stress, and so on. Long polysyllables are not often used in conversation.\n\nCompounds, however, preserve the stress patterns of the constituent words. Thus , the name of a kind of cookie (literally 'bird's nest'), is pronounced , with secondary stress on the second rather than the first syllable, because it is composed of the words ('nest') and ('bird').\n\nKhmer once had a phonation distinction in its vowels, but this now survives only in the most archaic dialect (Western Khmer). The distinction arose historically when vowels after Old Khmer voiced consonants became breathy voiced and diphthongized; for example became . When consonant voicing was lost, the distinction was maintained by the vowel (); later the phonation disappeared as well (). These processes explain the origin of what are now called a-series and o-series consonants in the Khmer script.\n\nAlthough most Cambodian dialects are not tonal, colloquial Phnom Penh dialect has developed a tonal contrast (level versus peaking tone) to compensate for the elision of .\n\nIntonation often conveys semantic context in Khmer, as in distinguishing declarative statements, questions and exclamations. The available grammatical means of making such distinctions are not always used, or may be ambiguous; for example, the final interrogative particle can also serve as an emphasizing (or in some cases negating) particle.\n\nThe intonation pattern of a typical Khmer declarative phrase is a steady rise throughout followed by an abrupt drop on the last syllable.\n\nOther intonation contours signify a different type of phrase such as the \"full doubt\" interrogative, similar to yes-no questions in English. Full doubt interrogatives remain fairly even in tone throughout, but rise sharply towards the end.\n\nExclamatory phrases follow the typical steadily rising pattern, but rise sharply on the last syllable instead of falling.\n\nKhmer is primarily an analytic language with no inflection. Syntactic relations are mainly determined by word order. Old and Middle Khmer used particles to mark grammatical categories and many of these have survived in Modern Khmer but are used sparingly, mostly in literary or formal language. Khmer makes extensive use of auxiliary verbs, \"directionals\" and serial verb construction. Colloquial Khmer is a zero copula language, instead preferring predicative adjectives (and even predicative nouns) unless using a copula for emphasis or to avoid ambiguity in more complex sentences. Basic word order is subject–verb–object (SVO), although subjects are often dropped; prepositions are used rather than postpositions. Topic-Comment constructions are common and the language is generally head-initial (modifiers follow the words they modify). Some grammatical processes are still not fully understood by western scholars. For example, it's not clear if certain features of Khmer grammar, such as actor nominalization, should be treated as a morphological process or a purely syntactic device, and some derivational morphology seems to be \"purely decorative\" and performs no known syntactic work.\n\nLexical categories have been hard to define in Khmer. Henri Maspero, an early scholar of Khmer, claimed the language had no parts of speech, while a later scholar, Judith Jacob, posited four parts of speech and innumerable particles. John Haiman, on the other hand, identifies \"a couple dozen\" parts of speech in Khmer with the caveat that Khmer words have the freedom to perform a variety of syntactic functions depending on such factors as word order, relevant particles, location within a clause, intonation and context. Some of the more important lexical categories and their function are demonstrated in the following example sentence taken from a hospital brochure:\n\nModern Khmer is an isolating language, which means that it uses little productive morphology. There is some derivation by means of prefixes and infixes, but this is a remnant of Old Khmer and not always productive in the modern language. Khmer morphology is evidence of a historical process through which the language was, at some point in the past, changed from being an agglutinative language to adopting an isolating typology. Affixed forms are lexicalized and cannot be used productively to form new words. Below are some of the most common affixes with examples as given by Huffman.\nCompounding in Khmer is a common derivational process that takes two forms, coordinate compounds and repetitive compounds. Coordinate compounds join two unbound morphemes (independent words) of similar meaning to form a compound signifying a concept more general than either word alone. Coordinate compounds join either two nouns or two verbs. Repetitive compounds, one of the most productive derivational features of Khmer, use reduplication of an entire word to derive words whose meaning will depend on the class of the reduplicated word. A repetitive compound of a noun indicates plurality or generality while that of an adjectival verb could mean either an intensification or plurality.\n\nCoordinate compounds:\n\nRepetitive compounds:\n\nKhmer nouns do not inflect for grammatical gender or singular/plural. There are no articles, but indefiniteness is often expressed by the word for \"one\" (, ) following the noun as in ( \"a dog\"). Plurality can be marked by postnominal particles, numerals, or reduplication of a following adjective, which, although similar to intensification, is usually not ambiguous due to context.\n\nClassifying particles are used after numerals, but are not always obligatory as they are in Thai or Chinese, for example, and are often dropped in colloquial speech. Khmer nouns are divided into two groups: mass nouns, those which take classifiers, and specific nouns, which do not. The overwhelming majority are mass nouns.\n\nPossession is colloquially expressed by word order. The possessor is placed after that which is possessed. Alternatively, in more complex sentences or when emphasis is required, a possessive construction using the word (, \"property, object\") may be employed. In formal and literary contexts, the possessive particle () is used:\n\nPronouns are subject to a complicated system of social register, the choice of pronoun depending on the perceived relationships between speaker, audience and referent (see Social registers below). Kinship terms, nicknames and proper names are often used as pronouns (including for the first person) among intimates. Subject pronouns are frequently dropped in colloquial conversation.\n\nAdjectives, verbs and verb phrases may be made into nouns by the use of nominalization particles. Three of the more common particles used to create nouns are , , and . These particles are prefixed most often to verbs in order to form abstract nouns. The latter, derived from Sanskrit, also occurs as a suffix in fixed forms borrowed from Sanskrit and Pali such as (\"health\") from (\"to be healthy\").\n\nAdjectives, demonstratives and numerals follow the noun they modify. Adverbs likewise follow the verb. Morphologically, adjectives and adverbs are not distinguished, with many words often serving either function. Adjectives are also employed as verbs as Khmer sentences rarely use a copula.\n\nDegrees of comparison are constructed syntactically. Comparatives are expressed using the word : \"A X [B]\" (A is more X [than B]). The most common way to express superlatives is with : \"A X \" (A is the most X). Intensity is also expressed syntactically, similar to other languages of the region, by reduplication or with the use of intensifiers.\n\nAs is typical of most East Asian languages, Khmer verbs do not inflect at all; tense, aspect and mood can be expressed using auxiliary verbs, particles (such as កំពុង , placed before a verb to express continuous aspect) and adverbs (such as \"yesterday\", \"earlier\", \"tomorrow\"), or may be understood from context. Serial verb construction is quite common.\n\nKhmer verbs are a relatively open class and can be divided into two types, main verbs and auxiliary verbs. Huffman defined a Khmer verb as \"any word that can be (negated)\", and further divided main verbs into three classes.\n\nTransitive verbs are verbs which may be followed by a direct object:\n\nIntransitive verbs are verbs which can not be followed by an object:\n\nAdjectival verbs are a word class that has no equivalent in English. When modifying a noun or verb, they function as adjectives or adverbs, respectively, but they may also be used as main verbs equivalent to English \"be + \"adjective\"\".\n\nSyntax is the rules and processes that describe how sentences are formed in a particular language, how words relate to each other within clauses or phrases and how those phrases relate to each other within a sentence to convey meaning. Khmer syntax is very analytic. Relationships between words and phrases are signified primarily by word order supplemented with auxiliary verbs and, particularly in formal and literary registers, grammatical marking particles. Grammatical phenomena such as negation and aspect are marked by particles while interrogative sentences are marked either by particles or interrogative words equivalent to English \"wh-words\".\n\nA complete Khmer sentence consists of four basic elements which include an optional topic, an optional subject, an obligatory predicate and various adverbials and particles. The topic and subject are noun phrases, predicates are verb phrases and another noun phrase acting as an object or verbal attribute often follows the predicate.\n\nWhen combining these noun and verb phrases into a sentence the order is typically SVO:\n\nWhen both a direct object and indirect object are present without any grammatical markers, the preferred order is SV(DO)(IO). In such a case, if the direct object phrase contains multiple components, the indirect object immediately follows the noun of the direct object phrase and the direct object's modifiers follow the indirect object:\n\nThis ordering of objects can be changed and the meaning clarified with the inclusion of particles. The word , which normally means \"to arrive\" or \"towards\", can be used as a preposition meaning \"to\":\n\nAlternatively, the indirect object could precede the direct object if the object marking preposition /nəw/ were used:\n\nHowever, in spoken discourse OSV is possible when emphasizing the object in a topic-comment-like structure.\n\nThe noun phrase in Khmer typically has the following structure:\nThe elements in parentheses are optional. Honorifics are a class of words that serve to index the social status of the referent. Honorifics can be kinship terms or personal names, both of which are often used as first and second person pronouns, or specialized words such as ('god') before royal and religious objects. The most common demonstratives are ('this, these') and ('that, those'). The word ('those over there') has a more distal or vague connotation. If the noun phrase contains a possessive adjective, it follows the noun and precedes the numeral. If a descriptive attribute co-occurs with a possessive, the possessive construction () is expected.\n\nSome examples of typical Khmer noun phrases are:\nThe Khmer particle marked attributes in Old Khmer noun phrases and is used in formal and literary language to signify that what precedes is the noun and what follows is the attribute. Modern usage may carry the connotation of mild intensity.\n\nKhmer verbs are completely uninflected, and once a subject or topic has been introduced or is clear from context the noun phrase may be dropped. Thus, the simplest possible sentence in Khmer consists of a single verb. For example, /tɨw/ can mean \"I'm going.\", \"He went.\", \"They've gone.\", \"Let's go.\", etc. This also results in long strings of verbs such as:\n\nKhmer uses three verbs for what translates into English as the copula. The general copula is ; it is used to convey identity with nominal predicates. For locative predicates, the copula is . The verb is the \"existential\" copula meaning \"there is\" or \"there exists\".\n\nNegation is achieved by putting មិន before the verb and the particle ទេ at the end of the sentence or clause. In colloquial speech, verbs can also be negated without the need for a final particle, by placing ឥត before them.\n\nPast tense can be conveyed by adverbs, such as \"yesterday\" or by the use of perfective particles such as \n\nDifferent senses of future action can also be expressed by the use of adverbs like \"tomorrow\" or by the future tense marker , which is placed immediately before the verb, or both:\n\nImperatives are often unmarked. For example, in addition to the meanings given above, the \"sentence\" can also mean \"Go!\". Various words and particles may be added to the verb to soften the command to varying degrees, including to the point of politeness (jussives):\n\nProhibitives take the form \" + V\" and also are often softened by the addition of the particle to the end of the phrase.\n\nThere are three basic types of questions in Khmer. Questions requesting specific information use question words. Polar questions are indicated with interrogative particles, most commonly a homonym of the negation particle. Tag questions are indicated with various particles and rising inflection. The SVO word order is generally not inverted for questions.\n\nIn more formal contexts and in polite speech, questions are also marked at their beginning by the particle .\n\nKhmer does not have a passive voice, but there is a construction utilizing the main verb (\"to hit\", \"to be correct\", \"to affect\") as an auxiliary verb meaning \"to be subject to\" or \"to undergo\" which results in sentences that are translated to English using the passive voice.\n\nComplex sentences are formed in Khmer by the addition of one or more clauses to the main clause. The various types of clauses in Khmer include the coordinate clause, the relative clause and the subordinate clause. Word order in clauses is the same for that of the basic sentences described above. Coordinate clauses do not necessarily have to be marked; they can simply follow one another. When explicitly marked, they are joined by words similar to English conjunctions such as (\"and\") and (\"and then\") or by clause-final conjunction-like adverbs and , both of which can mean \"also\" or \"and also\"; disjunction is indicated by (\"or\"). Relative clauses can be introduced by /deal/ (\"that\") but, similar to coordinate clauses, often simply follow the main clause. For example, both phrases below can mean \"the hospital bed that has wheels\".\n\nRelative clauses are more likely to be introduced with if they do not immediately follow the head noun. Khmer subordinate conjunctions always precede a subordinate clause. Subordinate conjunctions include words such as (\"because\"), (\"seems as if\") and (\"in order to\").\n\nCounting in Khmer is based on a biquinary system (the numbers from 6 to 9 have the form \"five one\", \"five two\", etc.) However, the words for multiples of ten from 30 to 90 are not related to the basic Khmer numbers, but are probably borrowed from Thai. The Khmer script has its own versions of the Arabic numerals.\n\nThe principal number words are listed in the following table, which gives Western and Khmer digits, Khmer spelling and IPA transcription.\n\nIntermediate numbers are formed by compounding the above elements. Powers of ten are denoted by loan words: (100), (1,000), (10,000), (100,000) and (1,000,000) from Thai and (10,000,000) from Sanskrit.\n\nOrdinal numbers are formed by placing the particle before the corresponding cardinal number.\n\nKhmer employs a system of registers in which the speaker must always be conscious of the social status of the person spoken to. The different registers, which include those used for common speech, polite speech, speaking to or about royals and speaking to or about monks, employ alternate verbs, names of body parts and pronouns. This results in what appears to foreigners as separate languages and, in fact, isolated villagers often are unsure how to speak with royals and royals raised completely within the court do not feel comfortable speaking the common register. As an example, the word for \"to eat\" used between intimates or in reference to animals is . Used in polite reference to commoners, it is . When used of those of higher social status, it is or . For monks the word is and for royals, . Another result is that the pronominal system is complex and full of honorific variations, just a few of which are shown in the table below.\n\nKhmer is written with the Khmer script, an abugida developed from the Pallava script of India before the 7th century when the first known inscription appeared. Written left-to-right with vowel signs that can be placed after, before, above or below the consonant they follow, the Khmer script is similar in appearance and usage to Thai and Lao, both of which were based on the Khmer system. The Khmer script is also distantly related to the Mon script, the ancestor of the modern Burmese script. Khmer numerals, which were inherited from Indian numerals, are used more widely than Hindu-Arabic numerals. Within Cambodia, literacy in the Khmer alphabet is estimated at 77.6%.\n\nConsonant symbols in Khmer are divided into two groups, or series. The first series carries the inherent vowel while the second series carries the inherent vowel . The Khmer names of the series, ('voiceless') and ('voiced'), respectively, indicate that the second series consonants were used to represent the voiced phonemes of Old Khmer. As the voicing of stops was lost, however, the contrast shifted to the phonation of the attached vowels which, in turn, evolved into a simple difference of vowel quality, often by diphthongization. This process has resulted in the Khmer alphabet having two symbols for most consonant phonemes and each vowel symbol having two possible readings, depending on the series of the initial consonant:\n\n\n"}
{"id": "5218", "url": "https://en.wikipedia.org/wiki?curid=5218", "title": "Central processing unit", "text": "Central processing unit\n\nA central processing unit (CPU) is the electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logical, control and input/output (I/O) operations specified by the instructions. The computer industry has used the term \"central processing unit\" at least since the early 1960s. Traditionally, the term \"CPU\" refers to a processor, more specifically to its processing unit and control unit (CU), distinguishing these core elements of a computer from external components such as main memory and I/O circuitry.\n\nThe form, design and implementation of CPUs have changed over the course of their history, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory) and execution of instructions by directing the coordinated operations of the ALU, registers and other components.\n\nMost modern CPUs are microprocessors, meaning they are contained on a single integrated circuit (IC) chip. An IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC). Some computers employ a multi-core processor, which is a single chip containing two or more CPUs called \"cores\"; in that context, one can speak of such single chips as \"sockets\". Array processors or vector processors have multiple processors that operate in parallel, with no unit considered central. There also exists the concept of virtual CPUs which are an abstraction of dynamical aggregated computational resources.\n\nEarly computers such as the ENIAC had to be physically rewired to perform different tasks, which caused these machines to be called \"fixed-program computers\". Since the term \"CPU\" is generally defined as a device for software (computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of the stored-program computer.\n\nThe idea of a stored-program computer was already present in the design of J. Presper Eckert and John William Mauchly's ENIAC, but was initially omitted so that it could be finished sooner. On June 30, 1945, before ENIAC was made, mathematician John von Neumann distributed the paper entitled \"First Draft of a Report on the EDVAC\". It was the outline of a stored-program computer that would eventually be completed in August 1949. EDVAC was designed to perform a certain number of instructions (or operations) of various types. Significantly, the programs written for EDVAC were to be stored in high-speed computer memory rather than specified by the physical wiring of the computer. This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task. With von Neumann's design, the program that EDVAC ran could be changed simply by changing the contents of the memory. EDVAC, however, was not the first stored-program computer; the Manchester Small-Scale Experimental Machine, a small prototype stored-program computer, ran its first program on 21 June 1948 and the Manchester Mark 1 ran its first program during the night of 16–17 June 1949.\n\nEarly CPUs were custom designs used as part of a larger and sometimes distinctive computer. However, this method of designing custom CPUs for a particular application has largely given way to the development of multi-purpose processors produced in large quantities. This standardization began in the era of discrete transistor mainframes and minicomputers and has rapidly accelerated with the popularization of the integrated circuit (IC). The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order of nanometers. Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in electronic devices ranging from automobiles to cellphones, and sometimes even in toys.\n\nWhile von Neumann is most often credited with the design of the stored-program computer because of his design of EDVAC, and the design became known as the von Neumann architecture, others before him, such as Konrad Zuse, had suggested and implemented similar ideas. The so-called Harvard architecture of the Harvard Mark I, which was completed before EDVAC, also utilized a stored-program design using punched paper tape rather than electronic memory. The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data, while the former uses the same memory space for both. Most modern CPUs are primarily von Neumann in design, but CPUs with the Harvard architecture are seen as well, especially in embedded applications; for instance, the Atmel AVR microcontrollers are Harvard architecture processors.\n\nRelays and vacuum tubes (thermionic tubes) were commonly used as switching elements; a useful computer requires thousands or tens of thousands of switching devices. The overall speed of a system is dependent on the speed of the switches. Tube computers like EDVAC tended to average eight hours between failures, whereas relay computers like the (slower, but earlier) Harvard Mark I failed very rarely. In the end, tube-based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at low clock rates compared to modern microelectronic designs. Clock signal frequencies ranging from 100 kHz to 4 MHz were very common at this time, limited largely by the speed of the switching devices they were built with.\n\nThe design complexity of CPUs increased as various technologies facilitated building smaller and more reliable electronic devices. The first such improvement came with the advent of the transistor. Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky, unreliable, and fragile switching elements like vacuum tubes and relays. With this improvement more complex and reliable CPUs were built onto one or several printed circuit boards containing discrete (individual) components.\n\nIn 1964, IBM introduced its System/360 computer architecture that was used in a series of computers capable of running the same programs with different speed and performance. This was significant at a time when most electronic computers were incompatible with one another, even those made by the same manufacturer. To facilitate this improvement, IBM utilized the concept of a microprogram (often called \"microcode\"), which still sees widespread usage in modern CPUs. The System/360 architecture was so popular that it dominated the mainframe computer market for decades and left a legacy that is still continued by similar modern computers like the IBM zSeries. In 1965, Digital Equipment Corporation (DEC) introduced another influential computer aimed at the scientific and research markets, the PDP-8.\n\nTransistor-based computers had several distinct advantages over their predecessors. Aside from facilitating increased reliability and lower power consumption, transistors also allowed CPUs to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay. The increased reliability and dramatically increased speed of the switching elements (which were almost exclusively transistors by this time), CPU clock rates in the tens of megahertz were easily obtained during this period. Additionally while discrete transistor and IC CPUs were in heavy usage, new high-performance designs like SIMD (Single Instruction Multiple Data) vector processors began to appear. These early experimental designs later gave rise to the era of specialized supercomputers like those made by Cray Inc and Fujitsu Ltd.\n\nDuring this period, a method of manufacturing many interconnected transistors in a compact space was developed. The integrated circuit (IC) allowed a large number of transistors to be manufactured on a single semiconductor-based die, or \"chip\". At first, only very basic non-specialized digital circuits such as NOR gates were miniaturized into ICs. CPUs based on these \"building block\" ICs are generally referred to as \"small-scale integration\" (SSI) devices. SSI ICs, such as the ones used in the Apollo guidance computer, usually contained up to a few dozen transistors. To build an entire CPU out of SSI ICs required thousands of individual chips, but still consumed much less space and power than earlier discrete transistor designs.\n\nIBM's System/370 follow-on to the System/360 used SSI ICs rather than Solid Logic Technology discrete-transistor modules. DEC's PDP-8/I and KI10 PDP-10 also switched from the individual transistors used by the PDP-8 and PDP-10 to SSI ICs, and their extremely popular PDP-11 line was originally built with SSI ICs but was eventually implemented with LSI components once these became practical.\n\nLee Boysel published influential articles, including a 1967 \"manifesto\", which described how to build the equivalent of a 32-bit mainframe computer from a relatively small number of large-scale integration circuits (LSI). At the time, the only way to build LSI chips, which are chips with a hundred or more gates, was to build them using a MOS process (i.e., PMOS logic, NMOS logic, or CMOS logic). However, some companies continued to build processors out of bipolar chips because bipolar junction transistors were so much faster than MOS chips; for example, Datapoint built processors out of transistor–transistor logic (TTL) chips until the early 1980s. At the time, MOS ICs were so slow that they were considered useful only in a few niche applications that required low power.\n\nAs the microelectronic technology advanced, an increasing number of transistors were placed on ICs, decreasing the number of individual ICs needed for a complete CPU. MSI and LSI ICs increased transistor counts to hundreds, and then thousands. By 1968, the number of ICs required to build a complete CPU had been reduced to 24 ICs of eight different types, with each IC containing roughly 1000 MOSFETs. In stark contrast with its SSI and MSI predecessors, the first LSI implementation of the PDP-11 contained a CPU composed of only four LSI integrated circuits.\n\nSince the introduction of the first commercially available microprocessor, the Intel 4004 in 1970, and the first widely used microprocessor, the Intel 8080 in 1974, this class of CPUs has almost completely overtaken all other central processing unit implementation methods. Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older computer architectures, and eventually produced instruction set compatible microprocessors that were backward-compatible with their older hardware and software. Combined with the advent and eventual success of the ubiquitous personal computer, the term \"CPU\" is now applied almost exclusively to microprocessors. Several CPUs (denoted \"cores\") can be combined in a single processing chip.\nPrevious generations of CPUs were implemented as discrete components and numerous small integrated circuits (ICs) on one or more circuit boards. Microprocessors, on the other hand, are CPUs manufactured on a very small number of ICs; usually just one. The overall smaller CPU size, as a result of being implemented on a single die, means faster switching time because of physical factors like decreased gate parasitic capacitance. This has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz. Additionally, the ability to construct exceedingly small transistors on an IC has increased the complexity and number of transistors in a single CPU many fold. This widely observed trend is described by Moore's law, which has proven to be a fairly accurate predictor of the growth of CPU (and other IC) complexity.\n\nWhile the complexity, size, construction, and general form of CPUs have changed enormously since 1950, it is notable that the basic design and function has not changed much at all. Almost all common CPUs today can be very accurately described as von Neumann stored-program machines. As the aforementioned Moore's law continues to hold true, concerns have arisen about the limits of integrated circuit transistor technology. Extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant. These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer, as well as to expand the usage of parallelism and other methods that extend the usefulness of the classical von Neumann model.\n\nThe fundamental operation of most CPUs, regardless of the physical form they take, is to execute a sequence of stored instructions that is called a program. The instructions to be executed are kept in some kind of computer memory. Nearly all CPUs follow the fetch, decode and execute steps in their operation, which are collectively known as the instruction cycle.\n\nAfter the execution of an instruction, the entire process repeats, with the next instruction cycle normally fetching the next-in-sequence instruction because of the incremented value in the program counter. If a jump instruction was executed, the program counter will be modified to contain the address of the instruction that was jumped to and program execution continues normally. In more complex CPUs, multiple instructions can be fetched, decoded, and executed simultaneously. This section describes what is generally referred to as the \"classic RISC pipeline\", which is quite common among the simple CPUs used in many electronic devices (often called microcontroller). It largely ignores the important role of CPU cache, and therefore the access stage of the pipeline.\n\nSome instructions manipulate the program counter rather than producing result data directly; such instructions are generally called \"jumps\" and facilitate program behavior like loops, conditional program execution (through the use of a conditional jump), and existence of functions. In some processors, some other instructions change the state of bits in a \"flags\" register. These flags can be used to influence how a program behaves, since they often indicate the outcome of various operations. For example, in such processors a \"compare\" instruction evaluates two values and sets or clears bits in the flags register to indicate which one is greater or whether they are equal; one of these flags could then be used by a later jump instruction to determine program flow.\n\nThe first step, fetch, involves retrieving an instruction (which is represented by a number or sequence of numbers) from program memory. The instruction's location (address) in program memory is determined by a program counter (PC), which stores a number that identifies the address of the next instruction to be fetched. After an instruction is fetched, the PC is incremented by the length of the instruction so that it will contain the address of the next instruction in the sequence. Often, the instruction to be fetched must be retrieved from relatively slow memory, causing the CPU to stall while waiting for the instruction to be returned. This issue is largely addressed in modern processors by caches and pipeline architectures (see below).\n\nThe instruction that the CPU fetches from memory determines what the CPU will do. In the decode step, performed by the circuitry known as the \"instruction decoder\", the instruction is converted into signals that control other parts of the CPU.\n\nThe way in which the instruction is interpreted is defined by the CPU's instruction set architecture (ISA). Often, one group of bits (that is, a \"field\") within the instruction, called the opcode, indicates which operation is to be performed, while the remaining fields usually provide supplemental information required for the operation, such as the operands. Those operands may be specified as a constant value (called an immediate value), or as the location of a value that may be a processor register or a memory address, as determined by some addressing mode.\n\nIn some CPU designs the instruction decoder is implemented as a hardwired, unchangeable circuit. In others, a microprogram is used to translate instructions into sets of CPU configuration signals that are applied sequentially over multiple clock pulses. In some cases the memory that stores the microprogram is rewritable, making it possible to change the way in which the CPU decodes instructions.\n\nAfter the fetch and decode steps, the execute step is performed. Depending on the CPU architecture, this may consist of a single action or a sequence of actions. During each action, various parts of the CPU are electrically connected so they can perform all or part of the desired operation and then the action is completed, typically in response to a clock pulse. Very often the results are written to an internal CPU register for quick access by subsequent instructions. In other cases results may be written to slower, but less expensive and higher capacity main memory.\n\nFor example, if an addition instruction is to be executed, the arithmetic logic unit (ALU) inputs are connected to a pair of operand sources (numbers to be summed), the ALU is configured to perform an addition operation so that the sum of its operand inputs will appear at its output, and the ALU output is connected to storage (e.g., a register or memory) that will receive the sum. When the clock pulse occurs, the sum will be transferred to storage and, if the resulting sum is too large (i.e., it is larger than the ALU's output word size), an arithmetic overflow flag will be set.\n\nHardwired into a CPU's circuitry is a set of basic operations it can perform, called an instruction set. Such operations may involve, for example, adding or subtracting two numbers, comparing two numbers, or jumping to a different part of a program. Each basic operation is represented by a particular combination of bits, known as the machine language opcode; while executing instructions in a machine language program, the CPU decides which operation to perform by \"decoding\" the opcode. A complete machine language instruction consists of an opcode and, in many cases, additional bits that specify arguments for the operation (for example, the numbers to be summed in the case of an addition operation). Going up the complexity scale, a machine language program is a collection of machine language instructions that the CPU executes.\n\nThe actual mathematical operation for each instruction is performed by a combinational logic circuit within the CPU's processor known as the arithmetic logic unit or ALU. In general, a CPU executes an instruction by fetching it from memory, using its ALU to perform an operation, and then storing the result to memory. Beside the instructions for integer mathematics and logic operations, various other machine instructions exist, such as those for loading data from memory and storing it back, branching operations, and mathematical operations on floating-point numbers performed by the CPU's floating-point unit (FPU).\n\nThe control unit of the CPU contains circuitry that uses electrical signals to direct the entire computer system to carry out stored program instructions. The control unit does not execute program instructions; rather, it directs other parts of the system to do so. The control unit communicates with both the ALU and memory.\n\nThe arithmetic logic unit (ALU) is a digital circuit within the processor that performs integer arithmetic and bitwise logic operations. The inputs to the ALU are the data words to be operated on (called operands), status information from previous operations, and a code from the control unit indicating which operation to perform. Depending on the instruction being executed, the operands may come from internal CPU registers or external memory, or they may be constants generated by the ALU itself.\n\nWhen all input signals have settled and propagated through the ALU circuitry, the result of the performed operation appears at the ALU's outputs. The result consists of both a data word, which may be stored in a register or memory, and status information that is typically stored in a special, internal CPU register reserved for this purpose.\n\nMost high-end microprocessors (in desktop, laptop, server computers) have a memory management unit, translating logical addresses into physical RAM addresses, providing memory protection and paging abilities, useful for virtual memory. Simpler processors, especially microcontrollers, usually don't include an MMU.\n\nMost CPUs are synchronous circuits, which means they employ a clock signal to pace their sequential operations. The clock signal is produced by an external oscillator circuit that generates a consistent number of pulses each second in the form of a periodic square wave. The frequency of the clock pulses determines the rate at which a CPU executes instructions and, consequently, the faster the clock, the more instructions the CPU will execute each second.\n\nTo ensure proper operation of the CPU, the clock period is longer than the maximum time needed for all signals to propagate (move) through the CPU. In setting the clock period to a value well above the worst-case propagation delay, it is possible to design the entire CPU and the way it moves data around the \"edges\" of the rising and falling clock signal. This has the advantage of simplifying the CPU significantly, both from a design perspective and a component-count perspective. However, it also carries the disadvantage that the entire CPU must wait on its slowest elements, even though some portions of it are much faster. This limitation has largely been compensated for by various methods of increasing CPU parallelism (see below).\n\nHowever, architectural improvements alone do not solve all of the drawbacks of globally synchronous CPUs. For example, a clock signal is subject to the delays of any other electrical signal. Higher clock rates in increasingly complex CPUs make it more difficult to keep the clock signal in phase (synchronized) throughout the entire unit. This has led many modern CPUs to require multiple identical clock signals to be provided to avoid delaying a single signal significantly enough to cause the CPU to malfunction. Another major issue, as clock rates increase dramatically, is the amount of heat that is dissipated by the CPU. The constantly changing clock causes many components to switch regardless of whether they are being used at that time. In general, a component that is switching uses more energy than an element in a static state. Therefore, as clock rate increases, so does energy consumption, causing the CPU to require more heat dissipation in the form of CPU cooling solutions.\n\nOne method of dealing with the switching of unneeded components is called clock gating, which involves turning off the clock signal to unneeded components (effectively disabling them). However, this is often regarded as difficult to implement and therefore does not see common usage outside of very low-power designs. One notable recent CPU design that uses extensive clock gating is the IBM PowerPC-based Xenon used in the Xbox 360; that way, power requirements of the Xbox 360 are greatly reduced. Another method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether. While removing the global clock signal makes the design process considerably more complex in many ways, asynchronous (or clockless) designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs. While somewhat uncommon, entire asynchronous CPUs have been built without utilizing a global clock signal. Two notable examples of this are the ARM compliant AMULET and the MIPS R3000 compatible MiniMIPS.\n\nRather than totally removing the clock signal, some CPU designs allow certain portions of the device to be asynchronous, such as using asynchronous ALUs in conjunction with superscalar pipelining to achieve some arithmetic performance gains. While it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts, it is evident that they do at least excel in simpler math operations. This, combined with their excellent power consumption and heat dissipation properties, makes them very suitable for embedded computers.\n\nEvery CPU represents numerical values in a specific way. For example, some early digital computers represented numbers as familiar decimal (base 10) numeral system values, and others have employed more unusual representations such as ternary (base three). Nearly all modern CPUs represent numbers in binary form, with each digit being represented by some two-valued physical quantity such as a \"high\" or \"low\" voltage.\n\nRelated to numeric representation is the size and precision of integer numbers that a CPU can represent. In the case of a binary CPU, this is measured by the number of bits (significant digits of a binary encoded integer) that the CPU can process in one operation, which is commonly called \"word size\", \"bit width\", \"data path width\", \"integer precision\", or \"integer size\". A CPU's integer size determines the range of integer values it can directly operate on. For example, an 8-bit CPU can directly manipulate integers represented by eight bits, which have a range of 256 (2) discrete integer values.\n\nInteger range can also affect the number of memory locations the CPU can directly address (an address is an integer value representing a specific memory location). For example, if a binary CPU uses 32 bits to represent a memory address then it can directly address 2 memory locations. To circumvent this limitation and for various other reasons, some CPUs use mechanisms (such as bank switching) that allow additional memory to be addressed.\n\nCPUs with larger word sizes require more circuitry and consequently are physically larger, cost more, and consume more power (and therefore generate more heat). As a result, smaller 4- or 8-bit microcontrollers are commonly used in modern applications even though CPUs with much larger word sizes (such as 16, 32, 64, even 128-bit) are available. When higher performance is required, however, the benefits of a larger word size (larger data ranges and address spaces) may outweigh the disadvantages. A CPU can have internal data paths shorter than the word size to reduce size and cost. For example, even though the IBM System/360 instruction set was a 32-bit instruction set, the System/360 Model 30 and Model 40 had 8-bit data paths in the arithmetic logical unit, so that a 32-bit add required four cycles, one for each 8 bits of the operands, and, even though the Motorola 68k instruction set was a 32-bit instruction set, the Motorola 68000 and Motorola 68010 had 16-bit data paths in the arithmetic logical unit, so that a 32-bit add required two cycles.\n\nTo gain some of the advantages afforded by both lower and higher bit lengths, many instruction sets have different bit widths for integer and floating-point data, allowing CPUs implementing that instruction set to have different bit widths for different portions of the device. For example, the IBM System/360 instruction set was primarily 32 bit, but supported 64-bit floating point values to facilitate greater accuracy and range in floating point numbers. The System/360 Model 65 had an 8-bit adder for decimal and fixed-point binary arithmetic and a 60-bit adder for floating-point arithmetic. Many later CPU designs use similar mixed bit width, especially when the processor is meant for general-purpose usage where a reasonable balance of integer and floating point capability is required.\n\nThe description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take. This type of CPU, usually referred to as \"subscalar\", operates on and executes one instruction on one or two pieces of data at a time, that is less than one instruction per clock cycle ().\n\nThis process gives rise to an inherent inefficiency in subscalar CPUs. Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction. As a result, the subscalar CPU gets \"hung up\" on instructions which take more than one clock cycle to complete execution. Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up and the number of unused transistors is increased. This design, wherein the CPU's execution resources can operate on only one instruction at a time, can only possibly reach \"scalar\" performance (one instruction per clock cycle, ). However, the performance is nearly always subscalar (less than one instruction per clock cycle, ).\n\nAttempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel. When referring to parallelism in CPUs, two terms are generally used to classify these design techniques:\n\nEach methodology differs both in the ways in which they are implemented, as well as the relative effectiveness they afford in increasing the CPU's performance for an application.\n\nOne of the simplest methods used to accomplish increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing. This is the simplest form of a technique known as instruction pipelining, and is utilized in almost all modern general-purpose CPUs. Pipelining allows more than one instruction to be executed at any given time by breaking down the execution pathway into discrete stages. This separation can be compared to an assembly line, in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired.\n\nPipelining does, however, introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation; a condition often termed data dependency conflict. To cope with this, additional care must be taken to check for these sorts of conditions and delay a portion of the instruction pipeline if this occurs. Naturally, accomplishing this requires additional circuitry, so pipelined processors are more complex than subscalar ones (though not very significantly so). A pipelined processor can become very nearly scalar, inhibited only by pipeline stalls (an instruction spending more than one clock cycle in a stage).\n\nFurther improvement upon the idea of instruction pipelining led to the development of a method that decreases the idle time of CPU components even further. Designs that are said to be \"superscalar\" include a long instruction pipeline and multiple identical execution units. In a superscalar pipeline, multiple instructions are read and passed to a dispatcher, which decides whether or not the instructions can be executed in parallel (simultaneously). If so they are dispatched to available execution units, resulting in the ability for several instructions to be executed simultaneously. In general, the more instructions a superscalar CPU is able to dispatch simultaneously to waiting execution units, the more instructions will be completed in a given cycle.\n\nMost of the difficulty in the design of a superscalar CPU architecture lies in creating an effective dispatcher. The dispatcher needs to be able to quickly and correctly determine whether instructions can be executed in parallel, as well as dispatch them in such a way as to keep as many execution units busy as possible. This requires that the instruction pipeline is filled as often as possible and gives rise to the need in superscalar architectures for significant amounts of CPU cache. It also makes hazard-avoiding techniques like branch prediction, speculative execution, and out-of-order execution crucial to maintaining high levels of performance. By attempting to predict which branch (or path) a conditional instruction will take, the CPU can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed. Speculative execution often provides modest performance increases by executing portions of code that may not be needed after a conditional operation completes. Out-of-order execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies. Also in case of single instruction stream, multiple data stream—a case when a lot of data from the same type has to be processed—, modern processors can disable parts of the pipeline so that when a single instruction is executed many times, the CPU skips the fetch and decode phases and thus greatly increases performance on certain occasions, especially in highly monotonous program engines such as video creation software and photo processing.\n\nIn the case where a portion of the CPU is superscalar and part is not, the part which is not suffers a performance penalty due to scheduling stalls. The Intel P5 Pentium had two superscalar ALUs which could accept one instruction per clock cycle each, but its FPU could not accept one instruction per clock cycle. Thus the P5 was integer superscalar but not floating point superscalar. Intel's successor to the P5 architecture, P6, added superscalar capabilities to its floating point features, and therefore afforded a significant increase in floating point instruction performance.\n\nBoth simple pipelining and superscalar design increase a CPU's ILP by allowing a single processor to complete execution of instructions at rates surpassing one instruction per clock cycle. Most modern CPU designs are at least somewhat superscalar, and nearly all general purpose CPUs designed in the last decade are superscalar. In later years some of the emphasis in designing high-ILP computers has been moved out of the CPU's hardware and into its software interface, or ISA. The strategy of the very long instruction word (VLIW) causes some ILP to become implied directly by the software, reducing the amount of work the CPU must perform to boost ILP and thereby reducing the design's complexity.\n\nAnother strategy of achieving performance is to execute multiple threads or processes in parallel. This area of research is known as parallel computing. In Flynn's taxonomy, this strategy is known as multiple instruction stream, multiple data stream (MIMD).\n\nOne technology used for this purpose was multiprocessing (MP). The initial flavor of this technology is known as symmetric multiprocessing (SMP), where a small number of CPUs share a coherent view of their memory system. In this scheme, each CPU has additional hardware to maintain a constantly up-to-date view of memory. By avoiding stale views of memory, the CPUs can cooperate on the same program and programs can migrate from one CPU to another. To increase the number of cooperating CPUs beyond a handful, schemes such as non-uniform memory access (NUMA) and directory-based coherence protocols were introduced in the 1990s. SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors. Initially, multiprocessing was built using multiple discrete CPUs and boards to implement the interconnect between the processors. When the processors and their interconnect are all implemented on a single chip, the technology is known as chip-level multiprocessing (CMP) and the single chip as a multi-core processor.\n\nIt was later recognized that finer-grain parallelism existed with a single program. A single program might have several threads (or functions) that could be executed separately or in parallel. Some of the earliest examples of this technology implemented input/output processing such as direct memory access as a separate thread from the computation thread. A more general approach to this technology was introduced in the 1970s when systems were designed to run multiple computation threads in parallel. This technology is known as multi-threading (MT). This approach is considered more cost-effective than multiprocessing, as only a small number of components within a CPU is replicated to support MT as opposed to the entire CPU in the case of MP. In MT, the execution units and the memory system including the caches are shared among multiple threads. The downside of MT is that the hardware support for multithreading is more visible to software than that of MP and thus supervisor software like operating systems have to undergo larger changes to support MT. One type of MT that was implemented is known as temporal multithreading, where one thread is executed until it is stalled waiting for data to return from external memory. In this scheme, the CPU would then quickly context switch to another thread which is ready to run, the switch often done in one CPU clock cycle, such as the UltraSPARC T1. Another type of MT is simultaneous multithreading, where instructions from multiple threads are executed in parallel within one CPU clock cycle.\n\nFor several decades from the 1970s to early 2000s, the focus in designing high performance general purpose CPUs was largely on achieving high ILP through technologies such as pipelining, caches, superscalar execution, out-of-order execution, etc. This trend culminated in large, power-hungry CPUs such as the Intel Pentium 4. By the early 2000s, CPU designers were thwarted from achieving higher performance from ILP techniques due to the growing disparity between CPU operating frequencies and main memory operating frequencies as well as escalating CPU power dissipation owing to more esoteric ILP techniques.\n\nCPU designers then borrowed ideas from commercial computing markets such as transaction processing, where the aggregate performance of multiple programs, also known as throughput computing, was more important than the performance of a single thread or process.\n\nThis reversal of emphasis is evidenced by the proliferation of dual and more core processor designs and notably, Intel's newer designs resembling its less superscalar P6 architecture. Late designs in several processor families exhibit CMP, including the x86-64 Opteron and Athlon 64 X2, the SPARC UltraSPARC T1, IBM POWER4 and POWER5, as well as several video game console CPUs like the Xbox 360's triple-core PowerPC design, and the PlayStation 3's 7-core Cell microprocessor.\n\nA less common but increasingly important paradigm of processors (and indeed, computing in general) deals with data parallelism. The processors discussed earlier are all referred to as some type of scalar device. As the name implies, vector processors deal with multiple pieces of data in the context of one instruction. This contrasts with scalar processors, which deal with one piece of data for every instruction. Using Flynn's taxonomy, these two schemes of dealing with data are generally referred to as single instruction stream, multiple data stream (SIMD) and single instruction stream, single data stream (SISD), respectively. The great utility in creating processors that deal with vectors of data lies in optimizing tasks that tend to require the same operation (for example, a sum or a dot product) to be performed on a large set of data. Some classic examples of these types of tasks include multimedia applications (images, video, and sound), as well as many types of scientific and engineering tasks. Whereas a scalar processor must complete the entire process of fetching, decoding, and executing each instruction and value in a set of data, a vector processor can perform a single operation on a comparatively large set of data with one instruction. Of course, this is only possible when the application tends to require many steps which apply one operation to a large set of data.\n\nMost early vector processors, such as the Cray-1, were associated almost exclusively with scientific research and cryptography applications. However, as multimedia has largely shifted to digital media, the need for some form of SIMD in general-purpose processors has become significant. Shortly after inclusion of floating-point units started to become commonplace in general-purpose processors, specifications for and implementations of SIMD execution units also began to appear for general-purpose processors. Some of these early SIMD specifications - like HP's Multimedia Acceleration eXtensions (MAX) and Intel's MMX - were integer-only. This proved to be a significant impediment for some software developers, since many of the applications that benefit from SIMD primarily deal with floating-point numbers. Progressively, developers refined and remade these early designs into some of the common modern SIMD specifications, which are usually associated with one ISA. Some notable modern examples include Intel's SSE and the PowerPC-related AltiVec (also known as VMX).\n\nCloud computing can involve subdividing CPU operation into virtual central processing units (vCPUs).\n\nA host is the virtual equivalent of a physical machine, on which a virtual system is operating. When there are several physical machines operating in tandem and managed as a whole, the grouped computing and memory resources form a cluster. In some systems it is possible to dynamically add and remove from a cluster. Resources available at a host and cluster level can be partitioned out into resources pools with fine granularity.\n\nThe \"performance\" or \"speed\" of a processor depends on, among many other factors, the clock rate (generally given in multiples of hertz) and the instructions per clock (IPC), which together are the factors for the instructions per second (IPS) that the CPU can perform.\nMany reported IPS values have represented \"peak\" execution rates on artificial instruction sequences with few branches, whereas realistic workloads consist of a mix of instructions and applications, some of which take longer to execute than others. The performance of the memory hierarchy also greatly affects processor performance, an issue barely considered in MIPS calculations. Because of these problems, various standardized tests, often called \"benchmarks\" for this purposesuch as SPECinthave been developed to attempt to measure the real effective performance in commonly used applications.\n\nProcessing performance of computers is increased by using multi-core processors, which essentially is plugging two or more individual processors (called \"cores\" in this sense) into one integrated circuit. Ideally, a dual core processor would be nearly twice as powerful as a single core processor. In practice, the performance gain is far smaller, only about 50%, due to imperfect software algorithms and implementation. Increasing the number of cores in a processor (i.e. dual-core, quad-core, etc.) increases the workload that can be handled. This means that the processor can now handle numerous asynchronous events, interrupts, etc. which can take a toll on the CPU when overwhelmed. These cores can be thought of as different floors in a processing plant, with each floor handling a different task. Sometimes, these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information.\n\nDue to specific capabilities of modern CPUs, such as hyper-threading and uncore, which involve sharing of actual CPU resources while aiming at increased utilization, monitoring performance levels and hardware utilization gradually became a more complex task. As a response, some CPUs implement additional hardware logic that monitors actual utilization of various parts of a CPU and provides various counters accessible to software; an example is Intel's \"Performance Counter Monitor\" technology.\n\n"}
{"id": "5221", "url": "https://en.wikipedia.org/wiki?curid=5221", "title": "Carnivora", "text": "Carnivora\n\nCarnivora (; from Latin \"carō\" (stem \"carn-\") \"flesh\" and \"vorāre\" \"to devour\") is a diverse scrotiferan order that includes over 280 species of placental mammals. Its members are formally referred to as carnivorans, whereas the word \"carnivore\" (often popularly applied to members of this group) can refer to any meat-eating organism. Carnivorans are the most diverse in size of any mammalian order, ranging from the least weasel (\"Mustela nivalis\"), at as little as and , to the polar bear (\"Ursus maritimus\"), which can weigh up to , to the southern elephant seal (\"Mirounga leonina\"), whose adult males weigh up to and measure up to in length.\n\nSome carnivorans, such as cats and pinnipeds, depend entirely on meat for their nutrition. Others, such as raccoons and bears, depending on the local habitat, are more omnivorous: the giant panda is almost exclusively a herbivore, but will take fish, eggs and insects, while the polar bear subsists mainly on seals. Carnivorans have teeth and claws adapted for catching and eating other animals. Many hunt in packs and are social animals, giving them an advantage over larger prey.\n\nCarnivorans are split into two suborders: feliforms (\"cat-like\") and caniforms (\"dog-like\").\n\nCarnivorans all share the same arrangement of teeth in which the last upper premolar (named P4) and the first lower molar (named m1) have blade-like enamel crowns that work together as carnassial teeth to shear meat. Carnivorans have had this arrangement for over 60 million years with many adaptions, and these dental adaptions help identify carnivoran species and groupings of species.\n\nCarnivorans evolved in North America out of members of the family Miacidae (miacids) about 42 million years ago. They soon split into cat-like and dog-like forms (Feliformia and Caniformia). Their molecular phylogeny shows the extant Carnivora are a monophyletic group, the crown group of the Carnivoramorpha.\n\nMost carnivorans are terrestrial; they usually have strong, sharp claws, typically with five, but never fewer than four, toes on each foot, and well-developed, prominent canine teeth, cheek teeth (premolars, and molars) that generally have cutting edges. The last premolar of the upper jaw and first molar of the lower are termed the carnassials or sectorial teeth. These blade-like teeth occlude (close) with a scissor-like action for shearing and shredding meat. Carnassials are most highly developed in the Felidae and the least developed in the Ursidae. Carnivorans have six incisors and two conical canines in each jaw. The only two exceptions to this are the sea otter (\"Enhydra lutris\"), which has four incisors in the lower jaw, and the sloth bear (\"Melursus ursinus\"), which has four incisors in the upper jaw. The number of molars and premolars is variable between carnivoran species, but all teeth are deeply rooted and are diphyodont. Incisors are retained by carnivorans and the third incisor is commonly large and sharp (canine-like). Carnivorans have either four or five digits on each foot, with the first digit on the forepaws, also known as the dew claw, being vestigial in most species and absent in some.\n\nThe superfamily Canoidea (or suborder Caniformia) – Canidae (wolves, dogs and foxes), Mephitidae (skunks and stink badgers), Mustelidae (weasels, badgers, and otters), Procyonidae (raccoons), Ursidae (bears), Ailuridae (red panda), Otariidae (eared seals), Odobenidae (walrus), and Phocidae (earless seals) (the last three families formerly classified in the superfamily Pinnipedia) and the extinct family Amphicyonidae (bear-dogs) – are characterized by having nonchambered or partially chambered auditory bullae, nonretractable claws, and a well-developed baculum. Most species are rather plain in coloration, lacking the flashy spotted or rosetted coats like many species of felids and viverrids have. This is because Canoidea tend to range in the temperate and subarctic biomes, although Mustelidae and Procyonidae have a few tropical species. Most are terrestrial, although a few species, like procyonids, are arboreal. All families except the Canidae and a few species of Mustelidae are plantigrade. Diet is varied and most tend to be omnivorous to some degree, and thus the carnassial teeth are less specialized. Canoidea have more premolars and molars in an elongated skull.\n\nThe superfamily Feloidea (or suborder Feliformia)– Felidae (cats), Prionodontidae (Asiatic linsangs), Herpestidae (mongooses), Hyaenidae (hyenas), Viverridae (civets), and Eupleridae (Malagasy carnivorans), as well as the extinct family Nimravidae (paleofelids) – often have spotted, rosetted or striped coats, and tend to be more brilliantly colored than their Canoidean counterparts. This is because these species tend to range in tropical habitats, although a few species do inhabit temperate and subarctic habitats. Many are arboreal or semiarboreal, and the majority are digitigrade. Diet tends to be more strictly carnivorous, especially in the family Felidae. They have fewer teeth and shorter skulls, with much more specialized carnassials meant for shearing meat. Feliformia claws are often retractile, or rarely, semiretractile. The terminal phalanx, with the claw attached, folds back in the forefoot into a sheath by the outer side of the middle phalanx of the digit, and is retained in this position when at rest by a strong elastic ligament. In the hindfoot, the terminal joint or phalanx is retracted on to the top, and not the side of the middle phalanx. Deep flexor muscles straighten the terminal phalanges, so the claws protrude from their sheaths, and the soft \"velvety\" paw becomes suddenly converted into a formidable weapon. The habitual retraction of the claws preserves their points from wear.\n\nThe superfamily Pinnipedia (walruses, seals, and sea lions), now considered to be part of Caniformia, are medium to large (to 6.5 m) aquatic mammals. Being homeothermic (warm-blooded) marine mammals, pinnipeds need a low surface area to body mass ratio. Otherwise, they would suffer from excessive heat loss due to water's high capacity for heat conduction. The body is usually insulated with a thick layer of fat called blubber and typically covered with hair. The digits are not separate, but connected by a thick web that forms flippers for swimming; thus, the forelimbs and hindlimbs are transformed into paddles. This enables them to dive at extreme depths (600 m for the Weddell seal). They can remain underwater for long periods of time, sometimes an hour or more, but most dives are usually short. The facial region of skull is relatively small, with pinnae very small or lacking, and the vibrissae are well developed. The molariform teeth are mostly homodont and the canines are well developed. The tail is very short or absent, the ears are small or absent as well, and the external genitalia are hidden in slits or depressions in the body.\n\nMembers of Carnivora have a characteristic skull shape with relatively large brains encased in a heavy skull. The skull has a highly developed zygomatic arch just behind the maxilla (common to all mammals and their cynodont forebears), and they have ossified external auditory bullae. Feloidea have a two-chambered auditory bulla. In addition to allowing extra room for the passage of muscles to work the lower jaw, the zygomatic arch also allows for differentiation of separate muscle groups to be involved in biting and chewing. Masseters attach from the dentary (specifically, the masseteric fossa) to the zygomatic arch and onto the maxilla in front of the arch, providing crushing force. The temporalis attaches from the dentary (specifically, the coronoid process) to the side of the braincase, providing torque about the axis of jaw articulation.\n\nIn comparing the skulls of carnivores and herbivores, it can be seen that the shearing force of the temporalis is somewhat more important to carnivores, which have more room on the braincase (this is not unrelated to carnivoran intelligence) and commonly develop a sagittal crest (running from posterior to anterior on the skull), providing yet additional room for temporalis attachment. Carnivoran jaws can only move on a vertical axis, in an up-and-down motion, and cannot move from side-to-side. The jaw joint in carnivores tends to lie within the plane of tooth occlusion, an arrangement that further emphasizes shearing (as in a pair of scissors). In herbivores, the crushing force of the masseters is relatively more important than is shearing. The jaw joint is generally well above the plane of tooth occlusion, allowing extra room for masseteric attachment on the dentary and causing the rotation of the lower jaw to be translated into straight-ahead crushing force between the teeth of the upper and lower jaws.\n\nDentition relates to the arrangement of teeth in the mouth, with the dental notation for the upper-jaw teeth using the upper-case letters I to denote incisors, C for canines, P for premolars, and M for molars, and the lower-case letters i, c, p and m to denote the mandible teeth. Teeth are numbered using one side of the mouth and from the front of the mouth to the back. In carnivores, the upper premolar P4 and the lower molar m1 form the carnassials that are used together in a scissor-like action to shear the muscle and tendon of prey.\n\nCarnivora have a simple stomach adapted to digest primarily meat, as compared to the elaborate digestive systems of herbivorous animals, which are necessary to break down tough, complex plant fibers. The caecum is either absent or short and simple, and the colon is not sacculated or much wider than the small intestine. Most species of Carnivora are, to some degree, omnivorous, except the Felidae and Pinnipedia, which are obligate carnivores. Most have highly developed senses, especially vision and hearing, and often a highly acute sense of smell in many species, such as in the Canoidea. They are excellent runners: some are long-distance runners, but more commonly are sprinters. Even bears and raccoons, although seemingly slow and clumsy, are capable of remarkable bursts of speed.\n\nCarnivorans include carnivores, omnivores, and even a few primarily herbivorous species, such as the giant panda and the binturong. Important teeth for carnivorans are the large, slightly recurved canines, used to dispatch prey, and the carnassial complex, used to rend meat from bone and slice it into digestible pieces. Dogs have molar teeth behind the carnassials for crushing bones, but cats have only a greatly reduced, functionless molar behind the carnassial in the upper jaw. Cats will strip bones clean but will not crush them to get the marrow inside. Omnivores, such as bears and raccoons, have developed blunt, molar-like carnassials. Carnassials are a key adaptation for terrestrial vertebrate predation; all other placental orders are primarily herbivores, insectivores, or aquatic.\n\nMost male carnivorans have a baculum, although it is relatively short in felids, and absent in hyenas. Carnivorans tend to produce a single litter annually, but some produce multiple litters a year, and larger carnivorans, like bears, have gaps of 2–3 yr between litters. The average gestation period lies between 50 and 115 days, although the ursids and mustelids have delayed implantation, thus extending the gestation period six to 9 months beyond the normal period. Litter sizes are usually small, ranging from one to 13 young, which are born with underdeveloped eyes and ears. In most species, the mother has exclusive or at least primary care of the offspring. Many species of carnivorans are solitary, but a few are gregarious.\n\nCarnivorans evolved from members of the family Miacidae (miacids, now recognized as paraphyletic). The transition from Miacidae to Carnivora was a general trend in the middle and late Eocene, with taxa from both North America and Eurasia involved. The divergence of carnivorans from other miacids, as well as the divergence of the two clades within Carnivora, Caniformia and Feliformia, is now inferred to have happened in the middle Eocene, about 42 million years ago (mya).\n\nTraditionally, the extinct family Viverravidae (viverravids) had been thought to be the earliest carnivorans, with fossil records first appearing in the Paleocene of North America about 60 mya, but recently described evidence from cranial morphology now places them outside the order Carnivora.\n\nThe Miacidae are not a monophyletic group, but a paraphyletic array of stem taxa. Today, Carnivora is restricted to the crown group, Carnivora and miacoids are grouped in the clade Carnivoramorpha, and the miacoids are regarded as basal carnivoramorphs. Based on dental features and braincase sizes, it is now known that Carnivora must have evolved from a form even more primitive than Creodonta, and thus these two orders may not even be sister groups. The Carnivora, Creodonta, Pholidota, and a few other extinct orders are informally grouped together in the clade Ferae. Older classification schemes divided the order into two suborders: Fissipedia (which included the families of primarily land Carnivora) and Pinnipedia (which included the true seals, eared seals, and walrus). However, it is now recognized that the Fissipedia is a paraphyletic group and that the pinnipeds were not the sister group to the fissipeds but rather had arisen from among them.\n\nCarnivora are generally divided into the suborders Feliformia (cat-like) and Caniformia (dog-like), the latter of which includes the pinnipeds. The pinnipeds are part of a clade, known as the Arctoidea, which also includes the Ursidae (bears) and the superfamily Musteloidea. The Musteloidea in turn consists of the Mustelidae (mustelids: weasels), Procyonidae (procyonids: raccoons), Mephitidae (skunks) and \"Ailurus\" (red panda). The oldest caniforms are the \"Miacis\" species \"Miacis cognitus\", the Amphicyonidae (bear-dogs) such as \"Daphoenus\", and \"Hesperocyon\" (of the family Canidae, subfamily Hesperocyoninae). Hesperocyonine canids first appeared in North America, and the earliest species is currently dated at 39.74 mya, but they were not represented in Europe until well into the Miocene, and not into Asia and Africa until the Pliocene. \"Miacis\" and Amphicyonidae were the first of the caniforms to split from the others and are sometimes considered to be sister groups to Ursidae, but the exact closeness of Amphicyonidae and Ursidae, as well as Arctoidae to Ursidae, is still uncertain. The Canidae (wolves, coyotes, jackals, foxes and dogs) are generally considered to be the sister group to Arctoidea. The Ursidae first occur in North America in the Late Eocene (ca. 38 mya) as the very small and graceful \"Parictis\" that had a skull only 7 cm long. Like the canids, this family does not appear in Eurasia and Africa until the Miocene. The other caniform families Amphicyonidae, Mustelidae and Procyonidae occur in both the Old World and the New World by the Late Eocene and Early Oligocene.\n\nThe ancestor of all Feliformia evolved from the Caniformia-Feliformia split. \"Nandinia\", the African palm civet, seems to be the most primitive of all the feliforms and the very first to split from the others. The Asiatic linsangs of the genus \"Prionodon\" (traditionally placed in the Viverridae) form a family of their own, as some recent studies indicate that \"Prionodon\" is actually the closest living relative to the cats. The Nimravidae are sometimes seen as the most basal of all feliforms and the first to split from the others, but there is a possibility that Nimravidae might not even belong within the order, and therefore its position as a clade within Carnivora is currently unstable. Other studies indicate that the barbourofelids form a separate family, which is closely related to the true felids instead of being related to the nimravids. Recognizable nimravid fossils date from the late Eocene (37 mya), from the Chadronian White River Carnivora Formation at Flagstaff Rim, Wyoming. Nimravid diversity appears to have peaked about 28 mya. The hypercarnivorous (strictly meat-eating) nimravid feliforms were extinct in North America after 26 mya and felids did not arrive in North America until the early middle Miocene (16 mya).\n\nIt has been suggested that canids evolved hypercarnivorous morphologies because feliforms were absent during this period (the \"cat-gap\", 26-18.5 mya), however recent data do not support this hypothesis. Hypercarnivore feliforms (felids and nimravids) occupied an area that canids did not and where felids, nimravids, and hypercarnivorous creodonts are found. Hypercarnivorous canids were present before the disappearance of the nimravids, and all became extinct before the appearance of felids. Following the extinction of nimravids, only three taxa originated, two of which were relatively small in body size. Disparity increased during the \"cat-gap\" even with the extinction of the hypercarnivorous extremes. This was due to the extinction of morphological intermediates, and because carnivorans began to occupy hypocarnivorous (nonmeat-specialist) morphospace for the first time in North America. Procyonids did not arrive in North America until the early Miocene, and \"modern\" ursids (e.g., Ursinae), did not arrive until the late Miocene. Extinct lineages of Ursidae were present in North America from the late Eocene through the Miocene and amphicyonids (bear-dogs) were present during this period as well, but occupied a morphospace generally shared with canids and not in close proximity to ursids. A large question remains as to why there was a progressive decline in hypercarnivorous carnivoramorphans during the late Oligocene/early Miocene. During this period all hypercarnivorous forms disappeared from the fossil record, including hypercarnivorous feliforms, canids, and mustelids. One possible explanation is climate change. Earth was gradually cooling after the late Paleocene, and over a period spanning the Eocene/Oligocene boundary, a dramatic climatic cooling event occurred.\n\nA recent study has finally resolved the exact position of \"Ailurus\": the red panda is neither a procyonid nor an ursid, but forms a monotypic family, with the other musteloids as its closest living relatives. The same study also showed that the mustelids are not a primitive family, as was once thought. Their small body size is a secondary trait—the primitive body form of the arctoids was large, not small. Recent molecular studies also suggest that the endemic Carnivora of Madagascar, including three genera usually classed with the civets and four genera of mongooses classed with the Herpestidae, are all descended from a single ancestor. They form a single sister taxon to the Herpestidae. The hyenas are also closely related to this clade.\n\nWhen order Carnivora was first discovered and named, there were only 5 families\n\nThe most common modern classification scheme divides the Carnivora into sixteen living and a number of extinct families, as follows:\n\nThe cladogram is based on Flynn (2005).\n\n\n"}
{"id": "5222", "url": "https://en.wikipedia.org/wiki?curid=5222", "title": "Colombia", "text": "Colombia\n\nColombia ( or ; ), officially the Republic of Colombia (), is a sovereign state largely situated in the northwest of South America, with territories in Central America. Colombia shares a border to the northwest with Panama, to the east with Venezuela and Brazil and to the south with Ecuador and Peru. It shares its maritime limits with Costa Rica, Nicaragua, Honduras, Jamaica, Haiti and the Dominican Republic. It is a unitary, constitutional republic comprising thirty-two departments. The territory of what is now Colombia was originally inhabited by indigenous peoples, with as most advanced the Muisca, Quimbaya and the Tairona.\n\nThe Spanish set foot on Colombian soil for the first time in 1499 and in the first half of the 16th century initiated a period of conquest and colonization, ultimately creating the New Kingdom of Granada, with as capital Santafé de Bogotá. Independence from Spain was acquired in 1819, but by 1830 the \"Gran Colombia\" Federation was dissolved. What is now Colombia and Panama emerged as the Republic of New Granada. The new nation experimented with federalism as the Granadine Confederation (1858), and then the United States of Colombia (1863), before the Republic of Colombia was finally declared in 1886. Panama seceded in 1903. Since the 1960s, the country has suffered from an asymmetric low-intensity armed conflict, which escalated in the 1990s but then decreased from 2005 onward. Colombia is one of the most ethnically and linguistically diverse countries in the world, and thereby possesses a rich cultural heritage. The urban centres are mostly located in the highlands of the Andes mountains.\n\nColombian territory also encompasses Amazon rainforest, tropical grassland and both Caribbean and Pacific coastlines. Ecologically, it is one of the world's 17 megadiverse countries, and the most densely biodiverse of these per square kilometer. Colombia is a middle power and a regional actor with the fourth-largest economy in Latin America, is part of the CIVETS group of six leading emerging markets and is a member of the UN, the WTO, the OAS, the Pacific Alliance, and other international organizations. Colombia has a diversified economy with macroeconomic stability and favorable growth prospects in the long run.\n\nThe name \"Colombia\" is derived from the last name of Christopher Columbus (, ). It was conceived by the Venezuelan revolutionary Francisco de Miranda as a reference to all the New World, but especially to those portions under Spanish and Portuguese rule. The name was later adopted by the Republic of Colombia of 1819, formed from the territories of the old Viceroyalty of New Granada (modern-day Colombia, Panama, Venezuela, Ecuador, and northwest Brazil).\n\nWhen Venezuela, Ecuador and Cundinamarca came to exist as independent states, the former Department of Cundinamarca adopted the name \"Republic of New Granada\". New Granada officially changed its name in 1858 to the Granadine Confederation. In 1863 the name was again changed, this time to United States of Colombia, before finally adopting its present name – the Republic of Colombia – in 1886.\n\nTo refer to this country, the Colombian government uses the terms \"Colombia\" and \"República de Colombia\".\n\nOwing to its location, the present territory of Colombia was a corridor of early human migration from Mesoamerica and the Caribbean to the Andes and Amazon basin. The oldest archaeological finds are from the Pubenza and El Totumo sites in the Magdalena Valley southwest of Bogotá. These sites date from the Paleoindian period (18,000–8000 BCE). At Puerto Hormiga and other sites, traces from the Archaic Period (~8000–2000 BCE) have been found. Vestiges indicate that there was also early occupation in the regions of El Abra and Tequendama in Cundinamarca. The oldest pottery discovered in the Americas, found at San Jacinto, dates to 5000–4000 BCE.\nIndigenous people inhabited the territory that is now Colombia by 12,500 BCE. Nomadic hunter-gatherer tribes at the El Abra, Tibitó and Tequendama sites near present-day Bogotá traded with one another and with other cultures from the Magdalena River Valley. Between 5000 and 1000 BCE, hunter-gatherer tribes transitioned to agrarian societies; fixed settlements were established, and pottery appeared. Beginning in the 1st millennium BCE, groups of Amerindians including the Muisca, Quimbaya, and Tairona developed the political system of \"cacicazgos\" with a pyramidal structure of power headed by caciques. The Muisca inhabited mainly the area of what is now the Departments of Boyacá and Cundinamarca high plateau (\"Altiplano Cundiboyacense\") where they formed the Muisca Confederation. They farmed maize, potato, quinoa and cotton, and traded gold, emeralds, blankets, ceramic handicrafts, coca and especially rock salt with neighboring nations. The Tairona inhabited northern Colombia in the isolated mountain range of Sierra Nevada de Santa Marta. The Quimbaya inhabited regions of the Cauca River Valley between the Western and Central Ranges of the Colombian Andes. Most of the Amerindians practiced agriculture and the social structure of each indigenous community was different. Some groups of indigenous people such as the Caribs lived in a state of permanent war, but others had less bellicose attitudes. The Incas expanded their empire onto the southwest part of the country.\n\nAlonso de Ojeda (who had sailed with Columbus) reached the Guajira Peninsula in 1499. Spanish explorers, led by Rodrigo de Bastidas, made the first exploration of the Caribbean coast in 1500. Christopher Columbus navigated near the Caribbean in 1502. In 1508, Vasco Núñez de Balboa accompanied an expedition to the territory through the region of Gulf of Urabá and they founded the town of Santa María la Antigua del Darién in 1510, the first stable settlement on the continent. \n\nSanta Marta was founded in 1525, and Cartagena in 1533. Spanish conquistador Gonzalo Jiménez de Quesada led an expedition to the interior in April 1536, and christened the districts through which he passed \"New Kingdom of Granada\". In August 1538, he founded provisionally its capital near the Muisca cacicazgo of Bacatá, and named it \"Santa Fe\". The name soon acquired a suffix and was called Santa Fe de Bogotá. Two other notable journeys by early conquistadors to the interior took place in the same period. Sebastián de Belalcázar, conqueror of Quito, traveled north and founded Cali, in 1536, and Popayán, in 1537; from 1536 to 1539, German conquistador Nikolaus Federmann crossed the Llanos Orientales and went over the Cordillera Oriental in a search for El Dorado, the \"city of gold\". The legend and the gold would play a pivotal role in luring the Spanish and other Europeans to New Granada during the 16th and 17th centuries.\n\nThe conquistadors made frequent alliances with the enemies of different indigenous communities. Indigenous allies were crucial to conquest, as well as to creating and maintaining empire. Indigenous peoples in New Granada experienced a decline in population due to conquest as well as Eurasian diseases, such as smallpox, to which they had no immunity. With the risk that the land was deserted, the Spanish Crown sold properties to all persons interested in colonise territories creating large farms and possession of mines.\n\nIn the 16th century, the nautical science in Spain reached a great development thanks to numerous scientific figures of the Casa de Contratación and nautical science was an essential pillar of the Iberian expansion.\n\nIn 1542, the region of New Granada, along with all other Spanish possessions in South America, became part of the Viceroyalty of Peru, with its capital at Lima. In 1547, New Granada became the Captaincy-General of New Granada within the viceroyalty.\n\nIn 1549, the Royal Audiencia was created by a royal decree, and New Granada was ruled by the Royal Audience of Santa Fe de Bogotá, which at that time comprised the provinces of Santa Marta, Rio de San Juan, Popayán, Guayana and Cartagena. But important decisions were taken from the colony to Spain by the Council of the Indies.\nIn the 16th century, Europeans began to bring slaves from Africa. Spain was the only European power that could not establish factories in Africa to purchase slaves and therefore the Spanish empire relied on the asiento system, awarding merchants (mostly from Portugal, France, England and the Dutch Empire) the license to trade enslaved people to their overseas territories. Also there were people who defended the human rights and freedoms of oppressed peoples. The indigenous peoples could not be enslaved because they were legally subjects of the Spanish Crown and to protect the indigenous peoples, several forms of land ownership and regulation were established: \"resguardos\", \"encomiendas\" and \"haciendas\".\nIn 1717 the Viceroyalty of New Granada was originally created, and then it was temporarily removed, to finally be reestablished in 1739. The Viceroyalty had Santa Fé de Bogotá as its capital. This Viceroyalty included some other provinces of northwestern South America which had previously been under the jurisdiction of the Viceroyalties of New Spain or Peru and correspond mainly to today's Venezuela, Ecuador and Panama. So, Bogotá became one of the principal administrative centers of the Spanish possessions in the New World, along with Lima and Mexico City, though it remained somewhat backward compared to those two cities in several economic and logistical ways.\n\nAfter Great Britain declared war on Spain in 1739, Cartagena quickly became the British forces' top target but an upset Spanish victory during the War of Jenkins' Ear, a war with Great Britain for economic control of the Caribbean, cemented Spanish dominance in the Caribbean until the Seven Years' War.\n\nThe 18th-century priest, botanist and mathematician José Celestino Mutis was delegated by Viceroy Antonio Caballero y Góngora to conduct an inventory of the nature of the New Granada. Started in 1783, this became known as the Royal Botanical Expedition to New Granada which classified plants, wildlife and founded the first astronomical observatory in the city of Santa Fe de Bogotá. In July 1801 the Prussian scientist Alexander von Humboldt reached Santa Fe de Bogotá where he met with Mutis. In addition, historical figures in the process of independence in New Granada emerged from the expedition as the astronomer Francisco José de Caldas, the scientist Francisco Antonio Zea, the zoologist Jorge Tadeo Lozano and the painter Salvador Rizo.\n\nSince the beginning of the periods of conquest and colonization, there were several rebel movements against Spanish rule, but most were either crushed or remained too weak to change the overall situation. The last one that sought outright independence from Spain sprang up around 1810, following the independence of St. Domingue (present-day Haiti) in 1804, which provided some support to an eventual leader of this rebellion: Simón Bolívar. Francisco de Paula Santander also would play a decisive role.\n\nA movement was initiated by Antonio Nariño, who opposed Spanish centralism and led the opposition against the Viceroyalty. Cartagena became independent in November 1811. In 1811 the United Provinces of New Granada were proclaimed, headed by Camilo Torres Tenorio. The emergence of two distinct ideological currents among the patriots (federalism and centralism) gave rise to a period of instability. Shortly after the Napoleonic Wars ended, Ferdinand VII, recently restored to the throne in Spain, unexpectedly decided to send military forces to retake most of northern South America. The viceroyalty was restored under the command of Juan Sámano, whose regime punished those who participated in the patriotic movements, ignoring the political nuances of the juntas. The retribution stoked renewed rebellion, which, combined with a weakened Spain, made possible a successful rebellion led by the Venezuelan-born Simón Bolívar, who finally proclaimed independence in 1819. The pro-Spanish resistance was defeated in 1822 in the present territory of Colombia and in 1823 in Venezuela.\n\nThe territory of the Viceroyalty of New Granada became the Republic of Colombia, organized as a union of the current territories of Colombia, Panama, Ecuador, Venezuela, parts of Guyana and Brazil and north of Marañón River. The Congress of Cúcuta in 1821 adopted a constitution for the new Republic. Simón Bolívar became the first President of Colombia, and Francisco de Paula Santander was made Vice President. However, the new republic was unstable and three countries emerged from the collapse of Gran Colombia in 1830 (New Granada, Ecuador and Venezuela).\n\nColombia was the first constitutional government in South America, and the Liberal and Conservative parties, founded in 1848 and 1849 respectively, are two of the oldest surviving political parties in the Americas. Slavery was abolished in the country in 1851.\n\nInternal political and territorial divisions led to the dissolution of Gran Colombia in 1830. The so-called \"Department of Cundinamarca\" adopted the name \"New Granada\", which it kept until 1858 when it became the \"Confederación Granadina\" (Granadine Confederation). After a two-year civil war in 1863, the \"United States of Colombia\" was created, lasting until 1886, when the country finally became known as the Republic of Colombia. Internal divisions remained between the bipartisan political forces, occasionally igniting very bloody civil wars, the most significant being the Thousand Days' War (1899–1902).\n\nThe United States of America's intentions to influence the area (especially the Panama Canal construction and control) led to the separation of the Department of Panama in 1903 and the establishment of it as a nation. The United States paid Colombia $25,000,000 in 1921, seven years after completion of the canal, for redress of President Roosevelt's role in the creation of Panama, and Colombia recognized Panama under the terms of the Thomson–Urrutia Treaty. Colombia and Peru went to war because of territory disputes far in the Amazon basin. The war ended with a peace deal brokered by the League of Nations. The League finally awarded the disputed area to Colombia in June 1934.\n\nSoon after, Colombia achieved some degree of political stability, which was interrupted by a bloody conflict that took place between the late 1940s and the early 1950s, a period known as \"La Violencia\" (\"The Violence\"). Its cause was mainly mounting tensions between the two leading political parties, which subsequently ignited after the assassination of the Liberal presidential candidate Jorge Eliécer Gaitán on 9 April 1948. The ensuing riots in Bogotá, known as El Bogotazo, spread throughout the country and claimed the lives of at least 180,000 Colombians.\n\nColombia entered the Korean War when Laureano Gómez was elected president. It was the only Latin American country to join the war in a direct military role as an ally of the United States. Particularly important was the resistance of the Colombian troops at Old Baldy.\n\nThe violence between the two political parties decreased first when Gustavo Rojas deposed the President of Colombia in a coup d'état and negotiated with the guerrillas, and then under the military junta of General Gabriel París.\n\nAfter Rojas' deposition, the Colombian Conservative Party and Colombian Liberal Party agreed to create the \"National Front\", a coalition which would jointly govern the country. Under the deal, the presidency would alternate between conservatives and liberals every 4 years for 16 years; the two parties would have parity in all other elective offices. The National Front ended \"La Violencia\", and National Front administrations attempted to institute far-reaching social and economic reforms in cooperation with the Alliance for Progress. Despite the progress in certain sectors, many social and political problems continued, and guerrilla groups were formally created such as the FARC, the ELN and the M-19 to fight the government and political apparatus.\n\nSince the 1960s, the country has suffered from an asymmetric low-intensity armed conflict between the government forces, left-wing guerrilla groups and right-wing paramilitaries. The conflict escalated in the 1990s, mainly in remote rural areas. Since the beginning of the armed conflict, human rights defenders have fought for the respect for human rights, despite staggering opposition. Several guerrillas' organizations decided to demobilize after peace negotiations in 1989–1994.\n\nThe United States has been heavily involved in the conflict since its beginnings, when in the early 1960s the U.S. government encouraged the Colombian military to attack leftist militias in rural Colombia. This was part of the U.S. fight against communism.\n\nOn 4 July 1991, a new Constitution was promulgated. The changes generated by the new constitution are viewed as positive by Colombian society.\n\nThe administration of President Álvaro Uribe (2002–10), adopted the democratic security policy which included an integrated counter-terrorism and counter-insurgency campaign. The Government economic plan also promoted confidence in investors. As part of a controversial peace process the AUC (right-wing paramilitaries) as a formal organization had ceased to function. In February 2008, millions of Colombians demonstrated against FARC and other outlawed groups.\n\nAfter peace negotiations in Cuba, the Colombian government of President Juan Manuel Santos and guerrilla of FARC-EP announced consensus on a 6-point plan towards peace. The first peace accord was submitted to voters in a national referendum and was rejected with 50.2% voting against it and 49.8% voting in favor, on a 37.4% turnout. Afterward, the Colombian government and the FARC signed a revised peace deal in November 2016, which the Colombian congress approved. In 2016, President Santos was awarded the Nobel Peace Prize. The Government began a process of assistance, attention and comprehensive reparation for victims of conflict. Colombia shows modest progress in the struggle to defend human rights, as expressed by HRW.\n\nIn terms of international relations, Colombia and Venezuela have agreed to restore diplomatic relations. Latin America with a long memory of U.S. interventions such as the infamous Operation Condor rejects US military threat against Venezuela. Colombia's Foreign Ministry said that all efforts to resolve Venezuela's crisis should be peaceful and respect its sovereignty. Colombia with a very clean electricity generation matrix reaffirms its support for the Paris Climate Agreement.\n\nThe geography of Colombia is characterized by its six main natural regions that present their own unique characteristics, from the Andes mountain range region shared with Ecuador and Venezuela; the Pacific coastal region shared with Panama and Ecuador; the Caribbean coastal region shared with Venezuela and Panama; the \"Llanos\" (plains) shared with Venezuela; the Amazon Rainforest region shared with Venezuela, Brazil, Peru and Ecuador; to the insular area, comprising islands in both the Atlantic and Pacific oceans.\n\nColombia is bordered to the northwest by Panama; to the east by Venezuela and Brazil; to the south by Ecuador and Peru; it established its maritime boundaries with neighboring countries through seven agreements on the Caribbean Sea and three on the Pacific Ocean. It lies between latitudes 12°N and 4°S, and longitudes 67° and 79°W.\n\nPart of the Ring of Fire, a region of the world subject to earthquakes and volcanic eruptions, In the interior of Colombia the Andes are the prevailing geographical feature. Most of Colombia's population centers are located in these interior highlands. Beyond the Colombian Massif (in the south-western departments of Cauca and Nariño) these are divided into three branches known as \"cordilleras\" (mountain ranges): the Cordillera Occidental, running adjacent to the Pacific coast and including the city of Cali; the Cordillera Central, running between the Cauca and Magdalena River valleys (to the west and east respectively) and including the cities of Medellín, Manizales, Pereira and Armenia; and the Cordillera Oriental, extending north east to the Guajira Peninsula and including Bogotá, Bucaramanga and Cúcuta.\n\nPeaks in the Cordillera Occidental exceed , and in the Cordillera Central and Cordillera Oriental they reach . At , Bogotá is the highest city of its size in the world.\n\nEast of the Andes lies the savanna of the \"Llanos\", part of the Orinoco River basin, and, in the far south east, the jungle of the Amazon rainforest. Together these lowlands comprise over half Colombia's territory, but they contain less than 6% of the population. To the north the Caribbean coast, home to 21.9% of the population and the location of the major port cities of Barranquilla and Cartagena, generally consists of low-lying plains, but it also contains the Sierra Nevada de Santa Marta mountain range, which includes the country's tallest peaks (Pico Cristóbal Colón and Pico Simón Bolívar), and the La Guajira Desert. By contrast the narrow and discontinuous Pacific coastal lowlands, backed by the Serranía de Baudó mountains, are sparsely populated and covered in dense vegetation. The principal Pacific port is Buenaventura.\n\nThe main rivers of Colombia are Magdalena, Cauca, Guaviare, Atrato, Meta, Putumayo and Caquetá. Colombia has four main drainage systems: the Pacific drain, the Caribbean drain, the Orinoco Basin and the Amazon Basin. The Orinoco and Amazon Rivers mark limits with Colombia to Venezuela and Peru respectively.\n\nProtected areas and the \"National Park System\" cover an area of about and account for 12.77% of the Colombian territory. Compared to neighboring countries, rates of deforestation in Colombia are still relatively low. Colombia is the sixth country in the world by magnitude of total renewable freshwater supply, and still has large reserves of freshwater.\n\nThe climate of Colombia is characterized for being tropical presenting variations within six natural regions and depending on the altitude, temperature, humidity, winds and rainfall. The diversity of climate zones in Colombia is characterized for having tropical rainforests, savannas, steppes, deserts and mountain climate.\n\nMountain climate is one of the unique features of the Andes and other high altitude reliefs where climate is determined by elevation. Below in elevation is the warm altitudinal zone, where temperatures are above . About 82.5% of the country's total area lies in the warm altitudinal zone. The temperate climate altitudinal zone located between ) is characterized for presenting an average temperature ranging between . The cold climate is present between and the temperatures vary between . Beyond the cold land lie the alpine conditions of the forested zone and then the treeless grasslands of the páramos. Above , where temperatures are below freezing, the climate is glacial, a zone of permanent snow and ice.\n\nColombia is one of the megadiverse countries in biodiversity, ranking first in bird species. As for plants, the country has between 40,000 and 45,000 plant species, equivalent to 10 or 20% of total global species, this is even more remarkable given that Colombia is considered a country of intermediate size. Colombia is the second most biodiverse country in the world, lagging only after Brazil which is approximately 7 times bigger.\n\nColombia is the country in the planet more characterized by a high biodiversity, with the highest rate of species by area unit worldwide and it has the largest number of endemisms (species that are not found naturally anywhere else) of any country. About 10% of the species of the Earth live in Colombia, including over 1,900 species of bird, more than in Europe and North America combined, Colombia has 10% of the world’s mammals species, 14% of the amphibian species and 18% of the bird species of the world.\n\nColombia has about 2,000 species of marine fish and is the second most diverse country in freshwater fish. Colombia is the country with more endemic species of butterflies, number 1 in terms of orchid species and approximately 7,000 species of beetles. Colombia is second in the number of amphibian species and is the third most diverse country in reptiles and palms. There are about 1,900 species of mollusks and according to estimates there are about 300,000 species of invertebrates in the country. In Colombia there are 32 terrestrial biomes and 314 types of ecosystems.\n\nThe government of Colombia takes place within the framework of a presidential participatory democratic republic as established in the Constitution of 1991. In accordance with the principle of separation of powers, government is divided into three branches: the executive branch, the legislative branch and the judicial branch.\n\nAs the head of the executive branch, the President of Colombia serves as both head of state and head of government, followed by the Vice President and the Council of Ministers. The president is elected by popular vote to serve four-year term (In 2015, Colombia’s Congress approved the repeal of a 2004 constitutional amendment that eliminated the one-term limit for presidents). At the provincial level executive power is vested in department governors, municipal mayors and local administrators for smaller administrative subdivisions, such as \"corregimientos\" or \"comunas\". All regional elections are held one year and five months after the presidential election.\n\nThe legislative branch of government is represented nationally by the Congress, a bicameral institution comprising a 166-seat Chamber of Representatives and a 102-seat Senate. The Senate is elected nationally and the Chamber of Representatives is elected in electoral districts. Members of both houses are elected to serve four-year terms two months before the president, also by popular vote.\n\nThe judicial branch is headed by four high courts, consisting of the Supreme Court which deals with penal and civil matters, the Council of State, which has special responsibility for administrative law and also provides legal advice to the executive, the Constitutional Court, responsible for assuring the integrity of the Colombian constitution, and the Superior Council of Judicature, responsible for auditing the judicial branch. Colombia operates a system of civil law, which since 2005 has been applied through an adversarial system.\n\nDespite a number of controversies, the democratic security policy has ensured that former President Uribe remained popular among Colombian people, with his approval rating peaking at 76%, according to a poll in 2009. However, having served two terms, he was constitutionally barred from seeking re-election in 2010. In the run-off elections on 20 June 2010 the former Minister of defense Juan Manuel Santos won with 69% of the vote against the second most popular candidate, Antanas Mockus. A second round was required since no candidate received over the 50% winning threshold of votes. Santos won nearly 51% of the vote in second-round elections on 15 June 2014, beating right-wing rival Óscar Iván Zuluaga, who won 45%. His term as Colombia's president runs for four years beginning 7 August 2014.\n\nThe foreign affairs of Colombia are headed by the President, as head of state, and managed by the Minister of Foreign Affairs. Colombia has diplomatic missions in all continents.\n\nColombia was one of the 4 founding members of the Pacific Alliance, which is a political, economic and co-operative integration mechanism that promotes the free circulation of goods, services, capital and persons between the members, as well as a common stock exchange and joint embassies in several countries. Colombia is also a member of the United Nations, the World Trade Organization, the Organization of American States, the Organization of Ibero-American States, the Union of South American Nations and the Andean Community of Nations. Colombia is a global partner of NATO. Colombia is currently in the accession process with the OECD.\n\nThe executive branch of government is responsible for managing the defense of Colombia, with the President commander-in-chief of the armed forces. The Ministry of Defence exercises day-to-day control of the military and the Colombian National Police. Colombia has 455,461 active military personnel. And in 2016 3.4% of the country's GDP went towards military expenditure, placing it 24th in the world. Colombia's armed forces are the largest in Latin America, and it is the second largest spender on its military after Brazil.\n\nThe Colombian military is divided into three branches: the National Army of Colombia; the Colombian Air Force; and the Colombian Navy. The National Police functions as a gendarmerie, operating independently from the military as the law enforcement agency for the entire country. Each of these operates with their own intelligence apparatus separate from the National Intelligence Directorate (DNI, in Spanish).\n\nThe National Army is formed by divisions, brigades, special brigades and special units; the Colombian Navy by the Naval Infantry, the Naval Force of the Caribbean, the Naval Force of the Pacific, the Naval Force of the South, the Naval Force of the East, Colombia Coast Guards, Naval Aviation and the Specific Command of San Andres y Providencia; and the Air Force by 15 air units. The National Police has a presence in all municipalities.\n\nColombia is divided into 32 departments and one capital district, which is treated as a department (Bogotá also serves as the capital of the department of Cundinamarca). Departments are subdivided into municipalities, each of which is assigned a municipal seat, and municipalities are in turn subdivided into \"corregimientos\" in rural areas and into \"comunas\" in urban areas. Each department has a local government with a governor and assembly directly elected to four-year terms, and each municipality is headed by a mayor and council. There is a popularly elected local administrative board in each of the \"corregimientos\" or \"comunas\".\n\nIn addition to the capital four other cities have been designated districts (in effect special municipalities), on the basis of special distinguishing features. These are Barranquilla, Cartagena, Santa Marta and Buenaventura. Some departments have local administrative subdivisions, where towns have a large concentration of population and municipalities are near each other (for example in Antioquia and Cundinamarca). Where departments have a low population (for example Amazonas, Vaupés and Vichada), special administrative divisions are employed, such as \"department \"corregimientos\"\", which are a hybrid of a municipality and a \"corregimiento\".\n\nHistorically an agrarian economy, Colombia urbanised rapidly in the 20th century, by the end of which just 15.8% of the workforce were employed in agriculture, generating just 6.8% of GDP; 19.6% of the workforce were employed in industry and 64.6% in services, responsible for 34.0% and 59.2% of GDP respectively. The country's economic production is dominated by its strong domestic demand. Consumption expenditure by households is the largest component of GDP.\n\nColombia's market economy grew steadily in the latter part of the 20th century, with gross domestic product (GDP) increasing at an average rate of over 4% per year between 1970 and 1998. The country suffered a recession in 1999 (the first full year of negative growth since the Great Depression), and the recovery from that recession was long and painful. However, in recent years growth has been impressive, reaching 6.9% in 2007, one of the highest rates of growth in Latin America. According to International Monetary Fund estimates, in 2012 Colombia's GDP (PPP) was US$500 billion (28th in the world and third in South America).\n\nTotal government expenditures account for 28.7 percent of the domestic economy. Public debt equals 41 percent of gross domestic product. A strong fiscal climate was reaffirmed by a boost in bond ratings. Annual inflation closed 2016 at 5.75% YoY (vs. 6.77% YoY in 2015). The average national unemployment rate in 2016 was 9.2%, although the informality is the biggest problem facing the labour market (the income of formal workers climbed 24.8% in 5 years while labor incomes of informal workers rose only 9%). Colombia has Free trade Zone (FTZ), such as Zona Franca del Pacifico, located in the Valle del Cauca, one of the most striking areas for foreign investment.\n\nThe financial sector has grown favorably due to good liquidity in the economy, the growth of credit and the positive performance of the Colombian economy. The Colombian Stock Exchange through the Latin American Integrated Market (MILA) offers a regional market to trade equities. Colombia is now one of only three economies with a perfect score on the strength of legal rights index, according to the World Bank.\n\nThe electricity production in Colombia comes mainly from renewable energy sources. 69.97% is obtained from the hydroelectric generation. Colombia's commitment to renewable energy was recognized in the 2014 \"Global Green Economy Index (GGEI)\", ranking among the top 10 nations in the world in terms of greening efficiency sectors.\n\nColombia is rich in natural resources, and its main exports include mineral fuels, oils, distillation products, fruit and other agricultural products, sugars and sugar confectionery, food products, plastics, precious stones, metals, forest products, chemical goods, pharmaceuticals, vehicles, electronic products, electrical equipments, perfumery and cosmetics, machinery, manufactured articles, textile and fabrics, clothing and footwear, glass and glassware, furniture, prefabricated buildings, military products, home and office material, construction equipment, software, among others. Principal trading partners are the United States, China, the European Union and some Latin American countries.\n\nNon-traditional exports have boosted the growth of Colombian foreign sales as well as the diversification of destinations of export thanks to new free trade agreements.\n\nIn 2016, the National Administrative Department of Statistics (DANE) reported that 28.0% of the population were living below the poverty line, of which 8.5% in \"extreme poverty\". The Government has also been developing a process of financial inclusion within the country's most vulnerable population.\n\nRecent economic growth has led to a considerable increase of new millionaires, including the new entrepreneurs, Colombians with a net worth exceeding US $1 billion.\n\nThe contribution of Travel & Tourism to GDP was USD5,880.3bn (2.0% of total GDP) in 2016. Tourism generated 556,135 jobs (2.5% of total employment) in 2016. Foreign tourist visits were predicted to have risen from 0.6 million in 2007 to 2.98 million in 2015.\n\nColombia has more than 3,950 research groups in science and technology. iNNpulsa, a government body that promotes entrepreneurship and innovation in the country, provides grants to startups, in addition to other services it and institutions like Apps.co provide. Co-working spaces have arisen to serve as communities for startups large and small. Organizations such as the Corporation for Biological Research (CIB) for the support of young people interested in scientific work has been successfully developed in Colombia. The International Center for Tropical Agriculture based in Colombia investigates the increasing challenge of global warming and food security.\n\nImportant inventions related to the medicine have been made in Colombia, such as the first external artificial pacemaker with internal electrodes, invented by the electronics engineer Jorge Reynolds Pombo, invention of great importance for those who suffer from heart failure. Also invented in Colombia were the microkeratome and keratomileusis technique, which form the fundamental basis of what now is known as LASIK (one of the most important techniques for the correction of refractive errors of vision) and the Hakim valve for the treatment of Hydrocephalus, among others. Colombia has begun to innovate in military technology for its army and other armies of the world; especially in the design and creation of personal ballistic protection products, military hardware, military robots, bombs, simulators and radar.\n\nSome leading Colombian scientists are Joseph M. Tohme, researcher recognized for his work on the genetic diversity of food, Manuel Elkin Patarroyo who is known for his groundbreaking work on synthetic vaccines for malaria, Francisco Lopera who discovered the \"Paisa Mutation\" or a type of early-onset Alzheimer's, Rodolfo Llinás known for his study of the intrinsic neurons properties and the theory of a syndrome that had changed the way of understanding the functioning of the brain, Jairo Quiroga Puello recognized for his studies on the characterization of synthetic substances which can be used to fight fungus, tumors, tuberculosis and even some viruses and Ángela Restrepo who established accurate diagnoses and treatments to combat the effects of a disease caused by the \"Paracoccidioides brasiliensis\", among other scientists.\n\nTransportation in Colombia is regulated within the functions of the Ministry of Transport and entities such as the National Roads Institute (INVÍAS) responsible for the Highways in Colombia, the Aerocivil, responsible for civil aviation and airports, the National Infrastructure Agency, in charge of concessions through public–private partnerships, for the design, construction, maintenance, operation, and administration of the transport infrastructure, the General Maritime Directorate (Dimar) has the responsibility of coordinating maritime traffic control along with the Colombian Navy, among others and under the supervision of the Superintendency of Ports and Transport. The road network in Colombia has a length of about 215,000 km of which 23,000 are paved. Rail transportation in Colombia is dedicated almost entirely to freight shipments and the railway network has a length of 1,700 km of potentially active rails. Colombia has 3,960 kilometers of gas pipelines, 4,900 kilometers of oil pipelines, and 2,990 kilometers of refined-products pipelines.\n\nThe target of Colombia’s government is to build 7,000 km of roads for the 2016–2020 period and reduce travel times by 30 per cent and transport costs by 20 per cent. A toll road concession programme will comprise 40 projects, and is part of a larger strategic goal to invest nearly $50bn in transport infrastructure, including: railway systems; making the Magdalena river navigable again; improving port facilities; as well as an expansion of Bogotá’s airport.\n\nWith an estimated 49 million people in 2017, Colombia is the third-most populous country in Latin America, after Brazil and Mexico. It is also home to the third-largest number of Spanish speakers in the world after Mexico and the United States. At the beginning of the 20th century, Colombia's population was approximately 4 million. Since the early 1970s Colombia has experienced steady declines in its fertility, mortality, and population growth rates. The population growth rate for 2015 is estimated to be 0.9%. The total fertility rate was 1.9 births per woman in 2015. About 26.8% of the population were 15 years old or younger, 65.7% were between 15 and 64 years old, and 7.4% were over 65 years old. The proportion of older persons in the total population has begun to increase substantially. Colombia is projected to have a population of 50.2 million by 2020 and 55.3 million by 2050.\n\nThe population is concentrated in the Andean highlands and along the Caribbean coast, also the population densities are generally higher in the Andean region. The nine eastern lowland departments, comprising about 54% of Colombia's area, have less than 6% of the population. Traditionally a rural society, movement to urban areas was very heavy in the mid-20th century, and Colombia is now one of the most urbanized countries in Latin America. The urban population increased from 31% of the total in 1938 to nearly 60% in 1973, and by 2014 the figure stood at 76%. The population of Bogotá alone has increased from just over 300,000 in 1938 to approximately 8 million today. In total seventy-two cities now have populations of 100,000 or more (2015). Colombia has the world's largest populations of internally displaced persons (IDPs), estimated to be up to 4.9 million people.\n\nThe life expectancy is 74.8 years in 2015 and infant mortality is 13.6 per thousand in 2015. In 2015, 94.58% of adults and 98.66% of youth are literate and the government spends about 4.49% of its GDP in education.\n\nColombia is ranked third in the world in the Happy Planet Index.\n\nMore than 99.2% of Colombians speak Spanish, also called Castilian; 65 Amerindian languages, two Creole languages, the Romani language and Colombian Sign Language are also spoken in the country. English has official status in the archipelago of San Andrés, Providencia and Santa Catalina.\n\nIncluding Spanish, a total of 101 languages are listed for Colombia in the Ethnologue database. The specific number of spoken languages varies slightly since some authors consider as different languages what others consider to be varieties or dialects of the same language. Best estimates recorded 71 languages that are spoken in-country today—most of which belong to the Chibchan, Tucanoan, Bora–Witoto, Guajiboan, Arawakan, Cariban, Barbacoan, and Saliban language families. There are currently about 850,000 speakers of native languages.\n\nColombia is ethnically diverse, its people descending from the original native inhabitants, Spanish colonists, Africans originally brought to the country as slaves, and 20th-century immigrants from Europe and the Middle East, all contributing to a diverse cultural heritage. The demographic distribution reflects a pattern that is influenced by colonial history. Whites tend to live mainly in urban centers, like Bogotá, Medellín or Cali, and the burgeoning highland cities. The populations of the major cities also include mestizos. Mestizo \"campesinos\" (people living in rural areas) also live in the Andean highlands where some Spanish conquerors mixed with the women of Amerindian chiefdoms. Mestizos include artisans and small tradesmen that have played a major part in the urban expansion of recent decades.\n\nThe 2005 census reported that the \"non-ethnic population\", consisting of whites and mestizos (those of mixed white European and Amerindian ancestry), constituted 86% of the national population. 10.6% is of African ancestry. Indigenous Amerindians comprise 3.4% of the population. 0.01% of the population are Roma. An extraofficial estimate considers that the 49% of the Colombian population is Mestizo or of mixed European and Amerindian ancestry, and that approximately 37% is White, mainly of Spanish lineage, but there is also a large population of Middle East descent; in some sectors of society there is a considerable input of Italian and German ancestry.\n\nMany of the Indigenous peoples experienced a reduction in population during the Spanish rule and many others were absorbed into the mestizo population, but the remainder currently represents over eighty distinct cultures. Reserves (\"resguardos\") established for indigenous peoples occupy (27% of the country's total) and are inhabited by more than 800,000 people. Some of the largest indigenous groups are the Wayuu, the Paez, the Pastos, the Emberá and the Zenú. The departments of La Guajira, Cauca, Nariño, Córdoba and Sucre have the largest indigenous populations.\n\nThe Organización Nacional Indígena de Colombia (ONIC), founded at the first National Indigenous Congress in 1982, is an organization representing the indigenous peoples of Colombia. In 1991, Colombia signed and ratified the current international law concerning indigenous peoples, Indigenous and Tribal Peoples Convention, 1989.\n\nBlack Africans were brought as slaves, mostly to the coastal lowlands, beginning early in the 16th century and continuing into the 19th century. Large Afro-Colombian communities are found today on the Caribbean and Pacific coasts. The population of the department of Chocó, running along the northern portion of Colombia's Pacific coast, is over 80% black. British and Jamaicans migrated mainly to the islands of San Andres and Providencia. A number of other Europeans and North Americans migrated to the country in the late 19th and early 20th centuries, including people from the former USSR during and after the Second World War.\n\nMany immigrant communities have settled on the Caribbean coast, in particular recent immigrants from the Middle East. Barranquilla (the largest city of the Colombian Caribbean) and other Caribbean cities have the largest populations of Lebanese, Palestinian, and other Arabs. There are also important communities of Chinese, Japanese, Romanis and Jews. There is a major migration trend of Venezuelans, due to the political and economic situation in Venezuela.\n\nThe National Administrative Department of Statistics (DANE) does not collect religious statistics, and accurate reports are difficult to obtain. However, based on various studies and a survey, about 90% of the population adheres to Christianity, the majority of which (70.9%) are Roman Catholic, while a significant minority (16.7%) adhere to Protestantism (primarily Evangelicalism). Some 4.7% of the population is atheist or agnostic, while 3.5% claim to believe in God but do not follow a specific religion. 1.8% of Colombians adhere to Jehovah's Witnesses and Adventism and less than 1% adhere to other religions, such as Islam, Judaism, Buddhism, Mormonism, Hinduism, Indigenous religions, Hare Krishna movement, Rastafari movement, Orthodox Catholic Church, and spiritual studies. The remaining people either did not respond or replied that they did not know. In addition to the above statistics, 35.9% of Colombians reported that they did not practice their faith actively.\n\nWhile Colombia remains a mostly Roman Catholic country by baptism numbers, the 1991 Colombian constitution guarantees freedom of religion and all religious faiths and churches are equally free before the law.\n\nColombia is a highly urbanized country. The largest cities in the country are Bogotá, with an estimated 8 million inhabitants, Medellín, with an estimated 2.5 million inhabitants, Cali, with an estimated 2.4 million inhabitants, and Barranquilla, with an estimated 1.2 million inhabitants. Cartagena highlights in number of inhabitants and the city of Bucaramanga is relevant in terms of metropolitan area population.\nColombia lies at the crossroads of Latin America and the broader American continent, and as such has been hit by a wide range of cultural influences. Native American, Spanish and other European, African, American, Caribbean, and Middle Eastern influences, as well as other Latin American cultural influences, are all present in Colombia's modern culture. Urban migration, industrialization, globalization, and other political, social and economic changes have also left an impression.\n\nMany national symbols, both objects and themes, have arisen from Colombia's diverse cultural traditions and aim to represent what Colombia, and the Colombian people, have in common. Cultural expressions in Colombia are promoted by the government through the Ministry of Culture.\n\nColombian literature dates back to pre-Columbian era; a notable example of the period is the epic poem known as the \"Legend of Yurupary\". In Spanish colonial times, notable writers include Juan de Castellanos (\"Elegías de varones ilustres de Indias\"), Hernando Domínguez Camargo and his epic poem to San Ignacio de Loyola, Pedro Simón, Juan Rodríguez Freyle (\"El Carnero\"), Lucas Fernández de Piedrahita, and the nun Francisca Josefa de Castillo, representative of mysticism.\n\nPost-independence literature linked to Romanticism highlighted Antonio Nariño, José Fernández Madrid, Camilo Torres Tenorio and Francisco Antonio Zea. In the second half of the nineteenth century and early twentieth century the literary genre known as \"costumbrismo\" became popular; great writers of this period were Tomás Carrasquilla, Jorge Isaacs and Rafael Pombo (the latter of whom wrote notable works of children's literature). Within that period, authors such as José Asunción Silva, José Eustasio Rivera, León de Greiff, Porfirio Barba-Jacob and José María Vargas Vila developed the modernist movement. In 1872, Colombia established the Colombian Academy of Language, the first Spanish language academy in the Americas. Candelario Obeso wrote the groundbreaking \"Cantos Populares de mi Tierra\" (1877), the first book of poetry by an Afro-Colombian author.\n\nBetween 1939 and 1940 seven books of poetry were published under the name \"Stone and Sky\" in the city of Bogotá that significantly impacted the country; they were edited by the poet Jorge Rojas. In the following decade, Gonzalo Arango founded the movement of \"nothingness\" in response to the violence of the time; he was influenced by nihilism, existentialism, and the thought of another great Colombian writer: Fernando González Ochoa. During the boom in Latin American literature, successful writers emerged, led by Nobel laureate Gabriel García Márquez and his magnum opus, \"One Hundred Years of Solitude\", Eduardo Caballero Calderón, Manuel Mejía Vallejo, and Álvaro Mutis, a writer who was awarded the Cervantes Prize and the Prince of Asturias Award for Letters. Other leading contemporary authors are Fernando Vallejo, William Ospina (Rómulo Gallegos Prize) and Germán Castro Caycedo.\n\nColombian art has over 3,000 years of history. Colombian artists have captured the country's changing political and cultural backdrop using a range of styles and mediums. There is archeological evidence of ceramics being produced earlier in Colombia than anywhere else in the Americas, dating as early as 3,000 BCE.\n\nThe earliest examples of gold craftsmanship have been attributed to the Tumaco people of the Pacific coast and date to around 325 BCE. Roughly between 200 BCE and 800 CE, the San Agustín culture, masters of stonecutting, entered its “classical period\". They erected raised ceremonial centres, sarcophagi, and large stone monoliths depicting anthropomorphic and zoomorphhic forms out of stone.\n\nColombian art has followed the trends of the time, so during the 16th to 18th centuries, Spanish Catholicism had a huge influence on Colombian art, and the popular baroque style was replaced with rococo when the Bourbons ascended to the Spanish crown. More recently, Colombian artists Pedro Nel Gómez and Santiago Martínez Delgado started the Colombian Murial Movement in the 1940s, featuring the neoclassical features of Art Deco.\n\nSince the 1950s, the Colombian art started to have a distinctive point of view, reinventing traditional elements under the concepts of the 20th century. Examples of this are the Greiff portraits by Ignacio Gómez Jaramillo, showing what the Colombian art could do with the new techniques applied to typical Colombian themes. Carlos Correa, with his paradigmatic “Naturaleza muerta en silencio” (silent dead nature), combines geometrical abstraction and cubism. Alejandro Obregón is often considered as the father of modern Colombian painting, and one of the most influential artist in this period, due to his originality, the painting of Colombian landscapes with symbolic and expressionist use of animals, (specially the Andean condor). Fernando Botero, Omar Rayo, Enrique Grau, Édgar Negret, David Manzur, Rodrigo Arenas Betancourt and Oscar Murillo are some of the Colombian artists featured at the international level.\nThe Colombian sculpture from the sixteenth to 18th centuries was mostly devoted to religious depictions of ecclesiastic art, strongly influenced by the Spanish schools of sacred sculpture. During the early period of the Colombian republic, the national artists were focused in the production of sculptural portraits of politicians and public figures, in a plain neoclassicist trend. During the 20th century, the Colombian sculpture began to develop a bold and innovative work with the aim of reaching a better understanding of national sensitivity.\n\nColombian photography was marked by the arrival of the daguerreotype. Jean-Baptiste Louis Gros was who brought the daguerreotype process to Colombia in 1841. The Piloto public library has Latin America’s largest archive of negatives, containing 1.7 million antique photographs covering Colombia 1848 until 2005.\n\nThe Colombian press has promoted the work of the cartoonists. In recent decades, fanzines, internet and independent publishers have been fundamental to the growth of the comic in Colombia.\n\nThroughout the times, there have been a variety of architectural styles, from those of indigenous peoples to contemporary ones, passing through colonial (military and religious), Republican, transition and modern styles.\n\nAncient habitation areas, longhouses, crop terraces, roads as the Inca road system, cemeteries, hypogeums and necropolises are all part of the architectural heritage of indigenous peoples. Some prominent indigenous structures are the preceramic and ceramic archaeological site of Tequendama, Tierradentro (a park that contains the largest concentration of pre-Columbian monumental shaft tombs with side chambers), the largest collection of religious monuments and megalithic sculptures in South America, located in San Agustín, Huila. Lost city (an archaeological site with a series of terraces carved into the mountainside, a net of tiled roads and several circular plazas) and also stand out the large villages mainly built with stone, wood, cane and mud.\n\nArchitecture during the period of conquest and colonization is mainly derived of adapting European styles to local conditions, and Spanish influence, especially Andalusian and Extremaduran, can be easily seen. When Europeans founded cities two things were making simultaneously: the dimensioning of geometrical space (town square, street), and the location of a tangible point of orientation. The construction of forts was common throughout the Caribbean and in some cities of the interior, because of the dangers that represented the hostile indigenous groups and the pirates who roamed the seas. Churches, chapels, schools, and hospitals belonging to religious orders cause a great urban impact. Baroque architecture is used in military buildings and public spaces. Marcelino Arroyo, Francisco José de Caldas and Domingo de Petrés were great representatives of neo-classical architecture.\n\nThe National Capitol is a great representative of romanticism. Wood is extensively used in doors, windows, railings and ceilings during the colonization of Antioquia. The Caribbean architecture acquires a strong Arabic influence. The Teatro Colón in Bogotá is a lavish example of architecture from the 19th century. The quintas houses with innovations in the volumetric conception are some of the best examples of the Republican architecture; the Republican action in the city focused on the design of three types of spaces: parks with forests, small urban parks and avenues and the Gothic style was most commonly used for the design of churches.\n\nDeco style, modern neoclassicism, eclecticism folklorist and art deco ornamental resources significantly influenced the architecture of Colombia, especially during the transition period. Modernism contributed with new construction technologies and new materials (steel, reinforced concrete, glass and synthetic materials) and the topology architecture and lightened slabs system also have a great influence. The most influential architects of the modern movement were Rogelio Salmona and Fernando Martínez Sanabria.\n\nThe contemporary architecture of Colombia is designed to give greater importance to the materials, this architecture takes into account the specific natural and artificial geographies and is also an architecture that appeals to the senses. The conservation of the architectural and urban heritage of Colombia has been promoted in recent years.\n\nColombian music blends European-influenced guitar and song structure with large gaita flutes and percussion instruments from the indigenous population, while its percussion structure and dance forms come from Africa. Colombia has a diverse and dynamic musical environment. Musicians, composers, music producers and singers from Colombia are recognized internationally such as Shakira, Juanes, Carlos Vives and others.\n\nGuillermo Uribe Holguín, an important cultural figure in the National Symphony Orchestra of Colombia, Luis Antonio Calvo and Blas Emilio Atehortúa are some of the greatest exponents of the art music. The Bogotá Philharmonic Orchestra is one of the most active orchestras in Colombia.\n\nCaribbean music has many vibrant rhythms, such as cumbia (it is played by the maracas, the drums, the gaitas and guacharaca), porro (it is a monotonous but joyful rhythm), mapalé (with its fast rhythm and constant clapping) and the \"vallenato\", which originated in the northern part of the Caribbean coast (the rhythm is mainly played by the caja, the guacharaca, and accordion).\n\nThe music from the Pacific coast, such as the currulao is characterized by its strong use of drums (instruments such as the native marimba, the conunos, the bass drum, the side drum and the cuatro guasas or tubular rattle). An important rhythm of the south region of the Pacific coast is the contradanza (it is used in dance shows, as a result of the striking colours of the costumes). Marimba music, traditional chants and dances from the Colombia South Pacific region are on UNESCO's Representative List of the Intangible Cultural Heritage of Humanity.\n\nImportant musical rhythms of the Andean Region are the danza (dance of Andean folklore arising from the transformation of the European contredance), the bambuco (it is played with guitar, tiple and mandolin, the rhythm is danced by couples), the pasillo (a rhythm inspired by the Austrian waltz and the Colombian \"danza\", the lyrics have been composed by well-known poets), the guabina (the tiple, the bandola and the requinto are the basic instruments), the sanjuanero (it originated in Tolima and Huila Departments, the rhythm is joyful and fast). Apart from these traditional rhythms, salsa music has spread throughout the country, and the city of Cali is considered by many salsa singers to be 'The New Salsa Capital of the World'.\n\nThe instruments that distinguish the music of the Eastern Plains are the harp, the cuatro (a type of four-stringed guitar) and maracas. Important rhythms of this region are the joropo (a fast rhythm and there is also tapping as a result of its flamenco ancestry) and the galeron (it is heard a lot while cowboys are working).\n\nThe music of the Amazon region is strongly influenced by the indigenous religious practices. Some of the musical instruments used are the manguaré (a musical instrument of ceremonial type, consisting of a pair of large cylindrical drums), the quena (melodic instrument), the rondador, the congas, bells, and different types of flutes.\n\nThe music of the Archipelago of San Andrés, Providencia and Santa Catalina is usually accompanied by a mandolin, a tub-bass, a jawbone, a guitar and maracas. Some popular archipelago rhythms are the Schottische, the Calypso, the Polka and the Mento.\n\nTheater was introduced in Colombia during the Spanish colonization in 1550 through zarzuela companies. Colombian theater is supported by the Ministry of Culture and a number of private and state owned organizations. The Ibero-American Theater Festival of Bogotá is the cultural event of the highest importance in Colombia and one of the biggest theater festivals in the world. Other important theater events are: The Festival of Puppet The Fanfare (Medellín), The Manizales Theater Festival, The Caribbean Theatre Festival (Santa Marta) and The Art Festival of Popular Culture \"Cultural Invasion\" (Bogotá).\n\nAlthough the Colombian cinema is young as an industry, more recently the film industry was growing with support from the Film Act passed in 2003. Many film festivals take place in Colombia, but the two most important are the Cartagena Film Festival, which is the oldest film festival in Latin America, and the Bogotá Film Festival.\n\nSome important national circulation newspapers are \"El Tiempo\" and \"El Espectador\". Television in Colombia has two privately owned TV networks and three state-owned TV networks with national coverage, as well as six regional TV networks and dozens of local TV stations. Private channels, RCN and Caracol are the highest-rated. The regional channels and regional newspapers cover a department or more and its content is made in these particular areas.\n\nColombia has three major national radio networks: Radiodifusora Nacional de Colombia, a state-run national radio; Caracol Radio and RCN Radio, privately owned networks with hundreds of affiliates. There are other national networks, including Cadena Super, Todelar, and Colmundo. Many hundreds of radio stations are registered with the Ministry of Information Technologies and Communications.\n\nColombia's varied cuisine is influenced by its diverse fauna and flora as well as the cultural traditions of the ethnic groups. Colombian dishes and ingredients vary widely by region. Some of the most common ingredients are: cereals such as rice and maize; tubers such as potato and cassava; assorted legumes; meats, including beef, chicken, pork and goat; fish; and seafood. Colombia cuisine also features a variety of tropical fruits such as cape gooseberry, feijoa, arazá, dragon fruit, mangostino, granadilla, papaya, guava, mora (blackberry), lulo, soursop and passionfruit. Colombia is one of the world's largest consumers of fruit juices.\n\nAmong the most representative appetizers and soups are patacones (fried green plantains), sancocho de gallina (chicken soup with root vegetables) and ajiaco (potato and corn soup). Representative snacks and breads are pandebono, arepas (corn cakes), aborrajados (fried sweet plantains with cheese), torta de choclo, empanadas and almojábanas. Representative main courses are bandeja paisa, lechona tolimense, mamona, tamales and fish dishes (such as arroz de lisa), especially in coastal regions where kibbeh, suero, costeño cheese and carimañolas are also eaten. Representative side dishes are papas chorreadas (potatoes with cheese), remolachas rellenas con huevo duro (beets stuffed with hard-boiled egg) and arroz con coco (coconut rice). Organic food is a current trend in big cities, although in general across the country the fruits and veggies are very natural and fresh.\n\nRepresentative desserts are buñuelos, natillas, Maria Luisa cake, bocadillo made of guayaba (guava jelly), cocadas (coconut balls), casquitos de guayaba (candied guava peels), torta de natas, obleas, flan de mango, roscón, milhoja, manjar blanco, dulce de feijoa, dulce de papayuela, torta de mojicón, and esponjado de curuba. Typical sauces (salsas) are hogao (tomato and onion sauce) and Colombian-style ají.\n\nSome representative beverages are coffee (Tinto), champús, cholado, lulada, avena colombiana, sugarcane juice, aguapanela, aguardiente, hot chocolate and fresh fruit juices (often made with water or milk).\n\nTejo is Colombia’s national sport and is a team sport that involves launching projectiles to hit a target. But of all sports in Colombia, football is the most popular. Colombia was the champion of the 2001 Copa América, in which they set a new record of being undefeated, conceding no goals and winning each match. Interestingly, Colombia has been awarded “mover of the year” twice.\n\nColombia is a mecca for roller skaters. The national team is a perennial powerhouse at the World Roller Speed Skating Championships. Colombia has traditionally been very good in cycling and a large number of Colombian cyclists have triumphed in major competitions of cycling.\n\nIn baseball, another sport rooted in the Caribbean Coast, Colombia was world amateur champion in 1947 and 1965.\nBaseball is popular in the Caribbean, mainly in the cities Cartagena, Barranquilla and Santa Marta. Of those cities have come good players like: Orlando Cabrera, Édgar Rentería who was champion of the World Series in 1997 and 2010, and others who have played in Major League Baseball.\n\nBoxing is one of the sports that more world champions has produced for Colombia.\nMotorsports also occupies an important place in the sporting preferences of Colombians; Juan Pablo Montoya is a race car driver known for winning 7 Formula One events. Colombia also has excelled in sports such as BMX, judo, shooting sport, taekwondo, wrestling, high diving and athletics, also has a long tradition in weightlifting and bowling.\n\nThe overall life expectancy in Colombia at birth is 74.8 years (71.2 years for males and 78.4 years for females). Health standards in Colombia have improved very much since the 1980s, healthcare reforms have led to the massive improvements in the healthcare systems of the country. Although this new system has widened population coverage by the social and health security system from 21% (pre-1993) to 96% in 2012, health disparities persist.\n\nThrough health tourism, many people from over the world travel from their places of residence to other countries in search of medical treatment and the attractions in the countries visited. Colombia is projected as one of Latin America’s main destinations in terms of health tourism due to the quality of its health care professionals, a good number of institutions devoted to health, and an immense inventory of natural and architectural sites. Cities such as Bogotá, Cali, Medellín and Bucaramanga are the most visited in cardiology procedures, neurology, dental treatments, stem cell therapy, ENT, ophthalmology and joint replacements among others for the medical services of high quality.\n\nA study conducted by \"América Economía\" magazine ranked 21 Colombian health care institutions among the top 44 in Latin America, amounting to 48 percent of the total.\n\nThe educational experience of many Colombian children begins with attendance at a preschool academy until age five (\"Educación preescolar\"). Basic education (\"Educación básica\") is compulsory by law. It has two stages: Primary basic education (\"Educación básica primaria\") which goes from first to fifth grade – children from six to ten years old, and Secondary basic education (\"Educación básica secundaria\"), which goes from sixth to ninth grade. Basic education is followed by Middle vocational education (\"Educación media vocacional\") that comprises the tenth and eleventh grades. It may have different vocational training modalities or specialties (academic, technical, business, and so on.) according to the curriculum adopted by each school.\n\nAfter the successful completion of all the basic and middle education years, a high-school diploma is awarded. The high-school graduate is known as a \"bachiller\", because secondary basic school and middle education are traditionally considered together as a unit called \"bachillerato\" (sixth to eleventh grade). Students in their final year of middle education take the ICFES test (now renamed Saber 11) in order to gain access to higher education (\"Educación superior\"). This higher education includes undergraduate professional studies, technical, technological and intermediate professional education, and post-graduate studies. Technical professional institutions of Higher Education are also opened to students holder of a qualification in Arts and Business. This qualification is usually awarded by the SENA after a two years curriculum.\n\n\"Bachilleres\" (high-school graduates) may enter into a professional undergraduate career program offered by a university; these programs last up to five years (or less for technical, technological and intermediate professional education, and post-graduate studies), even as much to six to seven years for some careers, such as medicine. In Colombia, there is not an institution such as college; students go directly into a career program at a university or any other educational institution to obtain a professional, technical or technological title. Once graduated from the university, people are granted a (professional, technical or technological) diploma and licensed (if required) to practice the career they have chosen. For some professional career programs, students are required to take the Saber-Pro test, in their final year of undergraduate academic education.\n\nPublic spending on education as a proportion of gross domestic product in 2015 was 4.49%. This represented 15.05% of total government expenditure. The primary and secondary gross enrolment ratios stood at 113.56% and 98.09% respectively. School-life expectancy was 14.42 years. A total of 94.58% of the population aged 15 and older were recorded as literate, including 98.66% of those aged 15–24.\n\n\nGeneral information\n\nGovernment\n\nCulture\n\nGeography\n"}
{"id": "5224", "url": "https://en.wikipedia.org/wiki?curid=5224", "title": "Citizen Kane", "text": "Citizen Kane\n\nCitizen Kane is a 1941 American mystery drama film by Orson Welles, its producer, co-screenwriter, director and star. The picture was Welles's first feature film. Nominated for Academy Awards in nine categories, it won an Academy Award for Best Writing (Original Screenplay) by Herman J. Mankiewicz and Welles. Considered by many critics, filmmakers, and fans to be the greatest film of all time, \"Citizen Kane\" was voted as such in five consecutive British Film Institute \"Sight & Sound\" polls of critics, until it was displaced by \"Vertigo\" in the 2012 poll. It topped the American Film Institute's 100 Years ... 100 Movies list in 1998, as well as its 2007 update. \"Citizen Kane\" is particularly praised for its cinematography, music, and narrative structure, which have been considered innovative and precedent-setting.\n\nThe quasi-biographical film examines the life and legacy of Charles Foster Kane, played by Welles, a character based in part upon the American newspaper magnate William Randolph Hearst, Chicago tycoons Samuel Insull and Harold McCormick, and aspects of Welles's own life. Upon its release, Hearst prohibited mention of the film in any of his newspapers. Kane's career in the publishing world is born of idealistic social service, but gradually evolves into a ruthless pursuit of power. Narrated principally through flashbacks, the story is told through the research of a newsreel reporter seeking to solve the mystery of the newspaper magnate's dying word: \"Rosebud.\"\n\nAfter the Broadway successes of Welles's Mercury Theatre and the controversial 1938 radio broadcast \"The War of the Worlds\" on \"The Mercury Theatre on the Air\", Welles was courted by Hollywood. He signed a contract with RKO Pictures in 1939. Unusually for an untried director, he was given the freedom to develop his own story, to use his own cast and crew, and to have final cut privilege. Following two abortive attempts to get a project off the ground, he wrote the screenplay for \"Citizen Kane\", collaborating on the effort with Herman Mankiewicz. Principal photography took place in 1940 and the film received its American release in 1941.\n\nWhile a critical success, \"Citizen Kane\" failed to recoup its costs at the box office. The film faded from view after its release but was subsequently returned to the public's attention when it was praised by such French critics as André Bazin and given an American revival in 1956. The film was released on Blu-ray on September 13, 2011, for a special 70th anniversary edition.\n\nIn a mansion in Xanadu, a vast palatial estate in Florida, the elderly Charles Foster Kane is on his deathbed. Holding a snow globe, he utters a word, \"Rosebud\", and dies; the globe slips from his hand and smashes on the floor. A newsreel obituary tells the life story of Kane, an enormously wealthy newspaper publisher. Kane's death becomes sensational news around the world, and the newsreel's producer tasks reporter Jerry Thompson with discovering the meaning of \"Rosebud\".\n\nThompson sets out to interview Kane's friends and associates. He approaches Kane's second wife, Susan Alexander Kane, now an alcoholic who runs her own nightclub, but she refuses to talk to him. Thompson goes to the private archive of the late banker Walter Parks Thatcher. Through Thatcher's written memoirs, Thompson learns that Kane's childhood began in poverty in Colorado.\n\nIn 1871, after a gold mine was discovered on her property, Kane's mother Mary Kane sends Charles away to live with Thatcher so that he would be properly educated. While Thatcher and Charles' parents discuss arrangements inside, the young Kane plays happily with a sled in the snow outside his parents' boarding-house and protests being sent to live with Thatcher.\n\nYears later, after gaining full control over his trust fund at the age of 25, Kane enters the newspaper business and embarks on a career of yellow journalism. He takes control of the \"New York Inquirer\" and starts publishing scandalous articles that attack Thatcher's business interests. After the stock market crash in 1929, Kane is forced to sell controlling interest of his newspaper empire to Thatcher.\n\nBack in the present, Thompson interviews Kane's personal business manager, Mr. Bernstein. Bernstein recalls how Kane hired the best journalists available to build the \"Inquirer\"s circulation. Kane rose to power by successfully manipulating public opinion regarding the Spanish–American War and marrying Emily Norton, the niece of a President of the United States.\n\nThompson interviews Kane's estranged best friend, Jedediah Leland, in a retirement home. Leland recalls how Kane's marriage to Emily disintegrates more and more over the years, and he begins an affair with amateur singer Susan Alexander while he is running for Governor of New York. Both his wife and his political opponent discover the affair and the public scandal ends his political career. Kane marries Susan and forces her into a humiliating operatic career for which she has neither the talent nor the ambition.\n\nBack in the present, Susan now consents to an interview with Thompson, and recalls her failed opera career. Kane finally allows her to abandon her singing career after she attempts suicide. After years spent dominated by Kane and living in isolation at Xanadu, Susan leaves Kane. Kane's butler Raymond recounts that, after Susan leaves him, Kane begins violently destroying the contents of her bedroom. He suddenly calms down when he sees a snow globe and says, \"Rosebud.\"\n\nBack at Xanadu, Kane's belongings are being cataloged or discarded. Thompson concludes that he is unable to solve the mystery and that the meaning of Kane's last word will forever remain an enigma. As the film ends, the camera reveals that \"Rosebud\" is the trade name of the sled on which the eight-year-old Kane was playing on the day that he was taken from his home in Colorado. Thought to be junk by Xanadu's staff, the sled is burned in a furnace.\n\nThe beginning of the film's ending credits state that \"Most of the principal actors in \"Citizen Kane\" are new to motion pictures. The Mercury Theatre is proud to introduce them.\" The cast is listed in the following order:\n\n\nAdditionally, Charles Bennett appears as the entertainer at the head of the chorus line in the \"Inquirer\" party sequence, and cinematographer Gregg Toland makes a cameo appearance as an interviewer depicted in part of the \"News on the March\" newsreel. Actor Alan Ladd makes a cameo appearance as a reporter smoking a pipe at the end of the film.\n\nHollywood had shown interest in Welles as early as 1936. He turned down three scripts sent to him by Warner Bros. In 1937, he declined offers from David O. Selznick, who asked him to head his film company's story department, and William Wyler, who wanted him for a supporting role in \"Wuthering Heights\". \"Although the possibility of making huge amounts of money in Hollywood greatly attracted him,\" wrote biographer Frank Brady, \"he was still totally, hopelessly, insanely in love with the theater, and it is there that he had every intention of remaining to make his mark.\"\n\nFollowing \"The War of the Worlds\" broadcast of his CBS radio series \"The Mercury Theatre on the Air\", Welles was lured to Hollywood with a remarkable contract. RKO Pictures studio head George J. Schaefer wanted to work with Welles after the notorious broadcast, believing that Welles had a gift for attracting mass attention. RKO was also uncharacteristically profitable and was entering into a series of independent production contracts that would add more artistically prestigious films to its roster. Throughout the spring and early summer of 1939, Schaefer constantly tried to lure the reluctant Welles to Hollywood. Welles was in financial trouble after failure of his plays \"Five Kings\" and \"The Green Goddess\". At first he simply wanted to spend three months in Hollywood and earn enough money to pay his debts and fund his next theatrical season. Welles first arrived on July 20, 1939 and on his first tour, he called the movie studio \"the greatest electric train set a boy ever had\".\n\nWelles signed his contract with RKO on August 21. This legendary contract stipulated that Welles would act in, direct, produce and write two films. Mercury would get $100,000 for the first film by January 1, 1940, plus 20% of profits after RKO recouped $500,000, and $125,000 for a second film by January 1, 1941, plus 20% of profits after RKO recouped $500,000. The most controversial aspect of the contract was granting Welles complete artistic control of the two films so long as RKO approved both project's stories and so long as the budget did not exceed $500,000. RKO executives would not be allowed to see any footage until Welles chose to show it to them, and no cuts could be made to either film without Welles's approval. Welles was allowed to develop the story without interference, select his own cast and crew, and have the right of final cut. Granting final cut privilege was unprecedented for a studio since it placed artistic considerations over financial investment. The contract was deeply resented in the film industry, and the Hollywood press took every opportunity to mock RKO and Welles. Schaefer remained a great supporter and saw the unprecedented contract as good publicity. Film scholar Robert L. Carringer wrote: \"The simple fact seems to be that Schaefer believed Welles was going to pull off something really big almost as much as Welles did himself.\"\n\nWelles spent the first five months of his RKO contract trying to get his first project going, without success. \"They are laying bets over on the RKO lot that the Orson Welles deal will end up without Orson ever doing a picture there,\" wrote \"The Hollywood Reporter\". It was agreed that Welles would film \"Heart of Darkness\", previously adapted for \"The Mercury Theatre on the Air\", which would be presented entirely through a first-person camera. After elaborate pre-production and a day of test shooting with a hand-held camera—unheard of at the time—the project never reached production because Welles was unable to trim $50,000 from its budget. Schaefer told Welles that the $500,000 budget could not be exceeded; revenue was declining sharply in Europe by the fall of 1939.\n\nHe then started work on the idea that became \"Citizen Kane\". Knowing the script would take time to prepare, Welles suggested to RKO that while that was being done—\"so the year wouldn't be lost\"—he make a humorous political thriller. Welles proposed \"The Smiler with a Knife\", from a novel by Cecil Day-Lewis. When that project stalled in December 1939, Welles began brainstorming other story ideas with screenwriter Herman J. Mankiewicz, who had been writing Mercury radio scripts. \"Arguing, inventing, discarding, these two powerful, headstrong, dazzlingly articulate personalities thrashed toward \"Kane\"\", wrote biographer Richard Meryman.\n\nOne of the long-standing controversies about \"Citizen Kane\" has been the authorship of the screenplay. Welles conceived the project with screenwriter Herman J. Mankiewicz, who was writing radio plays for Welles's CBS Radio series, \"The Campbell Playhouse\". Mankiewicz based the original outline on the life of William Randolph Hearst, whom he knew socially and came to hate after being exiled from Hearst's circle.\n\nIn February 1940 Welles supplied Mankiewicz with 300 pages of notes and put him under contract to write the first draft screenplay under the supervision of John Houseman, Welles's former partner in the Mercury Theatre. Welles later explained, \"I left him on his own finally, because we'd started to waste too much time haggling. So, after mutual agreements on storyline and character, Mank went off with Houseman and did his version, while I stayed in Hollywood and wrote mine.\" Taking these drafts, Welles drastically condensed and rearranged them, then added scenes of his own. The industry accused Welles of underplaying Mankiewicz's contribution to the script, but Welles countered the attacks by saying, \"At the end, naturally, I was the one making the picture, after all—who had to make the decisions. I used what I wanted of Mank's and, rightly or wrongly, kept what I liked of my own.\"\n\nThe terms of the contract stated that Mankiewicz was to receive no credit for his work, as he was hired as a script doctor. Before he signed the contract Mankiewicz was particularly advised by his agents that all credit for his work belonged to Welles and the Mercury Theatre, the \"author and creator\". As the film neared release, however, Mankiewicz began threatening Welles to get credit for the film—including threats to place full-page ads in trade papers and to get his friend Ben Hecht to write an exposé for \"The Saturday Evening Post\". Mankiewicz also threatened to go to the Screen Writers Guild and claim full credit for writing the entire script by himself.\n\nAfter lodging a protest with the Screen Writers Guild, Mankiewicz withdrew it, then vacillated. The question was resolved in January 1941 when the studio, RKO Pictures, awarded Mankiewicz credit. The guild credit form listed Welles first, Mankiewicz second. Welles's assistant Richard Wilson said that the person who circled Mankiewicz's name in pencil, then drew an arrow that put it in first place, was Welles. The official credit reads, \"Screenplay by Herman J. Mankiewicz and Orson Welles\". Mankiewicz's rancor toward Welles grew over the remaining 12 years of his life.\n\nQuestions over the authorship of the \"Citizen Kane\" screenplay were revived in 1971 by influential film critic Pauline Kael, whose controversial 50,000-word essay \"Raising Kane\" was commissioned as an introduction to the shooting script in \"The Citizen Kane Book\", published in October 1971. The book-length essay first appeared in February 1971, in two consecutive issues of \"The New Yorker\" magazine.\nIn the ensuing controversy Welles was defended by colleagues, critics, biographers and scholars, but his reputation was damaged by its charges. The essay was later discredited and Kael's own scholarship was called into question.\n\nAny question of authorship was resolved with Carringer's 1978 essay, \"The Scripts of Citizen Kane\". Carringer studied the collection of script records—\"almost a day-to-day record of the history of the scripting\"—that was then still intact at RKO. He reviewed all seven drafts and concluded that \"the full evidence reveals that Welles's contribution to the \"Citizen Kane\" script was not only substantial but definitive.\"\n\nWelles never confirmed a principal source for the character of Charles Foster Kane. Houseman wrote that Kane is a synthesis of different personalities, with Hearst's life used as the main source. Some events and details were invented, and Houseman and Mankiewicz also \"grafted anecdotes from other giants of journalism, including Pulitzer, Northcliffe and Mank's first boss, Herbert Bayard Swope.\" Welles said, \"Mr. Hearst was quite a bit like Kane, although Kane isn't really founded on Hearst in particular, many people sat for it so to speak\". He specifically acknowledged that aspects of Kane were drawn from the lives of two business tycoons familiar from his youth in Chicago—Samuel Insull and Harold Fowler McCormick.\n\nThe character of Jedediah Leland was based on drama critic Ashton Stevens, George Stevens's uncle and Welles's close boyhood friend. Some detail came from Mankiewicz's own experience as a drama critic in New York.\n\nThe assumption that the character of Susan Alexander Kane was based on Marion Davies was a major reason Hearst tried to destroy \"Citizen Kane\". Welles denied that the character was based on Davies, whom he called \"an extraordinary woman—nothing like the character Dorothy Comingore played in the movie.\" He cited Insull's building of the Chicago Opera House, and McCormick's lavish promotion of the opera career of his second wife, Ganna Walska, as direct influences on the screenplay.\n\nThe character of political boss Jim W. Gettys is based on Charles F. Murphy, a leader in New York City's infamous Tammany Hall political machine.\n\nWelles credited \"Rosebud\" to Mankiewicz. Biographer Richard Meryman wrote that the symbol of Mankiewicz's own damaged childhood was a treasured bicycle, stolen while he visited the public library and not replaced by his family as punishment. He regarded it as the prototype of Charles Foster Kane's sled. In his 2015 Welles biography, Patrick McGilligan reported that Mankiewicz himself stated that the word \"Rosebud\" was taken from the name of a famous racehorse, Old Rosebud. Mankiewicz had a bet on the horse in the 1914 Kentucky Derby, which he won, and McGilligan wrote that \"Old Rosebud symbolized his lost youth, and the break with his family\". In testimony for the Lundberg suit, Mankiewicz said, \"I had undergone psycho-analysis, and Rosebud, under circumstances slightly resembling the circumstances in [\"Citizen Kane\"], played a prominent part.\" Other modern claims that the term was a nickname Hearst used for Davies' clitoris were rejected by Houseman and dismissed by Brady.\n\nThe \"News on the March\" sequence that begins the film satirizes the journalistic style of \"The March of Time\", the news documentary and dramatization series presented in movie theaters by Time Inc. From 1935 to 1938 Welles was a member of the uncredited company of actors that presented the original radio version.\n\nHouseman claimed that banker Walter P. Thatcher was loosely based on J. P. Morgan. Bernstein was named for Dr. Maurice Bernstein, appointed Welles's guardian; Sloane's portrayal was said to be based on Bernard Herrmann. Herbert Carter, editor of \"The Inquirer\", was named for actor Jack Carter.\n\n\"Citizen Kane\" was a rare film in that its principal roles were played by actors new to motion pictures. Ten were billed as Mercury Actors, members of the skilled repertory company assembled by Welles for the stage and radio performances of the Mercury Theatre, an independent theater company he founded with Houseman in 1937. \"He loved to use the Mercury players,\" wrote biographer Charles Higham, \"and consequently he launched several of them on movie careers.\"\n\nThe film represents the feature film debuts of William Alland, Ray Collins, Joseph Cotten, Agnes Moorehead, Erskine Sanford, Everett Sloane, Paul Stewart, and Welles himself. Despite never having appeared in feature films, some of the cast members were already well known to the public. Cotten had recently become a Broadway star in the hit play \"The Philadelphia Story\" with Katharine Hepburn and Sloane was well known for his role on the radio show \"The Goldbergs\". Mercury actor George Coulouris was a star of the stage in New York and London.\n\nNot all of the cast came from the Mercury Players. Welles cast Dorothy Comingore as Susan Alexander Kane. Comingore had never appeared in a film and was a discovery of Charlie Chaplin. Chaplin recommended Comingore to Welles, who then met Comingore at a party in Los Angeles and immediately cast her.\n\nWelles had met stage actress Ruth Warrick while visiting New York on a break from Hollywood and remembered her as a good fit for Emily Norton Kane, later saying that she looked the part. Warrick told Carringer that she was struck by the extraordinary resemblance between herself and Welles's mother when she saw a photograph of Beatrice Ives Welles. She characterized her own personal relationship with Welles as motherly.\n\n\"He trained us for films at the same time that he was training himself,\" recalled Agnes Moorehead. \"Orson believed in good acting, and he realized that rehearsals were needed to get the most from his actors. That was something new in Hollywood: nobody seemed interested in bringing in a group to rehearse before scenes were shot. But Orson knew it was necessary, and we rehearsed every sequence before it was shot.\"\n\nWhen \"The March of Time\" narrator Westbrook Van Voorhis asked for $25,000 to narrate the \"News on the March\" sequence, Alland demonstrated his ability to imitate Van Voorhis and Welles cast him.\n\nWelles later said that casting character actor Gino Corrado in the small part of the waiter at the El Rancho broke his heart. Corrado had appeared in many Hollywood films, often as a waiter, and Welles wanted all of the actors to be new to films.\n\nOther uncredited roles went to Thomas A. Curran as Teddy Roosevelt in the faux newsreel; Richard Baer as Hillman, a man at Madison Square Garden, and a man in the \"News on the March\" screening room; and Alan Ladd, Arthur O'Connell and Louise Currie as reporters at Xanadu.\n\nWhen Kathryn Trosper Popper died on March 6, 2016, at the age of 100 she was reported to be the last surviving actor to appear in \"Citizen Kane\". Jean Forward, a soprano who dubbed the singing voice of Susan Alexander, was the last surviving performer from the film before her death in 2016. Warrick was the last surviving member of the principal cast at the time of her death in 2005. Sonny Bupp, who played Kane's young son, was the last surviving credited cast member of \"Citizen Kane\" when he died in 2007.\n\nProduction advisor Miriam Geiger quickly compiled a handmade film textbook for Welles, a practical reference book of film techniques that he studied carefully. He then taught himself filmmaking by matching its visual vocabulary to \"The Cabinet of Dr. Caligari\", which he ordered from the Museum of Modern Art, and films by Frank Capra, René Clair, Fritz Lang, King Vidor and Jean Renoir. The one film he genuinely studied was John Ford's \"Stagecoach\", which he watched 40 times. \"As it turned out, the first day I ever walked onto a set was my first day as a director,\" Welles said. \"I'd learned whatever I knew in the projection room—from Ford. After dinner every night for about a month, I'd run \"Stagecoach\", often with some different technician or department head from the studio, and ask questions. 'How was this done?' 'Why was this done?' It was like going to school.\"\n\nWelles's cinematographer for the film was Gregg Toland, described by Welles as \"just then, the number-one cameraman in the world.\" To Welles's astonishment, Toland visited him at his office and said, \"I want you to use me on your picture.\" He had seen some of the Mercury stage productions (including \"Caesar\") and said he wanted to work with someone who had never made a movie. RKO hired Toland on loan from Samuel Goldwyn Productions in the first week of June 1940.\n\n\"And he never tried to impress us that he was doing any miracles,\" Welles recalled. \"I was calling for things only a beginner would have been ignorant enough to think anybody could ever do, and there he was, \"doing\" them.\" Toland later explained that he wanted to work with Welles because he anticipated the first time director's inexperience and reputation for audacious experimentation in the theater would allow the cinematographer to try new and innovative camera techniques that typical Hollywood films would never have allowed him to do. Unaware of filmmaking protocol, Welles adjusted the lights on set as he was accustomed to doing in the theater; Toland quietly re-balanced them, and was angry when one of the crew informed Welles that he was infringing on Toland's responsibilities. During the first few weeks of June, Welles had lengthy discussions about the film with Toland and art director Perry Ferguson in the morning, and in the afternoon and evening he worked with actors and revised the script.\nOn June 29, 1940—a Saturday morning when few inquisitive studio executives would be around—Welles began filming \"Citizen Kane\". After the disappointment of having \"Heart of Darkness\" cancelled, Welles followed Ferguson's suggestion and deceived RKO into believing that he was simply shooting camera tests. \"But we were shooting the \"picture\",\" Welles said, \"because we wanted to get started and be already into it before anybody knew about it.\"\n\nAt the time RKO executives were pressuring him to agree to direct a film called \"The Men from Mars\", to capitalize on \"The War of the Worlds\" radio broadcast. Welles said that he would consider making the project but wanted to make a different film first. At this time he did not inform them that he had already begun filming \"Citizen Kane.\"\n\nThe early footage was called \"Orson Welles Tests\" on all paperwork. The first \"test\" shot was the \"News on the March\" projection room scene, economically filmed in a real studio projection room in darkness that masked many actors who appeared in other roles later in the film. \"At $809 Orson did run substantially beyond the test budget of $528—to create one of the most famous scenes in movie history,\" wrote Barton Whaley.\n\nThe next scenes were the El Rancho nightclub scenes and the scene in which Susan attempts suicide. Welles later said that the nightclub set was available after another film had wrapped and that filming took 10 to 12 days to complete. For these scenes Welles had Comingore's throat sprayed with chemicals to give her voice a harsh, raspy tone. Other scenes shot in secret included those in which Thompson interviews Leland and Bernstein, which were also shot on sets built for other films.\n\nDuring production, the film was referred to as \"RKO 281\". Most of the filming took place in what is now Stage 19 on the Paramount Pictures lot in Hollywood. There was some location filming at Balboa Park in San Diego and the San Diego Zoo.\n\nIn the end of July, RKO approved the film and Welles was allowed to officially begin shooting, despite having already been filming \"tests\" for several weeks. Welles leaked stories to newspaper reporters that the tests had been so good that there was no need to re-shoot them. The first official scene to be shot was the breakfast montage sequence between Kane and his first wife Emily. To strategically save money and appease the RKO executives who opposed him, Welles rehearsed scenes extensively before actually shooting and filmed very few takes of each shot set-up. Welles never shot master shots for any scene after Toland told him that Ford never shot them. To appease the increasingly curious press, Welles threw a cocktail party for selected reporters, promising that they could watch a scene being filmed. When the journalists arrived Welles told them they had \"just finished\" shooting for the day but still had the party. Welles told the press that he was ahead of schedule (without factoring in the month of \"test shooting\"), thus discrediting claims that after a year in Hollywood without making a film he was a failure in the film industry.\nWelles usually worked 16 to 18 hours a day on the film. He often began work at 4 a.m. since the special effects make-up used to age him for certain scenes took up to four hours to apply. Welles used this time to discuss the day's shooting with Toland and other crew members. The special contact lenses used to make Welles look elderly proved very painful, and a doctor was employed to place them into Welles's eyes. Welles had difficulty seeing clearly while wearing them, which caused him to badly cut his wrist when shooting the scene in which Kane breaks up the furniture in Susan's bedroom. While shooting the scene in which Kane shouts at Gettys on the stairs of Susan Alexander's apartment building, Welles fell ten feet; an X-ray revealed two bone chips in his ankle. The injury required him to direct the film from a wheelchair for two weeks. He eventually wore a steel brace to resume performing on camera; it is visible in the low-angle scene between Kane and Leland after Kane loses the election. For the final scene, a stage at the Selznick studio was equipped with a working furnace, and multiple takes were required to show the sled being put into the fire and the word \"Rosebud\" consumed. Paul Stewart recalled that on the ninth take the Culver City Fire Department arrived in full gear because the furnace had grown so hot the flue caught fire. \"Orson was delighted with the commotion\", he said.\n\nWhen \"Rosebud\" was burned, Welles choreographed the scene while he had composer Bernard Herrmann's cue playing on the set.\n\nUnlike Schaefer, many members of RKO's board of governors did not like Welles or the control that his contract gave him. However such board members as Nelson Rockefeller and NBC chief David Sarnoff were sympathetic to Welles. Throughout production Welles had problems with these executives not respecting his contract's stipulation of non-interference and several spies arrived on set to report what they saw to the executives. When the executives would sometimes arrive on set unannounced the entire cast and crew would suddenly start playing softball until they left. Before official shooting began the executives intercepted all copies of the script and delayed their delivery to Welles. They had one copy sent to their office in New York, resulting in it being leaked to press.\n\nPrincipal shooting wrapped October 24. Welles then took several weeks off of the film for a lecture tour, during which he also scouted additional locations with Toland and Ferguson. Filming resumed November 15 with some re-shoots. Toland had to leave due to a commitment to shoot Howard Hughes' \"The Outlaw\", but Toland's camera crew continued working on the film and Toland was replaced by RKO cinematographer Harry J. Wild. The final day of shooting on November 30 was Kane's death scene. Welles boasted that he only went 21 days over his official shooting schedule, without factoring in the month of \"camera tests.\" According to RKO records, the film cost $839,727. Its estimated budget had been $723,800.\n\n\"Citizen Kane\" was edited by Robert Wise and assistant editor Mark Robson. Both would become successful film directors. Wise was hired after Welles finished shooting the \"camera tests\" and began officially making the film. Wise said that Welles \"had an older editor assigned to him for those tests and evidently he was not too happy and asked to have somebody else. I was roughly Orson's age and had several good credits.\" Wise and Robson began editing the film while it was still shooting and said that they \"could tell certainly that we were getting something very special. It was outstanding film day in and day out.\" Welles gave Wise detailed instructions and was usually not present during the film's editing. The film was very well planned out and intentionally shot for such post-production techniques as slow dissolves. The lack of coverage made editing easy since Welles and Toland edited the film \"in camera\" by leaving few options of how it could be put together. Wise said the breakfast table sequence took weeks to edit and get the correct \"timing\" and \"rhythm\" for the whip pans and over-lapping dialogue. The \"News on the March\" sequence was edited by RKO's newsreel division to give it authenticity. They used stock footage from Pathé News and the General Film Library.\n\nDuring post-production Welles and special effects artist Linwood G. Dunn experimented with an optical printer to improve certain scenes that Welles found unsatisfactory from the footage. Whereas Welles was often immediately pleased with Wise's work, he would require Dunn and post-production audio engineer James G. Stewart to re-do their work several times until he was satisfied.\n\nWelles hired Bernard Herrmann to compose the film's score. Where most Hollywood film scores were written quickly, in as few as two or three weeks after filming was completed, Herrmann was given 12 weeks to write the music. He had sufficient time to do his own orchestrations and conducting, and worked on the film reel by reel as it was shot and cut. He wrote complete musical pieces for some of the montages, and Welles edited many of the scenes to match their length.\n\nWritten and directed by Welles at Toland's suggestion, the theatrical trailer for \"Citizen Kane\" differs from other trailers in that it did not feature a single second of footage of the actual film itself, but acts as a wholly original, tongue-in-cheek, pseudo-documentary piece on the film's production. Filmed at the same time as \"Citizen Kane\" itself, it offers the only existing behind-the-scenes footage of the film. The trailer, shot by Harry J. Wild instead of Toland, follows an unseen Welles as he provides narration for a tour around the film set, introductions to the film's core cast members, and a brief overview of Kane's character. The trailer also contains a number of trick shots, including one of Everett Sloane appearing at first to be running into the camera, which turns out to be the reflection of the camera in a mirror.\n\nAt the time, it was almost unprecedented for a film trailer to not actually feature anything of the film itself; and while \"Citizen Kane\" is frequently cited as a ground-breaking, influential film, Simon Callow argues its trailer was no less original in its approach. Callow writes that it has \"great playful charm ... it is a miniature documentary, almost an introduction to the cinema ... Teasing, charming, completely original, it is a sort of conjuring trick: without his face appearing once on the screen, Welles entirely dominates its five [sic] minutes' duration.\"\n\nFilm scholars and historians view \"Citizen Kane\" as Welles's attempt to create a new style of filmmaking by studying various forms of film making, and combining them all into one. However, Welles stated that his love for cinema began only when he started the work on the film. When asked where he got the confidence as a first-time director to direct a film so radically different from contemporary cinema, he responded, \"Ignorance, ignorance, sheer ignorance—you know there's no confidence to equal it. It's only when you know something about a profession, I think, that you're timid or careful.\"\n\nDavid Bordwell wrote that \"The best way to understand \"Citizen Kane\" is to stop worshiping it as a triumph of technique.\" Bordwell argues that the film did not invent any of its famous techniques such as deep focus cinematography, shots of the ceilings, chiaroscuro lighting and temporal jump-cuts, and many of these stylistics had been used in German Expressionist films of the 1920s, such as \"The Cabinet of Dr. Caligari\". But Bordwell asserts that the film did put them all together for the first time and perfected the medium in one single film. In a 1948 interview D. W. Griffith said \"I loved \"Citizen Kane\" and particularly loved the ideas he took from me.\"\n\nArguments against the film's cinematic innovations were made as early as 1946 when French historian Georges Sadoul wrote that \"the film is an encyclopedia of old techniques.\" Sadoul pointed out such examples as compositions that used both the foreground and the background in the films of Auguste and Louis Lumière, special effects used in the films of Georges Méliès, shots of the ceiling in Erich von Stroheim's \"Greed\" and newsreel montages in the films of Dziga Vertov.\n\nFrench film critic André Bazin defended the film and wrote that \"In this respect, the accusation of plagiarism could very well be extended to the film's use of panchromatic film or its exploitation of the properties of gelatinous silver halide.\" Bazin disagreed with Sadoul's comparison to Lumière's cinematography since \"Citizen Kane\" used more sophisticated lenses, but acknowledged that the film had similarities to such previous works as \"The 49th Parallel\" and \"The Power and the Glory\". Bazin stated that \"even if Welles did not invent the cinematic devices employed in \"Citizen Kane\", one should nevertheless credit him with the invention of their \"meaning\".\" Bazin championed the techniques in the film for its depiction of heightened reality, but Bordwell believes that the film's use of special effects contradict some of Bazin's theories.\n\n\"Citizen Kane\" eschews the traditional linear, chronological narrative, and tells Kane's story entirely in flashback using different points of view, many of them from Kane's aged and forgetful associates, the cinematic equivalent of the unreliable narrator in literature. Welles also dispenses with the idea of a single storyteller and uses multiple narrators to recount Kane's life. The use of multiple narrators was unheard of in Hollywood films. Each narrator recounts a different part of Kane's life, with each story partly overlapping. The film depicts Kane as an enigma, a complicated man who, in the end, leaves viewers with more questions than answers as to his character, such as the newsreel footage where he is attacked for being both a communist and a fascist.\n\nThe technique of using flashbacks had been used in earlier films—most notably in \"The Power and the Glory\" (1933)—but no film was as immersed in this technique as \"Citizen Kane\". The use of the reporter Thompson acts as a surrogate for the audience, questioning Kane's associates and piecing together his life.\n\nAt that time films typically had an \"omniscient perspective\", which Marilyn Fabe says give the audience the \"illusion that we are looking with impunity into a world which is unaware of our gaze, Hollywood movies give us a feeling of power.\" The film begins in this fashion up until the \"News on the March\" sequence, after which we the audience see the film through the perspectives of others. The \"News on the March\" sequence gives an overview of Kane's entire life (and the film's entire story) at the beginning of the film, leaving the audience without the typical suspense of wondering how it will end. Instead the film's repetitions of events compels the audience to analyze and wonder why Kane's life happened the way that it did, under the pretext of finding out what \"Rosebud\" means. The film then returns to the omniscient perspective in the final scene, when only the audience discovers what \"Rosebud\" is.\n\nThe most innovative technical aspect of \"Citizen Kane\" is the extended use of deep focus. In nearly every scene in the film, the foreground, background and everything in between are all in sharp focus. Cinematographer Toland did this through his experimentation with lenses and lighting. Toland described the achievement, made possible by the sensitivity of modern speed film, in an article for \"Theatre Arts\" magazine:\n\nNew developments in the science of motion picture photography are not abundant at this advanced stage of the game but periodically one is perfected to make this a greater art. Of these I am in an excellent position to discuss what is termed \"Pan-focus\", as I have been active for two years in its development and used it for the first time in \"Citizen Kane\". Through its use, it is possible to photograph action from a range of eighteen inches from the camera lens to over two hundred feet away, with extreme foreground and background figures and action both recorded in sharp relief. Hitherto, the camera had to be focused either for a close or a distant shot, all efforts to encompass both at the same time resulting in one or the other being out of focus. This handicap necessitated the breaking up of a scene into long and short angles, with much consequent loss of realism. With pan-focus, the camera, like the human eye, sees an entire panorama at once, with everything clear and lifelike.\n\nBoth this article and a May 1941 \"Life\" magazine article with illustrated examples helped popularize deep focus cinematography and Toland's achievements on the film.\n\nAnother unorthodox method used in the film was the way low-angle shots were used to display a point of view facing upwards, thus allowing ceilings to be shown in the background of several scenes. Breaking with studio convention, every set was built with a ceiling—many constructed of fabric that ingeniously concealed microphones. Welles felt that the camera should show what the eyes see, and that it was a bad theatrical convention to pretend there was no ceiling—\"a big lie in order to get all those terrible lights up there,\" he said. He became fascinated with the look of low angles, which made even dull interiors look interesting. One extremely low angle is used to photograph the encounter between Kane and Leland after Kane loses the election. A hole was dug for the camera, which required drilling into the concrete floor.\n\nWelles credited Toland on the same title card as himself and said \"It's impossible to say how much I owe to Gregg. He was superb.\" He called Toland \"the best director of photography that ever existed.\"\n\n\"Citizen Kane\"s sound was recorded by Bailey Fesler and re-recorded in post-production by audio engineer James G. Stewart, both of whom had worked in radio. Stewart said that Hollywood films never deviated from a basic pattern of how sound could be recorded or used, but with Welles \"deviation from the pattern was possible because he demanded it.\" Although the film is known for its complex soundtrack, much of the audio is heard as it was recorded by Fesler and without manipulation.\n\nWelles used techniques from radio like overlapping dialogue. The scene in which characters sing \"Oh, Mr. Kane\" was especially complicated and required mixing several soundtracks together. He also used different \"sound perspectives\" to create the illusion of distances, such as in scenes at Xanadu where characters speak to each other at far distances. Welles experimented with sound in post-production, creating audio montages, and chose to create all of the sound effects for the film instead of using RKO's library of sound effects.\n\nWelles used an aural technique from radio called the \"lightning-mix\". Welles used this technique to link complex montage sequences via a series of related sounds or phrases. For example, Kane grows from a child into a young man in just two shots. As Thatcher hands eight-year-old Kane a sled and wishes him a Merry Christmas, the sequence suddenly jumps to a shot of Thatcher fifteen years later, completing the sentence he began in both the previous shot and the chronological past. Other radio techniques include using a number of voices, each saying a sentence or sometimes merely a fragment of a sentence, and splicing the dialogue together in quick succession, such as the projection room scene. The film's sound cost $16,996, but was originally budgeted at $7,288.\n\nFilm critic and director François Truffaut wrote that \"Before \"Kane\", nobody in Hollywood knew how to set music properly in movies. \"Kane\" was the first, in fact the only, great film that uses radio techniques. ... A lot of filmmakers know enough to follow Auguste Renoir's advice to fill the eyes with images at all costs, but only Orson Welles understood that the sound track had to be filled in the same way.\" Cedric Belfrage of \"The Clipper\" wrote \"of all of the delectable flavours that linger on the palate after seeing \"Kane\", the use of sound is the strongest.\"\n\nThe make-up for \"Citizen Kane\" was created and applied by Maurice Seiderman (1907–1989), a junior member of the RKO make-up department. Seiderman's family came to the United States from Russia in 1920, escaping persecution. As a child Seiderman had won a drawing competition and received an apprenticeship at the Moscow Art Theatre, where his father was a wigmaker and make-up artist. In New York his uncle was a theatrical scenic painter, and he helped Seiderman get into the union. He worked on Max Reinhardt's 1924 production of \"The Miracle\" and with the Yiddish Art Theatre, and he studied the human figure at the Art Students League of New York. After he moved to Los Angeles he was hired first by Max Factor and then by RKO. Seiderman had not been accepted into the union, which recognized him as only an apprentice, but RKO nevertheless used him to make up principal actors.\n\n\"Apprentices were not supposed to make up any principals, only extras, and an apprentice could not be on a set without a journeyman present,\" wrote make-up artist Dick Smith, who became friends with Seiderman in 1979. \"During his years at RKO I suspect these rules were probably overlooked often.\" By 1940 Seiderman's uncredited film work included \"Winterset\", \"Gunga Din\", \"The Hunchback of Notre Dame\", \"Swiss Family Robinson\" and \"Abe Lincoln in Illinois\". \"Seiderman had gained a reputation as one of the most inventive and creatively precise up-and-coming makeup men in Hollywood,\" wrote biographer Frank Brady.\n\nOn an early tour of RKO, Welles met Seiderman in the small make-up lab he created for himself in an unused dressing room. \"Welles fastened on to him at once,\" wrote biographer Charles Higham. \"With his great knowledge of makeup—indeed, his obsession with it, for he hated his flat nose—Welles was fascinated ... Seiderman had an intimate knowledge of anatomy and the process of aging and was acquainted with every line, wrinkle and accretion of fat in aging men and women. Impatient with most makeup methods of his era, he used casts of his subjects in order to develop makeup methods that ensured complete naturalness of expression—a naturalness unrivaled in Hollywood.\"\n\n\"When \"Kane\" came out in script form, Orson told all of us about the picture and said that the most important aspect was the makeup,\" Seiderman recalled. \"I felt that I was being given an assignment that was unique—so I worked accordingly. And there was a lot of work to do. Straight makeups were done in the makeup department by staff, but all the trick stuff and the principal characters were my personal work; nobody else ever touched them. They could not have handled it.\"\n\nSeiderman developed a thorough plan for aging the principal characters, first making a plaster cast of the face of each of the actors who aged, except Joseph Cotten who was unavailable at that time. He made a plaster mold of Welles's body down to the hips.\n\n\"My sculptural techniques for the characters' aging were handled by adding pieces of white modeling clay, which matched the plaster, onto the surface of each bust,\" Seiderman told visual arts historian Norman Gambill. When Seiderman achieved the desired effect he cast the clay pieces in a soft plastic material that he formulated himself. These appliances were then placed onto the plaster bust and a four-piece mold was made for each phase of aging. The castings were then fully painted and paired with the appropriate wig for evaluation.\n\nBefore the actors went before the cameras each day, the pliable pieces were applied directly to their faces to recreate Seiderman's sculptural image. Welles was allergic to Max Factor's gum, so Seiderman invented an alternative that also photographed more realistically. The facial surface was underpainted in a flexible red plastic compound; Cotten recalled being instructed to puff out his cheeks during this process. Later, seeing the results in the mirror, Cotten told Seiderman, \"I am acting the part of a nice old gentleman, not a relief map of the Rocky Mountains.\" Seiderman replied, \"You'd be surprised at what the camera doesn't see unless we place it within its view. How about some more coffee?\"\n\nThe red ground resulted in a warmth of tone that was picked up by the sensitive panchromatic film. Over that was applied liquid greasepaint, and then finally a colorless translucent talcum. Seiderman created the effect of skin pores on Kane's face by stippling the surface with a negative cast he made from an orange peel.\n\nWelles was just as heavily made up as young Kane as he was for old Kane, and he often arrived on the set at 2:30 a.m. Application of the sculptural make-up for the oldest incarnation of the character took three-and-a-half hours. The make-up included appliances to age Welles's shoulders, breast and stomach. \"In the film and production photographs, you can see that Kane had a belly that overhung,\" Seiderman said. \"That was not a costume, it was the rubber sculpture that created the image. You could see how Kane's silk shirt clung wetly to the character's body. It could not have been done any other way.\"\n\nSeiderman worked with Charles Wright on the wigs. These went over a flexible skull cover that Seiderman created and sewed into place with elastic thread. When he found the wigs too full he untied one hair at a time to alter their shape. Kane's mustache was inserted into the makeup surface a few hairs at a time, to realistically vary the color and texture.\n\nSeiderman made scleral lenses for Welles, Dorothy Comingore, George Coulouris and Everett Sloane, to dull the brightness of their young eyes. The lenses took a long time to fit properly, and Seiderman began work on them before devising any of the other makeup. \"I painted them to age in phases, ending with the blood vessels and the \"Aurora Senilis\" of old age.\"\n\n\"Cotten was the only principal for whom I had not made any sculptural casts, wigs or lenses,\" Seiderman said. When Cotten's old-age scenes needed to be shot out of sequence due to Welles's injured ankle, Seiderman improvised with appliances made for Kane's make-up. A sun visor was chosen to conceal Cotten's low hairline and the lenses he wore—hastily supplied by a Beverly Hills ophthalmologist—were uncomfortable.\nSeiderman's tour de force, the breakfast montage, was shot all in one day. \"Twelve years, two years shot at each scene,\" he said. \"Please realize, by the way, that a two-year jump in age is a bit harder to accomplish visually than one of 20 years.\"\n\nAs they did with art direction, the major studios gave screen credit for make-up to only the department head. When RKO make-up department head Mel Berns refused to share credit with Seiderman, who was only an apprentice, Welles told Berns that there would be no make-up credit. Welles signed a large advertisement in the Los Angeles newspaper:\n\nTHANKS TO EVERYBODY WHO GETS SCREEN CREDIT FOR \"CITIZEN KANE\"<br>AND THANKS TO THOSE WHO DON'T<br>TO ALL THE ACTORS, THE CREW, THE OFFICE, THE MUSICIANS, EVERYBODY<br>AND PARTICULARLY TO MAURICE SEIDERMAN, THE BEST MAKE-UP MAN IN THE WORLD\n\n\"To put this event in context, remember that I was a very low man,\" Seiderman recalled. \"I wasn't even called a make-up man. I had started their laboratory and developed their plastic appliances for make-up. But my salary was $25 a week. And I had no union card.\"\n\nSeiderman told Gambill that after \"Citizen Kane\" was released, Welles was invited to a White House dinner where Frances Perkins was among the guests. Welles told her about the Russian immigrant who did the make-up for his film but could not join the union. Seiderman said the head of the union received a call from the Labor Department the next day, and in November 1941 he was a full union member.\n\nAlthough credited as an assistant, the film's art direction was done by Perry Ferguson. Welles and Ferguson got along during their collaboration. In the weeks before production began Welles, Toland and Ferguson met regularly to discuss the film and plan every shot, set design and prop. Ferguson would take notes during these discussions and create rough designs of the sets and story boards for individual shots. After Welles approved the rough sketches, Ferguson made miniature models for Welles and Toland to experiment on with a periscope in order to rehearse and perfect each shot. Ferguson then had detailed drawings made for the set design, including the film's lighting design. The set design was an integral part of the film's overall look and Toland's cinematography.\n\nIn the original script the Great Hall at Xanadu was modeled after the Great Hall in Hearst Castle and its design included a mixture of Renaissance and Gothic styles. \"The Hearstian element is brought out in the almost perverse juxtaposition of incongruous architectural styles and motifs,\" wrote Carringer. Before RKO cut the film's budget, Ferguson's designs were more elaborate and resembled the production designs of early Cecil B. DeMille films and \"Intolerance\". The budget cuts reduced Ferguson's budget by 33 percent and his work cost $58,775 total, which was below average at that time. To save costs Ferguson and Welles re-wrote scenes in Xanadu's living room and transported them to the Great Hall. A large staircase from another film was found and used at no additional cost. When asked about the limited budget, Ferguson said \"Very often—as in that much-discussed 'Xanadu' set in \"Citizen Kane\"—we can make a foreground piece, a background piece, and imaginative lighting suggest a great deal more on the screen than actually exists on the stage.\" According to the film's official budget there were 81 sets built, but Ferguson said there were between 106 and 116.\n\nStill photographs of Oheka Castle in Huntington, New York, were used in the opening montage, representing Kane's Xanadu estate. Ferguson also designed statues from Kane's collection with styles ranging from Greek to German Gothic. The sets were also built to accommodate Toland's camera movements. Walls were built to fold and furniture could quickly be moved. The film's famous ceilings were made out of muslin fabric and camera boxes were built into the floors for low angle shots. Welles later said that he was proud that the film production value looked much more expensive than the film's budget. Although neither worked with Welles again, Toland and Ferguson collaborated in several films in the 1940s.\n\nThe film's special effects were supervised by RKO department head Vernon L. Walker. Welles pioneered several visual effects to cheaply shoot things like crowd scenes and large interior spaces. For example, the scene in which the camera in the opera house rises dramatically to the rafters, to show the workmen showing a lack of appreciation for Susan Alexander Kane's performance, was shot by a camera craning upwards over the performance scene, then a curtain wipe to a miniature of the upper regions of the house, and then another curtain wipe matching it again with the scene of the workmen. Other scenes effectively employed miniatures to make the film look much more expensive than it truly was, such as various shots of Xanadu.\n\nSome shots included rear screen projection in the background, such as Thompson's interview of Leland and some of the ocean backgrounds at Xanadu. Bordwell claims that the scene where Thatcher agrees to be Kane's guardian used rear screen projection to depict young Kane in the background, despite this scene being cited as a prime example of Toland's deep focus cinematography. A special effects camera crew from Walker's department was required for the extreme close-up shots such as Kane's lips when he says \"Rosebud\" and the shot of the typewriter typing Susan's bad review.\n\nOptical effects artist Dunn claimed that \"up to 80 percent of some reels was optically printed.\" These shots were traditionally attributed to Toland for years. The optical printer improved some of the deep focus shots. One problem with the optical printer was that it sometimes created excessive graininess, such as the optical zoom out of the snow globe. Welles decided to superimpose snow falling to mask the graininess in these shots. Toland said that he disliked the results of the optical printer, but acknowledged that \"RKO special effects expert Vernon Walker, ASC, and his staff handled their part of the production—a by no means inconsiderable assignment—with ability and fine understanding.\"\n\nAny time deep focus was impossible—as in the scene in which Kane finishes a negative review of Susan's opera while at the same time firing the person who began writing the review—an optical printer was used to make the whole screen appear in focus, visually layering one piece of film onto another. However, some apparently deep-focus shots were the result of in-camera effects, as in the famous scene in which Kane breaks into Susan's room after her suicide attempt. In the background, Kane and another man break into the room, while simultaneously the medicine bottle and a glass with a spoon in it are in closeup in the foreground. The shot was an in-camera matte shot. The foreground was shot first, with the background dark. Then the background was lit, the foreground darkened, the film rewound, and the scene re-shot with the background action.\n\nThe film's music was composed by Bernard Herrmann. Herrmann had composed for Welles for his Mercury Theatre radio broadcasts. Because it was Herrmann's first motion picture score, RKO wanted to pay him only a small fee, but Welles insisted he be paid at the same rate as Max Steiner.\n\nThe score established Herrmann as an important new composer of film soundtracks and eschewed the typical Hollywood practice of scoring a film with virtually non-stop music. Instead Herrmann used what he later described as '\"radio scoring\", musical cues typically 5–15 seconds in length that bridge the action or suggest a different emotional response. The breakfast montage sequence begins with a graceful waltz theme and gets darker with each variation on that theme as the passage of time leads to the hardening of Kane's personality and the breakdown of his first marriage.\n\nHerrmann realized that musicians slated to play his music were hired for individual unique sessions; there was no need to write for existing ensembles. This meant that he was free to score for unusual combinations of instruments, even instruments that are not commonly heard. In the opening sequence, for example, the tour of Kane's estate Xanadu, Herrmann introduces a recurring leitmotiv played by low woodwinds, including a quartet of alto flutes.\n\nFor Susan Alexander Kane's operatic sequence, Welles suggested that Herrmann compose a witty parody of a Mary Garden vehicle, an aria from \"Salammbô\". \"Our problem was to create something that would give the audience the feeling of the quicksand into which this simple little girl, having a charming but small voice, is suddenly thrown,\" Herrmann said. Writing in the style of a 19th-century French Oriental opera, Herrmann put the aria in a key that would force the singer to strain to reach the high notes, culminating in a high D, well outside the range of Susan Alexander. Soprano Jean Forward dubbed the vocal part for Comingore. Houseman claimed to have written the libretto, based on Jean Racine's \"Athalie\" and \"Phedre\", although some confusion remains since Lucille Fletcher remembered preparing the lyrics. Fletcher, then Herrmann's wife, wrote the libretto for his opera \"Wuthering Heights\".\n\nMusic enthusiasts consider the scene in which Susan Alexander Kane attempts to sing the famous cavatina \"Una voce poco fa\" from \"Il barbiere di Siviglia\" by Gioachino Rossini with vocal coach Signor Matiste as especially memorable for depicting the horrors of learning music through mistakes.\n\nIn 1972, Herrmann said, \"I was fortunate to start my career with a film like \"Citizen Kane\", it's been a downhill run ever since!\" Welles loved Herrmann's score and told director Henry Jaglom that it was 50 percent responsible for the film's artistic success.\n\nSome incidental music came from other sources. Welles heard the tune used for the publisher's theme, \"Oh, Mr. Kane\", in Mexico. Called \"A Poco No\", the song was written by Pepe Guízar and special lyrics were written by Herman Ruby.\n\n\"In a Mizz\", a 1939 jazz song by Charlie Barnet and Haven Johnson, bookends Thompson's second interview of Susan Alexander Kane. \"I kind of based the whole scene around that song,\" Welles said. \"The music is by Nat Cole—it's his trio.\" Later—beginning with the lyrics, \"It can't be love\"—\"In a Mizz\" is performed at the Everglades picnic, framing the fight in the tent between Susan and Kane. Musicians including bandleader Cee Pee Johnson (drums), Alton Redd (vocals), Raymond Tate (trumpet), Buddy Collette (alto sax) and Buddy Banks (tenor sax) are featured.\n\nAll of the music used in the newsreel came from the RKO music library, edited at Welles's request by the newsreel department to achieve what Herrmann called \"their own crazy way of cutting\". The \"News on the March\" theme that accompanies the newsreel titles is \"Belgian March\" by Anthony Collins, from the film \"Nurse Edith Cavell\". Other examples are an excerpt from Alfred Newman's score for \"Gunga Din\" (the exploration of Xanadu), Roy Webb's theme for the film \"Reno\" (the growth of Kane's empire), and bits of Webb's score for \"Five Came Back\" (introducing Walter Parks Thatcher).\n\nOne of the editing techniques used in \"Citizen Kane\" was the use of montage to collapse time and space, using an episodic sequence on the same set while the characters changed costume and make-up between cuts so that the scene following each cut would look as if it took place in the same location, but at a time long after the previous cut. In the breakfast montage, Welles chronicles the breakdown of Kane's first marriage in five vignettes that condense 16 years of story time into two minutes of screen time. Welles said that the idea for the breakfast scene \"was stolen from \"The Long Christmas Dinner\" of Thornton Wilder ... a one-act play, which is a long Christmas dinner that takes you through something like 60 years of a family's life.\" The film often uses long dissolves to signify the passage of time and its psychological effect of the characters, such as the scene in which the abandoned sled is covered with snow after the young Kane is sent away with Thatcher.\n\nWelles was influenced by the editing theories of Sergei Eisenstein by using jarring cuts that caused \"sudden graphic or associative contrasts\", such as the cut from Kane's deathbed to the beginning of the \"News on the March\" sequence and a sudden shot of a shrieking bird at the beginning of Raymond's flashback. Although the film typically favors mise-en-scène over montage, the scene in which Kane goes to Susan Alexander's apartment after first meeting her is the only one that is primarily cut as close-ups with shots and counter shots between Kane and Susan. Fabe says that \"by using a standard Hollywood technique sparingly, [Welles] revitalizes its psychological expressiveness.\"\n\nIn her 1992 monograph for the British Film Institute, critic Laura Mulvey explored the anti-fascist themes of \"Citizen Kane\". The \"News on the March\" newsreel presents Kane keeping company with Hitler and other dictators while he smugly assures the public there will be no war. Mulvey wrote that the film reflects \"the battle between intervention and isolationism\" then being waged in the United States; the film was released six months before the attack on Pearl Harbor, while President Franklin D. Roosevelt was laboring to win public opinion for entering World War II. \"Not only was the war in Europe the burning public issue of the time,\" Mulvey wrote, \"it was of passionate personal importance to Orson Welles ... In the rhetoric of \"Citizen Kane\", the destiny of isolationism is realised in metaphor: in Kane's own fate, dying wealthy and lonely, surrounded by the detritus of European culture and history.\"\n\nJournalist Ignacio Ramonet has cited the film as an early example of mass media manipulation of public opinion and the power that media conglomerates have on influencing the democratic process. Ramonet believes that this early example of a media mogul influencing politics is outdated and that \"today Citizen Kane would be a dwarf. He owned a few papers in one country. The forces that dominate today have integrated image with text and sound and the world is their market. There are media groups with the power of a thousand Citizen Kanes.\" Media mogul Rupert Murdoch is sometimes labeled as a latter-day \"Citizen Kane\".\nTo ensure that \"Citizen Kane\"s influence from Hearst's life was a secret, Welles limited access to dailies and managed the film's publicity. A December 1940 feature story in \"Stage\" magazine compared the film's narrative to \"Faust\" and made no mention of Hearst.\n\nThe film was scheduled to premiere at RKO's flagship theater Radio City Music Hall on February 14, but in early January 1941 Welles was not finished with post-production work and told RKO that it still needed its musical score. Writers for national magazines had early deadlines and so a rough cut was previewed for a select few on January 3, 1941 for such magazines as \"Life\", \"Look\" and \"Redbook\". Gossip columnist Hedda Hopper (and Parsons' arch rival) showed up to the screening uninvited. Most of the critics at the preview said that they liked the film and gave it good advanced reviews. Hopper wrote negatively about it, calling the film a \"vicious and irresponsible attack on a great man\" and criticizing its corny writing and old fashioned photography. \"Friday\" magazine ran an article drawing point-by-point comparisons between Kane and Hearst and documented how Welles had led on Parsons, Hollywood correspondent for Hearst papers. Up until this Welles had been friendly with Parsons. The magazine quoted Welles as saying that he couldn't understand why she was so nice to him and that she should \"wait until the woman finds out that the picture's about her boss.\" Welles immediately denied making the statement and the editor of \"Friday\" admitted that it may be false. Welles apologized to Parsons and assured her that he had never made that remark.\nShortly after \"Friday\"s article, Hearst sent Parsons an angry letter complaining that he had learned about \"Citizen Kane\" from Hopper and not her. The incident made a fool of Parsons and compelled her to start attacking Welles and the film. Parsons demanded a private screening of the film and personally threatened Schaefer on Hearst's behalf, first with a lawsuit and then with a vague threat of consequences for everyone in Hollywood. On January 10 Parsons and two lawyers working for Hearst were given a private screening of the film. James G. Stewart was present at the screening and said that she walked out of the film. Soon after, Parsons called Schaefer and threatened RKO with a lawsuit if they released \"Kane\". She also contacted the management of Radio City Music Hall and demanded that they not screen it. The next day, the front page headline in \"Daily Variety\" read, \"HEARST BANS RKO FROM PAPERS.\" Hearst began this ban by suppressing promotion of RKO's \"Kitty Foyle\", but in two weeks the ban was lifted for everything except \"Kane.\"\n\nWhen Schaefer did not submit to Parsons she called other studio heads and made more threats on behalf of Hearst to expose the private lives of people throughout the entire film industry. Welles was threatened with an exposé about his romance with the married actress Dolores del Rio, who wanted the affair kept secret until her divorce was finalized. In a statement to journalists Welles denied that the film was about Hearst. Hearst began preparing an injunction against the film for libel and invasion of privacy, but Welles's lawyer told him that he doubted Hearst would proceed due to the negative publicity and required testimony that an injunction would bring.\n\n\"The Hollywood Reporter\" ran a front-page story on January 13 that Hearst papers were about to run a series of editorials attacking Hollywood's practice of hiring refugees and immigrants for jobs that could be done by Americans. The goal was to put pressure on the other studios to force RKO to shelve \"Kane\". Many of those immigrants had fled Europe after the rise of fascism and feared losing the safe haven of the United States. Soon afterwards, Schaefer was approached by Nicholas Schenck, head of Metro-Goldwyn-Mayer's parent company, with an offer on the behalf of Louis B. Mayer and other Hollywood executives to RKO Pictures of $805,000 to destroy all prints of the film and burn the negative. Once RKO's legal team reassured Schaefer, the studio announced on January 21 that \"Kane\" would be released as scheduled, and with one of the largest promotional campaigns in the studio's history. Schaefer brought Welles to New York City for a private screening of the film with the New York corporate heads of the studios and their lawyers. There was no objection to its release provided that certain changes, including the removal or softening of specific references that might offend Hearst, were made. Welles agreed and cut the running time from 122 minutes to 119 minutes. The cuts satisfied the corporate lawyers.\n\nHearing about \"Citizen Kane\" enraged Hearst so much that he banned any advertising, reviewing, or mentioning of it in his papers, and had his journalists libel Welles. Welles used Hearst's opposition as a pretext for previewing the film in several opinion-making screenings in Los Angeles, lobbying for its artistic worth against the hostile campaign that Hearst was waging. A special press screening took place in early March. Henry Luce was in attendance and reportedly wanted to buy the film from RKO for $1 million to distribute it himself. The reviews for this screening were positive. A \"Hollywood Review\" headline read, \"Mr. Genius Comes Through; 'Kane' Astonishing Picture\". The \"Motion Picture Herald\" reported about the screening and Welles's intention to sue RKO. \"Time\" magazine wrote that \"The objection of Mr. Hearst, who founded a publishing empire on sensationalism, is ironic. For to most of the several hundred people who have seen the film at private screenings, \"Citizen Kane\" is the most sensational product of the U.S. movie industry.\" A second press screening occurred in April.\n\nWhen Schaefer rejected Hearst's offer to suppress the film, Hearst banned every newspaper and station in his media conglomerate from reviewing—or even mentioning—the film. He also had many movie theaters ban it, and many did not show it through fear of being socially exposed by his massive newspaper empire. The Oscar-nominated documentary \"The Battle Over Citizen Kane\" lays the blame for the film's relative failure squarely at the feet of Hearst. The film did decent business at the box office; it went on to be the sixth highest grossing film in its year of release, a modest success its backers found acceptable. Nevertheless, the film's commercial performance fell short of its creators' expectations. Hearst's biographer David Nasaw points out that Hearst's actions were not the only reason \"Kane\" failed, however: the innovations Welles made with narrative, as well as the dark message at the heart of the film (that the pursuit of success is ultimately futile) meant that a popular audience could not appreciate its merits.\n\nHearst's attacks against Welles went beyond attempting to suppress the film. Welles said that while he was on his post-filming lecture tour a police detective approached him at a restaurant and advised him not to go back to his hotel. A 14-year-old girl had reportedly been hidden in the closet of his room, and two photographers were waiting for him to walk in. Knowing he would be jailed after the resulting publicity, Welles did not return to the hotel but waited until the train left town the following morning. \"But that wasn't Hearst,\" Welles said, \"that was a hatchet man from the local Hearst paper who thought he would advance himself by doing it.\"\n\nIn March 1941 Welles directed a Broadway version of Richard Wright's \"Native Son\" (and, for luck, used a \"Rosebud\" sled as a prop). \"Native Son\" received positive reviews, but Hearst-owned papers used the opportunity to attack Welles as a communist. The Hearst papers vociferously attacked Welles after his April 1941 radio play, \"His Honor, the Mayor\", produced for The Free Company radio series on CBS.\n\nWelles described his chance encounter with Hearst in an elevator at the Fairmont Hotel on the night \"Citizen Kane\" opened in San Francisco. Hearst and Welles's father were acquaintances, so Welles introduced himself and asked Hearst if he would like to come to the opening. Hearst did not respond. \"As he was getting off at his floor, I said, 'Charles Foster Kane would have accepted.' No reply\", recalled Welles. \"And Kane would have you know. That was his style—just as he finished Jed Leland's bad review of Susan as an opera singer.\"\n\nIn 1945 Hearst journalist Robert Shaw wrote that the film got \"a full tide of insensate fury\" from Hearst papers, \"then it ebbed suddenly. With one brain cell working, the chief realized that such hysterical barking by the trained seals would attract too much attention to the picture. But to this day the name of Orson Welles is on the official son-of-a-bitch list of every Hearst newspaper.\"\n\nDespite Hearst's attempts to destroy the film, since 1941 references to his life and career have usually included a reference to \"Citizen Kane\", such as the headline 'Son of Citizen Kane Dies' for the obituary of Hearst's son. In 2012 the Hearst estate agreed to screen the film at Hearst Castle in San Simeon, breaking Hearst's ban on the film.\n\nRadio City Music Hall's management refused to screen \"Citizen Kane\" for its premiere. A possible factor was Parsons's threat that \"The American Weekly\" would run a defamatory story on the grandfather of major RKO stockholder Nelson Rockefeller. Other exhibitors feared being sued for libel by Hearst and refused to show the film. In March Welles threatened the RKO board of governors with a lawsuit if they did not release the film. Schaefer stood by Welles and opposed the board of governors. When RKO still delayed the film's release Welles offered to buy the film for $1 million and the studio finally agreed to release the film on May 1.\n\nSchaefer managed to book a few theaters willing to show the film. Hearst papers refused to accept advertising. RKO's publicity advertisements for the film erroneously promoted it as a love story.\n\n\"Kane\" opened at the RKO Palace Theatre on Broadway in New York on May 1, 1941, in Chicago on May 6, and in Los Angeles on May 8. Welles said that at the Chicago premiere that he attended the theater was almost empty. It did well in cities and larger towns but fared poorly in more remote areas. RKO still had problems getting exhibitors to show the film. For example, one chain controlling more than 500 theaters got Welles's film as part of a package but refused to play it, reportedly out of fear of Hearst. Hearst's disruption of the film's release damaged its box office performance and, as a result, it lost $160,000 during its initial run. The film earned $23,878 during its first week in New York. By the ninth week it only made $7,279. Overall it lost money in New York, Boston, Chicago, Los Angeles, San Francisco and Washington, D.C., but made a profit in Seattle.\n\n\"Citizen Kane\" received good reviews from several critics. \"New York Daily News\" critic Kate Cameron called it \"one of the most interesting and technically superior films that has ever come out of a Hollywood studio\". \"New York World-Telegram\" critic William Boehnel said that the film was \"staggering and belongs at once among the greatest screen achievements\". \"Time\" magazine wrote that \"it has found important new techniques in picture-making and story-telling.\" \"Life\" magazine's review said that \"few movies have ever come from Hollywood with such powerful narrative, such original technique, such exciting photography.\" John C. Mosher of \"The New Yorker\" called the film's style \"like fresh air\" and raved \"Something new has come to the movie world at last.\" Anthony Bower of \"The Nation\" called it \"brilliant\" and praised the cinematography and performances by Welles, Comingore and Cotten. John O'Hara's \"Newsweek\" review called it the best picture he'd ever seen and said Welles was \"the best actor in the history of acting.\" Welles called O'Hara's review \"the greatest review that anybody ever had.\"\n\nThe day following the premiere of \"Citizen Kane,\" \"The New York Times\" critic Bosley Crowther wrote that \"... it comes close to being the most sensational film ever made in Hollywood.\"\n\nCount on Mr. Welles: he doesn't do things by halves. ... Upon the screen he discovered an area large enough for his expansive whims to have free play. And the consequence is that he has made a picture of tremendous and overpowering scope, not in physical extent so much as in its rapid and graphic rotation of thoughts. Mr. Welles has put upon the screen a motion picture that really moves.\n\nIn the UK C. A. Lejeune of \"The Observer\" called it \"The most exciting film that has come out of Hollywood in twenty-five years\" and Dilys Powell of \"The Sunday Times\" said the film's style was made \"with the ease and boldness and resource of one who controls and is not controlled by his medium.\" Edward Tangye Lean of \"Horizon\" praised the film's technical style, calling it \"perhaps a decade ahead of its contemporaries.\"\n\nA few reviews were mixed. Otis Ferguson of \"The New Republic\" said it was \"the boldest free-hand stroke in major screen production since Griffith and Bitzer were running wild to unshackle the camera\", but also criticized its style, calling it a \"retrogression in film technique\" and stating that \"it holds no great place\" in film history. In a rare film review, filmmaker Erich von Stroheim criticized the film's story and non-linear structure, but praised the technical style and performances, and wrote \"Whatever the truth may be about it, \"Citizen Kane\" is a great picture and will go down in screen history. More power to Welles!\"\n\nSome prominent critics wrote negative reviews. In his 1941 review for \"Sur\", Jorge Luis Borges famously called the film \"a labyrinth with no center\" and predicted that its legacy would be a film \"whose historical value is undeniable but which no one cares to see again.\" \"The Argus Weekend Magazine\" critic Erle Cox called the film \"amazing\" but thought that Welles's break with Hollywood traditions was \"overdone.\" \"Tatler\"s James Agate called it \"the well-intentioned, muddled, amateurish thing one expects from high-brows\" and \"a quite good film which tries to run the psychological essay in harness with your detective thriller, and doesn't quite succeed.\" Eileen Creelman of \"The New York Sun\" called it \"a cold picture, unemotional, a puzzle rather than a drama\". Other people who disliked the film were W. H. Auden and James Agee.\n\n\"Citizen Kane\" received the New York Film Critics Circle Award for Best Picture. The National Board of Review voted it Best Film of 1941, and recognized Welles and Coulouris for their performances.\n\n\"Citizen Kane\" received nine nominations at the 1941 Academy Awards:\n\nIt was widely believed the film would win most of its Oscar nominations, but it received only the award for Best Writing (Original Screenplay), shared by Welles and Mankiewicz. \"Variety\" reported that block voting by screen extras deprived \"Citizen Kane\" of Academy Awards for Best Picture and Best Actor (Welles), and similar prejudices were likely to have been responsible for the film receiving no technical awards.\n\n\"Citizen Kane\" was the only film made under Welles's original contract with RKO Pictures, which gave him complete creative control. Welles's new business manager and attorney permitted the contract to lapse. In July 1941, Welles reluctantly signed a new and less favorable deal with RKO under which he produced and directed \"The Magnificent Ambersons\" (1942), produced \"Journey into Fear\" (1943), and began \"It's All True\", a film he agreed to do without payment. In the new contract Welles was an employee of the studio and lost the right to final cut, which later allowed RKO to modify and re-cut \"The Magnificent Ambersons\" over his objections. In June 1942 Schaefer resigned the presidency of RKO Pictures and Welles's contract was terminated by his successor.\n\nDuring World War II, \"Citizen Kane\" was not seen in most European countries. It was shown in France for the first time on July 10, 1946 at the Marbeuf theatre in Paris. Initially most French film critics were influenced by the negative reviews of Jean-Paul Sartre in 1945 and Georges Sadoul in 1946. At that time many French intellectuals and filmmakers shared Sartre's negative opinion that Hollywood filmmakers were uncultured. Sartre criticized the film's flashbacks for its nostalgic and romantic preoccupation with the past instead of the realities of the present and said that \"the whole film is based on a misconception of what cinema is all about. The film is in the past tense, whereas we all know that cinema has got to be in the present tense.\"\nAndré Bazin, a little-known film critic working for Sartre's \"Les Temps modernes\", was asked to give an impromptu speech about the film after a screening at the Colisée Theatre in the autumn of 1946 and changed the opinion of much of the audience. This speech led to Bazin's 1947 article \"The Technique of Citizen Kane\", which directly influenced public opinion about the film. Carringer wrote that Bazin was \"the one who did the most to enhance the film's reputation.\" Both Bazin's critique of the film and his theories about cinema itself centered around his strong belief in mise en scène. These theories were diametrically opposed to both the popular Soviet montage theory and the politically Marxist and anti-Hollywood beliefs of most French film critics at that time. Bazin believed that a film should depict reality without the filmmaker imposing their \"will\" on the spectator, which the Soviet theory supported. Bazin wrote that \"Citizen Kane\"s mise en scène created a \"new conception of filmmaking\" and that the freedom given to the audience from the deep focus shots was innovative by changing the entire concept of the cinematic image. Bazin wrote extensively about the mise en scène in the scene where Susan Alexander attempts suicide, which was one long take while other films would have used four or five shots in the scene. Bazin wrote that the film's mise en scène \"forces the spectator to participate in the meaning of the film\" and creates \"a psychological realism which brings the spectator back to the real conditions of perception.\"\n\nIn his 1950 essay \"The Evolution of the Language of Cinema\", Bazin placed \"Citizen Kane\" center stage as a work which ushered in a new period in cinema. One of the first critics to defend motion pictures as being on the same artistic level as literature or painting, Bazin often used the film as an example of cinema as an art form and wrote that \"Welles has given the cinema a theoretical restoration. He has enriched his filmic repertory with new or forgotten effects that, in today's artistic context, take on a significance we didn't know they could have.\" Bazin also compared the film to Roberto Rossellini's \"Paisà\" for having \"the same aesthetic concept of realism\" and to the films of William Wyler shot by Toland (such as \"The Little Foxes\" and \"The Best Years of Our Lives\"), all of which used deep focus cinematography that Bazin called \"a dialectical step forward in film language.\"\n\nBazin's praise of the film went beyond film theory and reflected his own philosophy towards life itself. His metaphysical interpretations about the film reflected humankind's place in the universe. Bazin believed that the film examined one person's identity and search for meaning. It portrayed the world as ambiguous and full of contradictions, whereas films up until then simply portrayed people's actions and motivations. Bazin's biographer Dudley Andrew wrote that:\n\nThe world of \"Citizen Kane\", that mysterious, dark, and infinitely deep world of space and memory where voices trail off into distant echoes and where meaning dissolves into interpretation, seemed to Bazin to mark the starting point from which all of us try to construct provisionally the sense of our lives.\n\nBazin went on to co-found \"Cahiers du cinéma\", whose contributors (including future film directors François Truffaut and Jean-Luc Godard) also praised the film. The popularity of Truffaut's auteur theory helped the film's and Welles's reputation.\n\nBy 1942 \"Citizen Kane\" had run its course theatrically and, apart from a few showings at big city arthouse cinemas, it largely vanished and both the film's and Welles's reputation fell among American critics. In 1949 critic Richard Griffith in his overview of cinema, \"The Film Till Now\", dismissed \"Citizen Kane\" as \"... tinpot if not crackpot Freud.\"\n\nIn the United States, it was neglected and forgotten until its revival on television in the mid-1950s. Three key events in 1956 led to its re-evaluation in the United States: first, RKO was one of the first studios to sell its library to television, and early that year \"Citizen Kane\" started to appear on television; second, the film was re-released theatrically to coincide with Welles's return to the New York stage, where he played \"King Lear\"; and third, American film critic Andrew Sarris wrote \"Citizen Kane: The American Baroque\" for \"Film Culture\", and described it as \"the great American film\" and \"the work that influenced the cinema more profoundly than any American film since \"Birth of a Nation\".\" Carringer considers Sarris's essay as the most important influence on the film's reputation in the US.\n\nDuring Expo 58, a poll of over 100 film historians named \"Kane\" one of the top ten greatest films ever made (the group gave first-place honors to \"The Battleship Potemkin\"). When a group of young film directors announced their vote for the top six, they were booed for not including the film.\n\nIn the decades since, its critical status as the greatest film ever made has grown, with numerous essays and books on it including Peter Cowie's \"The Cinema of Orson Welles\", Ronald Gottesman's \"Focus on Citizen Kane\", a collection of significant reviews and background pieces, and most notably Kael's essay, \"Raising Kane\", which promoted the value of the film to a much wider audience than it had reached before. Despite its criticism of Welles, it further popularized the notion of \"Citizen Kane\" as the great American film. The rise of art house and film society circuits also aided in the film's rediscovery. David Thomson said that the film 'grows with every year as America comes to resemble it.\"\n\nThe British magazine \"Sight & Sound\" has produced a Top Ten list surveying film critics every decade since 1952, and is regarded as one of the most respected barometers of critical taste. \"Citizen Kane\" was a runner up to the top 10 in its 1952 poll but was voted as the greatest film ever made in its 1962 poll, retaining the top spot in every subsequent poll until 2012, when \"Vertigo\" displaced it.\n\nThe film has also ranked number one in the following film \"best of\" lists: Julio Castedo's \"The 100 Best Films of the Century\", Cahiers du cinéma's 100 films pour une cinémathèque idéale, \"Kinovedcheskie Zapiski\", \"Time Out\" magazine's Top 100 Films (Centenary), \"The Village Voice\"s 100 Greatest Films, and The Royal Belgian Film Archive's Most Important and Misappreciated American Films.\n\nRoger Ebert called \"Citizen Kane\" the greatest film ever made: \"But people don't always ask about the greatest film. They ask, 'What's your favorite movie?' Again, I always answer with \"Citizen Kane\".\"\n\nIn 1989, the United States Library of Congress deemed the film \"culturally, historically, or aesthetically significant\" and selected it for preservation in the National Film Registry. \"Citizen Kane\" was one of the first 25 films inducted into the registry.\n\nOn February 18, 1999, the United States Postal Service honored \"Citizen Kane\" by including it in its Celebrate the Century series. The film was honored again February 25, 2003, in a series of U.S. postage stamps marking the 75th anniversary of the Academy of Motion Picture Arts and Sciences. Art director Perry Ferguson represents the behind-the-scenes craftsmen of filmmaking in the series; he is depicted completing a sketch for \"Citizen Kane\".\n\n\"Citizen Kane\" was ranked number one in the American Film Institute's polls of film industry artists and leaders in 1998 and 2007. \"Rosebud\" was chosen as the 17th most memorable movie quotation in a 2005 AFI poll. The film's score was one of 250 nominees for the top 25 film scores in American cinema in another 2005 AFI poll.\n\nIn 2012, the Motion Picture Editors Guild published a list of the 75 best-edited films of all time based on a survey of its membership. \"Citizen Kane\" was listed second.\n\nThe film currently has a 100% rating at Rotten Tomatoes, based on 73 reviews by approved critics, with an average rating of 9.4/10. The site's consensus states: \"Orson Welles's epic tale of a publishing tycoon's rise and fall is entertaining, poignant, and inventive in its storytelling, earning its reputation as a landmark achievement in film.\"\n\n\"Citizen Kane\" has been called the most influential film of all time. Richard Corliss has asserted that Jules Dassin's 1941 film \"The Tell-Tale Heart\" was the first example of its influence and the first pop culture reference to the film occurred later in 1941 when the spoof comedy \"Hellzapoppin'\" featured a \"Rosebud\" sled. The film's cinematography was almost immediately influential and in 1942 \"American Cinematographer\" wrote \"without a doubt the most immediately noticeable trend in cinematography methods during the year was the trend toward crisper definition and increased depth of field.\"\n\nThe cinematography influenced John Huston's \"The Maltese Falcon\". Cinematographer Arthur Edeson used a wider-angle lens than Toland and the film includes many long takes, low angles and shots of the ceiling, but it did not use deep focus shots on large sets to the extent that \"Citizen Kane\" did. Edeson and Toland are often credited together for revolutionizing cinematography in 1941. Toland's cinematography influenced his own work on \"The Best Years of Our Lives\". Other films influenced include \"Gaslight\", \"Mildred Pierce\" and \"Jane Eyre\". Cinematographer Kazuo Miyagawa said that his use of deep focus was influenced by \"the camera work of Gregg Toland in \"Citizen Kane\"\" and not by traditional Japanese art.\n\nIts cinematography, lighting, and flashback structure influenced such film noirs of the 1940s and 1950s as \"The Killers\", \"Keeper of the Flame\", \"Caught\", \"The Great Man\" and \"This Gun for Hire\". David Bordwell and Kristin Thompson have written that \"For over a decade thereafter American films displayed exaggerated foregrounds and somber lighting, enhanced by long takes and exaggerated camera movements.\" However, by the 1960s filmmakers such as those from the French New Wave and Cinéma vérité movements favored \"flatter, more shallow images with softer focus\" and \"Citizen Kane\"s style became less fashionable. American filmmakers in the 1970s combined these two approaches by using long takes, rapid cutting, deep focus and telephoto shots all at once. Its use of long takes influenced films such as \"The Asphalt Jungle\", and its use of deep focus cinematography influenced \"Gun Crazy\", \"The Whip Hand\", \"The Devil's General\" and \"Justice Is Done\". The flashback structure in which different characters have conflicting versions of past events influenced \"La commare secca\" and \"Man of Marble\".\n\nThe film's structure influenced the biographical films \"Lawrence of Arabia\" and \"\"—which begin with the subject's death and show their life in flashbacks—as well as Welles's thriller \"Mr. Arkadin\". Rosenbaum sees similarities in the film's plot to \"Mr. Arkadin\", as well as the theme of nostalgia for loss of innocence throughout Welles's career, beginning with \"Citizen Kane\" and including \"The Magnificent Ambersons\", \"Mr. Arkadin\" and \"Chimes at Midnight\". Rosenbaum also points out how the film influenced Warren Beatty's \"Reds\". The film depicts the life of Jack Reed through the eyes of Louise Bryant, much as Kane's life is seen through the eyes of Thompson and the people who he interviews. Rosenbaum also compared the romantic montage between Reed and Bryant with the breakfast table montage in \"Citizen Kane\".\n\nAkira Kurosawa's \"Rashomon\" is often compared to the film due to both having complicated plot structures told by multiple characters in the film. Welles said his initial idea for the film was \"Basically, the idea \"Rashomon\" used later on,\" however Kurosawa had not yet seen the film before making \"Rashomon\" in 1950. Nigel Andrews has compared the film's complex plot structure to \"Rashomon\", \"Last Year at Marienbad\", \"Memento\" and \"Magnolia\". Andrews also compares Charles Foster Kane to Michael Corleone in \"The Godfather\", Jake LaMotta in \"Raging Bull\" and Daniel Plainview in \"There Will Be Blood\" for their portrayals of \"haunted megalomaniac[s], presiding over the shards of [their] own [lives].\"\n\nThe films of Paul Thomas Anderson have been compared to it. \"Variety\" compared \"There Will Be Blood\" to the film and called it \"one that rivals \"Giant\" and \"Citizen Kane\" in our popular lore as origin stories about how we came to be the people we are.\" \"The Master\" has been called \"movieland's only spiritual sequel to \"Citizen Kane\" that doesn't shrivel under the hefty comparison\" and the film's loose depiction of L. Ron Hubbard has been compared to \"Citizen Kane\"s depiction of Hearst. \"The Social Network\" has been compared to the film for its depiction of a media mogul and by the character Erica Albright being similar to \"Rosebud\". The controversy of the Sony hacking before the release of \"The Interview\" brought comparisons of Hearst's attempt to suppress the film. The film's plot structure and some specific shots influenced Todd Haynes's \"Velvet Goldmine\". Abbas Kiarostami's \"The Traveler\" has been called \"the \"Citizen Kane\" of the Iranian children's cinema.\" The film's use of overlapping dialogue has influenced the films of Robert Altman and Carol Reed. Reed's films \"Odd Man Out\", \"The Third Man\" (in which Welles and Cotten appeared) and \"Outcast of the Islands\" were also influenced by the film's cinematography.\n\nMany directors have listed it as one of the greatest films ever made, including Woody Allen, Michael Apted, Les Blank, Kenneth Branagh, Paul Greengrass, Michel Hazanavicius, Michael Mann, Sam Mendes, Jiri Menzel, Paul Schrader, Martin Scorsese, Denys Arcand, Gillian Armstrong, John Boorman, Roger Corman, Alex Cox, Milos Forman, Norman Jewison, Richard Lester, Richard Linklater, Paul Mazursky, Ronald Neame, Sydney Pollack and Stanley Kubrick. Yasujirō Ozu said it was his favorite non-Japanese film and was impressed by its techniques. François Truffaut said that the film \"has inspired more vocations to cinema throughout the world than any other\" and recognized its influence in \"The Barefoot Contessa\", \"Les Mauvaises Rencontres\", \"Lola Montès\", and \"8 1/2\". Truffaut's \"Day for Night\" pays tribute to the film in a dream sequence depicting a childhood memory of the character played by Truffaut stealing publicity photos from the film. Numerous film directors have cited the film as influential on their own films, including Theo Angelopoulos, Luc Besson, the Coen brothers, Francis Ford Coppola, Brian De Palma, John Frankenheimer, Stephen Frears, Sergio Leone, Michael Mann, Ridley Scott, Martin Scorsese, Bryan Singer and Steven Spielberg. Ingmar Bergman disliked the film and called it \"a total bore. Above all, the performances are worthless. The amount of respect that movie has is absolutely unbelievable!\"\n\nWilliam Friedkin said that the film influenced him and called it \"a veritable quarry for filmmakers, just as Joyce's \"Ulysses\" is a quarry for writers.\" The film has also influenced other art forms. Carlos Fuentes's novel \"The Death of Artemio Cruz\" was partially inspired by the film and the rock band The White Stripes paid unauthorized tribute to the film in the song \"The Union Forever\".\n\nIn 1982, film director Steven Spielberg bought a \"Rosebud\" sled for $60,500; it was one of three balsa sleds used in the closing scenes and the only one that was not burned. After the Spielberg purchase, it was reported that retiree Arthur Bauer claimed to own another \"Rosebud\" sled. In early 1942 when Bauer was 12 he won an RKO publicity contest and selected the hardwood sled as his prize. In 1996, Bauer's estate offered the painted pine sled at auction through Christie's. Bauer's son told CBS News that his mother had once wanted to paint the sled and use it as a plant stand, but Bauer told her to \"just save it and put it in the closet.\" The sled was sold to an anonymous bidder for $233,500.\n\nWelles's Oscar for Best Original Screenplay was believed to be lost until it was rediscovered in 1994. It was withdrawn from a 2007 auction at Sotheby's when bidding failed to reach its estimate of $800,000 to $1.2 million. Owned by the charitable Dax Foundation, it was auctioned for $861,542 in 2011 to an anonymous buyer. Mankiewicz's Oscar was sold at least twice, in 1999 and again in 2012, the latest price being $588,455.\n\nIn 1989, Mankiewicz's personal copy of the \"Citizen Kane\" script was auctioned at Christie's. The leather-bound volume included the final shooting script and a carbon copy of \"American\" that bore handwritten annotations—purportedly made by Hearst's lawyers, who were said to have obtained it in the manner described by Kael in \"Raising Kane\". Estimated to bring $70,000 to $90,000, it sold for a record $231,000.\n\nIn 2007, Welles's personal copy of the last revised draft of \"Citizen Kane\" before the shooting script was sold at Sotheby's for $97,000. A second draft of the script titled \"American\", marked \"Mr. Welles' working copy\", was auctioned by Sotheby's in 2014 for $164,692. A collection of 24 pages from a working script found in Welles's personal possessions by his daughter Beatrice Welles was auctioned in 2014 for $15,000.\n\nIn 2014, a collection of approximately 235 \"Citizen Kane\" stills and production photos that had belonged to Welles was sold at auction for $7,812.\n\nThe composited camera negative of \"Citizen Kane\" was destroyed in a New Jersey film laboratory fire in the 1970s. Subsequent prints were derived from a master positive (a fine-grain preservation element) made in the 1940s and originally intended for use in overseas distribution. Modern techniques were used to produce a pristine print for a 50th Anniversary theatrical reissue in 1991 which Paramount released for then-owner Turner Broadcasting System, which earned $1.6 million in North America.\n\nIn 1955, RKO sold the American television rights to its film library, including \"Citizen Kane\", to C&C Television Corp. In 1960, television rights to the pre-1956 RKO library were acquired by United Artists. RKO kept the non-broadcast television rights to its library.\n\nIn 1976, when home video was in its infancy, entrepreneur Snuff Garrett bought cassette rights to the RKO library for what United Press International termed \"a pittance.\" In 1978 The Nostalgia Merchant released the film through Media Home Entertainment. By 1980 the 800-title library of The Nostalgia Merchant was earning $2.3 million a year. \"Nobody wanted cassettes four years ago,\" Garrett told UPI. \"It wasn't the first time people called me crazy. It was a hobby with me which became big business.\" RKO Home Video released the film on VHS and Betamax in 1985.\n\nIn 1984, The Criterion Collection released the film as its first LaserDisc. It was made from a fine grain master positive provided by the UCLA Film and Television Archive. When told about the then-new concept of having an audio commentary on the disc, Welles was skeptical but said \"theoretically, that's good for teaching movies, so long as they don't talk nonsense.\" In 1992 Criterion released a new 50th Anniversary Edition LaserDisc. This version had an improved transfer and additional special features, including the documentary \"The Legacy of Citizen Kane\" and Welles's early short \"The Hearts of Age\".\n\nTurner Broadcasting System acquired broadcast television rights to the RKO library in 1986 and the full worldwide rights to the library in 1987. The RKO Home Video unit was reorganized into Turner Home Entertainment that year. In 1991 Turner released a 50th Anniversary Edition on VHS and as a collector's edition that includes the film, the documentary \"Reflections On Citizen Kane\", Harlan Lebo's 50th anniversary album, a poster and a copy of the original script. In 1996, Time Warner acquired Turner and Warner Home Video absorbed Turner Home Entertainment. Today, Time Warner's Warner Bros. unit has distribution rights for the film.\n\nIn 2001, Warner Home Video released a 60th Anniversary Collectors Edition DVD. The two-disc DVD included feature-length commentaries by Roger Ebert and Peter Bogdanovich, as well as \"The Battle Over Citizen Kane\". It was simultaneously released on VHS. The DVD was criticized for being \"\"too\" bright, \"too clean\"; the dirt and grime had been cleared away, but so had a good deal of the texture, the depth, and the sense of film grain.\"\n\nIn 2003, Welles's daughter Beatrice Welles sued Turner Entertainment, claiming the Welles estate is the legal copyright holder of the film. She claimed that Welles's deal to terminate his contracts with RKO meant that Turner's copyright of the film was null and void. She also claimed that the estate of Orson Welles was owed 20% of the film's profits if her copyright claim was not upheld. In 2007 she was allowed to proceed with the lawsuit, overturning the 2004 decision in favor of Turner Entertainment on the issue of video rights.\n\nIn 2011, it was released on Blu-ray Disc and DVD in a 70th anniversary box set. The \"San Francisco Chronicle\" called it \"the Blu-ray release of the year.\" Supplements included everything available on the 2001 Warner Home Video release, as well as \"RKO 281\" and packaging extras that include a hardcover booklet and a folio containing a reproduction of the original souvenir program, miniature lobby cards and other memorabilia. The Blu-ray DVD was scanned as 4K resolution from three different 35mm prints and rectified the quality issues of the 2001 DVD.\n\nIn the 1980s, \"Citizen Kane\" became a catalyst in the controversy over the colorization of black-and-white films. One proponent of film colorization was Ted Turner, whose Turner Entertainment Company owned the RKO library. A Turner Entertainment spokesperson initially stated that \"Citizen Kane\" would not be colorized, but in July 1988 Turner said, \"\"Citizen Kane?\" I'm thinking of colorizing it.\" In early 1989 it was reported that two companies were producing color tests for Turner Entertainment. Criticism increased when filmmaker Henry Jaglom stated that shortly before his death Welles had implored him \"don't let Ted Turner deface my movie with his crayons.\"\n\nIn February 1989, Turner Entertainment president Roger Mayer announced that work to colorize the film had been stopped due to provisions in Welles's 1939 contract with RKO that \"could be read to prohibit colorization without permission of the Welles estate.\" Mayer added that Welles's contract was \"quite unusual\" and \"other contracts we have checked out are not like this at all.\" Turner had only colorized the final reel of the film before abandoning the project. In 1991 one minute of the colorized test footage was included in the BBC \"Arena\" documentary \"The Complete Citizen Kane\".\n\nThe colorization controversy was a factor in the passage of the National Film Preservation Act in 1988 which created the National Film Registry the following year. ABC News anchor Peter Jennings reported that \"one major reason for doing this is to require people like the broadcaster Ted Turner, who's been adding color to some movies and re-editing others for television, to put notices on those versions saying that the movies have been altered\".\n\n\nDatabase\n\nOther\n"}
{"id": "5225", "url": "https://en.wikipedia.org/wiki?curid=5225", "title": "Code", "text": "Code\n\nIn communications and information processing, code is a system of rules to convert information—such as a letter, word, sound, image, or gesture—into another form or representation, sometimes shortened or secret, for communication through a channel or storage in a medium. An early example is the invention of language which enabled a person, through speech, to communicate what he or she saw, heard, felt, or thought to others. But speech limits the range of communication to the distance a voice can carry, and limits the audience to those present when the speech is uttered. The invention of writing, which converted spoken language into visual symbols, extended the range of communication across space and time.\n\nThe process of encoding converts information from a source into symbols for communication or storage. Decoding is the reverse process, converting code symbols back into a form that the recipient understands.\n\nOne reason for coding is to enable communication in places where ordinary plain language, spoken or written, is difficult or impossible. For example, semaphore, where the configuration of flags held by a signaller or the arms of a semaphore tower encodes parts of the message, typically individual letters and numbers. Another person standing a great distance away can interpret the flags and reproduce the words sent.\n\nIn information theory and computer science, a code is usually considered as an algorithm which uniquely represents symbols from some source alphabet, by \"encoded\" strings, which may be in some other target alphabet. An extension of the code for representing sequences of symbols over the source alphabet is obtained by concatenating the encoded strings.\n\nBefore giving a mathematically precise definition, this is a brief example. The mapping \nis a code, whose source alphabet is the set formula_2 and whose target alphabet is the set formula_3. Using the extension of the code, the encoded string 0011001011 can be grouped into codewords as 0 011 0 01 011, and these in turn can be decoded to the sequence of source symbols \"acabc\".\n\nUsing terms from formal language theory, the precise mathematical definition of this concept is as follows: let S and T be two finite sets, called the source and target alphabets, respectively. A code formula_4 is a total function mapping each symbol from S to a sequence of symbols over T, and the extension of formula_5 to a homomorphism of formula_6 into formula_7, which naturally maps each sequence of source symbols to a sequence of target symbols, is referred to as its extension.\n\nIn this section, we consider codes which encode each source (clear text) character by a code word from some dictionary, and concatenation of such code words give us an encoded string.\nVariable-length codes are especially useful when clear text characters have different probabilities; see also entropy encoding.\n\nA \"prefix code\" is a code with the \"prefix property\": there is no valid code word in the system that is a prefix (start) of any other valid code word in the set. Huffman coding is the most known algorithm for deriving prefix codes. Prefix codes are widely referred to as \"Huffman codes\" even when the code was not produced by a Huffman algorithm.\nOther examples of prefix codes are country calling codes, the country and publisher parts of ISBNs, and the Secondary Synchronization Codes used in the UMTS W-CDMA 3G Wireless Standard.\n\nKraft's inequality characterizes the sets of codeword lengths that are possible in a prefix code. Virtually any uniquely decodable one-to-many code, not necessary a prefix one, must satisfy Kraft's inequality.\n\nCodes may also be used to represent data in a way more resistant\nto errors in transmission or storage. Such a \"code\" is\ncalled an error-correcting code, and works by including carefully crafted redundancy with the stored (or transmitted) data. Examples include Hamming codes, Reed–Solomon, Reed–Muller, Walsh–Hadamard, Bose–Chaudhuri–Hochquenghem, Turbo, Golay, Goppa, low-density parity-check codes, and space–time codes.\nError detecting codes can be optimised to detect \"burst errors\", or \"random errors\".\n\nA cable code replaces words (e.g., \"ship\" or \"invoice\") with shorter words, allowing the same information to be sent with fewer characters, more quickly, and most importantly, less expensively.\n\nCodes can be used for brevity. When telegraph messages were the state of the art in rapid long distance communication, elaborate systems of commercial codes that encoded complete phrases into single mouths (commonly five-minute groups) were developed, so that telegraphers became conversant with such \"words\" as \"BYOXO\" (\"Are you trying to weasel out of our deal?\"), \"LIOUY\" (\"Why do you not answer my question?\"), \"BMULD\" (\"You're a skunk!\"), or \"AYYLU\" (\"Not clearly coded, repeat more clearly.\"). Code words were chosen for various reasons: length, pronounceability, etc. Meanings were chosen to fit perceived needs: commercial negotiations, military terms for military codes, diplomatic terms for diplomatic codes, any and all of the preceding for espionage codes. Codebooks and codebook publishers proliferated, including one run as a front for the American Black Chamber run by Herbert Yardley between the First and Second World Wars. The purpose of most of these codes was to save on cable costs. The use of data coding for data compression predates the computer era; an early example is the telegraph Morse code where more-frequently used characters have shorter representations. Techniques such as Huffman coding are now used by computer-based algorithms to compress large data files into a more compact form for storage or transmission.\n\nProbably the most widely known data communications code so far (a.k.a. character representation) in use today is ASCII. In one or another (somewhat compatible) version, it is used by nearly all personal computers, terminals, printers, and other communication equipment. It represents 128 characters with seven-bit binary numbers—that is, as a string of seven 1s and 0s (bits). In ASCII, a lowercase \"a\" is always 1100001, an uppercase \"A\" always 1000001, and so on. There are many other encodings which represent each character by a byte (usually referred as code pages), integer code point (Unicode) or a byte sequence (UTF-8).\n\nBiological organisms contain genetic material that is used to control their function and development. This is DNA which contains units named genes from which messenger RNA is derived. This in turn produces proteins through a code (genetic code) in which a series of triplets (codons) of four possible nucleotides can be translated into one of twenty possible amino acids. A sequence of codons results in a corresponding sequence of amino acids that form a protein molecule; a type of codon called a stop codon signals the end of the sequence.\n\nIn mathematics, a Gödel code was the basis for the proof of Gödel's incompleteness theorem. Here, the idea was to map mathematical notation to a natural number (using a Gödel numbering).\n\nThere are codes using colors, like traffic lights, the color code employed to mark the nominal value of the electrical resistors or that of the trashcans devoted to specific types of garbage (paper, glass, biological, etc.)\n\nIn marketing, coupon codes can be used for a financial discount or rebate when purchasing a product from an internet retailer.\n\nIn military environments, specific sounds with the cornet are used for different uses: to mark some moments of the day, to command the infantry in the battlefield, etc.\n\nCommunication systems for sensory impairments, such as sign language for deaf people and braille for blind people, are based on movement or tactile codes.\n\nMusical scores are the most common way to encode music.\n\nSpecific games, as chess, have their own code systems to record the matches called chess notation.\n\nIn the history of cryptography, codes were once common for ensuring the confidentiality of communications, although ciphers are now used instead. See code (cryptography).\n\nSecret codes intended to obscure the real messages, ranging from serious (mainly espionage in military, diplomatic, business, etc.) to trivial (romance, games) can be any kind of imaginative encoding: flowers, game cards, clothes, fans, hats, melodies, birds, etc., in which the sole requisite is the previous agreement of the meaning by both the sender and the receiver.\n\nOther examples of encoding include:\n\nOther examples of decoding include:\n\nAcronyms and abbreviations can be considered codes, and in a sense all languages and writing systems are codes for human thought.\n\nInternational Air Transport Association airport codes are three-letter codes used to designate airports and used for bag tags. Station codes are similarly used on railways, but are usually national, so the same code can be used for different stations if they are in different countries.\n\nOccasionally a code word achieves an independent existence (and meaning) while the original equivalent phrase is forgotten or at least no longer has the precise meaning attributed to the code word. For example, '30' was widely used in journalism to mean \"end of story\", and has been used in other contexts to signify \"the end\".\n\n"}
{"id": "5228", "url": "https://en.wikipedia.org/wiki?curid=5228", "title": "Cheirogaleidae", "text": "Cheirogaleidae\n\nThe Cheirogaleidae are the family of strepsirrhine primates containing the various dwarf and mouse lemurs. Like all other lemurs, cheirogaleids live exclusively on the island of Madagascar.\n\nCheirogaleids are smaller than the other lemurs and, in fact, they are the smallest primates. They have soft, long fur, colored grey-brown to reddish on top, with a generally brighter underbelly. Typically, they have small ears, large, close-set eyes, and long hind legs. Like all strepsirrhines, they have fine claws at the second toe of the hind legs. They grow to a size of only 13 to 28 cm, with a tail that is very long, sometimes up to one and a half times as long as the body. They weigh no more than 500 grams, with some species weighing as little as 60 grams.\n\nDwarf and mouse lemurs are nocturnal and arboreal. They are excellent climbers and can also jump far, using their long tails for balance. When on the ground (a rare occurrence), they move by hopping on their hind legs. They spend the day in tree hollows or leaf nests. Cheirogaleids are typically solitary, but sometimes live together in pairs.\n\nTheir eyes possess a tapetum lucidum, a light-reflecting layer that improves their night vision. Some species, such as the lesser dwarf lemur, store fat at the hind legs and the base of the tail, and hibernate. Unlike lemurids, they have long upper incisors, although they do have the comb-like teeth typical of all strepsirhines. They have the dental formula: \n\nCheirogaleids are omnivores, eating fruits, flowers and leaves (and sometimes nectar), as well as insects, spiders, and small vertebrates.\n\nThe females usually have three pairs of nipples. After a meager 60-day gestation, they will bear two to four (usually two or three) young. After five to six weeks, the young are weaned and become fully mature near the end of their first year or sometime in their second year, depending on the species. In human care, they can live for up to 15 years, although their life expectancy in the wild is probably significantly shorter.\n\nThe five genera of cheirogaleids contain 34 species.\n\n\n"}
{"id": "5229", "url": "https://en.wikipedia.org/wiki?curid=5229", "title": "Callitrichidae", "text": "Callitrichidae\n\nThe Callitrichidae (also called Arctopitheci or Hapalidae) are a family of New World monkeys, including marmosets, tamarins and lion tamarins. At times, this group of animals has been regarded as a subfamily, called Callitrichinae, of the family Cebidae.\n\nThis taxon was traditionally thought to be a primitive lineage, from which all the larger-bodied platyrrhines evolved. However, some works argue that callitrichids are actually a dwarfed lineage.\n\nAncestral stem-callitrichids likely were \"normal-sized\" ceboids that were dwarfed through evolutionary time. This may exemplify a rare example of insular dwarfing in a mainland context, with the \"islands\" being formed by biogeographic barriers during arid climatic periods when forest distribution became patchy, and/or by the extensive river networks in the Amazon Basin.\n\nAll callitrichids are arboreal. They are the smallest of the simian primates. They eat insects, fruit, and the sap or gum from trees; occasionally they take small vertebrates. The marmosets rely quite heavily on tree exudates, with some species (e.g. \"Callithrix jacchus\" and \"Cebuella pygmaea\") considered obligate exudativores.\n\nCallitrichids typically live in small, territorial groups of about five or six animals. Their social organization is unique among primates and is called a \"cooperative polyandrous group\". This communal breeding system involves groups of multiple males and females, but only one female is reproductively active. Females mate with more than one male and each shares the responsibility of carrying the offspring.\n\nThey are the only primate group that regularly produces twins, which constitute over 80% of births in species that have been studied. Unlike other male primates, male callitrichids generally provide as much parental care as females. Parental duties may include carrying, protecting, feeding, comforting, and even engaging in play behavior with offspring. In some cases, such as in the cotton-top tamarin (\"Saguinus oedipus\"), males, particularly those that are paternal, will even show a greater involvement in caregiving than females. The typical social structure seems to constitute a breeding group, with several of their previous offspring living in the group and providing significant help in rearing the young.\n\n\n\n"}
{"id": "5230", "url": "https://en.wikipedia.org/wiki?curid=5230", "title": "Cebidae", "text": "Cebidae\n\nThe Cebidae are one of the five families of New World monkeys now recognised. Extant members are the capuchin monkeys and squirrel monkeys. These species are found throughout tropical and subtropical South and Central America.\n\nCebid monkeys are arboreal animals that only rarely travel on the ground. They are generally small monkeys, ranging in size up to that of the brown capuchin, with a body length of 33 to 56 cm, and a weight of 2.5 to 3.9 kilograms. They are somewhat variable in form and coloration, but all have the wide, flat, noses typical of New World monkeys. They are different from marmosets as they have additional molar tooth and a prehensile tail.\n\nThey are omnivorous, mostly eating fruit and insects, although the proportions of these foods vary greatly between species. They have the dental formula:\n\nFemales give birth to one or two young after a gestation period of between 130 and 170 days, depending on species. They are social animals, living in groups of between five and forty individuals, with the smaller species typically forming larger groups. They are generally diurnal in habit.\n\nPreviously, New World monkeys were divided between Callitrichidae and this family. For a few recent years, marmosets, tamarins, and lion tamarins were placed as a subfamily (Callitrichinae) in Cebidae, while moving other genera from Cebidae into the families Aotidae, Pitheciidae and Atelidae. The most recent classification of New World monkeys again splits the callitrichids off, leaving only the capuchins and squirrel monkeys in this family.\n\n\n"}
{"id": "5232", "url": "https://en.wikipedia.org/wiki?curid=5232", "title": "Chondrichthyes", "text": "Chondrichthyes\n\nChondrichthyes (; from Greek χονδρ- \"chondr-\" 'cartilage', ἰχθύς \"ichthys\" 'fish') is a class that contains the cartilaginous fishes: they are jawed vertebrates with paired fins, paired nares, scales, a heart with its chambers in series, and skeletons made of cartilage rather than bone. The class is divided into two subclasses: Elasmobranchii (sharks, rays, skates, and sawfish) and Holocephali (chimaeras, sometimes called ghost sharks, which are sometimes separated into their own class).\n\nWithin the infraphylum Gnathostomata, cartilaginous fishes are distinct from all other jawed vertebrates.\n\nThe skeleton is cartilaginous. The notochord, which is present in the young, is gradually replaced by cartilage. Chondrichthyans also lack ribs, so if they leave water, the larger species' own body weight would crush their internal organs long before they would suffocate. \n\nAs they do not have bone marrow, red blood cells are produced in the spleen and the epigonal organ (special tissue around the gonads, which is also thought to play a role in the immune system). They are also produced in the Leydig's organ, which is only found in certain cartilaginous fishes. The subclass Holocephali, which is a very specialized group, lacks both the Leydig's and epigonal organs.\n\nApart from electric rays, which have a thick and flabby body, with soft, loose skin, chondrichthyans have tough skin covered with dermal teeth (again, Holocephali is an exception, as the teeth are lost in adults, only kept on the clasping organ seen on the caudal ventral surface of the male), also called placoid scales (or \"dermal denticles\"), making it feel like sandpaper. In most species, all dermal denticles are oriented in one direction, making the skin feel very smooth if rubbed in one direction and very rough if rubbed in the other.\n\nOriginally, the pectoral and pelvic girdles, which do not contain any dermal elements, did not connect. In later forms, each pair of fins became ventrally connected in the middle when scapulocoracoid and pubioischiadic bars evolved. In rays, the pectoral fins have connected to the head and are very flexible.\n\nOne of the primary characteristics present in most sharks is the heterocercal tail, which aids in locomotion.\n\nChondrichthyans have toothlike scales called dermal denticles or placoid scales. Denticles provide two functions, protection and in most cases, streamlining. Mucous glands exist in some species as well.\n\nIt is assumed that their oral teeth evolved from dermal denticles that migrated into the mouth, but it could be the other way around as the teleost bony fish \"Denticeps clupeoides\" has most of its head covered by dermal teeth (as does, probably, \"Atherion elymus\", another bony fish). This is most likely a secondary evolved characteristic, which means there is not necessarily a connection between the teeth and the original dermal scales.\n\nThe old placoderms did not have teeth at all, but had sharp bony plates in their mouth. Thus, it is unknown whether the dermal or oral teeth evolved first. Nor is it sure how many times it has happened if it turns out to be the case. It has even been suggested that the original bony plates of all the vertebrates are gone and that the present scales are just modified teeth, even if both teeth and the body armor have a common origin a long time ago. However, there is no evidence of this at the moment.\n\nAll chondrichthyans breathe through five to seven pairs of gills, depending on the species. In general, pelagic species must keep swimming to keep oxygenated water moving through their gills, whilst demersal species can actively pump water in through their spiracles and out through their gills. However, this is only a general rule and many species differ.\n\nA spiracle is a small hole found behind each eye. These can be tiny and circular, such as found on the nurse shark (\"Ginglymostoma cirratum\"), to extended and slit-like, such as found on the wobbegongs (Orectolobidae). Many larger, pelagic species, such as the mackerel sharks (Lamnidae) and the thresher sharks (Alopiidae), no longer possess them.\n\nLike all other jawed vertebrates, members of Chondrichthyes have an adaptive immune system.\n\nFertilization is internal. Development is usually live birth (ovoviviparous species) but can be through eggs (oviparous). Some rare species are viviparous. There is no parental care after birth; however, some chondrichthyans do guard their eggs.\n\nThe class Chondrichthyes has two subclasses: the subclass Elasmobranchii (sharks, rays, skates, and sawfish) and the subclass Holocephali (chimaeras). To see the full list of the species, click here.\n\nCartilaginous fish are considered to have evolved from acanthodians. Originally assumed to be closely related to bony fish or a polyphyletic assemblage leading to both groups, the discovery of \"Entelognathus\" and several examinations of acanthodian characteristics indicate that bony fish evolved directly from placoderm like ancestors, while acanthodians represent a paraphyletic assemblage leading to Chondrichthyes. Some characteristics previously thought to be exclusive to acanthodians are also present in basal cartilaginous fish. In particular, new phylogenetic studies find cartilaginous fish to be well nested among acanthodians, with \"Doliodus\" and \"Tamiobatis\" being the closest relatives to Chondrichthyes. Recent studies vindicate this, as \"Doliodus\" had a mosaic of chondrichthyian and acanthodiian traits.\n\nUnequivocal fossils of cartilaginous fishes first appeared in the fossil record by about 395 million years ago, during the middle Devonian. The radiation of elasmobranches in the chart on the right is divided into the taxa: Cladoselache, Eugeneodontiformes, Symmoriida, Xenacanthiformes, Ctenacanthiformes, Hybodontiformes, Galeomorphi, Squaliformes and Batoidea.\n\nBy the start of the Early Devonian, 419 mya (million years ago), jawed fishes had divided into three distinct groups: the now extinct placoderms (a paraphyletic assemblage of ancient armoured fishes), the bony fishes and the clade including spiny sharks and early cartilaginous fish. The modern bony fishes, class Osteichthyes, appeared in the late Silurian or early Devonian, about 416 million years ago. Cartilaginous fishes first appeared about 395 Ma, having evolved from \"Doliodus\"-like spiny shark ancestors. The first abundant genus of shark, \"Cladoselache\", appeared in the oceans during the Devonian Period.\n\nA Bayesian analysis of molecular data suggests that the Holocephali and Elasmoblanchii diverged in the Silurian () and that the sharks and rays/skates split in the Carboniferous ().\n\n Subphylum Vertebrata\n\n\n"}
{"id": "5233", "url": "https://en.wikipedia.org/wiki?curid=5233", "title": "Carl Linnaeus", "text": "Carl Linnaeus\n\nCarl Linnaeus (; 23 May 1707 – 10 January 1778), also known after his ennoblement as Carl von Linné (), was a Swedish botanist, physician, and zoologist, who formalised the modern system of naming organisms called binomial nomenclature. He is known by the epithet \"father of modern taxonomy\". Many of his writings were in Latin, and his name is rendered in Latin as (after 1761 Carolus a Linné).\n\nLinnaeus was born in the countryside of Småland, in southern Sweden. He received most of his higher education at Uppsala University, and began giving lectures in botany there in 1730. He lived abroad between 1735 and 1738, where he studied and also published a first edition of his \" in the Netherlands. He then returned to Sweden, where he became professor of medicine and botany at Uppsala. In the 1740s, he was sent on several journeys through Sweden to find and classify plants and animals. In the 1750s and 1760s, he continued to collect and classify animals, plants, and minerals, and published several volumes. At the time of his death, he was one of the most acclaimed scientists in Europe.\n\nThe philosopher Jean-Jacques Rousseau sent him the message: \"Tell him I know no greater man on earth.\" The German writer Johann Wolfgang von Goethe wrote: \"With the exception of Shakespeare and Spinoza, I know no one among the no longer living who has influenced me more strongly.\" Swedish author August Strindberg wrote: \"Linnaeus was in reality a poet who happened to become a naturalist\". Among other compliments, Linnaeus has been called \" (Prince of Botanists), \"The Pliny of the North,\" and \"The Second Adam\". He is also considered as one of the founders of modern ecology.\n\nIn botany, the author abbreviation used to indicate Linnaeus as the authority for species' names is L. In older publications, sometimes the abbreviation \"Linn.\" is found (for instance in: ). Linnaeus' remains comprise the type specimen for the species \"Homo sapiens\", following the International Code of Zoological Nomenclature, since the sole specimen he is known to have examined when writing the species description was himself.\n\nLinnæus was born in the village of Råshult in Småland, Sweden, on 23 May 1707. He was the first child of Nicolaus (Nils) Ingemarsson (who later adopted the family name Linnæus) and Christina Brodersonia. His siblings were Anna Maria Linnæa, Sofia Juliana Linnæa, Samuel Linnæus (who would eventually succeed their father as rector of Stenbrohult and write a manual on beekeeping), and Emerentia Linnæa.\n\nOne of a long line of peasants and priests, Nils was an amateur botanist, a Lutheran minister, and the curate of the small village of Stenbrohult in Småland. Christina was the daughter of the rector of Stenbrohult, Samuel Brodersonius.\n\nA year after Linnæus' birth, his grandfather Samuel Brodersonius died, and his father Nils became the rector of Stenbrohult. The family moved into the rectory from the curate's house.\n\nEven in his early years, Linnæus seemed to have a liking for plants, flowers in particular. Whenever he was upset, he was given a flower, which immediately calmed him. Nils spent much time in his garden and often showed flowers to Linnaeus and told him their names. Soon Linnæus was given his own patch of earth where he could grow plants.\n\nCarl's father was the first in his ancestry to adopt a permanent surname. Before that, ancestors had used the patronymic naming system of Scandinavian countries: his father was named Ingemarsson after his father Ingemar Bengtsson. When Nils was admitted to the University of Lund, he had to take on a family name. He adopted the Latinate name Linnæus after a giant linden tree (or lime tree), \" in Swedish, that grew on the family homestead. This name was spelled with the æ ligature. When Carl was born, he was named Carl Linnæus, with his father's family name. The son also always spelled it with the æ ligature, both in handwritten documents and in publications. Carl's patronymic would have been Nilsson, as in Carl Nilsson Linnæus.\n\nLinnaeus' father began teaching him basic Latin, religion, and geography at an early age. When Linnaeus was seven, Nils decided to hire a tutor for him. The parents picked Johan Telander, a son of a local yeoman. Linnaeus did not like him, writing in his autobiography that Telander \"was better calculated to extinguish a child's talents than develop them.\"\n\nTwo years after his tutoring had begun, he was sent to the Lower Grammar School at Växjö in 1717. Linnaeus rarely studied, often going to the countryside to look for plants. He reached the last year of the Lower School when he was fifteen, which was taught by the headmaster, Daniel Lannerus, who was interested in botany. Lannerus noticed Linnaeus' interest in botany and gave him the run of his garden.\n\nHe also introduced him to Johan Rothman, the state doctor of Småland and a teacher at Katedralskolan (a gymnasium) in Växjö. Also a botanist, Rothman broadened Linnaeus' interest in botany and helped him develop an interest in medicine. By the age of 17, Linnaeus had become well acquainted with the existing botanical literature. He remarks in his journal that he \"read day and night, knowing like the back of my hand, Arvidh Månsson's Rydaholm Book of Herbs, Tillandz's Flora Åboensis, Palmberg's Serta Florea Suecana, Bromelii Chloros Gothica and Rudbeckii Hortus Upsaliensis...\"\n\nLinnaeus entered the Växjö Katedralskola in 1724, where he studied mainly Greek, Hebrew, theology and mathematics, a curriculum designed for boys preparing for the priesthood. In the last year at the gymnasium, Linnaeus' father visited to ask the professors how his son's studies were progressing; to his dismay, most said that the boy would never become a scholar. Rothman believed otherwise, suggesting Linnaeus could have a future in medicine. The doctor offered to have Linnaeus live with his family in Växjö and to teach him physiology and botany. Nils accepted this offer.\n\nRothman showed Linnaeus that botany was a serious subject. He taught Linnaeus to classify plants according to Tournefort's system. Linnaeus was also taught about the sexual reproduction of plants, according to Sébastien Vaillant. In 1727, Linnaeus, age 21, enrolled in Lund University in Skåne. He was registered as \", the Latin form of his full name, which he also used later for his Latin publications.\n\nProfessor Kilian Stobæus, natural scientist, physician and historian, offered Linnaeus tutoring and lodging, as well as the use of his library, which included many books about botany. He also gave the student free admission to his lectures. In his spare time, Linnaeus explored the flora of Skåne, together with students sharing the same interests.\n\nIn August 1728, Linnaeus decided to attend Uppsala University on the advice of Rothman, who believed it would be a better choice if Linnaeus wanted to study both medicine and botany. Rothman based this recommendation on the two professors who taught at the medical faculty at Uppsala: Olof Rudbeck the Younger and Lars Roberg. Although Rudbeck and Roberg had undoubtedly been good professors, by then they were older and not so interested in teaching. Rudbeck no longer gave public lectures, and had others stand in for him. The botany, zoology, pharmacology and anatomy lectures were not in their best state. In Uppsala, Linnaeus met a new benefactor, Olof Celsius, who was a professor of theology and an amateur botanist. He received Linnaeus into his home and allowed him use of his library, which was one of the richest botanical libraries in Sweden.\n\nIn 1729, Linnaeus wrote a thesis, ' on plant sexual reproduction. This attracted the attention of Rudbeck; in May 1730, he selected Linnaeus to give lectures at the University although the young man was only a second-year student. His lectures were popular, and Linnaeus often addressed an audience of 300 people. In June, Linnaeus moved from Celsius' house to Rudbeck's to become the tutor of the three youngest of his 24 children. His friendship with Celsius did not wane and they continued their botanical expeditions. Over that winter, Linnaeus began to doubt Tournefort's system of classification and decided to create one of his own. His plan was to divide the plants by the number of stamens and pistils. He began writing several books, which would later result in, for example, ' and '. He also produced a book on the plants grown in the Uppsala Botanical Garden, '.\n\nRudbeck's former assistant, Nils Rosén, returned to the University in March 1731 with a degree in medicine. Rosén started giving anatomy lectures and tried to take over Linnaeus' botany lectures, but Rudbeck prevented that. Until December, Rosén gave Linnaeus private tutoring in medicine. In December, Linnaeus had a \"disagreement\" with Rudbeck's wife and had to move out of his mentor's house; his relationship with Rudbeck did not appear to suffer. That Christmas, Linnaeus returned home to Stenbrohult to visit his parents for the first time in about three years. His mother had disapproved of his failing to become a priest, but she was pleased to learn he was teaching at the University.\n\nDuring a visit with his parents, Linnaeus told them about his plan to travel to Lapland; Rudbeck had made the journey in 1695, but the detailed results of his exploration were lost in a fire seven years afterwards. Linnaeus' hope was to find new plants, animals and possibly valuable minerals. He was also curious about the customs of the native Sami people, reindeer-herding nomads who wandered Scandinavia's vast tundras. In April 1732, Linnaeus was awarded a grant from the Royal Society of Sciences in Uppsala for his journey.\n\nLinnaeus began his expedition from Uppsala on May 12, 1732, just before he turned 25. He travelled on foot and horse, bringing with him his journal, botanical and ornithological manuscripts and sheets of paper for pressing plants. Near Gävle he found great quantities of \"Campanula serpyllifolia\", later known as \"Linnaea borealis\", the twinflower that would become his favourite. He sometimes dismounted on the way to examine a flower or rock and was particularly interested in mosses and lichens, the latter a main part of the diet of the reindeer, a common and economically important animal in Lapland.\n\nLinnaeus travelled clockwise around the coast of the Gulf of Bothnia, making major inland incursions from Umeå, Luleå and Tornio. He returned from his six-month-long, over expedition in October, having gathered and observed many plants, birds and rocks. Although Lapland was a region with limited biodiversity, Linnaeus described about 100 previously unidentified plants. These became the basis of his book \". However, on the expedition to Lapland, Linnaeus used Latin names to describe organisms because he had not yet developed the binomial system.\n\nIn ' Linnaeus' ideas about nomenclature and classification were first used in a practical way, making this the first proto-modern Flora. The account covered 534 species, used the Linnaean classification system and included, for the described species, geographical distribution and taxonomic notes. It was Augustin Pyramus de Candolle who attributed Linnaeus with ' as the first example in the botanical genre of Flora writing. Botanical historian E. L. Greene described \" as \"the most classic and delightful\" of Linnaeus's works.\n\nIt was also during this expedition that Linnaeus had a flash of insight regarding the classification of mammals. Upon observing the lower jawbone of a horse at the side of a road he was travelling, Linnaeus remarked: \"If I only knew how many teeth and of what kind every animal had, how many teats and where they were placed, I should perhaps be able to work out a perfectly natural system for the arrangement of all quadrupeds.\"\n\nIn 1734, Linnaeus led a small group of students to Dalarna. Funded by the Governor of Dalarna, the expedition was to catalogue known natural resources and discover new ones, but also to gather intelligence on Norwegian mining activities at Røros.\n\nBack in Uppsala, Linnaeus' relations with Nils Rosén worsened, and thus he gladly accepted an invitation from the student Claes Sohlberg to spend the Christmas holiday in Falun with Sohlberg's family. Sohlberg's father was a mining inspector, and let Linnaeus visit the mines near Falun. Sohlberg's father suggested to Linnaeus he should bring Sohlberg to the Dutch Republic and continue to tutor him there for an annual salary. At that time, the Dutch Republic was one of the most revered places to study natural history and a common place for Swedes to take their doctoral degree; Linnaeus, who was interested in both of these, accepted.\n\nIn April 1735, Linnaeus and Sohlberg set out for the Netherlands, with Linnaeus to take a doctoral degree in medicine at the University of Harderwijk. On the way, they stopped in Hamburg, where they met the mayor, who proudly showed them a wonder of nature which he possessed: the taxidermied remains of a seven-headed hydra. Linnaeus quickly discovered it was a fake: jaws and clawed feet from weasels and skins from snakes had been glued together. The provenance of the hydra suggested to Linnaeus it had been manufactured by monks to represent the Beast of Revelation. As much as this may have upset the mayor, Linnaeus made his observations public and the mayor's dreams of selling the hydra for an enormous sum were ruined. Fearing his wrath, Linnaeus and Sohlberg had to leave Hamburg quickly.\nWhen Linnaeus reached Harderwijk, he began working towards a degree immediately; at the time, Harderwijk was known for awarding \"instant\" degrees after as little as a week. First he handed in a thesis on the cause of malaria he had written in Sweden, which he then defended in a public debate. His dissertation, submitted on 23 June, was titled \"Dissertatio medica inauguralis in qua exhibetur hypothesis nova de febrium intermittentium causa\" (\"\"Inaugural thesis in medicine, in which a new hypothesis on the cause of intermittent fevers is presented\"\"). He concluded that malaria arose only in places with clay-rich soil. He is now known to have been wrong about the cause, not having a microscope good enough to see malarial parasites, which were spread by mosquitoes breeding in the water that collected in ruts and puddles. But he was right in predicting that traditional Chinese medicine, including the use of wormwood (\"Artemisia\"), is a potential source of antimalarial drugs; Artemisinins, derived from wormwood, are now the principal antimalarial drugs.\n\nThe next step was to take an oral examination and to diagnose a patient. After less than two weeks, he took his degree and became a doctor, at the age of 28. During the summer, Linnaeus met a friend from Uppsala, Peter Artedi. Before their departure from Uppsala, Artedi and Linnaeus had decided should one of them die, the survivor would finish the other's work. Ten weeks later, Artedi drowned in one of the canals of Amsterdam, and his unfinished manuscript on the classification of fish was left to Linnaeus to complete.\n\nOne of the first scientists Linnaeus met in the Netherlands was Johan Frederik Gronovius to whom Linnaeus showed one of the several manuscripts he had brought with him from Sweden. The manuscript described a new system for classifying plants. When Gronovius saw it, he was very impressed, and offered to help pay for the printing. With an additional monetary contribution by the Scottish doctor Isaac Lawson, the manuscript was published as \" (1735).\n\nLinnaeus became acquainted with one of the most respected physicians and botanists in the Netherlands, Herman Boerhaave, who tried to convince Linnaeus to make a career there. Boerhaave offered him a journey to South Africa and America, but Linnaeus declined, stating he would not stand the heat. Instead, Boerhaave convinced Linnaeus that he should visit the botanist Johannes Burman. After his visit, Burman, impressed with his guest's knowledge, decided Linnaeus should stay with him during the winter. During his stay, Linnaeus helped Burman with his '. Burman also helped Linnaeus with the books on which he was working: ' and \"\".\n\nIn August 1735, during Linnaeus' stay with Burman, he met George Clifford III, a director of the Dutch East India Company and the owner of a rich botanical garden at the estate of Hartekamp in Heemstede. Clifford was very impressed with Linnaeus' ability to classify plants, and invited him to become his physician and superintendent of his garden. Linnaeus had already agreed to stay with Burman over the winter, and could thus not accept immediately. However, Clifford offered to compensate Burman by offering him a copy of Sir Hans Sloane's \"Natural History of Jamaica\", a rare book, if he let Linnaeus stay with him, and Burman accepted. On 24 September 1735, Linnaeus moved to Hartekamp to become personal physician to Clifford, and curator of Clifford's herbarium. He was paid 1,000 florins a year, with free board and lodging. Though the agreement was only for a winter of that year, Linnaeus practically stayed there till 1738. It was here that he wrote a book \"Hortus Cliffortianus\", in the preface of which he described his experience as \"the happiest time of my life.\" (A portion of Hartekamp was declared as public garden in April 1956 by the Heemstede local authority, and was named \"Linnaeushof\". It eventually became, as it is claimed, the biggest playground in Europe.)\n\nIn July 1736, Linnaeus travelled to England, at Clifford's expense. He went to London to visit Sir Hans Sloane, a collector of natural history, and to see his cabinet, as well as to visit the Chelsea Physic Garden and its keeper, Philip Miller. He taught Miller about his new system of subdividing plants, as described in \"\". Miller was in fact reluctant to use the new binomial nomenclature, preferring the classifications of Joseph Pitton de Tournefort and John Ray at first. Linnaeus, nevertheless, applauded Miller's \"Gardeners Dictionary\", The conservative Scot actually retained in his dictionary a number of pre-Linnaean binomial signifiers discarded by Linnaeus but which have been retained by modern botanists. He only fully changed to the Linnaean system in the edition of \"The Gardeners Dictionary\" of 1768. Miller ultimately was impressed, and from then on started to arrange the garden according to Linnaeus' system.\n\nLinnaeus also travelled to Oxford University to visit the botanist Johann Jacob Dillenius. He failed to make Dillenius publicly fully accept his new classification system, though the two men remained in correspondence for many years afterwards. Linnaeus dedicated his \"Critica botanica\" to him, as \"\"opus botanicum quo absolutius mundus non vidit\"\". Linnaeus would later name a genus of tropical tree Dillenia in his honour. He then returned to Hartekamp, bringing with him many specimens of rare plants. The next year, he published ', in which he described 935 genera of plants, and shortly thereafter he supplemented it with ', with another sixty (\"sexaginta\") genera.\n\nHis work at Hartekamp led to another book, \"\", a catalogue of the botanical holdings in the herbarium and botanical garden of Hartekamp. He wrote it in nine months (completed in July 1737), but it was not published until 1738. It contains the first use of the name \"Nepenthes\", which Linnaeus used to describe a genus of pitcher plants.\n\nLinnaeus stayed with Clifford at Hartekamp until 18 October 1737 (new style), when he left the house to return to Sweden. Illness and the kindness of Dutch friends obliged him to stay some months longer in Holland. In May 1738, he set out for Sweden again. On the way home, he stayed in Paris for about a month, visiting botanists such as Antoine de Jussieu. After his return, Linnaeus never left Sweden again.\n\nWhen Linnaeus returned to Sweden on 28 June 1738, he went to Falun, where he entered into an engagement to Sara Elisabeth Moræa. Three months later, he moved to Stockholm to find employment as a physician, and thus to make it possible to support a family. Once again, Linnaeus found a patron; he became acquainted with Count Carl Gustav Tessin, who helped him get work as a physician at the Admiralty. During this time in Stockholm, Linnaeus helped found the Royal Swedish Academy of Science; he became the first Praeses in the academy by drawing of lots.\n\nBecause his finances had improved and were now sufficient to support a family, he received permission to marry his fiancée, Sara Elisabeth Moræa. Their wedding was held 26 June 1739. Seven months later, Sara gave birth to their first son, Carl. Two years later, a daughter, Elisabeth Christina, was born, and the subsequent year Sara gave birth to Sara Magdalena, who died when 15 days old. Sara and Linnaeus would later have four other children: Lovisa, Sara Christina, Johannes and Sophia.\nIn May 1741, Linnaeus was appointed Professor of Medicine at Uppsala University, first with responsibility for medicine-related matters. Soon, he changed place with the other Professor of Medicine, Nils Rosén, and thus was responsible for the Botanical Garden (which he would thoroughly reconstruct and expand), botany and natural history, instead. In October that same year, his wife and nine-year-old son followed him to live in Uppsala.\n\nTen days after he was appointed Professor, he undertook an expedition to the island provinces of Öland and Gotland with six students from the university, to look for plants useful in medicine. First, they travelled to Öland and stayed there until 21 June, when they sailed to Visby in Gotland. Linnaeus and the students stayed on Gotland for about a month, and then returned to Uppsala. During this expedition, they found 100 previously unrecorded plants. The observations from the expedition were later published in ', written in Swedish. Like ', it contained both zoological and botanical observations, as well as observations concerning the culture in Öland and Gotland.\n\nDuring the summer of 1745, Linnaeus published two more books: ' and '. ' was a strictly botanical book, while ' was zoological. Anders Celsius had created the temperature scale named after him in 1742. Celsius' scale was inverted compared to today, the boiling point at 0 °C and freezing point at 100 °C. In 1745, Linnaeus inverted the scale to its present standard.\n\nIn the summer of 1746, Linnaeus was once again commissioned by the Government to carry out an expedition, this time to the Swedish province of Västergötland. He set out from Uppsala on 12 June and returned on 11 August. On the expedition his primary companion was Erik Gustaf Lidbeck, a student who had accompanied him on his previous journey. Linnaeus described his findings from the expedition in the book \", published the next year. After returning from the journey the Government decided Linnaeus should take on another expedition to the southernmost province Scania. This journey was postponed, as Linnaeus felt too busy.\n\nIn 1747, Linnaeus was given the title archiater, or chief physician, by the Swedish king Adolf Frederick—a mark of great respect. The same year he was elected member of the Academy of Sciences in Berlin.\n\nIn the spring of 1749, Linnaeus could finally journey to Scania, again commissioned by the Government. With him he brought his student, Olof Söderberg. On the way to Scania, he made his last visit to his brothers and sisters in Stenbrohult since his father had died the previous year. The expedition was similar to the previous journeys in most aspects, but this time he was also ordered to find the best place to grow walnut and Swedish whitebeam trees; these trees were used by the military to make rifles. The journey was successful, and Linnaeus' observations were published the next year in \".\n\nIn 1750, Linnaeus became rector of Uppsala University, starting a period where natural sciences were esteemed. Perhaps the most important contribution he made during his time at Uppsala was to teach; many of his students travelled to various places in the world to collect botanical samples. Linnaeus called the best of these students his \"apostles\". His lectures were normally very popular and were often held in the Botanical Garden. He tried to teach the students to think for themselves and not trust anybody, not even him. Even more popular than the lectures were the botanical excursions made every Saturday during summer, where Linnaeus and his students explored the flora and fauna in the vicinity of Uppsala.\n\nLinnaeus published \"Philosophia Botanica\" in 1751. The book contained a complete survey of the taxonomy system he had been using in his earlier works. It also contained information of how to keep a journal on travels and how to maintain a botanical garden.\n\nDuring Linnaeus' time it was normal for upper class women to have wet nurses for their babies. Linnaeus joined an ongoing campaign to end this practice in Sweden and promote breast-feeding by mothers. In 1752 Linnaeus published a thesis along with Frederick Lindberg, a physician student, based on their experiences. In the tradition of the period, this dissertation was essentially an idea of the presiding reviewer (\"prases\") expounded upon by the student. Linnaeus' dissertation was translated into French by J.E. Gilibert in 1770 as \"La Nourrice marâtre, ou Dissertation sur les suites funestes du nourrisage mercénaire\". Linnaeus suggested that children might absorb the personality of their wet nurse through the milk. He admired the child care practices of the Lapps and pointed out how healthy their babies were compared to those of Europeans who employed wet nurses. He compared the behaviour of wild animals and pointed out how none of them denied their newborns their breastmilk. It is thought that his activism played a role in his choice of the term \"Mammalia\" for the class of organisms.\n\nLinnaeus published \"Species Plantarum\", the work which is now internationally accepted as the starting point of modern botanical nomenclature, in 1753. The first volume was issued on 24 May, the second volume followed on 16 August of the same year. The book contained 1,200 pages and was published in two volumes; it described over 7,300 species. The same year the king dubbed him knight of the Order of the Polar Star, the first civilian in Sweden to become a knight in this order. He was then seldom seen not wearing the order's insignia.\n\nLinnaeus felt Uppsala was too noisy and unhealthy, so he bought two farms in 1758: Hammarby and Sävja. The next year, he bought a neighbouring farm, Edeby. He spent the summers with his family at Hammarby; initially it only had a small one-storey house, but in 1762 a new, larger main building was added. In Hammarby, Linnaeus made a garden where he could grow plants that could not be grown in the Botanical Garden in Uppsala. He began constructing a museum on a hill behind Hammarby in 1766, where he moved his library and collection of plants. A fire that destroyed about one third of Uppsala and had threatened his residence there necessitated the move.\n\nSince the initial release of ' in 1735, the book had been expanded and reprinted several times; the tenth edition was released in 1758. This edition established itself as the starting point for zoological nomenclature, the equivalent of '.\n\nThe Swedish king Adolf Frederick granted Linnaeus nobility in 1757, but he was not ennobled until 1761. With his ennoblement, he took the name Carl von Linné (Latinised as \"\"), 'Linné' being a shortened and gallicised version of 'Linnæus', and the German nobiliary particle 'von' signifying his ennoblement. The noble family's coat of arms prominently features a twinflower, one of Linnaeus' favourite plants; it was given the scientific name \"Linnaea borealis\" in his honour by Gronovius. The shield in the coat of arms is divided into thirds: red, black and green for the three kingdoms of nature (animal, mineral and vegetable) in Linnaean classification; in the centre is an egg \"to denote Nature, which is continued and perpetuated \"in ovo\".\" At the bottom is a phrase in Latin, borrowed from the Aeneid, which reads \"Famam extendere factis\": we extend our fame by our deeds. Linnaeus inscribed this personal motto in books that were gifted to him by friends.\n\nAfter his ennoblement, Linnaeus continued teaching and writing. His reputation had spread over the world, and he corresponded with many different people. For example, Catherine II of Russia sent him seeds from her country. He also corresponded with Giovanni Antonio Scopoli, \"the Linnaeus of the Austrian Empire\", who was a doctor and a botanist in Idrija, Duchy of Carniola (nowadays Slovenia). Scopoli communicated all of his research, findings, and descriptions (for example of the olm and the dormouse, two little animals hitherto unknown to Linnaeus). Linnaeus greatly respected Scopoli and showed great interest in his work. He named a solanaceous genus, \"Scopolia\", the source of scopolamine, after him, but because of the great distance between them, they never met.\n\nLinnaeus was relieved of his duties in the Royal Swedish Academy of Science in 1763, but continued his work there as usual for more than ten years after. He stepped down as rector at Uppsala University in December 1772, mostly due to his declining health.\n\nLinnaeus' last years were troubled by illness. He had suffered from a disease called the Uppsala fever in 1764, but survived thanks to the care of Rosén. He developed sciatica in 1773, and the next year, he had a stroke which partially paralysed him. He suffered a second stroke in 1776, losing the use of his right side and leaving him bereft of his memory; while still able to admire his own writings, he could not recognise himself as their author.\n\nIn December 1777, he had another stroke which greatly weakened him, and eventually led to his death on 10 January 1778 in Hammarby. Despite his desire to be buried in Hammarby, he was buried in Uppsala Cathedral on 22 January.\n\nHis library and collections were left to his widow Sara and their children. Joseph Banks, an English botanist, wanted to buy the collection, but his son Carl refused and moved the collection to Uppsala. In 1783 Carl died and Sara inherited the collection, having outlived both her husband and son. She tried to sell it to Banks, but he was no longer interested; instead an acquaintance of his agreed to buy the collection. The acquaintance was a 24-year-old medical student, James Edward Smith, who bought the whole collection: 14,000 plants, 3,198 insects, 1,564 shells, about 3,000 letters and 1,600 books. Smith founded the Linnean Society of London five years later.\n\nThe von Linné name ended with his son Carl, who never married. His other son, Johannes, had died aged 3. There are over two hundred descendants of Linnaeus through two of his daughters.\n\nDuring Linnaeus' time as Professor and Rector of Uppsala University, he taught many devoted students, 17 of whom he called \"apostles\". They were the most promising, most committed students, and all of them made botanical expeditions to various places in the world, often with his help. The amount of this help varied; sometimes he used his influence as Rector to grant his apostles a scholarship or a place on an expedition. To most of the apostles he gave instructions of what to look for on their journeys. Abroad, the apostles collected and organised new plants, animals and minerals according to Linnaeus' system. Most of them also gave some of their collection to Linnaeus when their journey was finished. Thanks to these students, the Linnaean system of taxonomy spread through the world without Linnaeus ever having to travel outside Sweden after his return from Holland. The British botanist William T. Stearn notes without Linnaeus' new system, it would not have been possible for the apostles to collect and organise so many new specimens. Many of the apostles died during their expeditions.\n\nChristopher Tärnström, the first apostle and a 43-year-old pastor with a wife and children, made his journey in 1746. He boarded a Swedish East India Company ship headed for China. Tärnström never reached his destination, dying of a tropical fever on Côn Sơn Island the same year. Tärnström's widow blamed Linnaeus for making her children fatherless, causing Linnaeus to prefer sending out younger, unmarried students after Tärnström. Six other apostles later died on their expeditions, including Pehr Forsskål and Pehr Löfling.\n\nTwo years after Tärnström's expedition, Finnish-born Pehr Kalm set out as the second apostle to North America. There he spent two-and-a-half years studying the flora and fauna of Pennsylvania, New York, New Jersey and Canada. Linnaeus was overjoyed when Kalm returned, bringing back with him many pressed flowers and seeds. At least 90 of the 700 North American species described in \"Species Plantarum\" had been brought back by Kalm.\n\nDaniel Solander was living in Linnaeus' house during his time as a student in Uppsala. Linnaeus was very fond of him, promising Solander his oldest daughter's hand in marriage. On Linnaeus' recommendation, Solander travelled to England in 1760, where he met the English botanist Joseph Banks. With Banks, Solander joined James Cook on his expedition to Oceania on the \"Endeavour\" in 1768–71. Solander was not the only apostle to journey with James Cook; Anders Sparrman followed on the \"Resolution\" in 1772–75 bound for, among other places, Oceania and South America. Sparrman made many other expeditions, one of them to South Africa.\n\nPerhaps the most famous and successful apostle was Carl Peter Thunberg, who embarked on a nine-year expedition in 1770. He stayed in South Africa for three years, then travelled to Japan. All foreigners in Japan were forced to stay on the island of Dejima outside Nagasaki, so it was thus hard for Thunberg to study the flora. He did, however, manage to persuade some of the translators to bring him different plants, and he also found plants in the gardens of Dejima. He returned to Sweden in 1779, one year after Linnaeus' death.\n\nThe first edition of \"\" was printed in the Netherlands in 1735. It was a twelve-page work. By the time it reached its 10th edition in 1758, it classified 4,400 species of animals and 7,700 species of plants. People from all over the world sent their specimens to Linnaeus to be included. By the time he started work on the 12th edition, Linnaeus needed a new invention - the index card - to track classifications.\n\nIn \"Systema Naturae\", the unwieldy names mostly used at the time, such as \"\"\"\", were supplemented with concise and now familiar \"binomials\", composed of the generic name, followed by a specific epithet – in the case given, \"Physalis angulata\". These binomials could serve as a label to refer to the species. Higher taxa were constructed and arranged in a simple and orderly manner. Although the system, now known as binomial nomenclature, was partially developed by the Bauhin brothers (see Gaspard Bauhin and Johann Bauhin) almost 200 years earlier, Linnaeus was the first to use it consistently throughout the work, including in monospecific genera, and may be said to have popularised it within the scientific community.\n\nAfter the decline in Linnaeus' health in the early 1770s, publication of editions of \"Systema Naturae\" went in two different directions. Another Swedish scientist, Johan Andreas Murray issued the \"Regnum Vegetabile\" section separately in 1774 as the \"Systema Vegetabilium\", rather confusingly labelled the 13th edition. Meanwhile a 13th edition of the entire \"Systema\" appeared in parts between 1788 and 1793. It was through the \"Systema Vegetabilium\" that Linnaeus' work became widely known in England, following its translation from the Latin by the Lichfield Botanical Society as \"A System of Vegetables\" (1783–1785).\n\n' (or, more fully, ') was first published in 1753, as a two-volume work. Its prime importance is perhaps that it is the primary starting point of plant nomenclature as it exists today.\n\n\"\" was first published in 1737, delineating plant genera. Around 10 editions were published, not all of them by Linnaeus himself; the most important is the 1754 fifth edition. In it Linnaeus divided the plant Kingdom into 24 classes. One, Cryptogamia, included all the plants with concealed reproductive parts (algae, fungi, mosses and liverworts and ferns).\n\n' (1751) was a summary of Linnaeus' thinking on plant classification and nomenclature, and an elaboration of the work he had previously published in ' (1736) and ' (1737). Other publications forming part of his plan to reform the foundations of botany include his ' and ': all were printed in Holland (as were ' (1737) and \" (1735)), the \"Philosophia\" being simultaneously released in Stockholm.\n\nAt the end of his lifetime the Linnean collection in Uppsala was considered one of the finest collections of natural history objects in Sweden. Next to his own collection he had also built up a museum for the university of Uppsala, which was supplied by material donated by Carl Gyllenborg (in 1744–1745), crown-prince Adolf Fredrik (in 1745), Erik Petreus (in 1746), Claes Grill (in 1746), Magnus Lagerström (in 1748 and 1750) and Jonas Alströmer (in 1749). The relation between the museum and the private collection was not formalised and the steady flow of material from Linnean pupils were incorporated to the private collection rather than to the museum. Linnaeus felt his work was reflecting the harmony of nature and he said in 1754 'the earth is then nothing else but a museum of the all-wise creator's masterpieces, divided into three chambers'. He had turned his own estate into a microcosm of that 'world museum'.\n\nIn April 1766 parts of the town were destroyed by a fire and the Linnean private collection was subsequently moved to a barn outside the town, and shortly afterwards to a single-room stone building close to his country house at Hammarby near Uppsala. This resulted in a physical separation between the two collections, the museum collection remained in the botanical garden of the university. Some material which needed special care (alcohol specimens) or ample storage space was moved from the private collection to the museum.\n\nIn Hammarby the Linnean private collections suffered seriously from damp and the depredations by mice and insects. Carl von Linné's son (Carl Linnaeus) inherited the collections in 1778 and retained them until his own death in 1783. Shortly after Carl von Linné's death his son confirmed that mice had caused \"horrible damage\" to the plants and that also moths and mould had caused considerable damage. He tried to rescue them from the neglect they had suffered during his father's later years, and also added further specimens. This last activity however reduced rather than augmented the scientific value of the original material.\n\nIn 1784 the botanist James Edward Smith purchased nearly all of the Linnean private scientific effects from the widow and daughter of Carl Linnaeus and transferred them to London. Not all material in Linné's private collection was transported to England. Thirty-three fish specimens preserved in alcohol were not sent and were later lost.\n\nIn London Smith tended to neglect the zoological parts of the collection, he added some specimens and also gave some specimens away. Over the following centuries the Linnean collection in London suffered enormously at the hands of scientists who studied the collection, and in the process disturbed the original arrangement and labels, added specimens that did not belong to the original series and withdrew precious original type material.\n\nMuch material which had been intensively studied by Linné in his scientific career belonged to the collection of Queen Lovisa Ulrika (1720–1782) (in the Linnean publications referred to as \"Museum Ludovicae Ulricae\" or \"M. L. U.\"). This collection was donated by his grandson King Gustav IV Adolf (1778–1837) to the museum in Uppsala in 1804. Another important collection in this respect was that of her husband King Adolf Fredrik (1710–1771) (in the Linnean sources known as \"Museum Adolphi Friderici\" or \"Mus. Ad. Fr.\"), the wet parts (alcohol collection) of which were later donated to the Royal Swedish Academy of Sciences, and is today housed in the Swedish Museum of Natural History at Stockholm. The dry material was transferred to Uppsala.\n\nThe establishment of universally accepted conventions for the naming of organisms was Linnaeus' main contribution to taxonomy—his work marks the starting point of consistent use of binomial nomenclature. During the 18th century expansion of natural history knowledge, Linnaeus also developed what became known as the \"Linnaean taxonomy\"; the system of scientific classification now widely used in the biological sciences. A previous zoologist Rumphius (1627–1702) had more or less approximated the Linnaean system and his material contributed to the later development of the binomial scientific classification by Linnaeus.\n\nThe Linnaean system classified nature within a nested hierarchy, starting with three kingdoms. Kingdoms were divided into classes and they, in turn, into orders, and thence into genera (\"singular:\" genus), which were divided into Species (\"singular:\" species). Below the rank of species he sometimes recognised taxa of a lower (unnamed) rank; these have since acquired standardised names such as \"variety\" in botany and \"subspecies\" in zoology. Modern taxonomy includes a rank of family between order and genus and a rank of phylum between kingdom and class that were not present in Linnaeus' original system.\n\nLinnaeus' groupings were based upon shared physical characteristics, and not simply upon differences. Of his higher groupings, only those for animals are still in use, and the groupings themselves have been significantly changed since their conception, as have the principles behind them. Nevertheless, Linnaeus is credited with establishing the idea of a hierarchical structure of classification which is based upon observable characteristics and intended to reflect natural relationships. While the underlying details concerning what are considered to be scientifically valid \"observable characteristics\" have changed with expanding knowledge (for example, DNA sequencing, unavailable in Linnaeus' time, has proven to be a tool of considerable utility for classifying living organisms and establishing their evolutionary relationships), the fundamental principle remains sound.\n\nLinnaeus' applied science was inspired not only by the instrumental utilitarianism general to the early Enlightenment, but also by his adherence to the older economic doctrine of Cameralism. Additionally, Linnaeus was a state interventionist. He supported tariffs, levies, export bounties, quotas, embargoes, navigation acts, subsidised investment capital, ceilings on wages, cash grants, state-licensed producer monopolies, and cartels.\n\nAccording to German biologist Ernst Haeckel, the question of man's origin began with Linnaeus. He helped future research in the natural history of man by describing humans just as he described any other plant or animal.\n\nLinnaeus classified humans among the primates (as they were later called) beginning with the first edition of \"\". During his time at Hartekamp, he had the opportunity to examine several monkeys and noted similarities between them and man. He pointed out both species basically have the same anatomy; except for speech, he found no other differences. Thus he placed man and monkeys under the same category, \"Anthropomorpha\", meaning \"manlike.\" This classification received criticism from other biologists such as Johan Gottschalk Wallerius, Jacob Theodor Klein and Johann Georg Gmelin on the ground that it is illogical to describe a human as 'like a man'. In a letter to Gmelin from 1747, Linnaeus replied:\n\"It does not please [you] that I've placed Man among the Anthropomorpha, perhaps because of the term 'with human form', but man learns to know himself. Let's not quibble over words. It will be the same to me whatever name we apply. But I seek from you and from the whole world a generic difference between man and simian that [follows] from the principles of Natural History. I absolutely know of none. If only someone might tell me a single one! If I would have called man a simian or vice versa, I would have brought together all the theologians against me. Perhaps I ought to have by virtue of the law of the discipline.\"\n\nThe theological concerns were twofold: first, putting man at the same level as monkeys or apes would lower the spiritually higher position that man was assumed to have in the great chain of being, and second, because the Bible says man was created in the image of God (theomorphism), if monkeys/apes and humans were not distinctly and separately designed, that would mean monkeys and apes were created in the image of God as well. This was something many could not accept. The conflict between world views that was caused by asserting man was a type of animal would simmer for a century until the much greater, and still ongoing, creation–evolution controversy began in earnest with the publication of \"On the Origin of Species\" by Charles Darwin in 1859.\n\nAfter such criticism, Linnaeus felt he needed to explain himself more clearly. The 10th edition of ' introduced new terms, including \"Mammalia\" and \"Primates\", the latter of which would replace \"Anthropomorpha\" as well as giving humans the full binomial \"Homo sapiens\". The new classification received less criticism, but many natural historians still believed he had demoted humans from their former place to rule over nature, not be a part of it. Linnaeus believed that man biologically belongs to the animal kingdom and had to be included in it. In his book ', he said, \"One should not vent one's wrath on animals, Theology decree that man has a soul and that the animals are mere 'aoutomata mechanica,' but I believe they would be better advised that animals have a soul and that the difference is of nobility.\"\n\nLinnaeus added a second species to the genus \"Homo\" in \"\" based on a figure and description by Jacobus Bontius from a 1658 publication: \"Homo troglodytes\" (\"caveman\") and published a third in 1771: \"Homo lar\". Swedish historian Gunnar Broberg states that the new human species Linnaeus described were actually simians or native people clad in skins to frighten colonial settlers, whose appearance had been exaggerated in accounts to Linnaeus.\n\nIn early editions of \"\", many well-known legendary creatures were included such as the phoenix, dragon and manticore as well as cryptids like the satyrus, which Linnaeus collected into the catch-all category \"Paradoxa\". Broberg thought Linnaeus was trying to offer a natural explanation and demystify the world of superstition. Linnaeus tried to debunk some of these creatures, as he had with the hydra; regarding the purported remains of dragons, Linnaeus wrote that they were either derived from lizards or rays. For \"Homo troglodytes\" he asked the Swedish East India Company to search for one, but they did not find any signs of its existence. \"Homo lar\" has since been reclassified as \"Hylobates lar\", the lar gibbon.\n\nIn the first edition of \"\", Linnaeus subdivided the human species into four varieties based on continent and skin colour: \"Europæus albus\" (white European), \"Americanus rubescens\" (red American), \"Asiaticus fuscus\" (brown Asian) and \"Africanus Niger\" (black African). In the tenth edition of Systema Naturae he further detailed stereotypical characteristics for each variety, based on the concept of the four temperaments from classical antiquity, and changed the description of Asians' skin tone to \"luridus\" (yellow). Additionally, Linnaeus created a wastebasket taxon \"monstrosus\" for \"wild and monstrous humans, unknown groups, and more or less abnormal people\".\n\nAnniversaries of Linnaeus' birth, especially in centennial years, have been marked by major celebrations. Linnaeus has appeared on numerous Swedish postage stamps and banknotes. There are numerous statues of Linnaeus in countries around the world. The Linnean Society of London has awarded the Linnean Medal for excellence in botany or zoology since 1888. Following approval by the Riksdag of Sweden, Växjö University and Kalmar College merged on 1 January 2010 to become Linnaeus University. Other things named after Linnaeus include the twinflower genus \"Linnaea\", the crater Linné on the Earth's moon, a street in Cambridge, Massachusetts, and the cobalt sulfide mineral Linnaeite.\n\nAndrew Dickson White wrote in \"\" (1896):\n\nLinnaeus ... was the most eminent naturalist of his time, a wide observer, a close thinker; but the atmosphere in which he lived and moved and had his being was saturated with biblical theology, and this permeated all his thinking. ... Toward the end of his life he timidly advanced the hypothesis that all the species of one genus constituted at the creation one species; and from the last edition of his \"Systema Naturæ\" he quietly left out the strongly orthodox statement of the fixity of each species, which he had insisted upon in his earlier works. ... warnings came speedily both from the Catholic and Protestant sides.\nThe mathematical PageRank algorithm, applied to 24 multilingual Wikipedia editions in 2014, published in \"PLOS ONE\" in 2015, placed Carl Linnaeus at the top historical figure, above Jesus, Aristotle, Napoleon, and Adolf Hitler (in that order).\n\n\n\n\n\n\n"}
{"id": "5236", "url": "https://en.wikipedia.org/wiki?curid=5236", "title": "Coast", "text": "Coast\n\nA coastline or a seashore is the area where land meets the sea or ocean, or a line that forms the boundary between the land and the ocean or a lake. A precise line that can be called a coastline cannot be determined due to the Coastline paradox.\n\nThe term \"coastal zone\" is a region where interaction of the sea and land processes occurs. Both the terms coast and coastal are often used to describe a geographic location or region; for example, New Zealand's West Coast, or the East and West Coasts of the United States. Edinburgh for example is a city on the coast of Scotland.\n\nA pelagic coast refers to a coast which fronts the open ocean, as opposed to a more sheltered coast in a gulf or bay. A shore, on the other hand, can refer to parts of the land which adjoin any large body of water, including oceans (sea shore) and lakes (lake shore). Similarly, the somewhat related term \"[Stream bed/bank]\" refers to the land alongside or sloping down to a river (riverbank) or to a body of water smaller than a lake. \"Bank\" is also used in some parts of the world to refer to an artificial ridge of earth intended to retain the water of a river or pond; in other places this may be called a levee.\nWhile many scientific experts might agree on a common definition of the term \"coast\", the delineation of the extents of a coast differ according to jurisdiction, with many scientific and government authorities in various countries differing for economic and social policy reasons. According to the UN atlas, 44% of people live within of the sea.\n\nTides often determine the range over which sediment is deposited or eroded. Areas with high tidal ranges allow waves to reach farther up the shore, and areas with lower tidal ranges produce deprossosition at a smaller elevation interval. The tidal range is influenced by the size and shape of the coastline. Tides do not typically cause erosion by themselves; however, tidal bores can erode as the waves surge up river estuaries from the ocean.\n\nWaves erode coastline as they break on shore releasing their energy; the larger the wave the more energy it releases and the more sediment it moves. Coastlines with longer shores have more room for the waves to disperse their energy, while coasts with cliffs and short shore faces give little room for the wave energy to be dispersed. In these areas the wave energy breaking against the cliffs is higher, and air and water are compressed into cracks in the rock, forcing the rock apart, breaking it down. Sediment deposited by waves comes from eroded cliff faces and is moved along the coastline by the waves. This forms an abrasion or cliffed coast.\n\nSediment deposited by rivers is the dominant influence on the amount of sediment located on a coastline. Today riverine deposition at the coast is often blocked by dams and other human regulatory devices, which remove the sediment from the stream by causing it to be deposited inland.\n\nLike the ocean which shapes them, coasts are a dynamic environment with constant change. The Earth's natural processes, particularly sea level rises, waves and various weather phenomena, have resulted in the erosion, accretion and reshaping of coasts as well as flooding and creation of continental shelves and drowned river valleys (rias).\n\nThe coast and its adjacent areas on and off shore are an important part of a local ecosystem: the mixture of fresh water and salt water (brackish water) in estuaries provides many nutrients for marine life. Salt marshes and beaches also support a diversity of plants, animals and insects crucial to the food chain.\n\nThe high level of biodiversity creates a high level of biological activity, which has attracted human activity for thousands of years.\n\nMore and more of the world's people live in coastal regions. Many major cities are on or near good harbors and have port facilities. Some landlocked places have achieved port status by building canals.\n\nThe coast is a frontier that nations have typically defended against military invaders, smugglers and illegal migrants. Fixed coastal defenses have long been erected in many nations and coastal countries typically have a navy and some form of coast guard.\n\nCoasts, especially those with beaches and warm water, attract tourists. In many island nations such as those of the Mediterranean, South Pacific and Caribbean, tourism is central to the economy. Coasts offer recreational activities such as swimming, fishing, surfing, boating, and sunbathing. Growth management can be a challenge for coastal local authorities who often struggle to provide the infrastructure required by new residents.\n\nCoasts also face many human-induced environmental impacts. The human influence on climate change is thought to contribute to an accelerated trend in sea level rise which threatens coastal habitats.\n\nPollution can occur from a number of sources: garbage and industrial debris; the transportation of petroleum in tankers, increasing the probability of large oil spills; small oil spills created by large and small vessels, which flush bilge water into the ocean.\n\nFishing has declined due to habitat degradation, overfishing, trawling, bycatch and climate change. Since the growth of global fishing enterprises after the 1950s, intensive fishing has spread from a few concentrated areas to encompass nearly all fisheries. The scraping of the ocean floor in bottom dragging is devastating to coral, sponges and other long-lived species that do not recover quickly. This destruction alters the functioning of the ecosystem and can permanently alter species composition and biodiversity. Bycatch, the capture of unintended species in the course of fishing, is typically returned to the ocean only to die from injuries or exposure. Bycatch represents about a quarter of all marine catch. In the case of shrimp capture, the bycatch is five times larger than the shrimp caught.\n\nIt is believed that melting Arctic ice will cause sea levels to rise and flood coastal areas.\nExtraordinary population growth in the 21st century has placed stress on the planet's ecosystems. For example, on Saint Lucia, harvesting mangrove for timber and clearing for fishing reduced the mangrove forests, resulting in a loss of habitat and spawning grounds for marine life that was unique to the area. These forests also helped to stabilize the coastline. Conservation efforts since the 1980s have partially restored the ecosystem.\n\nAccording to one principle of classification, an emergent coastline is a coastline which has experienced a fall in sea level, because of either a global sea level change, or local uplift. Emergent coastlines are identifiable by the coastal landforms, which are above the high tide mark, such as raised beaches. In contrast, a submergent coastline is one where the sea level has risen, due to a global sea level change, local subsidence, or isostatic rebound. Submergent coastlines are identifiable by their submerged, or \"drowned\" landforms, such as rias (drowned valleys) and fjords.\nAccording to a second principle of classification, a concordant coastline is a coastline where bands of different rock types run parallel to the shore. These rock types are usually of varying resistance, so the coastline forms distinctive landforms, such as coves. Discordant coastlines feature distinctive landforms because the rocks are eroded by ocean waves. The less resistant rocks erode faster, creating inlets or bay; the more resistant rocks erode more slowly, remaining as headlands or outcroppings.\nOther coastal categories:\n\nThe following articles describe some coastal landforms\n\n\nThe following articles describe the various geologic processes that affect a coastal zone:\n\nSome of the animals live along a typical coast. There are animals like puffins, sea turtles and rockhopper penguins. Sea snails and various kinds of barnacles live on the coast and scavenge on food deposited by the sea. Most coastal animals are used to humans in developed areas, such as dolphins and seagulls who eat food thrown for them by tourists. Since the coastal areas are all part of the littoral zone, there is a profusion of marine life found just off-coast.\n\nThere are many kinds of seabirds on the coast. Pelicans and cormorants join up with terns and oystercatchers to forage for fish and shellfish on the coast. There are sea lions on the coast of Wales and other countries.\n\nCoastal areas are famous for their kelp beds. Kelp is a fast-growing seaweed that grows up to a metre a day. Corals and sea anemones are true animals, but live a lifestyle similar to that of plants. Mangroves, seagrasses and salt marsh are important coastal vegetation types in tropical and temperate environments respectively.\n\nShortly before 1951, Lewis Fry Richardson, in researching the possible effect of border lengths on the probability of war, noticed that the Portuguese reported their measured border with Spain to be 987 km, but the Spanish reported it as 1214 km. This was the beginning of the coastline problem, which is a mathematical uncertainty inherent in the measurement of boundaries that are irregular.\n\nThe prevailing method of estimating the length of a border (or coastline) was to lay out \"n\" equal straight-line segments of length \"ℓ\" with dividers on a map or aerial photograph. Each end of the segment must be on the boundary. Investigating the discrepancies in border estimation, Richardson discovered what is now termed the Richardson Effect: the sum of the segments is inversely proportional to the common length of the segments. In effect, the shorter the ruler, the longer the measured border; the Spanish and Portuguese geographers were simply using different-length rulers.\n\nThe result most astounding to Richardson is that, under certain circumstances, as \"ℓ\" approaches zero, the length of the coastline approaches infinity. Richardson had believed, based on Euclidean geometry, that a coastline would approach a fixed length, as do similar estimations of regular geometric figures. For example, the perimeter of a regular polygon inscribed in a circle approaches the circumference with increasing numbers of sides (and decrease in the length of one side). In geometric measure theory such a smooth curve as the circle that can be approximated by small straight segments with a definite limit is termed a rectifiable curve.\n\nMore than a decade after Richardson completed his work, Benoit Mandelbrot developed a new branch of mathematics, fractal geometry, to describe just such non-rectifiable complexes in nature as the infinite coastline. His own definition of the new figure serving as the basis for his study is:\n\nA key property of the fractal is self-similarity; that is, at any scale the same general configuration appears. A coastline is perceived as bays alternating with promontories. In the hypothetical situation that a given coastline has this property of self-similarity, then no matter how greatly any one small section of coastline is magnified, a similar pattern of smaller bays and promontories superimposed on larger bays and promontories appears, right down to the grains of sand. At that scale the coastline appears as a momentarily shifting, potentially infinitely long thread with a stochastic arrangement of bays and promontories formed from the small objects at hand. In such an environment (as opposed to smooth curves) Mandelbrot asserts \"coastline length turns out to be an elusive notion that slips between the fingers of those who want to grasp it.\"\n\nThere are different kinds of fractals. A coastline with the stated property is in \"a first category of fractals, namely curves whose fractal dimension is greater than 1.\" That last statement represents an extension by Mandelbrot of Richardson's thought. Mandelbrot's statement of the Richardson Effect is:\n\nwhere L, coastline length, a function of the measurement unit, ε, is approximated by the expression. F is a constant and D is a parameter that Richardson found depended on the coastline approximated by L. He gave no theoretical explanation but Mandelbrot identified D with a non-integer form of the Hausdorff dimension, later the fractal dimension. Rearranging the right side of the expression obtains:\n\nwhere Fε must be the number of units ε required to obtain L. The fractal dimension is the number of the dimensions of the figure being used to approximate the fractal: 0 for a dot, 1 for a line, 2 for a square. D in the expression is between 1 and 2, for coastlines typically less than 1.5. The broken line measuring the coast does not extend in one direction nor does it represent an area, but is intermediate. It can be interpreted as a thick line or band of width 2ε. More broken coastlines have greater D and therefore L is longer for the same ε. Mandelbrot showed that D is independent of ε.\n\n\n"}
{"id": "5237", "url": "https://en.wikipedia.org/wiki?curid=5237", "title": "Catatonia", "text": "Catatonia\n\nCatatonia is a state of psychogenic motor immobility and behavioral abnormality manifested by stupor. It was first described in 1874 by Karl Ludwig Kahlbaum, in \"Die Katatonie oder das Spannungsirresein\" (\"Catatonia or Tension Insanity\").\n\nIn the fifth edition of the \"Diagnostic and Statistical Manual of Mental Disorders\", catatonia is not recognized as a separate disorder, but is associated with psychiatric conditions such as schizophrenia (catatonic type), bipolar disorder, post-traumatic stress disorder, depression and other mental disorders, narcolepsy, as well as drug abuse or overdose (or both). It may also be seen in many medical disorders including infections (such as encephalitis), autoimmune disorders, focal neurologic lesions (including strokes), metabolic disturbances, alcohol withdrawal and abrupt or overly rapid benzodiazepine withdrawal.\n\nIt can be an adverse reaction to prescribed medication. It bears similarity to conditions such as encephalitis lethargica and neuroleptic malignant syndrome. There are a variety of treatments available; benzodiazepines are a first-line treatment strategy. Electroconvulsive therapy is also sometimes used. There is growing evidence for the effectiveness of NMDA antagonists for benzodiazepine resistant catatonia. Antipsychotics are sometimes employed but require caution as they can worsen symptoms and have serious adverse effects.\n\nPeople with catatonia may experience an extreme loss of motor skill or even constant hyperactive motor activity. Catatonic patients will sometimes hold rigid poses for hours and will ignore any external stimuli. People with catatonic excitement can suffer from exhaustion if not treated. Patients may also show stereotyped, repetitive movements.\n\nThey may show specific types of movement such as waxy flexibility, in which they maintain positions after being placed in them by someone else. Conversely, they may remain in a fixed position by resisting movement in proportion to the force applied by the examiner. They may repeat meaningless phrases or speak only to repeat what the examiner says.\n\nWhile catatonia is only identified as a symptom of schizophrenia in present psychiatric classifications, it is increasingly recognized as a syndrome with many faces. It appears as the Kahlbaum syndrome (motionless catatonia), malignant catatonia (neuroleptic malignant syndrome, toxic serotonin syndrome), and excited forms (delirious mania, catatonic excitement, oneirophrenia). \nIt has also been recognized as grafted on to autism spectrum disorders.\n\nAccording to the DSM-V, \"Catatonia Associated with Another Mental Disorder (Catatonia Specifier)\" is diagnosed if the clinical picture is dominated by at least three of the following:\n\n\n\nFink and Taylor developed a catatonia rating scale to identify the syndrome. A diagnosis is verified by a benzodiazepine or barbiturate test. The diagnosis is validated by the quick response to either benzodiazepines or electroconvulsive therapy (ECT). While proven useful in the past, barbiturates are no longer commonly used in psychiatry; thus the option of either benzodiazepines or ECT.\n\nInitial treatment is aimed at providing symptomatic relief. Benzodiazepines are the first line of treatment, and high doses are often required. A test dose of intramuscular lorazepam will often result in marked improvement within half an hour. In France, zolpidem has also been used in diagnosis, and response may occur within the same time period. Ultimately the underlying cause needs to be treated.\n\nElectroconvulsive therapy (ECT) is an effective treatment for catatonia. Antipsychotics should be used with care as they can worsen catatonia and are the cause of neuroleptic malignant syndrome, a dangerous condition that can mimic catatonia and requires immediate discontinuation of the antipsychotic.\n\nExcessive glutamate activity is believed to be involved in catatonia; when first-line treatment options fail, NMDA antagonists such as amantadine or memantine are used. Amantadine may have an increased incidence of tolerance with prolonged use and can cause psychosis, due to its additional effects on the dopamine system. Memantine has a more targeted pharmacological profile for the glutamate system, reduced incidence of psychosis and may therefore be preferred for individuals who cannot tolerate amantadine. Topiramate is another treatment option for resistant catatonia; it produces its therapeutic effects by producing glutamate antagonism via modulation of AMPA receptors.\n\n\n"}
{"id": "5244", "url": "https://en.wikipedia.org/wiki?curid=5244", "title": "Cipher", "text": "Cipher\n\nIn cryptography, a cipher (or cypher) is an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure. An alternative, less common term is \"encipherment\". To encipher or encode is to convert information into cipher or code. In common parlance, \"cipher\" is synonymous with \"code\", as they are both a set of steps that encrypt a message; however, the concepts are distinct in cryptography, especially classical cryptography.\n\nCodes generally substitute different length strings of characters in the output, while ciphers generally substitute the same number of characters as are input. There are exceptions and some cipher systems may use slightly more, or fewer, characters when output versus the number that were input.\n\nCodes operated by substituting according to a large codebook which linked a random string of characters or numbers to a word or phrase. For example, \"UQJHSE\" could be the code for \"Proceed to the following coordinates.\" When using a cipher the original information is known as plaintext, and the encrypted form as ciphertext. The ciphertext message contains all the information of the plaintext message, but is not in a format readable by a human or computer without the proper mechanism to decrypt it.\n\nThe operation of a cipher usually depends on a piece of auxiliary information, called a key (or, in traditional NSA parlance, a \"cryptovariable\"). The encrypting procedure is varied depending on the key, which changes the detailed operation of the algorithm. A key must be selected before using a cipher to encrypt a message. Without knowledge of the key, it should be extremely difficult, if not impossible, to decrypt the resulting ciphertext into readable plaintext.\n\nMost modern ciphers can be categorized in several ways\n\nThe word \"cipher\" (minority spelling \"cypher\") in former times meant \"zero\" and had the same origin: Middle French as ' and Medieval Latin as \"cifra,\" from the Arabic صفر\"' \"sifr\" = zero (see Zero—Etymology). \"Cipher\" was later used for any decimal digit, even any number. There are many theories about how the word \"cipher\" may have come to mean \"encoding\". In fact the more ancient source of word \"Cypher\" is the ancient Hebrew; there are more than 100 verses in the Hebrew Bible - Torah using word \"Cepher\": means (Book or Story telling), and in some of them the word \"Cipher\" literally means (Counting)-- (Numerical description)-- Example, Book 2 Samuel 24:10, Isaiah 33:18, and Jeremiah 52:25. \n\nIbrahim Al-Kadi concluded that the Arabic word \"sifr\", for the digit zero, developed into the European technical term for encryption.\n\nAs the decimal zero and its new mathematics spread from the Arabic world to Europe in the Middle Ages, words derived from \"sifr\" and \"zephyrus\" came to refer to calculation, as well as to privileged knowledge and secret codes. According to Ifrah, \"in thirteenth-century Paris, a 'worthless fellow' was called a , i.e., an 'arithmetical nothing'.\" Cipher was the European pronunciation of sifr, and cipher came to mean a message or communication not easily understood.\n\nIn non-technical usage, a \"(secret) code\" typically means a \"cipher\". Within technical discussions, however, the words \"code\" and \"cipher\" refer to two different concepts. Codes work at the level of meaning—that is, words or phrases are converted into something else and this chunking generally shortens the message.\n\nAn example of this is the Commercial Telegraph Code which was used to shorten long telegraph messages which resulted from entering into commercial contracts using exchanges of Telegrams.\n\nAnother example is given by whole words cipher s, which allow the user to replace an entire word with a symbol or character, much like the way Japanese utilize Kanji (Japanese) characters to supplement their language. ex \"The quick brown fox jumps over the lazy dog\" becomes \"The quick brown 狐 jumps 过 the lazy 狗\".\n\nCiphers, on the other hand, work at a lower level: the level of individual letters, small groups of letters, or, in modern schemes, individual bits and blocks of bits. Some systems used both codes and ciphers in one system, using superencipherment to increase the security. In some cases the terms codes and ciphers are also used synonymously to substitution and transposition.\n\nHistorically, cryptography was split into a dichotomy of codes and ciphers; and coding had its own terminology, analogous to that for ciphers: \"\"encoding\", \"codetext\", \"decoding\"\" and so on.\n\nHowever, codes have a variety of drawbacks, including susceptibility to cryptanalysis and the difficulty of managing a cumbersome codebook. Because of this, codes have fallen into disuse in modern cryptography, and ciphers are the dominant technique.\n\nThere are a variety of different types of encryption. Algorithms used earlier in the history of cryptography are substantially different from modern methods, and modern ciphers can be classified according to how they operate and whether they use one or two keys.\n\nHistorical pen and paper ciphers used in the past are sometimes known as classical ciphers. They include simple substitution ciphers (such as Rot 13) and transposition ciphers (such as a Rail Fence Cipher). For example, \"GOOD DOG\" can be encrypted as \"PLLX XLP\" where \"L\" substitutes for \"O\", \"P\" for \"G\", and \"X\" for \"D\" in the message. Transposition of the letters \"GOOD DOG\" can result in \"DGOGDOO\". These simple ciphers and examples are easy to crack, even without plaintext-ciphertext pairs.\n\nSimple ciphers were replaced by polyalphabetic substitution ciphers (such as the Vigenère) which changed the substitution alphabet for every letter. For example, \"GOOD DOG\" can be encrypted as \"PLSX TWF\" where \"L\", \"S\", and \"W\" substitute for \"O\". With even a small amount of known or estimated plaintext, simple polyalphabetic substitution ciphers and letter transposition ciphers designed for pen and paper encryption are easy to crack. It is possible to create a secure pen and paper cipher based on a one-time pad though, but the usual disadvantages of one-time pads apply.\n\nDuring the early twentieth century, electro-mechanical machines were invented to do encryption and decryption using transposition, polyalphabetic substitution, and a kind of \"additive\" substitution. In rotor machines, several rotor disks provided polyalphabetic substitution, while plug boards provided another substitution. Keys were easily changed by changing the rotor disks and the plugboard wires. Although these encryption methods were more complex than previous schemes and required machines to encrypt and decrypt, other machines such as the British Bombe were invented to crack these encryption methods.\n\nModern encryption methods can be divided by two criteria: by type of key used, and by type of input data.\n\nBy type of key used ciphers are divided into:\n\nIn a symmetric key algorithm (e.g., DES and AES), the sender and receiver must have a shared key set up in advance and kept secret from all other parties; the sender uses this key for encryption, and the receiver uses the same key for decryption. The Feistel cipher uses a combination of substitution and transposition techniques. Most block cipher algorithms are based on this structure. In an asymmetric key algorithm (e.g., RSA), there are two separate keys: a \"public key\" is published and enables any sender to perform encryption, while a \"private key\" is kept secret by the receiver and enables only him to perform correct decryption.\n\nCiphers can be distinguished into two types by the type of input data:\n\nIn a pure mathematical attack, (i.e., lacking any other information to help break a cipher) two factors above all count:\n\nSince the desired effect is computational difficulty, in theory one would choose an algorithm and desired difficulty level, thus decide the key length accordingly.\n\nAn example of this process can be found at Key Length which uses multiple reports to suggest that a symmetric cipher with 128 bits, an asymmetric cipher with 3072 bit keys, and an elliptic curve cipher with 512 bits, all have similar difficulty at present.\n\nClaude Shannon proved, using information theory considerations, that any theoretically unbreakable cipher must have keys which are at least as long as the plaintext, and used only once: one-time pad.\n\n\n"}
{"id": "5247", "url": "https://en.wikipedia.org/wiki?curid=5247", "title": "Country music", "text": "Country music\n\nCountry music (frequently referred to as just country) is a musical genre that originated in the Southern United States in the 1920s. It takes its roots from genres such as folk music (especially Appalachian folk music) and blues.\n\nCountry music often consists of ballads and dance tunes with generally simple forms and harmonies accompanied by mostly string instruments such as banjos, electric and acoustic guitars, steel guitars (such as pedal steels and dobros), and fiddles as well as harmonicas. Blues modes have been used extensively throughout its recorded history.\n\nAccording to Lindsey Starnes, the term \"country music\" gained popularity in the 1940s in preference to the earlier term \"hillbilly music\"; it came to encompass Western music, which evolved parallel to hillbilly music from similar roots, in the mid-20th century. In 2009 country music was the most listened to rush hour radio genre during the evening commute, and second most popular in the morning commute in the United States.\n\nThe term \"country music\" is used today to describe many styles and subgenres. The origins of country music are the folk music of working-class Americans, who blended popular songs, Irish and Celtic fiddle tunes, traditional English ballads, and cowboy songs, and various musical traditions from European immigrant communities.\n\nImmigrants to the Southern Appalachian Mountains of North America brought the music and instruments of Europe along with them for nearly 300 years. Country music was \"introduced to the world as a Southern phenomenon.\"\n\nBristol, Tennessee has been formally recognized by the U.S. Congress as the \"Birthplace of Country Music\", based on the historic Bristol recording sessions of 1927. Since 2014, the city has been home to the Birthplace of Country Music Museum. Historians have also noted the influence of the less-known Johnson City sessions of 1928 and 1929, and the Knoxville sessions of 1929 and 1930. Prior to these, pioneer settlers, in the Great Smoky Mountains region, had developed a rich musical heritage.\n\nThe first generation emerged in the early 1920s, with Atlanta's music scene playing a major role in launching country's earliest recording artists. Okeh Records began issuing hillbilly music records by Fiddlin' John Carson as early as 1923, followed by Columbia Records (series 15000D \"Old Familiar Tunes\") (Samantha Bumgarner) in 1924, and RCA Victor Records in 1927 (the Carter Family and Jimmie Rodgers). Many \"hillbilly\" musicians, such as Cliff Carlisle, recorded blues songs throughout the 1920s.\n\nDuring the second generation (1930s–1940s), radio became a popular source of entertainment, and \"barn dance\" shows featuring country music were started all over the South, as far north as Chicago, and as far west as California. The most important was the \"Grand Ole Opry\", aired starting in 1925 by WSM in Nashville and continuing to the present day. During the 1930s and 1940s, cowboy songs, or Western music, which had been recorded since the 1920s, were popularized by films made in Hollywood. Bob Wills was another country musician from the Lower Great Plains who had become very popular as the leader of a \"hot string band,\" and who also appeared in Hollywood westerns. His mix of country and jazz, which started out as dance hall music, would become known as Western swing. Wills was one of the first country musicians known to have added an electric guitar to his band, in 1938. Country musicians began recording boogie in 1939, shortly after it had been played at Carnegie Hall, when Johnny Barfield recorded \"Boogie Woogie\".\n\nThe third generation (1950s–1960s) started at the end of World War II with \"mountaineer\" string band music known as bluegrass, which emerged when Bill Monroe, along with Lester Flatt and Earl Scruggs were introduced by Roy Acuff at the Grand Ole Opry. Gospel music remained a popular component of country music. Another type of stripped-down and raw music with a variety of moods and a basic ensemble of guitar, bass, dobro or steel guitar (and later) drums became popular, especially among poor whites in Texas and Oklahoma. It became known as honky tonk, and had its roots in Western swing and the ranchera music of Mexico and the border states. By the early 1950s a blend of Western swing, country boogie, and honky tonk was played by most country bands. Rockabilly was most popular with country fans in the 1950s, and 1956 could be called the year of rockabilly in country music. Beginning in the mid-1950s, and reaching its peak during the early 1960s, the Nashville sound turned country music into a multimillion-dollar industry centered in Nashville, Tennessee. The late 1960s in American music produced a unique blend as a result of traditionalist backlash within separate genres. In the aftermath of the British Invasion, many desired a return to the \"old values\" of rock n' roll. At the same time there was a lack of enthusiasm in the country sector for Nashville-produced music. What resulted was a crossbred genre known as country rock.\n\nFourth generation (1970s–1980s) music included outlaw country with roots in the Bakersfield sound, and country pop with roots in the countrypolitan, folk music and soft rock. Between 1972 and 1975 singer/guitarist John Denver released a series of hugely successful songs blending country and folk-rock musical styles. During the early 1980s country artists continued to see their records perform well on the pop charts. In 1980 a style of \"neocountry disco music\" was popularized. During the mid-1980s a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts in favor of more traditional \"back-to-basics\" production. Attempts to combine punk and country were pioneered by Jason and the Scorchers, and in the 1980s Southern Californian cowpunk scene with bands like the Long Ryders and Mojo Nixon.\n\nDuring the fifth generation (1990s), country music became a worldwide phenomenon thanks to Garth Brooks and Alan Jackson. The Dixie Chicks became one of the most popular country bands in the 1990s and early 2000s.\n\nThe sixth generation (2000s–present) is exemplified by country singer Carrie Underwood. The influence of rock music in country has become more overt during the late 2000s and early 2010s. Hip-hop also made its mark on country music with the emergence of country rap. Most of the best-selling country songs of this era however were in the country pop genre, such as those by Lady Antebellum, Florida Georgia Line, and Taylor Swift.\n\nAtlanta's music scene played a major role in launching country's earliest recording artists in the early 1920s—many Appalachian people had come to the city to work in its cotton mills and brought their music with them. It would remain a major recording center for two decades and a major performance center for four decades, up to the first country music TV shows on local Atlanta stations in the 1950s. Some record companies in Atlanta turned away early artists such as Fiddlin' John Carson, while others realized that his music would fit perfectly with the lifestyle of the country's agricultural workers. The first commercial recordings of what was considered country music were \"Arkansas Traveler\" and \"Turkey in the Straw\" by fiddlers Henry Gilliland & A.C. (Eck) Robertson on June 30, 1922, for Victor Records and released in April 1923. Columbia Records began issuing records with \"hillbilly\" music (series 15000D \"Old Familiar Tunes\") as early as 1924.\nA year later, on June 14, 1923, Fiddlin' John Carson recorded \"Little Log Cabin in the Lane\" for Okeh Records. Vernon Dalhart was the first country singer to have a nationwide hit in May 1924 with \"Wreck of the Old 97\". The flip side of the record was \"Lonesome Road Blues\", which also became very popular. In April 1924, \"Aunt\" Samantha Bumgarner and Eva Davis became the first female musicians to record and release country songs. Many \"hillbilly\" musicians, such as Cliff Carlisle, recorded blues songs throughout the decade and into the 1930s. Other important early recording artists were Riley Puckett, Don Richardson, Fiddlin' John Carson, Uncle Dave Macon, Al Hopkins, Ernest V. Stoneman, Charlie Poole and the North Carolina Ramblers and The Skillet Lickers. The steel guitar entered country music as early as 1922, when Jimmie Tarlton met famed Hawaiian guitarist Frank Ferera on the West Coast.\n\nJimmie Rodgers and the Carter Family are widely considered to be important early country musicians. Their songs were first captured at a historic recording session in Bristol, Tennessee, on August 1, 1927, where Ralph Peer was the talent scout and sound recordist. A scene in the movie \"O Brother, Where Art Thou?\" depicts a similar occurrence in the same timeframe.\nRodgers fused hillbilly country, gospel, jazz, blues, pop, cowboy, and folk, and many of his best songs were his compositions, including \"Blue Yodel\", which sold over a million records and established Rodgers as the premier singer of early country music. Beginning in 1927, and for the next 17 years, the Carters recorded some 300 old-time ballads, traditional tunes, country songs and gospel hymns, all representative of America's southeastern folklore and heritage.\n\nRecord sales declined during the Great Depression, but radio became a popular source of entertainment, and \"barn dance\" shows featuring country music were started by radio stations all over the South, as far north as Chicago, and as far west as California.\n\nThe most important was the \"Grand Ole Opry\", aired starting in 1925 by WSM in Nashville and continuing to the present day. Some of the early stars on the \"Opry\" were Uncle Dave Macon, Roy Acuff and African American harmonica player DeFord Bailey. WSM's 50,000-watt signal (in 1934) could often be heard across the country. Many musicians performed and recorded songs in any number of styles. Moon Mullican, for example, played Western swing but also recorded songs that can be called rockabilly. Between 1947 and 1949, country crooner Eddy Arnold placed eight songs in the top 10. From 1945 to 1955 Jenny Lou Carson was one of the most prolific songwriters in country music.\n\nDuring the 1930s and 1940s, cowboy songs, or Western music, which had been recorded since the 1920s, were popularized by films made in Hollywood. Some of the popular singing cowboys from the era were Gene Autry, the Sons of the Pioneers, and Roy Rogers. Country music and western music were frequently played together on the same radio stations, hence the term \"country and western\" music. Cowgirls contributed to the sound in various family groups. Patsy Montana opened the door for female artists with her history-making song \"I Want To Be a Cowboy's Sweetheart\". This would begin a movement toward opportunities for women to have successful solo careers. Bob Wills was another country musician from the Lower Great Plains who had become very popular as the leader of a \"hot string band,\" and who also appeared in Hollywood westerns. His mix of country and jazz, which started out as dance hall music, would become known as Western swing. Cliff Bruner, Moon Mullican, Milton Brown and Adolph Hofner were other early Western swing pioneers. Spade Cooley and Tex Williams also had very popular bands and appeared in films. At its height, Western swing rivaled the popularity of big band swing music.\n\nDrums were scorned by early country musicians as being \"too loud\" and \"not pure\", but by 1935 Western swing big band leader Bob Wills had added drums to the Texas Playboys. In the mid-1940s, the Grand Ole Opry did not want the Playboys' drummer to appear on stage. Although drums were commonly used by rockabilly groups by 1955, the less-conservative-than-the-Grand-Ole-Opry \"Louisiana Hayride\" kept its infrequently used drummer back stage as late as 1956. By the early 1960s, however, it was rare that a country band didn't have a drummer. Bob Wills was one of the first country musicians known to have added an electric guitar to his band, in 1938. A decade later (1948) Arthur Smith achieved top 10 US country chart success with his MGM Records recording of \"Guitar Boogie\", which crossed over to the US pop chart, introducing many people to the potential of the electric guitar. For several decades Nashville session players preferred the warm tones of the Gibson and Gretsch archtop electrics, but a \"hot\" Fender style, using guitars which became available beginning in the early 1950s, eventually prevailed as the signature guitar sound of country.\n\nCountry musicians began recording boogie in 1939, shortly after it had been played at Carnegie Hall, when Johnny Barfield recorded \"Boogie Woogie\". The trickle of what was initially called hillbilly boogie, or okie boogie (later to be renamed country boogie), became a flood beginning in late 1945. One notable release from this period was The Delmore Brothers' \"Freight Train Boogie\", considered to be part of the combined evolution of country music and blues towards rockabilly. In 1948, Arthur \"Guitar Boogie\" Smith achieved top ten US country chart success with his MGM Records recordings of \"Guitar Boogie\" and \"Banjo Boogie\", with the former crossing over to the US pop charts. Other country boogie artists included Moon Mullican, Merrill Moore and Tennessee Ernie Ford. The hillbilly boogie period lasted into the 1950s and remains one of many subgenres of country into the 21st century.\n\nBy the end of World War II, \"mountaineer\" string band music known as bluegrass had emerged when Bill Monroe joined with Lester Flatt and Earl Scruggs, introduced by Roy Acuff at the Grand Ole Opry. Gospel music, too, remained a popular component of bluegrass and other sorts of country music. Red Foley, the biggest country star following World War II, had one of the first million-selling gospel hits (\"Peace in the Valley\") and also sang boogie, blues and rockabilly. In the post-war period, country music was called \"folk\" in the trades, and \"hillbilly\" within the industry. In 1944, \"The Billboard\" replaced the term \"hillbilly\" with \"folk songs and blues,\" and switched to \"country\" or \"country and Western\" in 1949.\n\nAnother type of stripped down and raw music with a variety of moods and a basic ensemble of guitar, bass, dobro or steel guitar (and later) drums became popular, especially among poor whites in Texas and Oklahoma. It became known as honky tonk and had its roots in Western swing and the ranchera music of Mexico and the border states, particularly Texas, together with the blues of the American South. Bob Wills and His Texas Playboys personified this music which has been described as \"a little bit of this, and a little bit of that, a little bit of black and a little bit of white ... just loud enough to keep you from thinking too much and to go right on ordering the whiskey.\" East Texan Al Dexter had a hit with \"Honky Tonk Blues\", and seven years later \"Pistol Packin' Mama\". These \"honky tonk\" songs associated barrooms, were performed by the likes of Ernest Tubb, Kitty Wells (the first major female country solo singer), Ted Daffan, Floyd Tillman, and the Maddox Brothers and Rose, Lefty Frizzell and Hank Williams, would later be called \"traditional\" country. Williams' influence in particular would prove to be enormous, inspiring many of the pioneers of rock and roll, such as Elvis Presley and Jerry Lee Lewis, as well as Chuck Berry and Ike Turner, while providing a framework for emerging honky tonk talents like George Jones. Webb Pierce was the top-charting country artist of the 1950s, with 13 of his singles spending 113 weeks at number one. He charted 48 singles during the decade; 31 reached the top ten and 26 reached the top four.\n\nBy the early 1950s a blend of Western swing, country boogie, and honky tonk was played by most country bands. Western music, influenced by the cowboy ballads and Tejano music rhythms of the southwestern U.S. and northern Mexico, reached its peak in popularity in the late 1950s, most notably with the song \"El Paso\", first recorded by Marty Robbins in September 1959. The country music scene largely kept the music of the folk revival and folk rock at a distance, despite the similarity in instrumentation and origins (see, for instance, The Byrds' negative reception during their appearance on the \"Grand Ole Opry\"). The main concern was politics: the folk revival was largely driven by progressive activists, a stark contrast to the culturally conservative audiences of country music. Only a handful of folk artists, such as Burl Ives, John Denver and Canadian musician Gordon Lightfoot, would cross over into country music after the folk revival died out. During the mid-1950s a new style of country music became popular, eventually to be referred to as rockabilly.\n\nRockabilly was most popular with country fans in the 1950s, and 1956 could be called the year of rockabilly in country music. Rockabilly was an early form of rock and roll, an upbeat combination of blues and country music. The number two, three and four songs on \"Billboard's\" charts for that year were Elvis Presley, \"Heartbreak Hotel\"; Johnny Cash, \"I Walk the Line\"; and Carl Perkins, \"Blue Suede Shoes\" Thumper Jones (George Jones) Cash and Presley placed songs in the top 5 in 1958 with No. 3 \"Guess Things Happen That Way/Come In, Stranger\" by Cash, and No. 5 by Presley \"Don't/I Beg of You.\" Presley acknowledged the influence of rhythm and blues artists and his style, saying \"The colored folk been singin' and playin' it just the way I'm doin' it now, man for more years than I know.\" But he also said, \"My stuff is just hopped-up country.\" Within a few years, many rockabilly musicians returned to a more mainstream style or had defined their own unique style.\n\nCountry music gained national television exposure through \"Ozark Jubilee\" on ABC-TV and radio from 1955 to 1960 from Springfield, Missouri. The program showcased top stars including several rockabilly artists, some from the Ozarks. As Webb Pierce put it in 1956, \"Once upon a time, it was almost impossible to sell country music in a place like New York City. Nowadays, television takes us everywhere, and country music records and sheet music sell as well in large cities as anywhere else.\" The late 1950s saw the emergence of Buddy Holly, but by the end of the decade, backlash as well as traditional artists such as Ray Price, Marty Robbins, and Johnny Horton began to shift the industry away from the rock n' roll influences of the mid-1950s.\n\nBeginning in the mid-1950s, and reaching its peak during the early 1960s, the Nashville sound turned country music into a multimillion-dollar industry centered in Nashville, Tennessee. Under the direction of producers such as Chet Atkins, Paul Cohen, Owen Bradley, Bob Ferguson, and later Billy Sherrill, the sound brought country music to a diverse audience and helped revive country as it emerged from a commercially fallow period. This subgenre was notable for borrowing from 1950s pop stylings: a prominent and smooth vocal, backed by a string section (violins and other orchestral strings) and vocal chorus. Instrumental soloing was de-emphasized in favor of trademark \"licks\". Leading artists in this genre included Jim Reeves, Skeeter Davis, Connie Smith, The Browns, Patsy Cline, and Eddy Arnold. The \"slip note\" piano style of session musician Floyd Cramer was an important component of this style. Nashville's pop song structure became more pronounced and it morphed into what was called countrypolitan. Countrypolitan was aimed straight at mainstream markets, and it sold well throughout the later 1960s into the early 1970s (a rarity in an era where American popular music was being decimated by the British Invasion). Top artists included Tammy Wynette, Lynn Anderson and Charlie Rich, as well as such former \"hard country\" artists as Ray Price and Marty Robbins. Despite the appeal of the Nashville sound, many traditional country artists emerged during this period and dominated the genre: Loretta Lynn, Merle Haggard, Buck Owens, Porter Wagoner, George Jones, and Sonny James among them.\n\nIn 1962, Ray Charles surprised the pop world by turning his attention to country and western music, topping the charts and rating number three for the year on \"Billboard's\" pop chart with the \"I Can't Stop Loving You\" single, and recording the landmark album \"Modern Sounds in Country and Western Music\".\n\nAnother genre of country music grew out of hardcore honky tonk with elements of Western swing and originated north-northwest of Los Angeles in Bakersfield, California. Influenced by one-time West Coast residents Bob Wills and Lefty Frizzell, by 1966 it was known as the Bakersfield sound. It relied on electric instruments and amplification, in particular the Telecaster electric guitar, more than other subgenres of country of the era, and can be described as having a sharp, hard, driving, no-frills, edgy flavor. Leading practitioners of this style were Buck Owens, Merle Haggard, Tommy Collins, Gary Allan, and Wynn Stewart, each of whom had his own style.\n\nBy the late 1960s, Western music, in particular the cowboy ballad, was in decline. Relegated to the \"country and Western\" genre by marketing agencies, popular Western recording stars released albums to only moderate success. Rock-and-roll artists got hit songs, but Western artists also got country hits. The latter was largely limited to Buck Owens, Merle Haggard, and a few other bands. In the process, country and western music as a genre lost most of its southwestern, ranchera, and Tejano musical influences. However the cowboy ballad and honky-tonk music would be resurrected and reinterpreted in the 1970s with the growth in popularity of \"outlaw country\" music from Texas and Oklahoma.\n\nDerived from the traditional Western and honky tonk musical styles of the late 1950s and 1960s, including Ray Price (whose band, the \"Cherokee Cowboys\", included Willie Nelson and Roger Miller) and mixed with the anger of an alienated subculture of the nation during the period, outlaw country revolutionized the genre of country music. \"After I left Nashville (the early 70s), I wanted to relax and play the music that I wanted to play, and just stay around Texas, maybe Oklahoma. Waylon and I had that outlaw image going, and when it caught on at colleges and we started selling records, we were O.K. The whole outlaw thing, it had nothing to do with the music, it was something that got written in an article, and the young people said, 'Well, that's pretty cool.' And started listening.\" (Willie Nelson) The term \"outlaw country\" is traditionally associated with Willie Nelson, Jerry Jeff Walker, Hank Williams, Jr., Merle Haggard, Waylon Jennings, Joe Ely, Steve Young, David Allan Coe, Whitey Morgan and the 78's, John Prine, Billy Joe Shaver, Gary Stewart, Townes Van Zandt, Kris Kristofferson, Michael Martin Murphey, Tompall Glaster, Steve Earle, and the later career renaissance of Johnny Cash, with a few female vocalists such as Jessi Colter, Sammi Smith, Tanya Tucker and Rosanne Cash. It was encapsulated in the 1976 album \"Wanted! The Outlaws\". A related subgenre is Red Dirt.\n\nCountry pop or soft pop, with roots in the countrypolitan sound, folk music, and soft rock, is a subgenre that first emerged in the 1970s. Although the term first referred to country music songs and artists that crossed over to top 40 radio, country pop acts are now more likely to cross over to adult contemporary music. It started with pop music singers like Glen Campbell, Bobbie Gentry, John Denver, Olivia Newton-John, Anne Murray, B. J. Thomas, The Bellamy Brothers, and Linda Ronstadt having hits on the country charts. Between 1972 and 1975, singer/guitarist John Denver released a series of hugely successful songs blending country and folk-rock musical styles (\"Rocky Mountain High\", \"Sunshine on My Shoulders\", \"Annie's Song\", \"Thank God I'm a Country Boy\", and \"I'm Sorry\"), and was named Country Music Entertainer of the Year in 1975. The year before, Olivia Newton-John, an Australian pop singer, won the \"Best Female Country Vocal Performance\" as well as the Country Music Association's most coveted award for females, \"Female Vocalist of the Year\". In response George Jones, Tammy Wynette, Jean Shepard and other traditional Nashville country artists dissatisfied with the new trend formed the short-lived Association of Country Entertainers in 1974; the ACE soon unraveled in the wake of Jones and Wynette's bitter divorce and Shepard's realization that most others in the industry lacked her passion for the movement.\nDuring the mid-1970s, Dolly Parton, a successful mainstream country artist since the late 1960s, mounted a high-profile campaign to cross over to pop music, culminating in her 1977 hit \"Here You Come Again\", which topped the U.S. country singles chart, and also reached No. 3 on the pop singles charts. Parton's male counterpart, Kenny Rogers, came from the opposite direction, aiming his music at the country charts, after a successful career in pop, rock and folk music with The First Edition, achieving success the same year with \"Lucille\", which topped the country charts and reached No. 5 on the U.S. pop singles charts, as well as reaching Number 1 on the British all-genre chart. Parton and Rogers would both continue to have success on both country and pop charts simultaneously, well into the 1980s. Artists like Crystal Gayle, Ronnie Milsap and Barbara Mandrell would also find success on the pop charts with their records. In 1975, author Paul Hemphill stated in the \"Saturday Evening Post\", \"Country music isn't really country anymore; it is a hybrid of nearly every form of popular music in America.\"\n\nDuring the early 1980s, country artists continued to see their records perform well on the pop charts. Willie Nelson and Juice Newton each had two songs in the top 5 of the Billboard Hot 100 in the early eighties: Nelson charted \"Always on My Mind\" (No. 5, 1982) and \"To All the Girls I've Loved Before\" (No. 5, 1984, a duet with Julio Iglesias), and Newton achieved success with \"Queen of Hearts\" (No. 2, 1981) and \"Angel of the Morning\" (No. 4, 1981). Four country songs topped the \"Billboard\" Hot 100 in the 1980s: \"Lady\" by Kenny Rogers, from the late fall of 1980; \"9 to 5\" by Dolly Parton, \"I Love a Rainy Night\" by Eddie Rabbitt (these two back-to-back at the top in early 1981); and \"Islands in the Stream\", a duet by Dolly Parton and Kenny Rogers in 1983, a pop-country crossover hit written by Barry, Robin, and Maurice Gibb of the Bee Gees. Newton's \"Queen of Hearts\" almost reached No. 1, but was kept out of the spot by the pop ballad juggernaut \"Endless Love\" by Diana Ross and Lionel Richie. The move of country music toward neotraditional styles led to a marked decline in country/pop crossovers in the late 1980s, and only one song in that period—Roy Orbison's \"You Got It\", from 1989—made the top 10 of both the \"Billboard\" Hot Country Singles\" and Hot 100 charts, due largely to a revival of interest in Orbison after his sudden death. The record-setting, multi-platinum group Alabama was named Artist of the Decade for the 1980s by the Academy of Country Music.\n\nCountry rock is a genre that started in the 1960s but became prominent in the 1970s. The late 1960s in American music produced a unique blend as a result of traditionalist backlash within separate genres. In the aftermath of the British Invasion, many desired a return to the \"old values\" of rock n' roll. At the same time there was a lack of enthusiasm in the country sector for Nashville-produced music. What resulted was a crossbred genre known as country rock. Early innovators in this new style of music in the 1960s and 1970s included Bob Dylan, who was the first to revert to country music with his 1967 album \"John Wesley Harding\" (and even more so with that album's follow-up, \"Nashville Skyline\"), followed by Gene Clark, Clark's former band The Byrds (with Gram Parsons on \"Sweetheart of the Rodeo\") and its spin-off The Flying Burrito Brothers (also featuring Gram Parsons), guitarist Clarence White, Michael Nesmith (The Monkees and the First National Band), the Grateful Dead, Neil Young, Commander Cody, The Allman Brothers, The Marshall Tucker Band, Poco, Buffalo Springfield, and Eagles, among many, even the former folk music duo Ian & Sylvia, who formed Great Speckled Bird in 1969. The Eagles would become the most successful of these country rock acts, and their compilation album \"Their Greatest Hits (1971–1975)\" remains the second best-selling album of all time in the US with 29 million copies sold. The Rolling Stones also got into the act with songs like \"Dead Flowers\" and a country version of \"Honky Tonk Women\".\n\nDescribed by AllMusic as the \"father of country-rock\", Gram Parsons' work in the early 1970s was acclaimed for its purity and for his appreciation for aspects of traditional country music. Though his career was cut tragically short by his 1973 death, his legacy was carried on by his protégé and duet partner Emmylou Harris; Harris would release her debut solo in 1975, an amalgamation of country, rock and roll, folk, blues and pop. Subsequent to the initial blending of the two polar opposite genres, other offspring soon resulted, including Southern rock, heartland rock and in more recent years, alternative country. In the decades that followed, artists such as Juice Newton, Alabama, Hank Williams, Jr. (and, to an even greater extent, Hank Williams III), Gary Allan, Shania Twain, Brooks & Dunn, Faith Hill, Garth Brooks, Alan Jackson, Dwight Yoakam, Steve Earle, Dolly Parton, Rosanne Cash and Linda Ronstadt moved country further towards rock influence.\n\nIn 1980, a style of \"neocountry disco music\" was popularized by the film \"Urban Cowboy\", which also included more traditional songs such as \"The Devil Went Down to Georgia\" by the Charlie Daniels Band. It was during this time that a glut of pop-country crossover artists began appearing on the country charts: former pop stars Bill Medley (of The Righteous Brothers), \"England Dan\" Seals (of England Dan and John Ford Coley), Tom Jones, and Merrill Osmond (both alone and with some of his brothers; his younger sister Marie Osmond was already an established country star) all recorded significant country hits in the early 1980s. Sales in record stores rocketed to $250 million in 1981; by 1984, 900 radio stations began programming country or neocountry pop full-time. As with most sudden trends, however, by 1984 sales had dropped below 1979 figures.\n\nTruck driving country music is a genre of country music\nand is a fusion of honky-tonk, country rock and the Bakersfield sound.\nIt has the tempo of country rock and the emotion of honky-tonk, and its lyrics focus on a truck driver's lifestyle. Truck driving country songs often deal with the profession of trucking and love. Well-known artists who sing truck driving country include Dave Dudley, Red Sovine, Dick Curless, Red Simpson, Del Reeves, The Willis Brothers and Jerry Reed, with C. W. McCall and Cledus Maggard (pseudonyms of Bill Fries and Jay Huguely, respectively) being more humorous entries in the subgenre. Dudley is known as the father of truck driving country.\n\nDuring the mid-1980s, a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts, in favor of more, traditional, \"back-to-basics\" production. Many of the artists during the latter half of the 1980s drew on traditional honky-tonk, bluegrass, folk and western swing. Artists who typified this sound included Travis Tritt, Reba McEntire, George Strait, Keith Whitley, Alan Jackson, Ricky Skaggs, Patty Loveless, Kathy Mattea, Randy Travis, Dwight Yoakam, and The Judds. Beginning in 1989, a confluence of events brought an unprecedented commercial boom to country music. The arrival of exceptionally talented artists coincided with new marketing strategies to engage fans, technology that more accurately tracked the popularity of country music, and a political and economic climate that focused attention on the genre. Garth Brooks (\"Friends in Low Places\") in particular attracted fans with his fusion of neotraditionalist country and stadium rock. His stadium concerts promised the same quality of special effects that fans expected from rock stars, while his music drew equally from George Strait and Journey. Other artists such as Brooks and Dunn (\"Boot Scootin' Boogie\") also combined conventional country with slick, rock elements, while Lorrie Morgan, Mary Chapin Carpenter, and Kathy Mattea updated neotraditionalist styles.\n\nCountry music was aided by the U.S. Federal Communications Commission's (FCC) Docket 80–90, which led to a significant expansion of FM radio in the 1980s by adding numerous higher-fidelity FM signals to rural and suburban areas. At this point, country music was mainly heard on rural AM radio stations; the expansion of FM was particularly helpful to country music, which migrated to FM from the AM band as AM became overcome by talk radio (the country music stations that stayed on AM developed the classic country format for the AM audience). At the same time, beautiful music stations already in rural areas began abandoning the format (leading to its effective demise) to adopt country music as well. This wider availability of country music led to producers seeking to polish their product for a wider audience. Another force leading to changes in the country music industry was the changing sound of rock music, which was increasingly being influenced by the noisier, less melodic alternative rock scene. \"New country\" ended up absorbing rock influence from more electric musicians that were too melodic for modern rock but too electric for the classic country music sound. (A number of \"classic rock\" artists, especially Southern rock ones such as Charlie Daniels and Lynyrd Skynyrd, are more closely associated with the modern country music scene than that of the modern rock scene.)\n\nIn the 1990s, country music became a worldwide phenomenon thanks to Garth Brooks, who enjoyed one of the most successful careers in popular music history, breaking records for both sales and concert attendance throughout the decade. The RIAA has certified his recordings at a combined (128× platinum), denoting roughly 113 million U.S. shipments. Other artists that experienced success during this time included Clint Black, Sammy Kershaw, Aaron Tippin, Travis Tritt, Alan Jackson and the newly formed duo of Brooks & Dunn; George Strait, whose career began in the 1980s, also continued to have widespread success in this decade and beyond. Toby Keith began his career as a more pop-oriented country singer in the 1990s, evolving into an outlaw persona in the late 1990s with \"Pull My Chain\" and its follow-up, \"Unleashed\".\n\nFemale artists such as Reba McEntire, Patty Loveless, Faith Hill, Martina McBride, Deana Carter, LeAnn Rimes, Mindy McCready, Lorrie Morgan, Shania Twain, and Mary Chapin Carpenter all released platinum-selling albums in the 1990s. The Dixie Chicks became one of the most popular country bands in the 1990s and early 2000s. Their 1998 debut album \"Wide Open Spaces\" went on to become certified 12x platinum while their 1999 album \"Fly\" went on to become 10x platinum. After their third album, \"Home\", was released in 2003, the band made political news in part because of lead singer Natalie Maines's comments disparaging then-President George W. Bush while the band was overseas (Maines stated that she and her bandmates were ashamed to be from the same state as Bush, who had just commenced the Iraq War a few days prior). The comments caused a rift between the band and the country music scene, and the band's fourth (and most recent) album, 2006's \"Taking the Long Way\", took a more rock-oriented direction; the album was commercially successful overall but largely ignored among country audiences. After \"Taking the Long Way\", the band broke up for a decade (with two of its members continuing as the Court Yard Hounds) before embarking on a reunion tour in 2016.\n\nShania Twain became the best selling female country artist of the decade. This was primarily due to the success of her breakthrough sophomore 1995 album, \"The Woman in Me\", which was certified 12x platinum sold over 20 million copies worldwide and its follow up, 1997's \"Come On Over\", which was certified 20x platinum and sold over 40 million copies. The album became a major worldwide phenomenon and became one of the world's best selling albums of 1998, 1999 and 2000; it also went on to become the best selling country album of all time. Unlike the majority of her contemporaries, Twain enjoyed large international success that had been seen by very few country artists, before or after her. Critics have noted that Twain enjoyed much of her success due to breaking free of traditional country stereotypes and for incorporating elements of rock and pop into her music. In 2002, she released her successful fourth studio album, titled \"Up!\", which was certified 11x platinum and sold over 15 million copies worldwide. Twain has been credited with breaking international boundaries for country music, as well as inspiring many country artists to incorporate different genres into their music in order to attract a wider audience. She is also credited with changing the way in which many female country performers would market themselves, as unlike many before her she used fashion and her sex appeal to get rid of the stereotypical 'honky-tonk' image the majority of country singers had in order to distinguish herself from many female country artists of the time.\n\nIn the early-mid-1990s, country western music was influenced by the popularity of line dancing. This influence was so great that Chet Atkins was quoted as saying, \"The music has gotten pretty bad, I think. It's all that damn line dancing.\" By the end of the decade, however, at least one line dance choreographer complained that good country line dance music was no longer being released. In contrast, artists such as Don Williams and George Jones who had more or less had consistent chart success through the 1970s and 1980s suddenly had their fortunes fall rapidly around 1991 as these new artists rose to prominence.\n\nThe musical combination of punk, alternative rock and country was pioneered by the \"cowpunk\" scene in Southern California during the 1980s, which included bands such as The Long Ryders, Lone Justice and The Beat Farmers, as well as the established punk group X, whose music had begun to include country and rockabilly influences. Other artists from outside California who were associated with early alternative country included singer-songwriters such as Lucinda Williams, Lyle Lovett and Steve Earle, the Nashville country rock band Jason and the Scorchers and the British post-punk band The Mekons. Earle, in particular, was noted for his popularity with both country and college rock audiences: He promoted his 1986 debut album \"Guitar Town\" with a tour that saw him open for both country singer Dwight Yoakam and alternative rock band The Replacements.\n\nThese early styles had coalesced into a genre by the time the Illinois group Uncle Tupelo released their influential debut album \"No Depression\" in 1990. The album is widely credited as being the first \"alternative country\" album, and inspired the name of \"No Depression\" magazine, which exclusively covered the new genre. Following Uncle Tupelo's disbanding in 1994, its members formed two significant bands in genre: Wilco and Son Volt. Other acts who became prominent in the genre during the 1990s and 2000s included The Bottle Rockets, The Handsome Family, Blue Mountain, Robbie Fulks, Blood Oranges, Bright Eyes, Drive-By Truckers, Old 97's and Whiskeytown, whose lead singer Ryan Adams later had a successful solo-career. Some alt-country songs have been crossover hits to mainstream country radio, including Lucinda Williams' \"Passionate Kisses\", which was a hit for Mary Chapin Carpenter in 1993, and Ryan Adams's \"When The Stars Go Blue,\" which was a hit for Tim McGraw in 2007.\n\nThe sixth generation of the country continued the crossover between country and pop music. Richard Marx crossed over with his \"Days in Avalon\" album, which features five country songs and several singers and musicians. Alison Krauss sang background vocals to Marx's single \"Straight from My Heart.\" Also, Bon Jovi had a hit single, \"Who Says You Can't Go Home\", with Jennifer Nettles of Sugarland. Kid Rock's collaboration with Sheryl Crow, \"Picture,\" was a major crossover hit in 2001 and began Kid Rock's transition from hard rock to a country-rock hybrid that would later produce another major crossover hit, 2008's \"All Summer Long.\" (Crow would also cross over into country with her hit \"Easy.\") Darius Rucker, former frontman for the 1990s pop-rock band Hootie & the Blowfish, began a country solo career in the late 2000s, one that to date has produced three albums and several hits on both the country charts and the Billboard Hot 100. Singer-songwriter Unknown Hinson became famous for his appearance in the Charlotte television show \"Wild, Wild, South\", after which Hinson started his own band and toured in southern states. Other rock stars who featured a country song on their albums were Don Henley and Poison.\nIn 2005, country singer Carrie Underwood rose to fame as the winner of the fourth season of \"American Idol\" and has since become one of the most prominent recording artists of 2006 through 2016, with worldwide sales of more than 65 million records and seven Grammy Awards. With her first single, \"Inside Your Heaven\", Underwood became the only solo country artist to have a #1 hit on the \"Billboard\" Hot 100 chart in the 2000–2009 decade and also broke \"Billboard\" chart history as the first country music artist ever to debut at No. 1 on the Hot 100. Underwood's debut album, \"Some Hearts\", became the best-selling solo female debut album in country music history, the fastest-selling debut country album in the history of the SoundScan era and the best-selling country album of the last 10 years, being ranked by \"Billboard\" as the #1 Country Album of the 2000–2009 decade. She has also become the female country artist with the most number one hits on the \"Billboard\" Hot Country Songs chart in the Nielsen SoundScan era (1991–present), having 14 No. 1s and breaking her own \"Guinness Book\" record of ten. In 2007, Underwood won the Grammy Award for Best New Artist, becoming only the second Country artist in history (and the first in a decade) to win it. She also made history by becoming the seventh woman to win Entertainer of the Year at the Academy of Country Music Awards, and the first woman in history to win the award twice, as well as twice consecutively. \"Time\" has listed Underwood as one of the 100 most influential people in the world.\nIn 2016, Underwood topped the Country Airplay chart for the 15th time, becoming the female artist with most number ones on that chart.\n\nCarrie Underwood was one of several country stars produced by a television series in the 2000s. In addition to Underwood, \"American Idol\" launched the careers of Kellie Pickler, Josh Gracin, Bucky Covington, Kristy Lee Cook, Danny Gokey and Scotty McCreery (as well as that of occasional country singer Kelly Clarkson) in the decade, and would continue to launch country careers in the 2010s. The series \"Nashville Star\", while not nearly as successful as \"Idol\", did manage to bring Miranda Lambert, Kacey Musgraves and Chris Young to mainstream success, also launching the careers of lower-profile musicians such as Buddy Jewell, Sean Patrick McGraw, and Canadian musician George Canyon. \"Can You Duet?\" produced the duos Steel Magnolia and Joey + Rory. Teen sitcoms also have influenced modern country music; in 2008, actress Jennette McCurdy (best known as the sidekick Sam on the teen sitcom \"iCarly\") released her first single, \"So Close\", following that with the single \"Generation Love\" in 2011. Another teen sitcom star, Miley Cyrus (of \"Hannah Montana\"), also had a crossover hit in the late 2000s with \"The Climb\" and another with a duet with her father, Billy Ray Cyrus, with \"Ready, Set, Don't Go.\" Jana Kramer, an actress in the teen drama \"One Tree Hill\", released a country album in 2012 that has produced two hit singles as of 2013. Actresses Hayden Panettiere and Connie Britton began recording country songs as part of her role in the TV series \"Nashville\".\n\nIn 2010, the group Lady Antebellum won five Grammys, including the coveted Song of the Year and Record of the Year for \"Need You Now, a UK number 15 hit on the mainstream singles chart, a rarity for a country song these days \". A large number of duos and vocal groups emerged on the charts in the 2010s, many of which feature close harmony in the lead vocals. In addition to Lady Antebellum, groups such as The Quebe Sisters Band, Little Big Town, The Band Perry, Gloriana, Thompson Square, Eli Young Band, Zac Brown Band and British duo The Shires have emerged to occupy a large portion of the new country artists in the popular scene along with solo singers Kacey Musgraves and Miranda Lambert.\nOne of the most commercially successful country artists of the late 2000s and early 2010s has been singer-songwriter Taylor Swift. Swift first became widely known in 2006 when her debut single, \"Tim McGraw,\" was released when Swift was only 16. In 2006, Taylor released her first studio album, \"Taylor Swift\", which spent 275 weeks on Billboard 200, one of the longest runs of any album on that chart. In 2008, Taylor Swift released her second studio album, \"Fearless\", which made her the second-longest Number One charted on Billboard 200 and the second best-selling album (just behind Adele's \"21\") within the past 5 years. At the 2010 Grammys, Taylor Swift was 20 and won Album of the Year for \"Fearless\", which made her the youngest artist to win this award. Swift has received ten Grammys already. Buoyed by her teen idol status among girls and a change in the methodology of compiling the \"Billboard\" charts to favor pop-crossover songs, Swift's 2012 single \"We Are Never Ever Getting Back Together\" spent the most weeks at the top of Billboard's Hot Country Songs chart of any song in nearly five decades. The song's long run at the top of the chart was somewhat controversial, as the song was largely a pop song without much country influence and its success on the charts driven by a change to the chart's criteria to include airplay on non-country radio stations, prompting disputes over what constitutes a country song; many of Swift's later releases, such as \"Shake It Off,\" were released solely to pop audiences.\n\nThe September 11 attacks of 2001 and the economic recession helped move country music back into the spotlight. Many country artists, such as Alan Jackson with his ballad on terrorist attacks, \"Where Were You (When the World Stopped Turning)\", wrote songs that celebrated the military, highlighted the gospel, and emphasized home and family values over wealth. Alt-Country singer Ryan Adams song \"New York, New York\" pays tribute to New York City, and it's popular music video (which was shot 4 days before the attacks) shows Adams playing in front of the Manhattan skyline, Along with several shots of the city. In contrast, more rock-oriented country singers took more direct aim at the attacks' perpetrators; Toby Keith's \"The Angry American (Courtesy of the Red, White and Blue)\" threatened to \"a boot in\" the posterior of the enemy, while Charlie Daniels's \"This Ain't No Rag, It's a Flag\" promised to \"hunt\" the perpetrators \"down like a mad dog hound.\" These songs gained such recognition that it put Country music back into popular culture. The influence of rock music in country has become more overt during the late 2000s and early 2010s as artists like Eric Church, Jason Aldean, and Brantley Gilbert have had success; Aaron Lewis, former frontman for the rock group Staind, had a moderately successful entry into country music in 2011 and 2012. Also rising in the late 2000s and early 2010s was the insertion of rap and spoken-word elements into country songs; artists such as Cowboy Troy and Colt Ford have focused almost exclusively on country rap (also known as hick hop) while other, more mainstream artists (such as Big & Rich and Jason Aldean) have used it on occasion.\n\nIn the 2010s, Bro-country, a genre noted primarily for its themes on drinking and partying, girls, and pickup trucks became particularly popular. Notable artists associated with this genre are Luke Bryan, Jason Aldean, Blake Shelton, and Florida Georgia Line whose song \"Cruise\" became the best-selling country song of all time. Research in the mid-2010s suggested that about 45 percent of country's best-selling songs could be considered bro-country, with the top two artists being Luke Bryan and Florida Georgia Line. Albums by bro-country singers also sold very well—in 2013, Luke Bryan's \"Crash My Party\" was the third best-selling of all albums in the US, with Florida Georgia Line's \"Here's to the Good Times\" at sixth, and Blake Shelton's \"Based on a True Story\" at ninth. It is also thought that the popularity of bro-country helped country music to surpass classic rock as the most popular genre in America in 2012. The genre however is controversial as it has been criticized by other country musicians and commentators over its themes and depiction of women, opening up a divide between the older generation of country singers and the younger bro country singers that was described as \"civil war\" by musicians, critics, and journalists.\" In 2014, Maddie & Tae's \"Girl in a Country Song\", addressing many of the controversial bro-country themes, peaked at number one on the \"Billboard\" Country Airplay chart.\n\nOutside of the United States, Canada has the largest country music fan and artist base, something that is to be expected given the two countries' proximity and cultural parallels. Mainstream country music is culturally ingrained in the prairie provinces, Ontario, and in Atlantic Canada. Celtic traditional music developed in Atlantic Canada in the form of Scottish, Acadian and Irish folk music popular amongst Irish, French and Scottish immigrants to Canada's Atlantic Provinces (Newfoundland, Nova Scotia, New Brunswick, and Prince Edward Island). Like the southern United States and Appalachia, all four regions are of heavy British Isles stock and rural; as such, the development of traditional music in the Maritimes somewhat mirrored the development of country music in the US South and Appalachia. Country and Western music never really developed separately in Canada; however, after its introduction to Canada, following the spread of radio, it developed quite quickly out of the Atlantic Canadian traditional scene. While true Atlantic Canadian traditional music is very Celtic or \"sea shanty\" in nature, even today, the lines have often been blurred. Certain areas often are viewed as embracing one strain or the other more openly. For example, in Newfoundland the traditional music remains unique and Irish in nature, whereas traditional musicians in other parts of the region may play both genres interchangeably.\n\n\"Don Messer's Jubilee\" was a Halifax, Nova Scotia-based country/folk variety television show that was broadcast nationally from 1957 to 1969. In Canada it out-performed \"The Ed Sullivan Show\" broadcast from the United States and became the top-rated television show throughout much of the 1960s. \"Don Messer's Jubilee\" followed a consistent format throughout its years, beginning with a tune named \"Goin' to the Barndance Tonight\", followed by fiddle tunes by Messer, songs from some of his \"Islanders\" including singers Marg Osburne and Charlie Chamberlain, the featured guest performance, and a closing hymn. It ended with \"Till We Meet Again\".\n\nThe guest performance slot gave national exposure to numerous Canadian folk musicians, including Stompin' Tom Connors and Catherine McKinnon. Some Maritime country performers went on to further fame beyond Canada. Hank Snow, Wilf Carter (also known as Montana Slim), and Anne Murray are the three most notable.\n\nThe cancellation of the show by the public broadcaster in 1969 caused a nationwide protest, including the raising of questions in the Parliament of Canada.\n\nThe Prairie provinces, due to their western cowboy and agrarian nature, are the true heartland of Canadian country music. While the Prairies never developed a traditional music culture anything like the Maritimes, the folk music of the Prairies often reflected the cultural origins of the settlers, who were a mix of Scottish, Ukrainian, German and others. For these reasons polkas and Western music were always popular in the region, and with the introduction of the radio, mainstream country music flourished. As the culture of the region is western and frontier in nature, the specific genre of country and western is more popular today in the Prairies than in any other part of the country. No other area of the country embraces all aspects of the culture, from two-step dancing, to the cowboy dress, to rodeos, to the music itself, like the Prairies do. The Atlantic Provinces, on the other hand, produce far more traditional musicians, but they are not usually specifically country in nature, usually bordering more on the folk or Celtic genres.\n\nMany traditional country artists are present in eastern and western Canada. They make common use of fiddle and pedal steel guitar styles. Some notable Canadian country artists include Shania Twain, Anne Murray, k.d. lang, Gordon Lightfoot, Buffy Sainte-Marie, George Canyon, Blue Rodeo, Tommy Hunter, Rita MacNeil, Stompin' Tom Connors, Stan Rogers, Ronnie Prophet, Carroll Baker, The Rankin Family, Ian Tyson, Johnny Reid, Paul Brandt, Jason McCoy, George Fox, Carolyn Dawn Johnson, Hank Snow, Don Messer, Wilf Carter, Michelle Wright, Terri Clark, Prairie Oyster, Family Brown, Johnny Mooring, Marg Osburne, Doc Walker, Emerson Drive, The Wilkinsons, Corb Lund and the Hurtin' Albertans, Crystal Shawanda, Dean Brody, Shane Yellowbird, Gord Bamford, Chad Brownlee, The Road Hammers, Rowdy Spurs and The Higgins.\n\nAustralian country music has a long tradition. Influenced by American country music, it has developed a distinct style, shaped by British and Irish folk ballads and Australian bush balladeers like Henry Lawson and Banjo Paterson. Country instruments, including the guitar, banjo, fiddle and harmonica, create the distinctive sound of country music in Australia and accompany songs with strong storyline and memorable chorus.\n\nFolk songs sung in Australia between the 1780s and 1920s, based around such themes as the struggle against government tyranny, or the lives of bushrangers, swagmen, drovers, stockmen and shearers, continue to influence the genre. This strain of Australian country, with lyrics focusing on Australian subjects, is generally known as \"bush music\" or \"bush band music\". \"Waltzing Matilda\", often regarded as Australia's unofficial national anthem, is a quintessential Australian country song, influenced more by British and Irish folk ballads than by American country and western music. The lyrics were composed by the poet Banjo Paterson in 1895. Other popular songs from this tradition include \"The Wild Colonial Boy\", \"Click Go the Shears\", \"The Queensland Drover\" and \"The Dying Stockman\". Later themes which endure to the present include the experiences of war, of droughts and flooding rains, of Aboriginality and of the railways and trucking routes which link Australia's vast distances.\n\nPioneers of a more Americanised popular country music in Australia included Tex Morton (known as \"The Father of Australian Country Music\") in the 1930s. Author Andrew Smith delivers a through research and engaged view of Tex Morton's life and his impact on the country music scene in Australia in the 1930s and 1940s. Other early stars included Buddy Williams, Shirley Thoms and Smoky Dawson. Buddy Williams (1918–1986) was the first Australian-born to record country music in Australia in the late 1930s and was the pioneer of a distinctly Australian style of country music called the bush ballad that others such as Slim Dusty would make popular in later years. During World War II, many of Buddy Williams recording sessions were done whilst on leave from the Army. At the end of the war, Williams would go on to operate some of the largest travelling tent rodeo shows Australia has ever seen.\n\nIn 1952, Dawson began a radio show and went on to national stardom as a singing cowboy of radio, TV and film. Slim Dusty (1927–2003) was known as the \"King of Australian Country Music\" and helped to popularise the Australian bush ballad. His successful career spanned almost six decades, and his 1957 hit \"A Pub with No Beer\" was the biggest-selling record by an Australian to that time, and with over seven million record sales in Australia he is the most successful artist in Australian musical history. Dusty recorded and released his one-hundredth album in the year 2000 and was given the honour of singing \"Waltzing Matilda\" in the closing ceremony of the Sydney 2000 Olympic Games. Dusty's wife Joy McKean penned several of his most popular songs.\n\nChad Morgan, who began recording in the 1950s, has represented a vaudeville style of comic Australian country; Frank Ifield achieved considerable success in the early 1960s, especially in the UK Singles Charts, and Reg Lindsay was one of the first Australians to perform at Nashville's Grand Ole Opry in 1974. Eric Bogle's 1972 folk lament to the Gallipoli Campaign \"And the Band Played Waltzing Matilda\" recalled the British and Irish origins of Australian folk-country. Singer-songwriter Paul Kelly, whose music style straddles folk, rock, and country, is often described as the poet laureate of Australian music. \n\nBy the 1990s, country music had attained crossover success in the pop charts, with artists like James Blundell and James Reyne singing \"Way Out West\", and country star Kasey Chambers winning the ARIA for Best Female Artist in 2003. The crossover influence of Australian country is also evident in the music of successful contemporary bands The Waifs and the John Butler Trio. Nick Cave has been heavily influenced by the country artist Johnny Cash. In 2000, Cash, covered Cave's \"The Mercy Seat\" on the album \"\", seemingly repaying Cave for the compliment he paid by covering Cash's \"The Singer\" (originally \"The Folk Singer\") on his \"Kicking Against the Pricks\" album. Subsequently, Cave cut a duet with Cash on a version of Hank Williams' \"I'm So Lonesome I Could Cry\" for Cash's \"\" album (2002).\n\nPopular contemporary performers of Australian country music include John Williamson (who wrote the iconic \"True Blue\"), Lee Kernaghan (whose hits include \"Boys from the Bush\" and \"The Outback Club\"), Gina Jeffreys, Forever Road and Sara Storer. In the United States, Olivia Newton-John, Sherrié Austin and Keith Urban have attained great success.\n\nCountry music has been a particularly popular form of musical expression among Indigenous Australians. Troy Cassar-Daley is among Australia's successful contemporary indigenous performers, and Kev Carmody and Archie Roach employ a combination of folk-rock and country music to sing about Aboriginal rights issues.\n\nThe Tamworth Country Music Festival began in 1973 and now attracts up to 100,000 visitors annually. Held in Tamworth, New South Wales (country music capital of Australia), it celebrates the culture and heritage of Australian country music. During the festival the CMAA holds the Country Music Awards of Australia ceremony awarding the Golden Guitar trophies. Other significant country music festivals include the Whittlesea Country Music Festival (near Melbourne) and the Mildura Country Music Festival for \"independent\" performers during October, and the Canberra Country Music Festival held in the national capital during November.\n\n\"Country HQ\" showcases new talent on the rise in the country music scene down under. CMC (the Country Music Channel), a 24‑hour music channel dedicated to non-stop country music, can be viewed on pay TV and features once a year the Golden Guitar Awards, CMAs and CCMAs alongside international shows such as \"The Wilkinsons\", \"The Road Hammers\", and \"Country Music Across America\".\n\nCountry music is popular in the UK, although somewhat less so than in other English-speaking countries. There are some British country music acts and publications. Although radio stations devoted to country are among the most popular in other Anglophone nations, none of the top 10 most-listened-to stations in the UK are country stations, and national broadcaster BBC Radio does not offer a country station.\n\nThe most successful British country music act of the 21st Century are Ward Thomas and The Shires. In 2015, The Shires' album \"Brave\", became the first UK country act ever to chart in the Top 10 of the UK Albums Chart. In 2016, Ward Thomas then became the first UK country act to hit number 1 in the UK Albums Chart with their album \"Cartwheels\". \n\nThere is the festival held every year, and for many years there was a festival at Wembley Arena, which was broadcast on the BBC, the International Festivals of Country Music, promoted by Mervyn Conn, held at the venue between 1969 and 1991. The shows were later taken into Europe, and featured such stars as Johnny Cash, Dolly Parton, Tammy Wynette, David Allan Coe, Emmylou Harris, Boxcar Willie, Johnny Russell and Jerry Lee Lewis. A handful of country musicians had even greater success in mainstream UK music than they did in the US, despite a certain amount of disdain from the music press; Faron Young, Slim Whitman and (at least from number of top 40 pop singles) Garth Brooks are some examples. The UK's largest music festival Glastonbury has featured major US country acts in recent years, such as Kenny Rogers in 2013 and Dolly Parton in 2014.\n\nFrom within the UK, few country musicians achieved widespread mainstream success. Tom Jones, by this point near the end of his peak success as a pop singer, had a string of country hits in the late 1970s and early 1980s. The Bee Gees had some fleeting success in the genre, with one country hit as artists (\"Rest Your Love on Me\") and a major hit as songwriters (\"Islands in the Stream\"). Singer Engelbert Humperdinck, while charting only once in the U.S. country top 40 with \"After the Lovin',\" achieved widespread success on both the U.S. and UK pop charts with his faithful covers of Nashville country ballads such as \"Release Me,\" \"Am I That Easy to Forget\" and \"There Goes My Everything.\" The songwriting tandem of Roger Cook and Roger Greenaway wrote a number of country hits, in addition to their widespread success in pop songwriting; Cook is notable for being the only Briton to be inducted into the Nashville Songwriters Hall of Fame.\n\nTom Roland, from the Country Music Association International, explains country music's global popularity: \"In this respect, at least, Country Music listeners around the globe have something in common with those in the United States. In Germany, for instance, Rohrbach identifies three general groups that gravitate to the genre: people intrigued with the American cowboy icon, middle-aged fans who seek an alternative to harder rock music and younger listeners drawn to the pop-influenced sound that underscores many current Country hits.\" One of the first Americans to perform country music abroad was George Hamilton IV. He was the first country musician to perform in the Soviet Union; he also toured in Australia and the Middle East. He was deemed the \"International Ambassador of Country Music\" for his contributions to the globalization of country music. Johnny Cash, Emmylou Harris, Keith Urban, and Dwight Yoakam have also made numerous international tours. The Country Music Association undertakes various initiatives to promote country music internationally.\n\nIn Brazil, a musical genre known as música sertaneja, a very popular genre of music in Brazil, is very similar to American country music, sharing the music's rich history of development in the countryside. In South America, on the last weekend of September, the yearly San Pedro Country Music Festival takes place in the town of San Pedro, Argentina. The festival features bands from different places of Argentina, as well as international artists from Brazil, Uruguay, Chile, Peru and the United States.\n\nIn India, the Anglo-Indian community is well known for enjoying and performing country music. An annual concert festival called \"Blazing Guitars\" held in Chennai brings together Anglo-Indian musicians from all over the country (including some who have emigrated to places like Australia).\nThe year 2003 brought home - grown Indian, Bobby Cash to the forefront of the country music culture in India when he became India's first international country music artist to chart singles in Australia.\n\nIn Ireland TG4 began a quest for Ireland's next country star called \"Glór Tíre\", translated as \"Country Voice\". It is now in its sixth season and is one of TG4's most watched TV shows. Over the past ten years country and gospel recording artist James Kilbane has reached multi-platinum success with his mix of Christian and traditional country influenced albums. James Kilbane like many other Irish artists are today working closer with Nashville. A recent success in the Irish arena has been Crystal Swing. In Sweden, Rednex rose to stardom combining country music with electro-pop in the 1990s. In 1994, the group had a worldwide hit with their version of the traditional Southern tune \"Cotton-Eyed Joe\". Artists popularizing more traditional country music in Sweden have been Ann-Louise Hanson, Hasse Andersson, Kikki Danielsson, Elisabeth Andreassen and Jill Johnson. In Poland an international country music festival, known as Piknik Country, has been organized in Mrągowo in Masuria since 1983. There are more and more country music artists in France. Some of the most important are Liane Edwards, , Rockie Mountains, Tahiana, and Lili West. French rock and roll superstar Eddy Mitchell is also very inspired by Americana and country music.\n\nIn Iran, country music has appeared in recent years. According to \"Melody Music Magazine\", the pioneer of country music in Iran is the English-speaking country music band Dream Rovers, whose founder, singer and songwriter is Erfan Rezayatbakhsh (elf). The band was formed in 2007 in Tehran, and during this time they have been trying to introduce and popularize country music in Iran by releasing two studio albums and performing live at concerts, despite the difficulties that the Islamic regime in Iran makes for bands that are active in the western music field.\n\nIn Japan, electronic music producer and DJ Yasutaka Nakata started to create a country-folk style of music for model and entertainer Mito Natsume. Mito's activities as a singer has yielded to her debut studio album, \"Natsumelo,\" in 2017.\n\nIn Philippines, country music has found their way into Cordilleran way of life, often compared Igorot way of life to cowboys. Baguio City has a FM station that caters to country music, 99.9 Country. And Bombo Radyo Baguio has a segment on Sunday slot for Igorot, Ilocano and country music.\n\nSix U.S. cable TV networks are at least partly devoted to the genre: Country Music Television and CMT Music (both owned by Viacom), Rural Free Delivery TV (owned by Rural Media Group), Great American Country (owned by Scripps Networks), Heartland (owned by Luken Communications), and The Country Network (owned by TCN Country, LLC).\n\nThe first American country music video cable channel was The Nashville Network, launched in the early 1980s. In 2000, after it and CMT fell under the same corporate ownership, the channel was renamed and reformatted as The \"National\" Network, a general-interest network, and eventually became Spike TV, However Spike TV is no longer a music video channel. TNN was later revived from 2012 to 2013 after Jim Owens Entertainment acquired the trademark and licensed it to Luken Communications; that channel renamed itself Heartland after Luken was embroiled in an unrelated dispute that left the company bankrupt.\n\nOnly one television channel is currently dedicated to country music in Canada: CMT (Canada) owned by Corus Entertainment (90%) and Viacom (10%). In the past, country music had an extensive presence, especially on the Canadian national broadcaster, CBC Television. The show \"Don Messer's Jubilee\" significantly affected country music in Canada; for instance, it was the program that launched Anne Murray's career. Gordie Tapp's \"Country Hoedown\" and its successor, \"The Tommy Hunter Show\", ran for a combined 36 years on the CBC, from 1956 to 1992; in its last nine years on air, the U.S. cable network TNN carried Hunter's show. Also, in Degrassi Season 13 Episode 29: \"Sparks Will Fly\", Tristan, Miles and Zig form a country band to play at the school's \"Wild, Wild West Night\".\n\nThe only network dedicated to country music in Australia is the Country Music Channel owned by Foxtel.\n\n\n"}
{"id": "5248", "url": "https://en.wikipedia.org/wiki?curid=5248", "title": "Cold War (1947–1953)", "text": "Cold War (1947–1953)\n\nThe Cold War (1947–1953) is the period within the Cold War from the Truman Doctrine in 1947 to the conclusion of the Korean War in 1953. The Cold War began almost immediately following World War II and lasted through most of the 20th century. Political relations between the USA, Britain, Canada and the USSR were tainted within days of VJ-Day when cipher clerk Igor Gouzenko defected from the Soviet embassy in the Canadian capital, offering documentary proof of two wartime networks of Soviet spies in North America, one aimed at the Manhattan Project. In the next five years spies Julius and Ethel Rosenberg and Klaus Fuchs were exposed and British diplomats Guy Burgess and Donald Maclean defected to the Soviet Union.\n\nDuring World War II, the Soviet Union annexed several countries as Soviet Socialist Republics within the Union of Soviet Socialist Republics. Most of those countries had been ceded to it by the secret agreement portion of the Molotov-Ribbentrop Pact with Nazi Germany. These later annexed territories include Eastern Poland (incorporated into two different SSRs), Latvia (became Latvian SSR), Estonia (became Estonian SSR), Lithuania (became Lithuanian SSR), part of eastern Finland (became part of the Karelo-Finnish SSR) and northern Romania (became the Moldavian SSR).\n\nSeveral of the other countries it occupied that were not directly annexed into the Soviet Union became Soviet satellite states. In East Germany after local election losses, a forced merger of political parties in the Socialist Unity Party (\"SED\"), followed by elections in 1946 where political opponents were oppressed. In the non-USSR annexed portion of Poland, less than a third of Poland's population voted in favor of massive communist land reforms and industry nationalizations in a policies referendum known as \"3 times YES\" (\"3 razy TAK\"; \"3xTAK\"), whereupon a second vote rigged election was held to get the desired result. Fraudulent Polish elections held in January 1947 resulted in Poland's official transformation to the People's Republic of Poland.\n\nThe List of World Leaders at the Beginning of these Years are as follows:\n\n1947- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Chiang Kai-shek (Allied China)\n\n1948- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Chiang Kai-shek (Allied China)\n\n1949- Clement Attlee (UK): Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Chiang Kai-shek (Allied China)\n\n1950- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)\n\n1951- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)\n\n1952- Winston Churchill (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)\n\n1953- Winston Churchill (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)\n\nIn Hungary, when the Soviets installed a communist government, Mátyás Rákosi was appointed General Secretary of the Hungarian Communist Party, which began one of the harshest dictatorships in Europe under the People's Republic of Hungary. In Bulgaria, toward the end of World War II, the Soviet Union crossed the border and created the conditions for a communist coup d'état on the following night. The Soviet military commander in Sofia assumed supreme authority, and the communists whom he instructed, including Kimon Georgiev (who was not a communist himself, but a member of the elitarian political organization \"Zveno\", working together with the communists), took full control of domestic politics in the People's Republic of Bulgaria.\n\nWith Soviet backing, the Communist Party of Czechoslovakia assumed undisputed control over the government of Czechoslovakia in the Czechoslovak coup d'état of 1948, ushering in a dictatorship. In the Romanian general election elections of 1946, the Romanian Communist Party (PCR) employed widespread intimidation tactics and electoral fraud to obtain 80 percent of the vote and, thereafter, eliminated the role of the centrist parties and forced mergers, the result of which was that, by 1948, most non-Communist politicians were either executed, in exile or in prison. In the December 1945 Albanian election, the only effective ballot choices were those of the communist Democratic Front (Albania), led by Enver Hoxha. In 1946, Albania was declared the People's Republic of Albania.\n\nInitially, Stalin directed systems in the Eastern Bloc countries that rejected Western institutional characteristics of market economies, democratic governance (dubbed \"bourgeois democracy\" in Soviet parlance) and the rule of law subduing discretional intervention by the state. They were economically communist and depended upon the Soviet Union for significant amounts of materials. While in the first five years following World War II, massive emigration from these states to the West occurred, restrictions implemented thereafter stopped most East-West migration, except that under limited bilateral and other agreements.\n\nThe immediate post-1945 period may have been the historical high point for the popularity of communist ideology. The burdens the Red Army and the Soviet Union endured had earned it massive respect which, had it been fully exploited by Joseph Stalin, had a good chance of resulting in a communist Europe. Communist parties achieved a significant popularity in such nations as China, Greece, Iran, and the Republic of Mahabad. Communist parties had already come to power in Romania, Bulgaria, Albania, and Yugoslavia. The United Kingdom and the United States were concerned that electoral victories by communist parties in any of these countries could lead to sweeping economic and political change in Western Europe.\n\nHaving lost 27 million people in the war, the Soviet Union was determined to destroy Germany's capacity for another war, and pushed for such in wartime conferences. The resulting Morgenthau plan policy foresaw returning Germany to a pastoral state without heavy industry. Because of the increasing costs of food imports to avoid mass-starvation in Germany, and with the danger of losing the entire nation to communism, the U.S. government abandoned the Morgenthau plan in September 1946 with Secretary of State James F. Byrnes' speech Restatement of Policy on Germany.\n\nIn January 1947, Truman appointed General George Marshall as Secretary of State, and enacted JCS 1779, which decreed that an orderly and prosperous Europe requires the economic contributions of a stable and productive Germany.\" The directive comported with the view of General Lucius D. Clay and the Joint Chief of Staff over growing communist influence in Germany, as well as of the failure of the rest of the European economy to recover without the German industrial base on which it previously had been dependent. Administration officials met with Soviet Foreign Minister Vyacheslav Molotov and others to press for an economically self-sufficient Germany, including a detailed accounting of the industrial plants, goods and infrastructure already removed by the Soviets. After six weeks of negotiations, Molotov refused the demands and the talks were adjourned. Marshall was particularly discouraged after personally meeting with Stalin, who expressed little interest in a solution to German economic problems. The United States concluded that a solution could not wait any longer. In a June 5, 1947 speech, comporting with the Truman Doctrine, Marshall announced a comprehensive program of American assistance to all European countries wanting to participate, including the Soviet Union and those of Eastern Europe, called the Marshall Plan.\n\nFearing American political, cultural and economic penetration, Stalin eventually forbade Soviet Eastern bloc countries of the newly formed Cominform from accepting Marshall Plan aid. In Czechoslovakia, that required a Soviet-backed Czechoslovak coup d'état of 1948, the brutality of which shocked Western powers more than any event so far and set in a motion a brief scare that war would occur and swept away the last vestiges of opposition to the Marshall Plan in the United States Congress.\n\nBoth East and West regarded Greece as a nation well within the sphere of influence of Britain. Stalin had respected his agreement with Winston Churchill to not intervene, but Yugoslavia and Albania defied the USSR's advice and sent supplies during the Greek Civil War to the partisan forces of the Communist Party of Greece, the ELAS (National Popular Liberation Army). The UK had given aid to the royalist Greek forces and ELAS leaders who, failing to realize that there would be no Soviet aid and having boycotted the elections, were at a disadvantaged position. However, by 1947, the near-bankrupt British government could no longer maintain its massive overseas commitments. In addition to granting independence to India and handing back the Palestinian Mandate to the United Nations, the British government decided to withdraw from both Greece and nearby Turkey. This would have left the two nations, in particular Greece, on the brink of a communist-led revolution.\n\nNotified that British aid to Greece and Turkey would end in less than six weeks, and already hostile towards and suspicious of Soviet intentions, because of their reluctance to withdraw from Iran, the Truman administration decided that additional action was necessary. With Congress solidly in Republican hands, and with isolationist sentiment strong among the U.S. public, Truman adopted an ideological approach. In a meeting with congressional leaders, the argument of \"apples in a barrel infected by one rotten one\" was used to convince them of the significance in supporting Greece and Turkey. It was to become the \"domino theory\". On the morning of March 12, 1947, President Harry S. Truman appeared before Congress to ask for $400 million of aid to Greece and Turkey. Calling on congressional approval for the United States to \"support free peoples who are resisting attempted subjugation by armed minorities or by outside pressures,\" or in short a policy of \"containment\", Truman articulated a presentation of the ideological struggle that became known as the \"Truman Doctrine.\" Although based on a simplistic analysis of internal strife in Greece and Turkey, it became the single dominating influence over U.S. policy until at least the Vietnam War.\n\nTruman's speech had a tremendous effect. The anti-communist feelings that had just begun to hatch in the U.S. were given a great boost, and a silenced Congress voted overwhelmingly in approval of aid. The United States would not withdraw back to the Western Hemisphere as it had after World War I. From then on, the U.S. actively engaged any communist threats anywhere in the globe under the ostensible causes of \"freedom\", \"democracy\" and \"human rights.\" The U.S. brandished its role as the leader of the \"free world.\" Meanwhile, the Soviet Union brandished its position as the leader of the \"progressive\" and \"anti-imperialist\" camp.\n\nAfter World War II, the generals of the newly formed U.S. Air Force propounded a new doctrine: that strategic bombing, particularly with nuclear weapons, was the sole decisive element necessary to win any future war; and was therefore the sole means necessary to deter an adversary from launching a Pearl Harbor like surprise attack or war against the United States. To implement this doctrine, which the Air Force and its supporters regarded as the highest national priority, the Air Force proposed that it should be funded by the Congress to build a large fleet of U.S. based long-range strategic heavy bombers. The Air Force generals argued that this project should receive large amounts of funding, beginning with the B-36 Peacemaker bomber.\n\nThe admirals of the Navy disagreed. Pointing to the overwhelming dominance of the aircraft carrier in the Pacific Theater, they asked the United States Congress to fund a large fleet of \"supercarriers\" and their supporting battle groups, beginning with the USS \"United States\" (CVA-58). The Navy leadership believed that wars could not be won by strategic bombing alone, with or without the use of nuclear weapons. The Navy also maintained that to decide, at the outset of any future conflict, to initiate the widespread use of nuclear weapons—attacking the major population centers of the enemy homeland—was immoral.\n\nRelations further deteriorated when, in January 1948, the U.S. State Department also published a collection of documents titled \"Nazi-Soviet Relations, 1939–1941: Documents from the Archives of The German Foreign Office\", which contained documents recovered from the Foreign Office of Nazi Germany revealing Soviet conversations with Germany regarding the Molotov-Ribbentrop Pact, including its secret protocol dividing eastern Europe, the 1939 German-Soviet Commercial Agreement, and discussions of the Soviet Union potentially becoming the fourth Axis Power.\n\nIn response, one month later, the Soviet Union published \"Falsifiers of History\", a Stalin edited and partially re-written book attacking the West. The book did not attempt to directly counter or deal with the documents published in \"Nazi-Soviet Relations\" and rather, focused upon Western culpability for the outbreak of war in 1939. It argues that \"Western powers\" aided Nazi rearmament and aggression, including that American bankers and industrialists provided capital for the growth of German war industries, while deliberately encouraging Hitler to expand eastward. The book also included the claim that, during the Pact's operation, Stalin rejected Hitler's offer to share in a division of the world, without mentioning the Soviet offers to join the Axis. Historical studies, official accounts, memoirs and textbooks published in the Soviet Union used that depiction of events until the Soviet Union's dissolution.\n\nAfter the Marshall Plan, the introduction of a new currency to Western Germany to replace the debased Reichsmark and massive electoral losses for communist parties in 1946, in June 1948, the Soviet Union cut off surface road access to Berlin. On the day of the Berlin Blockade, a Soviet representative told the other occupying powers \"We are warning both you and the population of Berlin that we shall apply economic and administrative sanctions that will lead to circulation in Berlin exclusively of the currency of the Soviet occupation zone.\"\n\nThereafter, street and water communications were severed, rail and barge traffic was stopped and the Soviets initially stopped supplying food to the civilian population in the non-Soviet sectors of Berlin. Because Berlin was located within the Soviet-occupied zone of Germany and the other occupying powers had previously relied on Soviet good will for access to Berlin, the only available methods of supplying the city were three limited air corridors.\n\nBy February 1948, because of massive post-war military cuts, the entire United States army had been reduced to 552,000 men. Military forces in non-Soviet Berlin sectors totaled only 8,973 Americans, 7,606 British and 6,100 French. Soviet military forces in the Soviet sector that surrounded Berlin totaled one and a half million men. The two United States regiments in Berlin would have provided little resistance against a Soviet attack. Believing that Britain, France and the United States had little option other than to acquiesce, the Soviet Military Administration in Germany celebrated the beginning of the blockade. Thereafter, a massive aerial supply campaign of food, water and other goods was initiated by the United States, Britain, France and other countries. The Soviets derided \"the futile attempts of the Americans to save face and to maintain their untenable position in Berlin.\" The success of the airlift eventually caused the Soviets to lift their blockade in May 1949.\n\nAfter disagreements between Yugoslavian leader Josip Broz Tito and the Soviet Union regarding Greece and the People's Republic of Albania, a Tito–Stalin Split occurred, followed by Yugoslavia being expelled from the Cominform in June 1948 and a brief failed Soviet putsch in Belgrade. The split created two separate communist forces in Europe. A vehement campaign against \"Titoism\" was immediately started in the Eastern Bloc, describing agents of both the West and Tito in all places engaging in subversive activity. This resulted in the persecution of many major party cadres, including those in East Germany.\n\nThe United States joined Britain, France, Canada, Denmark, Portugal, Norway, Belgium, Iceland, Luxembourg, Italy, and the Netherlands in 1949 to form the North Atlantic Treaty Organization (NATO), the United States' first \"entangling\" European alliance in 170 years. West Germany, Spain, Greece, and Turkey would later join this alliance. The Eastern leaders retaliated against these steps by integrating the economies of their nations in Comecon, their version of the Marshall Plan; exploding the first Soviet atomic device in 1949; signing an alliance with People's Republic of China in February 1950; and forming the Warsaw Pact, Eastern Europe's counterpart to NATO, in 1955. The Soviet Union, Albania, Czechoslovakia, Hungary, East Germany, Bulgaria, Romania, and Poland founded this military alliance.\n\nU.S. officials quickly moved to escalate and expand \"containment.\" In a secret 1950 document, NSC-68, they proposed to strengthen their alliance systems, quadruple defense spending, and embark on an elaborate propaganda campaign to convince the U.S. public to fight this costly cold war. Truman ordered the development of a hydrogen bomb. In early 1950, the U.S. took its first efforts to oppose communist forces in Vietnam; planned to form a West German army, and prepared proposals for a peace treaty with Japan that would guarantee long-term U.S. military bases there.\n\nShortly after World War II, the civil war resumed in China between the Kuomintang (KMT) led by Generalissimo Chiang Kai-shek and the Communist Party of China led by Mao Zedong. The USSR had signed a Treaty of Friendship with the Kuomintang in 1945 and disavowed support for the Chinese Communists. The outcome was closely fought, with the Communists finally prevailing with superior military tactics. Although the Nationalists had an advantage in numbers of men and weapons, initially controlled a much larger territory and population than their adversaries, and enjoyed considerable international support, they were exhausted by the long war with Japan and the attendant internal responsibilities. In addition, the Chinese Communists were able to fill the political vacuum left in Manchuria after Soviet forces withdrew from the area and thus gained China's prime industrial base. The Chinese Communists were able to fight their way from the north and northeast, and virtually all of mainland China was taken by the end of 1949. On October 1, 1949, Mao Zedong proclaimed the People's Republic of China (PRC). Chiang Kai-shek and 600,000 Nationalist troops and 2 million refugees, predominantly from the government and business community, fled from the mainland to the island of Taiwan. In December 1949, Chiang proclaimed Taipei the temporary capital of the Republic of China (ROC) and continued to assert his government as the sole legitimate authority in China.\n\nThe continued hostility between the Communists on the mainland and the Nationalists on Taiwan continued throughout the Cold War. Though the United States refused to aide Chiang Kai-shek in his hope to \"recover the mainland,\" it continued supporting the Republic of China with military supplies and expertise to prevent Taiwan from falling into PRC hands. Through the support of the Western bloc (most Western countries continued to recognize the ROC as the sole legitimate government of China), the Republic of China on Taiwan retained China's seat in the United Nations until 1971.\n\nIn early 1950, the United States made its first commitment to form a peace treaty with Japan that would guarantee long-term U.S. military bases. Some observers (including George Kennan) believed that the Japanese treaty led Stalin to approve a plan to invade U.S.-supported South Korea on June 25, 1950. Korea had been divided at the end of World War II along the 38th parallel into Soviet and U.S. occupation zones, in which a communist government was installed in the North by the Soviets, and an elected government in the South came to power after UN-supervised elections in 1948.\n\nIn June 1950, Kim Il-sung's North Korean People's Army invaded South Korea. Fearing that communist Korea under a Kim Il Sung dictatorship could threaten Japan and foster other communist movements in Asia, Truman committed U.S. forces and obtained help from the United Nations to counter the North Korean invasion. The Soviets boycotted UN Security Council meetings while protesting the Council's failure to seat the People's Republic of China and, thus, did not veto the Council's approval of UN action to oppose the North Korean invasion. A joint UN force of personnel from South Korea, the United States, Britain, Turkey, Canada, Australia, France, the Philippines, the Netherlands, Belgium, New Zealand and other countries joined to stop the invasion. After a Chinese invasion to assist the North Koreans, fighting stabilized along the 38th parallel, which had separated the Koreas. Truman faced a hostile China, a Sino-Soviet partnership, and a defense budget that had quadrupled in eighteen months.\n\nThe Korean Armistice Agreement was signed in July 1953 after the death of Stalin, who had been insisting that the North Koreans continue fighting. In North Korea, Kim Il-sung created a highly centralized and brutal dictatorship, according himself unlimited power and generating a formidable cult of personality.\n\nA hydrogen bomb—which produced nuclear fusion instead of nuclear fission—was first tested by the United States in November 1952 and the Soviet Union in August 1953. Such bombs were first deployed in the 1960s.\n\nFear of a nuclear war spurred the production of public safety films by the United States federal government's Civil Defense branch that demonstrated ways on protecting oneself from a Soviet nuclear attack. The 1951 children's film \"Duck and Cover\" is a prime example.\n\nGeorge Orwell's classic dystopia Nineteen Eighty-Four was published in 1949. The novel explores life in an imagined future world where a totalitarian government has achieved terrifying levels of power and control. With Nineteen Eighty-Four, Orwell taps into the anti-communist fears that would continue to haunt so many in the West for decades to come. In a Cold War setting his descriptions could hardly fail to evoke comparison to Soviet communism and the seeming willingness of Stalin and his successors to control those within the Soviet bloc by whatever means necessary. Orwell's famous allegory of totalitarian rule, Animal Farm, published in 1945, provoked similar anti-communist sentiments.\n\n\n\n\n"}
{"id": "5249", "url": "https://en.wikipedia.org/wiki?curid=5249", "title": "Crony capitalism", "text": "Crony capitalism\n\nCrony capitalism is an economy in which businesses thrive not as a result of risk taken for them, but rather, as a return on money amassed through a nexus between a business class and the political class. This is done using state power to crush genuine competition in handing out permits, government grants, special tax breaks, or other forms of state intervention over resources where the state exercises monopolist control over public goods, for example, mining concessions for primary commodities or contracts for public works. Money is then made not merely by making a profit in the market, but by profiteering by 'rent seeking' using this monopoly or oligopoly. Entrepreneurship and innovative practices, which seek to reward risk are stifled, since the value-add is little by crony businesses as hardly anything of significant value is created by them, with transactions taking the form of 'trading'.\n\nCrony capitalism spills over into the government, the politics and the media, when this nexus distorts the economy and affects society to an extent it corrupts public-serving economic, political and social ideals.\n\nThe term \"crony capitalism\" made a significant impact in the public, as an explanation of the Asian financial crisis. It is also used to describe governmental decisions favoring \"cronies\" of governmental officials. In this context, the term is often used interchangeably with corporate welfare; to the extent that there is a difference, it may be the extent to which a government action can be said to benefit individuals rather than entire industries.\n\nCrony capitalism exists along a continuum. In its lightest form, crony capitalism consists of collusion among market players which is officially tolerated or encouraged by the government. While perhaps lightly competing against each other, they will present a unified front (sometimes called a trade association or industry trade group) to the government in requesting subsidies or aid or regulation. Newcomers to a market then need to surmount significant \"barriers to entry\" for instance, in seeking loans, acquire shelf space, or receive official sanction. Some such systems are very formalized, such as sports leagues and the Medallion System of the taxicabs of New York City, but often the process is more subtle, such as expanding training and certification exams to make it more expensive for new entrants to enter a market and thereby limit competition. In technological fields, there may evolve a system whereby new entrants may be accused of infringing on patents that the established competitors never assert against each other. In spite of this, some competitors may succeed when the legal barriers are light. \n\nThe term crony capitalism is generally used when these practices come to dominate the economy as a whole or to dominate the most valuable industries in an economy. Intentionally ambiguous laws and regulations are common in such systems. Taken strictly, such laws would greatly impede practically all business; in practice, they are only erratically enforced. The specter of having such laws suddenly brought down upon a business provides incentive to stay in the good graces of political officials. Troublesome rivals who have overstepped their bounds can have the laws suddenly enforced against them, leading to fines or even jail time. Even in high-income democracies with well-established legal systems and freedom of the press in place, a larger state is associated with more political corruption.\n\nThe term \"crony capitalism\" was initially applied to states involved in the 1997 Asian financial crisis such as Thailand and Indonesia. In these cases, the term was used to point out how family members of the ruling leaders become extremely wealthy with no non-political justification. Southeast Asian nations still score very poorly in rankings measuring this. Hong Kong, and Malaysia are perhaps most noted for this, and the term has also been applied to the system of oligarchs in Russia.\nOther states to which the term has been applied include India, in particular, the system after the 1990s liberalization whereby land and other resources were given at throwaway prices in the name of public private partnerships, the more recent coal-gate scam and cheap allocation of land and resources to Adani SEZ under the Congress and BJP governments.\n\nSimilar references to crony capitalism have been made to other countries such as Argentina and Greece. Wu Jinglian, one of China's leading economists and a longtime advocate of its transition to free markets, says that it faces two starkly contrasting futures: a market economy under the rule of law or crony capitalism.\n\nMany prosperous nations have also had varying amounts of cronyism throughout their history including the United Kingdom, especially in the 1600s and 1700s, United States, and Japan.\n\n\"The Economist\" benchmarks countries based on a \"crony-capitalism index\" calculated via how much economic activity occurs in industries prone to cronyism. Its 2014 Crony Capitalism Index ranking listed Hong Kong, Russia and Malaysia in the top 3 spots.\n\nCrony capitalism in finance was found in the Second Bank of the United States. It was a private company, but its largest stockholder was the federal government which owned 20%. It was an early bank regulator and grew to be one being the most powerful organizations in the country due largely to being the depository of the government's revenue.\n\nThe Gramm-Leach-Bliley Act in 1999 completely removed Glass-Steagall’s separation between commercial banks and investment banks. After this repeal, commercial banks, investment banks, and insurance companies combined their lobbying efforts. Critics claim this was instrumental in the passage of the Bankruptcy Abuse Prevention and Consumer Protection Act of 2005.\n\nMore direct government involvement in a specific sector can also lead to specific areas of crony capitalism, even if the economy as a whole may be competitive. This is most common in natural resource sectors through the granting of mining or drilling concessions, but it is also possible through a process known as regulatory capture where the government agencies in charge of regulating an industry come to be controlled by that industry. Governments will often, in good faith, establish government agencies to regulate an industry. However, the members of an industry have a very strong interest in the actions of that regulatory body, while the rest of the citizenry are only lightly affected. As a result, it is not uncommon for current industry players to gain control of the \"watchdog\" and to use it against competitors. This typically takes the form of making it very expensive for a new entrant to enter the market.\nAn 1824 landmark U.S. Supreme Court ruling overturned a New York State-granted monopoly (\"a veritable model of state munificence\" facilitated by one of the Founding Fathers, Robert R. Livingston) for the then-revolutionary technology of steamboats. Leveraging the Supreme Court's establishment of Congressional supremacy over commerce, the Interstate Commerce Commission was established in 1887 with the intent of regulating railroad \"robber barons\". President Grover Cleveland appointed Thomas M. Cooley, a railroad ally, as its first chairman and a permit system was used to deny access to new entrants and legalize price fixing.\n\nThe defense industry in the United States is often described as an example of crony capitalism in an industry. Connections with the Pentagon and lobbyists in Washington are described by critics as more important than actual competition, due to the political and secretive nature of defense contracts. In the Airbus-Boeing WTO dispute, Airbus (which receives outright subsidies from European governments) has stated Boeing receives similar subsidies, which are hidden as inefficient defense contracts. Other American defense companies were put under scrutiny for no-bid contracts for Iraq war and Hurricane Katrina related contracts purportedly due to having cronies in the Bush administration.\n\nGerald P. O'Driscoll, former vice president at the Federal Reserve Bank of Dallas, stated that Fannie Mae and Freddie Mac became examples of crony capitalism. Government backing let Fannie and Freddie dominate mortgage underwriting. \"The politicians created the mortgage giants, which then returned some of the profits to the pols—sometimes directly, as campaign funds; sometimes as \"contributions\" to favored constituents.\"\n\nIn its worst form, crony capitalism can devolve into simple corruption, where any pretense of a free market is dispensed with. Bribes to government officials are considered \"de rigueur\" and tax evasion is common; this is seen in many parts of Africa, for instance. This is sometimes called plutocracy (rule by wealth) or kleptocracy (rule by theft).\n\nCorrupt governments may favor one set of business owners who have close ties to the government over others. This may also be done with racial, religious, or ethnic favoritism; for instance, Alawites in Syria have a disproportionate share of power in the government and business there. (President Assad is an Alawite.) This can be explained by considering personal relationships as a social network. As government and business leaders try to accomplish various things, they naturally turn to other powerful people for support in their endeavors. These people form hubs in the network. In a developing country those hubs may be very few, thus concentrating economic and political power in a small interlocking group.\n\nNormally, this will be untenable to maintain in business; new entrants will affect the market. However, if business and government are entwined, then the government can maintain the small-hub network.\n\nRaymond Vernon, specialist in economics and international affairs,\nwrote that the Industrial Revolution began in Great Britain, because they were the first to successfully limit the power of veto groups (typically cronies of those with power in government) to block innovations. \"Unlike most other national environments, the British environment of the early 19th century contained relatively few threats to those who improved and applied existing inventions, whether from business competitors, labor, or the government itself. In other European countries, by contrast, the merchant guilds ... were a pervasive source of veto for many centuries. This power was typically bestowed upon them by government\". For example, a Russian inventor produced a steam engine in 1766 and disappeared without a trace. \"[A] steam powered horseless carriage produced in France in 1769 was officially suppressed.\" James Watt began experimenting with steam in 1763, got a patent in 1769, and began commercial production in 1775.\n\nRaghuram Rajan, former governor of the Reserve Bank of India, has said \"One of the greatest dangers to the growth of developing countries is the middle income trap, where crony capitalism creates oligarchies that slow down growth. If the debate during the elections is any pointer, this is a very real concern of the public in India today.\" Tavleen Singh, columnist for \"The Indian Express\" has disagreed. According to her, India's corporate success is not a product of crony capitalism, but because India is no longer under the influence of crony socialism.\n\nWhile the problem is generally accepted across the political spectrum, ideology shades the view of the problem's causes and therefore its solutions. Political views mostly fall into two camps which might be called the socialist and capitalist critique. The socialist position is that broadly democratic government must regulate economic, or wealthy, interests in order to restrict monopoly. The capitalist position is that \"natural monopolies\" are rare, therefore governmental regulations generally abet established wealthy interests by restricting competition.\n\nCritics of crony capitalism including socialists and anti-capitalists often assert that crony capitalism is the inevitable result of \"any\" strictly capitalist system. Jane Jacobs described it as a natural consequence of collusion between those managing power and trade, while Noam Chomsky has argued that the word \"crony\" is superfluous when describing capitalism. Since businesses make money and money leads to political power, business will inevitably use their power to influence governments. Much of the impetus behind campaign finance reform in the United States and in other countries is an attempt to prevent economic power being used to take political power.\n\nRavi Batra argues that \"all official economic measures adopted since 1981...have devastated the middle class\" and that the Occupy Wall Street movement should push for their repeal and thus end the influence of the super wealthy in the political process, which he considers a manifestation of crony capitalism.\n\nSocialist economists, such as Robin Hahnel, have criticized the term as an ideologically motivated attempt to cast what is in their view the fundamental problems of capitalism as avoidable irregularities. Socialist economists dismiss the term as an apologetic for failures of neoliberal policy and, more fundamentally, their perception of the weaknesses of market allocation.\n\nSupporters of capitalism generally oppose crony capitalism as well, and consider it an aberration brought on by governmental favors incompatible with free market. In this view, crony capitalism is the result of an excess of socialist-style interference in the market, which inherently will result in a toxic combination of corporations and government officials running the sector of the economy. Some advocates prefer to equate this problem with terms such as \"corporatism, a modern form of mercantilism\" to emphasize that the only way to run a profitable business in such a system is to have help from corrupt government officials.\n\nEven if the initial regulation was well-intentioned (to curb actual abuses), and even if the initial lobbying by corporations was well-intentioned (to reduce illogical regulations), the mixture of business and government stifle competition, a collusive result called regulatory capture. Burton W. Folsom, Jr. distinguishes those that engage in crony capitalism—designated by him \"political entrepreneurs\"—from those who compete in the marketplace without special aid from government, whom he calls \"market entrepreneurs\". The market entrepreneurs, such as Hill, Vanderbilt, and Rockefeller, succeeded by producing a quality product at a competitive price. The political entrepreneurs, for example, Edward Collins in steamships and the leaders of the Union Pacific Railroad in railroads, were men who used the power of government to succeed. They tried to gain subsidies or in some way use government to stop competitors.\n\n\n"}
{"id": "5252", "url": "https://en.wikipedia.org/wiki?curid=5252", "title": "Lists of universities and colleges", "text": "Lists of universities and colleges\n\nThis is a lists of universities and colleges.\n\n\n\n\n\n\n\n\n"}
{"id": "5253", "url": "https://en.wikipedia.org/wiki?curid=5253", "title": "Constitution", "text": "Constitution\n\nA constitution is a set of fundamental principles or established precedents according to which a state or other organization is governed. These rules together make up, i.e. \"constitute\", what the entity is. When these principles are written down into a single document or set of legal documents, those documents may be said to embody a \"written\" constitution; if they are written down in a single comprehensive document, it is said to embody a \"codified\" constitution. Some constitutions (such as the constitution of the United Kingdom) are uncodified, but written in numerous fundamental Acts of a legislature, court cases or treaties.\n\nConstitutions concern different levels of organizations, from sovereign states to companies and unincorporated associations. A treaty which establishes an international organization is also its constitution, in that it would define how that organization is constituted. Within states, a constitution defines the principles upon which the state is based, the procedure in which laws are made and by whom. Some constitutions, especially codified constitutions, also act as limiters of state power, by establishing lines which a state's rulers cannot cross, such as fundamental rights.\n\nThe Constitution of India is the longest written constitution of any sovereign country in the world, containing 444 articles in 22 parts, 12 schedules and 118 amendments, with 146,385 words in its English-language version, while the Constitution of Monaco is the shortest written constitution, containing 10 chapters with 97 articles, and a total of 3,814 words.\n\nThe term \"constitution\" comes through French from the Latin word \"constitutio\", used for regulations and orders, such as the imperial enactments (\"constitutiones principis\": edicta, mandata, decreta, rescripta). Later, the term was widely used in canon law for an important determination, especially a decree issued by the Pope, now referred to as an \"apostolic constitution\".\n\nGenerally, every modern written constitution confers specific powers to an organization or institutional entity, established upon the primary condition that it abide by the said constitution's limitations. According to Scott Gordon, a political organization is constitutional to the extent that it \"contain[s] institutionalized mechanisms of power control for the protection of the interests and liberties of the citizenry, including those that may be in the minority\".\n\nActivities of officials within an organization or polity that fall within the constitutional or statutory authority of those officials are termed \"within power\" (or, in Latin, \"intra vires\"); if they do not, they are termed \"beyond power\" (or, in Latin, \"ultra vires\"). For example, a students' union may be prohibited as an organization from engaging in activities not concerning students; if the union becomes involved in non-student activities, these activities are considered to be \"ultra vires\" of the union's charter, and nobody would be compelled by the charter to follow them. An example from the constitutional law of sovereign states would be a provincial parliament in a federal state trying to legislate in an area that the constitution allocates exclusively to the federal parliament, such as ratifying a treaty. Action that appears to be beyond power may be judicially reviewed and, if found to be beyond power, must cease. Legislation that is found to be beyond power will be \"invalid\" and of no force; this applies to primary legislation, requiring constitutional authorization, and secondary legislation, ordinarily requiring statutory authorization. In this context, \"within power\", \"intra vires\", \"authorized\" and \"valid\" have the same meaning; as do \"beyond power\", \"ultra vires\", \"not authorized\" and \"invalid\".\n\nIn most but not all modern states the constitution has supremacy over ordinary statutory law (see Uncodified constitution below); in such states when an official act is unconstitutional, i.e. it is not a power granted to the government by the constitution, that act is \"null and void\", and the nullification is \"ab initio\", that is, from inception, not from the date of the finding. It was never \"law\", even though, if it had been a statute or statutory provision, it might have been adopted according to the procedures for adopting legislation. Sometimes the problem is not that a statute is unconstitutional, but the application of it is, on a particular occasion, and a court may decide that while there are ways it could be applied that are constitutional, that instance was not allowed or legitimate. In such a case, only the application may be ruled unconstitutional. Historically, the remedy for such violations have been petitions for common law writs, such as \"quo warranto\".\n\nExcavations in modern-day Iraq by Ernest de Sarzec in 1877 found evidence of the earliest known code of justice, issued by the Sumerian king Urukagina of Lagash \"ca\" 2300 BC. Perhaps the earliest prototype for a law of government, this document itself has not yet been discovered; however it is known that it allowed some rights to his citizens. For example, it is known that it relieved tax for widows and orphans, and protected the poor from the usury of the rich.\n\nAfter that, many governments ruled by special codes of written laws. The oldest such document still known to exist seems to be the Code of Ur-Nammu of Ur (\"ca\" 2050 BC). Some of the better-known ancient law codes include the code of Lipit-Ishtar of Isin, the code of Hammurabi of Babylonia, the Hittite code, the Assyrian code and Mosaic law.\n\nIn 621 BC, a scribe named Draco codified the cruel oral laws of the city-state of Athens; this code prescribed the death penalty for many offences (nowadays very severe rules are often called \"Draconian\"). In 594 BC, Solon, the ruler of Athens, created the new \"Solonian Constitution\". It eased the burden of the workers, and determined that membership of the ruling class was to be based on wealth (plutocracy), rather than by birth (aristocracy). Cleisthenes again reformed the Athenian constitution and set it on a democratic footing in 508 BC.\nAristotle (\"ca\" 350 BC) was the first to make a formal distinction between ordinary law and constitutional law, establishing ideas of constitution and constitutionalism, and attempting to classify different forms of constitutional government. The most basic definition he used to describe a constitution in general terms was \"the arrangement of the offices in a state\". In his works \"Constitution of Athens\", \"Politics\", and \"Nicomachean Ethics\" he explores different constitutions of his day, including those of Athens, Sparta, and Carthage. He classified both what he regarded as good and what he regarded as bad constitutions, and came to the conclusion that the best constitution was a mixed system, including monarchic, aristocratic, and democratic elements. He also distinguished between citizens, who had the right to participate in the state, and non-citizens and slaves, who did not.\n\nThe Romans first codified their constitution in 450 BC as the \"Twelve Tables\". They operated under a series of laws that were added from time to time, but Roman law was never reorganised into a single code until the \"Codex Theodosianus\" (AD 438); later, in the Eastern Empire the \"Codex repetitæ prælectionis\" (534) was highly influential throughout Europe. This was followed in the east by the \"Ecloga\" of Leo III the Isaurian (740) and the \"Basilica\" of Basil I (878).\n\nThe \"Edicts of Ashoka\" established constitutional principles for the 3rd century BC Maurya king's rule in Ancient India. For constitutional principles almost lost to antiquity, see the code of Manu.\n\nMany of the Germanic people that filled the power vacuum left by the Western Roman Empire in the Early Middle Ages codified their laws. One of the first of these Germanic law codes to be written was the Visigothic \"Code of Euric\" (471). This was followed by the \"Lex Burgundionum\", applying separate codes for Germans and for Romans; the \"Pactus Alamannorum\"; and the Salic Law of the Franks, all written soon after 500. In 506, the \"Breviarum\" or \"\"Lex Romana\"\" of Alaric II, king of the Visigoths, adopted and consolidated the \"Codex Theodosianus\" together with assorted earlier Roman laws. Systems that appeared somewhat later include the \"Edictum Rothari\" of the Lombards (643), the \"Lex Visigothorum\" (654), the \"Lex Alamannorum\" (730) and the \"Lex Frisionum\" (\"ca\" 785). These continental codes were all composed in Latin, while Anglo-Saxon was used for those of England, beginning with the Code of Æthelberht of Kent (602). In ca. 893, Alfred the Great combined this and two other earlier Saxon codes, with various Mosaic and Christian precepts, to produce the \"Doom book\" code of laws for England.\n\nJapan's \"Seventeen-article constitution\" written in 604, reportedly by Prince Shōtoku, is an early example of a constitution in Asian political history. Influenced by Buddhist teachings, the document focuses more on social morality than institutions of government \"per se\" and remains a notable early attempt at a government constitution.\n\nThe Constitution of Medina (, Ṣaḥīfat al-Madīna), also known as the Charter of Medina, was drafted by the Islamic prophet Muhammad after his flight (hijra to Yathrib where he became political leader. It constituted a formal agreement between Muhammad and all of the significant tribes and families of Yathrib (later known as Medina), including Muslims, Jews, and pagans. The document was drawn up with the explicit concern of bringing to an end the bitter intertribal fighting between the clans of the Aws (Aus) and Khazraj within Medina. To this effect it instituted a number of rights and responsibilities for the Muslim, Jewish, and pagan communities of Medina bringing them within the fold of one community—the Ummah.\nThe precise dating of the Constitution of Medina remains debated but generally scholars agree it was written shortly after the Hijra (622).\n\nIn Wales, the \"Cyfraith Hywel\" was codified by Hywel Dda c. 942–950.\n\nThe \"Pravda Yaroslava\", originally combined by Yaroslav the Wise the Grand Prince of Kyiv, was granted to Great Novgorod around 1017, and in 1054 was incorporated into the \"Ruska Pravda\", that became the law for all of Kievan Rus. It survived only in later editions of the 15th century.\n\nIn England, Henry I's proclamation of the Charter of Liberties in 1100 bound the king for the first time in his treatment of the clergy and the nobility. This idea was extended and refined by the English barony when they forced King John to sign \"Magna Carta\" in 1215. The most important single article of the \"Magna Carta\", related to \"\"habeas corpus\"\", provided that the king was not permitted to imprison, outlaw, exile or kill anyone at a whim—there must be due process of law first. This article, Article 39, of the \"Magna Carta\" read:\n\n\"No free man shall be arrested, or imprisoned, or deprived of his property, or outlawed, or exiled, or in any way destroyed, nor shall we go against him or send against him, unless by legal judgement of his peers, or by the law of the land.\"\n\nThis provision became the cornerstone of English liberty after that point. The social contract in the original case was between the king and the nobility, but was gradually extended to all of the people. It led to the system of Constitutional Monarchy, with further reforms shifting the balance of power from the monarchy and nobility to the House of Commons.\n\nThe Nomocanon of Saint Sava () was the first Serbian constitution from 1219. This legal act was well developed. St. Sava's Nomocanon was the compilation of Civil law, based on Roman Law and Canon law, based on Ecumenical Councils and its basic purpose was to organize functioning of the young Serbian kingdom and the Serbian church. Saint Sava began the work on the Serbian Nomocanon in 1208 while being at Mount Athos, using \"The Nomocanon in Fourteen Titles\", \"Synopsis of Stefan the Efesian\", \"Nomocanon of John Scholasticus\", Ecumenical Councils' documents, which he modified with the canonical commentaries of Aristinos and Joannes Zonaras, local church meetings, rules of the Holy Fathers, the law of Moses, translation of Prohiron and the Byzantine emperors' Novellae (most were taken from Justinian's Novellae). The Nomocanon was completely new compilation of civil and canonical regulations, taken from the Byzantine sources, but completed and reformed by St. Sava to function properly in Serbia. Beside decrees that organized the life of church, there are various norms regarding civil life, most of them were taken from Prohiron. Legal transplants of Roman-Byzantine law became the basis of the Serbian medieval law. The essence of Zakonopravilo was based on Corpus Iuris Civilis.\n\nStefan Dušan, Emperor of Serbs and Greeks, enacted Dušan's Code () in Serbia, in two state congresses: in 1349 in Skopje and in 1354 in Serres. It regulated all social spheres, so it was the second Serbian constitution, after St. Sava's Nomocanon (Zakonopravilo). The Code was based on Roman-Byzantine law. The legal transplanting is notable with the articles 171 and 172 of Dušan's Code, which regulated the juridical independence. They were taken from the Byzantine code Basilika (book VII, 1, 16–17).\n\nIn 1222, Hungarian King Andrew II issued the Golden Bull of 1222.\n\nBetween 1220 and 1230, a Saxon administrator, Eike von Repgow, composed the \"Sachsenspiegel\", which became the supreme law used in parts of Germany as late as 1900.\n\nIn 1998, S. Kouyaté reconstructed from oral tradition what he claims is a 14th-century charter of the Mali Empire, called the \"Kouroukan Fouga\".\n\nAround 1240, the Coptic Egyptian Christian writer, 'Abul Fada'il Ibn al-'Assal, wrote the \"Fetha Negest\" in Arabic. 'Ibn al-Assal took his laws partly from apostolic writings and Mosaic law, and partly from the former Byzantine codes. There are a few historical records claiming that this law code was translated into Ge'ez and entered Ethiopia around 1450 in the reign of Zara Yaqob. Even so, its first recorded use in the function of a constitution (supreme law of the land) is with Sarsa Dengel beginning in 1563. The \"Fetha Negest\" remained the supreme law in Ethiopia until 1931, when a modern-style Constitution was first granted by Emperor Haile Selassie I.\nIn the Principality of Catalonia, the Catalan constitutions were promulgated by the Court from 1283 (or even two centuries before, if we consider the Usatges of Barcelona as part of the compilation of Constitutions) until 1716, when Philip V of Spain gave the Nueva Planta decrees, finishing with the historical laws of Catalonia. These Constitutions were usually made formally as a royal initiative, but required for its approval or repeal the favorable vote of the Catalan Courts, the medieval antecedent of the modern Parliaments. These laws had, as the other modern constitutions, preeminence over other laws, and they could not be contradicted by mere decrees or edicts of the king.\n\nThe Golden Bull of 1356 was a decree issued by a \"Reichstag\" in Nuremberg headed by Emperor Charles IV that fixed, for a period of more than four hundred years, an important aspect of the constitutional structure of the Holy Roman Empire.\n\nIn China, the Hongwu Emperor created and refined a document he called \"Ancestral Injunctions\" (first published in 1375, revised twice more before his death in 1398). These rules served in a very real sense as a constitution for the Ming Dynasty for the next 250 years.\n\nThe oldest written document still governing a sovereign nation today is that of San Marino. The \"Leges Statutae Republicae Sancti Marini\" was written in Latin and consists of six books. The first book, with 62 articles, establishes councils, courts, various executive officers and the powers assigned to them. The remaining books cover criminal and civil law, judicial procedures and remedies. Written in 1600, the document was based upon the \"Statuti Comunali\" (Town Statute) of 1300, itself influenced by the \"Codex Justinianus\", and it remains in force today.\n\nIn 1392 the \"Carta de Logu\" was legal code of the Giudicato of Arborea promulgated by the \"giudicessa\" Eleanor. It was in force in Sardinia until it was superseded by the code of Charles Felix in April 1827. The Carta was a work of great importance in Sardinian history. It was an organic, coherent, and systematic work of legislation encompassing the civil and penal law.\n\nThe \"Gayanashagowa\", the oral constitution of the Iroquois nation also known as the Great Law of Peace, established a system of governance in which sachems (tribal chiefs) of the members of the Iroquois League made decisions on the basis of universal consensus of all chiefs following discussions that were initiated by a single tribe. The position of sachem descended through families, and were allocated by senior female relatives.\n\nHistorians including Donald Grinde, Bruce Johansen and others believe that the Iroquois constitution provided inspiration for the United States Constitution and in 1988 was recognised by a resolution in Congress. The thesis is not considered credible by some scholars. Stanford University historian Jack N. Rakove stated that \"The voluminous records we have for the constitutional debates of the late 1780s contain no significant references to the Iroquois\" and stated that there are ample European precedents to the democratic institutions of the United States. Francis Jennings noted that the statement made by Benjamin Franklin frequently quoted by proponents of the thesis does not support this idea as it is advocating for a union against these \"ignorant savages\" and called the idea \"absurd\". Bruce Johansen contends Jennings, Tooker etc. have \"humorlessly missed the ironic nature of Franklin's statement\" and persist in \"ignoring the relevant sources\". Anthropologist Dean Snow stated that though Franklin's Albany Plan may have drawn some inspiration from the Iroquois League, there is little evidence that either the Plan or the Constitution drew substantially from this source and argues that \"...such claims muddle and denigrate the subtle and remarkable features of Iroquois government. The two forms of government are distinctive and individually remarkable in conception.\"\n\nIn 1639, the Colony of Connecticut adopted the Fundamental Orders, which was the first North American constitution, and is the basis for every new Connecticut constitution since, and is also the reason for Connecticut's nickname, \"the Constitution State\".\n\nThe English Protectorate that was set up by Oliver Cromwell after the English Civil War promulgated the first detailed written constitution adopted by a modern state; it was called the Instrument of Government. This formed the basis of government for the short lived republic from 1653 to 1657 by providing a legal rationale for the increasing power of Cromwell, after Parliament consistently failed to govern effectively. Most of the concepts and ideas embedded into modern constitutional theory, especially bicameralism, separation of powers, the written constitution, and judicial review, can be traced back to the experiments of that period.\nDrafted by Major-General John Lambert in 1653, the \"Instrument of Government\" included elements incorporated from an earlier document \"Heads of Proposals\", which had been agreed to by the Army Council in 1647, as a set of propositions intended to be a basis for a constitutional settlement after King Charles I was defeated in the First English Civil War. Charles had rejected the propositions, but before the start of the Second Civil War, the Grandees of the New Model Army had presented the \"Heads of Proposals\" as their alternative to the more radical Agreement of the People presented by the Agitators and their civilian supporters at the Putney Debates.\n\nOn January 4, 1649 the Rump Parliament declared \"that the people are, under God, the original of all just power; that the Commons of England, being chosen by and representing the people, have the supreme power in this nation\".\n\nThe \"Instrument of Government\" was adopted by Parliament on December 15, 1653 and Oliver Cromwell was installed as Lord Protector on the following day. The constitution set up a state council consisting of 21 members while executive authority was vested in the office of \"Lord Protector of the Commonwealth\"; this position was designated as a non-hereditary life appointment. It also required the calling of triennial Parliaments, with each sitting for at least five months.\n\nThe \"Instrument of Government\" was replaced in May 1657 by England's second, and last, codified constitution, the Humble Petition and Advice, proposed by Sir Christopher Packe. The Petition offered hereditary monarchy to Oliver Cromwell, asserted Parliament's control over issuing new taxation, provided an independent council to advise the king and safeguarded 'Triennial' meetings of Parliament. A modified version of the Humble Petition with the clause on kingship removed was ratified on 25 May. This finally met its demise in conjunction with the death of Cromwell and the Restoration of the monarchy.\n\nOther examples of European constitutions of this era were the Corsican Constitution of 1755 and the Swedish Constitution of 1772.\n\nAll of the British colonies in North America that were to become the 13 original United States, adopted their own constitutions in 1776 and 1777, during the American Revolution (and before the later Articles of Confederation and United States Constitution), with the exceptions of Massachusetts, Connecticut and Rhode Island. The Commonwealth of Massachusetts adopted its Constitution in 1780, the oldest still-functioning constitution of any U.S. state; while Connecticut and Rhode Island officially continued to operate under their old colonial charters, until they adopted their first state constitutions in 1818 and 1843, respectively.\n\nWhat is sometimes called the \"enlightened constitution\" model was developed by philosophers of the Age of Enlightenment such as Thomas Hobbes, Jean-Jacques Rousseau, and John Locke. The model proposed that constitutional governments should be stable, adaptable, accountable, open and should represent the people (i.e., support democracy).\n\n\"Agreements and Constitutions of Laws and Freedoms of the Zaporizian Host\" was written in 1710 by Pylyp Orlyk, \"hetman\" of the Zaporozhian Host. It was written to establish a free Zaporozhian-Ukrainian Republic, with the support of Charles XII of Sweden. It is notable in that it established a democratic standard for the separation of powers in government between the legislative, executive, and judiciary branches, well before the publication of Montesquieu's \"Spirit of the Laws\". This Constitution also limited the executive authority of the \"hetman\", and established a democratically elected Cossack parliament called the General Council. However, Orlyk's project for an independent Ukrainian State never materialized, and his constitution, written in exile, never went into effect.\n\nCorsican Constitutions of 1755 and 1794 were inspired by Jean-Jacques Rousseau. The later one introduced universal suffrage for property owners.\n\nThe United States Constitution, ratified June 21, 1788, was influenced by the writings of Polybius, Locke, Montesquieu, and others. The document became a benchmark for republicanism and codified constitutions written thereafter.\n\nThe Polish–Lithuanian Commonwealth Constitution was passed on May 3, 1791. Another landmark document was the French Constitution, ratified on September 3, 1791.\n\nOn March 19, the Spanish Constitution of 1812 was ratified by a parliament gathered in Cadiz, the only Spanish continental city which was safe from French occupation. The Spanish Constitution served as a model for other liberal constitutions of several South-European and Latin American nations like, for example, Portuguese Constitution of 1822, constitutions of various Italian states during Carbonari revolts (i.e., in the Kingdom of the Two Sicilies), the Norwegian constitution of 1814, or the Mexican Constitution of 1824.\n\nIn Brazil, the Constitution of 1824 expressed the option for the monarchy as political system after Brazilian Independence. The leader of the national emancipation process was the Portuguese prince Pedro I, elder son of the king of Portugal. Pedro was crowned in 1822 as first emperor of Brazil. The country was ruled by Constitutional monarchy until 1889, when finally adopted the Republican model.\n\nIn Denmark, as a result of the Napoleonic Wars, the absolute monarchy lost its personal possession of Norway to another absolute monarchy, Sweden. However the Norwegians managed to infuse a radically democratic and liberal constitution in 1814, adopting many facets from the American constitution and the revolutionary French ones; but maintaining a hereditary monarch limited by the constitution, like the Spanish one.\n\nThe first Swiss Federal Constitution was put in force in September 1848 (with official revisions in 1878, 1891, 1949, 1971, 1982 and 1999).\n\nThe Serbian revolution initially led to a proclamation of a proto-constitution in 1811; the full-fledged Constitution of Serbia followed few decades later, in 1835. The first Serbian constitution (Sretenjski ustav) was adopted at the national assembly in Kragujevac on February 15, 1835.\n\nThe Constitution of Canada came into force on July 1, 1867 as the British North America Act, an act of the British Parliament. Over a century later, the BNA Act was patriated to the Canadian Parliament and augmented with the Canadian Charter of Rights and Freedoms. Apart from the \"Constitution Acts, 1867 to 1982\", Canada's constitution also has unwritten elements based in common law and convention.\n\nAfter tribal people first began to live in cities and establish nations, many of these functioned according to unwritten customs, while some developed autocratic, even tyrannical monarchs, who ruled by decree, or mere personal whim. Such rule led some thinkers to take the position that what mattered was not the design of governmental institutions and operations, as much as the character of the rulers. This view can be seen in Plato, who called for rule by \"philosopher-kings.\" Later writers, such as Aristotle, Cicero and Plutarch, would examine designs for government from a legal and historical standpoint.\n\nThe Renaissance brought a series of political philosophers who wrote implied criticisms of the practices of monarchs and sought to identify principles of constitutional design that would be likely to yield more effective and just governance from their viewpoints. This began with revival of the Roman law of nations concept and its application to the relations among nations, and they sought to establish customary \"laws of war and peace\" to ameliorate wars and make them less likely. This led to considerations of what authority monarchs or other officials have and don't have, from where that authority derives, and the remedies for the abuse of such authority.\n\nA seminal juncture in this line of discourse arose in England from the Civil War, the Cromwellian Protectorate, the writings of Thomas Hobbes, Samuel Rutherford, the Levellers, John Milton, and James Harrington, leading to the debate between Robert Filmer, arguing for the divine right of monarchs, on the one side, and on the other, Henry Neville, James Tyrrell, Algernon Sidney, and John Locke. What arose from the latter was a concept of government being erected on the foundations of first, a state of nature governed by natural laws, then a state of society, established by a social contract or compact, which bring underlying natural or social laws, before governments are formally established on them as foundations.\n\nAlong the way several writers examined how the design of government was important, even if the government were headed by a monarch. They also classified various historical examples of governmental designs, typically into democracies, aristocracies, or monarchies, and considered how just and effective each tended to be and why, and how the advantages of each might be obtained by combining elements of each into a more complex design that balanced competing tendencies. Some, such as Montesquieu, also examined how the functions of government, such as legislative, executive, and judicial, might appropriately be separated into branches. The prevailing theme among these writers was that the design of constitutions is not completely arbitrary or a matter of taste. They generally held that there are underlying principles of design that constrain all constitutions for every polity or organization. Each built on the ideas of those before concerning what those principles might be.\n\nThe later writings of Orestes Brownson would try to explain what constitutional designers were trying to do. According to Brownson there are, in a sense, three \"constitutions\" involved: The first the \"constitution of nature\" that includes all of what was called \"natural law.\" The second is the \"constitution of society\", an unwritten and commonly understood set of rules for the society formed by a social contract before it establishes a government, by which it establishes the third, a \"constitution of government\". The second would include such elements as the making of decisions by public conventions called by public notice and conducted by established rules of procedure. Each constitution must be consistent with, and derive its authority from, the ones before it, as well as from a historical act of society formation or constitutional ratification. Brownson argued that a state is a society with effective dominion over a well-defined territory, that consent to a well-designed constitution of government arises from presence on that territory, and that it is possible for provisions of a written constitution of government to be \"unconstitutional\" if they are inconsistent with the constitutions of nature or society. Brownson argued that it is not ratification alone that makes a written constitution of government legitimate, but that it must also be competently designed and applied.\n\nOther writers have argued that such considerations apply not only to all national constitutions of government, but also to the constitutions of private organizations, that it is not an accident that the constitutions that tend to satisfy their members contain certain elements, as a minimum, or that their provisions tend to become very similar as they are amended after experience with their use. Provisions that give rise to certain kinds of questions are seen to need additional provisions for how to resolve those questions, and provisions that offer no course of action may best be omitted and left to policy decisions. Provisions that conflict with what Brownson and others can discern are the underlying \"constitutions\" of nature and society tend to be difficult or impossible to execute, or to lead to unresolvable disputes.\n\nConstitutional design has been treated as a kind of metagame in which play consists of finding the best design and provisions for a written constitution that will be the rules for the game of government, and that will be most likely to optimize a balance of the utilities of justice, liberty, and security. An example is the metagame Nomic.\n\nPolitical economy theory regards constitutions as coordination devices that help citizens to prevent rulers from abusing power. If the citizenry can coordinate a response to police government officials in the face of a constitutional fault, then the government have the incentives to honor the rights that the constitution guarantees. An alternative view considers that constitutions are not enforced by the citizens at-large, but rather by the administrative powers of the state. Because rulers cannot themselves implement their policies, they need to rely on a set of organizations (armies, courts, police agencies, tax collectors) to implement it. In this position, they can directly sanction the government by refusing to cooperate, disabling the authority of the rulers. Therefore, constitutions could be characterized by a self-enforcing equilibria between the rulers and powerful administrators.\n\nMost commonly, the term \"constitution\" refers to a set of rules and principles that define the nature and extent of government. Most constitutions seek to regulate the relationship between institutions of the state, in a basic sense the relationship between the executive, legislature and the judiciary, but also the relationship of institutions within those branches. For example, executive branches can be divided into a head of government, government departments/ministries, executive agencies and a civil service/administration. Most constitutions also attempt to define the relationship between individuals and the state, and to establish the broad rights of individual citizens. It is thus the most basic law of a territory from which all the other laws and rules are hierarchically derived; in some territories it is in fact called \"Basic Law\".\n\nThe following are features of democratic constitutions that have been identified by political scientists to exist, in one form or another, in virtually all national constitutions.\n\nA fundamental classification is codification or lack of codification. A codified constitution is one that is contained in a single document, which is the single source of constitutional law in a state. An uncodified constitution is one that is not contained in a single document, consisting of several different sources, which may be written or unwritten; see constitutional convention.\n\nMost states in the world have codified constitutions.\n\nCodified constitutions are often the product of some dramatic political change, such as a revolution. The process by which a country adopts a constitution is closely tied to the historical and political context driving this fundamental change. The legitimacy (and often the longevity) of codified constitutions has often been tied to the process by which they are initially adopted and some scholars have pointed out that high constitutional turnover within a given country may itself be detrimental to separation of powers and the rule of law.\n\nStates that have codified constitutions normally give the constitution supremacy over ordinary statute law. That is, if there is any conflict between a legal statute and the codified constitution, all or part of the statute can be declared \"ultra vires\" by a court, and struck down as unconstitutional. In addition, exceptional procedures are often required to amend a constitution. These procedures may include: convocation of a special constituent assembly or constitutional convention, requiring a supermajority of legislators' votes, the consent of regional legislatures, a referendum process, and/or other procedures that make amending a constitution more difficult than passing a simple law.\n\nConstitutions may also provide that their most basic principles can never be abolished, even by amendment. In case a formally valid amendment of a constitution infringes these principles protected against any amendment, it may constitute a so-called \"unconstitutional constitutional law\".\n\nCodified constitutions normally consist of a ceremonial preamble, which sets forth the goals of the state and the motivation for the constitution, and several articles containing the substantive provisions. The preamble, which is omitted in some constitutions, may contain a reference to God and/or to fundamental values of the state such as liberty, democracy or human rights. In ethnic nation-states such as Estonia, the mission of the state can be defined as preserving a specific nation, language and culture.\n\n only two sovereign states, New Zealand and the United Kingdom, have uncodified constitutions. The Basic Laws of Israel have since 1950 been intended to be the basis for a constitution, but as of 2017 it had not been drafted. The various Laws are considered to have precedence over other laws, and give the procedure by which they can be amended, typically by a simple majority of members of the Knesset (parliament).\n\nUncodified constitutions are the product of an \"evolution\" of laws and conventions over centuries. By contrast to codified constitutions (in the Westminster System that originated in England), uncodified constitutions include written sources: e.g. constitutional statutes enacted by the Parliament and also unwritten sources: constitutional conventions, observation of precedents, royal prerogatives, custom and tradition, such as always holding the General Election on Thursdays; together these constitute the British constitutional law.\n\nSome constitutions are largely, but not wholly, codified. For example, in the Constitution of Australia, most of its fundamental political principles and regulations concerning the relationship between branches of government, and concerning the government and the individual are codified in a single document, the Constitution of the Commonwealth of Australia. However, the presence of statutes with constitutional significance, namely the Statute of Westminster, as adopted by the Commonwealth in the Statute of Westminster Adoption Act 1942, and the Australia Act 1986 means that Australia's constitution is not contained in a single constitutional document. It means the Constitution of Australia is uncodified, it also contains constitutional conventions, thus is partially unwritten.\n\nThe Constitution of Canada, which evolved from the British North America Acts until severed from nominal British control by the Canada Act 1982 (analogous to the Australia Act 1986), is a similar example. Canada's constitution consists of almost 30 different statutes.\n\nThe terms \"written constitution\" and \"codified constitution\" are often used interchangeably, as are \"unwritten constitution\" and \"uncodified constitution\", although this usage is technically inaccurate. A codified constitution is a single document; states that do not have such a document have uncodified, but not entirely unwritten, constitutions, since much of an uncodified constitution is usually written in laws such as the Basic Laws of Israel and the Parliament Acts of the United Kingdom. Uncodified constitutions largely lack protection against amendment by the government of the time. For example, the UK Fixed-term Parliaments Act 2011 legislated by simple majority for strictly fixed-term parliaments; until then the ruling party could call a general election at any convenient time up to the maximum term of five years. This change would require a constitutional amendment in most nations.\n\nThe presence or lack of entrenchment is a fundamental feature of constitutions. An entrenched constitution cannot be altered in any way by a legislature as part of its normal business concerning ordinary statutory laws, but can only be amended by a different and more onerous procedure. There may be a requirement for a special body to be set up, or the proportion of favourable votes of members of existing legislative bodies may be required to be higher to pass a constitutional amendment than for statutes. The entrenched clauses of a constitution can create different degrees of entrenchment, ranging from simply excluding constitutional amendment from the normal business of a legislature, to making certain amendments either more difficult than normal modifications, or forbidden under any circumstances.\n\nEntrenchment is an inherent feature in most codified constitutions. A codified constitution will incorporate the rules which must be followed for the constitution itself to be changed.\n\nThe US constitution is an example of an entrenched constitution, and the UK constitution is an example of a constitution that is not entrenched (or codified). In some states the text of the constitution may be changed; in others the original text is not changed, and amendments are passed which add to and may override the original text and earlier amendments.\n\nProcedures for constitutional amendment vary between states. In a nation with a federal system of government the approval of a majority of state or provincial legislatures may be required. Alternatively, a national referendum may be required. Details are to be found in the articles on the constitutions of the various nations and federal states in the world.\n\nIn constitutions that are not entrenched, no special procedure is required for modification. Lack of entrenchment is a characteristic of uncodified constitutions; the constitution is not recognised with any higher legal status than ordinary statutes. In the UK, for example laws which modify written or unwritten provisions of the constitution are passed on a simple majority in Parliament. No special \"constitutional amendment\" procedure is required. The principle of parliamentary sovereignty holds that no sovereign parliament may be bound by the acts of its predecessors; and there is no higher authority that can create law which binds Parliament. The sovereign is nominally the head of state with important powers, such as the power to declare war; the uncodified and unwritten constitution removes all these powers in practice.\n\nIn practice democratic governments do not use the lack of entrenchment of the constitution to impose the will of the government or abolish all civil rights, as they could in theory do, but the distinction between constitutional and other law is still somewhat arbitrary, usually following historical principles embodied in important past legislation. For example, several British Acts of Parliament such as the Bill of Rights, Human Rights Act and, prior to the creation of Parliament, Magna Carta are regarded as granting fundamental rights and principles which are treated as almost constitutional. Several rights that in another state might be guaranteed by constitution have indeed been abolished or modified by the British parliament in the early 21st century, including the unconditional right to trial by jury, the right to silence without prejudicial inference, permissible detention before a charge is made extended from 24 hours to 42 days, and the right not to be tried twice for the same offence.\n\nThe strongest level of entrenchment exists in those constitutions that state that some of their most fundamental principles are absolute, i.e. certain articles may not be amended under any circumstances. An amendment of a constitution that is made consistently with that constitution, except that it violates the absolute non-modifiability, can be called an \"unconstitutional constitutional law\". Ultimately it is always possible for a constitution to be overthrown by internal or external force, for example, a revolution (perhaps claiming to be justified by the right to revolution) or invasion.\nIn the Constitution of India, the Supreme Court has created the Doctrine of Basic Structure in Kesavananda Bharti's case (1973) stating that the essential features of the Basic structure cannot be amended by the Parliament. The Court has identified judicial review, independence of Judiciary, free and fair election, core of Fundamental Rights as a few of the essential features which are unamendable. However, the Supreme Court did not identify specific provisions which are in the category of absolute entrenchment. A critical analysis of the Doctrine of Basic Structure appears in Professor M.K. Bhandari's book \"Basic Structure of Indian Constitution - A Critical Reconsideration\".\n\nAn example of absolute unmodifiability is found in the German constitution. Articles 1 and 20 protect human dignity, human rights, democracy, rule of law, federal and social state principles, and the people's right of resistance as a last resort against an attempt to abolish the constitutional order. Article 79, Section 3 states that these principles cannot be changed, even according to the methods of amendment defined elsewhere in the document, until a new constitution comes into effect.\n\nAnother example is the Constitution of Honduras, which has an article stating that the article itself and certain other articles cannot be changed in any circumstances. Article 374 of the Honduras Constitution asserts this unmodifiability, stating, \"It is not possible to reform, in any case, the preceding article, the present article, the constitutional articles referring to the form of government, to the national territory, to the presidential period, the prohibition to serve again as President of the Republic, the citizen who has performed under any title in consequence of which she/he cannot be President of the Republic in the subsequent period.\" This unmodifiability article played an important role in the 2009 Honduran constitutional crisis.\n\nConstitutions also establish where sovereignty is located in the state. There are three basic types of distribution of sovereignty according to the degree of centralisation of power: unitary, federal, and confederal. The distinction is not absolute.\n\nIn a unitary state, sovereignty resides in the state itself, and the constitution determines this. The territory of the state may be divided into regions, but they are not sovereign and are subordinate to the state. In the UK, the constitutional doctrine of Parliamentary sovereignty dictates than sovereignty is ultimately contained at the centre. Some powers have been devolved to Northern Ireland, Scotland, and Wales (but not England). Some unitary states (Spain is an example) devolve more and more power to sub-national governments until the state functions in practice much like a federal state.\n\nA federal state has a central structure with at most a small amount of territory mainly containing the institutions of the federal government, and several regions (called \"states\", \"provinces\", etc.) which compose the territory of the whole state. Sovereignty is divided between the centre and the constituent regions. The constitutions of Canada and the United States establish federal states, with power divided between the federal government and the provinces or states. Each of the regions may in turn have its own constitution (of unitary nature).\n\nA confederal state comprises again several regions, but the central structure has only limited coordinating power, and sovereignty is located in the regions. Confederal constitutions are rare, and there is often dispute to whether so-called \"confederal\" states are actually federal.\n\nTo some extent a group of states which do not constitute a federation as such may by treaties and accords give up parts of their sovereignty to a supranational entity. For example, the countries constituting the European Union have agreed to abide by some Union-wide measures which restrict their absolute sovereignty in some ways, e.g., the use of the metric system of measurement instead of national units previously used.\n\nConstitutions usually explicitly divide power between various branches of government. The standard model, described by the Baron de Montesquieu, involves three branches of government: executive, legislative and judicial. Some constitutions include additional branches, such as an auditory branch. Constitutions vary extensively as to the degree of separation of powers between these branches.\n\nIn presidential and semi-presidential systems of government, department secretaries/ministers are accountable to the president, who has patronage powers to appoint and dismiss ministers. The president is accountable to the people in an election.\n\nIn parliamentary systems, Cabinet Ministers are accountable to Parliament, but it is the prime minister who appoints and dismisses them. In the case of the United Kingdom and other countries with a monarchy, it is the monarch who appoints and dismisses ministers, on the advice of the prime minister. In turn the prime minister will resign if the government loses the confidence of the parliament (or a part of it). Confidence can be lost if the government loses a vote of no confidence or, depending on the country, loses a particularly important vote in parliament, such as vote on the budget. When a government loses confidence, it stays in office until a new government is formed; something which normally but not necessarily required the holding of a general election.\n\nMany constitutions allow the declaration under exceptional circumstances of some form of state of emergency during which some rights and guarantees are suspended. This provision can be and has been abused to allow a government to suppress dissent without regard for human rights—see the article on state of emergency.\n\nItalian political theorist Giovanni Sartori noted the existence of national constitutions which are a facade for authoritarian sources of power. While such documents may express respect for human rights or establish an independent judiciary, they may be ignored when the government feels threatened, or never put into practice. An extreme example was the Constitution of the Soviet Union that on paper supported freedom of assembly and freedom of speech; however, citizens who transgressed unwritten limits were summarily imprisoned. The example demonstrates that the protections and benefits of a constitution are ultimately provided not through its written terms but through deference by government and society to its principles. A constitution may change from being real to a facade and back again as democratic and autocratic governments succeed each other.\n\nConstitutions are often, but by no means always, protected by a legal body whose job it is to interpret those constitutions and, where applicable, declare void executive and legislative acts which infringe the constitution. In some countries, such as Germany, this function is carried out by a dedicated constitutional court which performs this (and only this) function. In other countries, such as Ireland, the ordinary courts may perform this function in addition to their other responsibilities. While elsewhere, like in the United Kingdom, the concept of declaring an act to be unconstitutional does not exist.\n\nA constitutional violation is an action or legislative act that is judged by a constitutional court to be contrary to the constitution, that is, unconstitutional. An example of constitutional violation by the executive could be a public office holder who acts outside the powers granted to that office by a constitution. An example of constitutional violation by the legislature is an attempt to pass a law that would contradict the constitution, without first going through the proper constitutional amendment process.\n\nSome countries, mainly those with uncodified constitutions, have no such courts at all. For example, the United Kingdom has traditionally operated under the principle of parliamentary sovereignty under which the laws passed by United Kingdom Parliament could not be questioned by the courts.\n\n\n\"Judicial philosophies of constitutional interpretation (note: generally specific to United States constitutional law)\"\n\n"}
{"id": "5254", "url": "https://en.wikipedia.org/wiki?curid=5254", "title": "Common law", "text": "Common law\n\nCommon law (also known as judicial precedent or judge-made law or case law) is the body of law developed by judges, courts, and similar tribunals. The defining characteristic of “common law” is that it arises as precedent. In cases where the parties disagree on what the law is, a common law court looks to past precedential decisions of relevant courts, and synthesizes the principles of those past cases as applicable to the current facts. If a similar dispute has been resolved in the past, the court is usually bound to follow the reasoning used in the prior decision (a principle known as \"stare decisis\"). If, however, the court finds that the current dispute is fundamentally distinct from all previous cases (called a \"matter of first impression\"), and legislative statutes are either silent or ambiguous on the question, judges have the authority and duty to resolve the issue (one party or the other has to win, and on disagreements of law, judges make that decision). The court states an opinion that gives reasons for the decision, and those reasons agglomerate with past decisions as precedent to bind future judges and litigants. Common law, as the body of law made by judges, stands in contrast to and on equal footing with statutes which are adopted through the legislative process, and regulations which are promulgated by the executive branch (the interactions are explained later in this article). \"Stare decisis\", the principle that cases should be decided according to consistent principled rules so that similar facts will yield similar results, lies at the heart of all common law systems.\n\nA \"common law system\" is a legal system that gives great precedential weight to common law, and to the style of reasoning inherited from the English legal system. Common law systems originated during the Middle Ages in England, and from there propagated to the colonies of the British Empire. Today, one third of the world's population live in common law jurisdictions or in systems mixed with civil law, including India, the United States (both the federal system and 49 of its 50 states), Pakistan, Nigeria, Bangladesh, Canada (both the federal system and all its provinces except Quebec), Malaysia, Ghana, Australia, Sri Lanka, Hong Kong, Singapore, Burma, Ireland, Israel, New Zealand, Papua New Guinea, Jamaica, Trinidad and Tobago, Cyprus, Antigua and Barbuda, Bahamas, Barbados, Belize, Dominica, Grenada, Marshall Islands, Micronesia, Nauru, Palau, South Africa, Zimbabwe, Cameroon, Namibia, Liberia, Sierra Leone, Botswana, Guyana, and Fiji. Many of these countries have interesting variants on common law systems, noted in the body of the article (and linked in the list above).\n\nThe term \"common law\" has many connotations. The first three set out here are the most-common usages within the legal community. Other connotations from past centuries are sometimes seen, and are sometimes heard in everyday speech.\n\nBlack's Law Dictionary, 10th Ed., gives as definition 1, \"1. The body of law derived from judicial decisions, rather than from statutes or constitutions; [synonym] CASELAW, [contrast] STATUTORY LAW.\" (Black's Law Dictionary is the main legal dictionary used among legal professionals in the U.S.) This usage is given as the first definition in modern legal dictionaries, is characterized as the “most common” usage among legal professionals, and is the usage frequently seen in decisions of courts. In this connotation, \"common law\" distinguishes the authority that promulgated a law. For example, the law in most Anglo-American jurisdictions includes \"statutory law\" enacted by a legislature, \"regulatory law\" (in the U.S.) or “delegated legislation” (in the U.K.) promulgated by executive branch agencies pursuant to delegation of rule-making authority from the legislature, and common law or \"case law\", \"i.e.\", decisions issued by courts (or quasi-judicial tribunals within agencies). This first connotation can be further differentiated into\nPublication of decisions, and indexing, is essential to the development of common law, and thus governments and private publishers publish law reports. While all decisions in common law jurisdictions are precedent (at varying levels and scope as discussed throughout the article on precedent), some become \"leading cases\" or \"landmark decisions\" that are cited especially often.\n\nBlack's 10th Ed., definition 2, differentiates \"common law\" jurisdictions and legal systems from \"civil law\" or \"code\" jurisdictions. Common law systems place great weight on court decisions, which are considered \"law\" with the same force of law as statutes—for nearly a millennium, common law courts have had the authority to make law where no legislative statute exists, and statutes mean what courts interpret them to mean.\n\nBy contrast, in civil law jurisdictions (the legal tradition that prevails, or is combined with common law, in Europe and most non-Islamic, non-common law countries), courts lack authority to act if there is no statute. Judicial precedent is given less interpretive weight, which means that a judge deciding a given case has more freedom to interpret the text of a statute independently, and less predictably. For example, the Napoleonic code expressly forbade French judges to pronounce general principles of law. The role of providing overarching principles, which in common law jurisdictions is provided in judicial opinions, in civil law jurisdictions is filled by giving greater weight to scholarly literature, as explained below.\n\nAs a rule of thumb, common law systems trace their history to England, while civil law systems trace their history through the Napoleonic Code back to the Corpus Juris Civilis of Roman law.\n\nBlack's 10th Ed., definition 4, differentiates \"common law\" (or just \"law\") from \"equity\". Additional legal dictionary cites include. Before 1873, England had two parallel court systems: courts of \"law\" which could only award money damages and recognized only the legal owner of property, and courts of \"equity\" (courts of chancery) that could issue injunctive relief (that is, a court order to a party to do something, give something to someone, or stop doing something) and recognized trusts of property. This split propagated to many of the colonies, including the United States. For most purposes, most jurisdictions, including the U.S. federal system and most states, have merged the two courts. Additionally, even before the separate courts were merged, most courts were permitted to apply both law and equity, though under potentially different procedural law. Nonetheless, the historical distinction between \"law\" and \"equity\" remains important today when the case involves issues such as the following:\nCourts of equity rely on common law principles of binding precedent.\n\nIn addition, there are several historical uses of the term that provide some background as to its meaning.\n\nIn one archaic usage, \"common law\" refers to the pre-Christian system of law, imported by the Saxons to England, and dating to before the Norman conquest, and before there was any consistent law to be applied. This definition is found or alluded to in some internet dictionaries.\n\n\"Common law\" as the term is used today in common law countries contrasts with \"ius commune\". While historically the \"ius commune\" became a secure point of reference in continental European legal systems, in England it was not a point of reference at all.\n\nThe English Court of Common Pleas dealt with lawsuits in which the Monarch had no interest, i.e., between commoners.\n\nBlack's definition 3 is \"3. General law common to a country as a whole, as opposed to special law that has only local application.\" From at least the 11th century and continuing for several centuries after that, there were several different circuits in the royal court system, served by itinerant judges who would travel from town to town dispensing the King's justice in \"assizes\". The term \"common law\" was used to describe the law held in common between the circuits and the different stops in each circuit. The more widely a particular law was recognized, the more weight it held, whereas purely local customs were generally subordinate to law recognized in a plurality of jurisdictions.\n\nA number of misconceptions of the term \"common law\" exist in popular culture and lay (nonlawyer) sources. These tend to mislead more than they illuminate.\n\n\nIn a common law jurisdiction several stages of research and analysis are required to determine \"what the law is\" in a given situation. First, one must ascertain the facts. Then, one must locate any relevant statutes and cases. Then one must extract the principles, analogies and statements by various courts of what they consider important to determine how the next court is likely to rule on the facts of the present case. Later decisions, and decisions of higher courts or legislatures carry more weight than earlier cases and those of lower courts. Finally, one integrates all the lines drawn and reasons given, and determines \"what the law is\". Then, one applies that law to the facts.\n\nIn practice, common law systems are considerably more complicated than the simplified system described above. The decisions of a court are binding only in a particular jurisdiction, and even within a given jurisdiction, some courts have more power than others. For example, in most jurisdictions, decisions by appellate courts are binding on lower courts in the same jurisdiction, and on future decisions of the same appellate court, but decisions of lower courts are only non-binding persuasive authority. Interactions between common law, constitutional law, statutory law and regulatory law also give rise to considerable complexity.\n\nOliver Wendell Holmes, Jr. cautioned that \"the proper derivation of general principles in both common and constitutional law ... arise gradually, in the emergence of a consensus from a multitude of particularized prior decisions.\" Justice Cardozo noted the \"common law does not work from pre-established truths of universal and inflexible validity to conclusions derived from them deductively,\" but \"[i]ts method is inductive, and it draws its generalizations from particulars.\"\n\nThe common law is more malleable than statutory law. First, common law courts are not absolutely bound by precedent, but can (when extraordinarily good reason is shown) reinterpret and revise the law, without legislative intervention, to adapt to new trends in political, legal and social philosophy. Second, the common law evolves through a series of gradual steps, that gradually works out all the details, so that over a decade or more, the law can change substantially but without a sharp break, thereby reducing disruptive effects. In contrast to common law incrementalism, the legislative process is very difficult to get started, as legislatures tend to delay action until a situation is totally intolerable. For these reasons, legislative changes tend to be large, jarring and disruptive (sometimes positively, sometimes negatively, and sometimes with unintended consequences).\n\nOne example of the gradual change that typifies evolution of the common law is the gradual change in liability for negligence. The traditional common law rule through most of the 19th century was that a plaintiff could not recover for a defendant's negligent production or distribution of a harmful instrumentality unless the two were in privity of contract. Thus, only the immediate purchaser could recover for a product defect, and if a part was built up out of parts from parts manufacturers, the ultimate buyer could not recover for injury caused by a defect in the part. In an 1842 English case, \"Winterbottom v. Wright\", the postal service had contracted with Wright to maintain its coaches. Winterbottom was a driver for the post. When the coach failed and injured Winterbottom, he sued Wright. The \"Winterbottom\" court recognized that there would be \"absurd and outrageous consequences\" if an injured person could sue any person peripherally involved, and knew it had to draw a line somewhere, a limit on the causal connection between the negligent conduct and the injury. The court looked to the contractual relationships, and held that liability would only flow as far as the person in immediate contract (\"privity\") with the negligent party.\n\nA first exception to this rule arose in 1852, in the case of \"Thomas v. Winchester\", when New York's highest court held that mislabeling a poison as an innocuous herb, and then selling the mislabeled poison through a dealer who would be expected to resell it, put \"human life in imminent danger.\" \"Thomas\" relied on this reason to create an exception to the \"privity\" rule. In, 1909, New York held in \"Statler v. Ray Mfg. Co.\" that a coffee urn manufacturer was liable to a person injured when the urn exploded, because the urn \"was of such a character inherently that, when applied to the purposes for which it was designed, it was liable to become a source of great danger to many people if not carefully and properly constructed.\"\n\nYet the privity rule survived. In \"Cadillac Motor Car Co. v. Johnson\", (decided in 1915 by the federal appeals court for New York and several neighboring states), the court held that a car owner could not recover for injuries from a defective wheel, when the automobile owner had a contract only with the automobile dealer and not with the manufacturer, even though there was \"no question that the wheel was made of dead and ‘dozy‘ wood, quite insufficient for its purposes.\" The \"Cadillac\" court was willing to acknowledge that the case law supported exceptions for \"an article dangerous in its nature or likely to become so in the course of the ordinary usage to be contemplated by the vendor.\" However, held the \"Cadillac\" court, \"one who manufactures articles dangerous only if defectively made, or installed, e.g., tables, chairs, pictures or mirrors hung on the walls, carriages, automobiles, and so on, is not liable to third parties for injuries caused by them, except in case of willful injury or fraud,\"\nFinally, in the famous case of \"MacPherson v. Buick Motor Co.\", in 1916, Judge Benjamin Cardozo for New York's highest court pulled a broader principle out of these predecessor cases. The facts were almost identical to \"Cadillac\" a year earlier: a wheel from a wheel manufacturer was sold to Buick, to a dealer, to MacPherson, and the wheel failed, injuring MacPherson. Judge Cardozo held:\n\nCardozo's new \"rule\" exists in no prior case, but is inferrable as a synthesis of the \"thing of danger\" principle stated in them, merely extending it to \"foreseeable danger\" even if \"the purposes for which it was designed\" were not themselves \"a source of great danger.\" \"MacPherson\" takes some care to present itself as foreseeable progression, not a wild departure. Cardozo continues to adhere to the original principle of \"Winterbottom\", that \"absurd and outrageous consequences\" must be avoided, and he does so by drawing a new line in the last sentence quoted above: \"There must be knowledge of a danger, not merely possible, but probable.\" But while adhering to the underlying principle that \"some\" boundary is necessary, \"MacPherson\" overruled the prior common law by rendering the formerly dominant factor in the boundary, that is, the privity formality arising out of a contractual relationship between persons, totally irrelevant. Rather, the most important factor in the boundary would be the nature of the thing sold and the foreseeable uses that downstream purchasers would make of the thing.\n\nThis illustrates two crucial principles that are often not well understood by non-lawyers. (a) The common law evolves, this evolution is in the hands of judges, and judges have \"made law\" for hundreds of years. (b) The reasons given for a decision are often more important in the long run than the outcome in a particular case. This is the reason that judicial opinions are usually quite long, and give rationales and policies that can be balanced with judgment in future cases, rather than the bright-line rules usually embodied in statutes.\n\nAll law systems rely on written publication of the law, so that it is accessible to all. Common law decisions are published in law reports for use by lawyers, courts and the general public.\n\nAfter the American Revolution, Massachusetts became the first state to establish an official Reporter of Decisions. As newer states needed law, they often looked first to the Massachusetts Reports for authoritative precedents as a basis for their own common law. The United States federal courts relied on private publishers until after the Civil War, and only began publishing as a government function in 1874. West Publishing in Minnesota is the largest private-sector publisher of law reports in the United States. Government publishers typically issue only decisions \"in the raw,\" while private sector publishers often add indexing, editorial analysis, and similar finding aids.\n\nIn common law legal systems, the common law is crucial to understanding almost all important areas of law. For example, in England and Wales, in English Canada, and in most states of the United States, the basic law of contracts, torts and property do not exist in statute, but only in common law (though there may be isolated modifications enacted by statute). As another example, the Supreme Court of the United States in 1877, held that a Michigan statute that established rules for solemnization of marriages did not abolish pre-existing common-law marriage, because the statute did not affirmatively require statutory solemnization and was silent as to preexisting common law.\n\nIn almost all areas of the law (even those where there is a statutory framework, such as contracts for the sale of goods, or the criminal law), legislature-enacted statutes generally give only terse statements of general principle, and the fine boundaries and definitions exist only in the interstitial common law. To find out what the precise law is that applies to a particular set of facts, one has to locate precedential decisions on the topic, and reason from those decisions by analogy.\n\nIn common law (as opposed to civil law) jurisdictions, legislatures operate under the assumption that statutes will be interpreted against the backdrop of the pre-existing common law. As the United States Supreme Court explained in \"United States v Texas\", 507 U.S. 529 (1993):\n\nFor example, in most U.S. states, the criminal statutes are primarily codification of pre-existing common law. (Codification is the process of enacting a statute that collects and restates pre-existing law in a single document—when that pre-existing law is common law, the common law remains relevant to the interpretation of these statutes.) In reliance on this assumption, modern statutes often leave a number of terms and fine distinctions unstated—for example, a statute might be very brief, leaving the precise definition of terms unstated, under the assumption that these fine distinctions will be inherited from pre-existing common law. (For this reason, many modern American law schools teach the common law of crime as it stood in England in 1789, because that centuries-old English common law is a necessary foundation to interpreting modern criminal statutes.)\n\nWith the transition from English law, which had common law crimes, to the new legal system under the U.S. Constitution, which prohibited \"ex post facto\" laws at both the federal and state level, the question was raised whether there could be common law crimes in the United States. It was settled in the case of \"United States v. Hudson\" , which decided that federal courts had no jurisdiction to define new common law crimes, and that there must always be a (constitutional) statute defining the offense and the penalty for it.\n\nStill, many states retain selected common law crimes. For example, in Virginia, the definition of the conduct that constitutes the crime of robbery exists only in the common law, and the robbery statute only sets the punishment. Virginia Code section 1-200 establishes the continued existence and vitality of common law principles and provides that \"The common law of England, insofar as it is not repugnant to the principles of the Bill of Rights and Constitution of this Commonwealth, shall continue in full force within the same, and be the rule of decision, except as altered by the General Assembly.\"\n\nBy contrast to statutory codification of common law, some statutes displace common law, for example to create a new cause of action that did not exist in the common law, or to legislatively overrule the common law. An example is the tort of wrongful death, which allows certain persons, usually a spouse, child or estate, to sue for damages on behalf of the deceased. There is no such tort in English common law; thus, any jurisdiction that lacks a wrongful death statute will not allow a lawsuit for the wrongful death of a loved one. Where a wrongful death statute exists, the compensation or other remedy available is limited to the remedy specified in the statute (typically, an upper limit on the amount of damages). Courts generally interpret statutes that create new causes of action narrowly—that is, limited to their precise terms—because the courts generally recognize the legislature as being supreme in deciding the reach of judge-made law unless such statute should violate some \"second order\" constitutional law provision (\"cf\". judicial activism).\n\nWhere a tort is rooted in common law, all traditionally recognized damages for that tort may be sued for, whether or not there is mention of those damages in the current statutory law. For instance, a person who sustains bodily injury through the negligence of another may sue for medical costs, pain, suffering, loss of earnings or earning capacity, mental and/or emotional distress, loss of quality of life, disfigurement and more. These damages need not be set forth in statute as they already exist in the tradition of common law. However, without a wrongful death statute, most of them are extinguished upon death.\n\nIn the United States, the power of the federal judiciary to review and invalidate unconstitutional acts of the federal executive branch is stated in the constitution, Article III sections 1 and 2: \"The judicial Power of the United States, shall be vested in one supreme Court, and in such inferior Courts as the Congress may from time to time ordain and establish. ... The judicial Power shall extend to all Cases, in Law and Equity, arising under this Constitution, the Laws of the United States, and Treaties made, or which shall be made, under their Authority...\" The first famous statement of \"the judicial power\" was \"Marbury v. Madison\", . Later cases interpreted the \"judicial power\" of Article III to establish the power of federal courts to consider or overturn any action of Congress or of any state that conflicts with the Constitution.\n\nThe interactions between decisions of different courts is discussed further in the article on precedent.\n\nThe United States federal courts are divided into twelve regional circuits, each with a circuit court of appeals (plus a thirteenth, the Court of Appeals for the Federal Circuit, which hears appeals in patent cases and cases against the federal government, without geographic limitation). Decisions of one circuit court are binding on the district courts within the circuit and on the circuit court itself, but are only persuasive authority on sister circuits. District court decisions are not binding precedent at all, only persuasive.\n\nMost of the U.S. federal courts of appeal have adopted a rule under which, in the event of any conflict in decisions of panels (most of the courts of appeal almost always sit in panels of three), the earlier panel decision is controlling, and a panel decision may only be overruled by the court of appeals sitting \"en banc\" (that is, all active judges of the court) or by a higher court. In these courts, the older decision remains controlling when an issue comes up the third time.\n\nOther courts, for example, the Court of Customs and Patent Appeals and the Supreme Court, always sit \"en banc\", and thus the \"later\" decision controls. These courts essentially overrule all previous cases in each new case, and older cases survive only to the extent they do not conflict with newer cases. The interpretations of these courts—for example, Supreme Court interpretations of the constitution or federal statutes—are stable only so long as the older interpretation maintains the support of a majority of the court. Older decisions persist through some combination of belief that the old decision is right, and that it is not sufficiently wrong to be overruled.\n\nIn the UK, since 2009, the Supreme Court of the United Kingdom has the authority to overrule and unify decisions of lower courts. From 1966 to 2009, this power lay with the House of Lords, granted by the Practice Statement of 1966.\n\nCanada's system, described below, avoids regional variability of federal law by giving national jurisdiction to both layers of appellate courts.\n\nThe reliance on judicial opinion is a strength of common law systems, and is a significant contributor to the robust commercial systems in the United Kingdom and United States. Because there is reasonably precise guidance on almost every issue, parties (especially commercial parties) can predict whether a proposed course of action is likely to be lawful or unlawful, and have some assurance of consistency. As Justice Brandeis famously expressed it, \"in most matters it is more important that the applicable rule of law be settled than that it be settled right.\" This ability to predict gives more freedom to come close to the boundaries of the law. For example, many commercial contracts are more economically efficient, and create greater wealth, because the parties know ahead of time that the proposed arrangement, though perhaps close to the line, is almost certainly legal. Newspapers, taxpayer-funded entities with some religious affiliation, and political parties can obtain fairly clear guidance on the boundaries within which their freedom of expression rights apply.\n\nIn contrast, in non-common-law countries, and jurisdictions with very weak respect for precedent, fine questions of law are redetermined anew each time they arise, making consistency and prediction more difficult, and procedures far more protracted than necessary because parties cannot rely on written statements of law as reliable guides. In jurisdictions that do not have a strong allegiance to a large body of precedent, parties have less \"a priori\" guidance and must often leave a bigger \"safety margin\" of unexploited opportunities, and final determinations are reached only after far larger expenditures on legal fees by the parties.\n\nThis is the reason for the frequent choice of the law of the State of New York in commercial contracts, even when neither entity has extensive contacts with New York—and remarkably often even when neither party has contacts with the United States. Commercial contracts almost always include a \"choice of law clause\" to reduce uncertainty. Somewhat surprisingly, contracts throughout the world (for example, contracts involving parties in Japan, France and Germany, and from most of the other states of the United States) often choose the law of New York, even where the relationship of the parties and transaction to New York is quite attenuated. Because of its history as the United States' commercial center, New York common law has a depth and predictability not (yet) available in any other jurisdictions of the United States. Similarly, American corporations are often formed under Delaware corporate law, and American contracts relating to corporate law issues (merger and acquisitions of companies, rights of shareholders, and so on.) include a Delaware choice of law clause, because of the deep body of law in Delaware on these issues. On the other hand, some other jurisdictions have sufficiently developed bodies of law so that parties have no real motivation to choose the law of a foreign jurisdiction (for example, England and Wales, and the state of California), but not yet so fully developed that parties with no relationship to the jurisdiction choose that law. Outside the United States, parties that are in different jurisdictions from each other often choose the law of England and Wales, particularly when the parties are each in former British colonies and members of the Commonwealth. The common theme in all cases is that commercial parties seek predictability and simplicity in their contractual relations, and frequently choose the law of a common law jurisdiction with a well-developed body of common law to achieve that result.\n\nLikewise, for litigation of commercial disputes arising out of unpredictable torts (as opposed to the prospective choice of law clauses in contracts discussed in the previous paragraph), certain jurisdictions attract an unusually high fraction of cases, because of the predictability afforded by the depth of decided cases. For example, London is considered the pre-eminent centre for litigation of admiralty cases.\n\nThis is not to say that common law is better in every situation. For example, civil law can be clearer than case law when the legislature has had the foresight and diligence to address the precise set of facts applicable to a particular situation. For that reason, civil law statutes tend to be somewhat more detailed than statutes written by common law legislatures—but, conversely, that tends to make the statute more difficult to read (the United States tax code is an example). Nonetheless, as a practical matter, no civil law legislature can ever address the full spectrum of factual possibilities in the breadth, depth and detail of the case law of the common law courts of even a smaller jurisdiction, and that deeper, more complete body of law provides additional predictability that promotes commerce.\n\nCommon law systems originated during the Middle Ages in England, and from there propagated to the colonies of the British Empire.\n\nIn the late 9th century, Alfred the Great assembled the Doom book (not to be confused with the more-famous Domesday Book from 200 years later), which collected the existing laws of Kent, Wessex, and Mercia, and attempted to blend in the Mosaic code, Christian principles, and Germanic customs dating as far as the 5th century.\n\nBefore the Norman conquest in 1066, justice was administered primarily by what is today known as the county courts (the modern \"counties\" were referred to as \"shires\" in pre-Norman times), presided by the diocesan bishop and the sheriff, exercising both ecclesiastical and civil jurisdiction. While in some sense an early form of jury came to be part of the procedure in the shire courts, the development of the common law grand jury and petty jury came later.\n\nThe main sources for the history of the common law in the Middle Ages are the plea rolls and the Year Books. The plea rolls, which were the official court records for the Courts of Common Pleas and King's Bench, were written in Latin. The rolls were made up in bundles by law term: Hilary, Easter, Trinity, and Michaelmas, or winter, spring, summer, and autumn. They are currently deposited in the UK National Archives, by whose permission images of the rolls for the Courts of Common Pleas, King's Bench, and Exchequer of Pleas, from the 13th century to the 17th, can be viewed online at the Anglo-American Legal Tradition site (The O'Quinn Law Library of the University of Houston Law Center).\n\nThe term \"common law\" originally derives from the 1150s and 1160s, when Henry II of England established the secular English tribunals. The \"common law\" was the law that emerged as \"common\" throughout the realm (as distinct from the various legal codes that preceded it, such as Mercian law, the Danelaw and the law of Wessex) as the king's judges followed each other's decisions to create a unified common law throughout England. From at least the 11th century and continuing for several centuries after that, there were several different circuits in the royal court system, served by itinerant judges who would travel from town to town dispensing the King's justice. The term \"common law\" was used to describe the law held in common between the circuits and the different stops in each circuit. The more widely a particular law was recognized, the more weight it held, whereas purely local customs were generally subordinate to law recognized in a plurality of jurisdictions.\n\nThe doctrine of precedent developed during the 12th and 13th centuries, as the collective judicial decisions that were based in tradition, custom and precedent.\n\nThe form of reasoning used in common law is known as casuistry or case-based reasoning. The common law, as applied in civil cases (as distinct from criminal cases), was devised as a means of compensating someone for wrongful acts known as torts, including both intentional torts and torts caused by negligence, and as developing the body of law recognizing and regulating contracts. The type of procedure practiced in common law courts is known as the adversarial system; this is also a development of the common law.\n\nThe early development of case-law in the thirteenth century has been traced to Bracton's \"On the Laws and Customs of England\" and led to the yearly compilations of court cases known as Year Books, of which the first extant was published in 1268, the same year that Bracton died. The Year Books are known as the law reports of medieval England, and are a principal source for knowledge of the developing legal doctrines, concepts, and methods in the period from the 13th to the 16th centuries, when the common law developed into recognizable form.\n\nIn 1154, Henry II became the first Plantagenet king. Among many achievements, Henry institutionalized common law by creating a unified system of law \"common\" to the country through incorporating and elevating local custom to the national, ending local control and peculiarities, eliminating arbitrary remedies and reinstating a jury system—citizens sworn on oath to investigate reliable criminal accusations and civil claims. The jury reached its verdict through evaluating common local knowledge, not necessarily through the presentation of evidence, a distinguishing factor from today's civil and criminal court systems.\n\nHenry II developed the practice of sending judges from his own central court to hear the various disputes throughout the country. His judges would resolve disputes on an ad hoc basis according to what they interpreted the customs to be. The king's judges would then return to London and often discuss their cases and the decisions they made with the other judges. These decisions would be recorded and filed. In time, a rule, known as \"stare decisis\" (also commonly known as precedent) developed, whereby a judge would be bound to follow the decision of an earlier judge; he was required to adopt the earlier judge's interpretation of the law and apply the same principles promulgated by that earlier judge if the two cases had similar facts to one another. Once judges began to regard each other's decisions to be binding precedent, the pre-Norman system of local customs and law varying in each locality was replaced by a system that was (at least in theory, though not always in practice) common throughout the whole country, hence the name \"common law.\"\n\nHenry II's creation of a powerful and unified court system, which curbed somewhat the power of canonical (church) courts, brought him (and England) into conflict with the church, most famously with Thomas Becket, the Archbishop of Canterbury. The murder of the Archbishop gave rise to a wave of popular outrage against the King. Henry was forced to repeal the disputed laws and to abandon his efforts to hold church members accountable for secular crimes (see also Constitutions of Clarendon).\n\nThe English Court of Common Pleas was established after Magna Carta to try lawsuits between commoners in which the monarch had no interest. Its judges sat in open court in the Great Hall of the king's Palace of Westminster, permanently except in the vacations between the four terms of the Legal year.\n\nJudge-made common law operated as the primary source of law for several hundred years, before Parliament acquired legislative powers to create statutory law. It is important to understand that common law is the older and more traditional source of law, and legislative power is simply a layer applied on top of the older common law foundation. Since the 12th century, courts have had parallel and co-equal authority to make law—\"legislating from the bench\" is a traditional and essential function of courts. Justice Oliver Wendell Holmes, Jr. summarized centuries of history in 1917, \"judges do and must legislate.\" There are legitimate debates on how the powers of courts and legislatures should be balanced. However, a view that courts lack law-making power is historically inaccurate and constitutionally unsupportable.\n\nThe term \"common law\" is often used as a contrast to Roman-derived \"civil law\", and the fundamental processes and forms of reasoning in the two are quite different. Nonetheless, there has been considerable cross-fertilization of ideas, while the two traditions and sets of foundational principles remain distinct.\n\nBy the time of the rediscovery of the Roman law in Europe in the 12th and 13th centuries, the common law had already developed far enough to prevent a Roman law reception as it occurred on the continent. However, the first common law scholars, most notably Glanvill and Bracton, as well as the early royal common law judges, had been well accustomed with Roman law. Often, they were clerics trained in the Roman canon law. One of the first and throughout its history one of the most significant treatises of the common law, Bracton's \"De Legibus et Consuetudinibus Angliae\" (On the Laws and Customs of England), was heavily influenced by the division of the law in Justinian's \"Institutes\". The impact of Roman law had decreased sharply after the age of Bracton, but the Roman divisions of actions into \"in rem\" (typically, actions against a \"thing\" or property for the purpose of gaining title to that property; must be filed in a court where the property is located) and \"in personam\" (typically, actions directed against a person; these can affect a person's rights and, since a person often owns things, his property too) used by Bracton had a lasting effect and laid the groundwork for a return of Roman law structural concepts in the 18th and 19th centuries. Signs of this can be found in Blackstone's \"Commentaries on the Laws of England\", and Roman law ideas regained importance with the revival of academic law schools in the 19th century. As a result, today, the main systematic divisions of the law into property, contract, and tort (and to some extent unjust enrichment) can be found in the civil law as well as in the common law.\n\nThe first attempt at a comprehensive compilation of centuries of common law was by Lord Chief Justice Edward Coke, in his treatise, \"Institutes of the Lawes of England\" in the 17th century.\n\nThe next definitive historical treatise on the common law is \"Commentaries on the Laws of England\", written by Sir William Blackstone and first published in 1765–1769.\n\nA reception statute is a statutory law adopted as a former British colony becomes independent, by which the new nation adopts (i.e. receives) pre-independence English law, to the extent not explicitly rejected by the legislative body or constitution of the new nation. Reception statutes generally consider the English common law dating prior to independence, and the precedent originating from it, as the default law, because of the importance of using an extensive and predictable body of law to govern the conduct of citizens and businesses in a new state. All U.S. states, with the partial exception of Louisiana, have either implemented reception statutes or adopted the common law by judicial opinion.\n\nOther examples of reception statutes in the United States, the states of the U.S., Canada and its provinces, and Hong Kong, are discussed in the reception statute article.\n\nYet, adoption of the common law in the newly-independent nation was not a foregone conclusion, and was controversial. Immediately after the American Revolution, there was widespread distrust and hostility to anything British, and the common law was no exception. Jeffersonians decried lawyers and their common law tradition as threats to the new republic. The Jeffersonians preferred a legislatively-enacted civil law under the control of the political process, rather than the common law developed by judges that—by design—were insulated from the political process. The Federalists believed that the common law was the birthright of Independence: after all, the natural rights to \"life, liberty, and the pursuit of happiness\" were the rights protected by common law. Even advocates for the common law approach noted that it was not an ideal fit for the newly-independent colonies: judges and lawyers alike were severely hindered by a lack of printed legal materials. Before Independence, the most comprehensive law libraries had been maintained by Tory lawyers, and those libraries vanished with the loyalist expatriation, and the ability to print books was limited. Lawyer (later president) John Adams complained that he \"suffered very much for the want of books.\" To bootstrap this most basic need of a common law system—knowable, written law—in 1803, lawyers in Massachusetts donated their books to found a law library. A Jeffersonian newspaper criticized the library, as it would carry forward \"all the old authorities practiced in England for centuries back ... whereby a new system of jurisprudence [will be founded] on the high monarchical system [to] become the Common Law of this Commonwealth... [The library] may hereafter have a very unsocial purpose.\"\n\nWell into the 19th century, ancient maxims played a large role in common law adjudication. Many of these maxims had originated in Roman Law, migrated to England before the introduction of Christianity to the British Isles, and were typically stated in Latin even in English decisions. Many examples are familiar in everyday speech even today, \"One cannot be a judge in one's own cause\" (see Dr. Bonham's Case), rights are reciprocal to obligations, and the like. Judicial decisions and treatises of the 17th and 18th centuries, such at those of Lord Chief Justice Edward Coke, presented the common law as a collection of such maxims.\n\nReliance on old maxims and rigid adherence to precedent, no matter how old or ill-considered, was under full attack by the late 19th century. Oliver Wendell Holmes, Jr. in his famous article, \"The Path of the Law\", commented, \"It is revolting to have no better reason for a rule of law than that so it was laid down in the time of Henry IV. It is still more revolting if the grounds upon which it was laid down have vanished long since, and the rule simply persists from blind imitation of the past.\" Justice Holmes noted that study of maxims might be sufficient for \"the man of the present,\" but \"the man of the future is the man of statistics and the master of economics.\" In an 1880 lecture at Harvard, he wrote:\n\nThe life of the law has not been logic; it has been experience. The felt necessities of the time, the prevalent moral and political theories, intuitions of public policy, avowed or unconscious, even the prejudices which judges share with their fellow men, have had a good deal more to do than the syllogism in determining the rules by which men should be governed. The law embodies the story of a nation's development through many centuries, and it cannot be dealt with as if it contained only the axioms and corollaries of a book of mathematics.\n\nIn the early 20th century, Louis Brandeis, later appointed to the United States Supreme Court, became noted for his use of policy-driving facts and economics in his briefs, and extensive appendices presenting facts that lead a judge to the advocate's conclusion. By this time, briefs relied more on facts than on Latin maxims.\n\nReliance on old maxims is now deprecated. Common law decisions today reflect both precedent and policy judgment drawn from economics, the social sciences, business, decisions of foreign courts, and the like. The degree to which these external factors \"should\" influence adjudication is the subject of active debate, but it is indisputable that judges \"do\" draw on experience and learning from everyday life, from other fields, and from other jurisdictions.\n\nAs early as the 15th century, it became the practice that litigants who felt they had been cheated by the common-law system would petition the King in person. For example, they might argue that an award of damages (at common law (as opposed to equity)) was not sufficient redress for a trespasser occupying their land, and instead request that the trespasser be evicted. From this developed the system of equity, administered by the Lord Chancellor, in the courts of chancery. By their nature, equity and law were frequently in conflict and litigation would frequently continue for years as one court countermanded the other, even though it was established by the 17th century that equity should prevail.\n\nIn England, courts of law (as opposed to equity) were combined with courts of equity by the Judicature Acts of 1873 and 1875, with equity prevailing in case of conflict.\n\nIn the United States, parallel systems of law (providing money damages, with cases heard by a jury upon either party's request) and equity (fashioning a remedy to fit the situation, including injunctive relief, heard by a judge) survived well into the 20th century. The United States federal courts procedurally separated law and equity: the same judges could hear either kind of case, but a given case could only pursue causes in law or in equity, and the two kinds of cases proceeded under different procedural rules. This became problematic when a given case required both money damages and injunctive relief. In 1937, the new Federal Rules of Civil Procedure combined law and equity into one form of action, the \"civil action.\" Fed.R.Civ.P. . The distinction survives to the extent that issues that were \"common law (as opposed to equity)\" as of 1791 (the date of adoption of the Seventh Amendment) are still subject to the right of either party to request a jury, and \"equity\" issues are decided by a judge.\n\nDelaware, Mississippi, and Tennessee still have separate courts of law and equity, for example, the Court of Chancery. In many states there are separate divisions for law and equity within one court.\n\nFor centuries, through the 19th century, the common law recognized only specific forms of action, and required very careful drafting of the opening pleading (called a writ) to slot into exactly one of them: Debt, Detinue, Covenant, Special Assumpsit, General Assumpsit, Trespass, Trover, Replevin, Case (or Trespass on the Case), and Ejectment. To initiate a lawsuit, a pleading had to be drafted to meet myriad technical requirements: correctly categorizing the case into the correct legal pigeonhole (pleading in the alternative was not permitted), and using specific \"magic words\" encrusted over the centuries. Under the old common law pleading standards, a suit by a \"pro se\" (\"for oneself,\" without a lawyer) party was all but impossible, and there was often considerable procedural jousting at the outset of a case over minor wording issues.\n\nOne of the major reforms of the late 19th century and early 20th century was the abolition of common law pleading requirements. A plaintiff can initiate a case by giving the defendant \"a short and plain statement\" of facts that constitute an alleged wrong. This reform moved the attention of courts from technical scrutiny of words to a more rational consideration of the facts, and opened access to justice far more broadly.\n\nThe main alternative to the common law system is the civil law system, which is used in Continental Europe, and most of the rest of the world.\n\nThe primary contrast between the two systems is the role of written decisions and precedent.\n\nIn common law jurisdictions, nearly every case that presents a \"bona fide\" disagreement on the law is resolved in a written opinion. In contrast, civil law decisions typically do not include explanatory opinions.\n\nIn common law systems, a single decided case is binding common law (connotation 1) to the same extent as statute or regulation, under the principle of \"stare decisis\". In contrast, in civil law systems, individual decisions have only advisory, not binding effect. In civil law systems, case law only acquires weight when a long series of cases use consistent reasoning, called \"jurisprudence constante\". Civil law lawyers consult case law to obtain their best prediction of how a court will rule, but comparatively, civil law judges are less bound to follow it.\n\nFor that reason, statutes in civil law systems are more comprehensive, detailed, and continuously updated, covering all matters capable of being brought before a court.\n\nCommon law systems tend to give more weight to separation of powers between the judicial branch and the executive branch (which promulgates regulatory law, called \"administrative law\" in civil law systems). In contrast, civil law systems often allow individual officials to exercise both powers.\n\nCommon law courts usually use an adversarial system, in which two sides present their cases to a neutral judge. In contrast, civil law systems usually use an inquisitorial system in which an examining magistrate serves two roles by developing the evidence and arguments for one side and then the other during the investigation phase.\n\nThe examining magistrate then presents the dossier detailing his or her findings to the president of the bench that will adjudicate on the case where it has been decided that a trial shall be conducted. Therefore, the president of the bench's view of the case is not neutral and may be biased while conducting the trial after the reading of the dossier. Unlike the common law proceedings, the president of the bench in the inquisitorial system is not merely an umpire and is entitled to directly interview the witnesses or express comments during the trial, as long as he or she does not express his or her view on the guilt of the accused.\n\nThe proceeding in the inquisitorial system is essentially by writing. Most of the witnesses would have given evidence in the investigation phase and such evidence will be contained in the dossier under the form of police reports. In the same way, the accused would have already put his or her case at the investigation phase but he or she will be free to change her or his evidence at trial. Whether the accused pleads guilty or not, a trial will be conducted. Unlike the adversarial system, the conviction and sentence to be served (if any) will be released by the trial jury together with the president of the trial bench, following their common deliberation.\n\nThere are many exceptions in both directions. For example, most proceedings before U.S. federal and state agencies are inquisitorial in nature, at least the initial stages (\"e.g.\", a patent examiner, a social security hearing officer, and so on), even though the law to be applied is developed through common law processes.\n\nThe role of the legal academy presents a significant \"cultural\" difference between common law (connotation 2) and civil law jurisdictions.\n\nIn common law jurisdictions, legal treatises compile common law decisions and state overarching principles that (in the author's opinion) explain the results of the cases. However, in common law jurisdictions, treatises are not the law, and lawyers and judges tend to use these treatises as only \"finding aids\" to locate the relevant cases. In common law jurisdictions, scholarly work is seldom cited as authority for what the law is. When common law courts rely on scholarly work, it is almost always only for factual findings, policy justification, or the history and evolution of the law, but the court's legal conclusion is reached through analysis of relevant statutes and common law, seldom scholarly commentary.\n\nIn contrast, in civil law jurisdictions, courts give the writings of law professors significant weight, partly because civil law decisions traditionally were very brief, sometimes no more than a paragraph stating who wins and who loses. The rationale had to come from somewhere else: the academy often filled that role.\n\nThe contrast between civil law and common law legal systems has become increasingly blurred, with the growing importance of jurisprudence (similar to case law but not binding) in civil law countries, and the growing importance of statute law and codes in common law countries.\nExamples of common law being replaced by statute or codified rule in the United States include criminal law (since 1812, U.S. federal courts and most but not all of the States have held that criminal law must be embodied in statute if the public is to have fair notice), commercial law (the Uniform Commercial Code in the early 1960s) and procedure (the Federal Rules of Civil Procedure in the 1930s and the Federal Rules of Evidence in the 1970s). But note that in each case, the statute sets the general principles, but the interstitial common law process determines the scope and application of the statute.\n\nAn example of convergence from the other direction is shown in \"Srl CILFIT and Lanificio di Gavardo SpA v Ministry of Health\", in which the European Court of Justice held that questions it has already answered need not be resubmitted. This brought in a distinctly common law principle into an essentially civil law jurisdiction.\n\nThe former Soviet Bloc and other Socialist countries used a Socialist law system.\n\nMuch of the Muslim world uses Sharia (also called Islamic law).\n\nThe common law constitutes the basis of the legal systems of: England and Wales and Northern Ireland in the UK, Ireland, the United States (both the federal system and the individual states (with the partial exception of Louisiana)), Canada (both federal and the individual provinces (except Quebec)), Australia (both federal and individual states), Kenya, New Zealand, South Africa, India, Myanmar, Malaysia, Bangladesh, Brunei, Pakistan, Singapore, Hong Kong, Antigua and Barbuda, Barbados, Bahamas, Belize, Dominica, Grenada, Jamaica, St Vincent and the Granadines, Saint Kitts and Nevis, Trinidad and Tobago, and many other generally English-speaking countries or Commonwealth countries (except the UK's Scotland, which is bijuridicial, and Malta). Essentially, every country that was colonised at some time by England, Great Britain, or the United Kingdom uses common law except those that were formerly colonised by other nations, such as Quebec (which follows the law of France in part), South Africa and Sri Lanka (which follow Roman Dutch law), where the prior civil law system was retained to respect the civil rights of the local colonists. Guyana and Saint Lucia have mixed Common Law and Civil Law systems.\n\nScotland is often said to use the civil law system, but it has a unique system that combines elements of an uncodified civil law dating back to the Corpus Juris Civilis with an element of its own common law long predating the Treaty of Union with England in 1707 (see Legal institutions of Scotland in the High Middle Ages), founded on the customary laws of the tribes residing there. Historically, Scottish common law differed in that the use of \"precedent\" was subject to the courts' seeking to discover the principle that justifies a law rather than searching for an example as a \"precedent\", and principles of natural justice and fairness have always played a role in Scots Law. From the 19th century, the Scottish approach to precedent developed into a \"stare decisis\" akin to that already established in England thereby reflecting a narrower, more modern approach to the application of case law in subsequent instances. This is not to say that the substantive rules of the common laws of both countries are the same although in many matters (particularly those of UK-wide interest) they are very similar.\n\nScotland shares the Supreme Court (formerly the House of Lords), with England, Wales and Northern Ireland for civil cases; and the Court's decisions are binding throughout the UK for civil cases and throughout England and Wales and Northern Ireland for criminal cases. This has had the effect of homogenising the law in certain areas. For instance, the modern UK law of negligence is based on \"Donoghue v Stevenson\", a case originating in Paisley, Scotland. Scotland maintains a separate criminal law system from the rest of the UK, with the High Court of Justiciary being the final court for criminal appeals.\n\nThe state of New York, which also has a civil law history from its Dutch colonial days, began a codification of its law in the 19th century. The only part of this codification process that was considered complete is known as the Field Code applying to civil procedure. The original colony of New Netherland was settled by the Dutch and the law was also Dutch. When the English captured pre-existing colonies they continued to allow the local settlers to keep their civil law. However, the Dutch settlers revolted against the English and the colony was recaptured by the Dutch. When the English finally regained control of New Netherland they forced, as a punishment unique in the history of the British Empire, the English imposed common law upon all the colonists, including the Dutch. This was problematic, as the patroon system of land holding, based on the feudal system and civil law, continued to operate in the colony until it was abolished in the mid-19th century. The influence of Roman-Dutch law continued in the colony well into the late 19th century. The codification of a law of general obligations shows how remnants of the civil law tradition in New York continued on from the Dutch days.\n\nUnder Louisiana's codified system, the Louisiana Civil Code, private law—that is, substantive law between private sector parties—is based on principles of law from continental Europe, with some common law influences. These principles derive ultimately from Roman law, transmitted through French law and Spanish law, as the state's current territory intersects the area of North America colonized by Spain and by France. Contrary to popular belief, the Louisiana code does not directly derive from the Napoleonic Code, as the latter was enacted in 1804, one year after the Louisiana Purchase. However, the two codes are similar in many respects due to common roots.\n\nLouisiana's criminal law largely rests on English common law. Louisiana's administrative law is generally similar to the administrative law of the U.S. federal government and other U.S. states. Louisiana's procedural law is generally in line with that of other U.S. states, which in turn is generally based on the U.S. Federal Rules of Civil Procedure.\n\nHistorically notable among the Louisiana code's differences from common law is the role of property rights among women, particularly in inheritance gained by widows.\n\nThe U.S. state of California has a system based on common law, but it has codified the law in the manner of the civil law jurisdictions. The reason for the enactment of the California Codes in the 19th century was to replace a pre-existing system based on Spanish civil law with a system based on common law, similar to that in most other states. California and a number of other Western states, however, have retained the concept of community property derived from civil law. The California courts have treated portions of the codes as an extension of the common-law tradition, subject to judicial development in the same manner as judge-made common law. (Most notably, in the case \"Li v. Yellow Cab Co.\", 13 Cal.3d 804 (1975), the California Supreme Court adopted the principle of comparative negligence in the face of a California Civil Code provision codifying the traditional common-law doctrine of contributory negligence.)\n\nThe United States federal government (as opposed to the states) has a variant on a common law system. United States federal courts only act as interpreters of statutes and the constitution by elaborating and precisely defining broad statutory language (connotation 1(b) above), but, unlike state courts, do not act as an independent source of common law.\n\nBefore 1938, the federal courts, like almost all other common law courts, decided the law on any issue where the relevant legislature (either the U.S. Congress or state legislature, depending on the issue), had not acted, by looking to courts in the same system, that is, other federal courts, even on issues of state law, and even where there was no express grant of authority from Congress or the Constitution.\n\nIn 1938, the U.S. Supreme Court in \"Erie Railroad Co. v. Tompkins\" 304 U.S. 64, 78 (1938), overruled earlier precedent, and held \"There is no federal general common law,\" thus confining the federal courts to act only as interpreters of law originating elsewhere. \"E.g.\", \"Texas Industries v. Radcliff\", (without an express grant of statutory authority, federal courts cannot create rules of intuitive justice, for example, a right to contribution from co-conspirators). Post-1938, federal courts deciding issues that arise under state law are required to defer to state court interpretations of state statutes, or reason what a state's highest court would rule if presented with the issue, or to certify the question to the state's highest court for resolution.\n\nLater courts have limited \"Erie\" slightly, to create a few situations where United States federal courts are permitted to create federal common law rules without express statutory authority, for example, where a federal rule of decision is necessary to protect uniquely federal interests, such as foreign affairs, or financial instruments issued by the federal government. \"See, e.g.\", \"Clearfield Trust Co. v. United States\", (giving federal courts the authority to fashion common law rules with respect to issues of federal power, in this case negotiable instruments backed by the federal government); \"see also\" \"International News Service v. Associated Press\", 248 U.S. 215 (1918) (creating a cause of action for misappropriation of \"hot news\" that lacks any statutory grounding); \"but see National Basketball Association v. Motorola, Inc.\", 105 F.3d 841, 843–44, 853 (2d Cir. 1997) (noting continued vitality of \"INS\" \"hot news\" tort under New York state law, but leaving open the question of whether it survives under federal law). Except on Constitutional issues, Congress is free to legislatively overrule federal courts' common law.\n\nMost executive branch agencies in the United States federal government have some adjudicatory authority. To greater or lesser extent, agencies honor their own precedent to ensure consistent results. Agency decision making is governed by the Administrative Procedure Act of 1946.\n\nFor example, the National Labor Relations Board issues relatively few regulations, but instead promulgates most of its substantive rules through common law (connotation 1).\n\nThe law of India, Pakistan, and Bangladesh are largely based on English common law because of the long period of British colonial influence during the period of the British Raj.\n\nAncient India represented a distinct tradition of law, and had an historically independent school of legal theory and practice. The \"Arthashastra\", dating from 400 BCE and the \"Manusmriti\", from 100 CE, were influential treatises in India, texts that were considered authoritative legal guidance. Manu's central philosophy was tolerance and pluralism, and was cited across Southeast Asia. Early in this period, which finally culminated in the creation of the Gupta Empire, relations with ancient Greece and Rome were not infrequent. The appearance of similar fundamental institutions of international law in various parts of the world show that they are inherent in international society, irrespective of culture and tradition. Inter-State relations in the pre-Islamic period resulted in clear-cut rules of warfare of a high humanitarian standard, in rules of neutrality, of treaty law, of customary law embodied in religious charters, in exchange of embassies of a temporary or semi-permanent character.\n\nWhen India became part of the British Empire, there was a break in tradition, and Hindu and Islamic law were supplanted by the common law. After the failed rebellion against the British in 1857, the British Parliament took over control of India from the British East India Company, and British India came under the direct rule of the Crown. The British Parliament passed the Government of India Act of 1858 to this effect, which set up the structure of British government in India. It established in Britain the office of the Secretary of State for India through whom the Parliament would exercise its rule, along with a Council of India to aid him. It also established the office of the Governor-General of India along with an Executive Council in India, which consisted of high officials of the British Government. As a result, the present judicial system of the country derives largely from the British system and has little correlation to the institutions of the pre-British era.\n\nPost-partition, India retained its common law system. Much of contemporary Indian law shows substantial European and American influence. Legislation first introduced by the British is still in effect in modified form today. During the drafting of the Indian Constitution, laws from Ireland, the United States, Britain, and France were all synthesized to produce a refined set of Indian laws. Indian laws also adhere to the United Nations guidelines on human rights law and environmental law. Certain international trade laws, such as those on intellectual property, are also enforced in India.\n\nThe exception to this rule is in the state of Goa, annexed in stages in the 1960s through 1980s. In Goa, a Portuguese uniform civil code is in place, in which all religions have a common law regarding marriages, divorces and adoption.\n\nPost-partition, Pakistan retained its common law system.\n\nPost-partition, Bangladesh retained its common law system.\n\nCanada has separate federal and provincial legal systems. The division of jurisdiction between the federal and provincial Parliaments is specified in the Canadian constitution.\n\nEach province and territory is considered a separate jurisdiction with respect to common law matters. As such, only the provincial legislature may enact legislation to amend private law. Each has its own procedural law, statutorily created provincial courts and superior trial courts with inherent jurisdiction culminating in the Court of Appeal of the province. This is the highest court in provincial jurisdiction, only subject to the Supreme Court of Canada in terms of appeal of their decisions. All but one of the provinces of Canada use a common law system (the exception being Quebec, which uses a civil law system for issues arising within provincial jurisdiction, such as property ownership and contracts).\n\nCanadian Federal Courts operate under a separate system throughout Canada and deal with narrower subject matter than superior courts in provincial jurisdiction. They hear cases reserved for federal jurisdiction by the Canadian constitution, such as immigration, intellectual property, judicial review of federal government decisions, and admiralty. The Federal Court of Appeal is the appellate level court in federal jurisdiction and hears cases in multiple cities, and unlike the United States, the Canadian Federal Court of Appeal is not divided into appellate circuits.\n\nCriminal law is uniform throughout Canada. It is based on the constitution and federal statutory Criminal Code, as interpreted by the Supreme Court of Canada. The administration of justice and enforcement of the criminal code are the responsibilities of the provinces.\n\nCanadian federal statutes must use the terminology of both the common law and civil law for those matters; this is referred to as legislative bijuralism.\n\nNicaragua's legal system is also a mixture of the English Common Law and Civil Law. This situation was brought through the influence of British administration of the Eastern half of the Mosquito Coast from the mid-17th century until about 1894, the William Walker period from about 1855 through 1857, USA interventions/occupations during the period from 1909 to 1933, the influence of USA institutions during the Somoza family administrations (1933 through 1979) and the considerable importation between 1979 and the present of USA culture and institutions.\n\nIsrael has a common law legal system. Its basic principles are inherited from the law of the British Mandate of Palestine and thus resemble those of British and American law, namely: the role of courts in creating the body of law and the authority of the supreme court in reviewing and if necessary overturning legislative and executive decisions, as well as employing the adversarial system. One of the primary reasons that the Israeli constitution remains unwritten is the fear by whatever party holds power that creating a written constitution, combined with the common-law elements, would severely limit the powers of the Knesset (which, following the doctrine of parliamentary sovereignty, holds near-unlimited power).\n\nRoman Dutch Common law is a bijuridical or mixed system of law similar to the common law system in Scotland and Louisiana. Roman Dutch common law jurisdictions include South Africa, Botswana, Lesotho, Namibia, Swaziland, Sri-Lanka and Zimbabwe. Many of these jurisdictions recognise customary law, and in some, such as South Africa the Constitution requires that the common law be developed in accordance with the Bill of Rights. Roman Dutch common law is a development of Roman Dutch law by courts in the Roman Dutch common law jurisdictions. During the Napoleonic wars the Kingdom of the Netherlands adopted the French \"code civil\" in 1809, however the Dutch colonies in the Cape of Good Hope and Sri Lanka, at the time called Ceylon, were seized by the British to prevent them being used as bases by the French Navy. The system was developed by the courts and spread with the expansion of British colonies in Southern Africa. Roman Dutch common law relies on legal principles set out in Roman law sources such as Justinian's Institutes and Digest, and also on the writing of Dutch jurists of the 17th century such as Grotius and Voet. In practice, the majority of decisions rely on recent precedent.\n\nEdward Coke, a 17th-century Lord Chief Justice of the English Court of Common Pleas and a Member of Parliament, wrote several legal texts that collected and integrated centuries of case law. Lawyers in both England and America learned the law from his \"Institutes\" and \"Reports\" until the end of the 18th century. His works are still cited by common law courts around the world.\n\nThe next definitive historical treatise on the common law is \"Commentaries on the Laws of England\", written by Sir William Blackstone and first published in 1765–1769. Since 1979, a facsimile edition of that first edition has been available in four paper-bound volumes. Today it has been superseded in the English part of the United Kingdom by Halsbury's Laws of England that covers both common and statutory English law.\nWhile he was still on the Massachusetts Supreme Judicial Court, and before being named to the U.S. Supreme Court, Justice Oliver Wendell Holmes, Jr. published a short volume called \"The Common Law\", which remains a classic in the field. Unlike Blackstone and the Restatements, Holmes' book only briefly discusses what the law \"is\"; rather, Holmes describes the common law \"process\". Law professor John Chipman Gray's \"The Nature and Sources of the Law\", an examination and survey of the common law, is also still commonly read in U.S. law schools.\n\nIn the United States, Restatements of various subject matter areas (Contracts, Torts, Judgments, and so on.), edited by the American Law Institute, collect the common law for the area. The ALI Restatements are often cited by American courts and lawyers for propositions of uncodified common law, and are considered highly persuasive authority, just below binding precedential decisions. The Corpus Juris Secundum is an encyclopedia whose main content is a compendium of the common law and its variations throughout the various state jurisdictions.\n\nScots \"common law\" covers matters including murder and theft, and has sources in custom, in legal writings and previous court decisions. The legal writings used are called \"Institutional Texts\" and come mostly from the 17th, 18th and 19th centuries. Examples include Craig, \"Jus Feudale\" (1655) and Stair, \"The Institutions of the Law of Scotland\" (1681).\n\n\n\n\n\n\n\n\n\n"}
{"id": "5255", "url": "https://en.wikipedia.org/wiki?curid=5255", "title": "Civil law", "text": "Civil law\n\nCivil law may refer to:\n\n"}
{"id": "5257", "url": "https://en.wikipedia.org/wiki?curid=5257", "title": "Court of appeals (disambiguation)", "text": "Court of appeals (disambiguation)\n\nA court of appeals is an appellate court generally.\n\nCourt of Appeals may refer to:\n\n\n"}
{"id": "5259", "url": "https://en.wikipedia.org/wiki?curid=5259", "title": "Common descent", "text": "Common descent\n\nCommon descent describes how, in evolutionary biology, a group of organisms share a most recent common ancestor. There is evidence of common descent that all life on Earth is descended from the last universal common ancestor (LUCA). In July 2016, scientists reported identifying a set of 355 genes from the LUCA of all organisms living on Earth.\n\nCommon ancestry between organisms of different species arises during speciation, in which new species are established from a single ancestral population. Organisms which share a more recent common ancestor are more closely related. The most recent common ancestor of all currently living organisms is the last universal ancestor, which lived about 3.9 billion years ago. The two earliest evidences for life on Earth are graphite found to be biogenic in 3.7 billion-year-old metasedimentary rocks discovered in western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia. All currently living organisms on Earth share a common genetic heritage (universal common descent), with each being the descendant from a single original species, though the suggestion of substantial horizontal gene transfer during early evolution has led to questions about monophyly of life.\n\nUniversal common descent through an evolutionary process was first proposed by the English naturalist Charles Darwin in \"On the Origin of Species\" (1859), which concluded: \"There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one; and that, whilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved.\"\n\nIn the 1740s, French mathematician Pierre Louis Maupertuis made the first known suggestion in a series of essays that all organisms may have had a common ancestor, and that they had diverged through random variation and natural selection. In \"Essai de cosmologie\" (1750), Maupertuis noted:\n\nMay we not say that, in the fortuitous combination of the productions of Nature, since only those creatures \"could\" survive in whose organizations a certain degree of adaptation was present, there is nothing extraordinary in the fact that such adaptation is actually found in all these species which now exist? Chance, one might say, turned out a vast number of individuals; a small proportion of these were organized in such a manner that the animals' organs could satisfy their needs. A much greater number showed neither adaptation nor order; these last have all perished... Thus the species which we see today are but a small part of all those that a blind destiny has produced.\n\nIn 1790, Immanuel Kant wrote in \"Kritik der Urteilskraft\" (\"Critique of Judgement\") that the analogy of animal forms implies a common original type, and thus a common parent.\n\nIn 1794, Charles Darwin's grandfather, Erasmus Darwin, asked:\n[W]ould it be too bold to imagine, that in the great length of time, since the earth began to exist, perhaps millions of ages before the commencement of the history of mankind, would it be too bold to imagine, that all warm-blooded animals have arisen from one living filament, which endued with animality, with the power of acquiring new parts attended with new propensities, directed by irritations, sensations, volitions, and associations; and thus possessing the faculty of continuing to improve by its own inherent activity, and of delivering down those improvements by generation to its posterity, world without end?\n\nCharles Darwin's views about common descent, as expressed in \"On the Origin of Species\", were that it was possible that there was only one progenitor for all life forms:\n\nTherefore I should infer from analogy that probably all the organic beings which have ever lived on this earth have descended from some one primordial form, into which life was first breathed.\n\nAll known forms of life are based on the same fundamental biochemical organization: genetic information encoded in DNA, transcribed into RNA, through the effect of protein- and RNA-enzymes, then translated into proteins by (highly similar) ribosomes, with ATP, NADPH and others as energy sources, etc. Furthermore, the genetic code (the \"translation table\" according to which DNA information is translated into proteins) is nearly identical for all known lifeforms, from bacteria and archaea to animals and plants. The universality of this code is generally regarded by biologists as definitive evidence in favor of the theory of universal common descent. Analysis of the small differences in the genetic code has also provided support for universal common descent. An example would be Cytochrome c which most organisms actually share. A statistical comparison of various alternative hypotheses has shown that universal common ancestry is significantly more probable than models involving multiple origins.\n\nSimilarities which have no adaptive relevance cannot be explained by convergent evolution, and therefore they provide compelling support for the theory of universal common descent.\n\nSuch evidence has come from two areas: amino acid sequences and DNA sequences. Proteins with the same three-dimensional structure need not have identical amino acid sequences; any irrelevant similarity between the sequences is evidence for common descent. In certain cases, there are several codons (DNA triplets) that code for the same amino acid. Thus, if two species use the same codon at the same place to specify an amino acid that can be represented by more than one codon, that is evidence for a recent common ancestor.\n\nThe universality of many aspects of cellular life is often pointed to as supportive evidence to the more compelling evidence listed above. These similarities include the energy carrier adenosine triphosphate (ATP), and the fact that all amino acids found in proteins are left-handed. It is, however, possible that these similarities resulted because of the laws of physics and chemistry, rather than universal common descent and therefore resulted in convergent evolution.\n\nAnother important piece of evidence is that it is possible to construct detailed phylogenetic trees (i.e., \"genealogic trees\" of species) mapping out the proposed divisions and common ancestors of all living species. In 2010, Douglas L. Theobald published a statistical analysis of available genetic data, mapping them to phylogenetic trees, that gave \"strong quantitative support, by a formal test, for the unity of life.\" It should be noted, however, that the \"formal test\" was criticised for not including consideration of convergent evolution, and Theobald has defended the method against this claim.\n\nTraditionally, these trees have been built using morphological methods, such as appearance, embryology, etc. Recently, it has been possible to construct these trees using molecular data, based on similarities and differences between genetic and protein sequences. All these methods produce essentially similar results, even though most genetic variation has no influence over external morphology. That phylogenetic trees based on different types of information agree with each other is strong evidence of a real underlying common descent.\n\nArtificial selection demonstrates the diversity that can exist among organisms that share a relatively recent common ancestor. In artificial selection, humans selectively direct the breeding of one species at each generation, allowing only those organisms that exhibit desired characteristics to reproduce. These characteristics become increasingly well-developed in successive generations. Artificial selection was successful long before science discovered the genetic basis.\n\nThe diversity of domesticated dogs is an example of the power of artificial selection. All breeds share common ancestry, having descended from wolves. Humans selectively bred them to enhance specific characteristics, such as color and length or body size. This created a range of breeds that include the Chihuahua, Great Dane, Basset Hound, Pug, and Poodle. Wild wolves, which did not undergo artificial selection, are relatively uniform in comparison.\n\nEarly farmers cultivated many popular vegetables from the \"Brassica oleracea\" (wild cabbage) by artificially selecting for certain attributes. Common vegetables such as cabbage, kale, broccoli, cauliflower, kohlrabi and Brussels sprouts are all descendants of the wild cabbage plant. Brussels sprouts were created by artificially selecting for large bud size. Broccoli was bred by selecting for large flower stalks. Cabbage was created by selecting for short petioles. Kale was bred by selecting for large leaves.\n\nNatural selection is the evolutionary process by which heritable traits that increase an individual's fitness become more common, and heritable traits that decrease an individual's fitness become less common.\n\nDuring his studies on the Galápagos Islands, Charles Darwin observed 13 species of finches that are closely related and differ most markedly in the shape of their beaks. The beak of each species is suited to the food available in its particular environment, suggesting that beak shapes evolved by natural selection. Large beaks were found on the islands where the primary source of food for the finches are nuts and therefore the large beaks allowed the birds to be better equipped for opening the nuts and staying well nourished. Slender beaks were found on the finches which found insects to be the best source of food on the island they inhabited; their slender beaks allowed the birds to be better equipped for pulling out the insects from their tiny hiding places. The finch is also found on the main land and it is thought that they migrated to the islands and began adapting to their environment through natural selection.\n\n"}
{"id": "5261", "url": "https://en.wikipedia.org/wiki?curid=5261", "title": "Celtic music", "text": "Celtic music\n\nCeltic music is a broad grouping of music genres that evolved out of the folk music traditions of the Celtic people of Western Europe. It refers to both orally-transmitted traditional music and recorded music and the styles vary considerably to include everything from \"trad\" (traditional) music to a wide range of hybrids.\n\n\"Celtic music\" means two things mainly. First, it is the music of the people that identify themselves as Celts. Secondly, it refers to whatever qualities may be unique to the music of the Celtic Nations. Many notable Celtic musicians such as Alan Stivell and Paddy Moloney claim that the different Celtic music genres have a lot in common. \n\nThese following melodic practices may be used widely across the different variants of Celtic Music:\n\n\nThese two latter usage patterns may simply be remnants of formerly widespread melodic practices.\n\nOften, the term \"Celtic music\" is applied to the music of Ireland and Scotland because both lands have produced well-known distinctive styles which actually have genuine commonality and clear mutual influences. The definition is further complicated by the fact that Irish independence has allowed Ireland to promote 'Celtic' music as a specifically Irish product. However, these are modern geographical references to a people who share a common Celtic ancestry and consequently, a common musical heritage.\n\nThese styles are known because of the importance of Irish and Scottish people in the English speaking world, especially in the United States, where they had a profound impact on American music, particularly bluegrass and country music. The music of Wales, Cornwall, the Isle of Man, Brittany, Galicia, Cantabria and Asturias (Spain) and Portugal are also considered Celtic music, the tradition being particularly strong in Brittany, where Celtic festivals large and small take place throughout the year, and in Wales, where the ancient eisteddfod tradition has been revived and flourishes. Additionally, the musics of ethnically Celtic peoples abroad are vibrant, especially in Canada and the United States. In Canada the provinces of Atlantic Canada are known for being a home of Celtic music, most notably on the islands of Newfoundland, Cape Breton and Prince Edward Island. The traditional music of Atlantic Canada is heavily influenced by the Irish, Scottish and Acadian ethnic makeup of much of the region's communities. In some parts of Atlantic Canada, such as Newfoundland, Celtic music is as or more popular than in the old country. Further, some older forms of Celtic music that are rare in Scotland and Ireland today, such as the practice of accompanying a fiddle with a piano, or the Gaelic spinning songs of Cape Breton remain common in the Maritimes. Much of the music of this region is Celtic in nature, but originates in the local area and celebrates the sea, seafaring, fishing and other primary industries.\n\nIn \"Celtic Music: A Complete Guide\", June Skinner Sawyers acknowledges six Celtic nationalities divided into two groups according to their linguistic heritage. The Q-Celtic nationalities are the Irish, Scottish and Manx peoples, while the P-Celtic groups are the Cornish, Bretons and Welsh peoples. Musician Alan Stivell uses a similar dichotomy, between the Gaelic (Irish/Scottish/Manx) and the Brythonic (Breton/Welsh/Cornish) branches, which differentiate \"mostly by the extended range (sometimes more than two octaves) of Irish and Scottish melodies and the closed range of Breton and Welsh melodies (often reduced to a half-octave), and by the frequent use of the pure pentatonic scale in Gaelic music.\"\n\nThere is also tremendous variation between \"Celtic\" regions. Ireland, Scotland, Brittany and Wales have living traditions of language and music, and there has been a recent major revival of interest in Celtic heritage in Cornwall and the Isle of Man. Galicia has a Celtic language revival movement to revive the Q-Celtic \"Gallaic language\" used into Roman times. Most of the Iberian Peninsula had a similar Celtic language in pre-Roman times. A Brythonic language was used in parts of Galicia and Asturias into early Medieval times brought by Britons fleeing the Anglo-Saxon invasions via Brittany. The Romance language currently spoken in Galicia, Galician (Galego) is closely related to the Portuguese language used mainly in Brazil and Portugal. Galician music is claimed to be \"Celtic\". The same is true of the music of Asturias, Cantabria, and that of Northern Portugal (some say even traditional music from Central Portugal can be labeled Celtic).\n\nBreton artist Alan Stivell was one of the earliest musicians to use the word \"Celtic\" and \"Keltia\" in his marketing materials, starting in the early 1960s as part of the worldwide folk music revival of that era with the term quickly catching on with other artists worldwide. Today, the genre is well established and incredibly diverse.\n\nThere are musical genres and styles specific to each Celtic country, due in part to the influence of individual song traditions and the characteristics of specific languages:\n\nThe modern Celtic music scene involves a large number of music festivals, as it has traditionally. Some of the most prominent festivals focused solely on music include:\n\n\nThe oldest musical tradition which fits under the label of Celtic fusion originated in the rural American south in the early colonial period and incorporated Scottish, Scots-Irish, Irish, Welsh, English, and African influences. Variously referred to as roots music, American folk music, or old-time music, this tradition has exerted a strong influence on all forms of American music, including country, blues, and rock and roll. In addition to its lasting effects on other genres, it marked the first modern large-scale mixing of musical traditions from multiple ethnic and religious communities within the Celtic diaspora.\n\nIn the 1960s several bands put forward modern adaptations of Celtic music pulling influences from several of the Celtic nations at once to create a modern pan-celtic sound. A few of those include bagadoù (Breton pipe bands), Fairport Convention, Pentangle, Steeleye Span and Horslips.\n\nIn the 1970s Clannad made their mark initially in the folk and traditional scene, and then subsequently went on to bridge the gap between traditional Celtic and pop music in the 1980s and 1990s, incorporating elements from new-age, smooth jazz, and folk rock. Traces of Clannad's legacy can be heard in the music of many artists, including Enya, Donna Taggart, Altan, Capercaillie, The Corrs, Loreena McKennitt, Anúna, Riverdance and U2. The solo music of Clannad's lead singer, Moya Brennan (often referred to as the First Lady of Celtic Music) has further enhanced this influence.\n\nLater, beginning in 1982 with The Pogues' invention of Celtic folk-punk and Stockton's Wing blend of Irish traditional and Pop, Rock and Reggae, there has been a movement to incorporate Celtic influences into other genres of music. Bands like Flogging Molly, Black 47, Dropkick Murphys, The Young Dubliners, The Tossers introduced a hybrid of Celtic rock, punk, reggae, hardcore and other elements in the 1990s that has become popular with Irish-American youth.\n\nToday there are Celtic-influenced subgenres of virtually every type of popular music including electronica, rock, metal, punk, hip hop, reggae, new-age, Latin, Andean and pop. Collectively these modern interpretations of Celtic music are sometimes referred to as Celtic fusion.\n\nOutside of America, the first deliberate attempts to create a \"Pan-Celtic music\" were made by the Breton Taldir Jaffrennou, having translated songs from Ireland, Scotland, and Wales into Breton between the two world wars. One of his major works was to bring \"Hen Wlad Fy Nhadau\" (the Welsh national anthem) back in Brittany and create lyrics in Breton. Eventually this song became \"\"Bro goz va zadoù\"\" (\"Old land of my fathers\") and is the most widely accepted Breton anthem. In the 70s, the Breton Alan Cochevelou (future Alan Stivell) began playing a mixed repertoire from the main Celtic countries on the Celtic harp his father created. \nProbably the most successful all inclusive Celtic music composition in recent years is Shaun Daveys composition 'The Pilgrim'. This suite depicts the journey of St. Colum Cille through the Celtic nations of Ireland, Scotland, the Isle of Man, Wales, Cornwall, Brittany and Galicia. The suite which includes a Scottish pipe band, Irish and Welsh harpists, Galician gaitas, Irish uilleann pipes, the bombardes of Brittany, two vocal soloists and a narrator is set against a background of a classical orchestra and a large choir.\n\nModern music may also be termed \"Celtic\" because it is written and recorded in a Celtic language, regardless of musical style. Many of the Celtic languages have experienced resurgences in modern years, spurred on partly by the action of artists and musicians who have embraced them as hallmarks of identity and distinctness. In 1971, the Irish band \"Skara Brae\" recorded its only LP (simply called \"Skara Brae\"), all songs in Irish. In 1978 Runrig recorded an album in Scottish Gaelic. In 1992 Capercaillie recorded \"A Prince Among Islands\", the first Scottish Gaelic language record to reach the UK top 40. In 1996, a song in Breton represented France in the 41st Eurovision Song Contest, the first time in history that France had a song without a word in French. Since about 2005, Oi Polloi (from Scotland) have recorded in Scottish Gaelic. Mill a h-Uile Rud (a Scottish Gaelic punk band from Seattle) recorded in the language in 2004.\n\nSeveral contemporary bands have Welsh language songs, such as Ceredwen, which fuses traditional instruments with trip hop beats, the Super Furry Animals, Fernhill, and so on (see the Music of Wales article for more Welsh and Welsh-language bands). The same phenomenon occurs in Brittany, where many singers record songs in Breton, traditional or modern (hip hop, rap, and so on.).\n\n\n"}
{"id": "5267", "url": "https://en.wikipedia.org/wiki?curid=5267", "title": "Constellation", "text": "Constellation\n\nA constellation is simply defined as a recognisable group of conspicuous stars that are placed together as imaginary patterns or outlines on the celestial sphere. They are usually typified or embodies human-made constructions and often represent animals, mythological people or gods, mythological creatures, or manufactured devices.\n\nA true origin for the earliest constellations likely dates back to prehistory, whose unknown creators collectively used them to relate important stories of either their beliefs, experiences, creation, or mythology. As such, different cultures and countries often adopted their own set of constellations outlines, some that persisted into the early 20th century. Adoption of many constellations has significantly changed throughout the centuries. Many have varied in size or shape, while some becoming popular then dropping into obscurity, or others being traditionally used only by various cultures or single nations.\n\nIn the northern hemisphere, the Western-traditional constellations were the \"forty-eight\" Greek classical patterns, as stated in both Aratus work known as the \"Phenomena\" or Ptolemy's \"Almagest\" — though their true existence probably predates these constellation names by several centuries. Newer constellations in the far southern sky were added much later during 15th and mid-18th century, when European explorers began travelling to the southern hemisphere. Twelve important constellations are assigned to the zodiac, where the Sun, Moon, and planets all follow the ecliptic. The origins of the zodiac probably date back into prehistory, whose astrological divisions became prominent around 400BCE within Babylonian or Chaldean astronomy. \n\nBased on the important astronomical need to formally define the placement of all celestial objects in the entire sky, the International Astronomical Union (IAU) ratified and recognized the 88 modern constellations in 1928. Therefore, any given point in a celestial coordinate system can now be unambiguously assigned to any modern constellation. Furthermore, many astronomical naming systems give the constellation where a given celestial object is found along with a designation in order to convey an approximate idea of its location in the sky. e.g. The Flamsteed designation for bright stars consists of a number and the genitive form of the constellation name.\n\nThe term constellation may also refer to the stars within or across the boundaries of constellations. Notable groupings of stars that do not form the modern constellations are usually called asterisms. e.g. The Pleiades, The Hyades, False Cross, or Venus' Mirror in the constellation of Orion. \n\nThe word \"constellation\" seems to come from the Late Latin term , which can be translated as \"set of stars\", and came into use in English during the 14th century. \nThe Ancient Greek word for constellation was \"\". A more modern astronomical sense of the term \"constellation\" is simply as a recognisable pattern of stars whose appearance is associated with mythological characters or creatures, or earthbound animals, or objects. It may also specifically denote the officially recognised 88 named constellations used today.\nColloquial usage does not draw any sharp distinction between \"constellations\" or the many smaller \"asterisms\" (pattern of stars), yet the modern accept astronomical constellations employs such a distinction. e.g. The Pleiades and The Hyades are both asterisms, and each lie within the boundaries of the constellation of Taurus. Another example is the popular northern asterism known as the Big Dipper (US) or the Plough (UK) that comprises the seven brightest stars within the area of the IAU defined constellation of Ursa Major. In the southern False Cross asterism includes portions of the constellations Carina and Vela.\n\nThe term circumpolar constellation is used for any constellation that, from a particular latitude on Earth, never sets below the horizon. From the North Pole or South Pole, all constellations south or north of the celestial equator are circumpolar constellations. Depending on the definition, equatorial constellations may include those that lie entirely between declinations 45° north and 45° south, or those that pass through the declination range of the ecliptic or zodiac ranging between 23½° north, the celestial equator, and 23½° south.\n\nAlthough stars in constellations appear near each other in the sky, they usually lie at a variety of distances away from the observer. Since stars also travel along their own orbits through the Milky Way, the constellation outlines change slowly over time. After tens to hundreds of thousands of years, their familiar outlines gradually become unrecognisable. Astronomers can predict the past or future constellation outlines by measuring their individual stars common proper motions or cpm. by accurate astrometry and their radial velocities by astronomical spectroscopy.\n\nThe earliest direct antecedent evidence for the constellations comes from inscribed stones and clay writing tablets dug up in Mesopotamia (within modern Iraq) dating back to 3000 BC. It seems that the bulk of the Mesopotamian constellations were created within a relatively short interval from around 1300 to 1000 BC. These groupings appeared later in many of the classical Greek constellations.\n\nThe Babylonians were the first to recognize that astronomical phenomena are periodic and apply mathematics to their predictions. The oldest Babylonian star catalogues of stars and constellations date back to the beginning in the Middle Bronze Age, most notably the \"Three Stars Each\" texts and the \"MUL.APIN\", an expanded and revised version based on more accurate observation from around 1000 BC. However, the numerous Sumerian names in these catalogues suggest that they built on older, but otherwise unattested, Sumerian traditions of the Early Bronze Age.\n\nThe classical Zodiac is a product of a revision of the Old Babylonian system in later Neo-Babylonian astronomy 6th century BC. Knowledge of the Neo-Babylonian zodiac is also reflected in the Hebrew Bible. E. W. Bullinger interpreted the creatures appearing in the books of Ezekiel (and thence in Revelation) as the middle signs of the four quarters of the Zodiac, with the Lion as Leo, the Bull as Taurus, the Man representing Aquarius and the Eagle standing in for Scorpio. The biblical Book of Job also makes reference to a number of constellations, including \"bier\", \"fool\" and \"heap\" (Job 9:9, 38:31-32), rendered as \"Arcturus, Orion and Pleiades\" by the KJV, but \"‘Ayish\" \"the bier\" actually corresponding to Ursa Major. The term \"Mazzaroth\" , a \"hapax legomenon\" in Job 38:32, may be the Hebrew word for the zodiacal constellations.\n\nThe Greeks adopted the Babylonian system in the 4th century BC. A total of twenty Ptolemaic constellations are directly continued from the Ancient Near East. Another ten have the same stars but different names.\n\nThere is only limited information on indigenous Greek constellations, with some fragmentary evidence being found in the \"Works and Days\" of Greek poet Hesiod, who mentioned the \"heavenly bodies\". Greek astronomy essentially adopted the older Babylonian system in the Hellenistic era, first introduced to Greece by Eudoxus of Cnidus in the 4th century BC. The original work of Eudoxus is lost, but it survives as a versification by Aratus, dating to the 3rd century BC. The most complete existing works dealing with the mythical origins of the constellations are by the Hellenistic writer termed pseudo-Eratosthenes and an early Roman writer styled pseudo-Hyginus. The basis of western astronomy as taught during Late Antiquity and until the Early Modern period is the \"Almagest\" by Ptolemy, written in the 2nd century.\n\nIn the Ptolemaic Kingdom, native Egyptian tradition of anthropomorphic figures representing the planets, stars and various constellations. Some of these were combined with Greek and Babylonian astronomical systems culminating in the Zodiac of Dendera, but it remains unclear when this occurred, but most were placed during the Roman period between 2nd to 4th centuries AD. The oldest known depiction of the zodiac showing all the now familiar constellations, along with some original Egyptian Constellations, Decans and Planets. Ptolemy's \"Almagest\" remained the standard definition of constellations in the medieval period both in Europe and in Islamic astronomy.\n\nIn ancient China astronomy has had a long tradition in accurately observing celestial phenomena. Star names later categorized in the twenty-eight mansions have been found on oracle bones unearthed at Anyang, dating back to the middle Shang Dynasty. These Chinese constellations are one of the most important and also the most ancient structures in the Chinese sky, attested from the 5th century BC. Parallels to the earliest Babylonian (Sumerian) star catalogues suggest that the ancient Chinese system did not arise independently.\n\nClassical Chinese astronomy is recorded in the Han period and appears in the form of three schools, which are attributed to astronomers of the Warring States period. The constellations of the three schools were conflated into a single system by Chen Zhuo, an astronomer of the 3rd century (Three Kingdoms period). Chen Zhuo's work has been lost, but information on his system of constellations survives in Tang period records, notably by Qutan Xida. The oldest extant Chinese star chart dates to that period and was preserved as part of the Dunhuang Manuscripts. Native Chinese astronomy flourished during the Song dynasty, and during the Yuan Dynasty became increasingly influenced by medieval Islamic astronomy (see Treatise on Astrology of the Kaiyuan Era). As maps were prepared during this period on more scientific lines they were considered as more reliable.\nA well known map prepared during the Song Period is the Suzhou Astronomical Chart prepared with carvings of most stars on the planisphere of the Chinese Sky on a stone plate; it is done accurately based on observations and has the suprnova of the year of 1054 in Taurus carved on it.\n\nInfluenced by European astronomy during the late Ming Dynasty, more stars were depicted on the charts but retaining the traditional constellations; new stars observed were incorporated as supplementary stars in old constellations in the southern sky which did not depict any of the traditional stars recorded by ancient Chinese astronomers. Further improvements were made during the later part of the Ming Dynasty by Xu Guangqi and Johann Adam Schall von Bell, the German Jesuit and was recorded in Chongzhen Lishu (Calendrical Treatise of Chongzhen Period, 1628). Traditional Chinese star maps incorporated 23 new constellations with 125 stars of the southern hemisphere of the sky based on the knowledge of western star charts; with this improvement the Chinese Sky was integrated with the World astronomy.\n\nHistorically, the constellations can be simply divided into two regions; namely the northern and southern sky, whose origins are distinctly different. The northern skies have constellations that have mostly survived since Antiquity, whose common names are based on Classical Greek legends or those whose true origins have now been lost. Evidence of these constellations have survived in the form of star charts, whose oldest representation appears on the statue known as the Farnese Atlas, which has been suggested to be based on the star catalogue of the Greek astronomer, Hipparchus. Southern hemisphere constellations are more modern inventions, which were created as new constellations, or become substitutes for some ancient constellation. e.g. Argo Navis. Some southern constellations were to become obsolete or had extended names that became foreshortened to more usable forms e.g. Musca Australis became simply Musca.\n\nHowever, all the early constellations were never universally adopted, whose popular usage was based on the culture or individual nations. Defining each constellation and their assigned stars also significantly differed in size and shape, whose arbitrary boundaries often lead to confusion to where celestial objects were to be placed. Before the constellation boundaries were defined by the International Astronomical Union (IAU) in 1930, they appeared as simply encircled areas of sky. Today they now follow officially accepted designated lines of Right Ascension and Declination based on those defined by Benjamin Gould in Epoch 1875.0 in his star catalogue known as \"Uranometria Argentina\".\n\nSince the invention of the optical telescope, astronomers have found the need to catalogue and position celestial bodies, whose knowledge could be used for navigational or astronomical purposes, and this required improved definitions of the constellations and their boundaries. Such changes also assigned stars within each constellation, as first accomplished in 1603 by Johann Bayer in the star atlas \"Uranometria\" using the \"twenty-four\" letters of the Greek alphabet. Subsequent star atlases defined under celestial cartography lead to the development towards today's accepted modern constellations.\n\nMuch of the sky near the South Celestial Pole below the declination of about –65° was not observed from the north hemisphere and became only partially catalogued by the ancient Babylonians, Egyptian, Greeks, Chinese, and Persian astronomers. Knowledge that stars in the southern skies were different go back into Antiquity, being mostly gained from later Classical writers about Phoenician sailors : like the African circumnavigation expedition commissioned by Egyptian Pharaoh Necho II in c. 600 BC or those of Hanno the Navigator in c. 500BC. However, most of the knowledge of these ancient origins were likely irrecoverably and forever lost with the Destruction of the Library of Alexandria. \n\nA true history of southern constellation names remains neither definitive nor straight forward. Various countries had adopted or ascribed different names or had used different stars to define them. Most these early forms of constellations remained as merely curiosities for their nobility or sponsors, but only became important to 14th to 16th Century seafarers who began journeying across the southern oceans using the stars for celestial navigation purposes. Examples of such Italian explorers who recorded the new southern constellations included : Andrea Corsali, Antonio Pigafetta, Amerigo Vespucci.\n\nMany of the 88 IAU-recognized constellations in this region were to be adopted from in the late 16th century by Petrus Plancius and were mainly based on the observations of the Dutch navigators Pieter Dirkszoon Keyser and Frederick de Houtman who had added fifteen by the end of the sixteenth century. Another ten were added by Petrus Plancius including: Apus, Chamaeleon, Columba, Dorado, Grus, Hydrus, Indus, Musca, Pavo, Phoenix, Triangulum Australe, Tucana, and Volans. However, most of these early constellations only formally appeared a century after their creation, when they were later depicted by German Johann Bayer in his star atlas \"Uranometria\" of 1603. more were created in 1763 by the French astronomer Nicolas Louis de Lacaille appearing in his star catalogue, published in 1756.\n\nSome other modern proposals were unsuccessful. For example, the large classical constellation of Argo Navis was broken up into three separate parts (Carina, Puppis, and Vela) by Lacaille, for the convenience of stellar cartographers. Others included constellations by the French astronomers Pierre Lemonnier and Joseph Lalande, whose additions were once popular, but have since been dropped. For the northern constellations, an example is Quadrans, eponymous of the Quadrantid meteors, now divided between Boötes and Draco.\n\nThe current list of 88 constellations recognized by the International Astronomical Union since 1922 is based on the 48 listed by Ptolemy in his \"Almagest\" in the 2nd century, with early modern modifications and additions (most importantly introducing constellations covering the parts of the southern sky unknown to Ptolemy) by Petrus Plancius (1592, 1597/98 and 1613), Johannes Hevelius (1690) and Nicolas Louis de Lacaille (1763), who named fourteen constellations and renamed a fifteenth one. De Lacaille studied the stars of the southern hemisphere from 1750 until 1754 from Cape of Good Hope, when he was said to have observed more than 10,000 stars using a refracting telescope.\n\nIn 1922, Henry Norris Russell aided the IAU (International Astronomical Union) in dividing the celestial sphere into 88 official constellations; Prior to this, Ptolemy's list of 48 constellations with many additions made by European astronomers had prevailed. However, these divisions did not have clear borders between them. It was only in 1930 that Eugene Delporte, the Belgian astronomer created an authoritative map demarcating the areas of sky under different constellations. Where possible, these modern constellations usually share the names of their Graeco-Roman predecessors, such as Orion, Leo or Scorpius. The aim of this system is area-mapping, i.e. the division of the celestial sphere into contiguous fields. Out of the 88 modern constellations, 36 lie predominantly in the northern sky, and the other 52 predominantly in the southern.\n\nIn 1930, the boundaries between the 88 constellations were devised by Eugène Delporte along vertical and horizontal lines of right ascension and declination. However, the data he used originated back to epoch B1875.0, which was when Benjamin A. Gould first made his proposal to designate boundaries for the celestial sphere, a suggestion upon which Delporte would base his work. The consequence of this early date is that because of the precession of the equinoxes, the borders on a modern star map, such as epoch J2000, are already somewhat skewed and no longer perfectly vertical or horizontal. This effect will increase over the years and centuries to come.\n\nThe Great Rift, a series of dark patches in the Milky Way, is more visible and striking in the southern hemisphere than in the northern. It vividly stands out when conditions are otherwise so dark that the Milky Way's central region casts shadows on the ground. Some cultures have discerned shapes in these patches and have given names to these \"dark cloud constellations\". Members of the Inca civilization identified various dark areas or dark nebulae in the Milky Way as animals, and associated their appearance with the seasonal rains. Australian Aboriginal astronomy also describes dark cloud constellations, the most famous being the \"emu in the sky\" whose head is formed by the Coalsack, a dark nebulae instead of the stars.\n\n\n\n\"General & Nonspecialized – Entire Celestial Heavens\":\n\n\"Northern Celestial Hemisphere & North Circumpolar Region\":\n\n\"Equatorial, Ecliptic, & Zodiacal Celestial Sky\":\n\n\"Southern Celestial Hemisphere & South Circumpolar Region\":\n\n\n"}
{"id": "5269", "url": "https://en.wikipedia.org/wiki?curid=5269", "title": "Character", "text": "Character\n\nCharacter(s) may refer to:\n\n\n\n\n\n"}
{"id": "5270", "url": "https://en.wikipedia.org/wiki?curid=5270", "title": "Car (disambiguation)", "text": "Car (disambiguation)\n\nA car is a wheeled motor vehicle used for transporting passengers.\n\nCar, Cars, CAR or CARs may also refer to:\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5272", "url": "https://en.wikipedia.org/wiki?curid=5272", "title": "Printer (computing)", "text": "Printer (computing)\n\nIn computing, a printer is a peripheral which makes a persistent human-readable representation of graphics or text on paper or similar physical media. \nThe first computer printer design was a mechanically driven apparatus by Charles Babbage for his difference engine in the 19th century; his mechanical printer design was not built until 2000. The first electronic printer was the EP-101, invented by Japanese company Epson and released in 1968. The first commercial printers generally used mechanisms from electric typewriters and Teletype machines. The demand for higher speed led to the development of new systems specifically for computer use. In the 1980s were daisy wheel systems similar to typewriters, line printers that produced similar output but at much higher speed, and dot matrix systems that could mix text and graphics but produced relatively low-quality output. The plotter was used for those requiring high quality line art like blueprints.\n\nThe introduction of the low-cost laser printer in 1984 with the first HP LaserJet, and the addition of PostScript in next year's Apple LaserWriter, set off a revolution in printing known as desktop publishing. Laser printers using PostScript mixed text and graphics, like dot-matrix printers, but at quality levels formerly available only from commercial typesetting systems. By 1990, most simple printing tasks like fliers and brochures were now created on personal computers and then laser printed; expensive offset printing systems were being dumped as scrap. The HP Deskjet of 1988 offered the same advantages as laser printer in terms of flexibility, but produced somewhat lower quality output (depending on the paper) from much less expensive mechanisms. Inkjet systems rapidly displaced dot matrix and daisy wheel printers from the market. By the 2000s high-quality printers of this sort had fallen under the $100 price point and became commonplace.\n\nThe rapid update of internet email through the 1990s and into the 2000s has largely displaced the need for printing as a means of moving documents, and a wide variety of reliable storage systems means that a \"physical backup\" is of little benefit today. Even the desire for printed output for \"offline reading\" while on mass transit or aircraft has been displaced by e-book readers and tablet computers. Today, traditional printers are being used more for special purposes, like printing photographs or artwork, and are no longer a must-have peripheral.\n\nStarting around 2010, 3D printing became an area of intense interest, allowing the creation of physical objects with the same sort of effort as an early laser printer required to produce a brochure. These devices are in their earliest stages of development and have not yet become commonplace.\n\n\"Personal\" printers are primarily designed to support individual users, and may be connected to only a single computer. These printers are designed for low-volume, short-turnaround print jobs, requiring minimal setup time to produce a hard copy of a given document. However, they are generally slow devices ranging from 6 to around 25 pages per minute (ppm), and the cost per page is relatively high. However, this is offset by the on-demand convenience. Some printers can print documents stored on memory cards or from digital cameras and scanners.\n\n\"Networked\" or \"shared\" printers are \"designed for high-volume, high-speed printing.\" They are usually shared by many users on a network and can print at speeds of 45 to around 100 ppm. The Xerox 9700 could achieve 120 ppm.\n\nA \"virtual printer\" is a piece of computer software whose user interface and API resembles that of a printer driver, but which is not connected with a physical computer printer. A virtual printer can be used to create a file which is an image of the data which would be printed, for archival purposes or as input to another program, for example to create a PDF or to transmit to another system or user.\n\nA \"3D printer\" is a device for making a three-dimensional object from a 3D model or other electronic data source through additive processes in which successive layers of material (including plastics, metals, food, cement, wood, and other materials) are laid down under computer control. It is called a printer by analogy with an inkjet printer which produces a two-dimensional document by a similar process of depositing a layer of ink on paper.\n\nThe choice of print technology has a great effect on the cost of the printer and cost of operation, speed, quality and permanence of documents, and noise. Some printer technologies don't work with certain types of physical media, such as carbon paper or transparencies.\n\nA second aspect of printer technology that is often forgotten is resistance to alteration: liquid ink, such as from an inkjet head or fabric ribbon, becomes absorbed by the paper fibers, so documents printed with liquid ink are more difficult to alter than documents printed with toner or solid inks, which do not penetrate below the paper surface.\n\nCheques can be printed with liquid ink or on special cheque paper with toner anchorage so that alterations may be detected. The machine-readable lower portion of a cheque must be printed using MICR toner or ink. Banks and other clearing houses employ automation equipment that relies on the magnetic flux from these specially printed characters to function properly.\n\nThe following printing technologies are routinely found in modern printers:\n\nA laser printer rapidly produces high quality text and graphics. As with digital photocopiers and multifunction printers (MFPs), laser printers employ a xerographic printing process but differ from analog photocopiers in that the image is produced by the direct scanning of a laser beam across the printer's photoreceptor.\n\nAnother toner-based printer is the LED printer which uses an array of LEDs instead of a laser to cause toner adhesion to the print drum.\n\nInkjet printers operate by propelling variably sized droplets of liquid ink onto almost any sized page. They are the most common type of computer printer used by consumers.\n\nSolid ink printers, also known as phase-change printers, are a type of thermal transfer printer. They use solid sticks of CMYK-coloured ink, similar in consistency to candle wax, which are melted and fed into a piezo crystal operated print-head. The printhead sprays the ink on a rotating, oil coated drum. The paper then passes over the print drum, at which time the image is immediately transferred, or transfixed, to the page. Solid ink printers are most commonly used as colour office printers, and are excellent at printing on transparencies and other non-porous media. Solid ink printers can produce excellent results. Acquisition and operating costs are similar to laser printers. Drawbacks of the technology include high energy consumption and long warm-up times from a cold state. Also, some users complain that the resulting prints are difficult to write on, as the wax tends to repel inks from pens, and are difficult to feed through automatic document feeders, but these traits have been significantly reduced in later models. In addition, this type of printer is only available from one manufacturer, Xerox, manufactured as part of their Xerox Phaser office printer line. Previously, solid ink printers were manufactured by Tektronix, but Tek sold the printing business to Xerox in 2001.\n\nA dye-sublimation printer (or dye-sub printer) is a printer which employs a printing process that uses heat to transfer dye to a medium such as a plastic card, paper or canvas. The process is usually to lay one colour at a time using a ribbon that has colour panels. Dye-sub printers are intended primarily for high-quality colour applications, including colour photography; and are less well-suited for text. While once the province of high-end print shops, dye-sublimation printers are now increasingly used as dedicated consumer photo printers.\n\nThermal printers work by selectively heating regions of special heat-sensitive paper. Monochrome thermal printers are used in cash registers, ATMs, gasoline dispensers and some older inexpensive fax machines. Colours can be achieved with special papers and different temperatures and heating rates for different colours; these coloured sheets are not required in black-and-white output. One example is Zink (a portmanteau of \"zero ink\").\n\nThe following technologies are either obsolete, or limited to special applications though most were, at one time, in widespread use.\n\nImpact printers rely on a forcible impact to transfer ink to the media. The impact printer uses a print head that either hits the surface of the ink ribbon, pressing the ink ribbon against the paper (similar to the action of a typewriter), or, less commonly, hits the back of the paper, pressing the paper against the ink ribbon (the IBM 1403 for example). All but the dot matrix printer rely on the use of \"fully formed characters\", letterforms that represent each of the characters that the printer was capable of printing. In addition, most of these printers were limited to monochrome, or sometimes two-color, printing in a single typeface at one time, although bolding and underlining of text could be done by \"overstriking\", that is, printing two or more impressions either in the same character position or slightly offset. Impact printers varieties include typewriter-derived printers, teletypewriter-derived printers, daisywheel printers, dot matrix printers and line printers. Dot matrix printers remain in common use in businesses where multi-part forms are printed. \"An overview of impact printing\" contains a detailed description of many of the technologies used.\n\nSeveral different computer printers were simply computer-controllable versions of existing electric typewriters. The Friden Flexowriter and IBM Selectric-based printers were the most-common examples. The Flexowriter printed with a conventional typebar mechanism while the Selectric used IBM's well-known \"golf ball\" printing mechanism. In either case, the letter form then struck a ribbon which was pressed against the paper, printing one character at a time. The maximum speed of the Selectric printer (the faster of the two) was 15.5 characters per second.\n\nThe common teleprinter could easily be interfaced to the computer and became very popular except for those computers manufactured by IBM. Some models used a \"typebox\" that was positioned, in the X- and Y-axes, by a mechanism and the selected letter form was struck by a hammer. Others used a type cylinder in a similar way as the Selectric typewriters used their type ball. In either case, the letter form then struck a ribbon to print the letterform. Most teleprinters operated at ten characters per second although a few achieved 15 CPS.\n\nDaisy wheel printers operate in much the same fashion as a typewriter. A hammer strikes a wheel with petals, the \"daisy wheel\", each petal containing a letter form at its tip. The letter form strikes a ribbon of ink, depositing the ink on the page and thus printing a character. By rotating the daisy wheel, different characters are selected for printing. These printers were also referred to as \"letter-quality printers\" because they could produce text which was as clear and crisp as a typewriter. The fastest letter-quality printers printed at 30 characters per second.\n\nThe term dot matrix printer is used for impact printers that use a matrix of small pins to transfer ink to the page. The advantage of dot matrix over other impact printers is that they can produce graphical images in addition to text; however the text is generally of poorer quality than impact printers that use letterforms (\"type\").\n\nDot-matrix printers can be broadly divided into two major classes:\n\nDot matrix printers can either be character-based or line-based (that is, a single horizontal series of pixels across the page), referring to the configuration of the print head.\n\nIn the 1970s & 80s, dot matrix printers were one of the more common types of printers used for general use, such as for home and small office use. Such printers normally had either 9 or 24 pins on the print head (early 7 pin printers also existed, which did not print descenders). There was a period during the early home computer era when a range of printers were manufactured under many brands such as the Commodore VIC-1525 using the Seikosha Uni-Hammer system. This used a single solenoid with an oblique striker that would be actuated 7 times for each column of 7 vertical pixels while the head was moving at a constant speed. The angle of the striker would align the dots vertically even though the head had moved one dot spacing in the time. The vertical dot position was controlled by a synchronised longitudinally ribbed platen behind the paper that rotated rapidly with a rib moving vertically seven dot spacings in the time it took to print one pixel column. 24-pin print heads were able to print at a higher quality and started to offer additional type styles and were marketed as Near Letter Quality by some vendors. Once the price of inkjet printers dropped to the point where they were competitive with dot matrix printers, dot matrix printers began to fall out of favour for general use.\n\nSome dot matrix printers, such as the NEC P6300, can be upgraded to print in colour. This is achieved through the use of a four-colour ribbon mounted on a mechanism (provided in an upgrade kit that replaces the standard black ribbon mechanism after installation) that raises and lowers the ribbons as needed. Colour graphics are generally printed in four passes at standard resolution, thus slowing down printing considerably. As a result, colour graphics can take up to four times longer to print than standard monochrome graphics, or up to 8-16 times as long at high resolution mode.\n\nDot matrix printers are still commonly used in low-cost, low-quality applications such as cash registers, or in demanding, very high volume applications like invoice printing. Impact printing, unlike laser printing, allows the pressure of the print head to be applied to a stack of two or more forms to print multi-part documents such as sales invoices and credit card receipts using continuous stationery with carbonless copy paper. Dot-matrix printers were being superseded even as receipt printers after the end of the twentieth century.\n\nLine printers print an entire line of text at a time. Four principal designs exist.\n\n\n\nIn each case, to print a line, precisely timed hammers strike against the back of the paper at the exact moment that the correct character to be printed is passing in front of the paper. The paper presses forward against a ribbon which then presses against the character form and the impression of the character form is printed onto the paper.\n\n\nLine printers are the fastest of all impact printers and are used for bulk printing in large computer centres. A line printer can print at 1100 lines per minute or faster, frequently printing pages more rapidly than many current laser printers. On the other hand, the mechanical components of line printers operat with tight tolerances and require regular preventive maintenance (PM) to produce top quality print. They are virtually never used with personal computers and have now been replaced by high-speed laser printers. The legacy of line printers lives on in many computer operating systems, which use the abbreviations \"lp\", \"lpr\", or \"LPT\" to refer to printers.\n\nLiquid ink electrostatic printers use a chemical coated paper, which is charged by the print head according to the image of the document. The paper is passed near a pool of liquid ink with the opposite charge. The charged areas of the paper attract the ink and thus form the image. This process was developed from the process of electrostatic copying. Color reproduction is very accurate, and because there is no heating the scale distortion is less than ±0.1%. (All laser printers have an accuracy of ±1%.)\n\nWorldwide, most survey offices used this printer before color inkjet plotters become popular. Liquid ink electrostatic printers were mostly available in width and also 6 color printing. These were also used to print large billboards. It was first introduced by Versatec, which was later bought by Xerox. 3M also used to make these printers.\n\nPen-based plotters were an alternate printing technology once common in engineering and architectural firms. Pen-based plotters rely on contact with the paper (but not impact, per se) and special purpose pens that are mechanically run over the paper to create text and images. Since the pens output continuous lines, they were able to produce technical drawings of higher resolution than was achievable with dot-matrix technology. Some plotters used roll-fed paper, and therefore had minimal restriction on the size of the output in one dimension. These plotters were capable of producing quite sizable drawings.\n\nA number of other sorts of printers are important for historical reasons, or for special purpose uses:\n\n\nMost printers other than line printers accept control characters or unique character sequences to control various printer functions. These may range from shifting from lower to upper case or from black to red ribbon on typewriter printers to switching fonts and changing character sizes and colors on raster printers. Early printer controls were not standardized, with each manufacturer's equipment having its own set. The IBM Personal Printer Data Stream (PPDS) became a commonly used command set for dot-matrix printers.\n\nToday, most printers accept one or more page description languages (PDLs). Laser printers with greater processing power frequently offer support for variants of Hewlett-Packard's Printer Command Language (PCL), PostScript or XML Paper Specification. Most inkjet devices support manufacturer proprietary PDLs such as ESC/P. The diversity in mobile platforms have led to various standardization efforts around device PDLs such as the Printer Working Group (PWG's) PWG Raster.\n\nThe speed of early printers was measured in units of \"characters per minute\" (cpm) for character printers, or \"lines per minute\" (lpm) for line printers. Modern printers are measured in \"pages per minute\" (ppm). These measures are used primarily as a marketing tool, and are not as well standardised as toner yields. Usually pages per minute refers to sparse monochrome office documents, rather than dense pictures which usually print much more slowly, especially colour images. PPM are most of the time referring to A4 paper in Europe and letter paper in the United States, resulting in a 5-10% difference.\n\nThe data received by a printer may be:\n\n\nSome printers can process all four types of data, others not.\n\n\nToday it is possible to print everything (even plain text) by sending ready bitmapped images to the printer. This allows better control over formatting, especially among machines from different vendors. Many printer drivers do not use the text mode at all, even if the printer is capable of it.\n\nA monochrome printer can only produce an image consisting of one colour, usually black. A monochrome printer may also be able to produce various tones of that color, such as a grey-scale. A colour printer can produce images of multiple colours. A photo printer is a colour printer that can produce images that mimic the colour range (gamut) and resolution of prints made from photographic film. Many can be used on a standalone basis without a computer, using a memory card or USB connector.\n\nThe page yield is number of pages that can be printed from a toner cartridge or ink cartridge—before the cartridge needs to be refilled or replaced.\nThe actual number of pages yielded by a specific cartridge depends on a number of factors.\n\nFor a fair comparison, many laser printer manufacturers use the ISO/IEC 19752 process to measure the toner cartridge yield.\n\nIn order to fairly compare operating expenses of printers with a relatively small ink cartridge to printers with a larger, more expensive toner cartridge that typically holds more toner and so prints more pages before the cartridge needs to be replaced, many people prefer to estimate operating expenses in terms of cost per page (CPP).\n\nOften the \"razor and blades\" business model is applied. That is, a company may sell a printer at cost, and make profits on the ink cartridge, paper, or some other replacement part. This has caused legal disputes regarding the right of companies other than the printer manufacturer to sell compatible ink cartridges. To protect their business model, several manufacturers invest heavily in developing new cartridge technology and patenting it.\n\nOther manufacturers, in reaction to the challenges from using this business model, choose to make more money on printers and less on the ink, promoting the latter through their advertising campaigns. Finally, this generates two clearly different proposals: \"cheap printer – expensive ink\" or \"expensive printer – cheap ink\". Ultimately, the consumer decision depends on their reference interest rate or their time preference. From an economics viewpoint, there is a clear trade-off between cost per copy and cost of the printer.\n\nPrinter steganography is a type of steganography – \"hiding data within data\" – produced by color printers, including Brother, Canon, Dell, Epson, HP, IBM, Konica Minolta, Kyocera, Lanier, Lexmark, Ricoh, Toshiba and Xerox brand color laser printers, where tiny yellow dots are added to each page. The dots are barely visible and contain encoded printer serial numbers, as well as date and time stamps.\n\nMore than half of all printers sold at U.S. retail in 2010 were wireless-capable, but nearly three-quarters of consumers who have access to those printers weren't taking advantage of the increased access to print from multiple devices according to the new Wireless Printing Study.\n"}

{"id": "10828", "url": "https://en.wikipedia.org/wiki?curid=10828", "title": "Fear", "text": "Fear\n\nFear is a feeling induced by perceived danger or threat that occurs in certain types of organisms, which causes a change in metabolic and organ functions and ultimately a change in behavior, such as fleeing, hiding, or freezing from perceived traumatic events. Fear in human beings may occur in response to a specific stimulus occurring in the present, or in anticipation or expectation of a future threat perceived as a risk to body or life. The fear response arises from the perception of danger leading to confrontation with or escape from/avoiding the threat (also known as the fight-or-flight response), which in extreme cases of fear (horror and terror) can be a freeze response or paralysis. \nIn humans and animals, fear is modulated by the process of cognition and learning. Thus fear is judged as rational or appropriate and irrational or inappropriate. An irrational fear is called a phobia.\n\nPsychologists such as John B. Watson, Robert Plutchik, and Paul Ekman have suggested that there is only a small set of basic or innate emotions and that fear is one of them. This hypothesized set includes such emotions as acute stress reaction, anger, angst, anxiety, fright, horror, joy, panic, and sadness. Fear is closely related to, but should be distinguished from, the emotion anxiety, which occurs as the result of threats that are perceived to be uncontrollable or unavoidable. The fear response serves survival by generating appropriate behavioral responses, so it has been preserved throughout evolution.\n\nMany physiological changes in the body are associated with fear, summarized as the fight-or-flight response. An inborn response for coping with danger, it works by accelerating the breathing rate (hyperventilation), heart rate, constriction of the peripheral blood vessels leading to blushing and vasodilation of the central vessels (pooling), increasing muscle tension including the muscles attached to each hair follicle to contract and causing \"goose bumps\", or more clinically, piloerection (making a cold person warmer or a frightened animal look more impressive), sweating, increased blood glucose (hyperglycemia), increased serum calcium, increase in white blood cells called neutrophilic leukocytes, alertness leading to sleep disturbance and \"butterflies in the stomach\" (dyspepsia). This primitive mechanism may help an organism survive by either running away or fighting the danger. With the series of physiological changes, the consciousness realizes an emotion of fear.\n\nPeople develop specific fears as a result of learning. This has been studied in psychology as fear conditioning, beginning with John B. Watson's Little Albert experiment in 1920, which was inspired after observing a child with an irrational fear of dogs. In this study, an 11-month-old boy was conditioned to fear a white rat in the laboratory. The fear became generalized to include other white, furry objects, such as a rabbit, dog, and even a ball of cotton.\n\nFear can be learned by experiencing or watching a frightening traumatic accident. For example, if a child falls into a well and struggles to get out, he or she may develop a fear of wells, heights (acrophobia), enclosed spaces (claustrophobia), or water (aquaphobia). There are studies looking at areas of the brain that are affected in relation to fear. When looking at these areas (such as the amygdala), it was proposed that a person learns to fear regardless of whether they themselves have experienced trauma, or if they have observed the fear in others. In a study completed by Andreas Olsson, Katherine I. Nearing and Elizabeth A. Phelps the amygdala were affected both when subjects observed someone else being submitted to an aversive event, knowing that the same treatment awaited themselves, and when subjects were subsequently placed in a fear-provoking situation. This suggests that fear can develop in both conditions, not just simply from personal history.\n\nFear is affected by cultural and historical context. For example, in the early 20th century, many Americans feared polio, a disease that can lead to paralysis. There are consistent cross-cultural differences in how people respond to fear. Display rules affect how likely people are to show the facial expression of fear and other emotions.\n\nAlthough many fears are learned, the capacity to fear is part of human nature. Many studies have found that certain fears (e.g. animals, heights) are much more common than others (e.g. flowers, clouds). These fears are also easier to induce in the laboratory. This phenomenon is known as preparedness. Because early humans that were quick to fear dangerous situations were more likely to survive and reproduce, preparedness is theorized to be a genetic effect that is the result of natural selection.\n\nFrom an evolutionary psychology perspective, different fears may be different adaptations that have been useful in our evolutionary past. They may have developed during different time periods. Some fears, such as fear of heights, may be common to all mammals and developed during the mesozoic period. Other fears, such as fear of snakes, may be common to all simians and developed during the cenozoic time period. Still others, such as fear of mice and insects, may be unique to humans and developed during the paleolithic and neolithic time periods (when mice and insects become important carriers of infectious diseases and harmful for crops and stored foods).\n\nFear is high only if the observed risk and seriousness both are high, and is low, if risk or seriousness is low.\n\nIn a 2005 Gallup Poll (U.S.), a national sample of adolescents between the ages of 13 and 17 were asked what they feared the most. The question was open-ended and participants were able to say whatever they wanted. The top ten fears were, in order: terrorist attacks, spiders, death, failure, war, criminal or gang violence, being alone, the future, and nuclear war.\n\nIn an estimate of what people fear the most, book author Bill Tancer analyzed the most frequent online queries that involved the phrase, \"fear of...\" following the assumption that people tend to seek information on the issues that concern them the most. His top ten list of fears published 2008 consisted of flying, heights, clowns, intimacy, death, rejection, people, snakes, failure, and driving.\n\nAccording to surveys, some of the most common fears are of demons and ghosts, the existence of evil powers, cockroaches, spiders, snakes, heights, water, enclosed spaces, tunnels, bridges, needles, social rejection, failure, examinations, and public speaking.\n\nDeath anxiety is multidimensional; it covers \"fears related to one's own death, the death of others, fear of the unknown after death, fear of obliteration, and fear of the dying process, which includes fear of a slow death and a painful death\".\n\nThe Yale philosopher Shelly Kagan examined fear of death in a 2007 Yale open course by examining the following questions: Is fear of death a reasonable appropriate response? What conditions are required and what are appropriate conditions for feeling fear of death? What is meant by fear, and how much fear is appropriate? According to Kagan for fear in general to make sense, three conditions should be met: the object of fear needs to be \"something bad\", there needs to be a non-negligible chance that the bad state of affairs will happen, and there needs to be some uncertainty about the bad state of affairs. The amount of fear should be appropriate to the size of \"the bad\". If the 3 conditions aren't met, fear is an inappropriate emotion. He argues, that death does not meet the first two criteria, even if death is a \"deprivation of good things\" and even if one believes in a painful afterlife. Because death is certain, it also does not meet the third criterion, but he grants that the unpredictability of when one dies \"may\" be cause to a sense of fear.\n\nIn a 2003 study of 167 women and 121 men, aged 65–87, low self-efficacy predicted fear of the unknown after death and fear of dying for women and men better than demographics, social support, and physical health. Fear of death was measured by a \"Multidimensional Fear of Death Scale\" which included the 8 subscales Fear of Dying, Fear of the Dead, Fear of Being Destroyed, Fear for Significant Others, Fear of the Unknown, Fear of Conscious Death, Fear for the Body After Death, and Fear of Premature Death. In hierarchical multiple regression analysis the most potent predictors of death fears were low \"spiritual health efficacy\", defined as beliefs relating to one's perceived ability to generate spiritually based faith and inner strength, and low \"instrumental efficacy\", defined as beliefs relating to one's perceived ability to manage activities of daily living.\n\nPsychologists have tested the hypothesis that fear of death motivates religious commitment, and assurances about an afterlife alleviate the fear and empirical research on this topic has been equivocal. Religiosity can be related to fear of death when the afterlife is portrayed as time of punishment. \"Intrinsic religiosity\", as opposed to mere \"formal religious involvement\" has been found to be negatively correlated with death anxiety. In a 1976 study people of various Christian denominations those most firm in their faith, attending religious services weekly were the least afraid of dying. The survey found a negative correlation between fear of death and \"religious concern\".\n\nIn a 2006 study of white, Christian men and women the hypothesis was tested that traditional, church-centered religiousness and de-institutionalized spiritual seeking are ways of approaching fear of death in old age. Both religiousness and spirituality were related to positive psychosocial functioning, but only church-centered religiousness protected subjects against the fear of death.\n\nFear of the unknown or irrational fear is caused by negative thinking (worry) which arises from anxiety accompanied with a subjective sense of apprehension or dread. Irrational fear like any other fears share common neural pathway that engages to mobilize bodily resources in the face of threat. Many people are scared of the \"unknown\". The irrational fear can branch out to many areas such as the hereafter, the next ten years, or even tomorrow. Chronic irrational fear has deleterious effects since the elicitor stimulus is commonly absent or perceived from delusions. In these cases specialists use False Evidence Appearing Real as a definition. Such fear can create comorbidity with the anxiety disorder umbrella. Being scared may cause people to experience anticipatory fear of what may lie ahead rather than planning and evaluating for the same. E.g. Continuation of scholarly education, most educators perceive this as a risk that may cause them fear and stress and they would rather teach things they've been taught than go and do research, this can lead to habits such as laziness and procrastination. The ambiguity of a situations that tend to be uncertain and unpredictable can cause anxiety, other psychological and physical problems in some populations; especially those who engage it constantly. E.g. War-ridden or Conflict places, Terrorism, Abuse ...etc. Poor parenting that instills fear can also debilitate children's psyche development or personality. E.g. Parents tell their children not to talk to strangers in order to protect them. In school they would be motivated to not show fear in talking with strangers, but to be assertive and also aware of the risks and the environment that it takes place. Ambiguous and mixed messages like this can affect their self-esteem and self-confidence. Researchers say talking to strangers isn't something to be thwarted but allowed in a parent's presence if required. Developing a sense of equanimity to handle various situations is often advocated as an antidote to irrational fear and essential skill by a number of ancient philosophies.\n\nOften laboratory studies with rats are conducted to examine the acquisition and extinction of conditioned fear responses. In 2004, researchers conditioned rats (\"Rattus norvegicus\") to fear a certain stimulus, through electric shock. The researchers were able to then cause an extinction of this conditioned fear, to a point that no medications or drugs were able to further aid in the extinction process. However the rats did show signs of avoidance learning, not fear, but simply avoiding the area that brought pain to the tests rats. The avoidance learning of rats is seen as a conditioned response, and therefore the behavior can be unconditioned, as supported by the earlier research. \nSpecies-specific defense reactions (SSDRs) or avoidance learning in nature is the specific tendency to avoid certain threats or stimuli, it is how animals survive in the wild. Humans and animals both share these species-specific defense reactions, such as the flight, fight, which also include pseudo-aggression, fake or intimidating aggression, freeze response to threats, which is controlled by the sympathetic nervous system. These SSDRs are learned very quickly through social interactions between others of the same species, other species, and interaction with the environment. These acquired sets of reactions or responses are not easily forgotten. The animal that survives is the animal that already knows what to fear and how to avoid this threat. An example in humans is the reaction to the sight of a snake, many jump backwards before cognitively realizing what they are jumping away from, and in some cases it is a stick rather than a snake.\n\nAs with many functions of the brain, there are various regions of the brain involved in deciphering fear in humans and other nonhuman species. The amygdala communicates both directions between the prefrontal cortex, hypothalamus, the sensory cortex, the hippocampus, thalamus, septum, and the brainstem. The amygdala plays an important role in SSDR, such as the ventral amygdalofugal, which is essential for associative learning, and SSDRs are learned through interaction with the environment and others of the same species. An emotional response is created only after the signals have been relayed between the different regions of the brain, and activating the sympathetic nervous systems; which controls the flight, fight, freeze, fright, and faint response. Often a damaged amygdala can cause impairment in the recognition of fear (like the human case of patient S.M.). This impairment can cause different species to lack the sensation of fear, and often can become overly confident, confronting larger peers, or walking up to predatory creatures.\n\nRobert C. Bolles (1970), a researcher at University of Washington, wanted to understand species-specific defense reactions and avoidance learning among animals, but found that the theories of avoidance learning and the tools that were used to measure this tendency were out of touch with the natural world. He theorized the species-specific defense reaction (SSDR). There are three forms of SSDRs: flight, fight (pseudo-aggression), or freeze. Even domesticated animals have SSDRs, and in those moments it is seen that animals revert to atavistic standards and become \"wild\" again. Dr. Bolles states that responses are often dependent on the reinforcement of a safety signal, and not the aversive conditioned stimuli. This safety signal can be a source of feedback or even stimulus change. Intrinsic feedback or information coming from within, muscle twitches, increased heart rate, is seen to be more important in SSRDs than extrinsic feedback, stimuli that comes from the external environment. Dr. Bolles found that most creatures have some intrinsic set of fears, to help assure survival of the species. Rats will run away from any shocking event, and pigeons will flap their wings harder when threatened, the wing flapping in pigeons and the scattered running of rats are considered a species-specific defense reaction or behavior. Bolles believed that SSDR are conditioned through pavlovian conditioning, and not operant conditioning; SSDR arise from the association between the environmental stimuli and adverse events. Michael S. Fanselow conducted an experiment, to test some specific defense reactions, he observed that rats in two different shock situations responded differently, based on instinct or defensive topography, rather than contextual information.\n\nSpecies specific defense responses are created out of fear, and are essential for survival. Rats that lack the gene stathmin show no avoidance learning, or a lack of fear, and will often walk directly up to cats and be eaten. Animals use these SSDR to continue living, to help increase their chance of fitness, by surviving long enough to procreate. Humans and animals alike have created fear to know what should be avoided, and this fear can be learned through association with others in the community, or learned through personal experience with a creature, species, or situations that should be avoided. SSDRs are an evolutionary adaptation that has been seen in many species throughout the world including rats, chimpanzees, prairie dogs, and even humans, an adaptation created to help individual creatures survive in a hostile world.\n\nFear learning changes across the lifetime due to natural developmental changes in the brain. This includes changes in the prefrontal cortex and the amygdala.\n\n\nThe brain structure that is the center of most neurobiological events associated with fear is the amygdala, located behind the pituitary gland. The amygdala is part of a circuitry of fear learning. It is essential for proper adaptation to stress and specific modulation of emotional learning memory. In the presence of a threatening stimulus, the amygdala generates the secretion of hormones that influence fear and aggression. Once response to the stimulus in the form of fear or aggression commences, the amygdala may elicit the release of hormones into the body to put the person into a state of alertness, in which they are ready to move, run, fight, etc. This defensive response is generally referred to in physiology as the fight-or-flight response regulated by the hypothalamus, part of the limbic system. Once the person is in safe mode, meaning that there are no longer any potential threats surrounding them, the amygdala will send this information to the medial prefrontal cortex (mPFC) where it is stored for similar future situations, which is known as memory consolidation.\nSome of the hormones involved during the state of fight-or-flight include epinephrine, which regulates heart rate and metabolism as well as dilating blood vessels and air passages, norepinephrine increasing heart rate, blood flow to skeletal muscles and the release of glucose from energy stores, and cortisol which increases blood sugar, increases circulating neutrophilic leukocytes, calcium amongst other things.\n\nAfter a situation which incites fear occurs, the amygdala and hippocampus record the event through synaptic plasticity. The stimulation to the hippocampus will cause the individual to remember many details surrounding the situation. Plasticity and memory formation in the amygdala are generated by activation of the neurons in the region. Experimental data supports the notion that synaptic plasticity of the neurons leading to the lateral amygdala occurs with fear conditioning. In some cases, this forms permanent fear responses such as posttraumatic stress disorder (PTSD) or a phobia. MRI and fMRI scans have shown that the amygdala in individuals diagnosed with such disorders including bipolar or panic disorder is larger and wired for a higher level of fear.\n\nPathogens can suppress amygdala activity. Rats infected with the toxoplasmosis parasite become less fearful of cats, sometimes even seeking out their urine-marked areas. This behavior often leads to them being eaten by cats. The parasite then reproduces within the body of the cat. There is evidence that the parasite concentrates itself in the amygdala of infected rats. In a separate experiment, rats with lesions in the amygdala did not express fear or anxiety towards unwanted stimuli. These rats pulled on levers supplying food that sometimes sent out electrical shocks. While they learned to avoid pressing on them, they did not distance themselves from these shock-inducing levers.\n\nSeveral brain structures other than the amygdala have also been observed to be activated when individuals are presented with fearful vs. neutral faces, namely the occipitocerebellar regions including the fusiform gyrus and the inferior parietal / superior temporal gyri. Interestingly, fearful eyes, brows and mouth seem to separately reproduce these brain responses. Scientist from Zurich studies show that the hormone oxytocin related to stress and sex reduces activity in your brain fear center.\n\nIn threatening situations insects, aquatic organisms, birds, reptiles, and mammals emit odorant substances, initially called alarm substances, which are chemical signals now called alarm pheromones (\"Schreckstoff\" in German). This is to defend themselves and at the same time to inform members of the same species of danger and leads to observable behavior change like freezing, defensive behavior, or dispersion depending on circumstances and species. For example, stressed rats release odorant cues that cause other rats to move away from the source of the signal. Pheromones are synthesized, emitted and perceived by all living organisms studied to date, with the exception of viruses and prions: i.e. in bacteria, prokaryotes, plants, plankton, parasites, insects, invertebrates and vertebrates (aquatic organisms, birds, reptiles, and mammals).\n\nAfter the discovery of pheromones in 1959, alarm pheromones were first described in 1968 in ants and earthworms, and 4 years later also found in mammals, both mice and rats. Over the next two decades identification and characterization of these pheromones proceeded in all manner of insects and sea animals, including fish, but it was not until 1990 that more insight into mammalian alarm pheromones was gleaned.\n\nEarly on, in 1985, a link between odors released by stressed rats and pain perception was discovered: unstressed rats exposed to these odors developed opioid-mediated analgesia. In 1997, researchers found bees became less responsive to pain after they had been stimulated with isoamyl acetate, a chemical smelling of banana, and a component of bee alarm pheromone. The experiment also showed that the bees' fear-induced pain tolerance was mediated by an endorphine.\n\nBy using the forced swimming test in rats as a model of fear-induction, the first mammalian \"alarm substance\" was found.\nIn 1991, this \"alarm substance\" was shown to fulfill criteria for pheromones: well-defined behavioral effect, species specificity, minimal influence of experience and control for nonspecific arousal. Rat activity testing with alarm pheromone and their preference/avoidance for odors from cylinders containing the pheromone showed, that the pheromone had very low volatility.\n\nIn 1993 a connection between alarm chemosignals in mice and their immune response was found.\n\nPheromone production in mice was found to be associated with or mediated by the pituitary gland in 1994.\n\nIt was not until 2011 that a link between severe pain, neuroinflammation and alarm pheromones release in rats was found: real time RT-PCR analysis of rat brain tissues indicated that shocking the footpad of a rat increased its production of proinflammatory cytokines in deep brain structures, namely of IL-1β, heteronuclear Corticotropin-releasing hormone and c-fos mRNA expressions in both the paraventricular nucleus and the bed nucleus of the stria terminalis, and it increased stress hormone levels in plasma (corticosterone).\n\nIn 2004, it was demonstrated that rats’ alarm pheromones had different effects on the “recipient“ rat (the rat perceiving the pheromone) depending which body region they were released from: Pheromone production from the face modified behavior in the recipient rat, e.g. caused sniffing or movement, whereas pheromone secreted from the rat's anal area induced autonomic nervous system stress responses, like an increase in core body temperature. Further experiments showed that when a rat perceived alarm pheromones, it increased its defensive and risk assessment behavior. and its acoustic startle reflex was enhanced.\n\nThe neurocircuit for how rats perceive alarm pheromones was shown to be related to hypothalamus, brainstem, and amygdala, all of which are evolutionary ancient structures deep inside or in the case of the brainstem underneath the brain away from the cortex, and involved in the fight-or-flight response, as is the case in humans.\n\nAlarm pheromone-induced anxiety in rats has been used to evaluate the degree to which anxiolytics can alleviate anxiety in humans. For this the change in the acoustic startle reflex of rats with alarm pheromone-induced anxiety (i.e. reduction of defensiveness) has been measured. Pretreatment of rats with one of five anxiolytics used in clinical medicine was able to reduce their anxiety: namely midazolam, phenelzine (a nonselective monoamine oxidase (MAO) inhibitor), propranolol, a nonselective beta blocker, clonidine, an alpha 2 adrenergic agonist or CP-154,526, a corticotropin-releasing hormone antagonist.\n\nFaulty development of odor discrimination impairs the perception of pheromones and pheromone-related behavior, like aggressive behavior and mating in male rats: The enzyme Mitogen-activated protein kinase 7 (MAPK7) has been implicated in regulating the development of the olfactory bulb and odor discrimination and it is highly expressed in developing rat brains, but absent in most regions of adult rat brains. conditional deletion of the MAPK7gene in mouse neural stem cells impairs several pheromone-mediated behaviors, including aggression and mating in male mice. These behavior impairments were not caused by a reduction in the level of testosterone, by physical immobility, by heightened fear or anxiety or by depression. Using mouse urine as a natural pheromone-containing solution, it has been shown that the impairment was associated with defective detection of related pheromones, and with changes in their inborn preference for pheromones related to sexual and reproductive activities.\n\nLastly, alleviation of an acute fear response because a friendly peer (or in biological language: an affiliative conspecific) tends and befriends is called \"social buffering\". The term is in analogy to the 1985 \"buffering\" hypothesis in psychology, where social support has been proven to mitigate the negative health effects of alarm pheromone mediated distress. The role of a \"social pheromone\" is suggested by the recent discovery that olfactory signals are responsible in mediating the \"social buffering\" in male rats. \"Social buffering\" was also observed to mitigate the conditioned fear responses of honeybees. A bee colony exposed to an environment of high threat of predation did not show increased aggression and aggressive-like gene expression patterns in individual bees, but decreased aggression. That the bees did not simply habituate to threats is suggested by the fact that the disturbed colonies also decreased their foraging.\n\nBiologists have proposed in 2012 that fear pheromones evolved as molecules of \"keystone significance\", a term coined in analogy to keystone species. Pheromones may determine species compositions, and affect rates of energy and material exchange in an ecological community. Thus pheromones generate structure in a trophic web and play critical roles in maintaining natural systems.\n\nEvidence of chemosensory alarm signals in humans has emerged slowly: Although alarm pheromones have not been physically isolated and their chemical structure has not been identified in man so far, there is evidence for their presence. Androstadienone, for example, a steroidal, endogenous odorant, is a pheromone candidate found in human sweat, axillary hair and plasma. The closely related compound androstenone is involved in communicating dominance, aggression or competition; sex hormone influences on androstenone perception in humans showed high testosterone level related to heightened androstenone sensitivity in men, a high testosterone level related to unhappiness in response to androstenone in men, and a high estradiol level related to disliking of androstenone in women.\n\nA German study from 2006 showed when anxiety-induced versus exercise-induced human sweat from a dozen people was pooled and offered to seven study participants, of five able to olfactorily distinguish exercise-induced sweat from room air, three could also distinguish exercise-induced sweat from anxiety induced sweat. The acoustic startle reflex response to a sound when sensing anxiety sweat was larger than when sensing exercise-induced sweat, as measured by electromyograph analysis of the orbital muscle, which is responsible for the eyeblink component. This showed for the first time that fear chemosignals can modulate the startle reflex in humans without emotional mediation; fear chemosignals primed the recipient's \"defensive behavior\" prior to the subjects' conscious attention on the acoustic startle reflex level.\n\nIn analogy to the social buffering of rats and honeybees in response to chemosignals, induction of empathy by \"smelling anxiety\" of another person has been found in humans.\n\nA study from 2013 provided brain imaging evidence that human responses to fear chemosignals may be gender-specific. Researchers collected alarm-induced sweat and exercise-induced sweat from donors extracted it, pooled it and presented it to 16 unrelated people undergoing functional brain MRI. While stress-induced sweat from males produced a comparably strong emotional response in both females and males, stress-induced sweat from females produced a markedly stronger arousal in women than in men. Statistical tests pinpointed this gender-specificity to the right amygdala and strongest in the superficial nuclei. Since no significant differences were found in the olfactory bulb, the response to female fear-induced signals is likely based on processing the meaning, i.e. on the emotional level, rather than the strength of chemosensory cues from each gender, i.e. the perceptual level.\n\nAn approach-avoidance task was set up where volunteers seeing either an angry or a happy cartoon face on a computer screen pushed away or pulled toward them a joystick as fast as possible. Volunteers smelling anandrostadienone, masked with clove oil scent responded faster, especially to angry faces, than those smelling clove oil only, which was interpreted as anandrostadienone-related activation of the fear system. A potential mechanism of action is, that androstadienone alters the \"emotional face processing\". Androstadienone is known to influence activity of the fusiform gyrus which is relevant for face recognition.\n\nA drug treatment for fear conditioning and phobias via the amygdala is the use of glucocorticoids. In one study, glucocorticoid receptors in the central nucleus of the amygdala were disrupted in order to better understand the mechanisms of fear and fear conditioning. The glucocorticoid receptors were inhibited using lentiviral vectors containing Cre-recombinase injected into mice. Results showed that disruption of the glucocorticoid receptors prevented conditioned fear behavior. The mice were subjected to auditory cues which caused them to freeze normally. However, a reduction of freezing was observed in the mice that had inhibited glucocorticoid receptors.\n\nCognitive behavioral therapy has been successful in helping people overcome fear. Because fear is more complex than just forgetting or deleting memories, an active and successful approach involves people repeatedly confronting their fears. By confronting their fears in a safe manner a person can suppress the fear-triggering memory or stimulus. Known as ‘exposure therapy’, this practice can help cure up to 90% of people, with specific phobias.\n\nThe fear of the end of life and its existence is in other words the fear of death. The fear of death ritualized the lives of our ancestors. These rituals were designed to reduce that fear; they helped collect the cultural ideas that we now have in the present. These rituals also helped preserve the cultural ideas. The results and methods of human existence had been changing at the same time that social formation was changing. One can say that the formation of communities happened because people lived in fear. The result of this fear forced people to unite to fight dangers together rather than fight alone.\n\nReligions are filled with different fears that humans have had throughout many centuries. The fears aren't just metaphysical (including the problems of life and death) but are also moral. Death is seen as a boundary to another world. That world would always be different depending on how each individual lived their lives. The origins of this intangible fear are not found in the present world. In a sense we can assume that fear was a big influence on things such as morality. This assumption, however, flies in the face of concepts such as moral absolutism and moral universalism – which would hold that our morals are rooted in either the divine or natural laws of the universe, and would not be generated by any human feeling, thought or emotion.\n\nFear may be politically and culturally manipulated to persuade citizenry of ideas which would otherwise be widely rejected or dissuade citizenry from ideas which would otherwise be wildly supported. In contexts of disasters, nation-states manage the fear not only to provide their citizens with an explanation about the event or blaming some minorities, but also to adjust their previous beliefs. The manipulation of fear is done by means of symbolic instruments as terror movies and the administration ideologies that lead to nationalism. After a disaster, the fear is re-channeled in a climate of euphoria based on patriotism.\n\nFear is found and reflected in mythology and folklore as well as in works of fiction such as novels and films.\n\nWorks of dystopian and (post)apocalyptic fiction convey the fears and anxieties of societies.\n\nThe fear of the world's end is about as old as civilization itself. In a 1967 study Frank Kermode suggests that the failure of religious prophecies led to a shift in how society apprehends this ancient mode. Scientific and critical thought supplanting religious and mythical thought as well as a public emancipation may be the cause of eschatology becoming replaced by more realistic scenarios. Such might constructively provoke discussion and steps to be taken to prevent depicted catastrophes.\n\n\"The Story of the Youth Who Went Forth to Learn What Fear Was\" is a German fairy tale dealing with the topic of not knowing fear.\nMany stories also include characters who fear the antagonist of the plot. One important characteristic of historical and mythical heroes across cultures is to be fearless in the face of big and often lethal enemies.\n\nPeople who have damage to the amygdala, such as from Urbach–Wiethe disease, are unable to experience fear. This is not debilitating, but a lack of fear can allow someone to get into a dangerous situation they otherwise would have avoided.\n\n\n"}
{"id": "10830", "url": "https://en.wikipedia.org/wiki?curid=10830", "title": "Football team", "text": "Football team\n\nA football team is a group of players selected to play together in the various team sports known as football. Such teams could be selected to play in a match against an opposing team, to represent a football club, group, state or nation, an all-star team or even selected as a hypothetical team (such as a Dream Team or Team of the Century) and never play an actual match. \n\nThere are several varieties of football, notably association football, gridiron football, Australian rules football, Gaelic football, rugby league and rugby union. The number of players selected for each team within these varieties and their associated codes can vary substantially. Sometimes, the word \"team\" is limited to those who play on the field in a match and does not always include other players who may take part as replacements or emergency players. \"Football squad\" may be used to be inclusive of these support and reserve players.\n\nThe term football club is the most commonly used for a sports club which is an organised or incorporated body with a president, committee and a set of rules responsible for ensuring the continued playing existence of one or more teams which are selected for regular competition play (and which may participate in several different divisions or leagues). The oldest football clubs date back to the early 19th century. The words team and club are sometimes used interchangeably by supporters, although they typically refer to the team within the club playing in the highest division or competition.\n\nThe number of players that take part in the sport simultaneously, thus forming the team are:\n\n\n\n"}
{"id": "10831", "url": "https://en.wikipedia.org/wiki?curid=10831", "title": "F", "text": "F\n\nF (named \"ef\" ) is the sixth letter in the modern English alphabet and the ISO basic Latin alphabet.\n\nThe origin of 'F' is the Semitic letter \"vâv\" (or \"waw\") that represented a sound like or . Graphically it originally probably depicted either a hook or a club. It may have been based on a comparable Egyptian hieroglyph such as (transliterated as ḥ(dj)): T3\n\nThe Phoenician form of the letter was adopted into Greek as a vowel, \"upsilon\" (which resembled its descendant 'Y' but was also the ancestor of the Roman letters 'U', 'V', and 'W'); and, with another form, as a consonant, \"digamma\", which indicated the pronunciation , as in Phoenician. Latin 'F,' despite being pronounced differently, is ultimately descended from digamma and closely resembles it in form.\n\nAfter sound changes eliminated from spoken Greek, \"digamma\" was used only as a numeral. However, the Greek alphabet also gave rise to other alphabets, and some of these retained letters descended from digamma. In the Etruscan alphabet, 'F' probably represented , as in Greek, and the Etruscans formed the digraph 'FH' to represent . (At the time these letters were borrowed, there was no Greek letter that represented /f/: the Greek letter phi 'Φ' then represented an aspirated voiceless bilabial plosive , although in Modern Greek it has come to represent .) When the Romans adopted the alphabet, they used 'V' (from Greek \"upsilon\") not only for the vowel , but also for the corresponding semivowel , leaving 'F' available for . And so out of the various \"vav\" variants in the Mediterranean world, the letter F entered the Roman alphabet attached to a sound which its antecedents in Greek and Etruscan did not have. The Roman alphabet forms the basis of the alphabet used today for English and many other languages.\n\nThe lowercase ' f ' is not related to the visually similar long s, ' ſ ' (or medial s). The use of the \"long s\" largely died out by the beginning of the 19th century, mostly to prevent confusion with ' f ' when using a short mid-bar (see more at: S).\n\nIn the English writing system is used to represent the sound , the voiceless labiodental fricative. It is commonly doubled at the end of words. Exceptionally, it represents the voiced labiodental fricative in the common word \"of\".\n\nIn the writing systems of other languages, commonly represents , or .\n\nThe International Phonetic Alphabet uses to represent the voiceless labiodental fricative.\n\n\n\n"}
{"id": "10834", "url": "https://en.wikipedia.org/wiki?curid=10834", "title": "Food preservation", "text": "Food preservation\n\nFood preservation is to prevent the growth of microorganisms (such as yeasts), or other micro-organisms (although some methods work by introducing benign bacteria or fungi to the food), as well as slowing the oxidation of fats that cause rancidity. Food preservation may also include processes that inhibit visual deterioration, such as the enzymatic browning reaction in apples after they are cut during food preparation.\n\nMany processes designed to preserve food involve more than one food preservation method. Preserving fruit by turning it into jam, for example, involves boiling (to reduce the fruit’s moisture content and to kill bacteria, etc.), sugaring (to prevent their re-growth) and sealing within an airtight jar (to prevent recontamination). Some traditional methods of preserving food have been shown to have a lower energy input and carbon footprint, when compared to modern methods.\n\nSome methods of food preservation are known to create carcinogens. In 2015, the International Agency for Research on Cancer of the World Health Organization classified processed meat, i.e. meat that has undergone salting, curing, fermenting, and smoking, as \"carcinogenic to humans\".\n\nMaintaining or creating nutritional value, texture and flavor is an important aspect of food preservation.\n\nNew techniques of food preservation became available to the home chef from the dawn of agriculture until the Industrial Revolution.\n\nThe earliest form of curing was dehydration or drying, used as early as 12,000 BC. Smoking and salting techniques improve on the drying process and add antimicrobial agents that aid in preservation. Smoke deposits a number of pyrolysis products onto the food, including the phenols syringol, guaiacol and catechol. Salt accelerates the drying process using osmosis and also inhibits the growth of several common strains of bacteria. More recently nitrites have been used to cure meat, contributing a characteristic pink colour.\n\nCooling preserves food by slowing down the growth and reproduction of microorganisms and the action of enzymes that causes the food to rot. The introduction of commercial and domestic refrigerators drastically improved the diets of many in the Western world by allowing foods such as fresh fruit, salads and dairy products to be stored safely for longer periods, particularly during warm weather.\n\nBefore the era of mechanical refrigeration, cooling for food storage occurred in the forms of root cellars and iceboxes. Rural people often did their own ice cutting, whereas town and city dwellers often relied on the ice trade. Today, root cellaring remains popular among people who value various goals, including local food, heirloom crops, traditional home cooking techniques, family farming, frugality, self-sufficiency, organic farming, and others.\n\nFreezing is also one of the most commonly used processes, both commercially and domestically, for preserving a very wide range of foods, including prepared foods that would not have required freezing in their unprepared state. For example, potato waffles are stored in the freezer, but potatoes themselves require only a cool dark place to ensure many months' storage. Cold stores provide large-volume, long-term storage for strategic food stocks held in case of national emergency in many countries.\n\nBoiling liquid food items can kill any existing microbes. Milk and water are often boiled to kill any harmful microbes that may be present in them.\n\nHeating to temperatures which are sufficient to kill microorganisms inside the food is a method used with perpetual stews. Milk is also boiled before storing to kill many microorganisms.\n\nThe earliest cultures have used sugar as a preservative, and it was commonplace to store fruit in honey. Similar to pickled foods, sugar cane was brought to Europe through the trade routes. In northern climates without sufficient sun to dry foods, preserves are made by heating the fruit with sugar. \"Sugar tends to draw water from the microbes (plasmolysis). This process leaves the microbial cells dehydrated, thus killing them. In this way, the food will remain safe from microbial spoilage.\" Sugar is used to preserve fruits, either in an antimicrobial syrup with fruit such as apples, pears, peaches, apricots, and plums, or in crystallized form where the preserved material is cooked in sugar to the point of crystallization and the resultant product is then stored dry. This method is used for the skins of citrus fruit (candied peel), angelica, and ginger. Also, sugaring can be used in the production of jam and jelly.\n\nPickling is a method of preserving food in an edible, antimicrobial liquid. Pickling can be broadly classified into two categories: chemical pickling and fermentation pickling.\n\nIn chemical pickling, the food is placed in an edible liquid that inhibits or kills bacteria and other microorganisms. Typical pickling agents include brine (high in salt), vinegar, alcohol, and vegetable oil. Many chemical pickling processes also involve heating or boiling so that the food being preserved becomes saturated with the pickling agent. Common chemically pickled foods include cucumbers, peppers, corned beef, herring, and eggs, as well as mixed vegetables such as piccalilli.\n\nIn fermentation pickling, bacteria in the liquid produce organic acids as preservation agents, typically by a process that produces lactic acid through the presence of lactobacillales. Fermented pickles include sauerkraut, nukazuke, kimchi, and surströmming.\n\nSodium hydroxide (lye) makes food too alkaline for bacterial growth. Lye will saponify fats in the food, which will change its flavor and texture. Lutefisk uses lye in its preparation, as do some olive recipes. Modern recipes for century eggs also call for lye.\n\nCanning involves cooking food, sealing it in sterilized cans or jars, and boiling the containers to kill or weaken any remaining bacteria as a form of sterilization. It was invented by the French confectioner Nicolas Appert. By 1806, this process was used by the French Navy to preserve meat, fruit, vegetables, and even milk. Although Appert had discovered a new way of preservation, it wasn't understood until 1864 when Louis Pasteur found the relationship between microorganisms, food spoilage, and illness.\n\nFoods have varying degrees of natural protection against spoilage and may require that the final step occur in a pressure cooker. High-acid fruits like strawberries require no preservatives to can and only a short boiling cycle, whereas marginal vegetables such as carrots require longer boiling and addition of other acidic elements. Low-acid foods, such as vegetables and meats, require pressure canning. Food preserved by canning or bottling is at immediate risk of spoilage once the can or bottle has been opened.\n\nLack of quality control in the canning process may allow ingress of water or micro-organisms. Most such failures are rapidly detected as decomposition within the can causes gas production and the can will swell or burst. However, there have been examples of poor manufacture (underprocessing) and poor hygiene allowing contamination of canned food by the obligate anaerobe \"Clostridium botulinum\", which produces an acute toxin within the food, leading to severe illness or death. This organism produces no gas or obvious taste and remains undetected by taste or smell. Its toxin is denatured by cooking, however. Cooked mushrooms, handled poorly and then canned, can support the growth of Staphylococcus aureus, which produces a toxin that is not destroyed by canning or subsequent reheating.\n\nFood may be preserved by cooking in a material that solidifies to form a gel. Such materials include gelatin, agar, maize flour, and arrowroot flour. Some foods naturally form a protein gel when cooked, such as eels and elvers, and sipunculid worms, which are a delicacy in Xiamen, in the Fujian province of the People's Republic of China. Jellied eels are a delicacy in the East End of London, where they are eaten with mashed potatoes. Potted meats in aspic (a gel made from gelatin and clarified meat broth) were a common way of serving meat off-cuts in the UK until the 1950s. Many jugged meats are also jellied.\n\nA traditional British way of preserving meat (particularly shrimp) is by setting it in a pot and sealing it with a layer of fat. Also common is potted chicken liver; jellying is one of the steps in producing traditional pâtés.\n\nMeat can be preserved by jugging. Jugging is the process of stewing the meat (commonly game or fish) in a covered earthenware jug or casserole. The animal to be jugged is usually cut into pieces, placed into a tightly-sealed jug with brine or gravy, and stewed. Red wine and/or the animal's own blood is sometimes added to the cooking liquid. Jugging was a popular method of preserving meat up until the middle of the 20th century.\n\nBurial of food can preserve it due to a variety of factors: lack of light, lack of oxygen, cool temperatures, pH level, or desiccants in the soil. Burial may be combined with other methods such as salting or fermentation. Most foods can be preserved in soil that is very dry and salty (thus a desiccant) such as sand, or soil that is frozen.\n\nMany root vegetables are very resistant to spoilage and require no other preservation than storage in cool dark conditions, for example by burial in the ground, such as in a storage clamp. Century eggs are traditionally created by placing eggs in alkaline mud (or other alkaline substance), resulting in their \"inorganic\" fermentation through raised pH instead of spoiling. The fermentation preserves them and breaks down some of the complex, less flavorful proteins and fats into simpler, more flavorful ones. Cabbage was traditionally buried during Autumn in northern US farms for preservation. Some methods keep it crispy while other methods produce sauerkraut. A similar process is used in the traditional production of kimchi. Sometimes meat is buried under conditions that cause preservation. If buried on hot coals or ashes, the heat can kill pathogens, the dry ash can desiccate, and the earth can block oxygen and further contamination. If buried where the earth is very cold, the earth acts like a refrigerator. Before burial, meat (pig/boar) can be fatted. The tallow of the animal is heated and poured over meat in a barrel. Once the fat hardens the barrel is sealed and buried in a cold cellar or ground.\n\nIn Orissa, India, it is practical to store rice by burying it underground. This method helps to store for three to six months during the dry season.\n\nSome foods, such as many cheeses, wines, and beers, use specific micro-organisms that combat spoilage from other less-benign organisms. These micro-organisms keep pathogens in check by creating an environment toxic for themselves and other micro-organisms by producing acid or alcohol. Methods of fermentation include, but are not limited to, starter micro-organisms, salt, hops, controlled (usually cool) temperatures and controlled (usually low) levels of oxygen. These methods are used to create the specific controlled conditions that will support the desirable organisms that produce food fit for human consumption.\n\nFermentation is the microbial conversion of starch and sugars into alcohol. Not only can fermentation produce alcohol, but it can also be a valuable preservation technique. Fermentation can also make foods more nutritious and palatable. For example, drinking water in the Middle Ages was dangerous because it often contained pathogens that could spread disease. When the water is made into beer, the boiling during the brewing process kills any bacteria in the water that could make people sick. Additionally, the water now has the nutrients from the barley and other ingredients, and the microorganisms can also produce vitamins as they ferment.\n\nTechniques of food preservation were developed in research laboratories for commercial applications.\n\nPasteurization is a process for preservation of liquid food. It was originally applied to combat the souring of young local wines. Today, the process is mainly applied to dairy products. In this method, milk is heated at about for 15–30 seconds to kill the bacteria present in it and cooling it quickly to to prevent the remaining bacteria from growing. The milk is then stored in sterilized bottles or pouches in cold places. This method was invented by Louis Pasteur, a French chemist, in 1862.\n\nVacuum-packing stores food in a vacuum environment, usually in an air-tight bag or bottle. The vacuum environment strips bacteria of oxygen needed for survival. Vacuum-packing is commonly used for storing nuts to reduce loss of flavor from oxidization. A major drawback to vacuum packaging, at the consumer level, is that vacuum sealing can deform contents and rob certain foods, such as cheese, of its flavor.\n\nPreservative food additives can be \"antimicrobial\"—which inhibit the growth of bacteria or fungi, including mold—or \"antioxidant\", such as oxygen absorbers, which inhibit the oxidation of food constituents. Common antimicrobial preservatives include calcium propionate, sodium nitrate, sodium nitrite, sulfites (sulfur dioxide, sodium bisulfite, potassium hydrogen sulfite, etc.), and EDTA. Antioxidants include butylated hydroxyanisole (BHA) and butylated hydroxytoluene (BHT). Other preservatives include formaldehyde (usually in solution), glutaraldehyde (insecticide), ethanol, and methylchloroisothiazolinone.\n\nIrradiation of food is the exposure of food to ionizing radiation. The two types of ionizing radiation used are beta particles (high-energy electrons) and gamma rays (emitted from radioactive sources such as cobalt-60 or cesium-137). Treatment effects include killing bacteria, molds, and insect pests, reducing the ripening and spoiling of fruits, and at higher doses inducing sterility. The technology may be compared to pasteurization; it is sometimes called \"cold pasteurization\", as the product is not heated.\n\nThe irradiation process is not directly related to nuclear energy, but does use radioactive isotopes produced in nuclear reactors. Cobalt-60, for example does not occur naturally and can only be produced through neutron bombardment of cobalt-59. Ionizing radiation at high energy levels is hazardous to life (hence its usefulness in sterilisation); for this reason, irradiation facilities have a heavily shielded irradiation room where the process takes place. Radiation safety procedures are used to ensure that neither the workers in such facilities nor the environment receives any radiation dose above administrative limits. Irradiated food does not and cannot become radioactive, and national and international expert bodies have declared food irradiation as wholesome. However, the wholesomeness of consuming such food is disputed by opponents and consumer organizations. National and international expert bodies have declared food irradiation as \"wholesome\"; organizations of the United Nations, such as the World Health Organization and Food and Agriculture Organization, endorse food irradiation. International legislation on whether food may be irradiated or not varies worldwide from no regulation to full banning. Irradiation may allow lower-quality or contaminated foods to be rendered marketable.\n\nApproximately 500,000 tons of food items are irradiated per year worldwide in over 40 countries. These are mainly spices and condiments with an increasing segment of fresh fruit irradiated for fruit fly quarantine.\n\nPulsed electric field (PEF) electroporation is a method for processing cells by means of brief pulses of a strong electric field. PEF holds potential as a type of low-temperature alternative pasteurization process for sterilizing food products. In PEF processing, a substance is placed between two electrodes, then the pulsed electric field is applied. The electric field enlarges the pores of the cell membranes, which kills the cells and releases their contents. PEF for food processing is a developing technology still being researched. There have been limited industrial applications of PEF processing for the pasteurization of fruit juices. To date, several PEF treated juices are available on the market in Europe. Furthermore, for several years a juice pasteurization application in the US has used PEF. For cell disintegration purposes especially potato processors show great interest in PEF technology as an efficient alternative for their preheaters. Potato applications are already operational in the US and Canada. There are also commercial PEF potato applications in various countries in Europe, as well as in Australia, India, and China.\n\nModifying atmosphere is a way to preserve food by operating on the atmosphere around it. Salad crops that are notoriously difficult to preserve are now being packaged in sealed bags with an atmosphere modified to reduce the oxygen (O) concentration and increase the carbon dioxide (CO) concentration. There is concern that, although salad vegetables retain their appearance and texture in such conditions, this method of preservation may not retain nutrients, especially vitamins.\nThere are two methods for preserving grains with carbon dioxide. One method is placing a block of dry ice in the bottom and filling the can with the grain. Another method is purging the container from the bottom by gaseous carbon dioxide from a cylinder or bulk supply vessel.\n\nCarbon dioxide prevents insects and, depending on concentration, mold and oxidation from damaging the grain. Grain stored in this way can remain edible for approximately five years.\n\nNitrogen gas (N) at concentrations of 98% or higher is also used effectively to kill insects in the grain through hypoxia. However, carbon dioxide has an advantage in this respect, as it kills organisms through hypercarbia and hypoxia (depending on concentration), but it requires concentrations of above 35%, or so. This makes carbon dioxide preferable for fumigation in situations where a hermetic seal cannot be maintained.\n\nControlled Atmospheric Storage (CA): \"CA storage is a non-chemical process. Oxygen levels in the sealed rooms are reduced, usually by the infusion of nitrogen gas, from the approximate 21 percent in the air we breathe to 1 percent or 2 percent. Temperatures are kept at a constant . Humidity is maintained at 95 percent and carbon dioxide levels are also controlled. Exact conditions in the rooms are set according to the apple variety. Researchers develop specific regimens for each variety to achieve the best quality. Computers help keep conditions constant.\"\n\"Eastern Washington, where most of Washington’s apples are grown, has enough warehouse storage for 181 million boxes of fruit, according to a report done in 1997 by managers for the Washington State Department of Agriculture Plant Services Division. The storage capacity study shows that 67 percent of that space—enough for 121,008,000 boxes of apples—is CA storage.\"\nAir-tight storage of grains (sometimes called hermetic storage) relies on the respiration of grain, insects, and fungi that can modify the enclosed atmosphere sufficiently to control insect pests. This is a method of great antiquity, as well as having modern equivalents. The success of the method relies on having the correct mix of sealing, grain moisture, and temperature.\n\nA patented process uses fuel cells to exhaust and automatically maintain the exhaustion of oxygen in a shipping container, containing, for example, fresh fish.\n\nThis process subjects the surface of food to a \"flame\" of ionized gas molecules, such as helium or nitrogen. This causes micro-organisms to die off on the surface.\n\nHigh-pressure food preservation or pascalization refers to the use of a food preservation technique that makes use of high pressure. \"Pressed inside a vessel exerting or more, food can be processed so that it retains its fresh appearance, flavor, texture and nutrients while disabling harmful microorganisms and slowing spoilage.\" By 2005, the process was being used for products ranging from orange juice to guacamole to deli meats and widely sold.\n\nBiopreservation is the use of natural or controlled microbiota or antimicrobials as a way of preserving food and extending its shelf life. Beneficial bacteria or the fermentation products produced by these bacteria are used in biopreservation to control spoilage and render pathogens inactive in food. It is a benign ecological approach which is gaining increasing attention.\n\nOf special interest are lactic acid bacteria (LAB). Lactic acid bacteria have antagonistic properties that make them particularly useful as biopreservatives. When LABs compete for nutrients, their metabolites often include active antimicrobials such as lactic acid, acetic acid, hydrogen peroxide, and peptide bacteriocins. Some LABs produce the antimicrobial nisin, which is a particularly effective preservative.\n\nThese days, LAB bacteriocins are used as an integral part of hurdle technology. Using them in combination with other preservative techniques can effectively control spoilage bacteria and other pathogens, and can inhibit the activities of a wide spectrum of organisms, including inherently resistant Gram-negative bacteria.\n\nHurdle technology is a method of ensuring that pathogens in food products can be eliminated or controlled by combining more than one approach. These approaches can be thought of as \"hurdles\" the pathogen has to overcome if it is to remain active in the food. The right combination of hurdles can ensure all pathogens are eliminated or rendered harmless in the final product.\n\nHurdle technology has been defined by Leistner (2000) as an intelligent combination of hurdles that secures the microbial safety and stability as well as the organoleptic and nutritional quality and the economic viability of food products. The organoleptic quality of the food refers to its sensory properties, that is its look, taste, smell, and texture.\n\nExamples of hurdles in a food system are high temperature during processing, low temperature during storage, increasing the acidity, lowering the water activity or redox potential, and the presence of preservatives or biopreservatives. According to the type of pathogens and how risky they are, the intensity of the hurdles can be adjusted individually to meet consumer preferences in an economical way, without sacrificing the safety of the product.\n\n\n"}
{"id": "10835", "url": "https://en.wikipedia.org/wiki?curid=10835", "title": "Frequency modulation", "text": "Frequency modulation\n\nIn telecommunications and signal processing, frequency modulation (FM) is the encoding of information in a carrier wave by varying the instantaneous frequency of the wave. This contrasts with amplitude modulation, in which the amplitude of the carrier wave varies, while the frequency remains constant.\n\nIn analog frequency modulation, such as FM radio broadcasting of an audio signal representing voice or music, the instantaneous frequency deviation, the difference between the frequency of the carrier and its center frequency, is proportional to the modulating signal.\n\nDigital data can be encoded and transmitted via FM by shifting the carrier's frequency among a predefined set of frequencies representing digits - for example one frequency can represent a binary 1 and a second can represent binary 0. This modulation technique is known as frequency-shift keying (FSK). FSK is widely used in modems and fax modems, and can also be used to send Morse code. Radioteletype also uses FSK.\n\nFrequency modulation is widely used for FM radio broadcasting. It is also used in telemetry, radar, seismic prospecting, and monitoring newborns for seizures via EEG, two-way radio systems, music synthesis, magnetic tape-recording systems and some video-transmission systems. In radio transmission, an advantage of frequency modulation is that it has a larger signal-to-noise ratio and therefore rejects radio frequency interference better than an equal power amplitude modulation (AM) signal. For this reason, most music is broadcast over FM radio.\n\nFrequency modulation has a close relationship with phase modulation; phase modulation is often used as an intermediate step to achieve frequency modulation. Mathematically both of these are considered a special case of quadrature amplitude modulation (QAM).\n\nIf the information to be transmitted (i.e., the baseband signal) is formula_1 and the sinusoidal carrier is formula_2, where \"f\" is the carrier's base frequency, and \"A\" is the carrier's amplitude, the modulator combines the carrier with the baseband data signal to get the transmitted signal:\n\nwhere formula_4 = formula_5formula_6 , formula_5 being the sensitivity of the frequency modulator and formula_6 being the amplitude of the modulating signal or baseband signal.\n\nIn this equation, formula_9 is the \"instantaneous frequency\" of the oscillator and formula_4 is the \"frequency deviation\", which represents the maximum shift away from \"f\" in one direction, assuming \"x\"(\"t\") is limited to the range ±1.\n\nWhile most of the energy of the signal is contained within \"f\" ± \"f\", it can be shown by Fourier analysis that a wider range of frequencies is required to precisely represent an FM signal. The frequency spectrum of an actual FM signal has components extending infinitely, although their amplitude decreases and higher-order components are often neglected in practical design problems.\n\nMathematically, a baseband modulated signal may be approximated by a sinusoidal continuous wave signal with a frequency \"f\".This method is also named as Single-tone Modulation.The integral of such a signal is:\n\nIn this case, the expression for y(t) above simplifies to:\n\nwhere the amplitude formula_13 of the modulating sinusoid is represented by the peak deviation formula_4 (see frequency deviation).\n\nThe harmonic distribution of a sine wave carrier modulated by such a sinusoidal signal can be represented with Bessel functions; this provides the basis for a mathematical understanding of frequency modulation in the frequency domain.\n\nAs in other modulation systems, the modulation index indicates by how much the modulated variable varies around its unmodulated level. It relates to variations in the carrier frequency:\n\nwhere formula_16 is the highest frequency component present in the modulating signal \"x\"(\"t\"), and formula_17 is the peak frequency-deviation—i.e. the maximum deviation of the \"instantaneous frequency\" from the carrier frequency. For a sine wave modulation, the modulation index is seen to be the ratio of the peak frequency deviation of the carrier wave to the frequency of the modulating sine wave.\n\nIf formula_18, the modulation is called narrowband FM, and its bandwidth is approximately formula_19. Sometimes modulation index h<0.3 rad is considered as Narrowband FM otherwise Wideband FM.\n\nFor digital modulation systems, for example Binary Frequency Shift Keying (BFSK), where a binary signal modulates the carrier, the modulation index is given by:\n\nwhere formula_21 is the symbol period, and formula_22 is used as the highest frequency of the modulating binary waveform by convention, even though it would be more accurate to say it is the highest \"fundamental\" of the modulating binary waveform. In the case of digital modulation, the carrier formula_23 is never transmitted. Rather, one of two frequencies is transmitted, either formula_24 or formula_25, depending on the binary state 0 or 1 of the modulation signal.\n\nIf formula_26, the modulation is called \"wideband FM\" and its bandwidth is approximately formula_27. While wideband FM uses more bandwidth, it can improve the signal-to-noise ratio significantly; for example, doubling the value of formula_17, while keeping formula_29 constant, results in an eight-fold improvement in the signal-to-noise ratio. (Compare this with Chirp spread spectrum, which uses extremely wide frequency deviations to achieve processing gains comparable to traditional, better-known spread-spectrum modes).\n\nWith a tone-modulated FM wave, if the modulation frequency is held constant and the modulation index is increased, the (non-negligible) bandwidth of the FM signal increases but the spacing between spectra remains the same; some spectral components decrease in strength as others increase. If the frequency deviation is held constant and the modulation frequency increased, the spacing between spectra increases.\n\nFrequency modulation can be classified as narrowband if the change in the carrier frequency is about the same as the signal frequency, or as wideband if the change in the carrier frequency is much higher (modulation index >1) than the signal frequency.\n\nFor the case of a carrier modulated by a single sine wave, the resulting frequency spectrum can be calculated using Bessel functions of the first kind, as a function of the sideband number and the modulation index. The carrier and sideband amplitudes are illustrated for different modulation indices of FM signals. For particular values of the modulation index, the carrier amplitude becomes zero and all the signal power is in the sidebands.\n\nSince the sidebands are on both sides of the carrier, their count is doubled, and then multiplied by the modulating frequency to find the bandwidth. For example, 3 kHz deviation modulated by a 2.2 kHz audio tone produces a modulation index of 1.36. Suppose that we limit ourselves to only those sidebands that have a relative amplitude of at least 0.01. Then, examining the chart shows this modulation index will produce three sidebands. These three sidebands, when doubled, gives us (6 * 2.2 kHz) or a 13.2 kHz required bandwidth.\nA rule of thumb, \"Carson's rule\" states that nearly all (~98 percent) of the power of a frequency-modulated signal lies within a bandwidth formula_30 of:\n\nwhere formula_33, as defined above, is the peak deviation of the instantaneous frequency formula_34 from the center carrier frequency formula_35, formula_36 is the Modulation index which is the ratio of frequency deviation to highest frequency in the modulating signal andformula_16 is the highest frequency in the modulating signal.\nCondition for application of Carson's rule is only sinusoidal signals.\n\nwhere W is the highest frequency in the modulating signal but non-sinusoidal in nature and D is the Deviation ratio which the ratio of frequency deviation to highest frequency of modulating non-sinusoidal signal.\n\nFM provides improved Signal-to-noise ratio (SNR), as compared for example with AM. Compared with an optimum AM scheme, FM typically has poorer SNR below a certain signal level called the noise threshold, but above a higher level – the full improvement or full quieting threshold – the SNR is much improved over AM. The improvement depends on modulation level and deviation. For typical voice communications channels, improvements are typically 5-15 dB. FM broadcasting using wider deviation can achieve even greater improvements. Additional techniques, such as pre-emphasis of higher audio frequencies with corresponding de-emphasis in the receiver, are generally used to improve overall SNR in FM circuits. Since FM signals have constant amplitude, FM receivers normally have limiters that remove AM noise, further improving SNR.\n\nFM signals can be generated using either direct or indirect frequency modulation:\n\nMany FM detector circuits exist. A common method for recovering the information signal is through a Foster-Seeley discriminator. A phase-locked loop can be used as an FM demodulator. \"Slope detection\" demodulates an FM signal by using a tuned circuit which has its resonant frequency slightly offset from the carrier. As the frequency rises and falls the tuned circuit provides a changing amplitude of response, converting FM to AM. AM receivers may detect some FM transmissions by this means, although it does not provide an efficient means of detection for FM broadcasts.\n\nFM is also used at intermediate frequencies by analog VCR systems (including VHS) to record the luminance (black and white) portions of the video signal. Commonly, the chrominance component is recorded as a conventional AM signal, using the higher-frequency FM signal as bias. FM is the only feasible method of recording the luminance (\"black and white\") component of video to (and retrieving video from) magnetic tape without distortion; video signals have a large range of frequency components – from a few hertz to several megahertz, too wide for equalizers to work with due to electronic noise below −60 dB. FM also keeps the tape at saturation level, acting as a form of noise reduction; a limiter can mask variations in playback output, and the FM capture effect removes print-through and pre-echo. A continuous pilot-tone, if added to the signal – as was done on V2000 and many Hi-band formats – can keep mechanical jitter under control and assist timebase correction.\n\nThese FM systems are unusual, in that they have a ratio of carrier to maximum modulation frequency of less than two; contrast this with FM audio broadcasting, where the ratio is around 10,000. Consider, for example, a 6-MHz carrier modulated at a 3.5-MHz rate; by Bessel analysis, the first sidebands are on 9.5 and 2.5 MHz and the second sidebands are on 13 MHz and −1 MHz. The result is a reversed-phase sideband on +1 MHz; on demodulation, this results in unwanted output at 6−1 = 5 MHz. The system must be designed so that this unwanted output is reduced to an acceptable level.\n\nFM is also used at audio frequencies to synthesize sound. This technique, known as FM synthesis, was popularized by early digital synthesizers and became a standard feature in several generations of personal computer sound cards.\n\nEdwin Howard Armstrong (1890–1954) was an American electrical engineer who invented wideband frequency modulation (FM) radio.\nHe patented the regenerative circuit in 1914, the superheterodyne receiver in 1918 and the super-regenerative circuit in 1922. Armstrong presented his paper, \"A Method of Reducing Disturbances in Radio Signaling by a System of Frequency Modulation\", (which first described FM radio) before the New York section of the Institute of Radio Engineers on November 6, 1935. The paper was published in 1936.\n\nAs the name implies, wideband FM (WFM) requires a wider signal bandwidth than amplitude modulation by an equivalent modulating signal; this also makes the signal more robust against noise and interference. Frequency modulation is also more robust against signal-amplitude-fading phenomena. As a result, FM was chosen as the modulation standard for high frequency, high fidelity radio transmission, hence the term \"FM radio\" (although for many years the BBC called it \"VHF radio\" because commercial FM broadcasting uses part of the VHF band—the FM broadcast band). FM receivers employ a special detector for FM signals and exhibit a phenomenon known as the \"capture effect\", in which the tuner \"captures\" the stronger of two stations on the same frequency while rejecting the other (compare this with a similar situation on an AM receiver, where both stations can be heard simultaneously). However, frequency drift or a lack of selectivity may cause one station to be overtaken by another on an adjacent channel. Frequency drift was a problem in early (or inexpensive) receivers; inadequate selectivity may affect any tuner.\n\nAn FM signal can also be used to carry a stereo signal; this is done with multiplexing and demultiplexing before and after the FM process. The FM modulation and demodulation process is identical in stereo and monaural processes. A high-efficiency radio-frequency switching amplifier can be used to transmit FM signals (and other constant-amplitude signals). For a given signal strength (measured at the receiver antenna), switching amplifiers use less battery power and typically cost less than a linear amplifier. This gives FM another advantage over other modulation methods requiring linear amplifiers, such as AM and QAM.\n\nFM is commonly used at VHF radio frequencies for high-fidelity broadcasts of music and speech. Analog TV sound is also broadcast using FM. Narrowband FM is used for voice communications in commercial and amateur radio settings. In broadcast services, where audio fidelity is important, wideband FM is generally used. In two-way radio, narrowband FM (NBFM) is used to conserve bandwidth for land mobile, marine mobile and other radio services.\n\nThere are reports that on October 5, 1924, Professor Mikhail A. Bonch-Bruevich, during a scientific and technical conversation in the Nizhny Novgorod Radio Laboratory, reported about his new method of telephony, based on a change in the period of oscillations. Demonstration of frequency modulation was carried out on the laboratory model.\n\n\n\n"}
{"id": "10837", "url": "https://en.wikipedia.org/wiki?curid=10837", "title": "Faith and rationality", "text": "Faith and rationality\n\nFaith and rationality are two ideologies that exist in varying degrees of conflict or compatibility. Rationality is based on reason or facts. Faith is belief in inspiration, revelation, or authority. The word \"faith\" sometimes refers to a belief that is held with lack of reason or evidence, a belief that is held in spite of or against reason or evidence, or it can refer to belief based upon a degree of evidential warrant.\n\nAlthough the words \"faith\" and \"belief\" are sometimes erroneously conflated and used as synonyms, \"faith\" properly refers to a particular type (or subset) of \"belief,\" as defined above.\n\nBroadly speaking, there are two categories of views regarding the relationship between faith and rationality:\n\nThe Catholic Church also has taught that true faith and correct reason can and must work together, and, viewed properly, can never be in conflict with one another, as both have their origin in God, as stated in the Papal encyclical letter issued by Pope John Paul II, \"Fides et Ratio\" (\"[On] Faith and Reason\").\n\nFrom at least the days of the Greek Philosophers, the relationship between faith and reason has been hotly debated. Plato argued that knowledge is simply memory of the eternal. Aristotle set down rules by which knowledge could be discovered by reason.\n\nRationalists point out that many people hold irrational beliefs, for many reasons. There may be evolutionary causes for irrational beliefs — irrational beliefs may increase our ability to survive and reproduce. Or, according to Pascal's Wager, it may be to our advantage to have faith, because faith may promise infinite rewards, while the rewards of reason are seen by many as finite. One more reason for irrational beliefs can perhaps be explained by operant conditioning. For example, in one study by B. F. Skinner in 1948, pigeons were awarded grain at regular time intervals regardless of their behaviour. The result was that each of pigeons developed their own idiosyncratic response which had become associated with the consequence of receiving grain.\n\nBelievers in faith — for example those who believe salvation is possible through faith alone — frequently suggest that everyone holds beliefs arrived at by faith, not reason. The belief that the universe is a sensible place and that our minds allow us to arrive at correct conclusions about it, is a belief we hold through faith. Rationalists contend that this is arrived at because they have observed the world being consistent and sensible, not because they have faith that it is.\n\nBeliefs held \"by faith\" may be seen existing in a number of relationships to rationality:\n\nSt. Thomas Aquinas, the most important doctor of the Catholic Church, was the first to write a full treatment of the relationship, differences, and similarities between faith—an intellectual assent—and reason, predominately in his \"Summa Theologica\", \"De Veritate\", and \"Summa contra Gentiles\".\n\nThe Council of Trent's catechism—the \"Roman Catechism\", written during the Catholic Church's Counter-Reformation to combat Protestantism and Martin Luther's antimetaphysical tendencies.\n\n\"Dei Filius\" was a dogmatic constitution of the First Vatican Council on the Roman Catholic faith. It was adopted unanimously on 24 April 1870 and was influenced by the philosophical conceptions of Johann Baptist Franzelin, who had written a great deal on the topic of faith and rationality.\n\nBecause the Roman Catholic Church does not disparage reason, but rather affirms its veracity and utility, there have been many Catholic scientists over the ages.\n\nTwentieth-century Thomist philosopher Étienne Gilson wrote about faith and reason in his 1922 book \"Le Thomisme\". His contemporary Jacques Maritain wrote about it in his \"The Degrees of Knowledge\".\n\n\"Fides et Ratio\" is an encyclical promulgated by Pope John Paul II on 14 September 1998. It deals with the relationship between faith and reason.\n\nPope Benedict XVI's 12 September 2006 Regensburg Lecture was about faith and reason.\n\nSome have asserted that Martin Luther taught that faith and reason were antithetical in the sense that questions of faith could not be illuminated by reason. Contemporary Lutheran scholarship however has found a different reality in Luther. Luther rather seeks to separate faith and reason in order to honor the separate spheres of knowledge that each understand. Bernhard Lohse for example has demonstrated in his classic work \"Fides Und Ratio\" that Luther ultimately sought to put the two together. More recently Hans-Peter Großhans has demonstrated that Luther's work on Bibilical Criticism stresses the need for external coherence in right exegetical method. This means that for Luther it is more important that the Bible be reasonable according to the reality outside of the scriptures than that the Bible make sense to itself, that it has internal coherence. The right tool for understanding the world outside of the Bible for Luther is none other than Reason which for Luther denoted science, philosophy, history and empirical observation. Here a differing picture is presented of a Luther who deeply valued both faith and reason, and held them in dialectical partnership. Luther's concern thus in separating them is honoring their different epistemological spheres.\n\nThe view that faith underlies all rationality holds that rationality is dependent on faith for its coherence. Under this view, there is no way to comprehensively \"prove\" that we are actually seeing what we appear to be seeing, that what we remember actually happened, or that the laws of logic and mathematics are actually real. Instead, all beliefs depend for their coherence on \"faith\" in our senses, memory, and reason, because the foundations of rationalism cannot be proven by evidence or reason. Rationally, you can not prove anything you see is real, but you can prove that you yourself are real, and rationalist belief would be that you can believe that the world is consistent until something demonstrates inconsistency. This differs from faith based belief, where you believe that your world view is consistent no matter what inconsistencies the world has with your beliefs.\n\nIn this view, there are many beliefs that are held by faith alone, that rational thought would force the mind to reject. As an example, many people believe in the Biblical story of Noah's flood: that the entire Earth was covered by water for forty days. But objected that most plants cannot survive being covered by water for that length of time, a boat of that magnitude could not have been built by wood, and there would be no way for two of every animal to survive on that ship and migrate back to their place of origin. (such as penguins), Although Christian apologists offer answers to these and such issues, under the premise that such responses are insufficient, one must choose between accepting the story on faith and rejecting reason, or rejecting the story by reason and thus rejecting faith.\n\nWithin the rationalist point of view, there remains the possibility of multiple rational explanations. For example, considering the biblical story of Noah's flood, one making rational determinations about the probability of the events does so via interpretation of modern evidence. Two observers of the story may provide different plausible explanations for the life of plants, construction of the boat, species living at the time, and migration following the flood. Some see this as meaning that a person is not strictly bound to choose between faith and reason.\n\nAmerican biblical scholar Archibald Thomas Robertson stated that the Greek word \"pistis\" used for faith in the New Testament (over two hundred forty times), and rendered \"assurance\" in Acts 17:31 (KJV), is \"an old verb to furnish, used regularly by Demosthenes for bringing forward evidence.\" Likewise Tom Price (Oxford Centre for Christian Apologetics) affirms that when the New Testament talks about faith positively it only uses words derived from the Greek root [pistis] which means \"to be persuaded.\"\n\nIn contrast to faith meaning blind trust, in the absence of evidence, even in the teeth of evidence, Alister McGrath quotes Oxford Anglican theologian W. H. Griffith-Thomas, (1861-1924), who states faith is \"not blind, but intelligent\" and \"commences with the conviction of the mind based on adequate evidence...\", which McGrath sees as \"a good and reliable definition, synthesizing the core elements of the characteristic Christian understanding of faith.\"\n\nAlvin Plantinga upholds that faith may be the result of evidence testifying to the reliability of the source of truth claims, but although it may involve this, he sees faith as being the result of hearing the truth of the gospel with the internal persuasion by the Holy Spirit moving and enabling him to believe. \"Christian belief is produced in the believer by the internal instigation of the Holy Spirit, endorsing the teachings of Scripture, which is itself divinely inspired by the Holy Spirit. The result of the work of the Holy Spirit is faith.\"\n\nThe 14th Century Jewish philosopher Levi ben Gerson tried to reconcile faith and reason. He wrote, \"The Torah cannot prevent us from considering to be true that which our reason urges us to believe.\" His contemporary Hasdai ben Abraham Crescas argued the contrary view, that reason is weak and faith strong, and that only through faith can we discover the fundamental truth that God is love, that through faith alone can we endure the suffering that is the common lot of God's chosen people.\n\n\n\n\n"}
{"id": "10839", "url": "https://en.wikipedia.org/wiki?curid=10839", "title": "List of film institutes", "text": "List of film institutes\n\nSome notable institutions celebrating film, including both national film institutes and independent and non-profit organizations. For the purposes of this list, institutions that do not have their own article on Wikipedia are not considered notable.\n\n\n"}
{"id": "10841", "url": "https://en.wikipedia.org/wiki?curid=10841", "title": "Forth", "text": "Forth\n\nForth or FORTH may refer to:\n\n\n\n\n\n"}
{"id": "10842", "url": "https://en.wikipedia.org/wiki?curid=10842", "title": "F wave", "text": "F wave\n\nIn neuroscience, an F wave is the second of two voltage changes observed after electrical stimulation is applied to the skin surface above the distal region of a nerve. F waves are often used to measure nerve conduction velocity, and are particularly useful for evaluating conduction problems in the proximal region of nerves (i.e., portions of nerves near the spinal cord).\n\nIt's called F wave because it was initially recorded in the foot muscles.\n\nIn a typical F wave study, a strong electrical stimulus (supramaximal stimulation) is applied to the skin surface above the distal portion of a nerve so that the impulse travels both distally (towards the muscle fiber) and proximally (back to the motor neurons of the spinal cord). (These directions are also known as orthodromic and antidromic, respectively.) When the \"orthodromic\" stimulus reaches the muscle fiber, it elicits a strong M-response indicative of muscle contraction. When the \"antidromic\" stimulus reaches the motor neuron cell bodies, a small portion of the motor neurons backfire and orthodromic wave travels back down the nerve towards the muscle. This reflected stimulus evokes small proportion of the muscle fibers causing a small, second CMAP called the F wave.\n\nBecause a different population of anterior horn cells is stimulated with each stimulation, each F wave have a slightly different shape, amplitude and latency.\n\nF wave properties include:\n\n\nSeveral measurements can be done on the F responses, including minimal and maximal latencies, and F wave persistence.\n\nThe minimal F wave latency is typically 25-32 ms in the upper extremities, and 45-56 ms in the lower extremities.\n\nF wave persistence is the number of F waves obtained per the number of stimulations, which is normally 80-100% (or above 50%).\n\n\n"}
{"id": "10843", "url": "https://en.wikipedia.org/wiki?curid=10843", "title": "Fruit", "text": "Fruit\n\nIn botany, a fruit is the seed-bearing structure in flowering plants (also known as angiosperms) formed from the ovary after flowering.\n\nFruits are the means by which angiosperms disseminate seeds. Edible fruits, in particular, have propagated with the movements of humans and animals in a symbiotic relationship as a means for seed dispersal and nutrition; in fact, humans and many animals have become dependent on fruits as a source of food. Accordingly, fruits account for a substantial fraction of the world's agricultural output, and some (such as the apple and the pomegranate) have acquired extensive cultural and symbolic meanings.\n\nIn common language usage, \"fruit\" normally means the fleshy seed-associated structures of a plant that are sweet or sour, and edible in the raw state, such as apples, bananas, grapes, lemons, oranges, and strawberries. On the other hand, in botanical usage, \"fruit\" includes many structures that are not commonly called \"fruits\", such as bean pods, corn kernels, tomatoes, and wheat grains. The section of a fungus that produces spores is also called a fruiting body.\n\nMany common terms for seeds and fruit do not correspond to the botanical classifications. In culinary terminology, a \"fruit\" is usually any sweet-tasting plant part, especially a botanical fruit; a \"nut\" is any hard, oily, and shelled plant product; and a \"vegetable\" is any savory or less sweet plant product. However, in botany, a \"fruit\" is the ripened ovary or carpel that contains seeds, a \"nut\" is a type of fruit and not a seed, and a \"seed\" is a ripened ovule.\n\nExamples of culinary \"vegetables\" and nuts that are botanically fruit include corn, cucurbits (e.g., cucumber, pumpkin, and squash), eggplant, legumes (beans, peanuts, and peas), sweet pepper, and tomato. In addition, some spices, such as allspice and chili pepper, are fruits, botanically speaking. In contrast, rhubarb is often referred to as a fruit, because it is used to make sweet desserts such as pies, though only the petiole (leaf stalk) of the rhubarb plant is edible, and edible gymnosperm seeds are often given fruit names, e.g., ginkgo nuts and pine nuts.\n\nBotanically, a cereal grain, such as corn, rice, or wheat, is also a kind of fruit, termed a caryopsis. However, the fruit wall is very thin and is fused to the seed coat, so almost all of the edible grain is actually a seed.\n\nThe outer, often edible layer, is the \"pericarp\", formed from the ovary and surrounding the seeds, although in some species other tissues contribute to or form the edible portion. The pericarp may be described in three layers from outer to inner, the \"epicarp\", \"mesocarp\" and \"endocarp\".\n\nFruit that bears a prominent pointed terminal projection is said to be \"beaked\".\n\nA fruit results from maturation of one or more flowers, and the gynoecium of the flower(s) forms all or part of the fruit.\n\nInside the ovary/ovaries are one or more ovules where the megagametophyte contains the egg cell. After double fertilization, these ovules will become seeds. The ovules are fertilized in a process that starts with pollination, which involves the movement of pollen from the stamens to the stigma of flowers. After pollination, a tube grows from the pollen through the stigma into the ovary to the ovule and two sperm are transferred from the pollen to the megagametophyte. Within the megagametophyte one of the two sperm unites with the egg, forming a zygote, and the second sperm enters the central cell forming the endosperm mother cell, which completes the double fertilization process. Later the zygote will give rise to the embryo of the seed, and the endosperm mother cell will give rise to endosperm, a nutritive tissue used by the embryo.\n\nAs the ovules develop into seeds, the ovary begins to ripen and the ovary wall, the \"pericarp\", may become fleshy (as in berries or drupes), or form a hard outer covering (as in nuts). In some multiseeded fruits, the extent to which the flesh develops is proportional to the number of fertilized ovules. The pericarp is often differentiated into two or three distinct layers called the \"exocarp\" (outer layer, also called epicarp), \"mesocarp\" (middle layer), and \"endocarp\" (inner layer). In some fruits, especially simple fruits derived from an inferior ovary, other parts of the flower (such as the floral tube, including the petals, sepals, and stamens), fuse with the ovary and ripen with it. In other cases, the sepals, petals and/or stamens and style of the flower fall off. When such other floral parts are a significant part of the fruit, it is called an \"accessory fruit\". Since other parts of the flower may contribute to the structure of the fruit, it is important to study flower structure to understand how a particular fruit forms.\n\nThere are three general modes of fruit development:\n\nPlant scientists have grouped fruits into three main groups, simple fruits, aggregate fruits, and composite or multiple fruits. The groupings are not evolutionarily relevant, since many diverse plant taxa may be in the same group, but reflect how the flower organs are arranged and how the fruits develop.\n\nSimple fruits can be either dry or fleshy, and result from the ripening of a simple or compound ovary in a flower with only one pistil. Dry fruits may be either dehiscent (they open to discharge seeds), or indehiscent (they do not open to discharge seeds). Types of dry, simple fruits, and examples of each, include:\n\nFruits in which part or all of the \"pericarp\" (fruit wall) is fleshy at maturity are \"simple fleshy fruits\". Types of simple, fleshy, fruits (with examples) include:\nAn aggregate fruit, or \"etaerio\", develops from a single flower with numerous simple pistils.\n\n\nThe pome fruits of the family Rosaceae, (including apples, pears, rosehips, and saskatoon berry) are a syncarpous fleshy fruit, a simple fruit, developing from a half-inferior ovary.\n\nSchizocarp fruits form from a syncarpous ovary and do not really dehisce, but rather split into segments with one or more seeds; they include a number of different forms from a wide range of families. Carrot seed is an example.\n\nAggregate fruits form from single flowers that have multiple carpels which are not joined together, i.e. each pistil contains one carpel. Each pistil forms a fruitlet, and collectively the fruitlets are called an etaerio. Four types of aggregate fruits include etaerios of achenes, follicles, drupelets, and berries. Ranunculaceae species, including \"Clematis\" and \"Ranunculus\" have an etaerio of achenes, \"Calotropis\" has an etaerio of follicles, and \"Rubus\" species like raspberry, have an etaerio of drupelets. \"Annona\" have an etaerio of berries.\n\nThe raspberry, whose pistils are termed \"drupelets\" because each is like a small drupe attached to the receptacle. In some bramble fruits (such as blackberry) the receptacle is elongated and part of the ripe fruit, making the blackberry an \"aggregate-accessory\" fruit. The strawberry is also an aggregate-accessory fruit, only one in which the seeds are contained in achenes. In all these examples, the fruit develops from a single flower with numerous pistils.\n\nA multiple fruit is one formed from a cluster of flowers (called an \"inflorescence\"). Each flower produces a fruit, but these mature into a single mass. Examples are the pineapple, fig, mulberry, osage-orange, and breadfruit.\nIn the photograph on the right, stages of flowering and fruit development in the noni or Indian mulberry (\"Morinda citrifolia\") can be observed on a single branch. First an inflorescence of white flowers called a head is produced. After fertilization, each flower develops into a drupe, and as the drupes expand, they become \"connate\" (merge) into a \"multiple fleshy fruit\" called a \"syncarp\".\n\nBerries are another type of fleshy fruit; they are simple fruit created from a single ovary. The ovary may be compound, with several carpels. Types include (examples follow in the table below):\n\nSome or all of the edible part of accessory fruit is not generated by the ovary. Accessory fruit can be simple, aggregate, or multiple, i.e., they can include one or more pistils and other parts from the same flower, or the pistils and other parts of many flowers.\n\nSeedlessness is an important feature of some fruits of commerce. Commercial cultivars of bananas and pineapples are examples of seedless fruits. Some cultivars of citrus fruits (especially grapefruit, mandarin oranges, navel oranges), satsumas, table grapes, and watermelons are valued for their seedlessness. In some species, seedlessness is the result of \"parthenocarpy\", where fruits set without fertilization. Parthenocarpic fruit set may or may not require pollination, but most seedless citrus fruits require a stimulus from pollination to produce fruit.\n\nSeedless bananas and grapes are triploids, and seedlessness results from the abortion of the embryonic plant that is produced by fertilization, a phenomenon known as \"stenospermocarpy\", which requires normal pollination and fertilization.\n\nVariations in fruit structures largely depend on their seeds' mode of dispersal. This dispersal can be achieved by animals, explosive dehiscence, water, or wind.\n\nSome fruits have coats covered with spikes or hooked burrs, either to prevent themselves from being eaten by animals, or to stick to the feathers, hairs, or legs of animals, using them as dispersal agents. Examples include cocklebur and unicorn plant.\n\nThe sweet flesh of many fruits is \"deliberately\" appealing to animals, so that the seeds held within are eaten and \"unwittingly\" carried away and deposited (i.e., defecated) at a distance from the parent. Likewise, the nutritious, oily kernels of nuts are appealing to rodents (such as squirrels), which hoard them in the soil to avoid starving during the winter, thus giving those seeds that remain uneaten the chance to germinate and grow into a new plant away from their parent.\n\nOther fruits are elongated and flattened out naturally, and so become thin, like wings or helicopter blades, e.g., elm, maple, and tuliptree. This is an evolutionary mechanism to increase dispersal distance away from the parent, via wind. Other wind-dispersed fruit have tiny \"parachutes\", e.g., dandelion, milkweed, salsify.\n\nCoconut fruits can float thousands of miles in the ocean to spread seeds. Some other fruits that can disperse via water are nipa palm and screw pine.\n\nSome fruits fling seeds substantial distances (up to 100 m in sandbox tree) via explosive dehiscence or other mechanisms, e.g., impatiens and squirting cucumber.\n\nMany hundreds of fruits, including fleshy fruits (like apple, kiwifruit, mango, peach, pear, and watermelon) are commercially valuable as human food, eaten both fresh and as jams, marmalade and other preserves. Fruits are also used in manufactured foods (e.g., cakes, cookies, ice cream, muffins, or yogurt) or beverages, such as fruit juices (e.g., apple juice, grape juice, or orange juice) or alcoholic beverages (e.g., brandy, fruit beer, or wine). Fruits are also used for gift giving, e.g., in the form of Fruit Baskets and Fruit Bouquets.\n\nMany \"vegetables\" in culinary \"parlance\" are botanical fruits, including bell pepper, cucumber, eggplant, green bean, okra, pumpkin, squash, tomato, and zucchini. Olive fruit is pressed for olive oil. Spices like allspice, black pepper, paprika, and vanilla are derived from berries.\n\nFresh fruits are generally high in fiber, vitamin C, and water.\n\nRegular consumption of fruit is generally associated with reduced risks of several diseases and functional declines associated with aging.\n\nBecause fruits have been such a major part of the human diet, various cultures have developed many different uses for fruits they do not depend on for food. For example:\n\nFor food safety, the CDC recommends proper fruit handling and preparation to reduce the risk of food contamination and foodborne illness. Fresh fruits and vegetables should be carefully selected; at the store, they should not be damaged or bruised; and precut pieces should be refrigerated or surrounded by ice.\n\nAll fruits and vegetables should be rinsed before eating. This recommendation also applies to produce with rinds or skins that are not eaten. It should be done just before preparing or eating to avoid premature spoilage.\n\nFruits and vegetables should be kept separate from raw foods like meat, poultry, and seafood, as well as from utensils that have come in contact with raw foods. Fruits and vegetables that are not going to be cooked should be thrown away if they have touched raw meat, poultry, seafood, or eggs.\n\nAll cut, peeled, or cooked fruits and vegetables should be refrigerated within two hours. After a certain time, harmful bacteria may grow on them and increase the risk of foodborne illness.\n\nFruit allergies make up about 10 percent of all food related allergies.\n\nAll fruits benefit from proper post harvest care, and in many fruits, the plant hormone ethylene causes ripening. Therefore, maintaining most fruits in an efficient cold chain is optimal for post harvest storage, with the aim of extending and ensuring shelf life.\n\n\n"}
{"id": "10844", "url": "https://en.wikipedia.org/wiki?curid=10844", "title": "French materialism", "text": "French materialism\n\nFrench materialism is the name given to a handful of French 18th-century philosophers during the Age of Enlightenment, many of them clustered around the salon of Baron d'Holbach. Although there are important differences between them, all of them were materialists who believed that the world was made up of a single substance, matter, the motions and properties of which could be used to explain all phenomena. \n\nProminent French materialists of the 18th century include:\n\n\n"}
{"id": "10845", "url": "https://en.wikipedia.org/wiki?curid=10845", "title": "February", "text": "February\n\nFebruary is the second month of the year in the Julian and Gregorian calendars. It is the shortest month of the year as it is the only month to have a length of less than 30 days. The month has 28 days in common years or 29 days in leap years, with the quadrennial 29th day being called the \"leap day.\"\n\nFebruary is the third month of meteorological winter in the Northern Hemisphere. In the Southern Hemisphere, February is the last month of summer (the seasonal equivalent of August in the Northern Hemisphere, in meteorological reckoning).\n\nFebruary may be pronounced either as ( or ). Many people pronounce it as ( rather than ), as if it were spelled \"Feb-u-ary\". This comes about by analogy with \"January\" (which ends in \"-uary\" but not \"-ruary\"), as well as by a dissimilation effect whereby having two \"r\"s close to each other causes one to change for ease of pronunciation.\n\nThe Roman month \"Februarius\" was named after the Latin term \"februum\", which means \"purification\", via the purification ritual \"Februa\" held on February 15 (full moon) in the old lunar Roman calendar. January and February were the last two months to be added to the Roman calendar, since the Romans originally considered winter a monthless period. They were added by Numa Pompilius about 713 BC. February remained the last month of the calendar year until the time of the decemvirs (c. 450 BC), when it became the second month. At certain intervals February was truncated to 23 or 24 days, and a 27-day intercalary month, Intercalaris, was inserted immediately after February to realign the year with the seasons.\n\nFebruary observances in Ancient Rome include Amburbium (precise date unknown, Sementivae (February 2), Februa (February 13–15), Lupercalia (February 13–15), Parentalia (February 13–22), Quirinalia (February 17), Feralia (February 21), Caristia (February 22), Terminalia (February 23), Regifugium (February 24), and Agonium Martiale (February 27). These days do not correspond to the modern Gregorian calendar. \n\nUnder the reforms that instituted the Julian calendar, Intercalaris was abolished, leap years occurred regularly every fourth year, and in leap years February gained a 29th day. Thereafter, it remained the second month of the calendar year, meaning the order that months are displayed (January, February, March, ..., December) within a year-at-a-glance calendar. Even during the Middle Ages, when the numbered Anno Domini year began on March 25 or December 25, the second month was February whenever all twelve months were displayed in order. The Gregorian calendar reforms made slight changes to the system for determining which years were leap years and thus contained a 29-day February.\n\nHistorical names for February include the Old English terms Solmonath (mud month) and Kale-monath (named for cabbage) as well as Charlemagne's designation Hornung. In Finnish, the month is called \"helmikuu\", meaning \"month of the pearl\"; when snow melts on tree branches, it forms droplets, and as these freeze again, they are like pearls of ice. In Polish and Ukrainian, respectively, the month is called \"luty\" or \"лютий\", meaning the month of ice or hard frost. In Macedonian the month is \"sechko\" (сечко), meaning month of cutting [wood]. In Czech, it is called \"únor\", meaning month of submerging [of river ice]. \n\nIn Slovene, February is traditionally called \"svečan\", related to icicles or Candlemas. This name originates from \"sičan\", written as \"svičan\" in the \"New Carniolan Almanac\" from 1775 and changed to its final form by Franc Metelko in his \"New Almanac\" from 1824. The name was also spelled \"sečan\", meaning \"the month of cutting down of trees\". \n\nIn 1848, a proposal was put forward in \"Kmetijske in rokodelske novice\" by the Slovene Society of Ljubljana to call this month \"talnik\" (related to ice melting), but it did not stick. The idea was proposed by a priest, Blaž Potočnik. Another name of February in Slovene was \"vesnar\", after the mythological character Vesna.\n\nHaving only 28 days in common years, it is the only month of the year that can pass without a single full moon. This last happened in 1999 and will next happen in 2018.\n\nFebruary is also the only month of the calendar that, once every six years and twice every 11 years consecutively, either back into the past or forward into the future, will have four full 7-day weeks. In countries that start their week on a Monday, it occurs as part of a common year starting on Friday, in which February 1st is a Monday and the 28th is a Sunday, this was observed in 2010 and can be traced back 11 years to 1999, 6 years back to 1993, 11 years back to 1982, 11 years back to 1971 and 6 years back to 1965, and will be observed in 2021. In countries that start their week on a Sunday, it occurs in a common year starting on Thursday, with the next occurrence in 2026, and previous occurrences in 2015 (11 years earlier than 2026), 2009 (6 years earlier than 2015), 1998 (11 years earlier than 2009) and 1987 (11 years earlier than 1998). This works unless the pattern is broken by a skipped leap year, but no leap year has been skipped since 1900 and no others will be skipped until 2100.\n\nFebruary meteor showers include the Alpha Centaurids (appearing in early February), the Beta Leonids, also known as the March Virginids (lasting from February 14 to April 25, peaking around March 20), the Delta Cancrids (appearing December 14 to February 14, peaking on January 17, the Omicron Centaurids (late January through February, peaking in mid-February), Theta Centaurids (January 23-March 12, only visible in the southern hemisphere), Eta Virginids (February 24 and March 27, peaking around March 18), and Pi Virginids (February 13 and April 8, peaking between March 3 and March 9).\n\nThe western zodiac signs of February are Aquarius (until February 19) and Pisces (February 20 onwards).\n\n\n\"This list does not necessarily imply either official status nor general observance.\"\n\n\n\nFirst Friday - February 3\nFirst Saturday - February 4\nFirst Sunday - February 5\nFirst Week of February (first Monday, ending on Sunday) - February 6–12\nFirst Monday - February 6\nSecond Day of the second week - February 7\nSecond Saturday - February 11\nSecond Sunday - February 12\nSecond Monday - February 13\nSecond Tuesday - February 14\nThird Thursday - February 16\nThird Friday - February 17\nWeek of February 22 - February 19-25\nThird Monday - February 20\nLast Friday - February 24\nLast Saturday - February 25\nLast Tuesday - February 28\nLast day of February - February 28\n\n\n\n\n\n"}
{"id": "10846", "url": "https://en.wikipedia.org/wiki?curid=10846", "title": "February 1", "text": "February 1\n\n\n\n"}
{"id": "10847", "url": "https://en.wikipedia.org/wiki?curid=10847", "title": "First Lady of the United States", "text": "First Lady of the United States\n\nFirst Lady of the United States (FLOTUS) is the informal but accepted title held by the wife of the President of the United States, concurrent with the president's term of office. Although the first lady’s role has never been codified or officially defined, she figures prominently in the political and social life of the nation. Melania Trump is the current First Lady.\n\nWhile the title was not in general use until much later, Martha Washington, the wife of George Washington, the first U.S. President (1789–1797), is considered to be the inaugural First Lady of the United States. During her lifetime she was often referred to as \"Lady Washington\".\n\nSince the 1790s the role of first lady has changed considerably. It has come to include involvement in political campaigns, management of the White House, championship of social causes, and representation of the president at official and ceremonial occasions. Because first ladies now typically publish their memoirs, which are viewed as potential sources of additional information about their husbands’ administrations, and because the public is interested in these increasingly independent women in their own right, first ladies frequently remain a focus of attention long after their husbands’ terms of office have ended. Additionally, over the years individual first ladies have held influence in a range of sectors, from fashion to public opinion on policy. Historically, should a president be unmarried, or a widower, the president usually asks a relative or friend to act as White House hostess.\n\nThere are five living former first ladies: Rosalynn Carter, wife of Jimmy Carter; Barbara Bush, wife of George H. W. Bush; Hillary Clinton, wife of Bill Clinton; Laura Bush, wife of George W. Bush; and Michelle Obama, wife of Barack Obama.\n\nThe use of the title \"First Lady\" to describe the spouse or hostess of an executive began in the United States. In the early days of the republic, there was not a generally accepted title for the wife of the president. Many early first ladies expressed their own preference for how they were addressed, including the use of such titles as \"Lady\", \"Mrs. President\", and \"Mrs. Presidentress\"; Martha Washington was often referred to as \"Lady Washington.\" One of the earliest uses of the term \"First Lady\" was applied to her in an 1838 newspaper article that appeared in the \"St. Johnsbury Caledonian\", the author, \"Mrs. Sigourney\", discussing how Martha Washington had not changed, even after her husband George became president, wrote that \"The first lady of the nation still preserved the habits of early life. Indulging in no indolence, she left the pillow at dawn, and after breakfast, retired to her chamber for an hour for the study of the scriptures and devotion\".\n\nDolley Madison was reportedly referred to as \"First Lady\" in 1849 at her funeral in a eulogy delivered by President Zachary Taylor; however, no written record of this eulogy exists, nor did any of the newspapers of her day refer to her by that title. Sometime after 1849, the title began being used in Washington, D.C., social circles. One of the earliest known written examples comes from November 3, 1863, diary entry of William Howard Russell, in which he referred to gossip about \"the First Lady in the Land,\" referring to Mary Todd Lincoln. The title first gained nationwide recognition in 1877, when newspaper journalist Mary C. Ames referred to Lucy Webb Hayes as \"the First Lady of the Land\" while reporting on the inauguration of Rutherford B. Hayes. The frequent reporting on Lucy Hayes' activities helped spread use of the title outside Washington. A popular 1911 comedic play about Dolley Madison by playwright Charles Nirdlinger, titled \"The First Lady in the Land\", popularized the title further. By the 1930s, it was in wide use. Use of the title later spread from the United States to other nations.\n\nWhen Edith Wilson took control of her husband's schedule in 1919 after he had a debilitating stroke, one Republican senator labeled her \"the Presidentress who had fulfilled the dream of the suffragettes by changing her title from First Lady to Acting First Man.\"\n\nThe wife of the Vice President of the United States is sometimes referred to as the Second Lady of the United States, but this title is much less common.\n\nSeveral women (at least thirteen) who were not presidents' wives have served as First Lady, as when the president was a bachelor or widower, or when the wife of the president was unable to fulfill the duties of the First Lady herself. In these cases, the position has been filled by a female relative or friend of the president, such as Martha Jefferson Randolph during Jefferson's presidency, Emily Donelson and Sarah Yorke Jackson during Jackson's, Mary Elizabeth (Taylor) Bliss during Taylor's, Mary Harrison McKee during Benjamin Harrison's presidency, upon her mother's death, Harriet Lane during Buchanan's, and Rose Cleveland prior to Cleveland's marriage.\n\nTo date, no man has taken the role of White House host. At the state level, the husband of a Governor is known as a First Gentleman. Had Hillary Clinton won the 2016 election, her husband Bill Clinton would have been the first man to be the partner of a U.S. president, but as a former president himself, per the \"AP Stylebook\" his title of \"former President\" would take precedence over \"first gentleman\".\n\nThe position of the First Lady is not an elected one and carries only ceremonial duties. Nonetheless, first ladies have held a highly visible position in American society. The role of the First Lady has evolved over the centuries. She is, first and foremost, the hostess of the White House. She organizes and attends official ceremonies and functions of state either along with, or in place of, the president. Lisa Burns identifies four successive main themes of the first ladyship: as public woman (1900–1929); as political celebrity (1932–1961); as political activist (1964–1977); and as political interloper (1980–2001).\n\nMartha Washington created the role and hosted many affairs of state at the national capital (New York and Philadelphia). This socializing became known as \"the Republican Court\" and provided elite women with an opportunity to play backstage political role. Both Martha Washington and Abigail Adams were treated as if they were \"ladies\" of the British royal court.\n\nDolley Madison popularized the First Ladyship by engaging in efforts to assist orphans and women, by dressing in elegant fashions and attracting newspaper coverage, and by risking her life to save iconic treasures during the War of 1812. Madison set the standard for the ladyship and her actions were the model for nearly every First Lady until Eleanor Roosevelt in the 1930s. She traveled widely and spoke to many groups, often voicing personal opinions to the left of the president's. She authored a weekly newspaper column and hosted a radio show. Jacqueline Kennedy led an effort to redecorate and restore the White House.\n\nMany first ladies became significant fashion trendsetters. Some have exercised a degree of political influence by virtue of being an important adviser to the president.\n\nOver the course of the 20th century it became increasingly common for first ladies to select specific causes to promote, usually ones that are not politically divisive. It is common for the First Lady to hire a staff to support these activities. Lady Bird Johnson pioneered environmental protection and beautification. Pat Nixon encouraged volunteerism and traveled extensively abroad; Betty Ford supported women's rights; Rosalynn Carter aided those with mental disabilities; Nancy Reagan founded the Just Say No drug awareness campaign; Barbara Bush promoted literacy; Hillary Clinton sought to reform the healthcare system in the U.S.; Laura Bush supported women's rights groups, and encouraged childhood literacy. Michelle Obama has become identified with supporting military families and tackling childhood obesity.\n\nNear the end of her husband's presidency, Clinton became the first First Lady to run for political office. During the campaign, her daughter, Chelsea, took over much of the First Lady's role. Victorious, Clinton served as U.S. Senator from New York from 2001 to 2009, when she resigned in order to become President Obama's Secretary of State. Clinton was the Democratic Party nominee for President in the 2016 election.\n\nThe Office of the First Lady of the United States is accountable to the First Lady for her to carry out her duties as hostess of the White House, and is also in charge of all social and ceremonial events of the White House. The First Lady has her own staff that includes a chief of staff, press secretary, White House Social Secretary, Chief Floral Designer, etc. The Office of the First Lady is an entity of the White House Office, a branch of the Executive Office of the President. When First Lady Hillary Clinton decided to pursue a run for Senator of New York, she set aside her duties as first lady and moved to Chappaqua, New York to establish state residency. She resumed her duties as First Lady after winning her senatorial campaign, and retained her duties as both first lady and U.S. Senator for the seventeen-day overlap before Bill Clinton's term came to an end.\n\nDespite the significant responsibilities usually handled by the First Lady, the First Lady does not receive a salary. This has been criticized by both Ronald Reagan and Barack Obama.\n\nEstablished in 1912, the First Ladies Collection has been one of the most popular attractions at the Smithsonian Institution. The original exhibition opened in 1914 and was one of the first at the Smithsonian to prominently feature women. Originally focused largely on fashion, the exhibition now delves deeper into the contributions of first ladies to the presidency and American society. In 2008, \"First Ladies at the Smithsonian\" opened at the National Museum of American History as part of its reopening year celebration. That exhibition served as a bridge to the museum's expanded exhibition on first ladies' history that opened on November 19, 2011. \"The First Ladies\" explores the unofficial but important position of first lady and the ways that different women have shaped the role to make their own contributions to the presidential administrations and the nation. The exhibition features 26 dresses and more than 160 other objects, ranging from those of Martha Washington to Michelle Obama, and includes White House china, personal possessions and other objects from the Smithsonian's unique collection of first ladies' materials.\n\nSome first ladies have garnered attention for their dress and style. Jacqueline Kennedy, for instance, became a global fashion icon: her style was copied by commercial manufacturers and imitated by many young women, and she was named to the International Best Dressed List Hall of Fame in 1965. Michelle Obama has also received significant attention for her fashion choices: style writer Robin Givhan praised her in \"The Daily Beast\", arguing that the First Lady's style has helped to enhance the public image of the office.\n\nAs of January 20, 2017, there are five living former First Ladies, as seen below.\n\n\n"}
{"id": "10852", "url": "https://en.wikipedia.org/wiki?curid=10852", "title": "Frank Herbert", "text": "Frank Herbert\n\nFrank Patrick Herbert, Jr. (October 8, 1920 – February 11, 1986) was an American science fiction writer best known for the novel \"Dune\" and its five sequels. Though he became famous for science fiction, he was also a newspaper journalist, photographer, short story writer, book reviewer, ecological consultant and lecturer.\n\nThe \"Dune\" saga, set in the distant future and taking place over millennia, deals with complex themes such as human survival and evolution, ecology, and the intersection of religion, politics and power. \"Dune\" itself is the best-selling science fiction novel of all time and the series is widely considered to be among the classics of the genre.\n\nFrank Herbert was born on October 8, 1920, in Tacoma, Washington, to Frank Patrick Herbert, Sr. and Eileen (McCarthy) Herbert. Because of a poor home environment, he ran away from home in 1938 to live with an aunt and uncle in Salem, Oregon. He enrolled in high school at Salem High School (now North Salem High School), where he graduated the next year. In 1939 he lied about his age to get his first newspaper job at the \"Glendale Star\". Herbert then returned to Salem in 1940 where he worked for the \"Oregon Statesman\" newspaper (now \"Statesman Journal\") in a variety of positions, including photographer.\n\nHe served in the U.S. Navy's Seabees for six months as a photographer during World War II, then he was given a medical discharge. He married Flora Parkinson in San Pedro, California in 1940. They had a daughter, Penny (b. February 16, 1942), but divorced in 1945.\n\nAfter the war Herbert attended the University of Washington, where he met Beverly Ann Stuart at a creative writing class in 1946. They were the only students who had sold any work for publication; Herbert had sold two pulp adventure stories to magazines, the first to \"Esquire\" in 1945, and Stuart had sold a story to \"Modern Romance\" magazine. They married in Seattle, Washington on June 20, 1946 and had two sons, Brian Patrick Herbert (b. June 29, 1947, Seattle, Washington) and Bruce Calvin Herbert (b. June 26, 1951, Santa Rosa, California d. June 15, 1993, San Rafael, California, a professional photographer and gay rights activist).\n\nIn 1949 Herbert and his wife moved to California to work on the \"Santa Rosa Press-Democrat\". Here they befriended the psychologists Ralph and Irene Slattery. The Slatterys introduced Herbert to the work of several thinkers who would influence his writing, including Freud, Jung, Jaspers and Heidegger; they also familiarized Herbert with Zen Buddhism.\n\nHerbert did not graduate from the university; according to his son Brian, he wanted to study only what interested him and so did not complete the required curriculum. He returned to journalism and worked at the \"Seattle Star\" and the \"Oregon Statesman\". He was a writer and editor for the \"San Francisco Examiner's\" \"California Living\" magazine for a decade.\n\nIn a 1973 interview, Herbert stated that he had been reading science fiction \"about ten years\" before he\nbegan writing in the genre, and he listed his favorite authors as H. G. Wells, Robert A. Heinlein, Poul Anderson and Jack Vance.\n\nHerbert's first science fiction story, \"Looking for Something\", was published in the April 1952 issue of \"Startling Stories\", then a monthly edited by Samuel Mines. Three more of his stories appeared in 1954 issues of \"Astounding Science Fiction\" and \"Amazing Stories\". His career as a novelist began in 1955 with the serial publication of \"Under Pressure\" in \"Astounding\" from November 1955; afterward it was issued as a book by Doubleday, \"The Dragon in the Sea\". The story explored sanity and madness in the environment of a 21st-century submarine and predicted worldwide conflicts over oil consumption and production. It was a critical success but not a major commercial one. During this time Herbert also worked as a speechwriter for Republican senator Guy Cordon.\n\nHerbert began researching \"Dune\" in 1959. He was able to devote himself wholeheartedly to his writing career because his wife returned to work full-time as an advertising writer for department stores, becoming the breadwinner during the 1960s. He later told Willis E. McNelly that the novel originated when he was supposed to do a magazine article on sand dunes in the Oregon Dunes near Florence, Oregon. He became too involved and ended up with far more raw material than needed for an article. The article was never written, but instead planted the seed that led to \"Dune\".\n\n\"Dune\" took six years of research and writing to complete and it was much longer than commercial science fiction of the time was supposed to run. \"Analog\" (the renamed \"Astounding\", still edited by John W. Campbell) published it in two parts comprising eight installments, \"Dune World\" from December 1963 and \"Prophet of Dune\" in 1965. It was then rejected by nearly twenty book publishers. One editor prophetically wrote, \"I might be making the mistake of the decade, but ...\".\n\nSterling E. Lanier, an editor of Chilton Book Company (known mainly for its auto-repair manuals) had read the Dune serials and offered a $7,500 advance plus future royalties for the rights to publish them as a hardcover book. Herbert rewrote much of his text. \"Dune\" was soon a critical success. It won the Nebula Award for Best Novel in 1965 and shared the Hugo Award in 1966 with \"...And Call Me Conrad\" by Roger Zelazny. \"Dune\" was the first major ecological science fiction novel, embracing a multitude of sweeping, interrelated themes and multiple character viewpoints, a method that ran through all Herbert's mature work.\n\n\"Dune\" was not immediately a bestseller. By 1968 Herbert had made $20,000 from it, far more than most science fiction novels of the time were generating, but not enough to let him take up full-time writing. However, the publication of \"Dune\" did open doors for him. He was the \"Seattle Post-Intelligencer's\" education writer from 1969 to 1972 and lecturer in general studies and interdisciplinary studies at the University of Washington (1970–1972). He worked in Vietnam and Pakistan as social and ecological consultant in 1972. In 1973 he was director-photographer of the television show \"The Tillers\".\n\nBy 1972, Herbert retired from newspaper writing and became a full-time fiction writer. During the 1970s and 1980s, Herbert enjoyed considerable commercial success as an author. He divided his time between homes in Hawaii and Washington's Olympic Peninsula; his home in Port Townsend on the peninsula was intended to be an \"ecological demonstration project\". During this time he wrote numerous books and pushed ecological and philosophical ideas. He continued his \"Dune\" saga, following it with \"Dune Messiah\", \"Children of Dune\", and \"God Emperor of Dune\". Other highlights were \"The Dosadi Experiment\", \"The Godmakers\", \"The White Plague\" and the books he wrote in partnership with Bill Ransom: \"The Jesus Incident\", \"The Lazarus Effect\", and \"The Ascension Factor\" which were sequels to \"\". He also helped launch the career of Terry Brooks with a very positive review of Brooks' first novel, \"The Sword of Shannara\", in 1977.\n\nHerbert's change in fortune was shadowed by tragedy. In 1974, Beverly underwent an operation for cancer. She lived ten more years, but her health was adversely affected by the surgery. During this period, Herbert was the featured speaker at the Octocon II science fiction convention at the El Rancho Tropicana in Santa Rosa, California in October 1978; in 1979, he met anthropologist James Funaro with whom he conceived the Contact Conference. Beverly Herbert died on February 7, 1984, the same year that \"Heretics of Dune\" was published; in his afterword to 1985's \"\", Frank Herbert wrote a eulogy for her.\n\nIn 1983, British heavy metal band Iron Maiden requested permission from Herbert's publisher to name a song on their album \"Piece of Mind\" after \"Dune\", but were told that the author had a strong distaste for their style of music. They instead titled the song \"To Tame a Land\".\n\n1984 was a tumultuous year in Herbert's life. During this same year of his wife's death, his career took off with the release of David Lynch's film version of \"Dune\". Despite high expectations, a big-budget production design and an A-list cast, the movie drew mostly poor reviews in the United States. However, despite a disappointing response in the USA, the film was a critical and commercial success in Europe and Japan.\n\nAfter Beverly's death, Herbert married Theresa Shackleford in 1985, the year he published \"Chapterhouse: Dune\", which tied up many of the saga's story threads. This would be Herbert's final single work (the anthology \"Eye\" was published that year, and \"Man of Two Worlds\" was published in 1986). He died of a massive pulmonary embolism while recovering from surgery for pancreatic cancer on February 11, 1986 in Madison, Wisconsin age 65. He was raised a Catholic but adopted Zen Buddhism as an adult.\n\nHerbert was a strong critic of the Soviet Union. He was a distant relative of the controversial Republican senator, Joseph McCarthy, whom he referred to as \"Cousin Joe.\" Herbert was appalled to learn of McCarthy's blacklisting of suspected Communists from working in certain careers and believed that he was endangering essential freedoms of citizens of the United States. Herbert believed that governments lie to protect themselves and that, following the infamous Watergate scandal, President Richard Nixon had unwittingly taught an important lesson in not trusting government.\n\nIn \"Chapterhouse: Dune\", he wrote:\n\nFrank Herbert used his science fiction novels to explore complex ideas involving philosophy, religion, psychology, politics and ecology. The underlying thrust of his work was a fascination with the question of human survival and evolution. Herbert has attracted a sometimes fanatical fan base, many of whom have tried to read everything he wrote, fiction or non-fiction, and see Herbert as something of an authority on the subject matters of his books. Indeed, such was the devotion of some of his readers that Herbert was at times asked if he was founding a cult, something he was very much against.\n\nThere are a number of key themes in Herbert's work:\n\nFrank Herbert refrained from offering his readers formulaic answers to many of the questions he explored.\n\n\"Dune\" and the \"Dune\" saga constitute one of the world's best-selling science fiction series and novels; \"Dune\" in particular has received widespread critical acclaim, winning the Nebula Award in 1965 and sharing the Hugo Award in 1966, and is frequently considered one of the best science fiction novels ever, if not the best. \"Locus\" subscribers voted it the all-time best SF novel in 1975, again in 1987, and the best \"before 1990\" in 1998.\n\n\"Dune\" is considered a landmark novel for a number of reasons:\n\n\nHerbert never again equalled the critical acclaim he received for \"Dune\". Neither his sequels to \"Dune\" nor any of his other books won a Hugo or Nebula Award, although almost all of them were \"New York Times\" Best Sellers. Some felt that \"Children of Dune\" was almost too literary and too dark to get the recognition it may have deserved; others felt that \"The Dosadi Experiment\" lacked an epic quality that fans had come to expect.\n\nMalcolm Edwards in the \"Encyclopedia of Science Fiction\" wrote:\n\nMuch of Herbert's work makes difficult reading. His ideas were genuinely developed concepts, not merely decorative notions, but they were sometimes embodied in excessively complicated plots and articulated in prose which did not always match the level of thinking [...] His best novels, however, were the work of a speculative intellect with few rivals in modern science fiction. \n\nThe Science Fiction Hall of Fame inducted Herbert in 2006.\n\nCalifornia State University, Fullerton's Pollack Library has several of Herbert's draft manuscripts of \"Dune\" and other works, with the author's notes, in their Frank Herbert Archives.\n\nBeginning in 2012, Herbert's estate and WordFire Press have released four previously unpublished novels in e-book and paperback formats: \"High-Opp\" (2012), \"Angels' Fall\" (2013), \"A Game of Authors\" (2013), and \"A Thorn in the Bush\" (2014).\n\nIn recent years, Frank Herbert's son Brian Herbert and author Kevin J. Anderson have added to the \"Dune\" franchise, using notes left behind by Frank Herbert and discovered over a decade after his death. Brian Herbert and Anderson have written two prequel trilogies (\"Prelude to Dune\" and \"Legends of Dune\") exploring the history of the \"Dune\" universe before the events within \"Dune\", as well as two post-\"Chapterhouse Dune\" novels that complete the original series (\"Hunters of Dune\" and \"Sandworms of Dune\") based on Frank Herbert's own \"Dune 7\" outline.\n\n\n\n\n"}
{"id": "10853", "url": "https://en.wikipedia.org/wiki?curid=10853", "title": "Fictional language", "text": "Fictional language\n\nFictional languages are constructed languages created as part of a fictional setting, for example in books, movies and video games. Fictional languages are intended to be the languages of a fictional world and are often designed with the intent of giving more depth and an appearance of plausibility to the fictional worlds with which they are associated, and to have their characters communicate in a fashion which is both alien and dislocated.\n\nSome of these languages, e.g., in worlds of fantasy fiction, alternate universes, Earth's future, or alternate history, are presented as distorted versions or dialects of modern English or other natural language, while others are independently designed conlangs.\n\nWhile most fictional languages are just gibberish, a few are fully functioning and comprehensive languages that can be learned.\n\nFictional languages are separated from artistic languages by both purpose and relative completion: a fictional language often has the least amount of grammar and vocabulary possible, and rarely extends beyond the absolutely necessary. At the same time, some others have developed languages in detail for their own sake, such as J. R. R. Tolkien's Quenya and Sindarin (two Elvish languages), Star Trek's Klingon language and Avatar's Na'vi language which exist as functioning, usable languages. Here \"fictional\" can be a misnomer.\n\nBy analogy with the word \"conlang,\" the term \"conworld\" is used to describe these fictional worlds, inhabited by fictional constructed cultures. The conworld influences vocabulary (what words the language will have for flora and fauna, articles of clothing, objects of technology, religious concepts, names of places and tribes, etc.), as well as influencing other factors such as pronouns, or how their cultures view the break-off points between colors or the gender and age of family members.\n\nProfessional fictional languages are those languages created for use in books, movies, television shows, video games, comics, toys, and musical albums (prominent examples of works featuring fictional languages include the Middle-Earth and Star Trek universes and the game \"Myst\").\n\nA notable subgenre of fictional languages are alien languages, the ones that are used or might be used by putative extraterrestrial life forms. Alien languages are subject of both science fiction and scientific research.\n\nPerhaps the most fully developed fictional alien language is the Klingon language of the Star Trek universe - a fully developed constructed language.\n\nThe problem of alien language has confronted generations of science fiction writers; some have created fictional languages for their characters to use, while others have circumvented the problem through translation devices or other fantastic technology.\n\nAlthough this field remains largely confined to science fiction, the possibility of intelligent extraterrestrial life makes the question of alien language a credible topic for scientific and philosophical speculation.\n\nWhile many cases an alien language is but an element of fictional reality, in a number of science fiction works the core of the plot are linguistic and psychological problems of communication between various alien races.\n\nInternet-based fictional languages are hosted along with their \"conworlds\" on the Internet, and based at these sites, becoming known to the world through the visitors to these sites; Verdurian, the language of Mark Rosenfelder's Verduria on the planet of Almea, is a flagship Internet-based fictional language. Many other fictional languages and their associated conworlds are created privately by their inventor, known only to the inventor and perhaps a few friends. In this context the term \"professional\" (used for the first category) as opposed to \"amateur\" (used for the second and third) refers only to the professionalism of the used medium, and not to the professionalism of the language itself or its creator. In fact, most professional languages are the work of non-linguists, while many amateur languages were in fact created by linguists, and in general the latter are better developed. \n\n\n\n"}
{"id": "10854", "url": "https://en.wikipedia.org/wiki?curid=10854", "title": "Formula One", "text": "Formula One\n\nFormula One (also Formula 1 or F1 and officially the FIA Formula One World Championship) is the highest class of single-seat auto racing that is sanctioned by the Fédération Internationale de l'Automobile (FIA). The FIA Formula One World Championship has been the premier form of racing since the inaugural season in 1950, although other Formula One races were regularly held until 1983. The \"formula\", designated in the name, refers to a set of rules, to which all participants' cars must conform. The F1 season consists of a series of races, known as \"Grands Prix\" (from French, meaning grand prizes), held worldwide on purpose-built F1 circuits and public roads.\n\nThe results of each race are evaluated using a points system to determine two annual World Championships, one for drivers, one for constructors. The racing drivers are required to be holders of valid Super Licences, the highest class of racing licence issued by the FIA. The races are required to be held on tracks graded 1 (formerly A), the highest grade a track can receive by the FIA. Most events are held in rural locations on purpose-built tracks, but there are several events in city centres throughout the world, with the Monaco Grand Prix being the most famous example.\n\nFormula One cars are the fastest road course racing cars in the world, owing to very high cornering speeds achieved through the generation of large amounts of aerodynamic downforce. Formula One cars race at speeds of up to approximately with engines currently limited in performance to a maximum of 15,000 rpm. The cars are capable of lateral acceleration in excess of six g in corners. The performance of the cars is very dependent on electronicsalthough traction control and other driving aids have been banned since 2008and on aerodynamics, suspension, and tyres. The formula has radically evolved and changed through the history of the sport.\n\nWhile Europe is the sport's traditional base, and hosts about half of each year's races, the sport's scope has expanded significantly and an increasing number of Grands Prix are held on other continents. F1 had a total global television audience of 425 million people during the course of the 2014 season. Grand Prix racing began in 1906 and became the most popular type internationally in the second half of the twentieth century. The Formula One Group is the legal holder of the commercial rights.\n\nWith the cost of designing and building mid-tier cars being of the order of $120 million, Formula One's economic effect and creation of jobs are significant, and its financial and political battles are widely reported. Its high profile and popularity have created a major merchandising environment, which has resulted in great investments from sponsors and budgets in the hundreds of millions for the constructors. Since 2000 the sport's spiralling expenditures and the distribution of prize money favouring established top teams have forced complaints from smaller teams and led several teams to bankruptcy.\n\nOn 8 September 2016 it was announced that Liberty Media Corp. had agreed to buy Delta Topco, the company that controls Formula One, from private equity firm CVC Capital Partners for $4.4 billion in cash, stock and convertible debt. On 23 January 2017 it was confirmed that Liberty Media had completed its $8 billion acquisition of Delta Topco.\n\nThe Formula One series originated with the European Grand Prix Motor Racing (\"q.v.\" for pre-1947 history) of the 1920s and 1930s. The formula is a set of rules that all participants' cars must meet. Formula One was a new formula agreed upon after World War II during 1946, with the first non-championship races being held that year. A number of Grand Prix racing organisations had laid out rules for a world championship before the war, but due to the suspension of racing during the conflict, the World Drivers' Championship was not formalised until 1947. The first world championship race was held at Silverstone, United Kingdom in 1950. A championship for constructors followed in 1958. National championships existed in South Africa and the UK in the 1960s and 1970s. Non-championship Formula One events were held for many years, but due to the increasing cost of competition, the last of these occurred in 1983.\n\nThe first World Championship for Drivers was won by Italian Giuseppe Farina in his Alfa Romeo in 1950, narrowly defeating his Argentine teammate Juan Manuel Fangio. However, Fangio won the title in 1951, 1954, 1955, 1956, and 1957 (His record of five World Championship titles stood for 45 years until German driver Michael Schumacher took his sixth title in 2003), his streak interrupted (after an injury) by two-time champion Alberto Ascari of Ferrari. Although the UK's Stirling Moss was able to compete regularly, he was never able to win the world championship, and is now widely considered to be the greatest driver never to have won the title. Fangio, however, is remembered for dominating Formula One's first decade and has long been considered the \"Grand Master\" of Formula One.\n\nThis period featured teams managed by road car manufacturers Alfa Romeo, Ferrari, Mercedes-Benz, and Maserati; all of whom had competed before the war. The first seasons were run using pre-war cars like Alfa's 158. They were front-engined, with narrow tyres and 1.5-litre supercharged or 4.5-litre normally aspirated engines. The 1952 and 1953 world championships were run to Formula Two regulations, for smaller, less powerful cars, due to concerns over the paucity of Formula One cars available. When a new Formula One, for engines limited to 2.5 litres, was reinstated to the world championship for 1954, Mercedes-Benz introduced the advanced W196, which featured innovations such as desmodromic valves and fuel injection as well as enclosed streamlined bodywork. Mercedes drivers won the championship for two years, before the team withdrew from all motorsport in the wake of the 1955 Le Mans disaster.\n\nThe first major technological development, Bugatti's re-introduction of mid-engined cars (following Ferdinand Porsche's pioneering Auto Unions of the 1930s), occurred with the Type 251, which was unsuccessful. Australian Jack Brabham, world champion during 1959, 1960, and 1966, soon proved the mid-engined design's superiority. By 1961, all regular competitors had switched to mid-engined cars. The Ferguson P99, a four-wheel drive design, was the last front-engined F1 car to enter a world championship race. It was entered in the 1961 British Grand Prix, the only front-engined car to compete that year.\n\nThe first British World Champion was Mike Hawthorn, who drove a Ferrari to the title during the 1958 season. However, when Colin Chapman entered F1 as a chassis designer and later founder of Team Lotus, British racing green came to dominate the field for the next decade. Including Brabham, Jim Clark, Jackie Stewart, John Surtees, Graham Hill, and Denny Hulme, British teams and Commonwealth drivers won twelve world championships between 1962 and 1973.\n\nDuring 1962, Lotus introduced a car with an aluminium-sheet monocoque chassis instead of the traditional space-frame design. This proved to be the greatest technological breakthrough since the introduction of mid-engined cars. During 1968, Lotus painted Imperial Tobacco livery on their cars, thus introducing sponsorship to the sport.\n\nAerodynamic downforce slowly gained importance in car design from the appearance of aerofoils during the late 1960s. During the late 1970s, Lotus introduced ground-effect aerodynamics (previously used on Jim Hall's Chaparral 2J during 1970) that provided enormous downforce and greatly increased cornering speeds. So great were the aerodynamic forces pressing the cars to the track (up to five times the car's weight), extremely stiff springs were needed to maintain a constant ride height, leaving the suspension virtually solid, depending entirely on the tyres for any small amount of cushioning of the car and driver from irregularities of the road surface.\n\nBeginning in the 1970s, Bernie Ecclestone rearranged the management of Formula One's commercial rights; he is widely credited with transforming the sport into the multibillion-dollar business it now is. When Ecclestone bought the Brabham team during 1971 he gained a seat on the Formula One Constructors' Association and during 1978 he became its president. Previously, the circuit owners controlled the income of the teams and negotiated with each individually, however Ecclestone persuaded the teams to \"hunt as a pack\" through FOCA. He offered Formula One to circuit owners as a package, which they could take or leave. In return for the package almost all that was required was to surrender trackside advertising.\n\nThe formation of the Fédération Internationale du Sport Automobile (FISA) during 1979 set off the FISA–FOCA controversy, during which FISA and its president Jean-Marie Balestre disputed repeatedly with FOCA over television revenues and technical regulations. \"The Guardian\" said of FOCA that Ecclestone and Max Mosley \"used it to wage a guerrilla war with a very long-term aim in view\". FOCA threatened to establish a rival series, boycotted a Grand Prix and FISA withdrew its sanction from races. The result was the 1981 Concorde Agreement, which guaranteed technical stability, as teams were to be given reasonable notice of new regulations. Although FISA asserted its right to the TV revenues, it handed the administration of those rights to FOCA.\nFISA imposed a ban on ground-effect aerodynamics during 1983. By then, however, turbocharged engines, which Renault had pioneered in 1977, were producing over and were essential to be competitive. By 1986, a BMW turbocharged engine achieved a flash reading of 5.5 bar pressure, estimated to be over in qualifying for the Italian Grand Prix. The next year power in race trim reached around , with boost pressure limited to only 4.0 bar. These cars were the most powerful open-wheel circuit racing cars ever. To reduce engine power output and thus speeds, the FIA limited fuel tank capacity in 1984 and boost pressures in 1988 before banning turbocharged engines completely in 1989.\n\nThe development of electronic driver aids began during the 1980s. Lotus began to develop a system of active suspension, which first appeared during 1982 on the 91. By 1987, this system had been perfected and was driven to victory by Ayrton Senna in the Monaco Grand Prix that year. In the early 1990s other teams followed suit and semi-automatic gearboxes and traction control were a natural progression. The FIA, due to complaints that technology was determining the outcome of races more than driver skill, banned many such aids for 1994. This resulted in cars that were previously dependent on electronic aids becoming very \"twitchy\" and difficult to drive (particularly the Williams FW16). Many observers felt the ban on driver aids was in name only as they \"proved difficult to police effectively\".\n\nThe teams signed a second Concorde Agreement during 1992 and a third in 1997, which expired on the last day of 2007.\n\nOn the track, the McLaren and Williams teams dominated the 1980s and 1990s, with Brabham also being competitive during the early part of the 1980s, winning two Drivers' Championships with Nelson Piquet. Powered by Porsche, Honda, and Mercedes-Benz, McLaren won sixteen championships (seven constructors' and nine drivers') in that period, while Williams used engines from Ford, Honda, and Renault to also win sixteen titles (nine constructors' and seven drivers'). The rivalry between racers Ayrton Senna and Alain Prost became F1's central focus during 1988, and continued until Prost retired at the end of 1993. Senna died at the 1994 San Marino Grand Prix after crashing into a wall on the exit of the notorious curve Tamburello, having taken over Prost's lead drive at Williams that year. The FIA worked to improve the sport's safety standards since that weekend, during which Roland Ratzenberger also lost his life in an accident during Saturday qualifying. No driver had died of injuries sustained on the track at the wheel of a Formula One car for 20 years, until the 2014 Japanese Grand Prix where Jules Bianchi collided with a recovery vehicle after aquaplaning off the circuit. Since 1994, three track marshals have lost their lives, one at the 2000 Italian Grand Prix, the second at the 2001 Australian Grand Prix and the third at the 2013 Canadian Grand Prix.\n\nSince the deaths of Senna and Ratzenberger, the FIA has used safety as a reason to impose rule changes that otherwise, under the Concorde Agreement, would have had to be agreed upon by all the teams — most notably the changes introduced for 1998. This so-called 'narrow track' era resulted in cars with smaller rear tyres, a narrower track overall, and the introduction of grooved tyres to reduce mechanical grip. There were to be four grooves on the front (three in the first year) and rear that ran through the entire circumference of the tyre. The objective was to reduce cornering speeds and to produce racing similar to rainy conditions by enforcing a smaller contact patch between tyre and track. This, according to the FIA, was to promote driver skill and provide a better spectacle.\n\nResults have been mixed as the lack of mechanical grip has resulted in the more ingenious designers clawing back the deficit with aerodynamic grip — pushing more force onto the tyres through wings and aerodynamic devices, which in turn has resulted in less overtaking as these devices tend to make the wake behind the car 'dirty' (turbulent), preventing other cars from following closely due to their dependence on 'clean' air to make the car stick to the track. The grooved tyres also had the unfortunate side effect of initially being of a harder compound to be able to hold the grooved tread blocks, which resulted in spectacular accidents in times of aerodynamic grip failure as the harder compound could not grip the track as well.\n\nDrivers from McLaren, Williams, Renault (formerly Benetton), and Ferrari, dubbed the \"Big Four\", won every World Championship from 1984 to 2008. The teams won every Constructors' Championship from 1979 to 2008 as well as placing themselves as the top four teams in the Constructors' Championship in every season between 1989 and 1997, and winning every race but one (the 1996 Monaco Grand Prix) between 1988 and 1997. Due to the technological advances of the 1990s, the cost of competing in Formula One increased dramatically. This increased financial burdens, combined with the dominance of four teams (largely funded by big car manufacturers such as Mercedes-Benz), caused the poorer independent teams to struggle not only to remain competitive, but to stay in business, and forced several teams to withdraw. Since 1990, twenty-eight teams have withdrawn from Formula One. This has prompted former Jordan owner Eddie Jordan to say that the days of competitive privateers are over.\n\nMichael Schumacher and Ferrari won five consecutive Drivers' Championships (2000–2004) and six consecutive Constructors' Championships (1999–2004). Schumacher set many new records, including those for Grand Prix wins (91), wins in a season (thirteen of eighteen), and most Drivers' Championships (seven). Schumacher's championship streak ended on 25 September 2005 when Renault driver Fernando Alonso became Formula One's youngest champion at that time, until Lewis Hamilton in 2008. During 2006, Renault and Alonso won both titles again. Schumacher retired at the end of 2006 after sixteen years in Formula One, but came out of retirement for the 2010 season, racing for the newly formed Mercedes works team, following the rebrand of Brawn GP.\n\nDuring this period, the championship rules were changed frequently by the FIA with the intention of improving the on-track action and cutting costs. Team orders, legal since the championship started during 1950, were banned during 2002 after several incidents in which teams openly manipulated race results, generating negative publicity, most famously by Ferrari at the 2002 Austrian Grand Prix. Other changes included the qualifying format, the points scoring system, the technical regulations, and rules specifying how long engines and tyres must last. A \"tyre war\" between suppliers Michelin and Bridgestone saw lap times fall, although at the 2005 United States Grand Prix at Indianapolis, seven out of ten teams did not race when their Michelin tyres were deemed unsafe for use, leading to Bridgestone becoming the sole tyre supplier to Formula One for the 2007 season. During 2006, Max Mosley outlined a \"green\" future for Formula One, in which efficient use of energy would become an important factor.\n\nSince 1983, Formula One had been dominated by specialist race teams like Williams, McLaren, and Benetton, using engines supplied by large car manufacturers like Mercedes-Benz, Honda, Renault, and Ford. Starting in 2000, with Ford's creation of the largely unsuccessful Jaguar team, new manufacturer-owned teams entered Formula One for the first time since the departure of Alfa Romeo and Renault at the end of 1985. By 2006, the manufacturer teams–Renault, BMW, Toyota, Honda, and Ferrari–dominated the championship, taking five of the first six places in the Constructors' Championship. The sole exception was McLaren, which at the time was part-owned by Mercedes Benz. Through the Grand Prix Manufacturers Association (GPMA), they negotiated a larger share of Formula One's commercial profit and a greater say in the running of the sport.\n\nIn 2008 and 2009, Honda, BMW, and Toyota all withdrew from Formula One racing within the space of a year, blaming the economic recession. This resulted in the end of manufacturer dominance within the sport. The Honda F1 team went through a management buyout to become Brawn GP with the notable F1 designer Ross Brawn and Nick Fry running and owning the majority of the organisation. Brawn GP went through a painful size reduction, laying off hundreds of employees, but eventually won the year's world championships with Jenson Button and Rubens Barrichello. BMW F1 was bought out by the original founder of the team, Peter Sauber. The Lotus F1 Team are another, formerly manufacturer-owned team that has reverted to \"privateer\" ownership, together with the buy-out of the Renault F1 Team by Genii Capital investors in recent years. A link with their previous owners still survived however, with their car continuing to be powered by a Renault Power Unit until 2014.\n\nMcLaren also announced that it was to reacquire the shares in its team from Mercedes Benz (McLaren's partnership with Mercedes was reported to have started to sour with the McLaren Mercedes SLR road car project and tough F1 championships which included McLaren being found guilty of spying on Ferrari). Hence, during the 2010 season, Mercedes Benz re-entered the sport as a manufacturer after its purchase of Brawn GP, and split with McLaren after 15 seasons with the team. This left Mercedes, McLaren, and Ferrari as the only car manufacturers in the sport, although both McLaren and Ferrari began as racing teams rather than manufacturers.\n\nTo compensate for the loss of manufacturer teams, four new teams were accepted entry into the 2010 season ahead of a much anticipated 'cost-cap' (see below). Entrants included a reborn Team Lotus – which was led by a Malaysian consortium including Tony Fernandes, the boss of Air Asia; Hispania Racing – the first Spanish Formula One team; as well as Virgin Racing – Richard Branson's entry into the series following a successful partnership with Brawn the year before. They were also joined by the US F1 Team, which planned to run out of the United States as the only non-European based team in the sport. Financial issues befell the squad before they even made the grid. Despite the entry of these new teams, the proposed cost-cap was repealed and these teams – who did not have the budgets of the midfield and top-order teams – ran around at the back of the field until they inevitably collapsed; HRT in 2012, Caterham (formerly Lotus) in 2014 and Manor (formerly Virgin then Marussia), having survived falling into administration in 2014, went under at the end of 2016.\n\nA rule shake-up in 2014 meant Mercedes emerged as the dominant force, with Lewis Hamilton winning the championship closely followed by his main rival and teammate, Nico Rosberg, with the team winning 16 out of the 19 races that season (all other victories coming from Daniel Ricciardo of Red Bull). 2014 also saw a financial crisis which resulted in the backmarker Marussia and Caterham teams being put into administration, alongside the uncertain futures of Force India and Sauber. Marussia returned under the Manor name in 2015, a season in which Ferrari were the only challengers to Mercedes, with Vettel taking victory in the three Grands Prix Mercedes did not win.\n\nThe 2016 season began in dominant fashion for Nico Rosberg, winning the first 4 Grands Prix. His charge was halted by Max Verstappen, who took his maiden win in Spain in his debut race for Red Bull. After that, the reigning champion Lewis Hamilton decreased the point gap between him and Rosberg to only one point, before taking the championship lead heading into the summer break. Following the break, the 1–2 positioning remained constant until an engine failure for Hamilton in Malaysia left Rosberg in a commanding lead that he would not relinquish in the 5 remaining races. Having won the title by a mere 5 points, Rosberg retired from Formula One at season's end. The final team remaining from the 2010 new entries process, Manor Racing, withdrew from the sport following the 2016 season, having lost 10th in the Constructors' Championship to Sauber with one race remaining, leaving the grid at 20 cars as Liberty Media took control of the series in the off-season.\n\nThe battle for control of Formula One was contested between the Fédération Internationale du Sport Automobile (FISA), at the time an autonomous subcommittee of the FIA, and FOCA (the Formula One Constructors' Association).\n\nThe beginnings of the dispute are numerous, and many of the underlying reasons may be lost in history. The teams (excepting Ferrari and the other major manufacturers – Renault and Alfa Romeo in particular) were of the opinion that their rights and ability to compete against the larger and better funded teams were being negatively affected by a perceived bias on the part of the controlling organisation (FISA) toward the major manufacturers.\n\nIn addition, the battle revolved around the commercial aspects of the sport (the FOCA teams were unhappy with the disbursement of proceeds from the races) and the technical regulations which, in FOCA's opinion, tended to be malleable according to the nature of the transgressor more than the nature of the transgression.\n\nThe war culminated in a FOCA boycott of the 1982 San Marino Grand Prix months later. In theory, all FOCA teams were supposed to boycott the Grand Prix as a sign of solidarity and complaint at the handling of the regulations and financial compensation (and extreme opposition to the accession of Balestre to the position of FISA president: both Colin Chapman of Lotus and Frank Williams of Williams stated clearly that they would not continue in Formula One with Balestre as its governor). In practice, several of the FOCA teams backed out of the boycott, citing \"sponsor obligations\". Notable among these were the Tyrrell and Toleman teams.\n\nDuring the 2009 season of Formula One, the sport was gripped in a governance crisis. The FIA President Max Mosley proposed numerous cost cutting measures for the following season, including an optional budget cap for the teams; teams electing to take the budget cap would be granted greater technical freedom, adjustable front and rear wings and an engine not subject to a rev limiter. The Formula One Teams Association (FOTA) believed that allowing some teams to have such technical freedom would have created a 'two-tier' championship, and thus requested urgent talks with the FIA. However, talks broke down and FOTA teams announced, with the exception of Williams and Force India, that 'they had no choice' but to form a breakaway championship series.\nOn 24 June, an agreement was reached between Formula One's governing body and the teams to prevent a breakaway series. It was agreed teams must cut spending to the level of the early 1990s within two years; exact figures were not specified, and Max Mosley agreed he would not stand for re-election to the FIA presidency in October. Following further disagreements after Max Mosley suggested he would stand for re-election, FOTA made it clear that breakaway plans were still being pursued. On 8 July, FOTA issued a press release stating they had been informed they were not entered for the 2010 season, and an FIA press release said the FOTA representatives had walked out of the meeting. On 1 August, it was announced FIA and FOTA had signed a new Concorde Agreement, bringing an end to the crisis and securing the sport's future until 2012.\n\nThe terms \"Formula One race\" and \"World Championship race\" are effectively synonymous; since 1984, every Formula One race has counted towards an official FIA World Championship, and every World Championship race has been held to Formula One regulations. In the earlier history of Formula One, many races took place outside the world championship, and local championships run to Formula One regulations also occurred. These events often took place on circuits that were not suitable for the World Championship, and featured local cars and drivers as well as those competing in the Championship.\n\nIn the early years of Formula One, before the world championship was established, there were around twenty races held from late Spring to early Autumn in Europe, although not all of these were considered significant. Most competitive cars came from Italy, particularly Alfa Romeo. After the start of the world championship, these non-championship races continued. In the 1950s and 1960s, there were many Formula One races which did not count for the World Championship; in a total of twenty-two Formula One races were held, of which only six counted towards the World Championship. In 1952 and 1953, when the world championship was run for Formula Two cars, non-championship events were the only Formula One races that took place.\n\nSome races, particularly in the UK, including the Race of Champions, Oulton Park International Gold Cup and the International Trophy, were attended by the majority of the world championship contenders. Other smaller events were regularly held in locations not part of the championship, such as the Syracuse and Danish Grands Prix, although these only attracted a small amount of the championship teams and relied on private entries and lower Formula cars to make up the grid. These became less common through the 1970s and 1983 saw the last non-championship Formula One race; the 1983 Race of Champions at Brands Hatch, won by reigning World Champion Keke Rosberg in a Williams-Cosworth in a close fight with American Danny Sullivan.\n\nSouth Africa's flourishing domestic Formula One championship ran from 1960 through to 1975. The frontrunning cars in the series were recently retired from the world championship although there was also a healthy selection of locally built or modified machines. Frontrunning drivers from the series usually contested their local World Championship Grand Prix, as well as occasional European events, although they had little success at that level.\n\nThe DFV helped make the UK domestic Formula One series possible between 1978 and 1980. As in South Africa a decade before, second hand cars from manufacturers like Lotus and Fittipaldi Automotive were the order of the day, although some, such as the March 781, were built specifically for the series. In 1980, the series saw South African Desiré Wilson become the only woman to win a Formula One race when she triumphed at Brands Hatch in a Wolf WR3.\n\nA Formula One Grand Prix event spans a weekend. It begins with two free practice sessions on Friday (except in Monaco, where Friday practices are moved to Thursday), and one free practice on Saturday. Additional drivers (commonly known as third drivers) are allowed to run on Fridays, but only two cars may be used per team, requiring a race driver to give up his seat. A qualifying session is held after the last free practice session. This session determines the starting order for the race on Sunday.\n\nAs of the 2016 season the tyre rules have changed. This had to do with Pirelli's introduction of the new ultrasoft compound during the 2016 Monaco Grand Prix. The FIA determines for every race which three of the total of 5 dry-weather compounds are to be used. In prior seasons only two compounds were available per race, the \"prime\" and the \"option\" compound.\n\nEvery driver starts the weekend with thirteen sets of tyres, at least fifteen weeks before a non-European race Pirelli will announce which three of the five dry-weather compounds are available during the weekend. For European races this is nine weeks. They also nominate two mandatory sets for the race, one of which has to be used in the race. With one of the thirteen sets being the softest and reserved for the final qualifying session. This leaves ten sets being freely choosable by each driver. To the thirteen sets of tyres, three sets of wet-weather tyres and four sets of intermediate tyres are added. During a weekend at certain moments drivers have to hand back sets of tyres. The first set has to be handed back after forty minutes in the first practice session and one at the end. For the other two practice sessions two sets have to be handed in at the end.\n\nFor much of the sport's history, qualifying sessions differed little from practice sessions; drivers would have one or more sessions in which to set their fastest time, with the grid order determined by each driver's best single lap, with the fastest on pole position. Grids were generally limited to 26 cars – if the race had more entries, qualification would also decide which drivers would start the race. During the early 1990s, the number of entries was so high that the worst-performing teams had to enter a pre-qualifying session, with the fastest cars allowed through to the main qualifying session. The qualifying format began to change in the late 1990s, with the FIA experimenting with limiting the number of laps, determining the aggregate time over two sessions, and allowing each driver only one qualifying lap.\nThe current qualifying system was adopted in the 2006 season. Known as \"knock-out\" qualifying, it is split into three periods, known as Q1, Q2, and Q3. In each period, drivers run qualifying laps to attempt to advance to the next period, with the slowest drivers being \"knocked out\" at the end of the period and their grid positions set, based on their best lap times. Drivers are allowed as many laps as they wish within each period. After each period, all times are reset, and only a driver's fastest lap in that period (barring infractions) counts. Any timed lap started before the end of that period may be completed, and will count toward that driver's placement. The number of cars eliminated in each period is dependent on the total number of cars entered into the championship. Currently, with 20 cars, Q1 runs for 18 minutes, and eliminates the slowest five drivers. During this period, any driver whose best lap time exceeds 107% of the fastest time in Q1, will not be allowed to start the race without permission from the stewards. This rule does not affect drivers in Q2 or Q3. In Q2, the 15 remaining drivers have 15 minutes to set one of the ten fastest times and proceed to the next period. Finally, Q3 lasts 12 minutes and sees the remaining ten drivers decide the first ten grid positions. At the beginning of the 2016 Formula 1 season, the FIA introduced a new qualifying format, whereby drivers were knocked out every 90 seconds after a certain amount of time had passed in each session. The aim was to mix up grid positions for the race, but due to unpopularity the FIA reverted to the above qualifying format for the Chinese GP, after running the format for only two races.\n\nEach car taking part in Q3 receives an 'extra' set of the softest available tyre. This set has to be handed in after qualifying, drivers knocked out in Q1 or Q2 can use this set for the race. The first ten drivers, i.e. the drivers through to Q3 must start the race on the tyre which set the fastest time in Q2, unless the weather requires the use of wet-weather tyres. In which case all of the rules about the tyres won't be followed. All of the drivers that did not participate in Q3 have free tyre choice for the start of the race. Any penalties that affect grid position are applied at the end of qualifying. Grid penalties can be applied for driving infractions in the previous or current Grand Prix, or for changing a gearbox or engine component. If a car fails scrutineering, the driver will be excluded from qualifying, but will be allowed to start the race from the back of the grid at the race steward's discretion.\n\nThe race begins with a warm-up lap, after which the cars assemble on the starting grid in the order they qualified. This lap is often referred to as the formation lap, as the cars lap in formation with no overtaking (although a driver who makes a mistake may regain lost ground provided he has not fallen to the back of the field). The warm-up lap allows drivers to check the condition of the track and their car, gives the tyres a chance to warm up to increase traction, and also gives the pit crews time to clear themselves and their equipment from the grid.\nOnce all the cars have formed on the grid, a light system above the track indicates the start of the race: five red lights are illuminated at intervals of one second; they are all then extinguished simultaneously after an unspecified time (typically less than 3 seconds) to signal the start of the race. The start procedure may be abandoned if a driver stalls on the grid, signalled by raising his arm. If this happens, the procedure restarts: a new formation lap begins with the offending car removed from the grid. The race may also be restarted in the event of a serious accident or dangerous conditions, with the original start voided. The race may be started from behind the Safety Car if officials feel a racing start would be excessively dangerous, such as extremely heavy rainfall. As of the 2017 season there will always be a standing restart. If due to heavy rainfall a start behind the safety car is necessary, then after the track has dried sufficiently, drivers will form up for a standing start. There is no formation lap when races start behind the Safety Car.\n\nUnder normal circumstances, the winner of the race is the first driver to cross the finish line having completed a set number of laps. Race officials may end the race early (putting out a red flag) due to unsafe conditions such as extreme rainfall, and it must finish within two hours, although races are only likely to last this long in the case of extreme weather or if the safety car is deployed during the race.\n\nIn the 1950s, race distances varied from to . The maximum race length was reduced to in 1966 and in 1971. The race length was standardised to the current in 1989. However, street races like Monaco have shorter distances, to keep under the 2 hour limit.\n\nDrivers may overtake one another for position over the course of the race and are \"Classified\" in the order they finished 90% of the race distance. If a leader comes across a back marker (slower car) who has completed fewer laps, the back marker is shown a blue flag telling him he is obliged to allow the leader to overtake him. The slower car is said to be \"lapped\" and, once the leader finishes the race, is classified as finishing the race \"one lap down\". A driver can be lapped numerous times, by any car in front of him. A driver who fails to finish a race, through mechanical problems, accident, or any other reason is said to have retired from the race and is \"Not Classified\" in the results. However, if the driver has completed more than 90% of the race distance, he will be classified.\n\nThroughout the race, drivers may make pit stops to change tyres and repair damage (from 1994 to 2009 inclusive, they could also refuel). Different teams and drivers employ different pit stop strategies in order to maximise their car's potential. Three dry tyre compounds, with different durability and adhesion characteristics, are available to drivers. Over the course of a race, drivers must use two of the three available compounds. The different compounds have different levels of performance, and choosing when to use which compound is a key tactical decision to make. Different tyres have different colours on their sidewalls; this allows spectators to understand the strategies. Under wet conditions, drivers may switch to one of two specialised wet weather tyres with additional grooves (one \"intermediate\", for mild wet conditions, such as after recent rain, one \"full wet\", for racing in or immediately after rain). A driver must make at least one stop to use two tyre compounds; up to three stops are typically made, although further stops may be necessary to fix damage or if weather conditions change. If rain tyres are used, drivers are no longer obliged to use both types of dry tyres.\n\n\nThe format of the race has changed little through Formula One's history. The main changes have revolved around what is allowed at pit stops. In the early days of Grand Prix racing, a driver would be allowed to continue a race in his teammate's car should his develop a problem—in the modern era, cars are so carefully fitted to drivers that this has become impossible. In recent years, the emphasis has been on changing refuelling and tyre change regulations. From the 2010 season, refuelling—which was reintroduced in 1994—has not been allowed, to encourage less tactical racing following safety concerns. The rule requiring both compounds of tyre to be used during the race was introduced in 2007, again to encourage racing on the track. The safety car is another relatively recent innovation that reduced the need to deploy the red flag, allowing races to be completed on time for a growing international live television audience.\n\nVarious systems for awarding championship points have been used since 1950. The current system, in place since 2010, awards the top ten cars points in the Drivers' and Constructors' championships, with the winner receiving 25 points. If both cars of a team finish in the points, they both receive Constructors' Championship points. All points won at each race are added up, and the driver and constructor with the most points at the end of the season are crowned World Champions. Regardless of whether a driver stays with the same team throughout the season, or switches teams, all points earned by him count for the Drivers' Championship.\n\nA driver must be classified to receive points. In order to be classified, a driver need not finish the race, but complete at least 90% of the winner's race distance. Therefore, it is possible for a driver to receive points even if they retired before the end of the race.\n\nIn the event that less than 75% of the race laps are completed by the winner, only half of the points listed in the table are awarded to the drivers and constructors. This has happened on only five occasions in the history of the championship, and it had a notable influence on the final standing of the 1984 season. The last occurrence was at the 2009 Malaysian Grand Prix when the race was called off after 31 laps due to torrential rain.\n\nSince 1981, Formula One teams have been required to build the chassis in which they compete, and consequently the terms \"team\" and \"constructor\" became more or less interchangeable. This requirement distinguishes the sport from series such as the IndyCar Series which allows teams to purchase chassis, and \"spec series\" such as GP2, which require all cars be kept to an identical specification. It also effectively prohibits privateers, which were common even in Formula One well into the 1970s.\n\nThe sport's debut season, 1950, saw eighteen teams compete, but due to high costs many dropped out quickly. In fact, such was the scarcity of competitive cars for much of the first decade of Formula One that Formula Two cars were admitted to fill the grids. Ferrari is the oldest Formula One team, the only still-active team which competed in 1950.\n\nEarly manufacturer involvement came in the form of a \"factory team\" or \"works team\" (that is, one owned and staffed by a major car company), such as those of Alfa Romeo, Ferrari, or Renault. After having virtually disappeared by the early 1980s, factory teams made a comeback in the 1990s and 2000s and formed up to half the grid with Ferrari, Jaguar, BMW, Renault, Toyota, and Honda either setting up their own teams or buying out existing ones. Mercedes-Benz owned 40% of the McLaren team and manufactured the team's engines. Factory teams make up the top competitive teams; in 2008 wholly owned factory teams took four of the top five positions in the Constructors' Championship, and McLaren the other. Ferrari holds the record for having won the most Constructors' Championships (sixteen). However, by the end of the 2000s factory teams were once again on the decline with only Ferrari, Mercedes-Benz and Renault lodging entries to the 2010 championship.\n\nCompanies such as Climax, Repco, Cosworth, Hart, Judd and Supertec, which had no direct team affiliation, often sold engines to teams that could not afford to manufacture them. In the early years, independently owned Formula One teams sometimes also built their engines, though this became less common with the increased involvement of major car manufacturers such as BMW, Ferrari, Honda, Mercedes-Benz, Renault, and Toyota, whose large budgets rendered privately built engines less competitive. Cosworth was the last independent engine supplier. Beginning in 2007, the manufacturers' deep pockets and engineering ability took over, eliminating the last of the independent engine manufacturers. It is estimated the major teams spend between €100 and €200 million ($125–$225 million) per year per manufacturer on engines alone.\n\nIn the 2007 season, for the first time since the 1981 rule, two teams used chassis built by other teams. Super Aguri started the season using a modified Honda Racing RA106 chassis (used by Honda the previous year), while Scuderia Toro Rosso used the same chassis used by the parent Red Bull Racing team, which was formally designed by a separate subsidiary. The usage of these loopholes was ended for 2010 with the publication of new technical regulations, which require each constructor to own the intellectual property rights to their chassis, which prevents a team using a chassis owned by another Formula One constructor. The regulations continue to allow a team to subcontract the design and construction of the chassis to a third-party, an option used by the HRT team in 2010.\n\nAlthough teams rarely disclose information about their budgets, it is estimated they range from US$66 million to US$400 million each.\n\nEntering a new team in the Formula One World Championship requires a £25 million (about US$47 million) up-front payment to the FIA, which is then repaid to the team over the course of the season. As a consequence, constructors desiring to enter Formula One often prefer to buy an existing team: B.A.R.'s purchase of Tyrrell and Midland's purchase of Jordan allowed both of these teams to sidestep the large deposit and secure the benefits the team already had, such as TV revenue.\n\nEvery team in Formula One must run two cars in every session in a Grand Prix weekend, and every team may use up to four drivers in a season. A team may also run two additional drivers in Free Practice sessions, which are often used to test potential new drivers for a career as a Formula One driver or gain experienced drivers to evaluate the car. Most modern drivers are contracted for at least the duration of a season, with driver changes taking place in between seasons, in comparison to early years where drivers often competed at an ad hoc basis from race to race. Each competitor must be in the possession of a FIA Super Licence to compete in a Grand Prix, which is issued to drivers who have met the criteria of success in junior motorsport categories and having achieved of running in a Formula One car. Drivers may also be issued a Super Licence by the World Motor Sport Council if they fail to meet the criteria. Teams also contract test and reserve drivers, to stand in for regular drivers when necessary and develop the team's car; although with the reduction on testing the reserve drivers' role mainly takes places on a simulator, such as rFactor Pro, which is used by most of the F1 teams. Although most drivers earn their seat on ability, commercial considerations also come into play with teams having to satisfy sponsors and financial demands.\n\nEach driver chooses an unassigned number from 2 to 99 (excluding 17) upon entering Formula One, and keeps that number during his time in the series. The number one is reserved for the reigning Drivers' Champion, who retains his previous number and may choose to (but doesn't have to) use it instead of the number one. At the onset of the championship, numbers were allocated by race organisers on an ad-hoc basis from race to race, and competitors did not have a permanent number throughout the season. Permanent numbers were introduced in to take effect in , when teams were allocated numbers in ascending order based on the Constructors' Championship standings at the end of the 1973 season. The teams would hold those numbers from season to season with the exception of the team with the world Drivers' Champion, which would swap its numbers with the one and two of the previous champion's team. New entrants were allocated spare numbers, with the exception of the number 13 which had been unused since . As teams kept their numbers for long periods of time, car numbers became associated with a team, such as Ferrari's 27 and 28. A different system was used from to : at the start of each season, the current Drivers' Champion was designated number one, his teammate number two, and the rest of the teams assigned ascending numbers according to previous season's Constructors' Championship order.\n\nA total of 33 separate drivers have won the world championship, with Michael Schumacher holding the record for most championships with seven, as well as holding the race wins and pole position records. Juan Manuel Fangio has won the next most, with five championships won during the 1950s, as well as having won the greatest percentage of wins, with 24 out of 52 entries. Jochen Rindt is the only posthumous World Champion, after his points total was not overhauled despite his fatal accident at the 1970 Italian Grand Prix. Drivers from the United Kingdom have been the most successful in the sport, with 14 championships from 10 drivers, and 214 wins from 19.\n\nMost F1 drivers start in kart racing competitions, and then come up through traditional European single seater series like Formula Ford and Formula Renault to Formula 3, and finally the GP2 Series. GP2 started in 2005, replacing Formula 3000, which itself had replaced Formula Two as the last major stepping-stone into F1. Most champions from this level graduate into F1, but 2006 GP2 champion Lewis Hamilton became the first F2, F3000 or GP2 champion to win the Formula One driver's title in 2008. Drivers are not required to have competed at this level before entering Formula One. British F3 has supplied many F1 drivers, with champions including Nigel Mansell, Ayrton Senna and Mika Häkkinen having moved straight from that series to Formula One. More rarely a driver may be picked from an even lower level, as was the case with 2007 World Champion Kimi Räikkönen, who went straight from Formula Renault to F1, as well as Max Verstappen, who made his debut following a single season in European F3.\n\nAmerican Championship Car Racing has also contributed to the Formula One grid with mixed results. CART Champions Mario Andretti and Jacques Villeneuve became F1 World Champions, while Juan Pablo Montoya won seven races in F1. Other CART (also known as ChampCar) Champions, like Michael Andretti and Alessandro Zanardi won no races in F1. Other drivers have taken different paths to F1; Damon Hill raced motorbikes, and Michael Schumacher raced in sports cars, albeit after climbing through the junior single seater ranks. Former F1 driver Paul di Resta raced in DTM until he was signed with Force India in 2011. To race, however, the driver must hold an FIA Super Licence–ensuring that the driver has the requisite skills, and will not therefore be a danger to others. Some drivers have not had the licence when first signed to a F1 team; Räikkönen received the licence despite having only 23 car races to his credit.\n\nMost F1 drivers retire in their mid to late 30s; however, many keep racing in disciplines which are less physically demanding. The German touring car championship, the DTM, is a popular category involving ex-drivers such as two-time champion Mika Häkkinen and F1 race winners Jean Alesi, David Coulthard and Ralf Schumacher. In recent years, it has become common for former drivers who have had shorter careers to take up factory seats driving LMP1 cars in the FIA World Endurance Championship, with notable drivers including Mark Webber, Allan McNish, Anthony Davidson, Alexander Wurz, Kazuki Nakajima, and Sébastien Buemi. Some F1 drivers have left to race in the United States—Nigel Mansell and Emerson Fittipaldi duelled for the 1993 CART title, Rubens Barrichello moved to IndyCar in 2012, while Jacques Villeneuve, Juan Pablo Montoya, Nelson Piquet, Jr. and Scott Speed moved to NASCAR. Many drivers entered Formula E such as Nelson Piquet Jr, Sebastien Buemi, Bruno Senna, Jaime Alguersuari, Nick Heidfeld, Jarno Trulli, Jean-Eric Vergne and more. Some drivers, such as Vitantonio Liuzzi, Narain Karthikeyan and Jos Verstappen went on to race in the A1 Grand Prix series. During its existence from 2008 to 2011, Superleague Formula attracted ex-Formula One drivers like Sébastien Bourdais, Antônio Pizzonia and Giorgio Pantano. A series for former Formula One drivers, called Grand Prix Masters, ran briefly in 2005 and 2006. Others, like Jackie Stewart, Gerhard Berger and Alain Prost, returned to F1 as team owners while their former competitors have become colour commentators for TV coverage such as James Hunt (BBC), Martin Brundle (BBC, ITV and Sky), David Hobbs (NBC), Alan Jones (BBC, Nine Network and Ten Network) David Coulthard (BBC and Channel 4), Luciano Burti for Globo (Brazil), and Jean Alesi for Italian national network RAI. Others, such as Damon Hill and Jackie Stewart, take active roles in running motorsport in their own countries. Carlos Reutemann became a politician and served as governor of his native state in Argentina.\n\nThe number of Grands Prix held in a season has varied over the years. The inaugural world championship season comprised only seven races, while the season contained twenty-one races. Although throughout the first decades of the world championship there were no more than eleven Grands Prix a season, a large number of non-championship Formula One events also took place. The number of Grands Prix increased to an average of sixteen/seventeen by the late 1970s; simultaneously non-championship events ended by 1983. More Grands Prix began to be held in the 2000s, and recent seasons have seen an average of 19 races. In the calendar peaked at twenty-one events, the highest number of world championship races in one season.\n\nSix of the original seven races took place in Europe; the only non-European race that counted towards the World Championship in 1950 was the Indianapolis 500, which was held to different regulations and later replaced by the United States Grand Prix. The F1 championship gradually expanded to other non-European countries. Argentina hosted the first South American Grand Prix in 1953, and Morocco hosted the first African World Championship race in . Asia (Japan in ) and Oceania (Australia in ) followed, and the first race in the Middle East was held in . The nineteen races of the 2014 Formula One season were spread over every populated continent except for Africa, with ten Grands Prix held outside Europe.\nSome of the Grands Prix, such as the oldest recognised event the French Grand Prix, pre-date the formation of the World Championship and were incorporated into the championship as Formula One races in 1950. The British and Italian Grands Prix are the only events to have been held every Formula One season; other long-running races include the Belgian, German and currently defunct French Grands Prix. The Monaco Grand Prix, first held in 1929 and run continuously since 1955, is widely considered to be one of the most important and prestigious automobile races in the world.\n\nTraditionally each nation has hosted a single Grand Prix, which carries the name of the country. If a single country hosts multiple Grands Prix in a year they receive different names. In European countries the second event has often been titled the European Grand Prix, or named after a neighbouring state without a race. The United States has held six separate Grands Prix, including the Indianapolis 500, with the additional events named after the host city. Grands Prix are not always held at the same circuit each year, and may switch locations due to the suitability of the track or the financial status of the race organisers. The German Grand Prix currently alternates between the Nürburgring and Hockenheimring circuits, and others such as the American and French races have switched venues throughout their history.\n\nAll Grands Prix have traditionally been run during the day, until the inaugural hosted the first Formula One night race, which was followed in 2009 by the day–night Abu Dhabi Grand Prix and then the Bahrain Grand Prix which converted to a night race in 2014. Along with holding races at night, other Grands Prix in Asia have had their start times adjusted to benefit the European television audience.\n\nRecent additions to the calendar include the Singapore Grand Prix which, in September 2008, hosted the first night race ever held in Formula One, the Abu Dhabi Grand Prix, which hosted the first day-to-night race in November 2009, the Korean Grand Prix, first held in October 2010 and the Indian Grand Prix, first held in October 2011. The United States Grand Prix held its first race in Austin, Texas, at the new Circuit of the Americas in 2012. The first F1 Russian Grand Prix was held in 2014 at the new Sochi circuit, that runs around a venue used for the 2014 Winter Olympics.\n\nA typical circuit usually features a stretch of straight road on which the starting grid is situated. The \"pit lane\", where the drivers stop for tyres, aerodynamic adjustments and minor repairs (such as changing the car's nose due to front wing damage) during the race, retirements from the race, and where the teams work on the cars before the race, is normally located next to the starting grid. The layout of the rest of the circuit varies widely, although in most cases the circuit runs in a clockwise direction. Those few circuits that run anticlockwise (and therefore have predominantly left-handed corners) can cause drivers neck problems due to the enormous lateral forces generated by F1 cars pulling their heads in the opposite direction to normal.\n\nMost of the circuits currently in use are specially constructed for competition. The current street circuits are Monaco, Melbourne, Montreal, Singapore, Sochi and Azerbaijan although races in other urban locations come and go (Las Vegas and Detroit, for example) and proposals for such races are often discussed—most recently New Jersey. Several circuits have been completely laid out on public roads in the past, such as Valencia in Spain, though Monaco is the only one that remains. The glamour and history of the Monaco race are the primary reasons why the circuit is still in use, even though it does not meet the strict safety requirements imposed on other tracks. Three-time World champion Nelson Piquet famously described racing in Monaco as \"like riding a bicycle around your living room\".\n\nCircuit design to protect the safety of drivers is becoming increasingly sophisticated, as exemplified by the new Bahrain International Circuit, added in and designed—like most of F1's new circuits—by Hermann Tilke. Several of the new circuits in F1, especially those designed by Tilke, have been criticised as lacking the \"flow\" of such classics as Spa-Francorchamps and Imola. His redesign of the Hockenheim circuit in Germany for example, while providing more capacity for grandstands and eliminating extremely long and dangerous straights, has been frowned upon by many who argue that part of the character of the Hockenheim circuits was the long and blinding straights into dark forest sections. These newer circuits, however, are generally agreed to meet the safety standards of modern Formula One better than the older ones.\n\nOld favourites The RedBull Ring and the Autodromo Hermanos Rodriguez, returned to the calendar in 2014 and 2015 respectively.\n\nThe Circuit of Americas in Austin, the Sochi Autodrom in Sochi and the Baku City Circuit in Azerbaijan have all been introduced as brand new tracks since 2012.\n\nA single race requires hotel rooms to accommodate at least 5000 visitors.\n\nModern Formula One cars are mid-engined open cockpit, open wheel single-seaters. The chassis is made largely of carbon-fibre composites, rendering it light but extremely stiff and strong. The whole car, including engine, fluids and driver, weighs only – the minimum weight set by the regulations. If the construction of the car is lighter than the minimum, it can be ballasted up to add the necessary weight. The race teams take advantage of this by placing this ballast at the extreme bottom of the chassis, thereby locating the centre of gravity as low as possible in order to improve handling and weight transfer.\n\nThe cornering speed of Formula One cars is largely determined by the aerodynamic downforce that they generate, which pushes the car down onto the track. This is provided by \"wings\" mounted at the front and rear of the vehicle, and by ground effect created by low air pressure under the flat bottom of the car. The aerodynamic design of the cars is very heavily constrained to limit performance and the current generation of cars sport a large number of small winglets, \"barge boards\", and turning vanes designed to closely control the flow of the air over, under, and around the car.\n\nThe other major factor controlling the cornering speed of the cars is the design of the tyres. From to , the tyres in Formula One were not \"slicks\" (tyres with no tread pattern) as in most other circuit racing series. Instead, each tyre had four large circumferential grooves on its surface designed to limit the cornering speed of the cars. Slick tyres returned to Formula One in the season. Suspension is double wishbone or multilink front and rear, with pushrod operated springs and dampers on the chassis – one exception being that of the 2009 specification Red Bull Racing car (RB5) which used pullrod suspension at the rear, the first car to do so since the Minardi PS01 in 2001. Ferrari used a pullrod suspension at both the front and rear in their 2012 car. Both Ferrari (F138) and McLaren (MP4-28) of the 2013 season used a pullrod suspension at both the front and the rear.\n\nCarbon-carbon disc brakes are used for reduced weight and increased frictional performance. These provide a very high level of braking performance and are usually the element which provokes the greatest reaction from drivers new to the formula.\n\nFormula One cars must have four uncovered wheels, all made of the same metallic material, which must be one of two magnesium alloys specified by the FIA. Magnesium alloy wheels made by forging are used to achieve maximum unsprung rotating weight reduction.\nStarting with the 2014 Formula 1 season the engines have changed from a 2.4-litre naturally aspirated V8 to turbocharged 1.6 litre V6 \"power-units\". These get a significant amount of their power from electric motors. In addition they include a lot of energy recovery technology. Engines run on unleaded fuel closely resembling publicly available petrol. The oil which lubricates and protects the engine from overheating is very similar in viscosity to water. The 2006 generation of engines spun up to 20,000 rpm and produced up to . For , engines were restricted to 19,000 rpm with limited development areas allowed, following the engine specification freeze from the end of . For the 2009 Formula One season the engines were further restricted to 18,000 rpm.\n\nA wide variety of technologies—including active suspension and ground effect aerodynamics —are banned under the current regulations. Despite this the current generation of cars can reach speeds in excess of at some circuits. The highest straight line speed recorded during a Grand Prix was , set by Juan Pablo Montoya during the 2005 Italian Grand Prix. A Honda Formula One car, running with minimum downforce on a runway in the Mojave desert achieved a top speed of in 2006. According to Honda, the car fully met the FIA Formula One regulations. Even with the limitations on aerodynamics, at aerodynamically generated downforce is equal to the weight of the car, and the oft-repeated claim that Formula One cars create enough downforce to \"drive on the ceiling\", while possible in principle, has never been put to the test. Downforce of 2.5 times the car's weight can be achieved at full speed. The downforce means that the cars can achieve a lateral force with a magnitude of up to 3.5 times that of the force of gravity (3.5g) in cornering. Consequently, the driver's head is pulled sideways with a force equivalent to the weight of 20 kg in corners. Such high lateral forces are enough to make breathing difficult and the drivers need supreme concentration and fitness to maintain their focus for the one to two hours that it takes to complete the race. A high-performance road car like the Ferrari Enzo only achieves around 1g. \n\nAs of 2015, each team may have no more than two cars available for use at any time. Each driver may use no more than four engines during a championship season unless he drives for more than one team. If more engines are used, he drops ten places on the starting grid of the event at which an additional engine is used. The only exception is where the engine is provided by a manufacturer or supplier taking part in its first championship season, in which case up to five may be used by a driver. Each driver may use no more than one gearbox for six consecutive events; every unscheduled gearbox change requires the driver to drop five places on the grid unless he failed to finish the previous race due to reasons beyond the team's control.\n\nIn March 2007, \"F1 Racing\" published its annual estimates of spending by Formula One teams. The total spending of all eleven teams in 2006 was estimated at $2.9 billion US. This was broken down as follows: Toyota $418.5 million, Ferrari $406.5 m, McLaren $402 m, Honda $380.5 m, BMW Sauber $355 m, Renault $324 m, Red Bull $252 m, Williams $195.5 m, Midland F1/Spyker-MF1 $120 m, Toro Rosso $75 m, and Super Aguri $57 million.\n\nCosts vary greatly from team to team. Honda, Toyota, McLaren-Mercedes, and Ferrari are estimated to have spent approximately $200 million on engines in 2006, Renault spent approximately $125 million and Cosworth's 2006 V8 was developed for $15 million. In contrast to the 2006 season on which these figures are based, the 2007 sporting regulations ban all performance related engine development.\n\nFormula One teams pay entry fees of $500,000, plus $5,000 per point scored the previous year or $6,000 per point for the winner of the Constructors' Championship. Formula One drivers pay a FIA Super Licence fee, which in 2013 was €10,000 plus €1,000 per point.\n\nThere have been controversies with the way profits are shared amongst the teams. The smaller teams have complained that the profits are unevenly shared, favouring established top teams. In September 2015, Force India and Sauber officially lodged a complaint with the European Union against Formula One questioning the governance and stating that the system of dividing revenues and determining the rules is unfair and unlawful.\n\nThe cost of building a brand new permanent circuit can be up to hundreds of millions of dollars, while the cost of converting a public road, such as Albert Park, into a temporary circuit is much less. Permanent circuits, however, can generate revenue all year round from leasing the track for private races and other races, such as MotoGP. The Shanghai International Circuit cost over $300 million and the Istanbul Park circuit cost $150 million to build.\n\nA number of Formula One drivers earn the highest salary of any drivers in auto racing. The highest paid driver in 2010 was Fernando Alonso, who received $40 million in salary from Ferrari—a record for any driver. The very top Formula One drivers get paid more than IndyCar or NASCAR drivers, however the earnings immediately fall off after the top three F1 drivers and the majority of NASCAR racers will make more money than their F1 counterparts. Most top IndyCar drivers are paid around a tenth of their Formula One counterparts.\n\nThe expense of Formula One has seen the FIA and the Formula One Commission attempt to create new regulations to lower the costs for a team to compete in the sport. Cost-saving proposals have included allowing customer cars, either by teams purchasing a car from another constructor, or the series supplying a basic chassis and engine to some teams at a low cost. Allowing teams to share more car components such as the monocoque and safety components is also under consideration. The FIA also continually researches new ways to increase safety in the sport, which includes introducing new regulations and accident procedures.\n\nIn the interest of making the sport truer to its role as a World Championship, Bernie Ecclestone had initiated and organised a number of Grands Prix in new countries. Proposals to hold future races are regularly made by both new locations and countries and circuits that have previously hosted a Formula One Grand Prix. The most recent addition is the Azerbaijan Grand Prix in Baku, Azerbaijan.\n\nFormula One can be seen live or tape delayed in almost every country and territory around the world and attracts one of the largest global television audiences. The 2008 season attracted a global audience of 600 million people per race. It is a massive television event; the cumulative television audience was calculated to be 54 billion for the 2001 season, broadcast to 200 territories.\n\nDuring the early 2000s, Formula One Group created a number of trademarks, an official logo, and an official website for the sport in an attempt to give it a corporate identity. Ecclestone experimented with a digital television package (known colloquially as Bernievision) which was launched at the 1996 German Grand Prix in cooperation with German digital television service \"DF1\", 30 years after the first GP colour TV broadcast, the 1967 German Grand Prix. This service offered the viewer several simultaneous feeds (such as super signal, on board, top of field, backfield, highlights, pit lane, timing) which were produced with cameras, technical equipment and staff different from those used for the conventional coverage. It was introduced in many countries over the years, but was shut down after the 2002 season for financial reasons.\n\nTV stations all take what is known as the \"World Feed\", either produced by the FOM (Formula One Management) or occasionally, the \"host broadcaster\". The only station that originally differed from this\nwas \"Premiere\"—a German channel which offers all sessions live and interactive, with features such as the onboard channel. This service was more widely available around Europe until the end of 2002, when the cost of a whole different feed for the digital interactive services was thought too much. This was in large part because of the failure of the \"F1 Digital +\" Channel launched through Sky in the United Kingdom. Prices were too high for viewers, considering they could watch both the qualifying and the races themselves free on ITV.\n\nHowever, upon the commencement of its coverage for the 2009 season, the BBC reintroduced complementary features such as the \"red button\" in-car camera angles, multiple soundtracks (broadcast commentary, CBBC commentary for children, or ambient sound only) and a rolling highlights package. Different combinations of these features are available across the various digital platforms (Freeview, Freesat, Sky, Virgin Media cable and the BBC F1 web site) prior to, during, and after the race weekend. Not all services are available across all the various platforms due to technical constraints. The BBC also broadcasts a post-race programme called \"F1 Forum\" on the digital terrestrial platforms' \"red button\" interactive services.\nAn announcement made on 12 January 2011, on the official Formula 1 website, announced that F1 would adopt the HD format for the 2011 season offering a world feed at a data rate of 42 Megabits/second (MPEG-2). The BBC subsequently announced later that day that their 2011 F1 coverage would be broadcast in HD which has been made immediately possible due to SIS LIVE, the provider of the BBC's F1 outside broadcast coverage, having already upgraded their technical facilities to HD as of the 2010 Belgian Grand Prix.\n\nIt was announced on 29 July 2011 that Sky Sports and the BBC would team up to show the races in F1 in 2012. In March 2012, Sky launched a channel dedicated to F1, with an HD counterpart. Sky Sports F1 covered all races live without commercial interruption as well as live practice and qualifying sessions, along with F1 programming, including interviews, archive action and magazine shows. The deal secured Formula 1 on Sky up to 2018. The BBC in 2012 featured live coverage of half of the races in the season: China, Spain, Monaco, Europe, Britain, Belgium, Singapore, Korea, Abu Dhabi, and Brazil. The BBC also showed live coverage of practice and qualifying sessions from those races. For the races that the BBC did not show live, \"extended highlights\" of the race were available a few hours after the live broadcast.\n\nBBC ended their joint television contract after the 2015 season, transferring their rights to Channel 4 until the end of the 2018 season, with their coverage being presented by former \"T4\" presenter Steve Jones. Sky Sports F1 coverage will remain unaffected and BBC Radio 5 Live and 5 Live Sports Extra will be extended until the 2021 season.\n\nFormula One has an extensive web following, with most major TV companies covering it such as the BBC. The official Formula One website (formula1.com) has a live timing JavaScript applet that can be used during the race to keep up with the leaderboard in real time. Recently an official application has been made available in the iTunes App Store and on Google Play that allows iOS and Android users to see a real time feed of driver positions, timing and commentary. The same official application has been available for Android phones and tablets since 2011.\n\nTo accommodate fans who were unable to view the races on live television, Formula One Management's in-house production team began producing exclusive race edits synchronized to music from some of the world's top artists.\n\nCurrently the terms \"Formula One race\" and \"World Championship race\" are effectively synonymous; since 1984, every Formula One race has counted towards the World Championship, and every World Championship race has been to Formula One regulations. But the two terms are not interchangeable.\n\nThe distinction is most relevant when considering career summaries and \"all time lists\". For example, in the List of Formula One drivers, Clemente Biondetti is shown with 1 race against his name. Biondetti actually competed in \"four\" Formula One races in 1950, but only one of these counted for the World Championship. Similarly, several Indianapolis 500 winners technically won their first World Championship race, though most record books choose to ignore this and instead only record regular participants.\n\n"}
{"id": "10855", "url": "https://en.wikipedia.org/wiki?curid=10855", "title": "Franco Baresi", "text": "Franco Baresi\n\nFranco Baresi (; born 8 May 1960) is an Italian football youth team coach and a former player and manager. He mainly played as a sweeper or as a central defender, and spent his entire 20-year career with Serie A club A.C. Milan, captaining the club for 15 seasons. He is considered one of the greatest defenders of all time, and was ranked 19th in World Soccer's list of the 100 greatest players of the twentieth century. He won the Champions League 3 times, as well as 6 Serie A titles, 4 Supercoppa Italiana titles, 2 European Super Cups and 2 Intercontinental Cups.\n\nWith Italy, he won the 1982 FIFA World Cup. He also played in the 1990 FIFA World Cup where he was named in the FIFA World Cup All-Star Team, finishing third in the competition. At the 1994 FIFA World Cup he was named Italy's captain and was an integral part of the team that reached the final, although he would miss a penalty in the resulting shoot-out, as Brazil lifted the trophy. Baresi also represented Italy at two UEFA European Championships, in 1980 and 1988, and at the 1984 Olympics, reaching the semi-finals on each occasion.\n\nThe younger brother of former footballer Giuseppe Baresi, after joining the Milan senior team as a youngster, Franco Baresi was initially nicknamed \"Piscinin\", Milanese for \"Little one\"; due to his skill and success, he was later known as \"Kaiser Franz\", a reference to fellow sweeper Franz Beckenbauer. In 1999, he was voted Milan's Player of the Century. After his final season at Milan in 1997, the club retired Baresi's shirt number 6. He was named by Pelé one of the 125 Greatest Living Footballers at the FIFA centenary awards ceremony in 2004. Baresi was inducted into the Italian Football Hall of Fame in 2013.\n\nOriginally a Milan youth product, Baresi went on to spend his entire twenty-year professional career with Milan, making his Serie A debut at the age of 17, during the 1977–78 season on 23 April 1978. He had initially been rejected by Inter, who chose his brother Giuseppe instead, while Milan signed Franco Baresi. The following season, he was made a member of the starting eleven, playing as a sweeper or as a centreback, winning the 1978–79 Serie A title, Milan's tenth overall, playing alongside Fabio Capello, and also Gianni Rivera, in what would be his last season at the club. This success was soon followed by a dark period in the club's history, when Milan was relegated to Serie B twice during the early 1980s. Milan were relegated in 1980 for being involved in the match fixing scandal of 1980, and once again after finishing third-last in the 1981–82 season, after having just returned to Serie A the previous season, after winning the 1980–81 Serie B title. Despite being a member of the Euro 1980 Italy squad that had finished fourth, and the 1982 World Cup winning team, Baresi elected to stay with Milan, winning the Serie B title for the second time during the 1982–83 season, and bringing Milan back to Serie A. After Aldo Maldera and Fulvio Collovati left the club in 1982, Baresi was appointed Milan's captain, at the age of 22, and would hold this position for much of his time at the club, becoming a symbol and a leader for the team. During this temporary bleak period for Milan, Baresi did manage to win a Mitropa Cup in 1982, and reached the Coppa Italia final during 1984–85 season, although the team failed to dominate in Serie A.\n\nDuring the end of the 1980s and the first half of the 1990s, Baresi was at the heart of a notable all-Italian defence alongside Paolo Maldini, Alessandro Costacurta, Mauro Tassotti, and later, Christian Panucci, under managers Arrigo Sacchi and Fabio Capello, a defence which is regarded by many as one of the greatest of all time. When the attacking Dutch trio of Marco van Basten, Ruud Gullit and Frank Rijkaard arrived at the club in the late 1980s, Milan began a period of domestic and international triumphs, and between 1987 and 1996, at the height of the club's success, the Milan squad contained many Italian and international stars, such as Roberto Donadoni, Carlo Ancelotti, Marco van Basten, Ruud Gullit, Frank Rijkaard, and later, Demetrio Albertini, Dejan Savićević, Zvonimir Boban, Marcel Desailly, George Weah, Jean-Pierre Papin, Brian Laudrup, and Roberto Baggio. Under Sacchi, Milan won the Serie A title in 1987–88, with Baresi helping Milan to concede only 14 goals. This title was immediately followed by an Italian Supercup in 1988 the next season, and back to back European Cups in 1988–89 and 1989–90. Baresi was also runner-up to team mate Van Basten for the Ballon d'Or in 1989, finishing ahead of his other team mate Frank Rijkaard, and was named Serie A Footballer of the Year in 1989–90. Milan also reached the Coppa Italia final during the 1989–90 season.\n\nBaresi went on to win four more Serie A titles with Milan under Fabio Capello, including three consecutive titles, during the 1991–92, 1992–93 and the 1993–94 seasons. Baresi helped Milan win the 1991–92 title undefeated, helping Milan to go unbeaten for an Italian record of 58 matches. Milan also scored a record 74 goals that season. During the 1993–94 season, Baresi helped Milan concede a mere 15 goals in Serie A, helping the club to finish the season with the best defence. Baresi also won three consecutive Italian Supercups under Capello, in 1992, 1993, and 1994. Milan also reached three consecutive UEFA Champions League Finals during the 1992–93, 1993–94 and 1994–95 seasons, losing out to Marseille in the 1992–93 tournament, and Ajax in the 1994–95 tournament. Baresi won the third European Cup (UEFA Champions League) of his career in 1993–94 where, Milan defeated Johan Cruyff's 'Dream Team' FC Barcelona 4–0 in the final. Baresi also managed to win the 1994 European Supercup, although Milan were defeated in the 1994 Intercontinental Cup, the 1993 UEFA Super Cup and the 1993 Intercontinental Cup. Under Capello, Milan and Baresi were able to capture another Serie A title during 1995–96 season, Baresi's sixth title in total.\n\nBaresi retired at the end of the 1996–97 Serie A season, at the age of 37. In his 20 seasons with Milan, he won six Serie A titles, three Champions League titles (reaching five finals in total), two Intercontinental Cups (four finals in total), four European Supercups (five finals in total), four Italian Supercups (five finals in total), two Serie B titles, and a Mitropa Cup. He scored 31 goals for Milan, 21 of which were on penalties, and, despite being a defender, he was the top scorer of the Coppa Italia during the 1989–90 season, the only trophy which he failed to win with Milan, reaching the final twice during his career. His final goal for Milan was scored in a 2–1 win against Padova on 27 August 1995. In his honour, Milan retired his number 6 shirt, which he had worn throughout his career. The captain's armband, which he had worn for 15 seasons, was handed over to Paolo Maldini. Milan organised a celebration match in his honour, which was played on 28 October 1997 at the San Siro stadium, featuring many footballing stars.\n\nAt the age of 20, whilst still playing in the Italy Under-21 side, Baresi was named in Italy's 22-man squad for the 1980 European Championship along with his older brother Giuseppe, by manager Enzo Bearzot. The tournament was held on home soil, and Italy went on to finish fourth, although, unlike his brother, Franco Baresi did not play a single match in the tournament. Euro 1980 would be the only time that the two brothers were on the Italy squad together at a major tournament. At the age of 22, Baresi was named in Italy's squad for the 1982 FIFA World Cup. The \"Azzurri\" won their third World Cup, beating West Germany in the final, but Baresi, once again, was not selected to play a match throughout the tournament. Baresi was also a member of the Italy squad that took part in the 1984 Olympics. Italy finished in fourth place after a semi-final defeat to Brazil, and losing the bronze medal match to Yugoslavia. Baresi scored a goal against the USA during the group stage. He won his first senior international cap in a 1984 UEFA European Championship qualifying match against Romania in Florence, on 14 December 1982, which ended 0–0. Italy, however, ultimately failed to qualify for the final tournament.\n\nBaresi was not included in Italy's squad for the 1986 World Cup by coach Enzo Bearzot, who saw him as being more of a midfielder than a defender (although his brother Giuseppe was selected as a defender for the World Cup, as well as Roberto Tricella). He returned to the team for the 1988 European Championships, playing as a sweeper, where Italy reached the semi-finals under Azeglio Vicini, becoming an undisputed first team member, playing in every match. He made his first appearance in a World Cup finals match in the 1990 tournament, which was held on home soil, and he played in every match as one of the starting centrebacks, as Italy finished in third-place, after being eliminated by defending champions Argentina in a penalty shootout in the semi-finals. Baresi helped the Italian defence to keep five consecutive clean sheets, only conceding 2 goals, and going unbeaten for a World Cup record of 518 minutes, until they were beaten by an Argentinian equaliser in the semi-final. His performances earned him a spot on the 1990 World Cup Team of the tournament.\n\nAfter replacing Giuseppe Bergomi as captain for the 1994 World Cup under his former manager at Milan Arrigo Sacchi, Baresi sustained an injury to his meniscus in Italy's second group match (a 1–0 win against Norway) and missed most of the tournament. He returned to the squad 25 days later, in record time for the final, with a dominant defensive performance, helping Italy to keep a clean sheet against the Brazilians, despite the key defensive absences of his Milan team mates Costacurta and Tassotti. After a 0–0 deadlock following extra time, the match went to a penalty shootout, and Baresi subsequently missed his penalty, suffering from severe cramps and fatigue. Following misses by Massaro and Baggio, Italy were defeated by Brazil in the penalty shootout. Following the World Cup defeat, Baresi made one more appearance for Italy, in an away Euro 1996 qualifying match against Slovenia, on 7 September 1994, which ended in a 1–1 draw. Baresi subsequently retired from the national side at the age of 34, and passed on the captain's armband to his Milan team-mate Maldini. Baresi amassed 81 caps for Italy, scoring one goal in a friendly win against the USSR, and he is one of seven players to have achieved the rare feat of winning Gold, Silver and Bronze World Cup medals during his international career.\n\nBaresi is regarded as one of the greatest defenders of all time. He played his entire twenty-year career with Milan, becoming a club legend. At Milan, he formed one of the most formidable defensive units of all time, alongside Maldini, Costacurta, Tassotti, Galli, and later Panucci. He was a complete and consistent defender, who combined power with elegance, and he was gifted with outstanding physical and mental attributes, such as pace, strength, tenacity, concentration and stamina, which made him effective in the air.\n\nAlthough Baresi was capable of playing anywhere along the backline, he primarily excelled as a centreback and as sweeper, where he combined his defensive attributes, and his ability to read the game, with his excellent vision, technique, distribution, and ball skills. His passing range, technical ability, and ball control allowed him to advance forward into the midfield to start attacking plays from the back, enabling him to function as a secondary playmaker for his team, and also play as a defensive or central midfielder when necessary. Despite being a defender, he was also an accurate penalty kick taker. Baresi was known for being a strong and accurate tackler, who was very good at winning back possession, and at anticipating and intercepting plays, due to his acute tactical intelligence, marking ability, and positional sense. He was also known for his professionalism, his longevity, his outstanding leadership, his commanding presence on the pitch, and his organisational skills throughout his career, captaining both Milan and the Italian national side.\n\nOn 1 June 2002, Baresi was officially appointed as director of football at Fulham, but tensions between Baresi and then Fulham manager Jean Tigana led to resignation from the club in August.\n\nHe was appointed head coach of Milan's \"Primavera\" Under-20 squad. In 2006, he was moved by the club to coach the \"Berretti\" Under-19 squad, with his former teammate Filippo Galli replacing him at the helm of the Primavera squad. He retired from coaching and was replaced by Roberto Bertuzzo.\n\nFranco Baresi is the younger brother of Internazionale legendary defender Giuseppe Baresi. Interestingly, as youngsters, both players had tryouts for Inter, but Franco was rejected, and purchased by local rivals Milan; as he was the younger player, he was initially known as \"Baresi 2\". Due to Franco's eventual great success and popularity throughout his career, however, which even surpassed that of his older brother's, Giuseppe later became known as \"the other Baresi\", despite also achieving notable success.\n\nBaresi is featured in the EA Sports football video game series \"FIFA 14\"'s Classic XI – a multi-national all-star team, along with compatriots Bruno Conti, Gianni Rivera, and Giacinto Facchetti. He was named in the Ultimate Team Legends in \"FIFA 15\".\n\n\"*European competitions include the UEFA Champions League, UEFA Cup, and UEFA Super Cup\"\n\n\n\n\n\n\n \n"}
{"id": "10857", "url": "https://en.wikipedia.org/wiki?curid=10857", "title": "Stage (stratigraphy)", "text": "Stage (stratigraphy)\n\nIn chronostratigraphy, a stage is a succession of rock strata laid down in a single age on the geologic timescale, which usually represents millions of years of deposition. A given stage of rock and the corresponding age of time will by convention have the same name, and the same boundaries.\n\nRock series are divided into stages, just as geological epochs are divided into ages. Stages can be divided into smaller stratigraphic units called chronozones. (See chart at right for full terminology hierarchy.) Stages may also be divided into substages or indeed grouped as superstages.\n\nThe term faunal stage is sometimes used, referring to the fact that the same fauna (animals) are found throughout the layer (by definition).\n\nStages are primarily defined by a consistent set of fossils (biostratigraphy) or a consistent magnetic polarity (see paleomagnetism) in the rock. Usually one or more index fossils that are common, found worldwide, easily recognized, and limited to a single, or at most a few, stages are used to define the stage's bottom. \n\nThus, for example in the local North American subdivision, a paleontologist finding fragments of the trilobite \"Olenellus\" would identify the beds as being from the Waucoban Stage whereas fragments of a later trilobite such as \"Elrathia\" would identify the stage as Albertan. \n\nStages were important in the 19th and early 20th centuries as they were the major tool available for dating and correlating rock units prior to the development of seismology and radioactive dating in the second half of the 20th Century. Microscopic analysis of the rock (petrology) is also sometimes useful in confirming that a given segment of rock is from a particular age.\n\nOriginally, faunal stages were only defined regionally; however as additional stratigraphic and geochonologic tools, were developed, stages were defined over broader and broader areas. More recently, the adjective \"faunal\" has been dropped as regional and global correlations of rock sequences have become relatively certain and there is less need for faunal labels to define the age of formations. A tendency developed to use European and, to a lesser extent, Asian, stage names for the same time period worldwide, even though the faunas in other regions often had little in common with the stage as originally defined.\n\nBoundaries and names are established by the International Commission on Stratigraphy (ICS) of the International Union of Geological Sciences. As of 2008, the ICS is nearly finished a task begun in 1974, subdividing the Phanerozoic eonothem into internationally accepted stages using two types of benchmark. For younger stages, a Global Boundary Stratotype Section and Point (GSSP), a physical outcrop clearly demonstrates the boundary. For older stages, a Global Standard Stratigraphic Age (GSSA) is an absolute date. The benchmarks will give a much greater certainty that results can be compared with confidence in the date determinations, and such results will have farther scope than any evaluation based solely on local knowledge and conditions. \n\nIn many regions local subdivisions and classification criteria are still used along with the newer internationally coordinated uniform system, but once the research establishes a more complete international system, it is expected that local systems will be abandoned.\n\nStages can include many lithostratigraphic units (for example formations, beds, members, etc.) of differing rock types that were being laid down in different environments at the same time. In the same way, a lithostratigraphic unit can include a number of stages or parts of them.\n\n\n\n"}
{"id": "10858", "url": "https://en.wikipedia.org/wiki?curid=10858", "title": "Franz Kafka", "text": "Franz Kafka\n\nFranz Kafka (3 July 1883 – 3 June 1924) was a German-language novelist and short story writer, widely regarded as one of the major figures of 20th-century literature. His work, which fuses elements of realism and the fantastic, typically features isolated protagonists faced by bizarre or surrealistic predicaments and incomprehensible social-bureaucratic powers, and has been interpreted as exploring themes of alienation, existential anxiety, guilt, and absurdity. His best known works include \"\" (\"The Metamorphosis\"), ' (\"The Trial\"), and ' (\"The Castle\"). The term \"\" has entered the English language to describe situations like those in his writing.\n\nKafka was born into a middle-class, German-speaking Jewish family in Prague, the capital of the Kingdom of Bohemia, then part of the Austro-Hungarian Empire, today part of the Czech Republic. He trained as a lawyer, and after completing his legal education he was employed with an insurance company, forcing him to relegate writing to his spare time. Over the course of his life, Kafka wrote hundreds of letters to family and close friends, including his father, with whom he had a strained and formal relationship. He became engaged to several women but never married. He died in 1924 at the age of 40 from tuberculosis.\n\nFew of Kafka's works were published during his lifetime: the story collections ' (\"Contemplation\") and ' (\"A Country Doctor\"), and individual stories (such as \"\") were published in literary magazines but received little public attention. Kafka's unfinished works, including his novels ', ' and ' (also known as ', \"The Man Who Disappeared\"), were ordered by Kafka to be destroyed by his friend Max Brod, who nonetheless ignored his friend's direction and published them after Kafka's death. His work went on to influence a vast range of writers, critics, artists, and philosophers during the 20th century.\n\nKafka was born near the Old Town Square in Prague, then part of the Austro-Hungarian Empire. His family were middle-class Ashkenazi Jews. His father, Hermann Kafka (1854–1931), was the fourth child of Jakob Kafka, a ' or ritual slaughterer in Osek, a Czech village with a large Jewish population located near Strakonice in southern Bohemia. Hermann brought the Kafka family to Prague. After working as a travelling sales representative, he eventually became a fancy goods and clothing retailer who employed up to 15 people and used the image of a jackdaw (' in Czech, pronounced and colloquially written as \"kafka\") as his business logo. Kafka's mother, Julie (1856–1934), was the daughter of Jakob Löwy, a prosperous retail merchant in Poděbrady, and was better educated than her husband.\nKafka's parents probably spoke a German influenced by Yiddish that was sometimes pejoratively called Mauscheldeutsch, but, as the German language was considered the vehicle of social mobility, they probably encouraged their children to speak High German. Hermann and Julie had six children, of whom Franz was the eldest. Franz's two brothers, Georg and Heinrich, died in infancy before Franz was seven; his three sisters were Gabriele (\"Ellie\") (1889–1944), Valerie (\"Valli\") (1890–1942) and Ottilie (\"Ottla\") (1892–1943). They all died during the Holocaust of World War II. Valli was deported to the Łódź Ghetto in Poland in 1942, but that is the last documentation of her. Ottilie was his favorite sister.\n\nHermann is described by the biographer Stanley Corngold as a \"huge, selfish, overbearing businessman\" and by Franz Kafka as \"a true Kafka in strength, health, appetite, loudness of voice, eloquence, self-satisfaction, worldly dominance, endurance, presence of mind, [and] knowledge of human nature\". On business days, both parents were absent from the home, with Julie Kafka working as many as 12 hours each day helping to manage the family business. Consequently, Kafka's childhood was somewhat lonely, and the children were reared largely by a series of governesses and servants. Kafka's troubled relationship with his father is evident in his \"\" (\"Letter to His Father\") of more than 100 pages, in which he complains of being profoundly affected by his father's authoritarian and demanding character; his mother, in contrast, was quiet and shy. The dominating figure of Kafka's father had a significant influence on Kafka's writing.\n\nThe Kafka family had a servant girl living with them in a cramped apartment. Franz's room was often cold. In November 1913 the family moved into a bigger apartment, although Ellie and Valli had married and moved out of the first apartment. In early August 1914, just after World War I began, the sisters did not know where their husbands were in the military and moved back in with the family in this larger apartment. Both Ellie and Valli also had children. Franz at age 31 moved into Valli's former apartment, quiet by contrast, and lived by himself for the first time.\n\nFrom 1889 to 1893, Kafka attended the ' German boys' elementary school at the ' (meat market), now known as Masná Street. His Jewish education ended with his Bar Mitzvah celebration at the age of 13. Kafka never enjoyed attending the synagogue and went with his father only on four high holidays a year.\n\nAfter leaving elementary school in 1893, Kafka was admitted to the rigorous classics-oriented state gymnasium, \"\", an academic secondary school at Old Town Square, within the Kinský Palace. German was the language of instruction, but Kafka also spoke and wrote in Czech. He studied the latter at the gymnasium for eight years, achieving good grades. Although Kafka received compliments for his Czech, he never considered himself fluent in Czech, though he spoke German with a Czech accent. He completed his Matura exams in 1901.\n\nAdmitted to the ' of Prague in 1901, Kafka began studying chemistry, but switched to law after two weeks. Although this field did not excite him, it offered a range of career possibilities which pleased his father. In addition, law required a longer course of study, giving Kafka time to take classes in German studies and art history. He also joined a student club, ' (Reading and Lecture Hall of the German students), which organized literary events, readings and other activities. Among Kafka's friends were the journalist Felix Weltsch, who studied philosophy, the actor Yitzchak Lowy who came from an orthodox Hasidic Warsaw family, and the writers Oskar Baum and Franz Werfel.\n\nAt the end of his first year of studies, Kafka met Max Brod, a fellow law student who became a close friend for life. Brod soon noticed that, although Kafka was shy and seldom spoke, what he said was usually profound. Kafka was an avid reader throughout his life; together he and Brod read Plato's \"Protagoras\" in the original Greek, on Brod's initiative, and Flaubert's ' and ' (\"The Temptation of Saint Anthony\") in French, at his own suggestion. Kafka considered Fyodor Dostoyevsky, Flaubert, Nikolai Gogol, Franz Grillparzer, and Heinrich von Kleist to be his \"true blood brothers\". Besides these, he took an interest in Czech literature and was also very fond of the works of Goethe. Kafka was awarded the degree of Doctor of Law on 18 July 1906 and performed an obligatory year of unpaid service as law clerk for the civil and criminal courts.\n\nOn 1 November 1907, Kafka was hired at the \"\", an insurance company, where he worked for nearly a year. His correspondence during that period indicates that he was unhappy with a working time schedule—from 08:00 until 18:00—making it extremely difficult to concentrate on writing, which was assuming increasing importance to him. On 15 July 1908, he resigned. Two weeks later he found employment more amenable to writing when he joined the Worker's Accident Insurance Institute for the Kingdom of Bohemia. The job involved investigating and assessing compensation for personal injury to industrial workers; accidents such as lost fingers or limbs were commonplace at this time owing to poor work safety policies at the time. It was especially true of factories fitted with machine lathes, drills, planing machines and rotary saws which were rarely fitted with safety guards.\n\nThe management professor Peter Drucker credits Kafka with developing the first civilian hard hat while employed at the Worker's Accident Insurance Institute, but this is not supported by any document from his employer. His father often referred to his son's job as an insurance officer as a ', literally \"bread job\", a job done only to pay the bills; Kafka often claimed to despise it. Kafka was rapidly promoted and his duties included processing and investigating compensation claims, writing reports, and handling appeals from businessmen who thought their firms had been placed in too high a risk category, which cost them more in insurance premiums. He would compile and compose the annual report on the insurance institute for the several years he worked there. The reports were received well by his superiors. Kafka usually got off work at 2 p.m., so that he had time to spend on his literary work, to which he was committed. Kafka's father also expected him to help out at and take over the family fancy goods store. In his later years, Kafka's illness often prevented him from working at the insurance bureau and at his writing. Years later, Brod coined the term ' (\"The Close Prague Circle\") to describe the group of writers, which included Kafka, Felix Weltsch and him.\n\nIn late 1911, Elli's husband Karl Hermann and Kafka became partners in the first asbestos factory in Prague, known as Prager Asbestwerke Hermann & Co., having used dowry money from Hermann Kafka. Kafka showed a positive attitude at first, dedicating much of his free time to the business, but he later resented the encroachment of this work on his writing time. During that period, he also found interest and entertainment in the performances of Yiddish theatre. After seeing a Yiddish theater troupe perform in October 1911, for the next six months Kafka \"immersed himself in Yiddish language and in Yiddish literature\". This interest also served as a starting point for his growing exploration of Judaism. It was at about this time that Kafka became a vegetarian. Around 1915 Kafka received his draft notice for military service in World WarI, but his employers at the insurance institute arranged for a deferment because his work was considered essential government service. Later he attempted to join the military but was prevented from doing so by medical problems associated with tuberculosis, with which he was diagnosed in 1917. In 1918 the Worker's Accident Insurance Institute put Kafka on a pension due to his illness, for which there was no cure at the time, and he spent most of the rest of his life in sanatoriums.\n\nKafka was never married. According to Brod, Kafka was \"tortured\" by sexual desire and Kafka's biographer Reiner Stach states that his life was full of \"incessant womanising\" and that he was filled with a fear of \"sexual failure\". He visited brothels for most of his adult life and was interested in pornography. In addition, he had close relationships with several women during his life. On 13 August 1912, Kafka met Felice Bauer, a relative of Brod, who worked in Berlin as a representative of a dictaphone company. A week after the meeting at Brod's home, Kafka wrote in his diary:\n\nShortly after this, Kafka wrote the story \"\" (\"The Judgment\") in only one night and worked in a productive period on ' (\"The Man Who Disappeared\") and \"Die Verwandlung\" (\"The Metamorphosis\"). Kafka and Felice Bauer communicated mostly through letters over the next five years, met occasionally, and were engaged twice. Kafka's extant letters to her were published as ' (\"Letters to Felice\"); her letters do not survive. According to biographers Stach and James Hawes, around 1920 Kafka was engaged a third time, to Julie Wohryzek, a poor and uneducated hotel chambermaid. Although the two rented a flat and set a wedding date, the marriage never took place. During this time Kafka began a draft of the \"Letter to His Father\", who objected to Julie because of her Zionist beliefs. Before the date of the intended marriage, he took up with yet another woman. While he needed women and sex in his life, he had low self-confidence, felt sex was dirty, and was shy—especially about his body.\n\nStach and Brod state that during the time that Kafka knew Felice Bauer, he had an affair with a friend of hers, Margarethe \"Grete\" Bloch, a Jewish woman from Berlin. Brod says that Bloch gave birth to Kafka's son, although Kafka never knew about the child. The boy, whose name is not known, was born in 1914 or 1915 and died in Munich in 1921. However, Kafka's biographer Peter-André Alt claims that, while Bloch had a son, Kafka was not the father as the pair were never intimate. Stach states that Bloch had a son, but there is not solid proof and moreover contradictory evidence that Kafka was the father.\n\nKafka was diagnosed with tuberculosis in August 1917 and moved for a few months to the Bohemian village of Zürau (Siřem in the Czech language), where his sister Ottla worked on the farm of her brother-in-law Karl Hermann. He felt comfortable there and later described this time as perhaps the best time in his life, probably because he had no responsibilities. He kept diaries and ' (octavo). From the notes in these books, Kafka extracted 109 numbered pieces of text on \"Zettel\", single pieces of paper in no given order. They were later published as ' (The Zürau Aphorisms or Reflections on Sin, Hope, Suffering, and the True Way).\n\nIn 1920 Kafka began an intense relationship with Milena Jesenská, a Czech journalist and writer. His letters to her were later published as '. During a vacation in July 1923 to Graal-Müritz on the Baltic Sea, Kafka met Dora Diamant, a 25-year-old kindergarten teacher from an orthodox Jewish family. Kafka, hoping to escape the influence of his family to concentrate on his writing, moved briefly to Berlin and lived with Diamant. She became his lover and caused him to become interested in the Talmud. He worked on four stories, which he prepared to be published as ' (\"A Hunger Artist\").\n\nKafka feared that people would find him mentally and physically repulsive. However, those who met him found him to possess a quiet and cool demeanor, obvious intelligence, and a dry sense of humour; they also found him boyishly handsome, although of austere appearance. Brod compared Kafka to Heinrich von Kleist, noting that both writers had the ability to describe a situation realistically with precise details. Brod thought Kafka was one of the most entertaining people he had met; Kafka enjoyed sharing humour with his friends, but also helped them in difficult situations with good advice. According to Brod, he was a passionate reciter, who was able to phrase his speaking as if it were music. Brod felt that two of Kafka's most distinguishing traits were \"absolute truthfulness\" (') and \"precise conscientiousness\" ('). He explored details, the inconspicuous, in depth and with such love and precision that things surfaced that were unforeseen, seemingly strange, but absolutely true (\"\").\n\nAlthough Kafka showed little interest in exercise as a child, he later showed interest in games and physical activity, as a good rider, swimmer, and rower. On weekends he and his friends embarked on long hikes, often planned by Kafka himself. His other interests included alternative medicine, modern education systems such as Montessori, and technical novelties such as airplanes and film. Writing was important to Kafka; he considered it a \"form of prayer\". He was highly sensitive to noise and preferred quiet when writing.\n\nPérez-Álvarez has claimed that Kafka may have possessed a schizoid personality disorder. His style, it is claimed, not only in \"Die Verwandlung\" (\"The Metamorphosis\"), but in various other writings, appears to show low to medium-level schizoid traits, which explain much of his work. His anguish can be seen in this diary entry from 21 June 1913:\n\nand in Zürau Aphorism number 50:\n\nThough Kafka never married, he held marriage and children in high esteem. He had several girlfriends. He may have suffered from an eating disorder. Doctor Manfred M. Fichter of the Psychiatric Clinic, University of Munich, presented \"evidence for the hypothesis that the writer Franz Kafka had suffered from an atypical anorexia nervosa\", and that Kafka was not just lonely and depressed but also \"occasionally suicidal\". In his 1995 book \"Franz Kafka, the Jewish Patient\", Sander Gilman investigated \"why a Jew might have been considered 'hypochondriac' or 'homosexual' and how Kafka incorporates aspects of these ways of understanding the Jewish male into his own self-image and writing\". Kafka considered committing suicide at least once, in late 1912.\n\nPrior to World War I, Kafka attended several meetings of the Klub mladých, a Czech anarchist, anti-militarist, and anti-clerical organization. Hugo Bergmann, who attended the same elementary and high schools as Kafka, fell out with Kafka during their last academic year (1900–1901) because \"[Kafka's] socialism and my Zionism were much too strident\". \"Franz became a socialist, I became a Zionist in 1898. The synthesis of Zionism and socialism did not yet exist\". Bergmann claims that Kafka wore a red carnation to school to show his support for socialism. In one diary entry, Kafka made reference to the influential anarchist philosopher Peter Kropotkin: \"Don't forget Kropotkin!\"\n\nDuring the communist era, the legacy of Kafka's work for Eastern bloc socialism was hotly debated. Opinions ranged from the notion that he satirised the bureaucratic bungling of a crumbling Austria-Hungarian Empire, to the belief that he embodied the rise of socialism. A further key point was Marx's theory of alienation. While the orthodox position was that Kafka's depictions of alienation were no longer relevant for a society that had supposedly eliminated alienation, a 1963 conference held in Liblice, Czechoslovakia, on the eightieth anniversary of his birth, reassessed the importance of Kafka's portrayal of bureaucracy. Whether or not Kafka was a political writer is still an issue of debate.\n\nKafka grew up in Prague as a German-speaking Jew. He was deeply fascinated by the Jews of Eastern Europe, who he thought possessed an intensity of spiritual life that was absent from Jews in the West. His diary is full of references to Yiddish writers. Yet he was at times alienated from Judaism and Jewish life: \"What have I in common with Jews? I have hardly anything in common with myself and should stand very quietly in a corner, content that I can breathe\". In his adolescent years, Kafka had declared himself an atheist.\n\nHawes suggests that Kafka, though very aware of his own Jewishness, did not incorporate it into his work, which, according to Hawes, lacks Jewish characters, scenes or themes. In the opinion of literary critic Harold Bloom, although Kafka was uneasy with his Jewish heritage, he was the quintessential Jewish writer. Lothar Kahn is likewise unequivocal: \"The presence of Jewishness in Kafka's ' is no longer subject to doubt\". Pavel Eisner, one of Kafka's first translators, interprets ' (\"The Trial\") as the embodiment of the \"triple dimension of Jewish existence in Prague... his protagonist Josef K. is (symbolically) arrested by a German (Rabensteiner), a Czech (Kullich), and a Jew (Kaminer). He stands for the 'guiltless guilt' that imbues the Jew in the modern world, although there is no evidence that he himself is a Jew\".\n\nIn his essay \"Sadness in Palestine?!\", Dan Miron explores Kafka's connection to Zionism: \"It seems that those who claim that there was such a connection and that Zionism played a central role in his life and literary work, and those who deny the connection altogether or dismiss its importance, are both wrong. The truth lies in some very elusive place between these two simplistic poles\". Kafka considered moving to Palestine with Felice Bauer, and later with Dora Diamant. He studied Hebrew while living in Berlin, hiring a friend of Brod's from Palestine, Pua Bat-Tovim, to tutor him and attending Rabbi Julius Grünthal's and Rabbi Julius Guttmann's classes in the Berlin \"\" (College for the Study of Judaism).\n\nLivia Rothkirchen calls Kafka the \"symbolic figure of his era\". His contemporaries included numerous Jewish, Czech, and German writers who were sensitive to Jewish, Czech, and German culture. According to Rothkirchen, \"This situation lent their writings a broad cosmopolitan outlook and a quality of exaltation bordering on transcendental metaphysical contemplation. An illustrious example is Franz Kafka\".\n\nTowards the end of his life Kafka sent a postcard to his friend Hugo Bergman in Tel Aviv, announcing his intention to emigrate to Palestine. Bergman refused to host Kafka because he had young children and was afraid that Kafka would infect them with tuberculosis.\n\nKafka's laryngeal tuberculosis worsened and in March 1924 he returned from Berlin to Prague, where members of his family, principally his sister Ottla, took care of him. He went to Dr. Hoffmann's sanatorium in Kierling just outside Vienna for treatment on 10 April, and died there on 3 June 1924. The cause of death seemed to be starvation: the condition of Kafka's throat made eating too painful for him, and since parenteral nutrition had not yet been developed, there was no way to feed him. Kafka was editing \"A Hunger Artist\" on his deathbed, a story whose composition he had begun before his throat closed to the point that he could not take any nourishment. His body was brought back to Prague where he was buried on 11 June 1924, in the New Jewish Cemetery in Prague-Žižkov. Kafka was unknown during his own lifetime, but he did not consider fame important. He became famous soon after his death. The Kafka tombstone was designed by architect Leopold Ehrmann.\n\nAll of Kafka's published works, except some letters he wrote in Czech to Milena Jesenská, were written in German. What little was published during his lifetime attracted scant public attention.\n\nKafka finished none of his full-length novels and burned around 90 percent of his work, much of it during the period he lived in Berlin with Diamant, who helped him burn the drafts. In his early years as a writer, he was influenced by von Kleist, whose work he described in a letter to Bauer as frightening, and whom he considered closer than his own family.\n\nKafka's earliest published works were eight stories which appeared in 1908 in the first issue of the literary journal \"Hyperion\" under the title \"\" (\"Contemplation\"). He wrote the story \"\" (\"Description of a Struggle\") in 1904; he showed it to Brod in 1905 who advised him to continue writing and convinced him to submit it to \"Hyperion\". Kafka published a fragment in 1908 and two sections in the spring of 1909, all in Munich.\n\nIn a creative outburst on the night of 22 September 1912, Kafka wrote the story \"Das Urteil\" (\"The Judgment\", literally: \"The Verdict\") and dedicated it to Felice Bauer. Brod noted the similarity in names of the main character and his fictional fiancée, Georg Bendemann and Frieda Brandenfeld, to Franz Kafka and Felice Bauer. The story is often considered Kafka's breakthrough work. It deals with the troubled relationship of a son and his dominant father, facing a new situation after the son's engagement. Kafka later described writing it as \"a complete opening of body and soul\", a story that \"evolved as a true birth, covered with filth and slime\". The story was first published in Leipzig in 1912 and dedicated \"to Miss Felice Bauer\", and in subsequent editions \"for F.\"\n\nIn 1912, Kafka wrote \"Die Verwandlung\" (\"The Metamorphosis\", or \"The Transformation\"), published in 1915 in Leipzig. The story begins with a travelling salesman waking to find himself transformed into a ', a monstrous vermin, ' being a general term for unwanted and unclean animals. Critics regard the work as one of the seminal works of fiction of the 20th century. The story \"In der Strafkolonie\" (\"In the Penal Colony\"), dealing with an elaborate torture and execution device, was written in October 1914, revised in 1918, and published in Leipzig during October 1919. The story \"Ein Hungerkünstler\" (\"A Hunger Artist\"), published in the periodical \" in 1924, describes a victimized protagonist who experiences a decline in the appreciation of his strange craft of starving himself for extended periods. His last story, \"Josefine, die Sängerin oder Das Volk der Mäuse\" (\"Josephine the Singer, or the Mouse Folk\"), also deals with the relationship between an artist and his audience.\n\nHe began his first novel in 1912; its first chapter is the story \"Der Heizer\" (\"The Stoker\"). Kafka called the work, which remained unfinished, \" (\"The Man Who Disappeared\" or \"The Missing Man\"), but when Brod published it after Kafka's death he named it \"Amerika\". The inspiration for the novel was the time spent in the audience of Yiddish theatre the previous year, bringing him to a new awareness of his heritage, which led to the thought that an innate appreciation for one's heritage lives deep within each person. More explicitly humorous and slightly more realistic than most of Kafka's works, the novel shares the motif of an oppressive and intangible system putting the protagonist repeatedly in bizarre situations. It uses many details of experiences of his relatives who had emigrated to America and is the only work for which Kafka considered an optimistic ending.\n\nDuring 1914, Kafka began the novel \"\" (\"The Trial\"), the story of a man arrested and prosecuted by a remote, inaccessible authority, with the nature of his crime revealed neither to him nor to the reader. Kafka did not complete the novel, although he finished the final chapter. According to Nobel Prize winner and Kafka scholar Elias Canetti, Felice is central to the plot of \"Der Process\" and Kafka said it was \"her story\". Canetti titled his book on Kafka's letters to Felice \"Kafka's Other Trial\", in recognition of the relationship between the letters and the novel. Michiko Kakutani notes in a review for \"The New York Times\" that Kafka's letters have the \"earmarks of his fiction: the same nervous attention to minute particulars; the same paranoid awareness of shifting balances of power; the same atmosphere of emotional suffocation—combined, surprisingly enough, with moments of boyish ardor and delight.\"\n\nAccording to his diary, Kafka was already planning his novel ' (The Castle), by 11 June 1914; however, he did not begin writing it until 27 January 1922. The protagonist is the ' (land surveyor) named K., who struggles for unknown reasons to gain access to the mysterious authorities of a castle who govern the village. Kafka's intent was that the castle's authorities notify K. on his deathbed that his \"legal claim to live in the village was not valid, yet, taking certain auxiliary circumstances into account, he was to be permitted to live and work there\". Dark and at times surreal, the novel is focused on alienation, bureaucracy, the seemingly endless frustrations of man's attempts to stand against the system, and the futile and hopeless pursuit of an unobtainable goal. Hartmut M. Rastalsky noted in his thesis: \"Like dreams, his texts combine precise \"realistic\" detail with absurdity, careful observation and reasoning on the part of the protagonists with inexplicable obliviousness and carelessness.\"\n\nKafka's stories were initially published in literary periodicals. His first eight were printed in 1908 in the first issue of the bi-monthly \"Hyperion\". Franz Blei published two dialogues in 1909 which became part of \"Beschreibung eines Kampfes\" (\"Description of a Struggle\"). A fragment of the story \"Die Aeroplane in Brescia\" (\"The Aeroplanes at Brescia\"), written on a trip to Italy with Brod, appeared in the daily \"Bohemia\" on 28 September 1909. On 27 March 1910, several stories that later became part of the book ' were published in the Easter edition of \"Bohemia\". In Leipzig during 1913, Brod and publisher Kurt Wolff included \"\" (\"The Judgment. A Story by Franz Kafka.\") in their literary yearbook for the art poetry \"Arkadia\". The story \"\" (\"Before the Law\") was published in the 1915 New Year's edition of the independent Jewish weekly '; it was reprinted in 1919 as part of the story collection ' (\"A Country Doctor\") and became part of the novel '. Other stories were published in various publications, including Martin Buber's \"Der Jude\", the paper ', and the periodicals ', \"Genius\", and \"Prager Presse\".\n\nKafka's first published book, ' (\"Contemplation\", or \"Meditation\"), was a collection of 18stories written between 1904 and 1912. On a summer trip to Weimar, Brod initiated a meeting between Kafka and Kurt Wolff; Wolff published ' in the at the end of 1912 (with the year given as 1913). Kafka dedicated it to Brod, \"\", and added in the personal copy given to his friend \"\" (\"As it is already printed here, for my dearest Max\").\n\nKafka's story \"Die Verwandlung\" (\"The Metamorphosis\") was first printed in the October 1915 issue of ', a monthly edition of expressionist literature, edited by René Schickele. Another story collection, ' (\"A Country Doctor\"), was published by Kurt Wolff in 1919, dedicated to Kafka's father. Kafka prepared a final collection of four stories for print, ' \"(A Hunger Artist)\", which appeared in 1924 after his death, in '. On 20 April 1924, the \" published Kafka's essay on Adalbert Stifter.\n\nKafka left his work, both published and unpublished, to his friend and literary executor Max Brod with explicit instructions that it should be destroyed on Kafka's death; Kafka wrote: \"Dearest Max, my last request: Everything I leave behind me... in the way of diaries, manuscripts, letters (my own and others'), sketches, and so on, [is] to be burned unread\". Brod ignored this request and published the novels and collected works between 1925 and 1935. He took many papers, which remain unpublished, with him in suitcases to Palestine when he fled there in 1939. Kafka's last lover, Dora Diamant (later, Dymant-Lask), also ignored his wishes, secretly keeping 20notebooks and 35letters. These were confiscated by the Gestapo in 1933, but scholars continue to search for them.\n\nAs Brod published the bulk of the writings in his possession, Kafka's work began to attract wider attention and critical acclaim. Brod found it difficult to arrange Kafka's notebooks in chronological order. One problem was that Kafka often began writing in different parts of the book; sometimes in the middle, sometimes working backwards from the end. Brod finished many of Kafka's incomplete works for publication. For example, Kafka left ' with unnumbered and incomplete chapters and ' with incomplete sentences and ambiguous content; Brod rearranged chapters, copy edited the text, and changed the punctuation. ' appeared in 1925 in '. Kurt Wolff published two other novels, ' in 1926 and \"Amerika\" in 1927. In 1931, Brod edited a collection of prose and unpublished stories as ' \"(The Great Wall of China)\", including the story of the same name. The book appeared in the \". Brod's sets are usually called the \"Definitive Editions\".\n\nIn 1961, Malcolm Pasley acquired most of Kafka's original handwritten work for the Oxford Bodleian Library. The text for ' was later purchased through auction and is stored at the German Literary Archives in Marbach am Neckar, Germany. Subsequently, Pasley headed a team (including Gerhard Neumann, Jost Schillemeit and Jürgen Born) which reconstructed the German novels; republished them. Pasley was the editor for ', published in 1982, and \", published in 1990. Jost Schillemeit was the editor of \" published in 1983. These are called the \"Critical Editions\" or the \"Fischer Editions\".\n\nWhen Brod died in 1968, he left Kafka's unpublished papers, which are believed to number in the thousands, to his secretary Esther Hoffe. She released or sold some, but left most to her daughters, Eva and Ruth, who also refused to release the papers. A court battle began in 2008 between the sisters and the National Library of Israel, which claimed these works became the property of the nation of Israel when Brod emigrated to British Palestine in 1939. Esther Hoffe sold the original manuscript of \"\" for US$2 million in 1988 to the German Literary Archive Museum of Modern Literature in Marbach am Neckar. Only Eva was still alive as of 2012. A ruling by a Tel Aviv family court in 2010 held that the papers must be released and a few were, including a previously unknown story, but the legal battle continued. The Hoffes claim the papers are their personal property, while the National Library argues they are \"cultural assets belonging to the Jewish people\". The National Library also suggests that Brod bequeathed the papers to them in his will. The Tel Aviv Family Court ruled in October 2012 that the papers were the property of the National Library.\n\nThe poet W. H. Auden called Kafka \"the Dante of the twentieth century\"; the novelist Vladimir Nabokov placed him among the greatest writers of the 20th century. Gabriel García Márquez noted the reading of Kafka's \"The Metamorphosis\" showed him \"that it was possible to write in a different way\". A prominent theme of Kafka's work, first established in the short story \"Das Urteil\", is father–son conflict: the guilt induced in the son is resolved through suffering and atonement. Other prominent themes and archetypes include alienation, physical and psychological brutality, characters on a terrifying quest, and mystical transformation.\n\nKafka's style has been compared to that of Kleist as early as 1916, in a review of \"Die Verwandlung\" and \"Der Heizer\" by Oscar Walzel in \"Berliner Beiträge\". The nature of Kafka's prose allows for varied interpretations and critics have placed his writing into a variety of literary schools. Marxists, for example, have sharply disagreed over how to interpret Kafka's works. Some accused him of distorting reality whereas others claimed he was critiquing capitalism. The hopelessness and absurdity common to his works are seen as emblematic of existentialism. Some of Kafka's books are influenced by the expressionist movement, though the majority of his literary output was associated with the experimental modernist genre. Kafka also touches on the theme of human conflict with bureaucracy. William Burroughs claims that such work is centred on the concepts of struggle, pain, solitude, and the need for relationships. Others, such as Thomas Mann, see Kafka's work as allegorical: a quest, metaphysical in nature, for God.\n\nAccording to Gilles Deleuze and Félix Guattari, the themes of alienation and persecution, although present in Kafka's work, have been over-emphasised by critics. They argue Kafka's work is more deliberate and subversive—and more joyful—than may first appear. They point out that reading his work while focusing on the futility of his characters' struggles reveals Kafka's play of humour; he is not necessarily commenting on his own problems, but rather pointing out how people tend to invent problems. In his work, Kafka often created malevolent, absurd worlds. Kafka read drafts of his works to his friends, typically concentrating on his humorous prose. The writer Milan Kundera suggests that Kafka's surrealist humour may have been an inversion of Dostoyevsky's presentation of characters who are punished for a crime. In Kafka's work a character is punished although a crime has not been committed. Kundera believes that Kafka's inspirations for his characteristic situations came both from growing up in a patriarchal family and living in a totalitarian state.\n\nAttempts have been made to identify the influence of Kafka's legal background and the role of law in his fiction. Most interpretations identify aspects of law and legality as important in his work, in which the legal system is often oppressive. The law in Kafka's works, rather than being representative of any particular legal or political entity, is usually interpreted to represent a collection of anonymous, incomprehensible forces. These are hidden from the individual but control the lives of the people, who are innocent victims of systems beyond their control. Critics who support this absurdist interpretation cite instances where Kafka describes himself in conflict with an absurd universe, such as the following entry from his diary:\n\nHowever, James Hawes argues many of Kafka's descriptions of the legal proceedings in \"—metaphysical, absurd, bewildering and nightmarish as they might appear—are based on accurate and informed descriptions of German and Austrian criminal proceedings of the time, which were inquisitorial rather than adversarial. Although he worked in insurance, as a trained lawyer Kafka was \"keenly aware of the legal debates of his day\". In an early 21st-century publication that uses Kafka's office writings as its point of departure, Pothik Ghosh states that with Kafka, law \"has no meaning outside its fact of being a pure force of domination and determination\".\n\nThe earliest English translations of Kafka's works were by Edwin and Willa Muir, who in 1930 translated the first German edition of \". This was published as \"The Castle\" by Secker & Warburg in England and Alfred A. Knopf in the United States. A 1941 edition, including a homage by Thomas Mann, spurred a surge in Kafka's popularity in the United States during the late 1940s. The Muirs translated all shorter works that Kafka had seen fit to print; they were published by Schocken Books in 1948 as \"\", including additionally \"The First Long Train Journey\", written by Kafka and Brod, Kafka's \"A Novel about Youth\", a review of Felix Sternheim's \"Die Geschichte des jungen Oswald\", his essay on Kleist's \"Anecdotes\", his review of the literary magazine \"Hyperion\", and an epilogue by Brod.\n\nLater editions, notably those of 1954 (\"Dearest Father. Stories and Other Writings\"), included text, translated by Eithne Wilkins and Ernst Kaiser, which had been deleted by earlier publishers. Known as \"Definitive Editions\", they include translations of \"The Trial, Definitive\", \"The Castle, Definitive\", and other writings. These translations are generally accepted to have a number of biases and are considered to be dated in interpretation. Published in 1961 by Schocken Books, \"Parables and Paradoxes\" presented in a bilingual edition by Nahum N. Glatzer selected writings, drawn from notebooks, diaries, letters, short fictional works and the novel \"Der Process\".\n\nNew translations were completed and published based on the recompiled German text of Pasley and Schillemeit\"The Castle, Critical\" by Mark Harman (Schocken Books, 1998), \"The Trial, Critical\" by Breon Mitchell (Schocken Books, 1998), and \"Amerika: The Man Who Disappeared\" by Michael Hofmann (New Directions Publishing, 2004).\n\nKafka often made extensive use of a characteristic particular to the German language which permits long sentences that sometimes can span an entire page. Kafka's sentences then deliver an unexpected impact just before the full stop—this being the finalizing meaning and focus. This is due to the construction of subordinate clauses in German which require that the verb be positioned at the end of the sentence. Such constructions are difficult to duplicate in English, so it is up to the translator to provide the reader with the same (or at least equivalent) effect found in the original text. German's more flexible word order and syntactical differences provide for multiple ways in which the same German writing can be translated into English. An example is the first sentence of Kafka's \"The Metamorphosis\", which is crucial to the setting and understanding of the entire story:\n\nAnother difficult problem facing translators is how to deal with the author's intentional use of ambiguous idioms and words that have several meanings which results in phrasing that is difficult to translate precisely. One such instance is found in the first sentence of \"The Metamorphosis\". English translators often render the word ' as \"insect\"; in Middle German, however, ' literally means \"an animal unclean for sacrifice\"; in today's German it means vermin. It is sometimes used colloquially to mean \"bug\" —a very general term, unlike the scientific \"insect\". Kafka had no intention of labeling Gregor, the protagonist of the story, as any specific thing, but instead wanted to convey Gregor's disgust at his transformation. Another example is Kafka's use of the German noun ' in the final sentence of \"Das Urteil\". Literally, ' means intercourse and, as in English, can have either a sexual or non-sexual meaning; in addition, it is used to mean transport or traffic. The sentence can be translated as: \"At that moment an unending stream of traffic crossed over the bridge\". The double meaning of \"Verkehr\" is given added weight by Kafka's confession to Brod that when he wrote that final line, he was thinking of \"a violent ejaculation\".\n\nUnlike many famous writers, Kafka is rarely quoted by others. Instead, he is noted more for his visions and perspective. Shimon Sandbank, a professor, literary critic, and writer, identifies Kafka as having influenced Jorge Luis Borges, Albert Camus, Eugène Ionesco, J. M. Coetzee and Jean-Paul Sartre. A \"Financial Times\" literary critic credits Kafka with influencing José Saramago, and Al Silverman, a writer and editor, states that J. D. Salinger loved to read Kafka's works. In 1999 a committee of 99 authors, scholars, and literary critics ranked ' and ' the second and ninth most significant German-language novels of the 20th century. Sandbank argues that despite Kafka's pervasiveness, his enigmatic style has yet to be emulated. Neil Christian Pages, a professor of German Studies and Comparative Literature at Binghamton University who specialises in Kafka's works, says Kafka's influence transcends literature and literary scholarship; it impacts visual arts, music, and popular culture. Harry Steinhauer, a professor of German and Jewish literature, says that Kafka \"has made a more powerful impact on literate society than any other writer of the twentieth century\". Brod said that the 20th century will one day be known as the \"century of Kafka\".\n\nMichel-André Bossy writes that Kafka created a rigidly inflexible and sterile bureaucratic universe. Kafka wrote in an aloof manner full of legal and scientific terms. Yet his serious universe also had insightful humour, all highlighting the \"irrationality at the roots of a supposedly rational world\". His characters are trapped, confused, full of guilt, frustrated, and lacking understanding of their surreal world. Much of the post-Kafka fiction, especially science fiction, follow the themes and precepts of Kafka's universe. This can be seen in the works of authors such as George Orwell and Ray Bradbury.\n\nThe following are examples of works across a range of literary, musical, and dramatic genres which demonstrate the extent of cultural influence:\n\nThe term \"Kafkaesque\" is used to describe concepts and situations reminiscent of his work, particularly \"\" (\"The Trial\") and \"Die Verwandlung\" (\"The Metamorphosis\"). Examples include instances in which bureaucracies overpower people, often in a surreal, nightmarish milieu which evokes feelings of senselessness, disorientation, and helplessness. Characters in a Kafkaesque setting often lack a clear course of action to escape a labyrinthine situation. Kafkaesque elements often appear in existential works, but the term has transcended the literary realm to apply to real-life occurrences and situations that are incomprehensibly complex, bizarre, or illogical.\nNumerous films and television works have been described as Kafkaesque, and the style is particularly prominent in dystopian science fiction. Works in this genre that have been thus described include Patrick Bokanowski's 1982 film \"The Angel\", Terry Gilliam's 1985 film \"Brazil\", and the 1998 science fiction film noir, \"Dark City\". Films from other genres which have been similarly described include \"The Tenant\" (1976) and \"Barton Fink\" (1991). The television series \"The Prisoner\" and \"The Twilight Zone\" are also frequently described as Kafkaesque.\n\nHowever, with common usage, the term has become so ubiquitous that Kafka scholars note it's often misused. More accurately then, according to author Ben Marcus, paraphrased in \"What it Means to be Kafkaesque\" by Joe Fassler in \"The Atlantic\", \"Kafka’s quintessential qualities are affecting use of language, a setting that straddles fantasy and reality, and a sense of striving even in the face of bleakness—hopelessly and full of hope.\" \n\nThe Franz Kafka Museum in Prague is dedicated to Kafka and his work. A major component of the museum is an exhibit \"The City of K. Franz Kafka and Prague\", which was first shown in Barcelona in 1999, moved to the Jewish Museum in New York City, and was finally established in 2005 in Prague in Malá Strana (Lesser Town), along the Moldau. The museum calls its display of original photos and documents \"Město K. Franz Kafka a Praha\" (City K. Kafka and Prague) and aims to immerse the visitor into the world in which Kafka lived and about which he wrote.\n\nThe Franz Kafka Prize is an annual literary award of the Franz Kafka Society and the City of Prague established in 2001. It recognizes the merits of literature as \"humanistic character and contribution to cultural, national, language and religious tolerance, its existential, timeless character, its generally human validity, and its ability to hand over a testimony about our times\". The selection committee and recipients come from all over the world, but are limited to living authors who have had at least one work published in the Czech language. The recipient receives $10,000, a diploma, and a bronze statuette at a presentation in Prague's Old Town Hall on the Czech State Holiday in late October.\n\nSan Diego State University (SDSU) operates the \"Kafka Project\", which began in 1998 as the official international search for Kafka's last writings.\n\n\n\n\"Journals\"\n\n\n\"Newspapers\"\n\n\"Online sources\"\n\n\n\"Journals\"\n\n"}
{"id": "10859", "url": "https://en.wikipedia.org/wiki?curid=10859", "title": "Fields Medal", "text": "Fields Medal\n\nThe Fields Medal is a prize awarded to two, three, or four mathematicians under 40 years of age at the International Congress of the International Mathematical Union (IMU), a meeting that takes place every four years.\n\nThe Fields Medal is, with the Abel Prize, viewed as the highest honor a mathematician can receive. The Fields Medal and the Abel Prize have often been described as the mathematician's \"Nobel Prize\". \n\nThe Fields Medal differs from the Abel in view of the age restriction mentioned above, and in its frequency (awarded once every four years).\n\nThe prize comes with a monetary award, which since 2006 has been $15,000 CAD. The colloquial name is in honour of Canadian mathematician John Charles Fields. Fields was instrumental in establishing the award, designing the medal itself, and funding the monetary component.\n\nThe medal was first awarded in 1936 to Finnish mathematician Lars Ahlfors and American mathematician Jesse Douglas, and it has been awarded every four years since 1950. Its purpose is to give recognition and support to younger mathematical researchers who have made major contributions. In 2014, Maryam Mirzakhani became the first Iranian and first woman to win the Fields Medal.\n\nThe Fields Medal is often described as the \"Nobel Prize of Mathematics\" and for a long time was regarded as the most prestigious award in the field of mathematics. However, in contrast to the Nobel Prize, the Fields Medal is only awarded every four years. The Fields Medal also has an age limit: a recipient must be under age 40 on 1 January of the year in which the medal is awarded. This is similar to restrictions applicable to the Clark Medal in economics. The under-40 rule is based on Fields' desire that \"while it was in recognition of work already done, it was at the same time intended to be an encouragement for further achievement on the part of the recipients and a stimulus to renewed effort on the part of others.\"\n\nThe monetary award is much lower than the 8,000,000 Swedish kronor (roughly 1,400,000 Canadian dollars) given with each Nobel prize as of 2014. Other major awards in mathematics, such as the Abel Prize and the Chern Medal, have larger monetary prizes compared to the Fields Medal.\n\nIn 1954, Jean-Pierre Serre became the youngest winner of the Fields Medal, at 27. He still retains this distinction.\n\nIn 1966, Alexander Grothendieck boycotted the ICM, held in Moscow, to protest Soviet military actions taking place in Eastern Europe. Léon Motchane, founder and director of the Institut des Hautes Études Scientifiques attended and accepted Grothendieck's Fields Medal on his behalf.\n\nIn 1970, Sergei Novikov, because of restrictions placed on him by the Soviet government, was unable to travel to the congress in Nice to receive his medal.\n\nIn 1978, Grigory Margulis, because of restrictions placed on him by the Soviet government, was unable to travel to the congress in Helsinki to receive his medal. The award was accepted on his behalf by Jacques Tits, who said in his address: \"I cannot but express my deep disappointment—no doubt shared by many people here—in the absence of Margulis from this ceremony. In view of the symbolic meaning of this city of Helsinki, I had indeed grounds to hope that I would have a chance at last to meet a mathematician whom I know only through his work and for whom I have the greatest respect and admiration.\"\n\nIn 1982, the congress was due to be held in Warsaw but had to be rescheduled to the next year, because of martial law introduced in Poland on 13 December 1981. The awards were announced at the ninth General Assembly of the IMU earlier in the year and awarded at the 1983 Warsaw congress.\n\nIn 1990, Edward Witten became the first physicist to win this award.\n\nIn 1998, at the ICM, Andrew Wiles was presented by the chair of the Fields Medal Committee, Yuri I. Manin, with the first-ever IMU silver plaque in recognition of his proof of Fermat's Last Theorem. Don Zagier referred to the plaque as a \"quantized Fields Medal\". Accounts of this award frequently make reference that at the time of the award Wiles was over the age limit for the Fields medal. Although Wiles was slightly over the age limit in 1994, he was thought to be a favorite to win the medal; however, a gap (later resolved by Taylor and Wiles) in the proof was found in 1993.\n\nIn 2006, Grigori Perelman, who proved the Poincaré conjecture, refused his Fields Medal and did not attend the congress.\n\nIn 2014, Maryam Mirzakhani became the first woman as well as the first Iranian, Artur Avila the first South American and Manjul Bhargava the first person of Indian origins to win the Fields Medal.\n\nThis is a list of the universities that have graduated Fields medalists. It only includes those institutions that have graduated two or more medalists. See List of Fields Medal winners by university affiliation for complete affiliation.\n\nThis is a list of the universities that Fields medalists have been affiliated to at the time the prize was awarded. It only includes those institutions that have had two or more medalist affiliates. See List of Fields Medal winners by university affiliation for complete affiliation.\n\n\nThe medal was designed by Canadian sculptor R. Tait McKenzie.\n\n\nTranslation: \"Mathematicians gathered from the entire world have awarded [understood but not written: 'this prize'] for outstanding writings.\"\n\nIn the background, there is the representation of Archimedes' tomb, with the carving illustrating his theorem On the Sphere and Cylinder, behind a branch. (This is the mathematical result of which Archimedes was reportedly most proud: Given a sphere and a circumscribed cylinder of the same height and diameter, the ratio between their volumes is equal to ⅔.)\n\nThe rim bears the name of the prizewinner.\n\nIn terms of the most prestigious awards in STEM fields, only a small proportion have been awarded to women. The Fields Medal was only obtained for the first time by a woman, Maryam Mirzakhani from Iran, in 2014 out of a total of 56 medallists.\n\n\n"}
{"id": "10861", "url": "https://en.wikipedia.org/wiki?curid=10861", "title": "The Trial", "text": "The Trial\n\nThe Trial (original German title: , later , and ) is a novel written by Franz Kafka from 1914 to 1915 and published in 1925. One of his best-known works, it tells the story of a man arrested and prosecuted by a remote, inaccessible authority, with the nature of his crime revealed neither to him nor to the reader. Heavily influenced by Dostoyevsky's \"Crime and Punishment\" and \"The Brothers Karamazov\", Kafka even went so far as to call Dostoyevsky a blood relative. Like Kafka's other novels, \"The Trial\" was never completed, although it does include a chapter which brings the story to an end.\n\nAfter Kafka's death in 1924 his friend and literary executor Max Brod edited the text for publication by Verlag Die Schmiede. The original manuscript is held at the Museum of Modern Literature, Marbach am Neckar, Germany. The first English language translation, by Willa and Edwin Muir, was published in 1937. In 1999, the book was listed in \"Le Monde\"'s 100 Books of the Century and as No. 2 of the Best German Novels of the Twentieth Century.\n\nOn his thirtieth birthday, the chief cashier of a bank, Josef K., is unexpectedly arrested by two unidentified agents from an unspecified agency for an unspecified crime. The agents' boss later arrives and holds a mini-tribunal in the room of K.'s neighbor, Fräulein Bürstner. K. is not taken away, however, but left \"free\" and told to await instructions from the Committee of Affairs. He goes to work, and that night apologizes to Fräulein Bürstner for the intrusion into her room. At the end of the conversation he suddenly kisses her.\n\nK. receives a phone call summoning him to court, and the coming Sunday is arranged as the date. No time is set, but the address is given to him. The address turns out to be a huge tenement building. K. has to explore to find the court, which turns out to be in the attic. The room is airless, shabby and crowded, and although he has no idea what he is charged with, or what authorizes the process, K. makes a long speech denigrating the whole process, including the agents who arrested him; during this speech an attendant's wife and a man engage in sexual activities. K. then returns home.\n\nK. later goes to visit the court again, although he has not been summoned, and finds that it is not in session. He instead talks with the attendant's wife, who attempts to seduce him into taking her away, and who gives him more information about the process and offers to help him. K. later goes with the attendant to a higher level of the attic where the shabby and airless offices of the court are housed.\n\nK. returns home to find Fräulein Montag, a lodger from another room, moving in with Fräulein Bürstner. He suspects that this is to prevent him from pursuing his affair with the latter woman. Yet another lodger, Captain Lanz, appears to be in league with Montag.\n\nLater, in a store room at his own bank, K. discovers the two agents who arrested him being whipped by a flogger for asking K. for bribes and as a result of complaints K. made at court. K. tries to argue with the flogger, saying that the men need not be whipped, but the flogger cannot be swayed. The next day he returns to the store room and is shocked to find everything as he had found it the day before, including the whipper and the two agents.\n\nK. is visited by his uncle, who was K.'s guardian. The uncle seems distressed by K.'s predicament. At first sympathetic, he becomes concerned that K. is underestimating the seriousness of the case. The uncle introduces K. to a lawyer, who is attended by Leni, a nurse, whom K.'s uncle suspects is the advocate's mistress. During the discussion it becomes clear how different this process is from regular legal proceedings: guilt is assumed, the bureaucracy running it is vast with many levels, and everything is secret, from the charge, to the rules of the court, to the authority behind the courts – even the identity of the judges at the higher levels. The attorney tells him that he can prepare a brief for K., but since the charge is unknown and the rules are unknown, it is difficult work. It also never may be read, but is still very important. The lawyer says that his most important task is to deal with powerful court officials behind the scenes. As they talk, the lawyer reveals that the Chief Clerk of the Court has been sitting hidden in the darkness of a corner. The Chief Clerk emerges to join the conversation, but K. is called away by Leni, who takes him to the next room, where she offers to help him and seduces him. They have a sexual encounter. Afterwards K. meets his uncle outside, who is angry, claiming that K.'s lack of respect has hurt K.'s case.\n\nK. visits the lawyer several times. The lawyer tells him incessantly how dire his situation is and tells many stories of other hopeless clients and of his behind-the-scenes efforts on behalf of these clients, and brags about his many connections. The brief is never complete. K.'s work at the bank deteriorates as he is consumed with worry about his case.\n\nK. is surprised by one of his bank clients, who tells K. that he is aware that K. is dealing with a trial. The client learned of K.'s case from Titorelli, a painter, who has dealings with the court and told the client about K.'s case. The client advises K. to go to Titorelli for advice. Titorelli lives in the attic of a tenement in a suburb on the opposite side of town from the court that K. visited. Three teenage girls taunt K. on the steps and tease him sexually. Titorelli turns out to be an official painter of portraits for the court (an inherited position), and has a deep understanding of the process. K. learns that, to Titorelli's knowledge, not a single defendant has ever been acquitted. He sets out K.'s options and offers to help K. with either. The options are: obtain a provisional verdict of innocence from the lower court, which can be overturned at any time by higher levels of the court, which would lead to re-initiation of the process; or curry favor with the lower judges to keep the process moving at a glacial pace. Titorelli has K. leave through a small back door, as the girls are blocking the door through which K. entered. To K.'s shock, the door opens into another warren of the court's offices – again shabby and airless.\n\nK. decides to take control of matters himself and visits his lawyer with the intention of dismissing him. At the lawyer's office he meets a downtrodden individual, Block, a client who offers K. some insight from a client's perspective. Block's case has continued for five years and he has gone from being a successful businessman to being almost bankrupt and is virtually enslaved by his dependence on the lawyer and Leni, with whom he appears to be sexually involved. The lawyer mocks Block in front of K. for his dog-like subservience. This experience further poisons K.'s opinion of his lawyer. (This chapter was left unfinished by the author.)\n\nK. is asked by the bank to show an Italian client around local places of cultural interest, but the Italian client, short of time, asks K. to take him only to the cathedral, setting a time to meet there. When the client does not show up, K. explores the cathedral, which is empty except for an old woman and a church official. K. notices a priest who seems to be preparing to give a sermon from a small second pulpit, and K. begins to leave, lest it begin and K. be compelled to stay for its entirety. Instead of giving a sermon, the priest calls out K.'s name. K. approaches the pulpit and the priest berates him for his attitude toward the trial and for seeking help, especially from women. K. asks him to come down and the two men walk inside the cathedral. The priest works for the court as a chaplain and tells K. a fable (which was published earlier as \"Before the Law\") that is meant to explain his situation. K. and the priest discuss the parable. The priest tells K. that the parable is an ancient text of the court, and many generations of court officials have interpreted it differently.\n\nOn the eve of K.'s thirty-first birthday, two men arrive at his apartment. He has been waiting for them, and he offers little resistance – indeed the two men take direction from K. as they walk through town. K. leads them to a quarry where the two men place K's head on a discarded block. One of the men produces a double-edged butcher knife, and as the two men pass it back and forth between them, the narrator tells us that \"K. knew then precisely, that it would have been his duty to take the knife... and thrust it into himself.\" He does not take the knife. One of the men holds his shoulder and pulls him up and the other man stabs him in the heart and twists the knife twice. K.'s last words are: \"Like a dog!\".\n\nJosef K. – The tale's protagonist.\n\nFräulein Bürstner – A boarder in the same house as Josef K. She lets him kiss her one night, but then rebuffs his advances. K. briefly catches sight of her, or someone who looks similar to her, in the final pages of the novel.\n\nFräulein Montag – Friend of Fräulein Bürstner, she talks to K. about ending his relationship with Fräulein Bürstner after his arrest. She claims she can bring him insight, because she is an objective third party.\n\nWillem and Franz – Officers who arrest K. one morning but refuse to disclose the crime he is said to have committed.\n\nInspector – Man who conducts a proceeding at Josef K.'s boardinghouse to inform K. officially that he is under arrest.\n\nRabinsteiner, Kullich and Kaminer – Junior bank employees who attend the proceeding at the boardinghouse.\n\nFrau Grubach – The proprietress of the lodging house in which K. lives. She holds K. in high esteem, despite his arrest.\n\nWoman in the Court – In her house happens the first judgment of K. She claims help from K. because she doesn't want to be abused by the magistrates.\n\nStudent – Deformed man who acts under orders of the instruction judge. Will be a powerful man in the future.\n\nInstruction Judge – First Judge of K. In his trial, he confuses K. with a Wall Painter.\n\nUncle Karl – K.'s impetuous uncle from the country, formerly his guardian. Upon learning about the trial, Karl insists that K. hire Herr Huld, the lawyer.\n\nHerr Huld, the Lawyer – K.'s pompous and pretentious advocate who provides precious little in the way of action and far too much in the way of anecdote.\n\nLeni – Herr Huld's nurse, she has feelings for Josef K. and soon becomes his lover. She shows him her webbed hand, yet another reference to the motif of the hand throughout the book. Apparently, she finds accused men extremely attractive—the fact of their indictment makes them irresistible to her.\n\nAlbert – Office director at the court and a friend of Huld.\n\nFlogger – Man who punishes Franz and Willem in the Bank after K's. complaints against the two agents in his first Judgement.\n\nVice-President – K.'s unctuous rival at the Bank, only too willing to catch K. in a compromising situation. He repeatedly takes advantage of K.'s preoccupation with the trial to advance his own ambitions.\n\nPresident – Manager of the Bank. A sickly figure, whose position the Vice-President is trying to assume. Gets on well with K., inviting him to various engagements.\n\nRudi Block, the Merchant – Block is another accused man and client of Huld. His case is five years old, and he is but a shadow of the prosperous grain dealer he once was. All his time, energy, and resources are now devoted to his case, to the point of detriment to his own life. Although he has hired five additional lawyers on the side, he is completely and pathetically subservient to Huld.\n\nManufacturer – Person who hears about K.'s case and advises him to see a painter who knows how the court system works.\n\nTitorelli, the Painter – Titorelli inherited the position of Court Painter from his father. He knows a great deal about the comings and goings of the Court's lowest level. He offers to help K., and manages to unload a few identical landscape paintings on the accused man.\n\nPriest – Prison chaplain whom K. encounters in a church. The priest advises K. that his case is going badly and tells him to accept his fate.\n\nDoorkeeper and Farmer – The characters of the Chaplain's Tale.\n\n\n\n\n\n\nNotes\nBibliography\n\n"}
{"id": "10862", "url": "https://en.wikipedia.org/wiki?curid=10862", "title": "The Metamorphosis", "text": "The Metamorphosis\n\nThe Metamorphosis () is a novella written by Franz Kafka which was first published in 1915.\n\nOne day, Gregor Samsa, a traveling salesman, wakes up to find himself transformed into a giant insect (the most common translation of the German description \"ungeheuer Ungeziefer\", literally \"monstrous vermin\"). He reflects on how dreary life as a traveling salesman is. As he looks at the wall clock, he notices that he has overslept and missed his train for work. He ponders the consequences of this delay. Gregor becomes annoyed at how his boss never accepts excuses or explanations from any of his employees no matter how hard-working they are, displaying an apparent lack of trusting abilities. Gregor's mother knocks on the door, and he answers her. She is concerned for Gregor because he is late for work, which is unorthodox for him. Gregor answers his mother and realizes that his voice has changed, but his answer is short, so his mother does not notice. His sister, Grete, to whom he is very close, then whispers through the door and begs him to open it. He tries to get out of bed but is incapable of moving his body. While trying to move, he finds that his office manager, the chief clerk, has shown up to check on him. He finally rocks his body to the floor and calls out that he will open the door shortly.\n\nOffended by Gregor's delayed response in opening the door, the clerk warns him of the consequences of missing work. He adds that Gregor's recent performance has been unsatisfactory. Gregor disagrees and tells him that he will open the door shortly. Nobody on the other side of the door has understood a single word he had uttered as Gregor's voice has also transformed, and they conclude that he is seriously ill. Finally, Gregor manages to unlock and open the door with his mouth. He apologizes to the office manager for the delay. Horrified by Gregor's appearance, his mother faints, and the manager bolts out of the apartment. Gregor tries to catch up with him, but his father drives him back into the bedroom with a shoe and a rolled magazine. Gregor injures himself squeezing back through the doorway, and his father slams the door shut. Gregor, exhausted, falls asleep.\n\nGregor awakens and sees that someone has put milk and bread in his room. Initially excited, he quickly discovers that he has no taste for milk, once one of his favorites. He settles himself under a couch. The next morning, his sister comes in, sees that he has not touched the milk, and replaces it with rotting food scraps, which Gregor happily eats. This begins a routine in which his sister feeds him and cleans up while he hides under the couch, afraid that his appearance will frighten her. Gregor spends his time listening through the wall to his family members talking. They often discuss the difficult financial situation they find themselves in now and that Gregor can't provide them any help. Gregor had plans of sending Grete to the conservatory to pursue violin lessons, something everyone else – including Grete – considered a dream. His incapability of providing for his family, coupled with his speechlessness, reduces his thought process greatly. Gregor also learns that his mother wants to visit him, but his sister and father will not let her.\n\nGregor grows more comfortable with his changed body. He begins climbing the walls and ceiling for amusement. Discovering Gregor's new pastime, Grete decides to remove some of the furniture to give Gregor more space. She and her mother begin taking furniture away, but Gregor finds their actions deeply distressing. He tries to save a picture on the wall of a woman wearing a fur hat, fur scarf, and fur muff. Gregor's mother sees him hanging on the wall and passes out. Grete angrily calls out to Gregor – the first time anyone has spoken directly to him since his transformation. Gregor runs out of the room and into the kitchen. He encounters his father, who has just returned home from work. The father throws apples at Gregor, and one of them sinks into a sensitive spot in his back and remains lodged there, paralyzing his movements for a month and damaging him permanently. Gregor manages to get back into his bedroom but is severely injured.\n\nOne evening, the cleaning lady leaves Gregor's door open while three boarders, whom the family has taken on for additional income, lounge about the living room. Grete has been asked to play the violin for them, and Gregor – who usually takes care to avoid crossing paths with anyone in the flat – creeps out of his bedroom to listen in the midst of his depression and resultant detachment. The boarders, who initially seemed interested in Grete, grow bored with her performance, but Gregor is transfixed by it. One of the boarders spots Gregor, and the rest become alarmed. Gregor's father tries to shove the boarders back into their rooms, but the three men protest and announce that they will move out immediately without paying rent because of the disgusting conditions in the apartment.\n\nGrete, who has by now become tired of taking care of Gregor and is realizing the burden his existence puts on each one in the family, tells her parents they must get rid of Gregor, or they will all be ruined. Her father agrees, wishing Gregor could understand them and would leave of his own accord. Gregor does, in fact, understand and slowly moves back to the bedroom. There, determined to rid his family of his presence, Gregor dies.\n\nUpon discovering Gregor is dead, the family feels a great sense of relief. The father kicks out the boarders and decides to fire the cleaning lady, who has disposed of Gregor's body. The family takes a trolley ride out to the countryside, during which they consider their finances. They decide to move to a smaller apartment to further save money, an act they were unable to carry out in Gregor's presence. During this short trip, Mr. and Mrs. Samsa realize that, in spite of going through hardships which have brought an amount of paleness to her face, Grete appears to have grown up into a pretty and well-figured lady, which leads her parents to think about finding her a husband.\n\nGregor is the main character of the story. He works as a traveling salesman in order to provide money for his sister and parents. He wakes up one morning finding himself transformed into an insect. After the metamorphosis, Gregor becomes unable to work and is confined to his room for the remainder of the story. This prompts his family to begin working once again. Gregor is depicted as isolated from society and often misunderstands the true intentions of others.\n\nThe name \"Gregor Samsa\" appears to derive partly from literary works Kafka had read. A character in \"The Story of Young Renate Fuchs\", by German-Jewish novelist Jakob Wassermann (1873–1934), is named Gregor Samassa. The Viennese author Leopold von Sacher-Masoch, whose sexual imagination gave rise to the idea of masochism, is also an influence. Sacher-Masoch wrote \"Venus in Furs\" (1870), a novel whose hero assumes the name Gregor at one point. A \"Venus in furs\" literally recurs in \"The Metamorphosis\" in the picture that Gregor Samsa has hung on his bedroom wall. \n\nGrete is Gregor's younger sister, who becomes his caretaker after his metamorphosis. Initially Grete and Gregor have a close relationship, but this quickly fades. While Grete initially volunteers to feed him and clean his room, she grows increasingly impatient with the burden and begins to leave his room in disarray out of spite. Her initial decision to take care of Gregor may have come from a desire to contribute and be useful to the family, since she becomes angry and upset when the mother cleans his room, and it is made clear that Grete is disgusted by Gregor; she could not enter Gregor's room without opening the window first because of the nausea he caused her, and leaves without doing anything if Gregor is in plain sight. She plays the violin and dreams of going to the conservatory, a dream Gregor had intended to make happen; Gregor had planned on making the announcement on Christmas Day. To help provide an income for the family after Gregor's transformation, she starts working as a salesgirl. Grete is also the first to suggest getting rid of Gregor. At the end of the story, Grete's parents realize that she has become beautiful and full-figured and decide to consider finding her a husband.\n\nMr. Samsa is Gregor's father. After the metamorphosis, he is forced to return to work in order to support the family financially. His attitude towards his son is harsh; he regards the transformed Gregor with disgust and possibly even fear, and he attacks him on multiple occasions.\n\nMrs. Samsa is Grete and Gregor's mother. She is initially shocked at Gregor's transformation; however, she wants to enter his room. This proves too much for her, thus giving rise to a conflict between her maternal impulse and sympathy, and her fear and revulsion at Gregor's new form.\n\nKafka's sentences often deliver an unexpected impact just before the period – that being the finalizing meaning and focus. This is achieved from the construction of sentences in the original German, where the verbs of subordinate clauses are put at the end. For example, in the opening sentence, it is the final word, \"verwandelt\", that indicates transformation:\n\nThese constructions are not directly replicable in English, so it is up to the translator to provide the reader with the effect of the original text.\n\nEnglish translators have often sought to render the word \"Ungeziefer\" as \"insect\", but this is not strictly accurate. In Middle High German, \"Ungeziefer\" literally means \"unclean animal not suitable for sacrifice\" and is sometimes used colloquially to mean \"bug\" – a very general term, unlike the scientific sounding \"insect\". Kafka had no intention of labeling Gregor as any specific thing, but instead wanted to convey Gregor's disgust at his transformation. The phrasing used by Joachim Neugroschel is: \"Gregor Samsa found himself, in his bed, transformed into a monstrous vermin\", whereas David Wyllie says\" \"transformed in his bed into a horrible vermin\".\n\nHowever, in Kafka's letter to his publisher of 25 October 1915, in which he discusses his concern about the cover illustration for the first edition, he uses the term \"Insekt\", saying: \"The insect itself is not to be drawn. It is not even to be seen from a distance.\"\n\n\"Ungeziefer\" has sometimes been translated as \"cockroach\", \"dung beetle\", \"beetle\", and other highly specific terms. The term \"dung beetle\" or \"Mistkäfer\" is, in fact, used by the cleaning lady near the end of the story, but it is not used in the narration. \"Ungeziefer\" also denotes a sense of separation between himself and his environment: he is unclean and must therefore be secluded.\n\nVladimir Nabokov, who was a lepidopterist as well as a writer and literary critic, insisted that Gregor was not a cockroach, but a beetle with wings under his shell, and capable of flight. Nabokov left a sketch annotated, \"just over three feet long\", on the opening page of his (heavily corrected) English teaching copy. In his accompanying lecture notes, Nabokov discusses the type of insect Gregor has been transformed into, concluding that Gregor \"is not, technically, a dung beetle. He is merely a big beetle\".\n\n\n\nThere are numerous film versions of the story, including:\n\n\n\n\n\n\nOnline editions\n, translated by David Wyllie\n\nCommentary\n\nRelated\n"}
{"id": "10865", "url": "https://en.wikipedia.org/wiki?curid=10865", "title": "FSF", "text": "FSF\n\nFSF may refer to:\n\n\n"}
{"id": "10868", "url": "https://en.wikipedia.org/wiki?curid=10868", "title": "Francisco Goya", "text": "Francisco Goya\n\nFrancisco José de Goya y Lucientes (; ; 30 March 1746 – 16 April 1828) was a Spanish romantic painter and printmaker. He is considered the most important Spanish artist of late 18th and early 19th centuries and throughout his long career was a commentator and chronicler of his era. Immensely successful in his lifetime, Goya is often referred to as both the last of the Old Masters and the first of the moderns. He was also one of the great portraitists of modern times.\n\nHe was born to a modest family in 1746 in the village of Fuendetodos in Aragon. He studied painting from age 14 under José Luzán y Martinez and moved to Madrid to study with Anton Raphael Mengs. He married Josefa Bayeu in 1773; the couple's life together was characterised by an almost constant series of pregnancies and miscarriages. He became a court painter to the Spanish Crown in 1786 and this early portion of his career is marked by portraits of the Spanish aristocracy and royalty, and Rococo style tapestry cartoons designed for the royal palace.\n\nGoya was a guarded man and although letters and writings survive, little is known about his thoughts. He suffered a severe and undiagnosed illness in 1793 which left him completely deaf. After 1793 his work became progressively darker and more pessimistic. His later easel and mural paintings, prints and drawings appear to reflect a bleak outlook on personal, social and political levels, and contrast with his social climbing. He was appointed \"Director of the Royal Academy\" in 1795, the year Manuel Godoy made an unfavorable treaty with France. In 1799 Goya became \"Primer Pintor de Cámara\", the then-highest rank for a Spanish court painter. In the late 1790s, commissioned by Godoy, he completed his \"La maja desnuda\", a remarkably daring nude for the time and clearly indebted to Diego Velázquez. In 1801 he painted \"Charles IV of Spain and His Family\".\n\nIn 1807 Napoleon led the French army into Spain. Goya remained in Madrid during the Peninsular War, which seems to have affected him deeply. Although he did not vocalise his thoughts in public, they can be inferred from his \"Disasters of War\" series of prints (although published 35 years after his death) and his 1814 paintings \"The Second of May 1808\" and \"The Third of May 1808\". Other works from his mid period include the \"Caprichos\" and \"Los Disparates\" etching series, and a wide variety of paintings concerned with insanity, mental asylums, witches, fantastical creatures and religious and political corruption, all of which suggest that he feared for both his country's fate and his own mental and physical health.\n\nHis late period culminates with the \"Black Paintings\" of 1819–1823, applied on oil on the plaster walls of his house the \"Quinta del Sordo\" (\"house of the deaf man\") where, disillusioned by political and social developments in Spain he lived in near isolation. Goya eventually abandoned Spain in 1824 to retire to the French city of Bordeaux, accompanied by his much younger maid and companion, Leocadia Weiss, who may or may not have been his lover. There he completed his \"La Tauromaquia\" series and a number of other, major, canvases. Following a stroke which left him paralyzed on his right side, and suffering failing eyesight and poor access to painting materials, he died and was buried on 16 April 1828 aged 82. His body was later re-interred in Spain.\n\nFrancisco Goya was born in Fuendetodos, Aragón, Spain, on 30 March 1746 to José Benito de Goya y Franque and Gracia de Lucientes y Salvador. The family had moved that year from the city of Zaragoza, but there is no record why; likely José was commissioned to work there. They were lower middle-class. José was the son of a notary and of Basque origin, his ancestors being from Zerain, earning his living as a gilder, specialising in religious and decorative craftwork. He oversaw the gilding and most of the ornamentation during the rebuilding of the Basilica of Our Lady of the Pillar (\"Santa Maria del Pilar\"), the principal cathedral of Zaragoza. Francisco was their fourth child, following his sister Rita (b. 1737), brother Tomás (b. 1739) (who was to follow in his father's trade) and second sister Jacinta (b. 1743). There were two younger sons, Mariano (b. 1750) and Camilo (b. 1753).\n\nHis mother's family had pretensions of nobility and the house, a modest brick cottage, was owned by her family and, perhaps fancifully, bore their crest. About 1749 José and Gracia bought a home in Zaragoza and were able to return to live in the city. Although there are no surviving records it is thought that Goya may have attended the Escuelas Pías de San Antón, which offered free schooling. His education seems to have been adequate but not enlightening; he had reading, writing and numeracy, and some knowledge of the classics. According to Robert Hughes the artist \"seems to have taken no more interest than a carpenter in philosophical or theological matters, and his views on painting ... were very down to earth: Goya was no theoretician. At school he formed a close and lifelong friendship with fellow pupil Martin Zapater; the 131 letters Goya wrote to him from 1775 until Zapater's death in 1801 give valuable insight into Goya's early years at the court in Madrid.\n\nAt age 14 Goya studied under the painter José Luzán, and in Luzán's workshop, copied stamps for 4 years until he decided to work on his own, as he wrote later on \"paint from my invention\". He moved to Madrid to study with Anton Raphael Mengs, a popular painter with Spanish royalty. He clashed with his master, and his examinations were unsatisfactory. Goya submitted entries for the Real Academia de Bellas Artes de San Fernando in 1763 and 1766, but was denied entrance.\nRome at the time was the cultural capital of Europe and held all the prototypes of classical antiquity, while Spain lacked a coherent artistic direction, with all of its significant visual achievements in the past. Having failed to earn a scholarship, Goya relocated at his own expense to Rome in the old tradition of European artists stretching back to at least to Albrecht Dürer. He was an unknown at the time and so the records are scant and uncertain. Early biographers have him travelling to Rome with a gang of bullfighters, where he worked as a street acrobat, or for a Russian diplomat, or fell in love with beautiful young nun whom he plotted to abduct from her convent. What is more certain is two surviving mythological painting completed during the visit, a \"Sacrifice to Vesta\" and a \"Sacrifice to Pan\", both dated 1771.\nIn 1771 he won second prize in a painting competition organized by the City of Parma. That year he returned to Zaragoza and painted parts of the cupolas of the Basilica of the Pillar (including \"Adoration of the Name of God\"), a cycle of frescoes for the monastic church of the Charterhouse of Aula Dei, and the frescoes of the Sobradiel Palace. He studied with the Aragónese artist Francisco Bayeu y Subías and his painting began to show signs of the delicate tonalities for which he became famous.\n\nGoya befriended Francisco Bayeu, and married Bayeu's sister Josefa (he nicknamed her \"Pepa\") on 25 July 1773 and they had their first child, Antonio Juan Ramon Carlos, on 29 August 1774.\n\nThe marriage and Francisco Bayeu's 1765 membership of the Real Academia de Bellas Artes de San Fernando and directorship of the tapestry works from 1777 helped Goya earn a commission for a series of tapestry cartoons for the Royal Tapestry Factory. Over five years he designed some 42 patterns, many of which were used to decorate and insulate the stone walls of El Escorial and the Palacio Real del Pardo, the residences of the Spanish monarchs. While designing tapestries was neither prestigious nor well paid, his cartoons are mostly popularist in a rococo style, and Goya used them to bring himself to wider attention.\nThe cartoons were not his only royal commissions, and were accompanied by a series of engravings, mostly copies after old masters such as Marcantonio Raimondi and Velázquez. Goya had a complicated relationship to the latter artist; while many of his contemporaries saw folly in Goya's attempts to copy and emulate him, he had access to a wide range of the long-dead painter's works that had been contained in the royal collection. Nonetheless, etching was a medium that the young artist was to master, a medium that was to reveal both the true depths of his imagination and his political beliefs. His c 1779 etching of \"The Garrotted Man\" (\"El agarrotado\") was the largest work he had produced to date, and an obvious foreboding of his later \"Disasters of War\" series.\n\nGoya was beset by illness, and his condition was used against him by his rivals, who looked jealously upon any artist seen to be rising in stature. Some of the larger cartoons, such as \"The Wedding\", were more than 8 by 10 feet, and had proved a drain on his physical strength. Ever resourceful, Goya turned this misfortune around, claiming that his illness had allowed him the insight to produce works that were more personal and informal. However, he found the format limiting, as it did not allow him to capture complex color shifts or texture, and was unsuited to the impasto and glazing techniques he was by then applying to his painted works. The tapestries seem as comments on human types, fashion and fads.\n\nOther works from the period include a canvas for the altar of the Church of San Francisco El Grande in Madrid, which led to his appointment as a member of the Royal Academy of Fine Art.\n\nIn 1783, the Count of Floridablanca, favorite of Charles III of Spain, commissioned Goya to paint his portrait. He became friends with Crown Prince Don Luis, spending two summers painting portraits of both the Infante and his family. During the 1780s, his circle of patrons grew to include the Duke and Duchess of Osuna, the king and other notable people of the kingdom whom he painted. In 1786, Goya was given a salaried position as painter to Charles III.\nIn 1789 he was appointed court painter to Charles IV. The following year he became First Court Painter, with a salary of 50,000 reales and an allowance of 500 ducats for a coach. He painted the king and the queen, royal family pictures, a portrait of the Spanish Prime Minister Manuel de Godoy and many other nobles. The portraits are notable for their disinclination to flatter, and in the case of \"Charles IV of Spain and His Family\", the lack of visual diplomacy is remarkable. Modern interpreters view the portrait as satirical; it is thought to reveal the corruption behind the rule of Charles IV. Under his reign his wife Louisa was thought to have had the real power, and thus Goya placed her at the center of the group portrait. From the back left of the painting one can see the artist himself looking out at the viewer, and the painting behind the family depicts Lot and his daughters, thus once again echoing the underlying message of corruption and decay.\n\nGoya received commissions from the highest ranks of the Spanish nobility, including Pedro Téllez-Girón, 9th Duke of Osuna and his wife María Josefa Pimentel, 12th Countess-Duchess of Benavente, María del Pilar de Silva, 13th Duchess of Alba (universally known simply as the \"Duchess of Alba\"), and her husband José María Álvarez de Toledo, 15th Duke of Medina Sidonia, and María Ana de Pontejos y Sandoval, Marchioness of Pontejos. In 1801 he painted Godoy in a commission to commemorate the victory in the brief War of the Oranges against Portugal. The two were friends, even if Goya's \"1801 portrait\" is usually seen as satire. Yet even after Godoy's fall from grace the politician referred to the artist in warm terms. Godoy saw himself as instrumental in the publication of the Caprichos and is widely believed to have commissioned \"La maja desnuda\".\n\n\"La Maja Desnuda\" (\"La maja desnuda\") was \"the first totally profane life-size female nude in Western art\" without pretense to allegorical or mythological meaning. The identity of the \"Majas\" is uncertain. The most popularly cited models are the Duchess of Alba, with whom Goya was sometimes thought to have had an affair, and Pepita Tudó, mistress of Manuel de Godoy. Neither theory has been verified, and it remains as likely that the paintings represent an idealized composite. The paintings were never publicly exhibited during Goya's lifetime and were owned by Godoy. In 1808 all Godoy's property was seized by Ferdinand VII after his fall from power and exile, and in 1813 the Inquisition confiscated both works as 'obscene', returning them in 1836 to the Academy of Fine Arts of San Fernando.\nIn 1798 he painted luminous and airy scenes for the pendentives and cupola of the Real Ermita (Chapel) of San Antonio de la Florida in Madrid. Many of these depict miracles of Saint Anthony of Padua set in the midst of contemporary Madrid.\nAt some time between late 1792 and early 1793 an undiagnosed illness left Goya deaf. He became withdrawn and introspective while the direction and tone of his work changed. He began the series of aquatinted etchings, published in 1799 as the \"Caprichos\"—completed in parallel with the more official commissions of portraits and religious paintings. In 1799 Goya published 80 \"Caprichos\" prints depicting what he described as \"the innumerable foibles and follies to be found in any civilized society, and from the common prejudices and deceitful practices which custom, ignorance, or self-interest have made usual\". The visions in these prints are partly explained by the caption \"The sleep of reason produces monsters\". Yet these are not solely bleak; they demonstrate the artist's sharp satirical wit, particularly evident in etchings such as \"Hunting for Teeth\".\n\nGoya's physical and mental breakdown seems to have happened a few weeks after the French declaration of war on Spain. A contemporary reported, \"The noises in his head and deafness aren’t improving, yet his vision is much better and he is back in control of his balance.\" These symptoms may indicate a prolonged viral encephalitis, or possibly a series of miniature strokes resulting from high blood pressure and which affected the hearing and balance centers of the brain. The triad of tinnitus, episodes of imbalance, and progressive deafness are also typical of Ménière's disease. It is possible that Goya suffered from cumulative lead poisoning, as he used massive amounts of lead white—which he ground himself—in his paintings, both as a canvas primer and as a primary color.\n\nOther postmortem diagnostic assessments point toward paranoid dementia, possibly due to an unknown brain trauma, as evidenced by marked changes in his work after his recovery. Increasingly paranoid features are observed in his post-illness paintings, culminating in the \"black\" paintings, of which \"Saturn Devouring His Sons\" is best known. Art historians have noted Goya's singular ability to express his personal demons as horrific and fantastic imagery that speaks universally, and allows his audience to find its own catharsis in the images.\n\nFrench forces invaded Spain in 1808, leading to the Peninsular War of 1808–1814. The extent of Goya's involvement with the court of the \"Intruder king\", Joseph I, the brother of Napoleon Bonaparte, is not known; he painted works for French patrons and sympathisers, but kept neutral during the fighting. After the restoration of the Spanish king, Ferdinand VII, in 1814, Goya denied any involvement with the French. When his wife Josefa died in 1812, he was mentally and emotionally processing the war by painting \"The Second of May 1808\" and \"The Third of May 1808\", and preparing the series of prints later known as \"The Disasters of War\" (\"Los desastres de la guerra\"). Ferdinand VII returned to Spain in 1814 but relations with Goya were not cordial. He painted portraits of the king for a variety of organizations, but not for the king himself.\nDuring a period of convalescence during 1793–1794, Goya completed a set of eleven small pictures painted on tin. Known as \"Fantasy and Invention\", they mark a significant change in the tone and subject matter of his art. They draw from dark and dramatic realms of fantasy nightmare. \"Yard with Lunatics\" is a horrifying, imaginary vision of loneliness, fear and social alienation. The condemnation of brutality towards prisoners (whether criminal or insane) is a subject that Goya assayed in later works that focused on the degradation of the human figure. It was one of the first of Goya's mid-1790s cabinet paintings, in which his earlier search for ideal beauty gave way to an examination of the relationship between naturalism and fantasy that would preoccupy him for the rest of his career. He was undergoing a nervous breakdown and entering prolonged physical illness, and admitted that the series was created to reflect his own self-doubt, anxiety and fear that he was going mad. Goya wrote that the works served \"to occupy my imagination, tormented as it is by contemplation of my sufferings.\" The series, he said, consisted of pictures which \"normally find no place in commissioned works.\"\n\nAlthough he did not make known his intention when creating the aquatint plates of \"The Disasters of War\" in the 1810s, art historians view them as a visual protest against the violence of the 1808 Dos de Mayo Uprising, the subsequent Peninsular War and the setbacks to the liberal cause following the restoration of the Bourbon monarchy in 1814. The scenes are singularly disturbing, sometimes macabre in their depiction of battlefield horror, and represent an outraged conscience in the face of death and destruction. They were not published until 1863, 35 years after his death. It is likely that only then was it considered politically safe to distribute a sequence of artworks criticising both the French and restored Bourbons.\n\nThe first 47 plates in the series focus on incidents from the war and show the consequences of the conflict on individual soldiers and civilians. The middle series (plates 48 to 64) record the effects of the famine that hit Madrid in 1811–12, before the city was liberated from the French. The final 17 reflect the bitter disappointment of liberals when the restored Bourbon monarchy, encouraged by the Catholic hierarchy, rejected the Spanish Constitution of 1812 and opposed both state and religious reform. Since their first publication, Goya's scenes of atrocities, starvation, degradation and humiliation have been described as the \"prodigious flowering of rage\".\n\nGoya's works from 1814 to 1819 are mostly commissioned portraits, but also include the altarpiece of Santa Justa and Santa Rufina for the Cathedral of Seville, the print series of \"La Tauromaquia\" depicting scenes from bullfighting, and probably the etchings of \"Los Disparates\".\n\nThe historical record of Goya's later life is relatively scant; no accounts of his thoughts from this time survive. He deliberately suppressed a number of his works from this period. Tormented by a dread of old age and fear of madness, the latter possibly from anxiety caused by an undiagnosed illness that left him deaf from the early 1790s. Goya had been a successful and royally placed artist, but withdrew from public life during his final years. From the late 1810s he lived in near-solitude outside Madrid in a farmhouse converted into a studio. The house had become known as \"La Quinta del Sordo\" (The House of the Deaf Man), after the nearest farmhouse had coincidentally also belonged to a deaf man.\n\nArt historians assume Goya felt alienated from the social and political trends that followed the 1814 restoration of the Bourbon monarchy, and that he viewed these developments as reactionary means of social control. In his unpublished art he seems to have railed against what he saw as a tactical retreat into Medievalism. It is thought that he had hoped for political and religious reform, but like many liberals became disillusioned when the restored Bourbon monarchy and Catholic hierarchy rejected the Spanish Constitution of 1812.\n\nAt the age of 75, alone and in mental and physical despair, he completed the work as one of his 14 \"Black Paintings\", all of which were executed in oil directly onto the plaster walls of his house. Goya did not intend for the paintings to be exhibited, did not write of them, and likely never spoke of them. It was not until around 1874, some 50 years after his death, that they were taken down and transferred to a canvas support. Many of the works were significantly altered during the restoration, and in the words of Arthur Lubow what remain are \"at best a crude facsimile of what Goya painted.\" The effects of time on the murals, coupled with the inevitable damage caused by the delicate operation of mounting the crumbling plaster on canvas, meant that most of the murals suffered extensive damage and loss of paint. Today they are on permanent display at the Museo del Prado, Madrid.\n\nLeocadia Weiss (née Zorrilla, b. 1790) the artist's maid, younger by 35 years, and a distant relative, lived with and cared for Goya after Bayeu's death. She stayed with him in his Quinta del Sordo villa until 1824 with her daughter Rosario. Leocadia was probably similar in features to Goya's first wife Josefa Bayeu, to the point that one of his well-known portraits bears the cautious title of \"Josefa Bayeu (or Leocadia Weiss)\".\n\nNot much is known about her beyond her fiery temperament. She was likely related to the Goicoechea family, a wealthy dynasty into which the artist's son, the feckless Javier, had married. It is believed she held liberal political views and was unafraid of expressing them, a fact met with disapproval by Goya's family. It is known that Leocadia had an unhappy marriage with a jeweler, Isideo Weiss, but was separated from him since 1811. Her husband cited \"illicit conduct\" during the divorce proceedings. She had two children before the marriage dissolved, and bore a third, Rosario, in 1814 when she was 26. Isideo was not the father, and it has often been speculated—although with little firm evidence—that the child belonged to Goya. There has been much speculation that Goya and Weiss were romantically linked, however, it is more likely the affection between them was sentimental.\n\nLeocadia was left nothing in Goya's will; mistresses were often omitted in such circumstances, but it is also likely that he did not want to dwell on his mortality by thinking about or revising his will. She wrote to a number of Goya's friends to complain of her exclusion but many of her friends were Goya's also and by then were old men or had died, and did not reply. Largely destitute she moved into rented accommodation and passed on her copy of the \"Caprichos\" for free.\n\n\n\n\n"}
{"id": "10869", "url": "https://en.wikipedia.org/wiki?curid=10869", "title": "Frequentist probability", "text": "Frequentist probability\n\nFrequentist probability or frequentism is an interpretation of probability; it defines an event's probability as the limit of its relative frequency in a large number of trials. This interpretation supports the statistical needs of experimental scientists and pollsters; probabilities can be found (in principle) by a repeatable objective process (and are thus ideally devoid of opinion). It does not support all needs; gamblers typically require estimates of the odds without experiments.\n\nThe development of the frequentist account was motivated by the problems and paradoxes of the previously dominant viewpoint, the classical interpretation. In the classical interpretation, probability was defined in terms of the principle of indifference, based on the natural symmetry of a problem, so, \"e.g.\" the probabilities of dice games arise from the natural symmetric 6-sidedness of the cube. This classical interpretation stumbled at any statistical problem that has no natural symmetry for reasoning.\n\nIn the frequentist interpretation, probabilities are discussed only when dealing with well-defined random experiments (or random samples). The set of all possible outcomes of a random experiment is called the sample space of the experiment. An event is defined as a particular subset of the sample space to be considered. For any given event, only one of two possibilities may hold: it occurs or it does not. The relative frequency of occurrence of an event, observed in a number of repetitions of the experiment, is a measure of the probability of that event. This is the core conception of probability in the frequentist interpretation.\n\nThus, if formula_1 is the total number of trials and formula_2 is the number of trials where the event formula_3 occurred, the probability formula_4 of the event occurring will be approximated by the relative frequency as follows:\n\nClearly, as the number of trials is increased, one might expect the relative frequency to become a better approximation of a \"true frequency\".\n\nA claim of the frequentist approach is that in the \"long run,\" as the number of trials approaches infinity, the relative frequency will converge \"exactly\" to the true probability:\n\nThe frequentist interpretation is a philosophical approach to the definition and use of probabilities; it is one of several such approaches. It does not claim to capture all connotations of the concept 'probable' in colloquial speech of natural languages.\n\nAs an interpretation, it is not in conflict with the mathematical axiomatization of probability theory; rather, it provides guidance for how to apply mathematical probability theory to real-world situations. It offers distinct guidance in the construction and design of practical experiments, especially when contrasted with the Bayesian interpretation. As to whether this guidance is useful, or is apt to mis-interpretation, has been a source of controversy. Particularly when the frequency interpretation of probability is mistakenly assumed to be the only possible basis for frequentist inference. So, for example, a list of mis-interpretations of the meaning of p-values accompanies the article on p-values; controversies are detailed in the article on statistical hypothesis testing. The Jeffreys–Lindley paradox shows how different interpretations, applied to the same data set, can lead to different conclusions about the 'statistical significance' of a result.\n\nAs William Feller noted:\nFeller's comment was criticism of Laplace, who published a solution to the sunrise problem using an alternative probability interpretation. Despite Laplace's explicit and immediate disclaimer in the source, based on expertise in astronomy as well as probability, two centuries of criticism have followed.\n\nThe frequentist view may have been foreshadowed by Aristotle, in \"Rhetoric\", when he wrote:\n\nPoisson clearly distinguished between objective and subjective probabilities in 1837. Soon thereafter a flurry of nearly simultaneous publications by Mill, Ellis (\"On the Foundations of the Theory of Probabilities\" and \"Remarks on the Fundamental Principles of the Theory of Probabilities\"), Cournot (\"Exposition de la théorie des chances et des probabilités\") and Fries introduced the frequentist view. Venn provided a thorough exposition (\"The Logic of Chance: An Essay on the Foundations and Province of the Theory of Probability\" (published editions in 1866, 1876, 1888)) two decades later. These were further supported by the publications of Boole and Bertrand. By the end of the 19th century the frequentist interpretation was well established and perhaps dominant in the sciences. The following generation established the tools of classical inferential statistics (significance testing, hypothesis testing and confidence intervals) all based on frequentist probability.\n\nAlternatively, Jacob Bernoulli (AKA James or Jacques) understood the concept of frequentist probability and published a critical proof (the weak law of large numbers) posthumously in \"1713\". He is also credited with some appreciation for subjective probability (prior to and without Bayes theorem). Gauss and Laplace used frequentist (and other) probability in derivations of the least squares method a century later, a generation before Poisson. Laplace considered the probabilities of testimonies, tables of mortality, judgments of tribunals, etc. which are unlikely candidates for classical probability. In this view, Poisson's contribution was his sharp criticism of the alternative \"inverse\" (subjective, Bayesian) probability interpretation. Any criticism by Gauss and Laplace was muted and implicit. (Their later derivations did not use inverse probability.)\n\nMajor contributors to \"classical\" statistics in the early 20th century included Fisher, Neyman and Pearson. Fisher contributed to most of statistics and made significance testing the core of experimental science; Neyman formulated confidence intervals and contributed heavily to sampling theory; Neyman and Pearson paired in the creation of hypothesis testing. All valued objectivity, so the best interpretation of probability available to them was frequentist. All were suspicious of \"inverse probability\" (the available alternative) with prior probabilities chosen by the using the principle of indifference. Fisher said, \"...the theory of inverse probability is founded upon an error, [referring to Bayes theorem] and must be wholly rejected.\" (from his Statistical Methods for Research Workers). While Neyman was a pure frequentist, Fisher's views of probability were unique; Both had nuanced view of probability. von Mises offered a combination of mathematical and philosophical support for frequentism in the era.\n\nAccording to the \"Oxford English Dictionary\", the term 'frequentist' was first used by M. G. Kendall in 1949, to contrast with Bayesians, whom he called \"non-frequentists\". He observed\n\n\"The Frequency Theory of Probability\" was used a generation earlier as a chapter title in Keynes (1921).\n\nThe historical sequence: probability concepts were introduced and much of probability mathematics derived (prior to the 20th century), classical statistical inference methods were developed, the mathematical foundations of probability were solidified and current terminology was introduced (all in the 20th century). The primary historical sources in probability and statistics did not use the current terminology of classical, subjective (Bayesian) and frequentist probability.\n\nProbability theory is a branch of mathematics. While its roots reach centuries into the past, it reached maturity with the axioms of Andrey Kolmogorov in 1933. The theory focuses on the valid operations on probability values rather than on the initial assignment of values; the mathematics is largely independent of any interpretation of probability.\n\nApplications and interpretations of probability are considered by philosophy, the sciences and statistics. All are interested in the extraction of knowledge from observations—inductive reasoning. There are a variety of competing interpretations; All have problems. Major interpretations include classical probability, subjective probability and frequency interpretations.\n\n\nThe frequentist interpretation does resolve difficulties with the classical interpretation, such as any problem where the natural symmetry of outcomes is not known. It does not address other issues, such as the dutch book. Propensity probability is an alternative physicalist approach.\n\n"}
{"id": "10870", "url": "https://en.wikipedia.org/wiki?curid=10870", "title": "List of French-language poets", "text": "List of French-language poets\n\nList of poets who have written in the French language:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "10871", "url": "https://en.wikipedia.org/wiki?curid=10871", "title": "FM-2030", "text": "FM-2030\n\nFM-2030 (October 15, 1930 – July 8, 2000) was an author, teacher, transhumanist philosopher, futurist, consultant and athlete. FM-2030 was born Fereidoun M. Esfandiary ().\n\nHe became notable as a transhumanist with the book \"Are You a Transhuman?: Monitoring and Stimulating Your Personal Rate of Growth in a Rapidly Changing World\", published in 1989. In addition, he wrote a number of works of fiction under his original name F.M. Esfandiary.\n\nThe son of an Iranian diplomat, he travelled widely as a child, living in 17 countries by age 11; then, as a young man, he represented Iran as a basketball player at the 1948 Olympic Games in London and served on the United Nations Conciliation Commission for Palestine from 1952 to 1954.\n\nIn the mid-1970s F.M. Esfandiary legally changed his name to FM-2030 for two main reasons. Firstly, to reflect the hope and belief that he would live to celebrate his 100th birthday in 2030; secondly, and more importantly, to break free of the widespread practice of naming conventions that he saw as rooted in a collectivist mentality, and existing only as a relic of humankind's tribalistic past. He viewed traditional names as almost always stamping a label of collective identity—varying from gender to nationality—on the individual, thereby existing as prima facie elements of thought processes in the human cultural fabric, that tended to degenerate into stereotyping, factionalism, and discrimination. In his own words, \"Conventional names define a person's past: ancestry, ethnicity, nationality, religion. I am not who I was ten years ago and certainly not who I will be in twenty years. [...] The name 2030 reflects my conviction that the years around 2030 will be a magical time. In 2030 we will be ageless and everyone will have an excellent chance to live forever. 2030 is a dream and a goal.\"\n\nHe was a lifelong vegetarian and said he would not eat anything that had a mother. FM-2030 once said, \"I am a 21st century person who was accidentally launched in the 20th. I have a deep nostalgia for the future.\" He taught at The New School, UCLA, and Florida International University. He worked as a corporate consultant for Lockheed and J.C. Penney. He was also an atheist.\n\nOn July 8, 2000, FM-2030 died from pancreatic cancer and was placed in cryonic suspension at the Alcor Life Extension Foundation in Scottsdale, Arizona, where his body remains today. He did not yet have remote standby arrangements, so no Alcor team member was present at his death, but FM-2030 was the first person to be vitrified, rather than simply frozen as previous cryonics patients had been. FM-2030 was survived by four sisters and one brother.\n\n\n\n"}
{"id": "10874", "url": "https://en.wikipedia.org/wiki?curid=10874", "title": "West Flemish", "text": "West Flemish\n\nWest Flemish (, ) is a dialect of the Dutch language spoken in western Belgium and adjoining parts of the Netherlands and France.\n\nWest Flemish is spoken by about a million people in the Belgian province of West Flanders, and a further 120,000 in the neighbouring Dutch coastal district of Zeelandic Flanders (and another 220,000 if Zealandic is included), and 10,000 in the northern part of the French \"département\" of Nord. Some of the main cities where West Flemish is widely spoken are Bruges, Kortrijk, Ostend, Roeselare, and Ypres. The dialects of the rest of the Dutch province of Zeeland, Zeelandic, are often included in West Flemish; these are part of a dialect continuum which proceeds further north into Hollandic.\n\nWest Flemish is listed as a \"vulnerable\" language in UNESCO's online Red Book of Endangered Languages.\n\nWest Flemish phonology differs significantly from the standard Dutch phonology. The best known are the (pre-)velar fricatives \"g\" and \"ch\" in Dutch (), being realised as glottal \"h\" - , and the overall lack of diphthongs compared to Dutch. The following differences are listed by their Dutch spelling, as some different letters have evolved to the same sound in Dutch, but stayed separate sounds in West Flemish. Pronunciations can also differ a bit from region to region.\n\n\nDue to the non-existent and sounds in West Flemish, native speakers of the dialect have to concentrate a lot to pronounce these sounds. This often results in hyper-correction of the sounds to a or .\n\nThe Dutch language also has many words with an \"-en\" () suffix (mostly plural forms of verbs and nouns). While standard Dutch and most Dutch dialects do not pronounce the final \"n\", West Flemish typically drops the \"e\" and pastes the \"n\" to the base word. For base words already ending with \"n\", the final \"n\" sound is often prolonged to make the suffix clear. This mute-e is similar to many English words: \"beaten\", \"listen\", ...\n\nThe short \"o\" () in words can also be pronounced as a short \"u\" (). This happens spontaneously on some words, but other words keep their original short \"o\" sounds. Similarly, the short \"a\" () can turn into a short \"o\" () in some words without apparent reason.\n\nThe diftong \"ui\" () does not exist in West Flemish, and is (depending on the word) pronounced as a long \"u\" () or a long \"ie\" (). Similar to the \"ui\", the long \"o\" () can turn into an (\"eu\") on some words, while it becomes a in other ones.\n\nThis transition often shows similarities with English.\n\nHere are some examples showing the sound shifts that are part of the vocabulary:\n\nPlural forms in Dutch are made most often by appending an \"-en\" suffix, while West Flemish uses the \"-s\" suffix on more plural forms. This phenomenon is shared with the Lower Saxon Germanic dialects, and even more prominent in English (where a plural form on \"-en\" has become very rare). Under influence of Standard Dutch, the number of people that uses the \"-s\" suffix for the plural form on these words diverging from Dutch is diminishing. Younger speakers tend to resort more to the plural form on \"-en\".\n\nThe verbs \"zijn\" (to be) and \"hebben\" (to have) are also conjugated differently.\n\nWest Flemish often shows a duplicated subject.\n\nIn Dutch, the indefinite article does not depend on gender, while in West Flemish, it does. Though this practice is dying, and the gender-independent article more often being used. And similar to English, a connection \"n\" is only made when the next word starts with a vowel.\n\nAnother feature of the West Flemish dialect is the conjugation of \"ja\" and \"nee\" (yes and no) to the subject of the sentence. This is somewhat related to the double subject, but even when the rest of the sentence is not pronounced, \"ja\" and \"nee\" are generally pronounced together with the first part of the double subject. There is also an extra word: \"toet\" () which is used for negating the previous sentence but giving a positive answer.\n\n\"Ja\", \"nee\" and \"toet\" can also always be made stronger by adding a \"mo-\" or \"ba-\" in front of it. Both mean \"but\" (and are derived from \"but\" or \"maar\" in Dutch), and they even can be added together (f.e. \"mobatoet\").\n\nWest Flemish inherited many words from Saxon settlers, and later on received loanwords from wool and cloth trade with England. These two categories both differ from standard Dutch, and show similarities with English, and as such, it is difficult to differentiate between the two categories.\n\nDuring the industrial revolution, the trade with French became more important, and many industrial words are French loanwords\n\nEven when words exist in Dutch and West Flemish, their meaning is not guaranteed to be the same. This can sometimes cause confusion for native speakers who do not realise these words are used differently.\n\n\n"}
{"id": "10875", "url": "https://en.wikipedia.org/wiki?curid=10875", "title": "Fritz Leiber", "text": "Fritz Leiber\n\nFritz Reuter Leiber, Jr. (December 24, 1910 – September 5, 1992) was an American writer of fantasy, horror, and science fiction. He was also a poet, actor in theater and films, playwright and chess expert. With writers such as Robert E. Howard and Michael Moorcock, Leiber can be regarded as one of the fathers of sword and sorcery fantasy, having created the term.\n\nFritz Leiber was born December 24, 1910, in Chicago, Illinois, to the actors Fritz Leiber and Virginia Bronson Leiber. For a time, he seemed inclined to follow in his parents' footsteps; the theater and actors are prominently featured in his fiction. He spent 1928 touring with his parents' Shakespeare company (Fritz Leiber & Co.) before entering the University of Chicago, where he was elected to Phi Beta Kappa and received an undergraduate Ph.B. degree in psychology and physiology or biology with honors in 1932. From 1932 to 1933, he worked as a lay reader and studied as a candidate for the ministry at the Episcopal Church-affiliated General Theological Seminary in Chelsea, Manhattan without taking a degree.\n\nAfter pursuing graduate studies in philosophy at the University of Chicago from 1933 to 1934 and failing once more to take a degree, he remained based in Chicago while touring intermittently with his parents' company (under the stage name of \"Francis Lathrop\") and pursuing a concurrent literary career; six short stories in the 2010 collection \"Strange Wonders: A Collection of Rare Fritz Leiber Works\" carry 1934 and 1935 dates. He also appeared alongside his father in uncredited parts in several films, including George Cukor's \"Camille\" (1936), James Whale's \"The Great Garrick\" (1937) and William Dieterle's \"The Hunchback of Notre Dame\" (1939).\n\nIn 1936, he initiated a brief yet intense correspondence with H.P. Lovecraft, who \"encouraged and influenced [Leiber's] literary development\" before succumbing to small intestine cancer and malnutrition in March 1937. Leiber introduced Fafhrd and the Gray Mouser in \"Two Sought Adventure\", his first professionally-published short story in the August 1939 edition of the John W. Campbell-edited \"Unknown\".\n\nLeiber married Jonquil Stephens on January 16, 1936; their only child, the philosopher and science fiction writer Justin Leiber, was born in 1938. From 1937 to 1941, he was employed by Consolidated Book Publishing as a staff writer for the \"Standard American Encyclopedia\". In 1941, the family moved to California, where Leiber served as a speech and drama instructor at Occidental College during the 1941–1942 academic year.\n\nUnable to conceal his disdain for academic politics as the United States entered World War II, he decided that the struggle against fascism was more important than his long-held pacifist convictions. He accepted a position with Douglas Aircraft in quality inspection, primarily working on the C-47 Skytrain; throughout the war, he continued to regularly publish fiction in a variety of periodicals.\n\nThereafter, the family returned to Chicago, where Leiber served as associate editor of \"Science Digest\" from 1945 to 1956. During this decade (forestalled by a fallow interregnum from 1954 to 1956), his output (including the 1947 Arkham House anthology \"Night's Black Agents\") was characterized by Poul Anderson as \"a lot of the best science fiction and fantasy in the business.\" In 1958, the Leibers returned to Los Angeles. By this juncture, he was able to relinquish his journalistic career and support his family as a full-time fiction writer.\n\nJonquil's death in 1969 precipitated Leiber's permanent relocation to San Francisco and exacerbated his longstanding alcohol use disorder after twelve years of fellowship in Alcoholics Anonymous; however, he would gradually regain relative sobriety (an effort impeded by comorbid barbiturate abuse) over the next two decades. In 1977, he returned to his original form with a fantasy novel set in modern-day San Francisco, \"Our Lady of Darkness\", which is about a writer of weird tales who must deal with the death of his wife and his recovery from alcoholism.\n\nPerhaps as a result of his substance abuse, Leiber seems to have suffered periods of penury in the 1970s; Harlan Ellison has written of his anger at finding that the much-awarded Leiber had to write his novels on a manual typewriter that was propped up over the sink in his apartment, and Marc Laidlaw wrote that, when visiting Leiber as a fan in 1976, he \"was shocked to find him occupying one small room of a seedy San Francisco residence hotel, its squalor relieved mainly by walls of books\". Other reports suggest that Leiber preferred to live simply in the city, spending his money on dining, movies and travel. In the last years of his life, royalty checks from TSR, Inc. (the makers of \"Dungeons and Dragons\", who had licensed the mythos of the Fafhrd and Gray Mouser series) were enough in themselves to ensure that he lived comfortably.\n\nIn 1992, the last year of his life, Leiber married his second wife, Margo Skinner, a journalist and poet with whom he had been friends for many years.\n\nLeiber's death occurred a few weeks after a physical collapse while traveling from a science fiction convention in London, Ontario, with Skinner. The cause of his death was given as \"organic brain disease\".\n\nHe wrote a 100-page-plus memoir, \"Not Much Disorder and Not So Early Sex\", which can be found in \"The Ghost Light\" (1984).\n\nLeiber's own literary criticism, including several essays on Lovecraft, was collected in the volume \"Fafhrd and Me\" (1990).\n\nAs the child of two Shakespearean actors—Fritz, Sr. and Virginia (née Bronson)—Leiber was fascinated with the stage, describing itinerant Shakespearean companies in stories like \"No Great Magic\" and \"Four Ghosts in Hamlet,\" and creating an actor/producer protagonist for his novel \"A Specter is Haunting Texas\".\n\nAlthough his \"Change War\" novel, \"The Big Time\", is about a war between two factions, the \"Snakes\" and the \"Spiders\", changing and rechanging history throughout the universe, all the action takes place in a small bubble of isolated space-time about the size of a theatrical stage, with only a handful of characters. Judith Merril (in the July 1969 issue of \"The Magazine of Fantasy & Science Fiction\") remarks on Leiber's acting skills when the writer won a science fiction convention costume ball. Leiber's costume consisted of a cardboard military collar over turned-up jacket lapels, cardboard insignia, an armband, and a spider pencilled large in black on his forehead, thus turning him into an officer of the Spiders, one of the combatants in his Change War stories. \"The only other component,\" Merril writes, \"was the Leiber instinct for theatre.\"\n\nDue to the similarity of the names of the father and the son, some filmographies incorrectly attribute to Fritz, Jr. roles which were in fact played by his father, Fritz Leiber, Sr. Fritz, Sr. was the evil Inquisitor in the Errol Flynn adventure film \"The Sea Hawk\" (1940) and had played in many other movies from 1917 onwards until the late 1950s. It is the elder Leiber, not the younger, who appears in the Vincent Price vehicle \"The Web\" (1947) and in Charlie Chaplin's \"Monsieur Verdoux\" (1947).\n\nIn the cult horror film \"Equinox\" (1970) directed by Dennis Muren and Jack Woods, Leiber has a cameo appearance as Dr. Watermann, a geologist. In the edited second version of the movie Leiber has no spoken dialogue in the film but features in a few scenes. The original version of the movie has a longer appearance by Leiber recounting the ancient book and a brief speaking role, all of which was cut from the re-release of the film.\n\nHe also appears in the 1979 Schick Sunn Classics documentary \"The Bermuda Triangle\", based on the book by Charles Berlitz, as Chavez.\n\nLeiber was heavily influenced by H. P. Lovecraft and Robert Graves in the first two decades of his career. Beginning in the late 1950s, he was increasingly influenced by the works of Carl Jung, particularly by the concepts of the anima and the shadow. From the mid-1960s onwards, he began incorporating elements of Joseph Campbell's \"The Hero with a Thousand Faces\". These concepts are often openly mentioned in his stories, especially the anima, which becomes a method of exploring his fascination with, but estrangement from, the female.\n\nLeiber liked cats, which feature prominently in many of his stories. Tigerishka, for example, is a cat-like alien who is sexually attractive to the human protagonist yet repelled by human customs in the novel \"The Wanderer\". Leiber's \"Gummitch\" stories feature a kitten with an I.Q. of 160, just waiting for his ritual cup of coffee so that he can become human, too.\n\nHis first stories were inspired by the Cthulhu Mythos. Leiber later wrote several essays on Lovecraft such as \"A Literary Copernicus\" which formed key moments in the serious critical appreciation of Lovecraft's life and work.\n\nLeiber's first professional sale was \"Two Sought Adventure\" (\"Unknown\", August 1939), which introduced his most famous characters, Fafhrd and the Gray Mouser. In 1943, his first two novels were serialized in \"Unknown\" (the supernatural horror-oriented \"Conjure Wife\", partially inspired by his deleterious experiences on the faculty of Occidental College) and \"Astounding Science Fiction\" (\"Gather, Darkness\").\n\n1947 marked the publication of his first book, \"Night's Black Agents\", a short story collection containing seven stories grouped as 'Modern Horrors', one as a 'Transition', and two grouped as 'Ancient Adventures': \"The Sunken Land\" and \"Adept's Gambit\", which are both stories of Fafhrd and the Gray Mouser.\n\nBook publication of the science fiction novel \"Gather, Darkness\" followed in 1950. It deals with a futuristic world that follows the Second Atomic Age which is ruled by scientists, until in the throes of a new Dark Age, the witches revolt.\n\nIn 1951, Leiber was Guest of Honor at the World Science Fiction Convention in New Orleans. Further novels followed during the 1950s, and in 1958 \"The Big Time\" won the Hugo Award for Best Novel.\n\nLeiber published further books in the 1960s. His novel \"The Wanderer\" (1964) also received the Hugo for Best Novel. In the novel, an artificial planet, quickly nicknamed the Wanderer, materializes from hyperspace within earth's orbit. The Wanderer's gravitational field captures the moon and shatters it into something like one of Saturn's rings. On Earth, the Wanderer's gravity well triggers massive earthquakes, tsunamis, and tidal phenomena. The multi-threaded plot follows the exploits of a large ensemble cast as they struggle to survive the global disaster.\n\nLeiber received the Hugo Award for Best Novella in 1970 and 1971 for \"Ship of Shadows\" (1969) and \"Ill Met in Lankhmar\" (1970). \"Gonna Roll the Bones\" (1967), his contribution to Harlan Ellison's \"Dangerous Visions\" anthology, received the Hugo Award for Best Novelette and the Nebula Award for Best Novelette in 1968.\n\n\"Our Lady of Darkness\" (1977)—originally serialized in short form in \"The Magazine of Fantasy & Science Fiction\" under the title \"The Pale Brown Thing\" (1977)—featured cities as the breeding grounds for new types of elementals called paramentals, summonable by the dark art of megapolisomancy, with such activities centering on the Transamerica Pyramid. Its main characters include Franz Westen, Jaime Donaldus Byers, and the magician Thibault de Castries. \"Our Lady of Darkness\" won the World Fantasy Award—Novel.\n\nLeiber also did the 1966 novelization of the Clair Huffaker screenplay of \"Tarzan and the Valley of Gold\".\n\nMany of Leiber's most-acclaimed works are short stories, especially in the horror genre. Owing to such stories as \"The Smoke Ghost\", \"The Girl With the Hungry Eyes\" and \"You're All Alone\" (later expanded as \"The Sinful Ones\"), he is widely regarded as one of the forerunners of the modern urban horror story. Leiber also challenged the conventions of science fiction through reflexive narratives such as \"A Bad Day For Sales\" (first published in \"Galaxy Science Fiction\", July 1953), in which the protagonist, Robie, \"America’s only genuine mobile salesrobot,\" references the title character of Isaac Asimov’s idealistic robot story, \"Robbie\". Questioning Isaac Asimov’s Three Laws of Robotics, Leiber imagines the futility of automatons in a post-apocalyptic New York City. In his later years, Leiber returned to short story horror in such works as \"Horrible Imaginings\", \"Black Has Its Charms\" and the award-winning \"The Button Moulder\".\n\nThe short parallel worlds story \"Catch That Zeppelin!\" (1975) received the Hugo Award for Best Short Story and the Nebula Award for Best Short Story in 1976. This story shows a plausible alternate reality that is much better than our own, whereas the typical parallel universe story depicts a world that is much worse than our own. \"Belsen Express\" (1975) won the World Fantasy Award—Short Fiction. Both stories reflect Leiber's uneasy fascination with Nazism, an uneasiness compounded by his mixed feelings about his German ancestry and his philosophical pacifism during World War II.\n\nLeiber was named the second Gandalf Grand Master of Fantasy by participants in the 1975 World Science Fiction Convention (Worldcon), after the posthumous inaugural award to J. R. R. Tolkien. Next year he won the World Fantasy Award for Life Achievement. He was Guest of Honor at the 1979 Worldcon in Brighton, England (1979). The Science Fiction Writers of America made him its fifth SFWA Grand Master in 1981; the Horror Writers Association made him an inaugural winner of the Bram Stoker Award for Lifetime Achievement in 1988 (named in 1987); and the Science Fiction and Fantasy Hall of Fame inducted him in 2001, its sixth class of two deceased and two living writers.\n\nLeiber was a founding member of the Swordsmen and Sorcerers' Guild of America (SAGA), a loose-knit group of Heroic fantasy authors founded in the 1960s, led by Lin Carter, with entry by fantasy credentials alone. Some works by SAGA members were published in Lin Carter's \"Flashing Swords!\" anthologies. Leiber himself is credited with inventing the term sword and sorcery for the particular subgenre of epic fantasy exemplified by his Fafhrd and Grey Mouser stories.\n\nIn an appreciation in the July 1969 \"Special Fritz Leiber Issue\" of \"The Magazine of Fantasy & Science Fiction\", Judith Merril writes of Leiber's connection with his readers: \"That this kind of \"personal\" response...is shared by thousands of other readers, has been made clear on several occasions.\" The November 1959 issue of \"Fantastic\", for instance: Leiber had just come out of one of his recurrent dry spells, and editor Cele Lalli bought up all his new material until there was enough [five stories] to fill an issue; the magazine came out with a big black headline across its cover — \"Leiber Is Back!\"\n\nHis legacy appears to have been consolidated by the most famous of his creations, the \"Fafhrd and the Gray Mouser\" stories, written over a span of 50 years. The first of them, \"Two Sought Adventure\", appeared in \"Unknown\", August 1939. They are concerned with an unlikely pair of heroes found in and around the city of Lankhmar. Fafhrd was based on Leiber himself and the Mouser on his friend Harry Otto Fischer, and the two characters were created in a series of letters exchanged by the two in the mid-1930s. These stories were among the progenitors of many of the tropes of the sword and sorcery genre. They are also notable among sword and sorcery stories in that, over the course of the stories, his two heroes mature, take on more responsibilities, and eventually settle down into marriage.\n\nSome Fafhrd and Mouser stories were recognized by annual genre awards: \"Scylla's Daughter\" (1961) was \"Short Story\" Hugo finalist and \"Ill Met in Lankhmar\" (1970) won the \"Best Novella\" Hugo and Nebula Awards. Leiber's last major work, \"The Knight and Knave of Swords\" (1991), brought the series to a close while leaving room for possible sequels. In the last year of his life, Leiber was considering allowing the series to be continued by other writers, but his sudden death made this more difficult. One new Fafhrd and the Mouser novel, \"Swords Against the Shadowland\", by Robin Wayne Bailey, did appear in 1998.\n\nThe stories were influential in shaping the genre and were influential on other works. Joanna Russ' stories about thief-assassin Alyx (collected in 1976 in \"The Adventures of Alyx\") were in part inspired by Fafhrd and the Gray Mouser, and Alyx in fact made guest appearances in two of Leiber's stories. Numerous writers have paid homage to the stories. For instance, Terry Pratchett's city of Ankh-Morpork bears something more than a passing resemblance to Lankhmar (acknowledged by Pratchett by the placing of the swordsman-thief \"The Weasel\" and his giant barbarian comrade \"Bravd\" in the opening scenes of the first Discworld novel). More recently, playing off the visit of Fafhrd and the Grey Mouser to our world in \"Adept's Gambit\" (set in second century B.C. Tyre), Steven Saylor's short story \"Ill Seen in Tyre\" takes his Roma Sub Rosa series hero Gordianus to the city of Tyre a hundred years later, where the two visitors from Nehwon are remembered as local legends.\n\nFischer and Leiber contributed to the original game design of the wargame \"Lankhmar\"—published in 1976 by TSR.\n\n\n\n\n\n\n\n\"Conjure Wife\" has been made into feature films three times under other titles:\n\nA new film adaptation of \"Conjure Wife\" was announced in 2008, to be filmed by US director Billy Ray. It is slated to be a United Artists/Studio Canal co-production.\n\n\"The Girl with the Hungry Eyes\" was filmed under that title by Kastenbaum Films in 1995. (This film is not to be confused with the 1967 William Rotsler film \"The Girl with the Hungry Eyes\" which is entirely unrelated to Leiber's story).\n\nTwo Leiber stories were filmed for TV for Rod Serling's \"Night Gallery\". These were \"The Girl with the Hungry Eyes\" (1970) (adapted by Robert M. Young and directed by John Badham); and \"The Dead Man\" (adapted and directed by Douglas Heyes).\n\n\n\n"}
{"id": "10878", "url": "https://en.wikipedia.org/wiki?curid=10878", "title": "Flanders", "text": "Flanders\n\nFlanders ( , , ) is the Dutch-speaking northern portion of Belgium, although there are several overlapping definitions, including ones related to culture, language, politics and history. It is one of the communities, regions and language areas of Belgium. The demonym associated with Flanders is Fleming, while the corresponding adjective is Flemish. The official capital of Flanders is Brussels, although Brussels itself has an independent regional government, and the government of Flanders only oversees some cultural aspects of Brussels life.\n\nIn historical contexts, Flanders originally refers to the County of Flanders (Flandria), which around AD 1000 stretched from the Strait of Dover to the Scheldt estuary. The core of historical Flanders is situated within modern-day Flanders and corresponds to the provinces West Flanders and East Flanders, but it sometimes stretched into what is now France and the Netherlands. Nevertheless, during the 19th and 20th centuries it became increasingly commonplace in English and some other languages to use the term \"Flanders\" to refer to the entire Dutch-speaking part of Belgium, stretching all the way to the River Maas, as well as cultural movements such as Flemish art. In accordance with late 20th century Belgian state reforms the area was made into two political entities: the \"Flemish Community\" () and the \"Flemish Region\" (). These entities were merged, although geographically the Flemish Community, which has a broader cultural mandate, covers Brussels, whereas the Flemish Region does not.\n\nFlanders has figured prominently in European history. During the late Middle Ages, cities such as Ghent, Bruges, Antwerp and Brussels made it one of the richest and most urbanized parts of Europe, weaving the wool of neighbouring lands into cloth for both domestic use and export. As a consequence, a very sophisticated culture developed, with impressive achievements in the arts and architecture, rivaling those of northern Italy. Belgium was one of the centres of the 19th century industrial revolution but Flanders was at first overtaken by French-speaking Wallonia. In the second half of the 20th century, however, Flanders' economy modernised rapidly, and today Flanders is significantly more wealthy than its southern counterpart and in general one of the most wealthy regions in Europe and the world.\n\nGeographically, Flanders is generally flat, and has a small section of coast on the North Sea. Much of Flanders is agriculturally fertile and densely populated, with a population density of almost 500 people per square kilometer (1,200 per square mile). It touches France to the west near the coast, and borders the Netherlands to the north and east, and Wallonia to the south. The Brussels Capital Region is an enclave within the Flemish Region. Flanders has exclaves of its own: Voeren in the east is between Wallonia and the Netherlands and Baarle-Hertog in the north consists of 22 exclaves surrounded by the Netherlands.\n\nThe term \"Flanders\" has several main meanings:\n\n\nThe significance of the County of Flanders and its counts eroded through time, but the designation remained in a very broad sense. In the Early modern period, the term Flanders was associated with the southern part of the Low Countries: the Southern Netherlands. During the 19th and 20th centuries, it became increasingly commonplace to refer to the Dutch-speaking part of Belgium as \"Flanders\". The linguistic limit between French and Dutch was recorded in the early '60's, from Kortrijk to Maastricht. Now, Flanders extends over the northern part of Belgium, including Belgian Limburg (corresponding closely to the medieval County of Loon), and the Belgian parts of the medieval Duchy of Brabant.\n\nThe ambiguity between this wider area and that of the County (or the Belgian parts thereof), still remains. In most present-day contexts however, in general the term Flanders is taken to refer to either the political, social, cultural, and linguistic community (and the corresponding official institution, the Flemish Community), or the geographical area, one of the three institutional regions in Belgium, namely the Flemish Region.\n\nIn the history of art and other fields, the adjectives Flemish and Netherlandish are commonly used to designate all the artistic production in this area before about 1580, after which it refers specifically to the southern Netherlands. For example, the term \"Flemish Primitives\", now outdated in English but used in French, Dutch and other languages, is a synonym for \"Early Netherlandish painting\", and it is not uncommon to see Mosan art categorized as Flemish art. In music the \"Franco-Flemish School\" is also known as the \"Dutch School\".\n\nWithin this Dutch-speaking part of Belgium, French has never ceased to be spoken by some citizens and Jewish groups have been speaking Yiddish in Antwerp for centuries. Today, Flanders' minority residents include 170 nationalities — the largest groups speaking French, English, Berber, Turkish, Arabic, Spanish, Italian and Polish.\n\nThe area, roughly encompassing the later geographical meanings of Flanders, was considered to be in the northern and less economically developed part of Gallia Belgica. Under the Roman empire this became an administrative province, but much of modern Belgium eventually became part of Germania Inferior. These were the most northerly continental provinces of the Roman empire. Linguistically, the tribes in this area were under Celtic influence in the south, and Germanic influence in the east, but there is disagreement about what language was spoken locally, which may even have been an intermediate \"Nordwestblock\" language related to both. By the first century BC Germanic languages had become prevalent. In the future county of Flanders, the main Belgic tribe in Roman times was the Menapii, but also on the coast were the Marsacii and Morini. In the central part of modern Belgium were the Nervii and in the east were the Tungri. The Tungri especially were thought to have links to Germanic tribes east of the Rhine. Another notable group were the Toxandrians who appear to have lived in the Kempen region, in the northern parts of both the Nervian and Tungrian provinces. The Roman provinces of the Menapii, Nervii and Tungri therefore correspond roughly with the medieval counties of Flanders, Brabant and Loon, and the modern Flemish provinces of East and West Flanders (Menapii), Brabant and Antwerp (Nervii), and Belgian Limburg (Tungri).\n\nCreated in the year 862 as a feudal fief in West Francia, the County of Flanders was divided when its western districts fell under French rule in the late 12th century. The remaining parts of Flanders came under the rule of the counts of neighbouring Hainaut in 1191. The entire area passed in 1384 to the dukes of Burgundy, in to the Habsburg dynasty, and in 1556 to the kings of Spain. The western districts of Flanders came finally under French rule under successive treaties of 1659 (Artois), 1668, and 1678.\n\nDuring the late Middle Ages Flanders' trading towns (notably Ghent, Bruges and Ypres) made it one of the richest and most urbanized parts of Europe, weaving the wool of neighbouring lands into cloth for both domestic use and export. As a consequence, a very sophisticated culture developed, with impressive achievements in the arts and architecture, rivaling those of northern Italy. Ghent, Bruges, Ypres and the Franc of Bruges formed the Four Members, a form of parliament that exercised considerable power in Flanders.\n\nIncreasingly powerful from the 12th century, the territory's autonomous urban communes were instrumental in defeating a French attempt at annexation (1300–1302), finally defeating the French in the Battle of the Golden Spurs (11 July 1302), near Kortrijk. Two years later, the uprising was defeated and Flanders remained part of the French Crown. Flemish prosperity waned in the following century, however, owing to widespread European population decline following the Black Death of 1348, the disruption of trade during the Anglo-French Hundred Years' War (1337–1453), and increased English cloth production. Flemish weavers had gone over to Worstead and North Walsham in Norfolk in the 12th century and established the woolen industry.\n\nThe County of Flanders started to take control of the neighbouring County of Brabant during the life of Louis II, Count of Flanders (1330-1384), who fought his sister-in-law Joanna, Duchess of Brabant for control of it. The titles were eventually more clearly united under Philip the Good (1396 – 1467), Duke of Burgundy. The County of Loon, approximately the modern Flemish province of Limburg, remained independent under the lordship of the Archbishop of Liège until the French Revolution, but surrounded by the Burgundians.\n\nIn 1500, Charles V was born in Ghent. He inherited the Seventeen Provinces (1506), Spain (1516) with its colonies and in 1519 was elected Holy Roman Emperor. The Pragmatic Sanction of 1549, issued by Charles V, established the Low Countries as the Seventeen Provinces (or Spanish Netherlands in its broad sense) as an entity separate from the Holy Roman Empire and from France. In 1556 Charles V abdicated due to ill health (he suffered from crippling gout). Spain and the Seventeen Provinces went to his son, king Philip II of Spain.\n\nOver the first half of the 16th century Antwerp grew to become the second-largest European city north of the Alps by 1560. Antwerp was the richest city in Europe at this time. According to Luc-Normand Tellier \"It is estimated that the port of Antwerp was earning the Spanish crown seven times more revenues than the Americas.\"\nMeanwhile, Protestantism had reached the Low Countries. Among the wealthy traders of Antwerp, the Lutheran beliefs of the German Hanseatic traders found appeal, perhaps partly for economic reasons. The spread of Protestantism in this city was aided by the presence of an Augustinian cloister (founded 1514) in the St. Andries quarter. Luther, an Augustinian himself, had taught some of the monks, and his works were in print by 1518. The first Lutheran martyrs came from Antwerp. The Reformation resulted in consecutive but overlapping waves of reform: a Lutheran, followed by a militant Anabaptist, then a Mennonite, and finally a Calvinistic movement. These movements existed independently of each other.\n\nPhilip II, a devout Catholic and self-proclaimed protector of the Counter-Reformation, suppressed Calvinism in Flanders, Brabant and Holland (what is now approximately Belgian Limburg was part of the Bishopric of Liège and was Catholic \"de facto\"). In 1566, the wave of iconoclasm known as the \"Beeldenstorm\" was a prelude to religious war between Catholics and Protestants, especially the Anabaptists. The \"Beeldenstorm\" started in what is now French Flanders, with open-air sermons () that spread through the Low Countries, first to Antwerp and Ghent, and from there further east and north. In total it lasted not even a month.\n\nSubsequently, Philip II sent the Duke of Alba to the Provinces to repress the revolt. Alba recaptured the southern part of the Provinces, who signed the Union of Atrecht, which meant that they would accept the Spanish government on condition of more freedom. But the northern part of the provinces signed the Union of Utrecht and settled in 1581 the Republic of the Seven United Netherlands. Spanish troops quickly started fighting the rebels, but before the revolt could be completely defeated, a war between England and Spain had broken out, forcing Philip's Spanish troops to halt their advance. Meanwhile, the Spanish armies had already conquered the important trading cities of Bruges and Ghent. Antwerp, which was then the most important port in the world, also had to be conquered. On 17 August 1585, Antwerp fell. This ended the Eighty Years' War for the (from now on) Southern Netherlands. The United Provinces (the Northern Netherlands) fought on until 1648 – the Peace of Westphalia.\nWhile Spain was at war with England, the rebels from the north, strengthened by refugees from the south, started a campaign to reclaim areas lost to Philip II's Spanish troops. They managed to conquer a considerable part of Brabant (the later Noord-Brabant of the Netherlands), and the south bank of the Scheldt estuary (Zeeuws-Vlaanderen), before being stopped by Spanish troops. The front line at the end of this war stabilized and became the current border between present-day Belgium and the Netherlands. The Dutch (as they later became known) had managed to reclaim enough of Spanish-controlled Flanders to close off the river Scheldt, effectively cutting Antwerp off from its trade routes.\n\nFirst the fall of Antwerp to the Spanish and later also the closing of the Scheldt were causes of a considerable emigration of Antverpians. Many of the Calvinist merchants of Antwerp and also of other Flemish cities left Flanders and emigrated to the north. A large number of them settled in Amsterdam, which was at the time a smaller port, of significance only in the Baltic trade. In the following years Amsterdam was rapidly transformed into one of the world's most important ports. Because of the contribution of the Flemish exiles to this transformation, the exodus is sometimes described as \"\"creating a new Antwerp\"\".\n\nFlanders and Brabant, due to these events, went into a period of relative decline from the time of the Thirty Years War. In the Northern Netherlands however, the mass emigration from Flanders and Brabant became an important driving force behind the Dutch Golden Age.\n\nAlthough arts remained at a relatively impressive level for another century with Peter Paul Rubens (1577–1640) and Anthony van Dyck, Flanders experienced a loss of its former economic and intellectual power under Spanish, Austrian, and French rule, with heavy taxation and rigid imperial political control compounding the effects of industrial stagnation and Spanish-Dutch and Franco-Austrian conflict. The Southern Netherlands suffered severely under the War of the Spanish Succession, but under the reign of empress Maria-Theresia these lands economically flourished again. Influenced by the Enlightenment, the Austrian emperor Joseph II was the first sovereign who has been in the Southern Netherlands since king Philip II of Spain left them in 1559.\n\nIn 1794 the French Republican Army started using Antwerp as the northernmost naval port of France, which country officially annexed Flanders the following year as the \"départements\" of Lys, Escaut, Deux-Nèthes, Meuse-Inférieure and Dyle. Obligatory (French) army service for all men aged 16–25 was one of the main reasons for the people's uprising against the French in 1798, known as the \"Boerenkrijg\" (\"Peasants' War\"), with the heaviest fighting in the Campine area.\n\nAfter the defeat of Napoleon Bonaparte at the 1815 Battle of Waterloo in Waterloo, Brabant, sovereignty over the Austrian Netherlands – Belgium minus the East Cantons and Luxembourg – was given by the Congress of Vienna (1815) to the United Netherlands (Dutch: \"Verenigde Nederlanden\"), the state that briefly existed under Sovereign Prince William I of Orange Nassau, the latter King William I of the United Kingdom of the Netherlands, after the French Empire was driven out of the Dutch territories. The United Kingdom of the Netherlands was born. The Protestant King of the Netherlands, William I rapidly started the industrialisation of the southern parts of the Kingdom. The political system that was set up however, slowly but surely failed to forge a true union between the northern and the southern parts of the Kingdom. The southern bourgeoisie mainly was Roman Catholic, in contrast to the mainly Protestant north; large parts of the southern bourgeoisie also primarily spoke French rather than Dutch.\n\nIn 1815 the Dutch Senate was reinstated (Dutch: \"Eerste Kamer der Staaten Generaal\"). The nobility, mainly coming from the south, became more and more estranged from their northern colleagues. Resentment grew both between the Roman Catholics from the south and the Protestants from the north and among the powerful liberal bourgeoisie from the south and their more moderate colleagues from the north. On 25 August 1830 (after the showing of the opera 'La Muette de Portici' of Daniel Auber in Brussels) the Belgian Revolution sparked off and became a fact. On 4 October 1830, the Provisional Government (Dutch: \"Voorlopig Bewind\") proclaimed the independence, which was later confirmed by the National Congress that issued a new Liberal Constitution and declared the new state a Constitutional Monarchy, under the House of Saxe-Coburg. Flanders now became part of the Kingdom of Belgium, which was recognized by the major European Powers on 20 January 1831. The de facto dissidence was finally recognized by the United Kingdom of the Netherlands on 19 April 1839.\n\nIn 1830, the Belgian Revolution led to the splitting up of the two countries. Belgium was confirmed as an independent state by the Treaty of London of 1839, but deprived of the eastern half of Limburg (now Dutch Limburg), and the Eastern half of Luxembourg (now the Grand-Duchy of Luxembourg). Sovereignty over Zeeuws Vlaanderen, south of the Westerscheldt river delta, was left with the Kingdom of the Netherlands, which was allowed to levy a toll on all traffic to Antwerp harbour until 1863.\n\nThe Belgian Revolution was not well supported in Flanders and even on 4 October 1830, when the Belgian independence was eventually declared, Flemish authorities refused to take orders from the new Belgian government in Brussels. Only after Flanders was subdued with the aid of a large French military force one month later, under the leadership of the Count de Pontécoulant, did Flanders become a true part of Belgium.\n\nThe French-speaking bourgeoisie showed very little respect for the Dutch-speaking part of the population. French became the only official language in Belgium and all secondary and higher education in the Dutch language was abolished. \n\nIn 1834, all people even remotely suspected of being \"Flemish minded\" or calling for the reunification of the Netherlands were prosecuted and their houses looted and burnt. Flanders, until then a very prosperous European region, was not considered worthwhile for investment and scholarship. A study in 1918 demonstrated that in the first 88 years of its existence, 80% of the Belgian GNP was invested in Wallonia. This led to a widespread poverty in Flanders, forcing roughly 300.000 Flemish to emigrate to Wallonia to start working there in the heavy industry.\n\nAll of these events led to a silent uprising in Flanders against the French-speaking domination. But it was not until 1878 that Dutch was allowed to be used for official purposes in Flanders (see language legislation in Belgium), although French remained the only official language in Belgium.\n\nIn 1873, Dutch became the official language in public secondary schools. In 1898 Dutch and French were declared equal languages in laws and Royal orders. In 1930 the first Flemish university was opened.\n\nThe first official translation of the Belgian constitution in Dutch was not published until 1967.\n\nFlanders (and Belgium as a whole) saw some of the greatest loss of life on the Western Front of the First World War, in particular from the three battles of Ypres.\n\nFlemish feeling of identity and consciousness grew through the events and experiences of war. The occupying German authorities took several Flemish-friendly measures. More importantly, the experiences of many Dutch-speaking soldiers on the front led by French-speaking officers catalysed Flemish emancipation. The French-speaking officers often gave orders in French only, followed by \"et pour les Flamands, la même chose!\", meaning \"and for the Flemish, the same thing!\" (which did not help the Flemish conscripts, who were mostly uneducated farmers and workers unable to have understood what had been said in French).\nThe resulting suffering is still remembered by Flemish organizations during the yearly Yser pilgrimage in Diksmuide at the monument of the Yser Tower.\n\nDuring the interbellum and World War II, several right-wing fascist and/or national-socialistic parties emerged in Belgium, the Flemish ones being energized by the anti-Flemish discrimination of the Wallonians. Since these parties were promised more rights for the Flemings by the German government during World War II, many of them collaborated with the Nazi regime. After the war, collaborators (or people who were \"Zwart\", \"Black\" during the war) were prosecuted and punished, among them many Flemish Nationalists whose main political goal had been the emancipation of Flanders. As a result, up until this day Flemish Nationalism is often associated with right-wing and sometimes fascist ideologies.\n\nAfter World War II, the differences between Dutch-speaking and French-speaking Belgians became clear in a number of conflicts, such as the Royal Question, the question whether King Leopold III should return (which most Flemings supported but not the Walloons) and the use of Dutch in the Catholic University of Leuven. As a result, several state reforms took place in the second half of the 20th century, which transformed the unitary Belgium into a federal state with communities, regions and language areas. This resulted also in the establishment of a Flemish Parliament and Government. During the 1970s, all major political parties split into a Dutch and French-speaking party.\n\nSeveral Flemish parties still advocate for more Flemish autonomy, some even for Flemish independence (see Partition of Belgium), whereas the French-speakers would like to keep the current state as it is. Recent governments (such as Verhofstadt I Government) have transferred certain federal competences to the regional governments.\n\nOn 13 December 2006, a spoof news broadcast by the Belgian Francophone public broadcasting station RTBF declared that Flanders had decided to declare independence from Belgium.\n\nThe 2007 federal elections showed more support for Flemish autonomy, marking the start of the 2007–2011 Belgian political crisis. All the political parties that advocated a significant increase of Flemish autonomy gained votes as well as seats in the Belgian federal parliament. This was especially the case for Christian Democratic and Flemish and New Flemish Alliance (N-VA) (who had participated on a shared electoral list). The trend continued during the 2009 regional elections, where CD&V and N-VA were the clear winners in Flanders, and N-VA became even the largest party in Flanders and Belgium during the 2010 federal elections, followed by the longest-ever government formation after which the Di Rupo I Government was formed excluding N-VA. Eight parties agreed on a sixth state reform which aim to solve the disputes between Flemings and French-speakers. The 2012 provincial and municipal elections however continued the trend of N-VA becoming the biggest party in Flanders.\n\nHowever, sociological studies show no parallel between the rise of nationalist parties and popular support for their agenda. Instead, a recent study revealed a majority in favour of returning regional competences to the federal level \n\nThese victories for the advocates of much more Flemish autonomy are very much in parallel with opinion polls that show a structural increase in . Since 2006, certain polls have started showing Flemish independence. Those polls are not yet representative, but they point to a significant long-term trend.\n\nBoth the Flemish Community and the Flemish Region are constitutional institutions of the Kingdom of Belgium, exercising certain powers within their jurisdiction, granted following a series of state reforms. In practice, the Flemish Community and Region together form a single body, with its own parliament and government, as the Community legally absorbed the competences of the Region. The parliament is a directly elected legislative body composed of 124 representatives. The government consists of up to a maximum of eleven members and is presided by a Minister-President, currently Geert Bourgeois (New Flemish Alliance) leading a coalition of his party (N-VA) with Christen-Democratisch en Vlaams (CD&V) and Open Vlaamse Liberalen en Democraten (Open VLD).\n\nThe area of the Flemish Community is represented on the maps above, including the area of the Brussels-Capital Region (hatched on the relevant map). Roughly, the Flemish Community exercises competences originally oriented towards the individuals of the Community's language: culture (including audiovisual media), education, and the use of the language. Extensions to personal matters less directly associated with language comprise sports, health policy (curative and preventive medicine), and assistance to individuals (protection of youth, social welfare, aid to families, immigrant assistance services, etc.)\n\nThe area of the Flemish Region is represented on the maps above. It has a population of more than 6 million (excluding the Dutch-speaking community in the Brussels Region, grey on the map for it is not a part of the Flemish Region). Roughly, the Flemish Region is responsible for territorial issues in a broad sense, including economy, employment, agriculture, water policy, housing, public works, energy, transport, the environment, town and country planning, nature conservation, credit, and foreign trade. It supervises the provinces, municipalities, and intercommunal utility companies.\n\nThe number of Dutch-speaking Flemish people in the Capital Region is estimated to be between 11% and 15% (official figures do not exist as there is no language census and no official subnationality). According to a survey conducted by the Université catholique de Louvain in Louvain-la-Neuve and published in June 2006, 51% of respondents from Brussels claimed to be bilingual, even if they do not have Dutch as their first language. They are governed by the Brussels Region for economics affairs and by the Flemish Community for educational and cultural issues.\nAs mentioned above, Flemish institutions such as the Flemish Parliament and Government, represent the Flemish Community and the Flemish Region. The region and the community thus \"de facto\" share the same parliament and the same government. All these institutions are based in Brussels. Nevertheless, both types of subdivisions (the Community and the Region) still exist legally and the distinction between both is important for the people living in Brussels. Members of the Flemish Parliament who were elected in the Brussels Region cannot vote on affairs belonging to the competences of the Flemish Region.\n\nThe official language for all Flemish institutions is Dutch. French enjoys a limited official recognition in a dozen municipalities along the borders with French-speaking Wallonia, and a large recognition in the bilingual Brussels Region. French is widely known in Flanders, with 59% claiming to know French according to a survey conducted by the Université catholique de Louvain in Louvain-la-Neuve and published in June 2006.\n\nHistorically, the political parties reflected the pillarisation (\"verzuiling\") in Flemish society. The traditional political parties of the three pillars are Christian-Democratic and Flemish (CD&V), the Open Flemish Liberals and Democrats (Open Vld) and the Socialist Party – Differently (sp.a).\n\nHowever, during the last half century, many new political parties were founded in Flanders. One of the first was the nationalist People's Union, of which the right nationalist Flemish Block (now Flemish Interest) split off, and which later dissolved into the now-defunct Spirit or Social Liberal Party, moderate nationalism rather left of the spectrum, on the one hand, and the New Flemish Alliance (N-VA), more conservative but independentist, on the other hand. Other parties are the leftist alternative/ecological Green party; the short-lived anarchistic libertarian spark ROSSEM and more recently the conservative-right liberal List Dedecker, founded by Jean-Marie Dedecker, and the socialist Workers' Party.\n\nParticularly the Flemish Block/Flemish Interest has seen electoral success roughly around the turn of the century, and the New Flemish Alliance during the last few elections, becoming even the largest party in the 2010 federal elections.\n\nFor some inhabitants, Flanders is more than just a geographical area or the federal institutions (Flemish Community and Region). Supporters of the Flemish Movement even call it a nation and pursue Flemish independence, but most people (approximately 75%) living in Flanders say they are proud to be Belgian and opposed to the dissolution of Belgium. 20% is even \"very proud\", while some 25% are not proud and 8% is \"very not proud\". Mostly students claim to be proud of their nationality, with 90% of them staying so. Of the people older than 55, 31% claim to be proud of being a Belgian. Particular opposition to secession comes from women, people employed in services, the highest social classes and people from big families. Strongest of all opposing the notion are housekeepers - both housewives and house husbands.\n\nIn 2012, the Flemish government drafted a \"Charter for Flanders\" (\"Handvest voor Vlaanderen\") of which the first article says \"\"Vlaanderen is een deelstaat van de federale Staat België en maakt deel uit van de Europese Unie.\"\" (\"Flanders is a component state of the federal State of Belgium and is part of the European Union\"). Though interpreted by many Flemish nationalists as a statement, this phrase is merely a quotation from the Belgian constitution and has no further legal value whatsoever.\n\nFlanders shares its borders with Wallonia in the south, Brussels being an enclave within the Flemish Region. The rest of the border is shared with the Netherlands (Zeelandic Flanders, North Brabant and Limburg) in the north and east, and with France (French Flanders) and the North Sea in the west. Voeren is an exclave of Flanders between Wallonia and the Netherlands, while Baarle-Hertog in Flanders forms a complicated series of enclaves and exclaves with Baarle-Nassau in the Netherlands. Germany, although bordering Wallonia and close to Voeren in Limburg, does not share a border with Flanders. The German-speaking Community of Belgium, also close to Voeren, does not border Flanders either. (The commune of Plombières, majority French speaking, lies between them.)\n\nFlanders is a highly urbanised area, lying completely within the Blue Banana. Antwerp, Ghent, Bruges and Leuven are the largest cities of the Flemish Region. Antwerp has a population of more than 500,000 citizens and is the largest city, Ghent has a population of 250,000 citizens, followed by Bruges with 120,000 citizens and Leuven counts almost 100,000 citizens. Brussels is a part of Flanders as far as community matters are concerned, but does not belong to the Flemish Region.\n\nFlanders has two main geographical regions: the coastal Yser basin plain in the north-west and a central plain. The first consists mainly of sand dunes and clayey alluvial soils in the polders. Polders are areas of land, close to or below sea level that have been reclaimed from the sea, from which they are protected by dikes or, a little further inland, by fields that have been drained with canals. With similar soils along the lowermost Scheldt basin starts the central plain, a smooth, slowly rising fertile area irrigated by many waterways that reaches an average height of about five metres (16.4 ft) above sea level with wide valleys of its rivers upstream as well as the Campine region to the east having sandy soils at altitudes around thirty metres Near its southern edges close to Wallonia one can find slightly rougher land richer of calcium with low hills reaching up to and small valleys, and at the eastern border with the Netherlands, in the Meuse basin, there are marl caves (\"mergelgrotten\"). Its exclave around Voeren between the Dutch border and the Walloon province of Liège attains a maximum altitude of above sea level.\n\nThe present-day Flemish Region covers and is divided into five provinces, 22 arrondissements and 308 cities or municipalities.\n\nThe province of Flemish Brabant is the most recent one, being formed in 1995 after the splitting of the province of Brabant.\n\nMost municipalities are made up of several former municipalities, now called \"deelgemeenten\". The largest municipality (both in terms of population and area) is Antwerp, having more than half a million inhabitants. Its nine \"deelgemeenten\" have a special status and are called districts, which have an elected council and a college. While any municipality with more than 100,000 inhabitants can establish districts, only Antwerp did this so far. The smallest municipality (also both in terms of population and area) is Herstappe (Limburg).\nThe Flemish Community covers both the Flemish Region and, together with the French Community, the Brussels-Capital Region. Brussels, an enclave within the province of Flemish Brabant, is not divided into any province nor is it part of any. It coincides with the Arrondissement of Brussels-Capital and includes 19 municipalities.\n\nThe Flemish Government has its own local institutions in the Brussels-Capital Region, being the \"Vlaamse Gemeenschapscommissie\" (VGC), and its municipal antennae (\"Gemeenschapscentra\", community centres for the Flemish community in Brussels). These institutions are independent from the educational, cultural and social institutions that depend directly on the Flemish Government. They exert, among others, all those cultural competences that outside Brussels fall under the provinces.\n\nThe climate is maritime temperate, with significant precipitation in all seasons (Köppen climate classification: \"Cfb\"; the average temperature is in January, and in July; the average precipitation is 65 millimetres (2.6 in) in January, and 78 millimetres (3.1 in) in July).\n\nTotal GDP of the Flemish Region in 2004 was €165,847 billion (Eurostat figures). Per capita GDP at purchasing power parity was 23% above the EU average. Flemish productivity per capita is about 13% higher than that in Wallonia, and wages are about 7% higher than in Wallonia.\n\nFlanders was one of the first continental European areas to undergo the Industrial Revolution, in the 19th century. Initially, the modernization relied heavily on food processing and textile. However, by the 1840s the textile industry of Flanders was in severe crisis and there was famine in Flanders (1846–50). After World War II, Antwerp and Ghent experienced a fast expansion of the chemical and petroleum industries. Flanders also attracted a large majority of foreign investments in Belgium. The 1973 and 1979 oil crises sent the economy into a recession. The steel industry remained in relatively good shape. In the 1980s and 90s, the economic centre of Belgium continued to shift further to Flanders and is now concentrated in the populous Flemish Diamond area. Nowadays, the Flemish economy is mainly service-oriented.\n\nBelgium is a founding member of the European Coal and Steel Community in 1951, which evolved into the present-day European Union. In 1999, the euro, the single European currency, was introduced in Flanders. It replaced the Belgian franc in 2002.\n\nThe Flemish economy is strongly export-oriented, in particular of high value-added goods. The main imports are food products, machinery, rough diamonds, petroleum and petroleum products, chemicals, clothing and accessories, and textiles. The main exports are automobiles, food and food products, iron and steel, finished diamonds, textiles, plastics, petroleum products, and non-ferrous metals. Since 1922, Belgium and Luxembourg have been a single trade market within a customs and currency union—the Belgium–Luxembourg Economic Union. Its main trading partners are Germany, the Netherlands, France, the United Kingdom, Italy, the United States, and Spain.\n\nAntwerp is the number one diamond market in the world, diamond exports account for roughly 1/10 of Belgian exports. The Antwerp-based BASF plant is the largest BASF-base outside Germany, and accounts on its own for about 2% of Belgian exports. Other industrial and service activities in Antwerp include car manufacturing, telecommunications, photographic products.\n\nFlanders is home to several science and technology institutes, such as IMEC, VITO, Flanders DC and Flanders DRIVE.\n\nFlanders has developed an extensive transportation infrastructure of ports, canals, railways and highways. The Port of Antwerp is the second-largest in Europe, after Rotterdam. Other ports are Bruges-Zeebrugge, Ghent and Ostend, of which Bruges and Ostend are located at the Belgian coast.\n\nWhereas railways are managed by the federal National Railway Company of Belgium, other public transport (De Lijn) and roads are managed by the Flemish region.\n\nThe main airport is Brussels Airport, the only other civilian airport with scheduled services in Flanders is Antwerp International Airport, but there are two other ones with cargo or charter flights: Ostend-Bruges International Airport and Kortrijk-Wevelgem International Airport, both in West Flanders.\n\nThe highest population density is found in the area circumscribed by the Brussels-Antwerp-Ghent-Leuven agglomerations that surround Mechelen and is known as the Flemish Diamond, in other important urban centres as Bruges and Kortrijk to the west, and notable centres Turnhout and Hasselt to the east. On 1 January 2015, the Flemish Region had a population of 6,444,127 and about 15% of the 1,175,173 people in the Brussels Region are also considered Flemish.\n\nThe (Belgian) \"laicist\", or secularist, constitution provides for freedom of religion, and the various governments in general respect this right in practice. Since independence, Catholicism, counterbalanced by strong freethought movements, has had an important role in Belgium's politics, since the 20th century in Flanders mainly via the Christian trade union ACV and the Christian Democratic and Flemish party (CD&V). According to the \"2001 Survey and Study of Religion\", about 47 percent of the Belgian population identify themselves as belonging to the Catholic Church, while Islam is the second-largest religion at 3.5 percent. A 2006 inquiry in Flanders, considered more religious than Wallonia, showed that 55% considered themselves religious, and 36% believed that God created the world.\n\nJews have been present in Flanders for a long time, in particular in Antwerp. More recently, Muslims have immigrated to Flanders, now forming the largest minority religion with about 3.9% in the Flemish Region and 25% in Brussels. The largest Muslim group is the Moroccans, while the second largest is the Turks.\n\nEducation is compulsory from the ages of six to 18, but most Flemings continue to study until around 23. Among the Organisation for Economic Co-operation and Development countries in 1999, Flanders had the third-highest proportion of 18- to 21-year-olds enrolled in postsecondary education. Flanders also scores very high in international comparative studies on education. Its secondary school students consistently rank among the top three for mathematics and science. However, the success is not evenly spread: ethnic minority youth score consistently lower, and the difference is larger than in most comparable countries.\n\nMirroring the historical political conflicts between the freethought and Catholic segments of the population, the Flemish educational system is split into a secular branch controlled by the communities, the provinces, or the municipalities, and a subsidised religious—mostly Catholic—branch. For the subsidised schools, the main costs such as the teacher's wages and building maintenance completely borne by the Flemish government. Subsidised schools are also free to determine their own teaching and examination methods, but in exchange, they must be able to prove that certain minimal terms are achieved by keeping records of the given lessons and exams. It should however be noted that—at least for the Catholic schools—the religious authorities have very limited power over these schools, neither do the schools have a lot of power on their own. Instead, the Catholic schools are a member of the Catholic umbrella organisation VSKO. The VSKO determines most practicalities for schools, like the advised schedules per study field. However, there's freedom of education in Flanders, which doesn't only mean that every pupil can choose his/her preferred school, but also that every organisation can found a school, and even be subsidised when abiding the different rules. This resulted also in some smaller school systems follow 'methodical pedagogies' (e.g. Steiner, Montessori, or Freinet) or serve the Jewish and Protestant minorities.\n\nDuring the school year 2003–2004, 68.30% of the total population of children between the ages of six and 18 went to subsidized private schools (both religious schools or 'methodical pedagogies' schools).\n\nThe big freedom given to schools results in a constant competition to be the \"best\" school. The schools get certain reputations amongst parents and employers. So it's important for schools to be the best school since the subsidies depend on the number of pupils. This competition has been pinpointed as one of the main reasons for the high overall quality of the Flemish education. However, the importance of a school's reputation also makes schools more eager to expel pupils that don't perform well. Resulting in the ethnic differences and the well-known waterfall system: pupils start high in the perceived hierarchy, and then drop towards more professional oriented directions or \"easier\" schools when they can't handle the pressure any longer.\n\nHealthcare is a federal matter, but the Flemish Government is responsible for care, health education and preventive care.\n\nAt first sight, \"Flemish culture\" is defined by the Dutch language and its gourmandic mentality, as compared to the more Calvinistic Dutch culture. Dutch and Flemish paintings enjoyed more equal international admiration.\n\nThe standard language in Flanders is Dutch; spelling and grammar are regulated by a single authority, the Dutch Language Union (\"Nederlandse Taalunie\"), comprising a committee of ministers of the Flemish and Dutch governments, their advisory council of appointed experts, a controlling commission of 22 parliamentarians, and a secretariate. The term Flemish can be applied to the Dutch spoken in Flanders; it shows many regional and local variations.\n\nThe biggest difference between Belgian Dutch and Dutch used in the Netherlands is in the pronunciation of words. The Dutch spoken in the north of the Netherlands is typically described as being \"sharper\", while Belgian Dutch is \"softer\". In Belgian Dutch, there are also less vowels pronounced as diphthongs. When it comes to spelling, Belgian Dutch language purists historically avoided writing words using a French spelling, or search for specific translations of words derived from French. While the Dutch prefer to stick with French spelling, as it differentiates Dutch more from the neighbouring German. For example, the Dutch word \"punaise\" (English: \"Drawing pin\") is derived directly from the French language. Belgian Dutch language purists have lobbied to accept the word \"duimspijker\" (literally: \"thumb spike\") as official Dutch, though the Dutch Language Union never accepted it as standard Dutch. Other proposals by purists were sometimes accepted, and sometimes reverted again in later spelling revisions. As language purists were quite often professionally involved in language (f.e. as a teacher), these unofficial purist translations are found more often in Belgian Dutch texts.\n\nLiterature in non-standardized dialects of the current area of Flanders originated with Hendrik van Veldeke's \"Eneas Romance\", the first courtly romance in a Germanic language (12th century). With a writer of Hendrik Conscience's stature, Flemish literature rose ahead of French literature in Belgium's early history. Guido Gezelle not only explicitly referred to his writings as Flemish but actually used it in many of his poems, and strongly defended it:\nOriginal \n<poem>\n\"Gij zegt dat ’t vlaamsch te niet zal gaan:\"\n\"’t en zal!\"\n\"dat ’t waalsch gezwets zal boven slaan:'\n\"’t en zal!\"\n\"Dat hopen, dat begeren wij:\"\n\"dat zeggen en dat zweren wij:\"\n\"zoo lange als wij ons weren, wij:\"\n\"’t en zal, ’t en zal,\"\n\"’t en zal!\"\n</poem>\n\nTranslation . For explanations, continue along\n\"It shan't!\"\n\"This we hope, for this we hanker:\"\n\"this we say and this we vow:\"\n\"as long as we fight back, we:\"\n\"It shan't, It shan't,\"\n\"It shan't!\"\n</poem>\nThe distinction between Dutch and Flemish literature, often perceived politically, is also made on intrinsic grounds by some experts such as Kris Humbeeck, professor of Literature at the University of Antwerp. Nevertheless, most Dutch-language literature read (and appreciated to varying degrees) in Flanders is the same as that in the Netherlands.\n\nInfluential Flemish writers include Ernest Claes, Stijn Streuvels and Felix Timmermans. Their novels mostly describe rural life in Flanders in the 19th century and at beginning of the 20th. Widely read by the older generations, they are considered somewhat old-fashioned by present-day critics. Some famous Flemish writers of the early 20th century wrote in French, including Nobel Prize winners (1911) Maurice Maeterlinck and Emile Verhaeren. They were followed by a younger generation, including Paul van Ostaijen and Gaston Burssens, who \"activated\" the Flemish Movement. Still widely read and translated into other languages (including English) are the novels of authors such as Willem Elsschot, Louis Paul Boon and Hugo Claus. The recent crop of writers includes the novelists Tom Lanoye and Herman Brusselmans, and poets such as the married couple Herman de Coninck and Kristien Hemmerechts.\n\nAt the creation of the Belgian state, French was the only official language. French was during a long period used as a second language in Flanders and, like elsewhere in Europe, commonly spoken among the aristocracy. There is still a French-speaking minority in Flanders, especially in the municipalities with language facilities, along the language border and the Brussels periphery (Vlaamse Rand), though many of them are French-speakers that migrated to Flanders in recent decades. French is the primary language in the officially bilingual Brussels Capital Region, (see Francization of Brussels). In French Flanders, French is now the native language of the majority of the population and the only official language. Historically it was a Dutch-speaking region and there is still a minority of Dutch-speakers living there.\n\nMany Flemings are also able to speak French, children in Flanders generally get their first French lessons in the 5th primary year (normally around 10 years). But the current lack of French outside the educational context makes it hard to maintain a decent level of French. As such, the proficiency of French is declining. Flemish pupils are also obligated to follow English lessons as their third language. Normally from the second secondary year (around 14 years old), but the ubiquity of English in movies, music, IT and even advertisements makes it easier to learn and maintain the English language. This makes the Flemish people very proficient in English (in Europe, only Sweden and Malta have a better knowledge of English as a second language).\n\nThe public radio and television broadcaster in Flanders is VRT, which operates the TV channels één, Canvas, Ketnet, OP12 and (together with the Netherlands) BVN. Flemish provinces each have up to two TV channels as well. Commercial television broadcasters include vtm and Vier (VT4). Popular TV series are for example \"Thuis\" and \"F.C. De Kampioenen\".\n\nThe five most successful Flemish films were \"Loft\" (2008; 1,186,071 visitors), \"Koko Flanel\" (1990; 1,082,000 tickets sold), \"Hector\" (1987; 933,000 tickets sold), \"Daens\" (1993; 848,000 tickets sold) and \"De Zaak Alzheimer\" (2003; 750,000 tickets sold). The first and last ones were directed by Erik Van Looy, and an American remake is being made of both of them, respectively \"The Loft\" (2012) and \"The Memory of a Killer\". The other three ones were directed by Stijn Coninx.\n\nNewspapers are grouped under three main publishers: De Persgroep with Het Laatste Nieuws, the most popular newspaper in Flanders, De Morgen and De Tijd. Then Corelio with De Gentenaar, the oldest extant Flemish newspaper, Het Nieuwsblad and De Standaard. Lastly, Concentra publishes Gazet van Antwerpen and Het Belang van Limburg.\n\nMagazines include Knack and HUMO.\n\nAssociation football (soccer) is one of the most popular sports in both parts of Belgium, together with cycling, tennis, swimming and judo.\n\nIn cycling, the Tour of Flanders is considered one of the five \"Monuments\". Other \"Flanders Classics\" races include \"Dwars door Vlaanderen\" and Gent–Wevelgem. Eddy Merckx is regarded as one of the greatest cyclists of all time, with five victories in the Tour de France and numerous other cycling records. His hour speed record (set in 1972) stood for 12 years.\n\nJean-Marie Pfaff, a former Belgian goalkeeper, is considered one of the greatest in the history of football (soccer).\n\nKim Clijsters (as well as the French-speaking Belgian Justine Henin) was Player of the Year twice in the Women's Tennis Association as she was ranked the number one female tennis player.\n\nKim Gevaert and Tia Hellebaut are notable track and field stars from Flanders.\n\nThe 1920 Summer Olympics were held in Antwerp. Jacques Rogge has been president of the International Olympic Committee since 2001.\n\nThe Flemish government agency for sports is Bloso.\n\nFlanders is known for its music festivals, like the annual Rock Werchter, Tomorrowland and Pukkelpop. The Gentse Feesten are another very large yearly event.\n\nThe best-selling Flemish group or artist is the (Flemish-Dutch) group 2 Unlimited, followed by (Italian-born) Rocco Granata, Technotronic, Helmut Lotti and Vaya Con Dios.\n\nThe weekly charts of best-selling singles is the Ultratop 50. \"Kvraagetaan\" by the Fixkes holds the current record for longest time at #1 on the chart.\n"}
{"id": "10879", "url": "https://en.wikipedia.org/wiki?curid=10879", "title": "Freud (disambiguation)", "text": "Freud (disambiguation)\n\nSigmund Freud (1856–1939) was the inventor of psychoanalysis, psychosexual stages, and the personality theory of Ego, Superego, and Id.\n\nFreud may also refer to:\n\nThe Freud family:\n\n"}
{"id": "10880", "url": "https://en.wikipedia.org/wiki?curid=10880", "title": "Plurality voting", "text": "Plurality voting\n\nPlurality voting is an electoral system in which each voter is allowed to vote for only one candidate, and the candidate who polls the most among their counterparts (a plurality) is elected. In a system based on single-member districts, it may be called first-past-the-post (FPTP), single-choice voting, simple plurality or relative/simple majority. In a system based on multi-member districts, it may be referred to as winner-takes-all or bloc voting. The system is often used to elect members of a legislative assembly or executive officers. It is the most common form of the system, and is used in Canada, the lower house (Lok Sabha) in India, most elections in the United Kingdom (excluding some Scottish and Northern Irish elections), and most elections in the United States.\n\nPlurality voting is distinguished from a majoritarian electoral system, in which, to win, a candidate must receive an absolute majority of votes—i.e. more votes than all other candidates combined. Both systems may use single-member or multi-member constituencies, In the latter case it may be referred to as an exhaustive counting system: one member is elected at a time and the process repeated until the number of vacancies is filled.\n\nIn some countries such as France (as well as in some jurisdictions of the United States, such as Louisiana and Georgia) a \"two-ballot\" or \"runoff election\" plurality system is used. This may require two rounds of voting. If on the first round no candidate receives over 50% of the votes, then a second round takes place, with just the two highest-voted candidates in the first round. This ensures that the winner gains a majority of votes in the second round. Alternatively, all candidates above a certain threshold in the first round may compete in the second round. If there are more than two candidates standing, then a plurality vote may decide the result.\n\nIn political science, the use of plurality voting with multiple, single-winner constituencies to elect a multi-member body is often referred to as single-member district plurality or SMDP. This combination is also variously referred to as winner-takes-all to contrast it with proportional representation systems. This term is sometimes also used to refer to elections for multiple winners in a particular constituency using bloc voting.\n\nMost experts group electoral systems into three general categories:\n\nPlurality voting is used for local and/or national elections in 43 of the 193 countries that are members of the United Nations. Plurality voting is particularly prevalent in the United Kingdom and former British colonies, including the United States, Canada and India.\n\nIn single winner plurality voting, each voter is allowed to vote for only one candidate, and the winner of the election is whichever candidate represents a plurality of voters, that is, whoever received the largest number of votes. This makes plurality voting among the simplest of all electoral systems for voters and vote counting officials. (However the drawing of district boundary lines can be very contentious in this system.)\n\nIn an election for a legislative body, with single-member seats, each voter in a given geographically-defined electoral district is entitled to vote for one candidate from a list of candidates competing to represent that district. Under the plurality system, the winner of the election then becomes the representative of the entire electoral district, and serves with representatives of other electoral districts.\n\nIn an election for a single seat, such as for president in a presidential system, the same style of ballot is used and the winner is the candidate who receives the largest number of votes.\n\nIn the two-round system, usually the two highest polling candidates in the first ballot progress to the second round Run-off ballot.\n\nIn a multiple member plurality election, with \"n\" seats available, the winners are the \"n\" candidates with the highest numbers of votes. The rules may allow the voter to vote for one candidate, or for up to \"n\" candidates, or maybe some other number.\n\nGenerally plurality ballots can be categorized into two forms. The simplest form is a blank ballot where the name of a candidate(s) is written in by hand. A more structured ballot will list all the candidates and allow a mark to be made next to the name of a single candidate (or more than one, in some cases); however a structured ballot can also include space for a write-in candidate.\n\nThe United Kingdom, like the United States and Canada, uses single-member districts as the base for national elections. Each electoral district (constituency) chooses one member of parliament, i.e. the candidate that gets the most votes, whether or not he/she gets 50% or more of the votes cast (\"first past the post\"). In 1992, for example, a Liberal Democrat in Scotland won a seat with just 26% of the votes. This system of single-member districts with plurality winners tends to produce two large political parties. (In countries with proportional representation there is not such a great incentive to vote for a large party, and that contributes to multi-party systems.)\n\nScotland, Wales and Northern Ireland use the first past the post system for UK general elections, but use versions of proportional representation for elections to their own assemblies/parliaments. All of the UK has used a form of proportional representation for European Parliament elections.\n\nThe countries that inherited the British majoritarian system tend toward two large parties: one left, the other right, such as the U.S. Democrats and Republicans. Canada is an exception, with three major political parties consisting of the New Democratic Party which is to the left, the Conservative Party which is to the right and the Liberal Party which is slightly off center to the left. A fourth party that no longer has major party status is the separatist Bloc Québécois party, which is territorial and concentrated in Quebec. New Zealand used the British system, and it too yielded two large parties. It also left many New Zealanders unhappy, because other viewpoints were ignored, so its parliament in 1993 adopted a new electoral law, modelled on Germany's system of proportional representation (PR) with a partial selection by constituencies. New Zealand soon developed a more complex party system.\n\nAfter the 2015 Elections in the United Kingdom, there were calls from UKIP to change to proportional representation after receiving 3,881,129 votes but only 1 MP. The Green Party was similarly under-represented. This contrasted greatly with the SNP in Scotland who only received 1,454,436 votes but won 56 seats, due to more concentrated support.\n\nIf each voter in each city naively selects one city on the ballot (Memphis voters select Memphis, Nashville voters select Nashville, and so on), then Memphis will be selected, as it has the most votes (42%). Note that this system does not require that the winner have a majority but only a plurality. Memphis wins because it has the most votes, even though 58% of the voters in this example preferred Memphis least. Notice that this problem does not hold anymore in the two-round system, in which Nashville would have won. (In practice, with FPTP, many voters in Chattanooga and Knoxville are likely to vote tactically for Nashville: see below.)\n\nTo a much greater extent than many other electoral methods, plurality electoral systems encourage tactical voting techniques, like \"compromising\". Voters are pressured to vote for one of the two candidates they predict are most likely to win, even if their true preference is neither, because a vote for any other candidate will likely have no impact on the final result. Any other party will typically need to build up its votes and credibility over a series of elections before it is seen as electable.\n\nIn the Tennessee example, if all the voters for Chattanooga and Knoxville had instead voted for Nashville, then Nashville would have won (with 58% of the vote); this would only have been the 3rd choice for those voters, but voting for their respective 1st choices (their own cities) actually results in their 4th choice (Memphis) being elected.\n\nThe difficulty is sometimes summed up, in an extreme form, as \"All votes for anyone other than the second place are votes for the winner\", because by voting for other candidates, they have denied those votes to the second place candidate who could have won had they received them. It is often claimed by United States Democrats that Democrat Al Gore lost the 2000 Presidential Election to Republican George W. Bush because some voters on the left voted for Ralph Nader of the Green Party, who exit polls indicated would have preferred Gore at 45% to Bush at 27%, with the rest not voting in Nader's absence.\n\nSuch a mentality is reflected by elections in Puerto Rico and its three principal voter groups: the Independentistas (pro-independence), the Populares (pro-commonwealth), and the Estadistas (pro-statehood). Historically, there has been a tendency for Independentista voters to elect Popular candidates and policies. This phenomenon is responsible for some Popular victories, even though the Estadistas have the most voters on the island. It is so widely recognised that the Puerto Ricans sometimes call the Independentistas who vote for the Populares \"melons\", because the fruit is green on the outside but red on the inside (in reference to the party colors).\n\nBecause voters have to predict in advance who the top two candidates will be, this can cause significant perturbation to the system:\n\n\nProponents of other single-winner electoral systems argue that their proposals would reduce the need for tactical voting and reduce the spoiler effect. Examples include the commonly used two-round system of runoffs and instant runoff voting, along with less tested systems such as approval voting, score voting and Condorcet methods.\n\nDuverger's law is a theory that constituencies that use first-past-the-post systems will have a two-party system, given enough time.\n\nFirst-past-the-post tends to reduce the number of political parties to a greater extent than most other methods do, making it more likely that a single party will hold a majority of legislative seats. (In the United Kingdom, 21 out of 24 General Elections since 1922 have produced a single-party majority government.)\n\nFPTP's tendency toward fewer parties and more frequent one-party rules can also produce government that may not consider as wide a range of perspectives and concerns. It is entirely possible that a voter finds all major parties to have similar views on issues and that a voter does not have a meaningful way of expressing a dissenting opinion through his vote.\n\nAs fewer choices are offered to voters, voters may vote for a candidate although they disagree with him, because they disagree even more with his opponents. Consequently, candidates will less closely reflect the viewpoints of those who vote for them.\n\nFurthermore, one-party rule is more likely to lead to radical changes in government policy even though the changes are favoured only by a plurality or a bare majority of the voters, whereas a multi-party system usually require greater consensus in order to make dramatic changes in policy.\n\nWasted votes are votes cast for losing candidates or votes cast for winning candidates in excess of the number required for victory. For example, in the UK General Election of 2005, 52% of votes were cast for losing candidates and 18% were excess votes—a total of 70% wasted votes. This is perhaps the most fundamental criticism of FPTP, that a large majority of votes may play no part in determining the outcome. Alternative electoral systems attempt to ensure that almost all votes are effective in influencing the result and the number of wasted votes is consequently minimised.\n\nBecause FPTP permits a high level of wasted vote, an election under FPTP is easily gerrymandered unless safeguards are in place. In gerrymandering, constituencies are deliberately designed to unfairly increase the number of seats won by one party at the expense of another.\n\nIn brief, suppose that governing party G wishes to reduce the seats that will be won by opposition party O in the next election. It creates a number of constituencies in each of which O has an overwhelming majority of votes. O will win these seats, but a large number of its voters will waste their votes. Then the rest of the constituencies are designed to have small majorities for G. Few G votes are wasted, and G will win a large number of seats by small margins. As a result of the gerrymander, O's seats have cost it more votes than G's seats.\n\nThe presence of spoilers often gives rise to suspicions that manipulation of the slate has taken place. The spoiler may have received incentives to run. A spoiler may also drop out at the last moment, inducing charges that such an act was intended from the beginning.\n\nThe spoiler effect is the effect of vote splitting between candidates or ballot questions with similar ideologies. One <nowiki>spoiler candidate'</nowiki>s presence in the election draws votes from a major candidate with similar politics thereby causing a strong opponent of both or several to win. Smaller parties can disproportionately change the outcome of an FPTP election by swinging what is called the 50-50% balance of two party systems, by creating a faction within one or both ends of the political spectrum which shifts the winner of the election from an absolute majority outcome to a simple majority outcome favouring the previously less favoured party. In comparison, for electoral systems using proportional representation small groups win only their proportional share of representation.\n\nIn August 2008, Sir Peter Kenilorea commented on what he perceived as the flaws of a first-past-the-post electoral system in the Solomon Islands:\nThe arguments for plurality voting rely on the preservation of the \"one person, one vote\" principle (also \"one man, one vote\", or OMOV, or more recently \"one member, one vote\"), as cited by the Supreme Court of the United States, wherein each voter is only able to cast one vote in a given election, where that vote can only go to one candidate. Plurality voting elects the candidate who is preferred first by the largest number of voters, although this need not be an absolute majority. Other electoral systems, such as instant-runoff voting, party-list proportional representation or single transferable vote also preserve OMOV, but it is not as obvious that they do so, because they rely on lower voter preference to enable a candidate to earn either an absolute majority (single member district) or a quota (multi-member district), respectively.\n\nPlurality is often conflated with single-winner electoral systems in general, in order to contrast it with proportional representation. In this context, it shares advantages, such as local accountability, with other single-winner systems.\n\nThe United Kingdom continues to use the first-past-the-post electoral system for general elections, and for local government elections in England and Wales. Changes to the UK system have been proposed, and alternatives were examined by the Jenkins Commission in the late 1990s. After the formation of a new coalition government in 2010, it was announced as part of the coalition agreement that a referendum would be held on switching to the alternative vote system. However the alternative vote system was rejected 2-1 by British voters in a referendum held on 5 May 2011.\n\nCanada also uses FPTP for national and provincial elections. In May 2005 the Canadian province of British Columbia had a referendum on abolishing single-member district plurality in favour of multi-member districts with the Single Transferable Vote system after the Citizens' Assembly on Electoral Reform made a recommendation for the reform. The referendum obtained 57% of the vote, but failed to meet the 60% requirement for passing. An October 2007 referendum in the Canadian province of Ontario on adopting a Mixed Member Proportional system, also requiring 60% approval, failed with only 36.9% voting in favour.\n\nNorthern Ireland, Scotland, Wales, the Republic of Ireland, Australia and New Zealand are notable examples of countries within the UK, or with previous links to it, that use non-FPTP electoral systems (Northern Ireland, Scotland and Wales use FPTP in United Kingdom general elections, however).\n\nNations which have undergone democratic reforms since 1990 but have not adopted the FPTP system include South Africa, almost all of the former Eastern bloc nations, Russia, Afghanistan and Iraq.\n\nCountries that use plurality voting to elect the lower or only house of their legislature include:\n"}
{"id": "10881", "url": "https://en.wikipedia.org/wiki?curid=10881", "title": "Fetish", "text": "Fetish\n\nFetish may refer to:\n\n\n\n"}
{"id": "10882", "url": "https://en.wikipedia.org/wiki?curid=10882", "title": "February 14", "text": "February 14\n\n\n\n"}
{"id": "10883", "url": "https://en.wikipedia.org/wiki?curid=10883", "title": "Free-trade area", "text": "Free-trade area\n\nA free-trade area is the region encompassing a trade bloc whose member countries have signed a free-trade agreement (FTA). Such agreements involve cooperation between at least two countries to reduce trade barriers import quotas and tariffs and to increase trade of goods and services with each other.\nIf people are also free to move between the countries, in addition to a free-trade agreement, it would also be considered an open border. It can be considered the second stage of economic integration.\n\nUnlike a customs union (the third stage of economic integration), members of a free trade area do not have a common external tariff, which means they have different quotas and customs taxes, as well as other policies with respect to non-members. To avoid tariff evasion (through re-exportation) the countries use the system of certification of origin most commonly called rules of origin, where there is a requirement for the minimum extent of local material inputs and local transformations adding value to the goods. Only goods that meet these minimum requirements are entitled to the special treatment envisioned by the free trade area provisions.\n\n\"Cumulation\" is the relationship between different FTAs regarding the rules of origin sometimes different FTAs supplement each other, in other cases there is no cross-cumulation between the FTAs. A free-trade area is a result of a free-trade agreement (a form of trade pact) between two or more countries. Free-trade areas and agreements (FTAs) are cascadable to some degree if some countries sign agreements to form a free-trade area and choose to negotiate together (either as a trade bloc or as a forum of individual members of their FTA) another free-trade agreement with another country (or countries) then the new FTA will consist of the old FTA plus the new country (or countries).\n\nWithin an industrialized country there are usually few if any significant barriers to the easy exchange of goods and services between parts of that country. For example, there are usually no trade tariffs or import quotas; there are usually no delays as goods pass from one part of the country to another (other than those that distance imposes); there are usually no differences of taxation and regulation. Between countries, on the other hand, many of these barriers to the easy exchange of goods often do occur. It is commonplace for there to be import duties of one kind or another (as goods enter a country) and the levels of sales tax and regulation often vary by country.\n\nThe aim of a free-trade area is to reduce barriers to exchange so that trade can grow as a result of specialisation, division of labour, and most importantly via comparative advantage. The theory of comparative advantage argues that in an unrestricted marketplace (in equilibrium) each source of production will tend to specialize in that activity where it has comparative (rather than absolute) advantage. The theory argues that the net result will be an increase in income and ultimately wealth and well-being for everyone in the free-trade area. The theory refers only to aggregate wealth and says nothing about the distribution of wealth; in fact there may be significant losers, in particular among the recently protected industries with a comparative disadvantage. The overall gains from trade can be used to compensate for the effects of reduced trade barriers by appropriate inter-party transfers.\n\n\nEvery customs union, trade common market, economic union, customs and monetary union and economic and monetary union also has a free-trade area, but these are each listed in their own articles only.\n\n\n\n"}
{"id": "10885", "url": "https://en.wikipedia.org/wiki?curid=10885", "title": "French fries", "text": "French fries\n\nFrench fries (North American English (American/Canadian)), chips (British English), fries, finger chips (Indian English), or French-fried potatoes are \"batonnet\" or \"allumette\"-cut deep-fried potatoes. In the United States and most of Canada, the term \"fries\" refers to all dishes of fried elongated pieces of potatoes, while in the United Kingdom, Australia, South Africa (rarely), Ireland and New Zealand, thinly cut fried potatoes are sometimes called \"shoestring fries\" or \"skinny fries\" to distinguish them from the thicker-cut \"chips\".\n\nFrench fries are served hot, either soft or crispy, and are generally eaten as part of lunch or dinner or by themselves as a snack, and they commonly appear on the menus of diners, fast food restaurants, pubs, and bars. Fries in America are generally salted and are almost always served with ketchup, but in many countries they have other condiments or toppings, like vinegar, mayonnaise, or other local specialties. Fries can be topped more heavily, as in the dishes of poutine and chili cheese fries. French fries can be made from sweet potatoes instead of potatoes. A baked variant of the French fry (\"chunky oven chips\") uses less or even no oil.\n\nFrench fries are prepared by first peeling and cutting the potato into even strips. These are then wiped off or soaked in cold water to remove the surface starch, and thoroughly dried. They may then be fried in one or two stages. Chefs generally agree that the \"two-bath\" technique produces better results.\n\nIn the two-stage or two-bath method, the first bath, sometimes called blanching, is in hot fat (around 160 °C / 320 °F) to cook them through. This may be done in advance. Then they are more briefly fried in very hot fat (190 °C / 375 °F) to crisp the exterior. They are then placed in a colander or on a cloth to drain, salted, and served. The exact times of the two baths depend on the size of the potatoes. For example, for 2–3mm strips, the first bath takes about 3 minutes, and the second bath takes only seconds.\n\nMost French fries are produced from frozen potatoes which have been blanched or at least air-dried industrially. The usual fat for making French fries is vegetable oil. In the past, beef suet was recommended as superior, with vegetable shortening as an alternative. In fact, McDonald's used a mixture of 93% beef tallow and 7% cottonseed oil until 1990, when they switched to vegetable oil with beef flavoring.\n\nThomas Jefferson had \"potatoes served in the French manner\" at a White House dinner in 1802. The expression \"French fried potatoes\" first occurred in print in English in the 1856 work \"Cookery for Maids of All Work\" by E. Warren: \"French Fried Potatoes. – Cut new potatoes in thin slices, put them in boiling fat, and a little salt; fry both sides of a light golden brown colour; drain.\" It is apparent that this account refers to thin, shallow-fried slices of potato – it is not clear where or when the now familiar deep-fried batons or fingers of potato were first prepared. In the early 20th century, the term \"French fried\" was being used in the sense of \"deep-fried\" for foods like onion rings or chicken. It is unlikely that \"French fried\" refers to \"frenching\" in the sense of \"julienning\", which is not attested until after \"French fried potatoes\". Previously, \"frenching\" referred only to trimming meat off the shanks of chops.\n\nSome claim that fries originated in Belgium; there is an ongoing dispute between the French and Belgians about where they were invented, with both countries claiming ownership. From the Belgian standpoint the popularity of the term \"French fries\" is explained as a \"French gastronomic hegemony\" into which the cuisine of Belgium was assimilated because of a lack of understanding coupled with a shared language and geographic proximity between the two countries.\n\nBelgian journalist Jo Gérard claims that a 1781 family manuscript recounts that potatoes were deep-fried prior to 1680 in the Meuse valley, in what was then the Spanish Netherlands (present-day Belgium): \"The inhabitants of Namur, Andenne, and Dinant had the custom of fishing in the Meuse for small fish and frying, especially among the poor, but when the river was frozen and fishing became hazardous, they cut potatoes in the form of small fish and put them in a fryer like those here.\"\nGérard has not produced the manuscript that supports this claim, which, even if true, is unrelated to the later history of the French fry, as the potato did not arrive in the region until around 1735. Also, given 18th century economic conditions: \"It is absolutely unthinkable that a peasant could have dedicated large quantities of fat for cooking potatoes. At most they were sautéed in a pan...\".\n\nSome people believe that the term \"French fries\" for deep fried potato batons was introduced when American soldiers arrived in Belgium during World War I. The Belgians had previously been catering to the British soldiers' love of chips and continued to serve them to the Americans when they took over the western end of the front. The Americans supposedly took them to be French fried potatoes because they believed themselves to be in France, French being the local language and the official language of the Belgian Army at that time. At that time, the term \"French fries\" was growing in popularity – the term was already used in America as early as 1899 – although it isn't clear whether this referred to batons (chips) or slices of potato e.g. in an item in \"Good Housekeeping\" which specifically references \"Kitchen Economy in France\": \"The perfection of French fries is due chiefly to the fact that plenty of fat is used\".\n\n\"\"Pommes frites\"\" or just \"\"frites\"\" (French), \"\"frieten\"\" (Flemish) or \"\"patat\"\" (Dutch) became the national snack and a substantial part of several national dishes, such as Moules-frites or Steak-frites.\n\nIn France and other French-speaking countries, fried potatoes are formally \"pommes de terre frites\", but more commonly \"pommes frites\", \"patates frites\", or simply \"frites\". The words \"aiguillettes\" (\"needle-ettes\") or \"allumettes\" (\"matchsticks\") are used when the French fries are very small and thin. One enduring origin story holds that French fries were invented by street vendors on the Pont Neuf bridge in Paris in 1789, just before the outbreak of the French Revolution. However, a reference exists in France from 1775 to \"a few pieces of fried potato\" and to \"fried potatoes\".\n\nEating potatoes for sustenance was promoted in France by Antoine-Augustin Parmentier, but he did not mention \"fried\" potatoes in particular. Many Americans attribute the dish to France and offer as evidence a notation by U.S. President Thomas Jefferson: \"\"Pommes de terre frites à cru, en petites tranches\"\" (\"Potatoes deep-fried while raw, in small slices\") in a manuscript in Thomas Jefferson's hand (circa 1801–1809) and the recipe almost certainly comes from his French chef, Honoré Julien.\nIn addition, from 1813 on, recipes for what can be described as \"French fries\" occur in popular American cookbooks. By the late 1850s, a cookbook was published that used the term \"French fried potatoes\".\n\n\"Frites\" are the main ingredient in the Canadian/Québécois dish known (in both Canadian English and French) as \"poutine\"; a dish consisting of fried potatoes covered with cheese curds and gravy. Poutine has a growing number of variations but is generally considered to have been developed in rural Québec sometime in the 1950s, although precisely where in the province it first appeared is a matter of contention.\n\nIn Spain, fried potatoes are called \"patatas fritas\" or \"papas fritas\". Another common form, involving larger irregular cuts, is \"patatas bravas\". The potatoes are cut into big chunks, partially boiled and then fried. They are usually seasoned with a spicy tomato sauce, and the dish is one of the most preferred tapas by Spaniards. Fries may have been invented in Spain, the first European country in which the potato appeared from the New World colonies, and assume fries' first appearance to have been as an accompaniment to fish dishes in Galicia, from which it spread to the rest of the country and then further away, to the \"Spanish Netherlands\", which became Belgium more than a century later.\n\nProfessor Paul Ilegems, curator of the Frietmuseum in Bruges, Belgium, believes that Saint Teresa of Ávila of Spain cooked the first French fries, and refers also to the tradition of frying in Mediterranean cuisine as evidence.\n\nThe J. R. Simplot Company is credited with successfully commercializing French fries in frozen form during the 1940s. Subsequently, in 1967, Ray Kroc of McDonald's contracted the Simplot company to supply them with frozen fries, replacing fresh-cut potatoes as an ingredient. In 2004, 29% of the United States' potato crop was used to make frozen fries – 90% consumed by the food services sector and 10% by retail. Meanwhile, in the UK, it is estimated that 80% of households buy frozen fries each year.\n\nFries are very popular in Belgium, where they are known as \"frieten\" (in Dutch) or \"frites\" (in French), and the Netherlands, where among the working classes they are known as \"patat\" in the north and, in the south, \"friet\". In Belgium, fries are sold in shops called \"friteries\" (French), \"frietkot\"/\"frituur\" (Dutch), or \"Fritüre\"/\"Frittüre\" (German). They are served with a large variety of Belgian sauces and eaten either on their own or with other snacks. Traditionally fries are served in a \"cornet de frites\" (French), \"patatzak\"/\"frietzak\"/\"fritzak\" (Dutch/Flemish), or \"Frittentüte\" (German), a white cardboard cone, then wrapped in paper, with a spoonful of sauce (often mayonnaise) on top. They may also be served with other traditional fast food items, such as \"frikandel\", burgers, fishsticks, meatballs or croquette. In the Netherlands, fries are sold at snack bars and often served a sauce like Fritessaus or curry ketchup.\n\n\"Friteries\" and other fast food establishments tend to offer a number of different sauces for the fries and meats. In addition to ketchup and mayonnaise, popular options include: aioli, sauce andalouse, sauce Americaine, \"Bicky\" Dressing (Gele Bicky-sauce), curry mayonnaise, mammoet-sauce, peanut sauce, samurai-sauce, sauce \"Pickles\", pepper-sauce, tartar sauce, zigeuner sauce, and À la zingara. These sauces are generally also available in supermarkets. In addition to this, hot sauces are sometimes offered by friteries, including hollandaise sauce, sauce provençale, Béarnaise sauce, or a splash of carbonade flamande stew from a constantly simmering pot, in the spirit of British \"chips and gravy\".\n\nThe town of Florenceville-Bristol, New Brunswick, headquarters of McCain Foods, calls itself \"the French fry capital of the world\" and also hosts a museum about potatoes called \"Potato World\". It is also one of the world's largest manufacturers of frozen French fries and other potato specialties. In the Canadian province of Quebec, French fries are often served with cheese curds and hot brown gravy, a dish called poutine.\n\nThe thick-cut fries are called \"Pommes Pont-Neuf\" or simply \"pommes frites (\"about 10 mm); thinner variants are \"pommes allumettes\" (matchstick potatoes; about 7 mm), and \"pommes paille\" (potato straws; 3–4 mm). (Roughly 0.4, 0.3 and 0.15 inch respectively.) \"Pommes gaufrettes\" are waffle fries. A popular dish in France is steak-frites, which is steak accompanied by thin French fries.\n\nFrench fries migrated to the German-speaking countries during the 19th century. In Germany, where they are usually known by the French words \"pommes frites\", or only \"Pommes\" or \"Fritten\" (derived from the French words but pronounced as German words)\".\" They are often served with mayonnaise, and are a popular walking snack offered by \"Schnellimbiss\" (\"quick bite\") kiosks. Since the advent of \"Currywurst\" in the 1950s, a paper tray of sausage (bratwurst or bockwurst) anointed with curry ketchup, laced with additional curry powder and a side of french fries, has become an immensely popular fast food meal.\n\nIn Denmark, Sweden and Norway, the French name \"pommes frites\" is used for fries. They are the most common form of potatoes when served together with breaded plaice (or certain other low fat fishes). When \"pommes frites\" are served with fish, remoulade and a lemon slice, the plate represents the typical Danish version of fish and chips. \n\"Pommes frites\" are also served across Scandinavia as a small stand-alone dish with ketchup or mayonnaise. Fried sausage (same kind as for hot dogs), hamburgers, or schnitzels may be the meat portion of a dish which includes french fries. \nSome traditional restaurants (as opposed to fast food) may serve french fries. This may be as an accompaniment to an entrecote or other beef steak, together with bearnaise. Better restaurants tend to avoid serving french fries, with the possible exception of fish and chips.\n\nThe standard deep-fried cut potatoes in the United Kingdom are called chips, and are cut into pieces between wide. They are occasionally made from unpeeled potatoes (skins showing). Chips are often less crisp than the continental European \"French fry\", owing to their relatively high water content. British \"chips\" are not the same thing as potato chips (an American term); those are called \"crisps\" in Britain. In the UK, chips are part of the popular, and now international, fast food dish fish and chips.\n\nThe first chips fried in the UK were sold by Mrs. 'Granny' Duce in one of the West Riding towns in 1854. A blue plaque in Oldham marks the origin of the fish-and-chip shop, and thus the start of the fast food industry in Britain. In Scotland, chips were first sold in Dundee: \"in the 1870s, that glory of British gastronomy – the chip – was first sold by Belgian immigrant Edward De Gernier in the city's Greenmarket\". In Ireland the first chip shop was \"opened by Giuseppe Cervi\", an Italian immigrant, \"who arrived there in the 1880s\".\n\nAlthough French fries were a popular dish in most British commonwealth countries, the \"thin style\" French fries have been popularized worldwide in large part by the large American fast food chains like McDonald's, Burger King, and Wendy's. Pre-made French fries have been available for home cooking since the 1960s, having been pre-fried (or sometimes baked), frozen and placed in a sealed plastic bag. Some varieties of French fries that appeared later have been battered and breaded, and many fast food chains in the U.S. dust the potatoes with kashi, dextrin, and other flavor coatings for crispier fries with particular tastes. Results with batterings and breadings, followed by microwaving, have not achieved widespread critical acceptance. Oven-frying yields a dish quite different from deep-fried potatoes.\n\nThere are several variants of French fries. They include (in alphabetical order):\n\nFries tend to be served with a variety of accompaniments, such as salt and vinegar (malt, balsamic or white), pepper, Cajun seasoning, grated cheese, melted cheese, mushy peas, heated curry sauce, curry ketchup (mildly spiced mix of the former), hot sauce, relish, mustard, mayonnaise, bearnaise sauce, tartar sauce, chili, tzatziki, feta cheese, garlic sauce, fry sauce, butter, sour cream, ranch dressing, barbecue sauce, gravy, honey, aioli, brown sauce, ketchup, lemon juice, piccalilli, pickled cucumber, pickled gherkins, pickled onions or pickled eggs.\n\nFrench fries primarily contain carbohydrates from the potato (mostly in the form of starch) and fat absorbed during the deep-frying process, as well as sodium depending upon the seasoning. For example, a large serving of French fries at McDonald's in the United States is 154 grams. Nearly all of the 500 calories per serving come from the 63 g of carbohydrates and the 25 g of fat, but a serving also contains 6 g of protein and 350 mg of sodium.\n\nExpert testimonials:\nFrying French fries in beef tallow, lard, or other animal fats adds saturated fat to the diet. Replacing animal fats with tropical vegetable oils, such as palm oil, simply substitutes one saturated fat for another. Replacing animal fats with partially hydrogenated oil reduces cholesterol but adds trans fat, which has been shown to both raise LDL cholesterol and lower HDL cholesterol. Canola/Rapeseed oil, or sunflower-seed oil are also used, as are mixes of vegetable oils, but beef tallow is generally more popular, especially amongst fast food outlets that use communal oil baths. Accordingly, many restaurants now advertise their use of unsaturated oils; for example, both Five Guys and Chick-fil-A advertise that their fries are prepared with peanut oil.\n\nFrench fries contain some of the highest levels of acrylamides of any foodstuff, and concerns have been raised about the impact of acrylamides on human health. According to the American Cancer Society, it is not clear whether acrylamide consumption affects people's risk of getting cancer.\nA lower-fat method for producing a French fry-like product is to coat \"Frenched\" or wedge potatoes in oil and spices/flavoring before baking them. The heat will not be as high as when deep frying, and this also reduces acrylamides.\n\nIn June 2004, the United States Department of Agriculture (USDA), with the advisement of a federal district judge from Beaumont, Texas, classified batter-coated French fries as a vegetable under the \"Perishable Agricultural Commodities Act\". This was primarily for trade reasons; French fries do not meet the standard to be listed as a processed food. This classification, referred to as the \"French fry rule\", was upheld in the United States Court of Appeals for the Fifth Circuit case \"Fleming Companies, Inc. v. USDA\".\n\nIn the United States, in 2002, the McDonald's Corporation agreed to donate to Hindus and other groups to settle lawsuits filed against the chain for mislabeling French fries and hash browns as vegetarian, because beef extract was added in their production.\n\n"}
{"id": "10886", "url": "https://en.wikipedia.org/wiki?curid=10886", "title": "Field hockey", "text": "Field hockey\n\nField hockey is a team sport of the hockey family. The earliest origins of the game date back to the Middle Ages in England, Scotland, France and the Netherlands. The game can be played on a grass field or a turf field as well as an indoor board surface. Each team plays with eleven players, including the goalie. Players use sticks made out of wood, carbon fibre, fibre glass or a combination of carbon fibre and fibre glass in different quantities (with the higher carbon fibre stick being more expensive and less likely to break) to hit a round, hard, plastic ball. The length of the stick depends on the player's individual height. Only one end of the stick is allowed to be used. Goalies often have a different kind of stick, however they can also use an ordinary field hockey stick. The specific goal-keeping sticks have another curve at the end of the stick, this is to give them more surface area to save the ball. The uniform consists of shin guards, shoes, shorts, a mouth guard and a jersey. Today, the game is played globally, with particular popularity throughout Western Europe, the Indian subcontinent, Southern Africa, Australia, New Zealand, Argentina, and parts of the United States (primarily New England and the Mid-Atlantic states). The term \"field hockey\" is used primarily in Canada and the United States where ice hockey is more popular. In Sweden the term \"landhockey\" is used and to some degree also in Norway. It is a section of Norway's Bandy Association. Until recently they called it \"hockey\", when it was changed to \"landhockey\".\n\nDuring play, goal keepers are the only players who are allowed to touch the ball with any part of their body (the player's hand is considered 'part of the stick' if on the stick), while field players play the ball with the flat side of their stick. Goal keepers also cannot play the ball with the back of their stick. Whoever scores the most goals by the end of the match wins. If the score is tied at the end of the game, either a draw is declared or the game goes into extra time or a penalty shootout, depending on the competition's format. There are many variations to overtime play that depend on the league and tournament play. In college play, a seven-aside overtime period consists of a 10-minute golden goal period with seven players for each team. If a tie still remains, the game enters a one-on-one competition where each team chooses 5 players to dribble from the 25 yard line down to the circle against the opposing goalie. The player has 8 seconds to score on the goalie keeping it in bounds. The play ends after a goal is scored, the ball goes out of bounds, a foul is committed (ending in either a penalty stroke or flick or the end of the one on one) or time expires. If the tie still persists extra rounds thereafter until one team has scored.\n\nThe governing body of hockey is the International Hockey Federation (FIH, in French), with men and women being represented internationally in competitions including the Olympic Games, World Cup, World League, Champions Trophy and Junior World Cup, with many countries running extensive junior, senior, and masters club competitions. The FIH is also responsible for organising the Hockey Rules Board and developing the rules for the game.\n\nA popular variant of field hockey is indoor field hockey, which differs in a number of respects while embodying the primary principles of hockey. Indoor hockey is a 5-a-side variant, with a field which is reduced to approximately . With many of the rules remaining the same, including obstruction and feet, there are several key variations: Players may not raise the ball unless shooting on goal, players may not hit the ball (instead using pushes to transfer the ball), and the sidelines are replaced with solid barriers which the ball will rebound off.\n\nThere is a depiction of a hockey-like game in Ancient Greece, dating to c. 510 BC, when the game may have been called (\"kerētízein\") because it was played with a horn (, \"kéras\", in Ancient Greek) and a ball. Researchers disagree over how to interpret this image. It could have been a team or one-on-one activity (the depiction shows two active players, and other figures who may be teammates awaiting a face-off, or non-players waiting for their turn at play). Billiards historians Stein and Rubino believe it was among the games ancestral to lawn-and-field games like hockey and ground billiards, and near-identical depictions (but with only two figures) appear both in the Beni Hasan tomb of Ancient Egyptian administrator Khety of the 11th Dynasty (c. 2000 BCE), and in European illuminated manuscripts and other works of the 14th through 17th centuries, showing contemporary courtly and clerical life. In East Asia, a similar game was entertained, using a carved wooden stick and ball prior, to 300 BC. In Inner Mongolia, China, the Daur people have for about 1,000 years been playing \"beikou\", a game with some similarities to field hockey. A similar field hockey or ground billiards variant, called \"suigan\", was played in China during the Ming dynasty (1368–1644, post-dating the Mongol-led Yuan dynasty). A game similar to hockey was played in the 17th century in Punjab state in India under name \"khido khundi\" (\"khido\" refers to the woolen ball, and \"khundi\" to the stick).\nIn South America, most specifically in Chile, the local natives of the 16th century used to play a game called chueca, which also has a lot of common elements with hockey. \n\nIn Northern Europe, the games of hurling (Ireland) and ' (Iceland), both team balls games involving sticks to drive a ball to the opponents' goal, date at least as far back as the Early Middle Ages. By the 12th century, a team ball game called ' or \"\", akin to a chaotic and sometimes long-distance version of hockey or rugby football (depending on whether sticks were used in a particular local variant), was regularly played in France and southern Britain between villages or parishes. Throughout the Middle Ages to the Early Modern era, such games often involved the local clergy or secular aristocracy, and in some periods were limited to them by various anti-gaming edicts, or even banned altogether. Stein and Rubino, among others, ultimately trace aspects of these games both to rituals in antiquity involving orbs and sceptres (on the aristocratic and clerical side), and to ancient military training exercises (on the popular side); polo (essentially hockey on horseback) was devised by the Ancient Persians for cavalry training, based on the local proto-hockey foot game of the region. \n\nThe word \"hockey\" itself has no clear origin. One belief is that it was recorded in 1363 when Edward III of England issued the proclamation: \"Moreover we ordain that you prohibit under penalty of imprisonment all and sundry from such stone, wood and iron throwing; handball, football, or hockey; coursing and cock-fighting, or other such idle games.\" The belief is based on modern translations of the proclamation, which was originally in Latin and explicitly forbade the games \"Pilam Manualem, Pedivam, & Bacularem: & ad Canibucam & Gallorum Pugnam\". It may be recalled at this point that \"baculum\" is the Latin for 'stick', so the reference would appear to be to a game played with sticks. The English historian and biographer John Strype did not use the word \"hockey\" when he translated the proclamation in 1720, and the word 'hockey' remains of unknown origin.\n\nThe modern game grew from English public schools in the early 19th century. The first club was in 1849 at Blackheath in south-east London, but the modern rules grew out of a version played by Middlesex cricket clubs for winter game. Teddington Hockey Club formed the modern game by introducing the striking circle and changing the ball to a sphere from a rubber cube. The Hockey Association was founded in 1886. The first international competition took place in 1895 (Ireland 3, Wales 0), and the International Rules Board was founded in 1900.\nField hockey was played at the Summer Olympics in 1908 and 1920. It was dropped in 1924, leading to the foundation of the Fédération Internationale de Hockey sur Gazon (FIH) as an international governing body by seven continental European nations; and hockey was reinstated as an Olympic game in 1928. Men's hockey united under the FIH in 1970.\n\nThe two oldest trophies are the Irish Senior Cup, which dates back to 1894, and the Irish Junior Cup, a second XI-only competition instituted in 1895.\n\nIn India, the Beighton Cup and the Aga Khan tournament commenced within ten years. Entering the Olympics in 1928, India won all five games without conceding a goal, and won from 1932 until 1956 and then in 1964 and 1980. Pakistan won in 1960, 1968 and 1984.\nIn the early 1970s, artificial turf began to be used. Synthetic pitches changed most aspects of field hockey, gaining speed. New tactics and techniques such as the Indian dribble developed, followed by new rules to take account. The switch to synthetic surfaces ended Indian and Pakistani domination because artificial turf was too expensive in developing countries. Since the 1970s, Australia, the Netherlands, Pakistan and Germany have dominated at the Olympics and World Cup stages. \n\nWomen's field hockey was first played at British universities and schools. The first club, the Molesey Ladies, was founded in 1887. The first national association was the Irish Ladies Hockey Union in 1894, and though rebuffed by the Hockey Association, women's field hockey grew rapidly around the world. This led to the International Federation of Women's Hockey Association (IFWHA) in 1927, though this did not include many continental European countries where women played as sections of men's associations and were affiliated to the FIH. The IFWHA held conferences every three years, and tournaments associated with these were the primary IFWHA competitions. These tournaments were non-competitive until 1975.\n\nBy the early 1970s, there were 22 associations with women's sections in the FIH and 36 associations in the IFWHA. Discussions started about a common rule book. The FIH introduced competitive tournaments in 1974, forcing the acceptance of the principle of competitive field hockey by the IFWHA in 1973. It took until 1982 for the two bodies to merge, but this allowed the introduction of women's field hockey to the Olympic games from 1980 where, as in the men's game, The Netherlands, Germany, and Australia have been consistently strong. Argentina has emerged as a team to be reckoned with since 2000, winning the world championship in 2002 and 2010 and medals at the last three Olympics.\n\nOutside North America, participation is now fairly evenly balanced between men and women. For example, in England, England Hockey reports that as of the 2008–09 season there were 2488 registered men's teams, 1969 women's teams, 1042 boys' teams, 966 girls' teams and 274 mixed teams. In 2006 the Irish Hockey Association reported that the gender split among its players was approximately 65% female and 35% male. In its 2008 census, Hockey Australia reported 40,534 male club players and 41,542 female. However, in the United States of America, there are few field hockey clubs, most play taking place between high school or college sides, consisting almost entirely of women. The strength of college field hockey reflects the impact of Title IX which mandated that colleges should fund men's and women's games programmes comparably.\n\nThe game's roots in the English public girls' school mean that the game is associated in the UK with active or overachieving middle class and upper class women. For example, in \"Nineteen Eighty-Four\", George Orwell's novel set in a totalitarian London, main character Winston Smith initially dislikes Julia, the woman he comes to love, because of \"the atmosphere of hockey-fields and cold baths and community hikes and general clean-mindedness which she managed to carry about with her.\"\n\nMost hockey field dimensions were originally fixed using whole numbers of imperial measures. Nevertheless, metric measurements are now the official dimensions as laid down by the International Hockey Federation (FIH) in the \"Rules of Hockey\". The pitch is a rectangular field. At each end is a goal high and wide, as well as lines across the field from each end-line (generally referred to as the 23-metre lines or the 25-yard lines) and in the center of the field. A spot in diameter, called the penalty spot or stroke mark, is placed with its centre from the centre of each goal. The shooting circle is from the base line.\n\nHistorically the game developed on natural grass turf. In the early 1970s, \"synthetic grass\" fields began to be used for hockey, with the first Olympic Games on this surface being held at the 1976 Montreal edition. Synthetic pitches are now mandatory for all international tournaments and for most national competitions. While hockey is still played on traditional grass fields at some local levels and lesser national divisions, it has been replaced by synthetic surfaces almost everywhere in the western world. There are three main types of artificial hockey surface:\n\n\nSince the 1970s, sand-based pitches have been favoured as they dramatically speed up the game. However, in recent years there has been a massive increase in the number of \"water-based\" artificial turfs. Water-based synthetic turfs enable the ball to be transferred more quickly than on sand-based surfaces. It is this characteristic that has made them the surface of choice for international and national league competitions. Water-based surfaces are also less abrasive than sand-based surfaces and reduce the level of injury to players when they come into contact with the surface. The FIH are now proposing that new surfaces being laid should be of a hybrid variety which require less watering. This is due to the negative ecological effects of the high water requirements of water-based synthetic fields. It has also been stated that the decision to make artificial surfaces mandatory greatly favoured more affluent countries who could afford these new pitches.\n\nThe game is played between two teams of whom eleven are permitted to be on the pitch at any one time. The remaining players may be substituted in any combination. There is an unlimited amount of times a team can sub in and out. Substitutions are permitted at any point in the game, apart from between the award and end of a penalty corner; two exceptions to this rule is for injury or suspension of the defending goalkeeper, which is not allowed when playing with a field keep, or a player can exit the field, but you must wait until after the inserter touches the ball to put somebody back in.\n\nPlayers are permitted to play the ball with the flat of the 'face side' and with the edges of the head and handle of the field hockey stick with the exception that, for reasons of safety, the ball may not be struck 'hard' with a forehand edge stroke, because of the difficulty of controlling the height and direction of the ball from that stroke.\n\nThe flat side is always on the \"natural\" side for a right-handed person swinging the stick at the ball from right to left. Left-handed sticks are rare, but available; however they are pointless as the rules forbid their use in a game. To make a strike at the ball with a left-to-right swing the player must present the flat of the 'face' of the stick to the ball by 'reversing' the stick head, i.e. by turning the handle through approximately 180° (while a reverse edge hit would turn the stick head through approximately 90° from the position of an upright forehand stroke with the 'face' of the stick head).\n\nEdge hitting of the ball underwent a two-year \"experimental period\", twice the usual length of an \"experimental trial\" and is still a matter of some controversy within the game. Ric Charlesworth, the former Australian coach, has been a strong critic of the unrestricted use of the reverse edge hit. The 'hard' forehand edge hit was banned after similar concerns were expressed about the ability of players to direct the ball accurately, but the reverse edge hit does appear to be more predictable and controllable than its counterpart. This type of hit is now more commonly referred to as the \"forehand sweep\" where the ball is hit with the flat side or \"natural\" side of the stick and not the rounded edge.\n\nOther rules include; no foot-to-ball contact, no use of hands, no obstructing other players, no high back swing, and no third party. If a player is dribbling the ball and either loses control and kicks the ball or another player interferes that player is not permitted to gain control and continue dribbling. The rules do not allow the person who kicked the ball to gain advantage from the kick, so the ball will automatically be passed on to the opposing team. Conversely, if no advantage is gained from kicking the ball, play should continue. Players may not obstruct another's chance of hitting the ball in any way. No shoving/using your body/stick to prevent advancement in the other team. Penalty for this is the opposing team receives the ball and if the problem continues, the player can be carded. While a player is taking a free hit or starting a corner the back swing of their hit cannot be too high for this is considered dangerous. Finally there may not be three players touching the ball at one time. Two players from opposing teams can battle for the ball, however if another player interferes it is considered third party and the ball automatically goes to the team who only had one player involved in the third party.\n\nWhen hockey positions are discussed, notions of fluidity are very common. Each team can be fielded with a maximum of 11 players and will typically arrange themselves into forwards, midfielders, and defensive players (fullbacks) with players frequently moving between theses lines with the flow of play. Each team may also play with:\n\n\"* a goalkeeper who wears a different color shirt and full protective equipment comprising at least headgear, leg guards and kickers; this player is referred to in the rules as a goalkeeper; or\"\n\n\"* a field player with goalkeeping privileges wearing a different color shirt and who may wear protective headgear (but not leg guards and kickers or other goalkeeping protective equipment) when inside their defending 23m area; they must wear protective headgear when defending a penalty corner or stroke; this player is referred to in the rules as a player with goalkeeping privileges; or\"\n\n\"* Only field players; no player has goalkeeping privileges or wears a different color shirt; no player may wear protective headgear except a face mask when defending a penalty corner or stroke.\"\n\nAs hockey has a very dynamic style of play, it is difficult to simplify positions to the static formations which are common in association football. Although positions will typically be categorized as either fullback, halfback, midfield/inner or striker, it is important for players to have an understanding of every position on the field. For example, it is not uncommon to see a halfback overlap and end up in either attacking position, with the midfield and strikers being responsible for re-adjusting to fill the space they left. Movement between lines like this is particularly common across all positions.\n\nThis fluid Australian culture of hockey has been responsible for developing an international trend towards players occupying spaces on the field, not having assigned positions. Although they may have particular spaces on the field which they are more comfortable and effective as players, they are responsible for occupying the space nearest them. This fluid approach to hockey and player movement has made it easy for teams to transition between formations such as; \"\"3 at the back\"\", \"\"2 centre halves\"\", \"\"5 at the front\",\" and more.\n\nWhen the ball is inside the circle they are defending and they have their stick in their hand, goalkeepers wearing full protective equipment are permitted to use their stick, feet, kickers or leg guards to propel the ball and to use their stick, feet, kickers, leg guards or any other part of their body to stop the ball or deﬂect it in any direction including over the back line. Similarly, field players are permitted to use their stick. They are not allowed to use their feet and legs to propel the ball, stop the ball or deflect it in any direction including over the back line. However, neither goalkeepers, or players with goalkeeping privileges are permitted to conduct themselves in a manner which is dangerous to other players by taking advantage of the protective equipment they wear.\n\nNeither goalkeepers or players with goalkeeping privileges may lie on the ball, however, they are permitted to use arms, hands and any other part of their body to push the ball away. Lying on the ball deliberately will result in a penalty stroke, whereas if an umpire deems a goalkeeper has lain on the ball accidentally (e.g. it gets stuck in their protective equipment), a penalty corner is awarded.\n\n\"* The action above is permitted only as part of a goal saving action or to move the ball away from the possibility of a goal scoring action by opponents. It does not permit a goalkeeper or player with goalkeeping privileges to propel the ball forcefully with arms, hands or body so that it travels a long distance\"\n\nWhen the ball is outside the circle they are defending, goalkeepers or players with goalkeeping privileges are only permitted to play the ball with their stick. Further, a goalkeeper, or player with goalkeeping privileges who is wearing a helmet, must not take part in the match outside the 23m area they are defending, except when taking a penalty stroke. A goalkeeper must wear protective headgear at all times, except when taking a penalty stroke.\n\nFor the purposes of the rules, all players on the team in possession of the ball are attackers, and those on the team without the ball are defenders, yet throughout the game being played you are always \"defending\" your goal and \"attacking\" the opposite goal.\nThe match is officiated by two field umpires. Traditionally each umpire generally controls half of the field, divided roughly diagonally. These umpires are often assisted by a technical bench including a timekeeper and record keeper.\n\nPrior to the start of the game, a coin is tossed and the winning captain can choose a starting end or whether to start with the ball. Since 2017 the game consists of four periods of 15 minutes with a 2-minute break after every period, and a 15-minute break at half time before changing ends. At the start of each period, as well as after goals are scored, play is started with a pass from the centre of the field. All players must start in their defensive half (apart from the player making the pass), but the ball may be played in any direction along the floor. Each team starts with the ball in one half, and the team that conceded the goal has possession for the restart. Teams trade sides at halftime.\n\nField players may only play the ball with the face of the stick. If the back side of the stick is used, it is a penalty and the other team will get the ball back. Tackling is permitted as long as the tackler does not make contact with the attacker or the other persons stick before playing the ball (contact after the tackle may also be penalized if the tackle was made from a position where contact was inevitable). Further, the player with the ball may not deliberately use his body to push a defender out of the way.\n\nField players may not play the ball with their feet, but if the ball accidentally hits the feet, and the player gains no benefit from the contact, then the contact is not penalized. Although there has been a change in the wording of this rule from 1 January 2007, the current FIH umpires' briefing instructs umpires not to change the way they interpret this rule.\n\nObstruction typically occurs in three circumstances – when a defender comes between the player with possession and the ball in order to prevent them tackling; when a defender's stick comes between the attacker's stick and the ball or makes contact with the attacker's stick or body; and also when blocking the opposition's attempt to tackle a teammate with the ball (called \"third party obstruction\").\n\nWhen the ball passes completely over the sidelines (on the sideline is still in), it is returned to play with a sideline hit, taken by a member of the team whose players were not the last to touch the ball before crossing the sideline. The ball must be placed on the sideline, with the hit taken from as near the place the ball went out of play as possible. If it crosses the back line after last touched by an attacker, a hit is awarded. A 15 m hit is also awarded for offences committed by the attacking side within 15 m of the end of the pitch they are attacking.\n\nSet plays are often utilized for specific situations such as a penalty corner or free hit. For instance, many teams have penalty corner variations that they can use to beat the defensive team. The coach may have plays that sends the ball between two defenders and lets the player attack the opposing team's goal. There are no set plays unless your team has them.\n\nFree hits are awarded when offences are committed outside the scoring circles (the term 'free hit' is standard usage but the ball need not be hit). The ball may be hit, pushed or lifted in any direction by the team offended against. The ball can be lifted from a free hit but not by hitting, you must flick or scoop to lift from a free hit. (In previous versions of the rules, hits in the area outside the circle in open play have been permitted but lifting one direction from a free hit was prohibited). Opponents must move from the ball when a free hit is awarded. A free hit must be taken from within playing distance of the place of the offence for which it was awarded and the ball must be stationary when the free hit is taken.\n\nAs mentioned above, a 15 m hit is awarded if an attacking player commits a foul forward of that line, or if the ball passes over the back line off an attacker. These free hits are taken in-line with where the foul was committed (taking a line parallel with the sideline between where the offence was committed, or the ball went out of play). When an attacking free hit is awarded within 5 m of the circle everyone including the person taking the penalty must be five metres from the circle and everyone apart from the person taking the free hit must be five metres away from the ball. When taking an attacking free hit, the ball may not be hit straight into the circle if you are within your attacking 23 meter area (25 yard area). It must travel 5 meters before going in.\n\nIn February 2009 the FIH introduced, as a \"Mandatory Experiment\" for international competition, an updated version of the free-hit rule. The changes allows a player taking a free hit to pass the ball to themselves. Importantly, this is not a \"play on\" situation, but to the untrained eye it may appear to be. The player must play the ball any distance in two separate motions, before continuing as if it were a play-on situation. They may raise an aerial or overhead immediately as the second action, or any other stroke permitted by the rules of field hockey. At high-school level, this is called a self pass and was adopted in Pennsylvania in 2010 as a legal technique for putting the ball in play.\n\nAlso, all players (from both teams) must be at least 5 m from any free hit awarded to the attack within the 23 m area. Additionally, no free hits to the attack are permitted within 5m of the circle, so if a free hit is awarded inside this area it must be dragged back outside this zone. The ball may not travel directly into the circle from a free hit to the attack within the 23 m area without first being touched by another player or being dribbled at least 5 m by a player making a \"self-pass\". These experimental rules apply to all free-hit situations, including sideline and corner hits. National associations may also choose to introduce these rules for their domestic competitions.\n\nA free hit from the quarter line is awarded to the attacking team if the ball goes over the back-line after last being touched by a defender, provided they do not play it over the back-line deliberately, in which case a penalty corner is awarded. This free hit is played by the attacking team from a spot on the quarter line closest to where the ball went out of play. All the parameters of an attacking free hit within the attacking quarter of the playing surface apply.\n\nThe short or penalty corner is awarded: \n\nShort corners begin with five defenders (usually including the keeper) positioned behind the back line and the ball placed at least 10 yards from the nearest goal post. All other players in the defending team must be beyond the centre line, that is not in their 'own' half of the pitch, until the ball is in play. Attacking players begin the play standing outside the scoring circle, except for one attacker who starts the corner by playing the ball from a mark 10 m either side of the goal (the circle has a 14.63 m radius). This player puts the ball into play by pushing or hitting the ball to the other attackers outside the circle; the ball must pass outside the circle and then put back into the circle before the attackers may make a shot at the goal from which a goal can be scored. FIH rules do not forbid a shot at goal before the ball leaves the circle after being 'inserted', nor is a shot at the goal from outside the circle prohibited, but a goal cannot be scored at all if the ball has not gone out of the circle and cannot be scored from a shot from outside the circle if it is not again played by an attacking player before it enters the goal.\n\nFor safety reasons, the first shot of a penalty corner must not exceed 460 mm high (the height of the \"backboard\" of the goal) at the point it crosses the goal line if it is hit. However, if the ball is deemed to be below backboard height, the ball can be subsequently deflected above this height by another player (defender or attacker), providing that this deflection does not lead to danger. Note that the \"Slap\" stroke (a sweeping motion towards the ball, where the stick is kept on or close to the ground when striking the ball) is classed as a hit, and so the first shot at goal must be below backboard height for this type of shot also.\n\nIf the first shot at goal in a short corner situation is a push, flick or scoop, in particular the \"drag flick\" (which has become popular at international and national league standards), the shot is permitted to rise above the height of the backboard, as long as the shot is not deemed dangerous to any opponent. This form of shooting was developed because it is not height restricted in the same way as the first hit shot at the goal and players with good technique are able to drag-flick with as much power as many others can hit a ball.\n\nA penalty stroke is awarded when a defender commits a foul in the circle (accidental or otherwise) that prevents a probable goal or commits a deliberate foul in the circle or if defenders repeatedly run from the back line too early at a penalty corner. The penalty stroke is taken by a single attacker in the circle, against the goalkeeper, from a spot 6.4 m from goal. The ball is played only once at goal by the attacker using a push, flick or scoop stroke. If the shot is saved, play is restarted with a 15 m hit to the defenders. When a goal is scored, play is restarted in the normal way.\n\nAccording to the current Rules of Hockey 2017 issued by the FIH there are only two criteria for a dangerously played ball. The first is legitimate evasive action by an opponent (what constitutes legitimate evasive action is an umpiring judgment). The second is specific to the rule concerning a shot at goal at a penalty corner but is generally, if somewhat inconsistently, applied throughout the game and in all parts of the pitch: it is that a ball lifted above knee height and at an opponent who is within 5m of the ball is certainly dangerous.\n\nThe velocity of the ball is not mentioned in the rules concerning a dangerously played ball. A ball that hits a player above the knee may on some occasions not be penalized, this is at the umpire's discretion. A jab tackle, for example, might accidentally lift the ball above knee height into an opponent from close range but at such low velocity as not to be, in the opinion of the umpire, dangerous play. In the same way a high-velocity hit at very close range into an opponent, but below knee height, could be considered to be dangerous or reckless play in the view of the umpire, especially when safer alternatives are open to the striker of the ball.\n\nA ball that has been lifted high so that it will fall among close opponents may be deemed to be potentially dangerous and play may be stopped for that reason. A lifted ball that is falling to a player in clear space may be made potentially dangerous by the actions of an opponent closing to within 5m of the receiver before the ball has been controlled to ground – a rule which is often only loosely applied; the distance allowed is often only what might be described as playing distance, 2–3 m, and opponents tend to be permitted to close on the ball as soon as the receiver plays it: these unofficial variations are often based on the umpire's perception of the skill of the players i.e. on the level of the game, in order to maintain game flow, which umpires are in general in both Rules and Briefing instructed to do, by not penalising when it is unnecessary to do so; this is also a matter at the umpire's discretion.\n\nThe term \"falling ball\" is important in what may be termed encroaching offences. It is generally only considered an offence to encroach on an opponent receiving a lifted ball that has been lifted to above head height (although the height is not specified in rule) and is falling. So, for example, a lifted shot at the goal which is still rising as it crosses the goal line (or would have been rising as it crossed the goal line) can be legitimately followed up by any of the attacking team looking for a rebound.\n\nIn general even potentially dangerous play is not penalised if an opponent is not disadvantaged by it or, obviously, not injured by it so that he cannot continue. A personal penalty, that is a caution or a suspension, rather than a team penalty, such as a free ball or a penalty corner, may be (many would say should be or even must be, but again this is at the umpire's discretion) issued to the guilty party after an advantage allowed by the umpire has been played out in any situation where an offence has occurred, including dangerous play (but once advantage has been allowed the umpire cannot then call play back and award a team penalty).\n\nIt is not an offence to lift the ball over an opponent's stick (or body on the ground), provided that it is done with consideration for the safety of the opponent and not dangerously. For example, a skillful attacker may lift the ball over a defenders stick or prone body and run past them, however if the attacker lifts the ball into or at the defender's body, this would almost certainly be regarded as dangerous.\n\nIt is not against the rules to bounce the ball on the stick and even to run with it while doing so, as long as that does not lead to a potentially dangerous conflict with an opponent who is attempting to make a tackle. For example, two players trying to play at the ball in the air at the same time, would probably be considered a dangerous situation and it is likely that the player who first put the ball up or who was so 'carrying' it would be penalised.\n\nDangerous play rules also apply to the usage of the stick when approaching the ball, making a stroke at it (replacing what was at one time referred to as the \"sticks\" rule, which once forbade the raising of any part of the stick above the shoulder during any play. This last restriction has been removed but the stick should still not be used in a way that endangers an opponent) or attempting to tackle, (fouls relating to tripping, impeding and obstruction). The use of the stick to strike an opponent will usually be much more severely dealt with by the umpires than offences such as barging, impeding and obstruction with the body, although these are also dealt with firmly, especially when these fouls are intentional: field hockey is a non-contact game.\n\nPlayers may not play or attempt to play at the ball above their shoulders unless trying to save a shot that could go into the goal, in which case they are permitted to stop the ball or deflect it safely away. A swing, as in a hit, at a high shot at the goal (or even wide of the goal) will probably be considered dangerous play if at opponents within 5 m and such a stroke would be contrary to rule in these circumstances anyway.\n\nWithin the English National League it is now a legal action to take a ball above shoulder height if completed using a controlled action.\n\nHockey uses a three-tier penalty card system of warnings and suspensions:\n\nDepending on national rules, if a coach is sent off a player may have to leave the field too for the time the coach is sent off.\n\nIf a coach is sent off, depending on local rules, a player may have to leave the field for the remaining length of the match.\n\nIn addition to their colours, field hockey penalty cards are often shaped differently, so they can be recognized easily. Green cards are normally triangular, yellow cards rectangular and red cards circular.\n\nUnlike football, a player may receive more than one green or yellow card. However, they cannot receive the same card for the same offence (for example two yellows for dangerous play), and the second must always be a more serious card. In the case of a second yellow card for a different breach of the rules (for example a yellow for deliberate foot, and a second later in the game for dangerous play) the temporary suspension would be expected to be of considerably longer duration than the first. However, local playing conditions may mandate that cards are awarded only progressively, and not allow any second awards.\n\nUmpires, if the free hit would have been in the attacking 23 m area, may upgrade the free hit to a penalty corner for dissent or other misconduct after the free hit has been awarded.\n\nThe teams' object is to play the ball into their attacking circle and, from there, hit, push or flick the ball into the goal, scoring a goal. The team with more goals after 60 minutes wins the game. The playing time may be shortened, particularly when younger players are involved, or for some tournament play. If the game is played in a countdown clock, like ice hockey, a goal can only count if the ball completely crosses the goaline and into the goal \"before\" time expires, not when the ball leaves the stick in the act of shooting.\nIn many competitions (such as regular club competition, or in pool games in FIH international tournaments such as the Olympics or the World Cup), a tied result stands and the overall competition standings are adjusted accordingly. Since March 2013, when tie breaking is required, the official FIH Tournament Regulations mandate to no longer have extra time and go directly into a penalty shoot-out when a classification match ends in a tie. However, many associations follow the previous procedure consisting of two periods of 7.5 minutes of \"golden goal\" extra time during which the game ends as soon as one team scores.\n\nThe FIH implemented a two-year rules cycle with the 2007–08 edition of the rules, with the intention that the rules be reviewed on a biennial basis. The 2009 rulebook was officially released in early March 2009 (effective 1 May 2009), however the FIH published the major changes in February. The current rule book is effective from 1 January 2017.\n\nThe FIH has adopted a policy of including major changes to the rules as \"Mandatory Experiments\", showing that they must be played at international level, but are treated as experimental and will be reviewed before the next rulebook is published and either changed, approved as permanent rules, or deleted.\n\nThere are sometimes minor variations in rules from competition to competition; for instance, the duration of matches is often varied for junior competitions or for carnivals. Different national associations also have slightly differing rules on player equipment.\n\nThe new Euro Hockey League and the Olympics has made major alterations to the rules to aid television viewers, such as splitting the game into four quarters, and to try to improve player behavior, such as a two-minute suspension for green cards—the latter was also used in the 2010 World Cup and 2016 Olympics. In the United States, the NCAA has its own rules for inter-collegiate competitions; high school associations similarly play to different rules, usually using the rules published by the National Federation of State High School Associations (NFHS). This article assumes FIH rules unless otherwise stated. USA Field Hockey produces an annual summary of the differences.\n\nIn the United States, the games at the junior high level consist of two 25-minute halves or four 12-minute periods, while the high-school level consists of two 30-minute halves or four 15-minute periods. Many private American schools play 25-minute halves, and some have adopted FIH rules rather than NFHS rules. \n\nPlayers are required to wear mouth guards and shin guards in order to play the game. Also, there is a newer rule requiring certain types of sticks be used. In recent years, the NFHS rules have moved closer to FIH, but in 2011 a new rule requiring protective eyewear was introduced for the 2011 Fall season. Metal 'cage style' goggles favored by US high school lacrosse and permitted in high school field hockey is prohibited under FIH rules.\n\nEach player carries a \"stick\" that normally measures between 80–95 cm (31–38\"); shorter or longer sticks are available. Sticks were traditionally made of wood, but are now often made also with fibreglass, kevlar or carbon fibre composites. Metal is forbidden from use in field hockey sticks, due to the risk of injury from sharp edges if the stick were to break. The stick has a rounded handle, has a J-shaped hook at the bottom, and is flattened on the left side (when looking down the handle with the hook facing upwards). All sticks must be right handed; left-handed ones are prohibited.\n\nThere was traditionally a slight curve (called the bow, or rake) from the top to bottom of the face side of the stick and another on the 'heel' edge to the top of the handle (usually made according to the angle at which the handle part was inserted into the splice of the head part of the stick), which assisted in the positioning of the stick head in relation to the ball and made striking the ball easier and more accurate.\n\nThe hook at the bottom of the stick was only recently the tight curve (Indian style) that we have nowadays. The older 'English' sticks had a longer bend, making it very hard to use the stick on the reverse. For this reason players now use the tight curved sticks.\n\nThe handle makes up about the top third of the stick. It is wrapped in a grip similar to that used on tennis racket. The grip may be made of a variety of materials, including chamois leather, which many players think improves grip in the wet.\n\nIt was recently discovered that increasing the depth of the face bow made it easier to get high speeds from the dragflick and made the stroke easier to execute. At first, after this feature was introduced, the Hockey Rules Board placed a limit of 50 mm on the maximum depth of bow over the length of the stick but experience quickly demonstrated this to be excessive. New rules now limit this curve to under 25 mm so as to limit the power with which the ball can be flicked.\n\nStandard field hockey balls are hard spherical balls, made of plastic (sometimes over a cork core), and are usually white, although they can be any colour as long as they contrast with the playing surface. The balls have a circumference of and a mass of . The ball is often covered with indentations to reduce aquaplaning that can cause an inconsistent ball speed on wet surfaces.\n\nThe 2007 rulebook has seen major changes regarding goalkeepers. A fully equipped goalkeeper must wear a helmet, leg guards and kickers. Usually the field hockey goalkeepers must wear extensive additional protective equipment including chest guards, padded shorts, heavily padded hand protectors, groin protectors, neck guards, arm guards, and like all players, they must carry a stick. A goalie may not cross the 23 m line, the sole exception to this being if the goalkeeper is to take a penalty stroke at the other end of the field, when the clock is stopped. The goalkeeper can also remove their helmet for this action. However, if the goalkeeper elects to wear only a helmet (and a different colored shirt), they may cross the 23 m line if they have removed their helmet (and placed it safely off the field of play). If play returns to the circle without them having opportunity to replace the helmet, this player still has \"goalkeeping privileges\", that is, they are not limited to using their stick to play the ball whilst it is in the circle, and the helmet must be worn whilst defending penalty corners and penalty strokes but the best thing to do would be to wear it at all times. While goaltenders are allowed to use their feet and hands to clear the ball, they too are only allowed to use one side of their stick. Slide tackling is permitted as long as it is with the intention of clearing the ball, not aimed at a player. \n\nIt is now also even possible for teams to have a full eleven outfield players and no goalkeeper at all. No player may wear a helmet or other goalkeeping equipment, neither will any player be able to play the ball with any other part of the body than with their stick. This may be used to offer a tactical advantage, or to allow for play to commence if no goalkeeper or kit is available.\n\nThe basic tactic in field hockey, as in association football and many other team games, is to outnumber the opponent in a particular area of the field at a moment in time. When in possession of the ball this temporary numerical superiority can be used to pass the ball around opponents so that they cannot effect a tackle because they cannot get within playing reach of the ball and to further use this numerical advantage to gain time and create clear space for making scoring shots on the opponent's goal. When not in possession of the ball numerical superiority is used to isolate and channel an opponent in possession and 'mark out' any passing options so that an interception or a tackle may be made to gain possession. Highly skillful players can sometimes get the better of more than one opponent and retain the ball and successfully pass or shoot but this tends to use more energy than quick early passing.\n\nEvery player has a role depending on their relationship to the ball if the team communicates throughout the play of the game. There will be players on the ball (offensively - ball carriers; defensively - pressure, support players, and movement players.\n\nThe main methods by which the ball is moved around the field by players are a) passing b) pushing the ball and running with it controlled to the front or right of the body and c) \"dribbling\"; where the player controls the ball with the stick and moves in various directions with it to elude opponents. To make a pass the ball may be propelled with a pushing stroke, where the player uses their wrists to push the stick head through the ball while the stick head is in contact with it; the \"flick\" or \"scoop\", similar to the push but with an additional arm and leg and rotational actions to lift the ball off the ground; and the \"hit\", where a swing at ball is taken and contact with it is often made very forcefully, causing the ball to be propelled at velocities in excess of . In order to produce a powerful hit, usually for travel over long distances or shooting at the goal, the stick is raised higher and swung with maximum power at the ball, a stroke sometimes known as a \"drive\".\n\nTackles are made by placing the stick into the path of the ball or playing the stick head or shaft directly at the ball. To increase the effectiveness of the tackle, players will often place the entire stick close to the ground horizontally, thus representing a wider barrier. To avoid the tackle, the ball carrier will either pass the ball to a teammate using any of the push, flick, or hit strokes, or attempt to maneuver or \"drag\" the ball around the tackle, trying to deceive the tackler.\n\nIn recent years, the penalty corner has gained importance as a goal scoring opportunity. Particularly with the technical development of the drag flick. Tactics at penalty corners to set up time for a shot with a drag flick or a hit shot at the goal involve various complex plays, including multiple passes before a deflections towards the goal is made but the most common method of shooting is the direct flick or hit at the goal.\n\nAt the highest level, field hockey is a fast moving, highly skilled game, with players using fast moves with the stick, quick accurate passing, and hard hits, in attempts to keep possession and move the ball towards the goal. Tackling with physical contact and otherwise physically obstructing players is not permitted. Some of the tactics used resemble football (soccer), but with greater ball speed.\n\nWith the 2009 changes to the rules regarding free hits in the attacking 23m area, the common tactic of hitting the ball hard into the circle was forbidden. Although at higher levels this was considered tactically risky and low-percentage at creating scoring opportunities, it was used with some effect to 'win' penalty corners by forcing the ball onto a defender's foot or to deflect high (and dangerously) off a defender's stick. The FIH felt it was a dangerous practice that could easily lead to raised deflections and injuries in the circle, which is often crowded at a free-hit situation, and outlawed it.\n\nThe biggest two field hockey tournaments are the Olympic Games tournament, and the Hockey World Cup, which is also held every 4 years. Apart from this, there is the Champions Trophy held each year for the six top-ranked teams. Field hockey has also been played at the Commonwealth Games since 1998. Amongst the men, India lead in Olympic competition, having won 8 golds (6 successive in row). Amongst the women, Australia and Netherlands have 3 Olympic golds while Netherlands has clinched the World Cup 6 times. The Sultan Azlan Shah Hockey Tournament and Sultan Ibrahim Ismail Hockey Tournament for the junior team, both tournaments held annually in Malaysia, are becoming prominent field hockey tournaments where teams from around the world participate to win the cup.\n\nIndia and Pakistan dominated men's hockey until the early 1980s, winning eight Olympic golds and three of the first five world cups respectively, but have become less prominent with the ascendancy of the Netherlands, Germany, New Zealand, Australia and Spain since the late 1980s, as grass playing surfaces were replaced with artificial turf (which conferred increased importance on athleticism). Other notable men's nations include Argentina, England (who combine with other British \"Home Nations\" to form the Great Britain side at Olympic events) and South Korea. Despite their recent drop in international rankings, Pakistan still holds the record of four World Cup wins.\n\nNetherlands, Australia and Argentina are the most successful national teams among women. The Netherlands was the predominant women's team before field hockey was added to Olympic events. In the early 1990s, Australia emerged as the strongest women's country although retirement of a number of players weakened the team. Argentina improved its play on the 2000s, heading IFH rankings in 2003, 2010 and 2013. Other prominent women's teams are China, South Korea, Germany and South Africa.\n\nThis is a list of the major International field hockey tournaments, in chronological order. Tournaments included are:\n\nAlthough invitational or not open to all countries, the following are also considered international tournaments:\n\nAs the name suggests, Hockey 5s is a hockey variant which features five players on each team (which must include a goalkeeper). The field of play is 55 m long and 41.70 m wide— this is approximately half the size of a regular pitch. Few additional markings are needed as there is no penalty circle nor penalty corners; shots can be taken from anywhere on the pitch. Penalty strokes are replaced by a \"challenge\" which is like the one-on-one method used in a penalty shoot-out. The duration of the match is three 12-minute periods with an interval of two minutes between periods. The rules are simpler and it is intended that the game is faster, creating more shots on goal with less play in midfield, and more attractive to spectators.\n\nAn Asian qualification tournament for two places at the 2014 Youth Olympic Games was the first time an FIH event used the Hockey 5s format. Hockey 5s was also used for the Youth Olympic hockey tournament, and at the Pacific Games in 2015.\n\n"}
{"id": "10887", "url": "https://en.wikipedia.org/wiki?curid=10887", "title": "Finagle's law", "text": "Finagle's law\n\nFinagle's Law of Dynamic Negatives, also known as Melody's law or Finagle's corollary to Murphy's law, is usually rendered:\nThe term \"Finagle's Law\" was first used by John W. Campbell, Jr., the influential editor of \"Astounding Science Fiction\" (later \"Analog\"). He used it frequently in his editorials for many years in the 1940s to 1960s but it never came into general usage the way Murphy's Law has.\n\nOne variant (known as O'Toole's Corollary of Finagle's Law) favored among hackers is a takeoff on the second law of thermodynamics (related to the augmentation of entropy):\nIn the \"Star Trek\" episode \"The Ultimate Computer\", Dr. McCoy refers to an alcoholic drink known as the \"Finagle's Folly,\" apparently a reference to \"Finagle's Law.\" In Season 2 episode \"Amok Time\" (written by Theodore Sturgeon, 1967), Captain Kirk tells Spock, \"As one of Finagle's Laws puts it: 'Any home port the ship makes will be somebody else's, not mine.'\"\n\nThe term \"Finagle's law\" was popularized by science fiction author Larry Niven in several stories depicting a frontier culture of asteroid miners; this \"Belter\" culture professed a religion or running joke involving the worship of the dread god Finagle and his mad prophet Murphy.\n\n\"Finagle's Law\" can also be the related belief \"Inanimate objects are out to get us\", also known as Resistentialism.\nSimilar to Finagle's Law is the verbless phrase of the German novelist Friedrich Theodor Vischer: \"\"die Tücke des Objekts\"\" (the perfidy of inanimate objects).\n\nA related concept, the \"Finagle factor\", is an \"ad hoc\" multiplicative or additive term in an equation which can only be justified by the fact that it gives more correct results. Also known as Finagle's variable constant, it is sometimes defined as the right answer divided by your answer.\n\n"}
{"id": "10890", "url": "https://en.wikipedia.org/wiki?curid=10890", "title": "Fundamental interaction", "text": "Fundamental interaction\n\nIn physics, the fundamental interactions, also known as fundamental forces, are the interactions that do not appear to be reducible to more basic interactions. There are four fundamental interactions known to exist: the gravitational and electromagnetic interactions, which produce significant long-range forces whose effects can be seen directly in everyday life, and the strong, and weak interactions, which produce forces at minuscule, subatomic distances and govern nuclear interactions. Some scientists speculate that a fifth force might exist but if so, it is not widely accepted nor proven.\n\nEach of the known fundamental interactions can be described mathematically as a \"field\". The gravitational force is attributed to the curvature of spacetime, described by Einstein's general theory of relativity. The other three are discrete quantum fields, and their interactions are mediated by elementary particles described by the Standard Model of particle physics.\n\nWithin the Standard Model, the strong interaction is carried by a particle called the gluon, and is responsible for the binding of quarks together to form hadrons, such as protons and neutrons. As a residual effect, it creates the nuclear force that binds the latter particles to form atomic nuclei. The weak interaction is carried by particles called W and Z bosons, and also acts on the nucleus of atoms, mediating radioactive decay. The electromagnetic force, carried by the photon, creates electric and magnetic fields, which are responsible for chemical bonding and electromagnetic waves, including visible light, and forms the basis for electrical technology. Although the electromagnetic force is far stronger than gravity, it tends to cancel itself out within large objects, so over the largest distances (on the scale of planets and galaxies), gravity tends to be the dominant force.\n\nAll four fundamental forces are believed to be related, and to unite into a single force at high energies on a minuscule scale, the Planck scale, but particle accelerators cannot produce the enormous energies required to experimentally probe this. Efforts to devise a common theoretical framework that would explain the relation between the forces are perhaps the greatest goal of theoretical physicists today. The weak and electromagnetic forces have already been unified with the electroweak theory of Sheldon Glashow, Abdus Salam, and Steven Weinberg for which they received the 1979 Nobel Prize in physics. Progress is currently being made in uniting the electroweak and strong fields within a Grand Unified Theory (GUT). A bigger challenge is to find a way to quantize the gravitational field, resulting in a theory of quantum gravity (QG) which would unite gravity in a common theoretical framework with the other three forces. Some theories, notably string theory, seek both QG and GUT within one framework, unifying all four fundamental interactions along with mass generation within a theory of everything (ToE).\n\nIn his 1687 theory, Isaac Newton postulated space as an infinite and unalterable physical structure existing before, within, and around all objects while their states and relations unfold at a constant pace everywhere, thus absolute space and time. Inferring that all objects bearing mass approach at a constant rate, but collide by impact proportional to their masses, Newton inferred that matter exhibits an attractive force. His law of universal gravitation mathematically stated it to span the entire universe instantly (despite absolute time), or, if not actually a force, to be instant interaction among all objects (despite absolute space.) As conventionally interpreted, Newton's theory of motion modelled a \"central force\" without a communicating medium. Thus Newton's theory violated the first principle of mechanical philosophy, as stated by Descartes, \"No action at a distance\". Conversely, during the 1820s, when explaining magnetism, Michael Faraday inferred a \"field\" filling space and transmitting that force. Faraday conjectured that ultimately, all forces unified into one.\n\nIn the early 1870s, James Clerk Maxwell unified electricity and magnetism as effects of an electromagnetic field whose third consequence was light, travelling at constant speed in a vacuum. The electromagnetic field theory contradicted predictions of Newton's theory of motion, unless physical states of the luminiferous aether—presumed to fill all space whether within matter or in a vacuum and to manifest the electromagnetic field—aligned all phenomena and thereby held valid the Newtonian principle relativity or invariance.\n\nThe Standard Model of particle physics was developed throughout the latter half of the 20th century. In the Standard Model, the electromagnetic, strong, and weak interactions associate with elementary particles, whose behaviours are modelled in quantum mechanics (QM). For predictive success with QM's probabilistic outcomes, particle physics conventionally models QM events across a field set to special relativity, altogether relativistic quantum field theory (QFT). Force particles, called gauge bosons—\"force carriers\" or \"messenger particles\" of underlying fields—interact with matter particles, called fermions. Everyday matter is atoms, composed of three fermion types: up-quarks and down-quarks constituting, as well as electrons orbiting, the atom's nucleus. Atoms interact, form molecules, and manifest further properties through electromagnetic interactions among their electrons absorbing and emitting photons, the electromagnetic field's force carrier, which if unimpeded traverse potentially infinite distance. Electromagnetism's QFT is quantum electrodynamics (QED).\n\nThe electromagnetic interaction was modelled with the weak interaction, whose force carriers are W and Z bosons, traversing the minuscule distance, in electroweak theory (EWT). Electroweak interaction would operate at such high temperatures as soon after the presumed Big Bang, but, as the early universe cooled, split into electromagnetic and weak interactions. The strong interaction, whose force carrier is the gluon, traversing minuscule distance among quarks, is modeled in quantum chromodynamics (QCD). EWT, QCD, and the Higgs mechanism, whereby the Higgs field manifests Higgs bosons that interact with some quantum particles and thereby endow those particles with mass comprise particle physics' Standard Model (SM). Predictions are usually made using calculational approximation methods, although such perturbation theory is inadequate to model some experimental observations (for instance bound states and solitons.) Still, physicists widely accept the Standard Model as science's most experimentally confirmed theory.\n\nBeyond the Standard Model, some theorists work to unite the electroweak and strong interactions within a Grand Unified Theory (GUT). Some attempts at GUTs hypothesize \"shadow\" particles, such that every known matter particle associates with an undiscovered force particle, and vice versa, altogether supersymmetry (SUSY). Other theorists seek to quantize the gravitational field by the modelling behaviour of its hypothetical force carrier, the graviton and achieve quantum gravity (QG). One approach to QG is loop quantum gravity (LQG). Still other theorists seek both QG and GUT within one framework, reducing all four fundamental interactions to a Theory of Everything (ToE). The most prevalent aim at a ToE is string theory, although to model matter particles, it added SUSY to force particles—and so, strictly speaking, became superstring theory. Multiple, seemingly disparate superstring theories were unified on a backbone, M-theory. Theories beyond the Standard Model remain highly speculative, lacking great experimental support.\n\nIn the conceptual model of fundamental interactions, matter consists of fermions, which carry properties called charges and spin ± (intrinsic angular momentum ±, where ħ is the reduced Planck constant). They attract or repel each other by exchanging bosons.\n\nThe interaction of any pair of fermions in perturbation theory can then be modelled thus:\n\nThe exchange of bosons always carries energy and momentum between the fermions, thereby changing their speed and direction. The exchange may also transport a charge between the fermions, changing the charges of the fermions in the process (e.g., turn them from one type of fermion to another). Since bosons carry one unit of angular momentum, the fermion's spin direction will flip from + to − (or vice versa) during such an exchange (in units of the reduced Planck's constant).\n\nBecause an interaction results in fermions attracting and repelling each other, an older term for \"interaction\" is force.\n\nAccording to the present understanding, there are four fundamental interactions or forces: gravitation, electromagnetism, the weak interaction, and the strong interaction. Their magnitude and behaviour vary greatly, as described in the table below. Modern physics attempts to explain every observed physical phenomenon by these fundamental interactions. Moreover, reducing the number of different interaction types is seen as desirable. Two cases in point are the unification of:\n\nBoth magnitude (\"relative strength\") and \"range\", as given in the table, are meaningful only within a rather complex theoretical framework. It should also be noted that the table below lists properties of a conceptual scheme that is still the subject of ongoing research.\n\nThe modern (perturbative) quantum mechanical view of the fundamental forces other than gravity is that particles of matter (fermions) do not directly interact with each other, but rather carry a charge, and exchange virtual particles (gauge bosons), which are the interaction carriers or force mediators. For example, photons mediate the interaction of electric charges, and gluons mediate the interaction of color charges.\n\n\"Gravitation\" is by far the weakest of the four interactions at the atomic scale, where electromagnetic interactions dominate. But the idea that the weakness of gravity can easily be demonstrated by suspending a pin using a simple magnet (such as a refrigerator magnet) is fundamentally flawed. The only reason the magnet is able to hold the pin against the gravitational pull of the entire Earth is due to its relative proximity. There is clearly a short distance of separation between magnet and pin where a breaking point is reached, and due to the large mass of Earth this distance is disappointingly small.\n\nThus gravitation is very important for macroscopic objects and over macroscopic distances for the following reasons. Gravitation:\n\nEven though electromagnetism is far stronger than gravitation, electrostatic attraction is not relevant for large celestial bodies, such as planets, stars, and galaxies, simply because such bodies contain equal numbers of protons and electrons and so have a net electric charge of zero. Nothing \"cancels\" gravity, since it is only attractive, unlike electric forces which can be attractive or repulsive. On the other hand, all objects having mass are subject to the gravitational force, which only attracts. Therefore, only gravitation matters on the large-scale structure of the universe.\n\nThe long range of gravitation makes it responsible for such large-scale phenomena as the structure of galaxies and black holes and it retards the expansion of the universe. Gravitation also explains astronomical phenomena on more modest scales, such as planetary orbits, as well as everyday experience: objects fall; heavy objects act as if they were glued to the ground, and animals can only jump so high.\n\nGravitation was the first interaction to be described mathematically. In ancient times, Aristotle hypothesized that objects of different masses fall at different rates. During the Scientific Revolution, Galileo Galilei experimentally determined that this was not the case — neglecting the friction due to air resistance, and buoyancy forces if an atmosphere is present (e.g. the case of a dropped air-filled balloon vs a water-filled balloon) all objects accelerate toward the Earth at the same rate. Isaac Newton's law of Universal Gravitation (1687) was a good approximation of the behaviour of gravitation. Our present-day understanding of gravitation stems from Albert Einstein's General Theory of Relativity of 1915, a more accurate (especially for cosmological masses and distances) description of gravitation in terms of the geometry of spacetime.\n\nMerging general relativity and quantum mechanics (or quantum field theory) into a more general theory of quantum gravity is an area of active research. It is hypothesized that gravitation is mediated by a massless spin-2 particle called the graviton.\n\nAlthough general relativity has been experimentally confirmed (at least for weak fields ) on all but the smallest scales, there are rival theories of gravitation. Those taken seriously by the physics community all reduce to general relativity in some limit, and the focus of observational work is to establish limitations on what deviations from general relativity are possible.\n\nProposed extra dimensions could explain why the gravity force is so weak.\n\nElectromagnetism and weak interaction appear to be very different at everyday low energies. They can be modelled using two different theories. However, above unification energy, on the order of 100 GeV, they would merge into a single electroweak force.\n\nElectroweak theory is very important for modern cosmology, particularly on how the universe evolved. This is because shortly after the Big Bang, the temperature was approximately above 10 K. Electromagnetic force and weak force were merged into a combined electroweak force.\n\nFor contributions to the unification of the weak and electromagnetic interaction between elementary particles, Abdus Salam, Sheldon Glashow and Steven Weinberg were awarded the Nobel Prize in Physics in 1979.\n\nElectromagnetism is the force that acts between electrically charged particles. This phenomenon includes the electrostatic force acting between charged particles at rest, and the combined effect of electric and magnetic forces acting between charged particles moving relative to each other.\n\nElectromagnetism is infinite-ranged like gravity, but vastly stronger, and therefore describes a number of macroscopic phenomena of everyday experience such as friction, rainbows, lightning, and all human-made devices using electric current, such as television, lasers, and computers. Electromagnetism fundamentally determines all macroscopic, and many atomic levels, properties of the chemical elements, including all chemical bonding.\n\nIn a four kilogram (~1 gallon) jug of water there are\n\nformula_1\n\nof total electron charge. Thus, if we place two such jugs a meter apart, the electrons in one of the jugs repel those in the other jug with a force of\n\nformula_2\n\nThis is larger than the planet Earth would weigh if weighed on another Earth. The atomic nuclei in one jug also repel those in the other with the same force. However, these repulsive forces are canceled by the attraction of the electrons in jug A with the nuclei in jug B and the attraction of the nuclei in jug A with the electrons in jug B, resulting in no net force. Electromagnetic forces are tremendously stronger than gravity but cancel out so that for large bodies gravity dominates.\n\nElectrical and magnetic phenomena have been observed since ancient times, but it was only in the 19th century that it was discovered that electricity and magnetism are two aspects of the same fundamental interaction. By 1864, Maxwell's equations had rigorously quantified this unified interaction. Maxwell's theory, restated using vector calculus, is the classical theory of electromagnetism, suitable for most technological purposes.\n\nThe constant speed of light in a vacuum (customarily described with the letter \"c\") can be derived from Maxwell's equations, which are consistent with the theory of special relativity. Einstein's 1905 theory of special relativity, however, which flows from the observation that the speed of light is constant no matter how fast the observer is moving, showed that the theoretical result implied by Maxwell's equations has profound implications far beyond electromagnetism on the very nature of time and space.\n\nIn another work that departed from classical electro-magnetism, Einstein also explained the photoelectric effect by hypothesizing that light was transmitted in quanta, which we now call photons. Starting around 1927, Paul Dirac combined quantum mechanics with the relativistic theory of electromagnetism. Further work in the 1940s, by Richard Feynman, Freeman Dyson, Julian Schwinger, and Sin-Itiro Tomonaga, completed this theory, which is now called quantum electrodynamics, the revised theory of electromagnetism. Quantum electrodynamics and quantum mechanics provide a theoretical basis for electromagnetic behavior such as quantum tunneling, in which a certain percentage of electrically charged particles move in ways that would be impossible under the classical electromagnetic theory, that is necessary for everyday electronic devices such as transistors to function.\n\nThe \"weak interaction\" or \"weak nuclear force\" is responsible for some nuclear phenomena such as beta decay. Electromagnetism and the weak force are now understood to be two aspects of a unified electroweak interaction — this discovery was the first step toward the unified theory known as the Standard Model. In the theory of the electroweak interaction, the carriers of the weak force are the massive gauge bosons called the W and Z bosons. The weak interaction is the only known interaction which does not conserve parity; it is left-right asymmetric. The weak interaction even violates CP symmetry but does conserve CPT.\n\nThe \"strong interaction\", or \"strong nuclear force\", is the most complicated interaction, mainly because of the way it varies with distance. At distances greater than 10 femtometers, the strong force is practically unobservable. Moreover, it holds only inside the atomic nucleus.\n\nAfter the nucleus was discovered in 1908, it was clear that a new force, today known as the nuclear force, was needed to overcome the electrostatic repulsion, a manifestation of electromagnetism, of the positively charged protons. Otherwise, the nucleus could not exist. Moreover, the force had to be strong enough to squeeze the protons into a volume that is about 10 m, much smaller than that of the entire atom. From the short range of this force, Hideki Yukawa predicted that it was associated with a massive particle, whose mass is approximately 100 MeV.\n\nThe 1947 discovery of the pion ushered in the modern era of particle physics. Hundreds of hadrons were discovered from the 1940s to 1960s, and an extremely complicated theory of hadrons as strongly interacting particles was developed. Most notably:\n\nWhile each of these approaches offered deep insights, no approach led directly to a fundamental theory.\n\nMurray Gell-Mann along with George Zweig first proposed fractionally charged quarks in 1961. Throughout the 1960s, different authors considered theories similar to the modern fundamental theory of quantum chromodynamics (QCD) as simple models for the interactions of quarks. The first to hypothesize the gluons of QCD were Moo-Young Han and Yoichiro Nambu, who introduced the quark color charge and hypothesized that it might be associated with a force-carrying field. At that time, however, it was difficult to see how such a model could permanently confine quarks. Han and Nambu also assigned each quark color an integer electrical charge, so that the quarks were fractionally charged only on average, and they did not expect the quarks in their model to be permanently confined.\n\nIn 1971, Murray Gell-Mann and Harald Fritzsch proposed that the Han/Nambu color gauge field was the correct theory of the short-distance interactions of fractionally charged quarks. A little later, David Gross, Frank Wilczek, and David Politzer discovered that this theory had the property of asymptotic freedom, allowing them to make contact with experimental evidence. They concluded that QCD was the complete theory of the strong interactions, correct at all distance scales. The discovery of asymptotic freedom led most physicists to accept QCD since it became clear that even the long-distance properties of the strong interactions could be consistent with experiment if the quarks are permanently confined.\n\nAssuming that quarks are confined, Mikhail Shifman, Arkady Vainshtein and Valentine Zakharov were able to compute the properties of many low-lying hadrons directly from QCD, with only a few extra parameters to describe the vacuum. In 1980, Kenneth G. Wilson published computer calculations based on the first principles of QCD, establishing, to a level of confidence tantamount to certainty, that QCD will confine quarks. Since then, QCD has been the established theory of the strong interactions.\n\nQCD is a theory of fractionally charged quarks interacting by means of 8 bosonic particles called gluons. The gluons interact with each other, not just with the quarks, and at long distances the lines of force collimate into strings. In this way, the mathematical theory of QCD not only explains how quarks interact over short distances but also the string-like behavior, discovered by Chew and Frautschi, which they manifest over longer distances.\n\nNumerous theoretical efforts have been made to systematize the existing four fundamental interactions on the model of electroweak unification.\n\nGrand Unified Theories (GUTs) are proposals to show that the three fundamental interactions described by the Standard Model are all different manifestations of a single interaction with symmetries that break down and create separate interactions below some extremely high level of energy. GUTs are also expected to predict some of the relationships between constants of nature that the Standard Model treats as unrelated, as well as predicting gauge coupling unification for the relative strengths of the electromagnetic, weak, and strong forces (this was, for example, verified at the Large Electron–Positron Collider in 1991 for supersymmetric theories).\n\nTheories of everything, which integrate GUTs with a quantum gravity theory face a greater barrier, because no quantum gravity theories, which include string theory, loop quantum gravity, and twistor theory, have secured wide acceptance. Some theories look for a graviton to complete the Standard Model list of force-carrying particles, while others, like loop quantum gravity, emphasize the possibility that time-space itself may have a quantum aspect to it.\n\nSome theories beyond the Standard Model include a hypothetical fifth force, and the search for such a force is an ongoing line of experimental research in physics. In supersymmetric theories, there are particles that acquire their masses only through supersymmetry breaking effects and these particles, known as moduli can mediate new forces. Another reason to look for new forces is the recent discovery that the expansion of the universe is accelerating (also known as dark energy), giving rise to a need to explain a nonzero cosmological constant, and possibly to other modifications of general relativity. Fifth forces have also been suggested to explain phenomena such as CP violations, dark matter, and dark flow.\n\nIn December 2015, two observations in the ATLAS and CMS detectors at the Large Hadron Collider hinted at the existence of a new particle six times heavier than the Higgs Boson. However, after obtaining more experimental data, the anomaly appeared to not be significant.\n\n\n\n"}
{"id": "10891", "url": "https://en.wikipedia.org/wiki?curid=10891", "title": "Floppy disk", "text": "Floppy disk\n\nA floppy disk, also called a floppy, diskette, or just disk, is a type of disk storage composed of a disk of thin and flexible magnetic storage medium, sealed in a rectangular plastic enclosure lined with fabric that removes dust particles. Floppy disks are read and written by a floppy disk drive (FDD).\n\nFloppy disks, initially as media and later in 5¼-inch (133 mm) and 3½-inch (90 mm) sizes, were a ubiquitous form of data storage and exchange from the mid-1970s into the mid-2000s. By the late 2000s, computers were rarely manufactured with installed floppy disk drives; 3½-inch floppy disks can be used with an external USB floppy disk drive, but USB drives for 5¼-inch, 8-inch, and non-standard diskettes are rare to non-existent. These formats are usually handled by older equipment.\n\nWhile floppy disk drives still have some limited uses, especially with legacy industrial computer equipment, they have been superseded by data storage methods with much greater capacity, such as USB flash drives, flash storage cards, portable external hard disk drives, optical discs, ROM cartridges, and storage available through computer networks.\n\nThe first commercial floppy disks, developed in the late 1960s, are in diameter; they became commercially available in 1971 as a component of IBM products and then were sold separately beginning in 1972 by Memorex and others. These disks and associated drives were produced and improved upon by IBM and other companies such as Memorex, Shugart Associates, and Burroughs Corporation. The term \"floppy disk\" appeared in print as early as 1970, and, although, in 1973, IBM announced its first media as the \"Type 1 Diskette\", the industry continued to use the terms \"floppy disk\" or \"floppy\".\n\nIn 1976, Shugart Associates introduced the first 5¼-inch FDD. By 1978 there were more than 10 manufacturers producing such FDDs. There were competing floppy disk formats, with hard- and soft-sector versions and encoding schemes such as FM, MFM, M²FM and GCR. The 5¼-inch format displaced the 8-inch one for most applications, and the hard-sectored disk format disappeared. The most common capacity of the 5¼-inch format in DOS-based PCs was 360 KB. In 1984 IBM introduced with its PC-AT model the 1.2 MB dual-sided floppy disk, but it never became very popular. IBM started using the 720 KB double-density 3½-inch microfloppy disk on its Convertible laptop computer in 1986 and the 1.44 MB high-density version with the PS/2 line in 1987. These disk drives could be added to older PC models. In 1988 IBM introduced a drive for 2.88 MB \"DSED\" diskettes in its top-of-the-line PS/2 models, but this was a commercial failure.\n\nThroughout the early 1980s, limitations of the 5¼-inch format became clear. Originally designed to be more practical than the 8-inch format, it was itself too large; as the quality of recording media grew, data could be stored in a smaller area. A number of solutions were developed, with drives at 2-, 2½-, 3-, 3¼-, 3½- and 4-inches (and Sony's 90.0 mm × 94.0 mm disk) offered by various companies. They all shared a number of advantages over the old format, including a rigid case with a sliding metal cover over the head slot, which helped protect the delicate magnetic medium from dust and damage, and a sliding write protection tab, which was far more practical than the adhesive tabs used with earlier disks. The large market share of the 5¼-inch format made it difficult for these new formats to gain significant market share. A variant on the Sony design, introduced in 1982 by a large number of manufacturers, was then rapidly adopted; by 1988 the 3½-inch was outselling the 5¼-inch.\n\nBy the end of the 1980s, 5¼-inch disks had been superseded by 3½-inch disks. During this time, PCs frequently came equipped with drives of both sizes. By the mid-1990s, 5¼-inch drives had virtually disappeared, as the 3½-inch disk became the predominant floppy disk. The advantages of the 3½-inch disk were its higher capacity, its smaller size, and its rigid case which provided better protection from dirt and other environmental risks. However, largely due to its simpler construction (with no metal parts) the 5¼-inch disk unit price was lower throughout its history, usually in the range of a third to a half that of a 3½-inch disk.\n\nFloppy disks became ubiquitous during the 1980s and 1990s in their use with personal computers to distribute software, transfer data, and create backups. Before hard disks became affordable to the general population, floppy disks were often used to store a computer's operating system (OS). Most home computers from that period have an elementary OS and BASIC stored in ROM, with the option of loading a more advanced operating system from a floppy disk.\n\nBy the early 1990s, the increasing software size meant large packages like Windows or Adobe Photoshop required a dozen disks or more. In 1996, there were an estimated five billion standard floppy disks in use. Then, distribution of larger packages was gradually replaced by CD-ROMs, DVDs and online distribution.\n\nAn attempt to enhance the existing 3½-inch form factor was the SuperDisk in the late 1990s, utilizing very narrow data tracks and a high precision head guidance mechanism with a capacity of 120 MB and backward-compatibility with standard 3½-inch floppies; a format war briefly occurred between SuperDisk and other high-density floppy-disk products, although ultimately recordable CDs/DVDs, solid-state flash storage, and eventually online storage would obsolete all these removable disk formats. External USB-based floppy disk drives are still available, and many modern systems provide firmware support for booting from such drives.\n\nIn the mid 1990s mechanically incompatible higher-density floppy disks were introduced, like the Iomega Zip disk. Adoption was limited by the competition between proprietary formats and the need to buy expensive drives for computers where the disks would be used. In some cases, failure in market penetration was exacerbated by release of higher-capacity versions of the drive and media not backward-compatible with the original drives, dividing the users between new and old adopters. A chicken-or-the-egg scenario ensued, with consumers wary of making costly investments into unproven and rapidly changing technologies, resulting in none of the technologies becoming an established standard.\n\nLittle-used floppy disk drives sitting unused for years in computers can actually damage disks, due to computer case air cooling that sucks air through the drive openings. This pulls dust into the mechanism, which coats all surfaces including the exposed read/write head. Upon loading a disk on the rare occasion the drive is needed, the dust is stirred up when the disk is inserted into the mechanism, and the dust on the head is ground into the medium while the stirred-up dust is deposited onto the disk through the open shutters, leading to nearly instant fouling of the media. The dust also gets into the drive mechanism lubricant, turning it into a sticky slime, which may jam the mechanism and prevent head movement. This causes the computer user to see read errors on every disk they try to use, and it may also lead to data corruption through writing in incorrect locations. Some users incorrectly assume that this means all their disks have gone bad, when it is in fact the drive that is the problem. The solution is for a technician to clean the drive before it is used after a long idle period.\n\nApple introduced the iMac in 1998 with a CD-ROM drive but no floppy drive; this made USB-connected floppy drives popular accessories, as the iMac came without any writable removable media device.\n\nRecordable CDs with even greater capacity, compatible with existing infrastructure of CD-ROM drives, made the expensive new high-capacity floppy technologies obsolete. The floppy disk's remaining reusability advantage was then eliminated by re-writeable CDs. However, writing and rewriting CDs was never as fast or convenient as writing on floppy disks, when the data fits on one or two floppy disks. Networking, advancements in flash-based devices, and widespread adoption of USB provided another alternative that in turn made both floppy disks and optical storage obsolete for some purposes. The rise of file sharing and multi-megapixel digital photography encouraged the use of files larger than most 3½-inch disks, the largest-capacity standard disks, could hold. Floppy disks were commonly used as sneakernet carriers for file transfer, but the broad availability of LANs and fast Internet connections provided a simpler and faster method of transferring such files, where the air-gapping of a sneakernet is not an important benefit. Other removable storage devices have advantages in both capacity and performance when network connections are unavailable or when networks are inadequate. \n\nNonetheless, as of 2002 most manufacturers still provided floppy disk drives as standard equipment to meet user demand for file-transfer and an emergency boot device, as well as for the general secure feeling of having the familiar device. By this time, the retail cost of a floppy drive had fallen to around $20, so there was little financial incentive to omit the device from a system. Subsequently, enabled by the widespread support for USB flash drives and BIOS boot, manufacturers and retailers progressively reduced the availability of floppy disk drives as standard equipment. In February 2003, Dell announced that floppy drives would no longer be pre-installed on Dell Dimension home computers, although they were still available as a selectable option and purchasable as an aftermarket OEM add-on. As of January 2007 only 2% of computers sold in stores contained built-in floppy disk drives.\n\nFloppy disks are used for emergency boots in aging systems lacking support for other bootable media and for BIOS updates, since most BIOS and firmware programs can still be executed from bootable floppy disks. If BIOS updates fail or become corrupt, floppy drives can sometimes be used to perform a recovery. The music and theatre industries still use equipment requiring standard floppy disks (e.g. synthesizers, samplers, drum machines, sequencers, and lighting consoles). Industrial automation equipment such as programmable machinery and industrial robots may not have a USB interface; data and programs are then loaded from disks, damageable in industrial environments. This equipment may not be replaced due to cost or requirement for continuous availability; existing software emulation and virtualization do not solve this problem because no operating system is present or a customized operating system is used that has no drivers for USB devices. Hardware floppy disk emulators can be made to interface floppy-disk controllers to a USB port that can be used for flash drives.\n\nCorporate computer environments may still make use of floppy disks for older machines that do not support the current company networks and, in the case of laptops, where Wi-Fi is not considered secure. The floppy disk provides for a controlled means of file transfer by permitting only a few files to be transmitted. This is as USB ports on enterprise computer terminals/workstations are often disabled in order to prevent employees from using a flash memory drive to take large amounts of data for unauthorized use. The capacity limit and low transfer speed of the floppy disk are advantages in this case. In addition, the loss of a floppy disk has less consequence than the loss of any modern piece of removable flash storage, and furthermore, the floppy disk, being much larger than most flash memory card formats—some of which are smaller than a fingernail—is harder to lose.\n\nThe abundance of old floppy drives has enabled forensics investigators to repurpose older computers as \"front-ends\" for forensic access to old floppy disks that have only recently been unearthed from some crime scenes.\n\nIn May 2016 the United States Government Accountability Office released a report that covered the need to upgrade or replace legacy computer systems within Federal Agencies. According to this document, old IBM Series/1 minicomputers running on 8-inch floppy disks are still used to coordinate \"the operational functions of the United States’ nuclear forces...\" The government plans to update some of the technology by the end of the 2017 fiscal year.\n\nWindows 10 no longer includes a generic driver for USB floppy drives.\n\nFor more than two decades, the floppy disk was the primary external writable storage device used. Most computing environments before the 1990s were non-networked, and floppy disks were the primary means of transferring data between computers, a method known informally as sneakernet. Unlike hard disks, floppy disks are handled and seen; even a novice user can identify a floppy disk. Because of these factors, a picture of a 3½-inch floppy disk has become an interface metaphor for saving data. The floppy disk symbol is still used by software on user-interface elements related to saving files, such as the release of Microsoft Office 2013, even though the physical floppy disks are largely obsolete.\n\nThe 8-inch and 5¼-inch floppy disks contain a magnetically coated disk-shaped plastic medium with a large circular hole in the center for a drive's spindle. The medium is contained in square shaped plastic cover that has a small oval aperture in both sides of a to allow the drive's heads to read and write data and a large hole in the center to allow the magnetic medium to spin by rotating it from its middle hole.\n\nInside the cover are two layers of fabric, with the magnetic medium sandwiched in the middle. The fabric is designed to reduce friction between the medium and the outer cover, and catch particles of debris abraded off the disk to keep them from accumulating on the heads. The cover is usually a one-part sheet, double-folded with flaps glued or spot-welded together. \n\nA small notch disk identifies that it is writable, detected by a mechanical switch or phototransistor above it; if it is not present, the disk can be written; in the 8-inch disk the notch is covered to enable writing while in the 5¼-inch disk the notch is open to enable writing. Tape may be used over the notch to change the mode of the disk. Punch devices were sold to convert read-only disks to writable ones and enable writing on the unused side of single sided disks; such modified disks became known as flippy disks. \n\nAnother LED/photo-transistor pair located near the center of the disk detects the \"index hole\" once per rotation in the magnetic disk; it is used to detect the angular start of each track and whether or not the disk is rotating at the correct speed. Early 8‑inch and 5¼‑inch disks had physical holes for each sector and were termed \"hard sectored\" disks. Later \"soft-sectored\" disks have only one index hole, and sector position is determined by the disk controller or low-level software from patterns marking the start of a sector. Generally, the same drives are used to read and write both types of disks, with only the disks and controllers differing. Some operating systems utilizing soft sectors, such as Apple DOS, do not use the index hole; the drives designed for such systems often lack the corresponding sensor; this was mainly a hardware cost-saving measure.\n\nThe core of the 3½-inch disk is the same as the other two disks, but the front has only a label and a small aperture for reading and writing data, protected by the \"slider\" — a spring-loaded metal or plastic cover, pushed to the side on entry into the drive. Rather than having a hole in the center, it has a metal hub which mates to the spindle of the drive. Typical 3½-inch disk magnetic coating materials are:\n\n\nTwo holes at the bottom left and right indicate whether the disk is write-protected and whether it is high-density; these holes are spaced as far apart as the holes in punched A4 paper, allowing write-protected high-density floppies to be clipped into standard ring binders. A notch at top right ensures that the disk is in the correct orientation and an arrow at top left indicating direction of insertion. The drive usually has a button that when pressed ejects the disk with varying degrees of force, the discrepancy due to the ejection force provided by the spring of the slider cover. In IBM PC compatibles, Commodores, Apple II/IIIs, and other non-Apple-Macintosh machines with standard floppy disk drives, a disk may be inserted or ejected manually at any time. The drive has a disk-change switch that detects when a disk is ejected or inserted. Failure of this mechanical switch is a common source of disk corruption if a disk is changed and the drive (and hence the operating system) fails to notice.\n\nOne of the chief usability problems of the floppy disk is its vulnerability; even inside a closed plastic housing, the disk medium is highly sensitive to dust, condensation and temperature extremes. As with all magnetic storage, it is vulnerable to magnetic fields. Blank disks have been distributed with an extensive set of warnings, cautioning the user not to expose it to dangerous conditions. Disks must not be roughly treated or removed from the drive while the magnetic media is still spinning, since doing so is likely to cause damage to the disk, drive head, or stored data. On the other hand, the 3½‑inch floppy has been lauded for its mechanical usability by HCI expert Donald Norman:\n\nA spindle motor in the drive rotates the magnetic medium at a certain speed, while a stepper motor-operated mechanism moves the magnetic read/write head(s) radially along the surface of the disk. Both read and write operations require the media to be rotating and the head to contact the disk media, an action originally accomplished by a disk-load solenoid. Later drives held the heads out of contact until a front-panel lever was rotated (5¼\") or disk insertion was complete (3½\"). To write data, current is sent through a coil in the head as the media rotates. The head's magnetic field aligns the magnetization of the particles directly below the head on the media. When the current is reversed the magnetization aligns in the opposite direction, encoding one bit of data. To read data, the magnetization of the particles in the media induce a tiny voltage in the head coil as they pass under it. This small signal is amplified and sent to the floppy disk controller, which converts the streams of pulses from the media into data, checks it for errors, and sends it to the host computer system.\n\nA blank unformatted diskette has a coating of magnetic oxide with no magnetic order to the particles. During formatting, the magnetizations of the particles are aligned forming tracks, each broken up into sectors, enabling the controller to properly read and write data. The tracks are concentric rings around the center, with spaces between tracks where no data is written; gaps with padding bytes are provided between the sectors and at the end of the track to allow for slight speed variations in the disk drive, and to permit better interoperability with disk drives connected to other similar systems. Each sector of data has a header that identifies the sector location on the disk. A cyclic redundancy check (CRC) is written into the sector headers and at the end of the user data so that the disk controller can detect potential errors. Some errors are soft and can be resolved by automatically re-trying the read operation; other errors are permanent and the disk controller will signal a failure to the operating system if multiple attempts to read the data still fail.\n\nAfter a disk is inserted, a catch or lever at the front of the drive is manually lowered to prevent the disk from accidentally emerging, engage the spindle clamping hub, and in two-sided drives, engage the second read/write head with the media. In some 5¼-inch drives, insertion of the disk compresses and locks an ejection spring which partially ejects the disk upon opening the catch or lever. This enables a smaller concave area for the thumb and fingers to grasp the disk during removal. Newer 5¼-inch drives and all 3½-inch drives automatically engage the spindle and heads when a disk is inserted, doing the opposite with the press of the eject button. On Apple Macintosh computers with built-in floppy drives, the ejection button is replaced by software controlling an ejection motor which only does so when the operating system no longer needs to access the drive. The user could drag the image of the floppy drive to the trash can on the desktop to eject the disk. In the case of a power failure or drive malfunction, a loaded disk can be removed manually by inserting a straightened paper clip into a small hole at the drive's front panel, just as one would do with a CD-ROM drive in a similar situation.\n\nBefore a disk can be accessed, the drive needs to synchronize its head position with the disk tracks. In some drives, this is accomplished with a Track Zero Sensor, while for others it involves the drive head striking an immobile reference surface. In either case, the head is moved so that it is approaching track zero position of the disk. When a drive with the sensor has reached track zero, the head stops moving immediately and is correctly aligned. For a drive without the sensor, the mechanism attempts to move the head the maximum possible number of positions needed to reach track zero, knowing that once this motion is complete, the head will be positioned over track zero.\n\nSome drive mechanisms such as the Apple II 5.25 inch drive without a track zero sensor, produce very characteristic mechanical noises when trying to move the heads past the reference surface. This physical striking is responsible for the 5.25 inch drive clicking during the boot of an Apple II, and the loud rattles of its DOS and ProDOS when disk errors occurred and track zero synchronization was attempted.\n\nDifferent sizes of floppy disks are mechanically incompatible, and disks can fit only one size of drive. Drive assemblies with both 3½-inch and 5¼-inch slots were available during the transition period between the sizes, but they contained two separate drive mechanisms. In addition, there are many subtle, usually software-driven incompatibilities between the two. 5¼-inch disks formatted for use with Apple II computers would be unreadable and treated as unformatted on a Commodore. As computer platforms began to form, attempts were made at interchangeability. For example, the \"Superdrive\" included from the Macintosh SE to the Power Macintosh G3 could read, write and format IBM PC format 3½-inch disks, but few IBM-compatible computers had drives that did the reverse. 8-inch, 5¼-inch and 3½-inch drives were manufactured in a variety of sizes, most to fit standardized drive bays. Alongside the common disk sizes were non-classical sizes for specialized systems.\n\nThe first floppy disk was 8 inches in diameter, was protected by a flexible plastic jacket and was a read-only device used by IBM as a way of loading microcode. Read/write floppy disks and their drives became available in 1972 but it was IBM's 1973 introduction of the 3740 data entry system that began the establishment of floppy disks, called by IBM the \"Diskette 1,\" as an industry standard for information interchange. Early microcomputers used for engineering, business, or word processing often used one or more 8-inch disk drives for removable storage; the CP/M operating system was developed for microcomputers with 8-inch drives.\n\nThe family of 8-inch disks and drives increased over time and later versions could store up to 1.2 MB; many microcomputer applications did not need that much capacity on one disk, so a smaller size disk with lower-cost media and drives was feasible. The 5¼-inch drive succeeded the 8-inch size in many applications, and developed to about the same storage capacity as the original 8-inch size, using higher-density media and recording techniques.\n\nThe head gap of an 80‑track high-density (1.2 MB in the MFM format) 5¼‑inch drive (a.k.a. Mini diskette, Mini disk, or Minifloppy) is smaller than that of a 40‑track double-density (360 KB) drive but can format, read and write 40‑track disks well provided the controller supports double stepping or has a switch to do such a process. 5.25-inch 80-track drives were also called hyper drives.\nA blank 40‑track disk formatted and written on an 80‑track drive can be taken to its native drive without problems, and a disk formatted on a 40‑track drive can be used on an 80‑track drive. Disks written on a 40‑track drive and then updated on an 80 track drive become unreadable on any 40‑track drives due to track width incompatibility.\n\nSingle sided disks were coated on both sides, despite the availability of more expensive double sided disks. The reason usually given for the higher cost was that double sided disks were certified error-free on both sides of the media. Double-sided disks could be used in some drives for single-sided disks, as long as an index signal was not needed. This was done one side at a time, by turning them over (flippy disks); more expensive dual-head drives which could read both sides without turning over were later produced, and eventually became used universally.\n\nIn the early 1980s, a number of manufacturers introduced smaller floppy drives and media in various formats. A consortium of 21 companies eventually settled on a 3½-inch floppy disk (actually 90 mm wide) a.k.a. Micro diskette, Micro disk, or Micro floppy, similar to a Sony design, but improved to support both single-sided and double-sided media, with formatted capacities generally of 360 KB and 720 KB respectively. Single-sided drives shipped in 1983, and double sided in 1984. What became the most common format, the double-sided, high-density (HD) 1.44 MB disk drive, shipped in 1986.\n\nThe first Macintosh computers used single-sided 3½-inch floppy disks, but with 400 KB formatted capacity. These were followed in 1986 by double-sided 800 KB floppies. The higher capacity was achieved at the same recording density by varying the disk rotation speed with head position so that the linear speed of the disk was closer to constant. Later Macs could also read and write 1.44 MB HD disks in PC format with fixed rotation speed.\n\nAll 3½-inch disks have a rectangular hole in one corner which, if obstructed, write-enabled the disk. A sliding detented piece can be moved to block or reveal the part of the rectangular hole that is sensed by the drive. The HD 1.44 MB disks have a second, unobstructed hole in the opposite corner which identifies them as being of that capacity.\n\nIn IBM-compatible PCs, the three densities of 3½-inch floppy disks are backwards-compatible: higher density drives can read, write and format lower density media. It is also possible to format a disk at a lower density than it was intended for, but only if the disk is first thoroughly demagnetized with a bulk eraser, as the high density format is magnetically stronger and will prevent the disk from working in lower density modes.\n\nWriting at different densities than disks were intended for, sometimes by altering or drilling holes, was possible but deprecated. The holes on the right side of a 3½‑inch disk can be altered as to make some disk drives and operating systems treat the disk as one of higher or lower density, for bidirectional compatibility or economical reasons. Some computers, such as the PS/2 and Acorn Archimedes, ignored these holes altogether.\n\nIt is possible to make a 3½-inch floppy disk drive be recognized by a system as a 5¼‑inch 360 KB or 1200 KB drive, and to read and write disks with the same number of tracks and sectors as those disks; this had some application in data exchange with obsolete CP/M systems.\n\nOther, smaller, floppy sizes were proposed, especially for portable or pocket-sized devices that needed a smaller storage device. 3-inch disks similar in construction to 3½-inch were manufactured and used for a time, particularly by Amstrad computers and word processors. A 2-inch nominal size was introduced by Sony for compact pocket computers and was used with some electronic musical instrument controllers. Neither of these sizes achieved much market success.\n\nFloppy disk size is often referred to in inches, even in countries using metric and though the size is defined in metric. The ANSI specification of 3½-inch disks is entitled in part \"90 mm (3.5 inch)\" though 90 mm is closer to 3.54 inches. Formatted capacities are generally set in terms of kilobytes and megabytes.\n\nData is generally written to floppy disks in sectors (angular blocks) and tracks (concentric rings at a constant radius). For example, the HD format of 3½-inch floppy disks uses 512 bytes per sector, 18 sectors per track, 80 tracks per side and two sides, for a total of 1,474,560 bytes per disk. Some disk controllers can vary these parameters at the user's request, increasing storage on the disk, although they may not be able to be read on machines with other controllers. For example, Microsoft applications were often distributed on 3½-inch 1.68 MB DMF disks formatted with 21 sectors instead of 18; they could still be recognized by a standard controller. On the IBM PC, MSX and most other microcomputer platforms, disks were written using a constant angular velocity (CAV) format, with the disk spinning at a constant speed and the sectors holding the same amount of information on each track regardless of radial location.\n\nBecause the sectors have constant angular size, the 512 bytes in each sector are compressed more near the disk's center. A more space-efficient technique would be to increase the number of sectors per track toward the outer edge of the disk, from 18 to 30 for instance, thereby keeping nearly constant the amount of physical disk space used for storing each sector; an example is zone bit recording. Apple implemented this in early Macintosh computers by spinning the disk more slowly when the head was at the edge, while maintaining the data rate, allowing 400 KB of storage per side and an extra 160 KB on a double-sided disk. This higher capacity came with a disadvantage: the format used a unique drive mechanism and control circuitry, meaning that Mac disks could not be read on other computers. Apple eventually reverted to constant angular velocity on HD floppy disks with their later machines, still unique to Apple as they supported the older variable-speed formats.\n\nDisk formatting is usually done by a utility program supplied by the computer OS manufacturer; generally, it sets up a file storage directory system on the disk, and initializes its sectors and tracks. Areas of the disk unusable for storage due to flaws can be locked (marked as \"bad sectors\") so that the operating system does not attempt to use them. This was time consuming so many environments had quick formatting which skipped the error checking process. When floppy disks were often used, disks pre-formatted for popular computers were sold. The unformatted capacity of a floppy disk does not include the sector and track headings of a formatted disk; the difference in storage between them depends on the drive's application. Floppy disk drive and media manufacturers specify the unformatted capacity (for example, 2 MB for a standard 3½-inch HD floppy). It is implied that this should not be exceeded, since doing so will most likely result in performance problems. DMF was introduced permitting 1.68 MB to fit onto an otherwise standard 3½-inch disk; utilities then appeared allowing disks to be formatted as such.\n\nMixtures of decimal prefixes and binary sector sizes require care to properly calculate total capacity. Whereas semiconductor memory naturally favors powers of two (size doubles each time an address pin is added to the integrated circuit), the capacity of a disk drive is the product of sector size, sectors per track, tracks per side and sides (which in hard disk drives with multiple platters can be greater than 2). Although other sector sizes have been known in the past, formatted sector sizes are now almost always set to powers of two (256 bytes, 512 bytes, etc.), and, in some cases, disk capacity is calculated as multiples of the sector size rather than only in bytes, leading to a combination of decimal multiples of sectors and binary sector sizes. For example, 1.44 MB 3½-inch HD disks have the \"M\" prefix peculiar to their context, coming from their capacity of 2,880 512-byte sectors (1,440 KiB), consistent with neither a decimal megabyte nor a binary mebibyte (MiB). Hence, these disks hold 1.47 MB or 1.41 MiB. Usable data capacity is a function of the disk format used, which in turn is determined by the FDD controller and its settings. Differences between such formats can result in capacities ranging from approximately 1300 to 1760 KiB (1.80 MB) on a standard 3½-inch high-density floppy (and up to nearly 2 MB with utilities such as 2M/2MGUI). The highest capacity techniques require much tighter matching of drive head geometry between drives, something not always possible and unreliable. For example, the LS-240 drive supports a 32 MB capacity on standard 3½-inch HD disks, but it is, however, a write-once technique, and requires its own drive.\n\nThe raw maximum transfer rate of 3½-inch HD floppy drives and interfaces, disregarding overheads, is as much as 1,000 kilobits/s, or approximately 83% that of single-speed CD‑ROM (71% of audio CD). This represents the speed of raw data bits moving under the read head; however, the effective speed is somewhat less due to space used for headers, gaps and other format fields.\n\n\n\n"}
{"id": "10893", "url": "https://en.wikipedia.org/wiki?curid=10893", "title": "Fencing", "text": "Fencing\n\nFencing is a sport in which two competitors fight using 'rapier-style' swords, called the foil, the épée, and the sabre; winning points are made through the contact with an opponent. Fencing was one of the first sports to be played in the Olympics. Based on the traditional skills of swordsmanship, the modern sport arose at the end of the 19th century, with the Italian school having modified the historical European martial art of classical fencing, and the French school later refining the Italian system. There are three forms of modern fencing, each uses a different kind of weapon and has different rules, this way the sport itself is divided into three competitive scenes: foil, épée, and sabre. Most competitive fencers choose to specialize in one weapon only.\n\nCompetitive fencing is one of the five activities which have been featured in every modern Olympic Games, the other four being athletics, cycling, swimming, and gymnastics.\n\nFencing is governed by Fédération Internationale d'Escrime (FIE). Today, its head office is in Lausanne, Switzerland. The FIE is composed of 145 national federations, each of which is recognised by its state Olympic Committee as the sole representative of Olympic-style fencing in that country.\n\nThe FIE maintains the current rules used by FIE sanctioned international events, including world cups, world championships and the Olympic Games. The FIE handles proposals to change the rules the first year after an Olympic year in the annual congress. The US Fencing Association has slightly different rules, but usually adheres to FIE standards.\n\nFencing traces its roots to the development of swordsmanship for duels and self defense. The origin of modern fencing is believed to be the Spain. Some of the most significant books on fencing were written by Spanish fencers. \"Treatise on Arms\" was written by Diego de Valera between 1458 and 1471 and is one of the oldest surviving manuals on western fencing shortly before dueling came under official ban by the Catholic Monarchs. In conquest, the Spanish forces carried fencing around the world, particularly to southern Italy, one of the major areas of strife between both nations. Fencing was mentioned in the play \"The Merry Wives of Windsor\" written sometime prior to 1602.\n\nThe mechanics of modern fencing originated in the 18th century in an Italian school of fencing of the Renaissance, and under their influence, were improved by the French school of fencing. The Spanish school of fencing stagnated and was replaced by the Italian and French schools.\n\nThe shift towards fencing as a sport rather than as military training happened from the mid-18th century, and was led by Domenico Angelo, who established a fencing academy, Angelo's School of Arms, in Carlisle House, Soho, London in 1763. There, he taught the aristocracy the fashionable art of swordsmanship. His school was run by three generations of his family and dominated the art of European fencing for almost a century.\n\nHe established the essential rules of posture and footwork that still govern modern sport fencing, although his attacking and parrying methods were still much different from current practice. Although he intended to prepare his students for real combat, he was the first fencing master to emphasize the health and sporting benefits of fencing more than its use as a killing art, particularly in his influential book \"L’École des armes\" (\"The School of Fencing\"), published in 1763.\n\nThe first regularized fencing competition was held at the inaugural Grand Military Tournament and Assault at Arms in 1880, held at the Royal Agricultural Hall, in Islington in June. The Tournament featured a series of competitions between army officers and soldiers. Each bout was fought for five hits and the foils were pointed with black to aid the judges. The Amateur Gymnastic & Fencing Association drew up an official set of fencing regulations in 1896.\n\nFencing was part of the Olympics Games in the summer of 1896. Sabre events have been held at every Summer Olympics; foil events have been held at every Summer Olympics except 1908; épée events have been held at every Summer Olympics except in the summer of 1896 because of unknown reasons.\n\nStarting with épée in 1933, side judges were replaced by the Laurent-Pagan electrical scoring apparatus, with an audible tone and a red or green light indicating when a touch landed. Foil was automated in 1956, sabre in 1988. The scoring box reduced the bias in judging, and permitted more accurate scoring of faster actions, lighter touches, and more touches to the back and flank than before.\n\nThere are three weapons in modern fencing: foil, épée, and sabre. Each weapon has its own rules and strategies. Equipment needed includes at least 2 swords, a Lame (not for épée), a white jacket, underarm protector, two body and mask cords, knee high socks, glove and knickers. \n\nThe foil is a light thrusting weapon with a maximum weight of 500 grams. The foil targets the torso, but not the arms or legs. The foil has a small circular hand guard that serves to protect the hand from direct stabs. As the hand is not a valid target in foil, this is primarily for safety. Touches are scored only with the tip; hits with the side of the blade do not register on the electronic scoring apparatus (and do not halt the action). Touches that land outside of the target area (called an \"off-target touch\" and signaled by a distinct color on the scoring apparatus) stop the action, but are not scored. Only a single touch can be award to either fencer at the end of a phrase. If both fencers land touches within a close enough interval of milliseconds to register two lights on the machine, the referee uses the rules of \"right of way\" to determine which fencer is awarded the touch, or if an off-target hit has priority over a valid hit, in which case no touch is awarded. If the referee is unable to determine which fencer has right of way, no touch is awarded.\n\nThe épée is a thrusting weapon like the foil, but heavier, with a maximum total weight of 775 grams. In épée, the entire body is valid target. The hand guard on the épée is a large circle that extends towards the pommel, effectively covering the hand, which is a valid target in épée. Like foil, all hits must be with the tip and not the sides of the blade. Hits with the side of the blade do not register on the electronic scoring apparatus (and do not halt the action). As the entire body is legal target, there is no concept of an off-target touch, except if the fencer accidentally strikes the floor, setting off the light and tone on the scoring apparatus. Unlike foil and sabre, épée does not use \"right of way\", and awards simultaneous touches to both fencers. However, if the score is tied in a match at the last point and a double touch is scored, the point is null and void.\n\nThe sabre is a light cutting and thrusting weapon that targets the entire body above the waist, except the weapon hand. Saber is the newest weapon to be used. Like the foil, the maximum legal weight of a sabre is 500 grams. The hand guard on the sabre extends from hilt to the point at which the blade connects to the pommel. This guard is generally turned outwards during sport to protect the sword arm from touches. Hits with the entire blade or point are valid. As in foil, touches that land outside of the target area are not scored. However, unlike foil, these \"off-target\" touches do not stop the action, and the fencing continues. In the case of both fencers landing a scoring touch, the referee determines which fencer receives the point for the action, again through the use of \"right of way\".\n\nMost personal protective equipment for fencing is made of tough cotton or nylon. Kevlar was added to top level uniform pieces (jacket, breeches, underarm protector, lamé, and the bib of the mask) following the death of Vladimir Smirnov at the 1982 World Championships in Rome. However, Kevlar breaks down into chlorine in UV light, complicating the cleaning process.\n\nOther ballistic fabrics, such as Dyneema, have been developed that resist puncture, and which do not degrade the way that Kevlar does. FIE rules state that tournament wear must be made of fabric that resists a force of , and that the mask bib must resist twice that amount.\n\nThe complete fencing kit includes:\n\n\nTraditionally, the fencer's uniform is white, and an instructor's uniform is black. This may be due to the occasional pre-electric practice of covering the point of the weapon in dye, soot, or colored chalk in order to make it easier for the referee to determine the placing of the touches. As this is no longer a factor in the electric era, the FIE rules have been relaxed to allow colored uniforms (save black). The guidelines also limit the permitted size and positioning of sponsorship logos.\n\nA set of electric fencing equipment is required to participate in electric fencing. Electric equipment in fencing varies depending on the weapon with which it is used in accordance. The main component of a set of electric equipment is the body cord. The body cord serves as the connection between a fencer and a reel of wire that is part of a system for electrically detecting that the weapon has touched the opponent. There are two types: one for épée, and one for foil and sabre.\n\nÉpée body cords consist of two sets of three prongs each connected by a wire. One set plugs into the fencer's weapon, with the other connecting to the reel. Foil and sabre body cords have only two prongs (or a twist-lock bayonet connector) on the weapon side, with the third wire connecting instead to the fencer's lamé. The need in foil and sabre to distinguish between on and off-target touches requires a wired connection to the valid target area.\n\nA body cord consists of three wires known as the A, B, and C lines. At the reel connector (and both connectors for Épée cords) The B pin is in the middle, the A pin is 1.5 cm to one side of B, and the C pin is 2 cm to the other side of B. This asymmetrical arrangement ensures that the cord cannot be plugged in the wrong way around.\n\nIn foil, the A line is connected to the lamé and the B line runs up a wire to the tip of the weapon. The B line is normally connected to the C line through the tip. When the tip is depressed, the circuit is broken and one of three things can happen:\n\nIn Épée, the A and B lines run up separate wires to the tip (there is no lamé). When the tip is depressed, it connects the A and B lines, resulting in a valid touch. However, if the tip is touching your opponents weapon (their C line) or the grounded strip, nothing happens when it is depressed, as the current is redirected to the C line. Grounded strips are particularly important in Épée, as without one, a touch to the floor registers as a valid touch (rather than off-target as in Foil).\n\nIn Sabre, similarly to Foil, the A line is connected to the lamé, but both the B and C lines are connected to the body of the weapon. Any contact between your B/C line (doesn't matter which, as they are always connected) and your opponent's A line (their lamé) results in a valid touch. There is no need for grounded strips in Sabre, as hitting something other than your opponent's lame does nothing.\n\nIn a professional fencing competition, a complete set of electric equipment is needed.\n\nA complete set of foil electric equipment includes:\nThe electric equipment of sabre is very similar to that of foil. In addition, equipment used in sabre includes:\nÉpée fencers lack a lamé, conductive bib, and head cord due to their target area. Also, their body cords are constructed differently as described above. However, they possess all of the other components of a foil fencer's equipment.\n\nTechniques or movements in fencing can be divided into two categories: offensive and defensive. Some techniques can fall into both categories (\"e.g.\" the beat). Certain techniques are used offensively, with the purpose of landing a hit on your opponent while holding the right of way (foil and sabre). Others are used defensively, to protect against a hit or obtain the right of way.\n\nThe attacks and defences may be performed in countless combinations of feet and hand actions. For example, fencer A attacks the arm of fencer B, drawing a high outside parry; fencer B then follows the parry with a high line riposte. Fencer A, expecting that, then makes his own parry by pivoting his blade under fencer B's weapon (from straight out to more or less straight down), putting fencer B's tip off target and fencer A now scoring against the low line by angulating the hand upwards.\n\nWhenever a point is scored, the fencers will go back to their starting mark. The fight will start again after the following commands have been given by the referee (in French in international settings): \"En garde\" (On guard), \"Êtes-vous prêts ?\" (Are you ready?), \"Allez\" (Fence!).\n\n\n\nFencing has a long history with universities and schools for at least 500 years. At least one style of fencing, Mensur in Germany, is practiced only within universities, notably at Heidelberg. University students compete internationally at the World University Games. The United States holds two national level university tournaments including the NCAA championship and the USACFC National Championships tournaments in the USA and the BUCS fencing championships in the United Kingdom. Prior to advances in modern weaponry post World War I, the United States Cavalry taught swordsmanship (mounted and dismounted) in Fort Riley, Kansas at its Mounted Service School. George S. Patton, Jr., while still a young lieutenant, was named “Master of the Sword,” an honor reserved for the top instructor. He invented what came to be known as the “Patton Saber,” in 1913, based on his studies with M. Clery L’Adjutant, reputed to be the finest Fencing Master in Europe at the time. While teaching at Fort Riley, he wrote two training manuals teaching the art of swordsmanship to Army Cavalry Officers, \"Saber Exercise 1914\" and \"Diary of the Instructor in Swordsmanship.\" Equipment costs and the relatively small scale of the sport limits university fencing to a small number of schools. National fencing organisations have set up programmes to encourage more students to fence. Examples include the Regional Youth Circuit program in the USA and the Leon Paul Youth Development series in the UK.\n\nIn recent years, attempts have been made to introduce fencing to a wider and younger audience, by using foam and plastic swords, which require much less protective equipment. This makes it much less expensive to provide classes, and thus easier to take fencing to a wider range of schools than traditionally has been the case. There is even a competition series in Scotland – the Plastic-and-Foam Fencing FunLeague – specifically for Primary and early Secondary school-age children using this equipment.\n\nThe UK hosts two national competitions in which schools compete against each other directly: the Public Schools Fencing Championship, a competition only open to Independent Schools, and the Scottish Secondary Schools Championships, open to all secondary schools in Scotland. It contains both teams and individual events and is highly anticipated. Schools organise matches directly against one another and school age pupils can compete individually in the British Youth Championships.\n\nMany universities in Ontario, Canada have fencing teams that participate in an annual inter-university competition called the OUA Finals.\n\nThe Venetian school of fencing is a style of fencing that occurred in Venice in the early 12th century, and prevailed until the beginning of the XIХ century. \n\nThe Venetians were masters of the art, and shared with their colleagues of Bologna the sound principles of fencing known as Bolognese or Venetian. \nFor the first time Venetian fencing was detailed in some directions, it was described the properties of different parts of the blade, which were used in defense and offense. With this approach, the swordsman had an idea of one thing, what now we calling like \"center of percussion\". It was suggested some divisions of a sword. The blade was divided into four parts, the first two parts from Ephesus should be used for protection; the third one near the center of the blow was used for striking; and the fourth part at the tip was used for pricking. \n\nOther variants include wheelchair fencing for those with disabilities, chair fencing, \"one-hit épée\" (one of the five events which constitute modern pentathlon) and the various types of non-Olympic competitive fencing. Chair fencing is similar to wheelchair fencing, but for the able bodied. The opponents set up opposing chairs and fence while seated; all the usual rules of fencing are applied. An example of the latter is the American Fencing League (distinct from the United States Fencing Association): the format of competitions is different and the right of way rules are interpreted in a different way. In a number of countries, school and university matches deviate slightly from the FIE format.\n\n\n\n"}
{"id": "10894", "url": "https://en.wikipedia.org/wiki?curid=10894", "title": "The Free Software Definition", "text": "The Free Software Definition\n\nThe Free Software Definition written by Richard Stallman and published by Free Software Foundation (FSF), defines free software as being software that ensures that the end users have freedom in using, studying, sharing and modifying that software. The term \"free\" is used in the sense of \"free speech,\" not of \"free of charge.\" The earliest known publication of the definition was in the February 1986 edition of the now-discontinued GNU's Bulletin publication of FSF. The canonical source for the document is in the philosophy section of the GNU Project website. , it is published there in 39 languages. FSF publishes a list of licences which meet this definition.\n\nThe definition published by FSF in February 1986 had two points:\nIn 1996, when the gnu.org website was launched, \"free software\" was defined referring to \"three levels of freedom\" by adding an explicit mention of the freedom to study the software (which could be read in the two-point definition as being part of the freedom to change the program). Stallman later avoided the word \"levels\", saying that you need all of the freedoms, so it's misleading to think in terms of levels.\n\nFinally, another freedom was added, to explicitly say that users should be able to run the program. The existing freedoms were already numbered one to three, but this freedom should come before the others, so it was added as \"freedom zero\".\n\nThe modern definition defines free software by whether or not the recipient has the following four freedoms:\nFreedoms 1 and 3 require source code to be available because studying and modifying software without its source code is highly impractical.\n\nIn July 1997, Bruce Perens published the Debian Free Software Guidelines. This was also used by Open Source Initiative (OSI) under the name \"\"The Open Source Definition\"\", the only change being that use of the term \"free software\" was replaced by OSI's alternative term for free software, \"open-source software\".\n\nDespite the philosophical differences between the free software movement and the open source movement, the official definitions of free software by the Free Software Foundation and of open source software by the Open Source Initiative basically refer to the same software licences, with a few minor exceptions. While stressing the philosophical differences, the Free Software Foundation comments:\n\n\n"}
{"id": "10896", "url": "https://en.wikipedia.org/wiki?curid=10896", "title": "Felix Bloch", "text": "Felix Bloch\n\nFelix Bloch (23 October 1905 – 10 September 1983) was a Swiss physicist, working mainly in the U.S. He and Edward Mills Purcell were awarded the 1952 Nobel Prize for Physics for \"their development of new ways and methods for nuclear magnetic precision measurements.\" In 1954–1955, he served for one year as the first Director-General of CERN.\n\nBloch was born in Zürich, Switzerland to Jewish parents Gustav and Agnes Bloch.\n\nHe was educated at the Cantonal Gymnasium in Zurich and at the Eidgenössische Technische Hochschule (ETHZ), also in Zürich. Initially studying engineering he soon changed to physics. During this time he attended lectures and seminars given by Peter Debye and Hermann Weyl at ETH Zürich and Erwin Schrödinger at the neighboring University of Zürich. A fellow student in these seminars was John von Neumann. Graduating in 1927 he continued his physics studies at the University of Leipzig with Werner Heisenberg, gaining his doctorate in 1928. His doctoral thesis established the quantum theory of solids, using Bloch waves to describe the electrons.\n\nIn 1940 he married Lore Misch.\n\nHe remained in European academia, studying with Wolfgang Pauli in Zürich, Niels Bohr in Copenhagen and Enrico Fermi in Rome before he went back to Leipzig assuming a position as privatdozent (lecturer). In 1933, immediately after Hitler came to power, he left Germany because he was Jewish. He emigrated to work at Stanford University in 1934. In the fall of 1938, Bloch began working with the 37\" cyclotron at the University of California at Berkeley to determine the magnetic moment of the neutron. Bloch went on to become the first professor for theoretical physics at Stanford. In 1939, he became a naturalized citizen of the United States. During WW II he worked on nuclear power at Los Alamos National Laboratory, before resigning to join the radar project at Harvard University.\n\nAfter the war he concentrated on investigations into nuclear induction and nuclear magnetic resonance, which are the underlying principles of MRI.\n\nAt Stanford, he was the advisor of Carson D. Jeffries, who became a professor of Physics at the University of California, Berkeley.\n\nIn 1964, he was elected a foreign member of the Royal Netherlands Academy of Arts and Sciences.\n\nHe died in Zurich.\n\n\n\n\n"}
{"id": "10897", "url": "https://en.wikipedia.org/wiki?curid=10897", "title": "Fugue", "text": "Fugue\n\nIn music, a fugue ( ) is a contrapuntal compositional technique in two or more voices, built on a subject (a musical theme) that is introduced at the beginning in imitation (repetition at different pitches) and which recurs frequently in the course of the composition. A fugue usually has three sections: an exposition, a development, and a final entry that contains the return of the subject in the fugue's tonic key. Some fugues have a recapitulation. In the Middle Ages, the term was widely used to denote any works in canonic style; by the Renaissance, it had come to denote specifically imitative works. Since the 17th century, the term \"fugue\" has described what is commonly regarded as the most fully developed procedure of imitative counterpoint.\n\nMost fugues open with a short main theme, the subject, which then sounds successively in each voice (after the first voice is finished stating the subject, a second voice repeats the subject at a different pitch, and other voices repeat in the same way); when each voice has entered, the \"exposition\" is complete. This is often followed by a connecting passage, or \"episode\", developed from previously heard material; further \"entries\" of the subject then are heard in related keys. Episodes (if applicable) and entries are usually alternated until the \"final entry\" of the subject, by which point the music has returned to the opening key, or tonic, which is often followed by closing material, the coda. In this sense, a fugue is a style of composition, rather than a fixed structure.\n\nThe form evolved during the 18th century from several earlier types of contrapuntal compositions, such as imitative ricercars, capriccios, canzonas, and fantasias. The famous fugue composer Johann Sebastian Bach (1685–1750) shaped his own works after those of Johann Jakob Froberger (1616–1667), Johann Pachelbel (1653–1706), Girolamo Frescobaldi (1583–1643), Dieterich Buxtehude (c. 1637–1707) and others. With the decline of sophisticated styles at the end of the baroque period, the fugue's central role waned, eventually giving way as sonata form and the symphony orchestra rose to a dominant position. Nevertheless, composers continued to write and study fugues for various purposes; they appear in the works of Wolfgang Amadeus Mozart (1756–1791) and Ludwig van Beethoven (1770–1827), as well as modern composers such as Dmitri Shostakovich (1906–1975).\n\nThe English term \"fugue\" originated in the 16th century and is derived from the French word \"fugue\" or the Italian \"fuga\". This in turn comes from Latin, also \"fuga\", which is itself related to both \"fugere\" (\"to flee\") and \"fugare\" (\"to chase\"). The adjectival form is \"fugal\". Variants include \"fughetta\" (literally, \"a small fugue\") and \"fugato\" (a passage in fugal style within another work that is not a fugue).\n\nA fugue begins with the \"exposition\" and is written according to certain predefined rules; in later portions the composer has more freedom, though a logical key structure is usually followed. Further entries of the subject will occur throughout the fugue, repeating the accompanying material at the same time. The various entries may or may not be separated by \"episodes\".\n\nWhat follows is a chart displaying a fairly typical fugal outline, and an explanation of the processes involved in creating this structure.\n\nA fugue begins with the exposition of its subject in one of the voices alone in the tonic key. After the statement of the subject, a second voice enters and states the subject with the subject transposed to another key (usually the dominant or subdominant), which is known as the \"answer\". To make the music run smoothly, it may also have to be altered slightly. When the answer is an exact copy of the subject to the new key, with identical intervals to the first statement, it is classified as a \"real answer\"; if the intervals are altered to maintain the key it is a \"tonal answer\".\n\nA tonal answer is usually called for when the subject begins with a prominent dominant note, or where there is a prominent dominant note very close to the beginning of the subject. To prevent an undermining of the music's sense of key, this note is transposed up a fourth to the tonic rather than up a fifth to the supertonic. Answers in the subdominant are also employed for the same reason.\n\nWhile the answer is being stated, the voice in which the subject was previously heard continues with new material. If this new material is reused in later statements of the subject, it is called a \"countersubject\"; if this accompanying material is only heard once, it is simply referred to as \"free counterpoint\". The countersubject is written in invertible counterpoint at the octave or fifteenth. The distinction is made between the use of free counterpoint and regular countersubjects accompanying the fugue subject/answer, because in order for a countersubject to be heard accompanying the subject in more than one instance, it must be capable of sounding correctly above or below the subject, and must be conceived, therefore, in invertible (double) counterpoint.\n\nIn tonal music, invertible contrapuntal lines must be written according to certain rules because several intervallic combinations, while acceptable in one particular orientation, are no longer permissible when inverted. For example, when the note \"G\" sounds in one voice above the note \"C\" in lower voice, the interval of a fifth is formed, which is considered consonant and entirely acceptable. When this interval is inverted (\"C\" in the upper voice above \"G\" in the lower), it forms a fourth, considered a dissonance in tonal contrapuntal practice, and requires special treatment, or preparation and resolution, if it is to be used. The countersubject, if sounding at the same time as the answer, is transposed to the pitch of the answer. Each voice then responds with its own subject or answer, and further countersubjects or free counterpoint may be heard.\n\nWhen a tonal answer is used, it is customary for the exposition to alternate subjects (S) with answers (A), however, in some fugues this order is occasionally varied: e.g., see the SAAS arrangement of \"Fugue No. 1 in C Major, BWV 846\", from the \"Well-Tempered Clavier, Book 1\" by J. S. Bach. A brief codetta is often heard connecting the various statements of the subject and answer. This allows the music to run smoothly. The codetta, just as the other parts of the exposition, can be used throughout the rest of the fugue.\n\nThe first answer must occur as soon after the initial statement of the subject as possible; therefore the first codetta is often extremely short, or not needed. In the above example this is the case: the subject finishes on the quarter note (or crotchet) B-flat of the third beat of the second bar which harmonizes the opening G of the answer. The later codettas may be considerably longer, and often serve to (a) develop the material heard so far in the subject/answer and countersubject and possibly introduce ideas heard in the second countersubject or free counterpoint that follows (b) delay, and therefore heighten the impact of the reentry of the subject in another voice as well as modulating back to the tonic.\n\nThe exposition usually concludes when all voices have given a statement of the subject or answer. In some fugues, the exposition will end with a redundant entry, or an extra presentation of the theme. Furthermore, in some fugues the entry of one of the voices may be reserved until later, for example in the pedals of an organ fugue (see J. S. Bach's Fugue in C major for Organ, BWV 547).\n\nFurther entries of the subject follow this initial exposition, either immediately (as for example in \"Fugue No. 1 in C major, BWV 846\" of the \"Well-Tempered Clavier\"), or separated by episodes. Episodic material is always modulatory and is usually based upon some element heard in the exposition. Each episode has the primary function of transitioning for the next entry of the subject in a new key, and may also provide release from the strictness of form employed in the exposition, and middle-entries. André Gedalge states that the episode of the fugue is generally based on a series of imitations of the subject that have been fragmented.\n\nFurther entries of the subject, or middle entries, occur throughout the fugue. They must state the subject or answer at least once in its entirety, and may also be heard in combination with the countersubject(s) from the exposition, new countersubjects, free counterpoint, or any of these in combination. It is uncommon for the subject to enter alone in a single voice in the middle-entries as in the exposition; rather, it is usually heard with at least one of the countersubjects and/or other free contrapuntal accompaniments. Middle-entries tend to occur at pitches other than the initial. As shown in the typical structure above, these are often closely related keys such as the relative dominant and subdominant, although the key structure of fugues varies greatly. In the fugues of J. S. Bach, the first middle-entry occurs most often in the relative major or minor of the work's overall key, and is followed by an entry in the dominant of the relative major or minor when the fugue's subject requires a tonal answer. In the fugues of earlier composers (notably Buxtehude and Pachelbel), middle entries in keys other than the tonic and dominant tend to be the exception, and non-modulation the norm. One of the famous examples of such non-modulating fugue occurs in Buxtehude's Praeludium (Fugue and Chaconne) in C, BuxWV 137.\n\nWhen there is no entrance of the subject and answer material, the composer can develop the subject by altering the subject . This is called an episode, often by \"inversion\", although the term is sometimes used synonymously with middle-entry and may also describe the exposition of completely new subjects, as in a double fugue for example (see below). In any of the entries within a fugue the subject may be altered, by inversion, retrograde (a less common form where the entire subject is heard back-to-front) and diminution (the reduction of the subject's rhythmic values by a certain factor), augmentation (the increase of the subject's rhythmic values by a certain factor) or any combination of them.\n\nThe excerpt below, bars 7–12 of J. S. Bach's Fugue no. 2 in C minor, BWV 847, from the \"Well-Tempered Clavier\", Book 1 illustrates the application of most of the characteristics described above. The fugue is for keyboard and in three voices, with regular countersubjects. This excerpt opens at last entry of the exposition: the subject is sounding in the bass, the first countersubject in the treble, while the middle-voice is stating a second version of the second countersubject, which concludes with the characteristic rhythm of the subject, and is always used together with the first version of the second countersubject. Following this an episode modulates from the tonic to the relative major by means of sequence, in the form of an accompanied canon at the fourth. Arrival in E-flat major is marked by a quasi perfect cadence across the barline, from the last quarter note beat of the first bar to the first beat of the second bar in the second system, and the first middle entry. Here Bach has altered countersubject 2 to accommodate the change of mode.\n\nAt any point in the fugue there may be \"false entries\" of the subject, which include the start of the subject but are not completed. False entries are often abbreviated to the head of the subject, and anticipate the \"true\" entry of the subject, heightening the impact of the subject proper.\nThe counter-exposition is a second exposition. However, there are only two entries, and the entries occur in reverse order. The counter-exposition in a fugue is separated from the exposition by an episode, and is in the same key as the original exposition.\n\nSometimes counter-expositions or the middle entries take place in \"stretto,\" whereby one voice responds with the subject/answer before the first voice has completed its entry of the subject/answer, usually increasing the intensity of the music. Only one entry of the subject must be heard in its completion in a stretto. However, a stretto in which the subject/answer is heard in completion in all voices is known as \"stretto maestrale\" or \"grand stretto\". Strettos may also occur by inversion, augmentation and diminution. A fugue in which the opening exposition takes place in stretto form is known as a \"close fugue\" or \"stretto fugue\" (see for example, the \"Gratias agimus tibi\" and \"\" choruses from Bach's Mass in B Minor).\n\nThe closing section of a fugue often includes one or two counter-expositions, and possibly a stretto, in the tonic; sometimes over a tonic or dominant pedal note. Any material that follows the final entry of the subject is considered to be the final coda and is normally cadential.\n\nA simple fugue has only one subject, and does not utilize invertible counterpoint.\n\nA double fugue has two subjects that are often developed simultaneously; similarly, it follows that a triple fugue has three subjects. There are two kinds of double fugue: (a) a fugue in which the second subject is presented simultaneously with the subject in the exposition (e.g. as in Kyrie Eleison of Mozart's Requiem in D minor), and (b) a fugue in which the second subject has its own exposition at some later point, and the two subjects are not combined until later (see for example, fugue no. 14 in f-sharp minor from Bach's \"Well-Tempered Clavier\" Book 2, or more famously, Bach's \"St. Anne\" Fugue in E-flat major, BWV 552, a triple fugue for organ.)\n\nA counter-fugue is a fugue in which the first answer is presented as the subject in inversion (upside down), and the inverted subject continues to feature prominently throughout the fugue. Examples include \"Contrapunctus V\" through \"Contrapunctus VII\", from Bach's \"The Art of Fugue\".\n\nPermutation fugue describes a type of composition (or technique of composition) in which elements of fugue and strict canon are combined. Each voice enters in succession with the subject, each entry alternating between tonic and dominant, and each voice, having stated the initial subject, continues by stating two or more themes (or countersubjects), which must be conceived in correct invertible counterpoint. (In other words, the subject and countersubjects must be capable of being played both above and below all the other themes without creating any unacceptable dissonances.) Each voice takes this pattern and states all the subjects/themes in the same order (and repeats the material when all the themes have been stated, sometimes after a rest). There is usually very little non-structural/thematic material. During the course of a permutation fugue, it is quite uncommon, actually, for every single possible voice-combination (or \"permutation\") of the themes to be heard. This limitation exists in consequence of sheer proportionality: the more voices in a fugue, the greater the amount of possible permutations. In consequence, composers exercise editorial judgment as to the most musical of permutations and processes leading thereto. One example of permutation fugue can be seen in the opening chorus of Bach's cantata, \"Himmelskönig, sei willkommen\", BWV182.\n\nPermutation fugues differ from conventional fugue in that there are no connecting episodes, nor statement of the themes in related keys. So for example, the fugue of Bach's Passacaglia and Fugue in C minor, BWV 582 is not purely a permutation fugue, as it does have episodes between permutation expositions. Invertible counterpoint is essential to permutation fugues but is not found in simple fugues.\n\nA fughetta is a short fugue that has the same characteristics as a fugue. Often the contrapuntal writing is not strict, and the setting less formal. See for example, variation 24 of Beethoven's \"Diabelli Variations\" Op. 120.\n\nThe term \"fuga\" was used as far back as the Middle Ages, but was initially used to refer to any kind of imitative counterpoint, including canons, which are now thought of as distinct from fugues. Prior to the 16th century, fugue was originally a genre. It was not until the 16th century that fugal technique as it is understood today began to be seen in pieces, both instrumental and vocal. Fugal writing is found in works such as \"fantasias\", \"ricercares\" and \"canzonas\".\n\n\"Fugue\" as a theoretical term first occurred in 1330 when Jacobus of Liege wrote about the fuga in his \"Speculum musicae\". The fugue arose from the technique of \"imitation\", where the same musical material was repeated starting on a different note. Gioseffo Zarlino, a composer, author, and theorist in the Renaissance, was one of the first to distinguish between the two types of imitative counterpoint: fugues and canons (which he called imitations). Originally this was to aid improvisation, but by the 1550s, it was considered a technique of composition. The Renaissance composer Giovanni Pierluigi da Palestrina (1525?–1594) wrote masses using modal counterpoint and imitation, and fugal writing became the basis for writing motets as well. Palestrina's imitative motets differed from fugues in that each phrase of the text had a different subject which was introduced and worked out separately, whereas a fugue continued working with the same subject or subjects throughout the entire length of the piece.\n\nIt was in the Baroque period that the writing of fugues became central to composition, in part as a demonstration of compositional expertise. Fugues were incorporated into a variety of musical forms. Jan Pieterszoon Sweelinck, Girolamo Frescobaldi, Johann Jakob Froberger and Dieterich Buxtehude all wrote fugues, and George Frideric Handel included them in many of his oratorios. Keyboard suites from this time often conclude with a fugal gigue. Domenico Scarlatti has only a few fugues among his corpus of over 500 harpsichord sonatas. The French overture featured a quick fugal section after a slow introduction. The second movement of a sonata da chiesa, as written by Arcangelo Corelli and others, was usually fugal.\n\nThe Baroque period also saw a rise in the importance of music theory. Some fugues during the Baroque period were pieces designed to teach contrapuntal technique to students. The most influential text was published by Johann Joseph Fux (1660–1741), his \"Gradus Ad Parnassum\" (\"Steps to Parnassus\"), which appeared in 1725. This work laid out the terms of \"species\" of counterpoint, and offered a series of exercises to learn fugue writing. Fux's work was largely based on the practice of Palestrina's modal fugues. Mozart studied from this book, and it remained influential into the nineteenth century. Haydn, for example, taught counterpoint from his own summary of Fux, and thought of it as the basis for formal structure.\n\nBach's most famous fugues are those for the harpsichord in \"The Well-Tempered Clavier\", which many composers and theorists look at as the greatest model of fugue. \"The Well-Tempered Clavier\" comprises two volumes written in different times of Bach's life, each comprising 24 prelude and fugue pairs, one for each major and minor key. Bach is also known for his organ fugues, which are usually preceded by a prelude or toccata. The Art of Fugue, BWV 1080, is a collection of fugues (and four canons) on a single theme that is gradually transformed as the cycle progresses. Bach also wrote smaller single fugues, and put fugal sections or movements into many of his more general works. J. S. Bach's influence extended forward through his son C.P.E. Bach and through the theorist Friedrich Wilhelm Marpurg (1718–1795) whose \"Abhandlung von der Fuge\" (\"Treatise on the fugue\", 1753) was largely based on J. S. Bach's work.\n\nDuring the Classical era, the fugue was no longer a central or even fully natural mode of musical composition. Nevertheless, both Haydn and Mozart had periods of their careers in which they in some sense \"rediscovered\" fugal writing and used it frequently in their work.\n\nHaydn was the leader of fugal composition and technique in the Classical era. Haydn's most famous fugues can be found in his Sun quartets (op. 20, 1772), of which three have fugal finales. This was a practice that Haydn repeated only once later in his quartet-writing career, with the finale of his quartet op. 50 no. 4 (1787). Some of the earliest examples of Haydn's use of counterpoint, however, are in three symphonies (No. 3, No. 13, and No. 40) that date from 1762–63. The earliest fugues, in both the symphonies and in the Baryton trios, exhibit the influence of Joseph Fux's treatise on counterpoint, \"Gradus ad Parnassum\" (1725), which Haydn studied carefully. Haydn's second fugal period occurred after he heard, and was greatly inspired by, the oratorios of Handel during his visits to London (1791–1793, 1794–1795). Haydn then studied Handel's techniques and incorporated Handelian fugal writing into the choruses of his mature oratorios \"The Creation\" and \"The Seasons,\" as well as several of his later symphonies, including No. 88, No. 95, and No. 101.\n\nWolfgang Amadeus Mozart studied counterpoint when young with Padre Martini in Bologna. However, the major impetus to fugal writing for Mozart was the influence of Baron Gottfried van Swieten in Vienna around 1782. Van Swieten, during diplomatic service in Berlin, had taken the opportunity to collect as many manuscripts by Bach and Handel as he could, and he invited Mozart to study his collection and also encouraged him to transcribe various works for other combinations of instruments. Mozart was evidently fascinated by these works, and wrote a set of transcriptions for string trio of fugues from Bach's \"Well-Tempered Clavier\", introducing them with preludes of his own. In a letter to his sister, dated in Vienna on April 20, 1782, Mozart recognizes that he had not written anything in this form, but moved by the interest of Constance he composed one piece, which is sent with the letter. He begs her sister does not let anybody to see the fugue and manifests the hope to write five more and then present them to Baron van Swieten. Regarding the piece, he said \"I have taken particular care to write \"andante maestoso\" upon it, so that it should not be played fast -for if a fugue is not played slowly the ear cannot clearly distinguish the new subject as it is introduced and the effect is missed\". Mozart then set to writing fugues on his own, mimicking the Baroque style. These included the fugues for string quartet, K. 405 (1782) and a fugue in C Minor K. 426 for two pianos (1783). Later, Mozart incorporated fugal writing into his opera \"Die Zauberflöte\" and the finale of his \"Symphony No. 41\". The parts of the Requiem he completed also contain several fugues (most notably the Kyrie, and the three fugues in the Domine Jesu; he also left behind a sketch for an Amen fugue which, some believe, would have come at the end of the Sequentia).\n\nLudwig van Beethoven was familiar with fugal writing from childhood, as an important part of his training was playing from \"The Well-Tempered Clavier\". During his early career in Vienna, Beethoven attracted notice for his performance of these fugues. There are fugal sections in Beethoven's early piano sonatas, and fugal writing is to be found in the second and fourth movements of the \"Eroica Symphony\" (1805). Beethoven incorporated fugues in his sonatas, and reshaped the episode's purpose and compositional technique for later generations of composers. Nevertheless, fugues did not take on a truly central role in Beethoven's work until his \"late period.\" The finale of Beethoven's \"Hammerklavier\" Sonata contains a fugue, which was practically unperformed until the late 19th century, due to its tremendous technical difficulty and length. The last movement of his Cello Sonata, Op. 102 No. 2 is a fugue, and there are fugal passages in the last movements of his piano sonatas in A major Op.101 and in A flat major Op.110. According to Rosen (1971, p. 503) \"With the finale of 110, Beethoven re-conceived the significance of the most traditional elements of fugue writing.\"\n\nFugal passages are also found in the \"Missa Solemnis\" and all movements of the Ninth Symphony, except the third one. A massive, dissonant fugue forms the finale of his String Quartet, Op. 130 (1825); the latter was later published separately as Op. 133, the \"Große Fuge\" (\"Great Fugue\"). However, it is the fugue that opens Beethoven's String Quartet in C sharp minor, Op. 131 that several commentators regard as one of the composer's greatest achievements. Joseph Kerman (1966, p. 330) calls it \"this most moving of all fugues\". Sullivan (1927, p. 235) hears it as \"the most superhuman piece of music that Beethoven has ever written.\" Philip Radcliffe (1965, p. 149) says \"A bare description of its formal outline can give but little idea of the extraordinary profundity of this fugue .\"]\n\nBy the beginning of the Romantic era, fugue writing had become specifically attached to the norms and styles of the Baroque. Felix Mendelssohn wrote many fugues inspired by his study of the music of Johann Sebastian Bach. The Piano Sonata in B minor (1853) by Franz Liszt contains a powerful fugue, demanding incisive virtuosity from its player:]Verdi included a whimsical example at the end of his opera\" Falstaff\" and his setting of the Requiem Mass contained two (originally three) choral fugues. Bruckner and Mahler also included them in their respective symphonies. The finale of Mahler's Symphony No. 5 features a \"fugue-like\" passage early in the movement, though this is not actually an example of a fugue.\n\nTwentieth-century composers brought fugue back to its position of prominence, realizing its uses in full instrumental works, its importance in development and introductory sections, and the developmental capabilities of fugal composition. The second movement of Ravel's piano suite \"Le Tombeau de Couperin\" (1917) is a fugue that Roy Howat (200, p. 88) describes as having \"a subtle glint of jazz\". Bartók's \"Music for Strings, Percussion and Celesta\" (1936) opens with a slow fugue that Pierre Boulez (1986, p. 346-7) regards as \"certainly the finest and most characteristic example of Bartók's subtle style...probably the most \"timeless\" of all Bartók's works - a fugue that unfolds like a fan to a point of maximum intensity and then closes, returning to the mysterious atmosphere of the opening.\"\n\nIgor Stravinsky also incorporated fugues into his works, including the \"Symphony of Psalms\" and the \"Dumbarton Oaks\" concerto. Stravinsky recognized the compositional techniques of Bach, and in the second movement of his \"Symphony of Psalms\" (1930), he lays out a fugue that is much like that of the Baroque era. It employs a double fugue with two distinct subjects, the first beginning in C and the second in E. Techniques such as stretto, sequencing, and the use of subject incipits are frequently heard in the movement.\n\nOlivier Messiaen, writing about his \"Vingt regards sur l'enfant-Jésus\" (1944) wrote of the sixth piece of that collection, \"\"Par Lui tout a été fait\"\" (\"By Him were all things made\"): \n\nGyörgy Ligeti wrote a five-part double fugue for his \"Requiem\"'s second movement, the \"Kyrie\", in which each part (S,M,A,T,B) is subdivided in four-voice \"bundles\" that make a canon. The melodic material in this fugue is totally chromatic, with melismatic (running) parts overlaid onto skipping intervals, and use of polyrhythm (multiple simultaneous subdivisions of the measure), blurring everything both harmonically and rhythmically so as to create an aural aggregate, thus highlighting the theoretical/aesthetic question of the next section as to whether fugue is a form or a texture.\n\nBenjamin Britten used a fugue in the final part of The Young Person's Guide to the Orchestra (1946). The Henry Purcell's theme is triumphantly cited at the end making it a choral fugue.\n\nCanadian pianist and musical thinker Glenn Gould composed \"So You Want to Write a Fugue?\", a full-scale fugue set to a text that cleverly explicates its own musical form.\n\nFugues (or fughettas/fugatos) have been incorporated into genres outside of Western classical music. Several examples exist within jazz, such as \"Bach goes to Town\", composed by the Welsh composer Alec Templeton and recorded by Benny Goodman in 1938, and \"Concorde\" composed by John Lewis and recorded by the Modern Jazz Quartet in 1955. A few examples also exist within progressive rock, such as the central movement of The Endless Enigma by Emerson, Lake & Palmer and On Reflection by Gentle Giant.\n\nA widespread view of the fugue is that it is not a musical form but rather a technique of composition.\n\nThe Austrian musicologist Erwin Ratz argues that the formal organization of a fugue involves not only the arrangement of its theme and episodes, but also its harmonic structure. In particular, the exposition and coda tend to emphasize the tonic key, whereas the episodes usually explore more distant tonalities. Ratz stressed, however, that this is the core, underlying form (\"Urform\") of the fugue, from which individual fugues may deviate. Thus it is to be noted that while certain related keys are more commonly explored in fugal development, the overall structure of a fugue does not limit its harmonic structure. For example, a fugue may not even explore the dominant, one of the most closely related keys to the tonic. Bach's Fugue in B major from book one of the \"Well Tempered Clavier\" explores the relative minor, the supertonic and the subdominant. This is unlike later forms such as the sonata, which clearly prescribes which keys are explored (typically the tonic and dominant in an ABA form). Then, many modern fugues dispense with traditional tonal harmonic scaffolding altogether, and either use serial (pitch-oriented) rules, or (as the \"Kyrie/Christe\" in György Ligeti's \"Requiem\", Witold Lutosławski works), use panchromatic or even denser harmonic spectra.\n\nFugue is the most complex of contrapuntal forms. In Ratz's words, \"fugal technique significantly burdens the shaping of musical ideas, and it was given only to the greatest geniuses, such as Bach and Beethoven, to breathe life into such an unwieldy form and make it the bearer of the highest thoughts.\" In presenting Bach's fugues as among the greatest of contrapuntal works, Peter Kivy points out that \"counterpoint itself, since time out of mind, has been associated in the thinking of musicians with the profound and the serious\" and argues that \"there seems to be some rational justification for their doing so.\"\n\nThis is related to the idea that restrictions create freedom for the composer, by directing their efforts. He also points out that fugue writing has its roots in improvisation, and was, during the Renaissance, practiced as an improvisatory art. Writing in 1555, Nicola Vicentino, for example, suggests that:\n\n"}
{"id": "10898", "url": "https://en.wikipedia.org/wiki?curid=10898", "title": "Fugue state", "text": "Fugue state\n\nDissociative fugue, formerly fugue state or psychogenic fugue, is a dissociative disorder. It is a rare psychiatric disorder characterized by reversible amnesia for personal identity, including the memories, personality, and other identifying characteristics of individuality. The state can last days, months or longer. Dissociative fugue usually involves unplanned travel or wandering, and is sometimes accompanied by the establishment of a new identity. It is a facet of dissociative amnesia, according to the fifth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-5).\n\nAfter recovery from fugue, previous memories usually return intact. Because of this, there is not normally any treatment necessary for people who have been in fugue states. Additionally, an episode of fugue is not characterized as attributable to a psychiatric disorder if it can be related to the ingestion of psychotropic substances, to physical trauma, to a general medical condition, or to dissociative identity disorder, delirium, or dementia. Fugues are precipitated by a series of long-term traumatic episodes. It is most commonly associated with childhood victims of sexual abuse who learn over time to dissociate memory of the abuse.(dissociative amnesia).\n\nSymptoms of a dissociative fugue include mild confusion, and once the fugue ends, possible depression, grief, shame and discomfort. People have also experienced a post-fugue anger.\n\nA doctor may suspect dissociative fugue when people seem confused about their identity or are puzzled about their past or when confrontations challenge their new identity or absence of one. The doctor carefully reviews symptoms and does a physical examination to exclude physical disorders that may contribute to or cause memory loss. A psychological examination is also done.\n\nSometimes dissociative fugue cannot be diagnosed until people abruptly return to their pre-fugue identity and are distressed to find themselves in unfamiliar circumstances. The diagnosis is usually made retroactively when a doctor reviews the history and collects information that documents the circumstances before people left home, the travel itself, and the establishment of an alternative life.\n\nThe cause of the fugue state is related to dissociative amnesia, (\"DSM-IV Codes 300.12\") which has several other subtypes: selective amnesia, generalised amnesia, continuous amnesia, and systematised amnesia, in addition to the subtype \"dissociative fugue\".\n\nUnlike retrograde amnesia (which is popularly referred to simply as \"amnesia\", the state where someone forgets events before brain damage), dissociative amnesia is not due to the direct physiological effects of a substance (e.g., a drug of abuse, a medication, \"DSM-IV Codes 291.1 & 292.83\") or a neurological or other general medical condition (e.g., amnestic disorder due to a head trauma, \"DSM-IV Codes 294.0\"). It is a complex neuropsychological process.\n\nAs the person experiencing a dissociative fugue may have recently suffered the reappearance of an event or person representing an earlier life trauma, the emergence of an armoring or defensive personality seems to be for some, a logical apprehension of the situation.\n\nTherefore, the terminology \"fugue state\" may carry a slight linguistic distinction from \"dissociative fugue\", the former implying a greater degree of \"motion\". For the purposes of this article then, a \"fugue state\" occurs while one is \"acting out\" a \"dissociative fugue\".\n\nThe DSM-IV defines as:\n\nThe \"Merck Manual\" defines \"dissociative fugue\" as:\n\nIn support of this definition, the \"Merck Manual\" further defines dissociative amnesia as:\n\nThe DSM-IV-TR states that the fugue may have a duration from days to months, and recovery is usually rapid. However, some cases may be refractory. An individual usually has only one episode.\n\n\n\n"}
{"id": "10902", "url": "https://en.wikipedia.org/wiki?curid=10902", "title": "Force", "text": "Force\n\nIn physics, a force is any interaction that, when unopposed, will change the motion of an object. A force can cause an object with mass to change its velocity (which includes to begin moving from a state of rest), i.e., to accelerate. Force can also be described intuitively as a push or a pull. A force has both magnitude and direction, making it a vector quantity. It is measured in the SI unit of newtons and represented by the symbol F.\n\nThe original form of Newton's second law states that the net force acting upon an object is equal to the rate at which its momentum changes with time. If the mass of the object is constant, this law implies that the acceleration of an object is directly proportional to the net force acting on the object, is in the direction of the net force, and is inversely proportional to the mass of the object\n\nConcepts related to force include: thrust, which increases the velocity of an object; drag, which decreases the velocity of an object; and torque, which produces changes in rotational speed of an object. In an extended body, each part usually applies forces on the adjacent parts; the distribution of such forces through the body is the internal mechanical stress. Such internal mechanical stresses cause no accelation of that body as the forces balance one another. Pressure, the distribution of many small forces applied over an area of a body, is a simple type of stress that if unbalanced can cause the body to accelerate. Stress usually causes deformation of solid materials, or flow in fluids.\n\nPhilosophers in antiquity used the concept of force in the study of stationary and moving objects and simple machines, but thinkers such as Aristotle and Archimedes retained fundamental errors in understanding force. In part this was due to an incomplete understanding of the sometimes non-obvious force of friction, and a consequently inadequate view of the nature of natural motion. A fundamental error was the belief that a force is required to maintain motion, even at a constant velocity. Most of the previous misunderstandings about motion and force were eventually corrected by Galileo Galilei and Sir Isaac Newton. With his mathematical insight, Sir Isaac Newton formulated laws of motion that were not improved for nearly three hundred years. By the early 20th century, Einstein developed a theory of relativity that correctly predicted the action of forces on objects with increasing momenta near the speed of light, and also provided insight into the forces produced by gravitation and inertia.\n\nWith modern insights into quantum mechanics and technology that can accelerate particles close to the speed of light, particle physics has devised a Standard Model to describe forces between particles smaller than atoms. The Standard Model predicts that exchanged particles called gauge bosons are the fundamental means by which forces are emitted and absorbed. Only four main interactions are known: in order of decreasing strength, they are: strong, electromagnetic, weak, and gravitational. High-energy particle physics observations made during the 1970s and 1980s confirmed that the weak and electromagnetic forces are expressions of a more fundamental electroweak interaction.\n\nSince antiquity the concept of force has been recognized as integral to the functioning of each of the simple machines. The mechanical advantage given by a simple machine allowed for less force to be used in exchange for that force acting over a greater distance for the same amount of work. Analysis of the characteristics of forces ultimately culminated in the work of Archimedes who was especially famous for formulating a treatment of buoyant forces inherent in fluids.\n\nAristotle provided a philosophical discussion of the concept of a force as an integral part of Aristotelian cosmology. In Aristotle's view, the terrestrial sphere contained four elements that come to rest at different \"natural places\" therein. Aristotle believed that motionless objects on Earth, those composed mostly of the elements earth and water, to be in their natural place on the ground and that they will stay that way if left alone. He distinguished between the innate tendency of objects to find their \"natural place\" (e.g., for heavy bodies to fall), which led to \"natural motion\", and unnatural or forced motion, which required continued application of a force. This theory, based on the everyday experience of how objects move, such as the constant application of a force needed to keep a cart moving, had conceptual trouble accounting for the behavior of projectiles, such as the flight of arrows. The place where the archer moves the projectile was at the start of the flight, and while the projectile sailed through the air, no discernible efficient cause acts on it. Aristotle was aware of this problem and proposed that the air displaced through the projectile's path carries the projectile to its target. This explanation demands a continuum like air for change of place in general.\n\nAristotelian physics began facing criticism in medieval science, first by John Philoponus in the 6th century.\n\nThe shortcomings of Aristotelian physics would not be fully corrected until the 17th century work of Galileo Galilei, who was influenced by the late medieval idea that objects in forced motion carried an innate force of impetus. Galileo constructed an experiment in which stones and cannonballs were both rolled down an incline to disprove the Aristotelian theory of motion. He showed that the bodies were accelerated by gravity to an extent that was independent of their mass and argued that objects retain their velocity unless acted on by a force, for example friction.\n\nSir Isaac Newton described the motion of all objects using the concepts of inertia and force, and in doing so he found they obey certain conservation laws. In 1687, Newton published his thesis \"Philosophiæ Naturalis Principia Mathematica\". In this work Newton set out three laws of motion that to this day are the way forces are described in physics.\n\nNewton's First Law of Motion states that objects continue to move in a state of constant velocity unless acted upon by an external net force (resultant force). This law is an extension of Galileo's insight that constant velocity was associated with a lack of net force (see a more detailed description of this below). Newton proposed that every object with mass has an innate inertia that functions as the fundamental equilibrium \"natural state\" in place of the Aristotelian idea of the \"natural state of rest\". That is, Newton's empirical First Law contradicts the intuitive Aristotelian belief that a net force is required to keep an object moving with constant velocity. By making \"rest\" physically indistinguishable from \"non-zero constant velocity\", Newton's First Law directly connects inertia with the concept of relative velocities. Specifically, in systems where objects are moving with different velocities, it is impossible to determine which object is \"in motion\" and which object is \"at rest\". The laws of physics are the same in every inertial frame of reference, that is, in all frames related by a Galilean transformation.\n\nFor instance, while traveling in a moving vehicle at a constant velocity, the laws of physics do not change as a result of its motion. If a person riding within the vehicle throws a ball straight up, that person will observe it rise vertically and fall vertically and not have to apply a force in the direction the vehicle is moving. Another person, observing the moving vehicle pass by, would observe the ball follow a curving parabolic path in the same direction as the motion of the vehicle. It is the inertia of the ball associated with its constant velocity in the direction of the vehicle's motion that ensures the ball continues to move forward even as it is thrown up and falls back down. From the perspective of the person in the car, the vehicle and everything inside of it is at rest: It is the outside world that is moving with a constant speed in the opposite direction of the vehicle. Since there is no experiment that can distinguish whether it is the vehicle that is at rest or the outside world that is at rest, the two situations are considered to be physically indistinguishable. Inertia therefore applies equally well to constant velocity motion as it does to rest.\n\nA modern statement of Newton's Second Law is a vector equation:\nwhere formula_2 is the momentum of the system, and formula_3 is the net (vector sum) force. If a body is in equilibrium, there is zero \"net\" force by definition (balanced forces may be present nevertheless). In contrast, the second law states that if there is an \"unbalanced\" force acting on an object it will result in the object's momentum changing over time.\n\nBy the definition of momentum,\nwhere \"m\" is the mass and formula_5 is the velocity.\n\nIf Newton's second law is applied to a system of constant mass, \"m\" may be moved outside the derivative operator. The equation then becomes\nBy substituting the definition of acceleration, the algebraic version of Newton's Second Law is derived:\nNewton never explicitly stated the formula in the reduced form above.\n\nNewton's Second Law asserts the direct proportionality of acceleration to force and the inverse proportionality of acceleration to mass. Accelerations can be defined through kinematic measurements. However, while kinematics are well-described through reference frame analysis in advanced physics, there are still deep questions that remain as to what is the proper definition of mass. General relativity offers an equivalence between space-time and mass, but lacking a coherent theory of quantum gravity, it is unclear as to how or whether this connection is relevant on microscales. With some justification, Newton's second law can be taken as a quantitative definition of \"mass\" by writing the law as an equality; the relative units of force and mass then are fixed.\n\nThe use of Newton's Second Law as a \"definition\" of force has been disparaged in some of the more rigorous textbooks, because it is essentially a mathematical truism. Notable physicists, philosophers and mathematicians who have sought a more explicit definition of the concept of force include Ernst Mach, Clifford Truesdell and Walter Noll.\n\nNewton's Second Law can be used to measure the strength of forces. For instance, knowledge of the masses of planets along with the accelerations of their orbits allows scientists to calculate the gravitational forces on planets.\n\nNewton's Third Law is a result of applying symmetry to situations where forces can be attributed to the presence of different objects. The third law means that all forces are \"interactions\" between different bodies, and thus that there is no such thing as a unidirectional force or a force that acts on only one body. Whenever a first body exerts a force F on a second body, the second body exerts a force −F on the first body. F and −F are equal in magnitude and opposite in direction. This law is sometimes referred to as the \"action-reaction law\", with F called the \"action\" and −F the \"reaction\". The action and the reaction are simultaneous:\n\nIf object 1 and object 2 are considered to be in the same system, then the net force on the system due to the interactions between those is zero since\n\nThis means that in a closed system of particles, all internal forces are balanced. That is, the action-reaction force shared between any two objects in a closed system will not cause the center of mass of the system to accelerate. The constituent objects may accelerate with respect to each other, but the system itself remains unaccelerated. Alternatively, if an external force acts on the system, then the center of mass will experience an acceleration proportional to the magnitude of the external force divided by the mass of the system.\n\nCombining Newton's Second and Third Laws, it is possible to show that the linear momentum of a system is conserved. Using\n\nand integrating with respect to time, the equation:\n\nis obtained. For a system that includes objects 1 and 2,\n\nwhich is the conservation of linear momentum. Using the similar arguments, it is possible to generalize this to a system of an arbitrary number of particles. This shows that exchanging momentum between constituent objects will not affect the net momentum of a system. In general, as long as all forces are due to the interaction of objects with mass, it is possible to define a system such that net momentum is never lost nor gained.\n\nIn the special theory of relativity, mass and energy are equivalent (as can be seen by calculating the work required to accelerate an object). When an object's velocity increases, so does its energy and hence its mass equivalent (inertia). It thus requires more force to accelerate it the same amount than it did at a lower velocity. Newton's Second Law\n\nremains valid because it is a mathematical definition. But in order to be conserved, relativistic momentum must be redefined as:\n\nwhere\n\nThe relativistic expression relating force and acceleration for a particle with constant non-zero rest mass formula_19 moving in the formula_20 direction is:\n\nwhere the Lorentz factor\n\nIn the early history of relativity, the expressions formula_25 and formula_26 were called longitudinal and transverse mass. Relativistic force does not produce a constant acceleration, but an ever-decreasing acceleration as the object approaches the speed of light. Note that formula_27 approaches asymptotically an infinite value and is undefined for an object with a non-zero rest mass as it approaches the speed of light, and the theory yields no prediction at that speed.\n\nIf formula_16 is very small compared to formula_17, then formula_30 is very close to 1 and \nis a close approximation. Even for use in relativity, however, one can restore the form of\n\nthrough the use of four-vectors. This relation is correct in relativity when formula_33 is the four-force, formula_19 is the invariant mass, and formula_35 is the four-acceleration.\n\nSince forces are perceived as pushes or pulls, this can provide an intuitive understanding for describing forces. As with other physical concepts (e.g. temperature), the intuitive understanding of forces is quantified using precise operational definitions that are consistent with direct observations and compared to a standard measurement scale. Through experimentation, it is determined that laboratory measurements of forces are fully consistent with the conceptual definition of force offered by Newtonian mechanics.\n\nForces act in a particular direction and have sizes dependent upon how strong the push or pull is. Because of these characteristics, forces are classified as \"vector quantities\". This means that forces follow a different set of mathematical rules than physical quantities that do not have direction (denoted scalar quantities). For example, when determining what happens when two forces act on the same object, it is necessary to know both the magnitude and the direction of both forces to calculate the result. If both of these pieces of information are not known for each force, the situation is ambiguous. For example, if you know that two people are pulling on the same rope with known magnitudes of force but you do not know which direction either person is pulling, it is impossible to determine what the acceleration of the rope will be. The two people could be pulling against each other as in tug of war or the two people could be pulling in the same direction. In this simple one-dimensional example, without knowing the direction of the forces it is impossible to decide whether the net force is the result of adding the two force magnitudes or subtracting one from the other. Associating forces with vectors avoids such problems.\n\nHistorically, forces were first quantitatively investigated in conditions of static equilibrium where several forces canceled each other out. Such experiments demonstrate the crucial properties that forces are additive vector quantities: they have magnitude and direction. When two forces act on a point particle, the resulting force, the \"resultant\" (also called the \"net force\"), can be determined by following the parallelogram rule of vector addition: the addition of two vectors represented by sides of a parallelogram, gives an equivalent resultant vector that is equal in magnitude and direction to the transversal of the parallelogram. The magnitude of the resultant varies from the difference of the magnitudes of the two forces to their sum, depending on the angle between their lines of action. However, if the forces are acting on an extended body, their respective lines of application must also be specified in order to account for their effects on the motion of the body.\n\nFree-body diagrams can be used as a convenient way to keep track of forces acting on a system. Ideally, these diagrams are drawn with the angles and relative magnitudes of the force vectors preserved so that graphical vector addition can be done to determine the net force.\n\nAs well as being added, forces can also be resolved into independent components at right angles to each other. A horizontal force pointing northeast can therefore be split into two forces, one pointing north, and one pointing east. Summing these component forces using vector addition yields the original force. Resolving force vectors into components of a set of basis vectors is often a more mathematically clean way to describe forces than using magnitudes and directions. This is because, for orthogonal components, the components of the vector sum are uniquely determined by the scalar addition of the components of the individual vectors. Orthogonal components are independent of each other because forces acting at ninety degrees to each other have no effect on the magnitude or direction of the other. Choosing a set of orthogonal basis vectors is often done by considering what set of basis vectors will make the mathematics most convenient. Choosing a basis vector that is in the same direction as one of the forces is desirable, since that force would then have only one non-zero component. Orthogonal force vectors can be three-dimensional with the third component being at right-angles to the other two.\n\nEquilibrium occurs when the resultant force acting on a point particle is zero (that is, the vector sum of all forces is zero). When dealing with an extended body, it is also necessary that the net torque be zero.\n\nThere are two kinds of equilibrium: static equilibrium and dynamic equilibrium.\n\nStatic equilibrium was understood well before the invention of classical mechanics. Objects that are at rest have zero net force acting on them.\n\nThe simplest case of static equilibrium occurs when two forces are equal in magnitude but opposite in direction. For example, an object on a level surface is pulled (attracted) downward toward the center of the Earth by the force of gravity. At the same time, a force is applied by the surface that resists the downward force with equal upward force (called a normal force). The situation produces zero net force and hence no acceleration.\n\nPushing against an object that rests on a frictional surface can result in a situation where the object does not move because the applied force is opposed by static friction, generated between the object and the table surface. For a situation with no movement, the static friction force \"exactly\" balances the applied force resulting in no acceleration. The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object.\n\nA static equilibrium between two forces is the most usual way of measuring forces, using simple devices such as weighing scales and spring balances. For example, an object suspended on a vertical spring scale experiences the force of gravity acting on the object balanced by a force applied by the \"spring reaction force\", which equals the object's weight. Using such tools, some quantitative force laws were discovered: that the force of gravity is proportional to volume for objects of constant density (widely exploited for millennia to define standard weights); Archimedes' principle for buoyancy; Archimedes' analysis of the lever; Boyle's law for gas pressure; and Hooke's law for springs. These were all formulated and experimentally verified before Isaac Newton expounded his Three Laws of Motion.\n\nDynamic equilibrium was first described by Galileo who noticed that certain assumptions of Aristotelian physics were contradicted by observations and logic. Galileo realized that simple velocity addition demands that the concept of an \"absolute rest frame\" did not exist. Galileo concluded that motion in a constant velocity was completely equivalent to rest. This was contrary to Aristotle's notion of a \"natural state\" of rest that objects with mass naturally approached. Simple experiments showed that Galileo's understanding of the equivalence of constant velocity and rest were correct. For example, if a mariner dropped a cannonball from the crow's nest of a ship moving at a constant velocity, Aristotelian physics would have the cannonball fall straight down while the ship moved beneath it. Thus, in an Aristotelian universe, the falling cannonball would land behind the foot of the mast of a moving ship. However, when this experiment is actually conducted, the cannonball always falls at the foot of the mast, as if the cannonball knows to travel with the ship despite being separated from it. Since there is no forward horizontal force being applied on the cannonball as it falls, the only conclusion left is that the cannonball continues to move with the same velocity as the boat as it falls. Thus, no force is required to keep the cannonball moving at the constant forward velocity.\n\nMoreover, any object traveling at a constant velocity must be subject to zero net force (resultant force). This is the definition of dynamic equilibrium: when all the forces on an object balance but it still moves at a constant velocity.\n\nA simple case of dynamic equilibrium occurs in constant velocity motion across a surface with kinetic friction. In such a situation, a force is applied in the direction of motion while the kinetic friction force exactly opposes the applied force. This results in zero net force, but since the object started with a non-zero velocity, it continues to move with a non-zero velocity. Aristotle misinterpreted this motion as being caused by the applied force. However, when kinetic friction is taken into consideration it is clear that there is no net force causing constant velocity motion.\n\nThe notion \"force\" keeps its meaning in quantum mechanics, though one is now dealing with operators instead of classical variables and though the physics is now described by the Schrödinger equation instead of Newtonian equations. This has the consequence that the results of a measurement are now sometimes \"quantized\", i.e. they appear in discrete portions. This is, of course, difficult to imagine in the context of \"forces\". However, the potentials V(x,y,z) or fields, from which the forces generally can be derived, are treated similar to classical position variables, i.e., formula_36.\n\nThis becomes different only in the framework of quantum field theory, where these fields are also quantized.\n\nHowever, already in quantum mechanics there is one \"caveat\", namely the particles acting onto each other do not only possess the spatial variable, but also a discrete intrinsic angular momentum-like variable called the \"spin\", and there is the Pauli principle relating the space and the spin variables. Depending on the value of the spin, identical particles split into two different classes, fermions and bosons. If two identical fermions (e.g. electrons) have a \"symmetric\" spin function (e.g. parallel spins) the spatial variables must be \"antisymmetric\" (i.e. they exclude each other from their places much as if there was a repulsive force), and vice versa, i.e. for antiparallel \"spins\" the \"position variables\" must be symmetric (i.e. the apparent force must be attractive). Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive.\n\nThus the notion \"force\" loses already part of its meaning.\n\nIn modern particle physics, forces and the acceleration of particles are explained as a mathematical by-product of exchange of momentum-carrying gauge bosons. With the development of quantum field theory and general relativity, it was realized that force is a redundant concept arising from conservation of momentum (4-momentum in relativity and momentum of virtual particles in quantum electrodynamics). The conservation of momentum can be directly derived from the homogeneity or symmetry of space and so is usually considered more fundamental than the concept of a force. Thus the currently known fundamental forces are considered more accurately to be \"fundamental interactions\". When particle A emits (creates) or absorbs (annihilates) virtual particle B, a momentum conservation results in recoil of particle A making impression of repulsion or attraction between particles A A' exchanging by B. This description applies to all forces arising from fundamental interactions. While sophisticated mathematical descriptions are needed to predict, in full detail, the accurate result of such interactions, there is a conceptually simple way to describe such interactions through the use of Feynman diagrams. In a Feynman diagram, each matter particle is represented as a straight line (see world line) traveling through time, which normally increases up or to the right in the diagram. Matter and anti-matter particles are identical except for their direction of propagation through the Feynman diagram. World lines of particles intersect at interaction vertices, and the Feynman diagram represents any force arising from an interaction as occurring at the vertex with an associated instantaneous change in the direction of the particle world lines. Gauge bosons are emitted away from the vertex as wavy lines and, in the case of virtual particle exchange, are absorbed at an adjacent vertex.\n\nThe utility of Feynman diagrams is that other types of physical phenomena that are part of the general picture of fundamental interactions but are conceptually separate from forces can also be described using the same rules. For example, a Feynman diagram can describe in succinct detail how a neutron decays into an electron, proton, and neutrino, an interaction mediated by the same gauge boson that is responsible for the weak nuclear force.\n\nAll of the forces in the universe are based on four fundamental interactions. The strong and weak forces are nuclear forces that act only at very short distances, and are responsible for the interactions between subatomic particles, including nucleons and compound nuclei. The electromagnetic force acts between electric charges, and the gravitational force acts between masses. All other forces in nature derive from these four fundamental interactions. For example, friction is a manifestation of the electromagnetic force acting between the atoms of two surfaces, and the Pauli exclusion principle, which does not permit atoms to pass through each other. Similarly, the forces in springs, modeled by Hooke's law, are the result of electromagnetic forces and the Exclusion Principle acting together to return an object to its equilibrium position. Centrifugal forces are acceleration forces that arise simply from the acceleration of rotating frames of reference.\n\nThe fundamental theories for forces developed from the unification of disparate ideas. For example, Isaac Newton unified, with his universal theory of gravitation, the force responsible for objects falling near the surface of the Earth with the force responsible for the falling of celestial bodies about the Earth (the Moon) and the Sun (the planets). Michael Faraday and James Clerk Maxwell demonstrated that electric and magnetic forces were unified through a theory of electromagnetism. In the 20th century, the development of quantum mechanics led to a modern understanding that the first three fundamental forces (all except gravity) are manifestations of matter (fermions) interacting by exchanging virtual particles called gauge bosons. This standard model of particle physics assumes a similarity between the forces and led scientists to predict the unification of the weak and electromagnetic forces in electroweak theory, which was subsequently confirmed by observation. The complete formulation of the standard model predicts an as yet unobserved Higgs mechanism, but observations such as neutrino oscillations suggest that the standard model is incomplete. A Grand Unified Theory that allows for the combination of the electroweak interaction with the strong force is held out as a possibility with candidate theories such as supersymmetry proposed to accommodate some of the outstanding unsolved problems in physics. Physicists are still attempting to develop self-consistent unification models that would combine all four fundamental interactions into a theory of everything. Einstein tried and failed at this endeavor, but currently the most popular approach to answering this question is string theory.\n\nWhat we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as formula_37 and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of formula_19 will experience a force:\n\nFor an object in free-fall, this force is unopposed and the net force on the object is its weight. For objects not in free-fall, the force of gravity is opposed by the reaction forces applied by their supports. For example, a person standing on the ground experiences zero net force, since a normal force (a reaction force) is exerted by the ground upward on the person that counterbalances his weight that is directed downward.\n\nNewton's contribution to gravitational theory was to unify the motions of heavenly bodies, which Aristotle had assumed were in a natural state of constant motion, with falling motion observed on the Earth. He proposed a law of gravity that could account for the celestial motions that had been described earlier using Kepler's laws of planetary motion.\n\nNewton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration of a body due to gravity is proportional to the mass of the other attracting body. Combining these ideas gives a formula that relates the mass (formula_40) and the radius (formula_41) of the Earth to the gravitational acceleration:\n\nwhere formula_43 is the distance between the two objects' centers of mass and formula_44 is the unit vector pointed in the direction away from the center of the first object toward the center of the second object.\n\nThis formula was powerful enough to stand as the basis for all subsequent descriptions of motion within the solar system until the 20th century. During that time, sophisticated methods of perturbation analysis were invented to calculate the deviations of orbits due to the influence of multiple bodies on a planet, moon, comet, or asteroid. The formalism was exact enough to allow mathematicians to predict the existence of the planet Neptune before it was observed.\n\nMercury's orbit, however, did not match that predicted by Newton's Law of Gravitation. Some astrophysicists predicted the existence of another planet (Vulcan) that would explain the discrepancies; however no such planet could be found. When Albert Einstein formulated his theory of general relativity (GR) he turned his attention to the problem of Mercury's orbit and found that his theory added a correction, which could account for the discrepancy. This was the first time that Newton's Theory of Gravity had been shown to be inexact.\n\nSince then, general relativity has been acknowledged as the theory that best explains gravity. In GR, gravitation is not viewed as a force, but rather, objects moving freely in gravitational fields travel under their own inertia in straight lines through curved space-time – defined as the shortest space-time path between two space-time events. From the perspective of the object, all motion occurs as if there were no gravitation whatsoever. It is only when observing the motion in a global sense that the curvature of space-time can be observed and the force is inferred from the object's curved path. Thus, the straight line path in space-time is seen as a curved line in space, and it is called the \"ballistic trajectory\" of the object. For example, a basketball thrown from the ground moves in a parabola, as it is in a uniform gravitational field. Its space-time trajectory is almost a straight line, slightly curved (with the radius of curvature of the order of few light-years). The time derivative of the changing momentum of the object is what we label as \"gravitational force\".\n\nThe electrostatic force was first described in 1784 by Coulomb as a force that existed intrinsically between two charges. The properties of the electrostatic force were that it varied as an inverse square law directed in the radial direction, was both attractive and repulsive (there was intrinsic polarity), was independent of the mass of the charged objects, and followed the superposition principle. Coulomb's law unifies all these observations into one succinct statement.\n\nSubsequent mathematicians and physicists found the construct of the \"electric field\" to be useful for determining the electrostatic force on an electric charge at any point in space. The electric field was based on using a hypothetical \"test charge\" anywhere in space and then using Coulomb's Law to determine the electrostatic force. Thus the electric field anywhere in space is defined as\n\nwhere formula_46 is the magnitude of the hypothetical test charge.\n\nMeanwhile, the Lorentz force of magnetism was discovered to exist between two electric currents. It has the same mathematical character as Coulomb's Law with the proviso that like currents attract and unlike currents repel. Similar to the electric field, the magnetic field can be used to determine the magnetic force on an electric current at any point in space. In this case, the magnitude of the magnetic field was determined to be\n\nwhere formula_48 is the magnitude of the hypothetical test current and formula_49 is the length of hypothetical wire through which the test current flows. The magnetic field exerts a force on all magnets including, for example, those used in compasses. The fact that the Earth's magnetic field is aligned closely with the orientation of the Earth's axis causes compass magnets to become oriented because of the magnetic force pulling on the needle.\n\nThrough combining the definition of electric current as the time rate of change of electric charge, a rule of vector multiplication called Lorentz's Law describes the force on a charge moving in a magnetic field. The connection between electricity and magnetism allows for the description of a unified \"electromagnetic force\" that acts on a charge. This force can be written as a sum of the electrostatic force (due to the electric field) and the magnetic force (due to the magnetic field). Fully stated, this is the law:\n\nwhere formula_51 is the electromagnetic force, formula_46 is the magnitude of the charge of the particle, formula_53 is the electric field, formula_54 is the velocity of the particle that is crossed with the magnetic field (formula_55).\n\nThe origin of electric and magnetic fields would not be fully explained until 1864 when James Clerk Maxwell unified a number of earlier theories into a set of 20 scalar equations, which were later reformulated into 4 vector equations by Oliver Heaviside and Josiah Willard Gibbs. These \"Maxwell Equations\" fully described the sources of the fields as being stationary and moving charges, and the interactions of the fields themselves. This led Maxwell to discover that electric and magnetic fields could be \"self-generating\" through a wave that traveled at a speed that he calculated to be the speed of light. This insight united the nascent fields of electromagnetic theory with optics and led directly to a complete description of the electromagnetic spectrum.\n\nHowever, attempting to reconcile electromagnetic theory with two observations, the photoelectric effect, and the nonexistence of the ultraviolet catastrophe, proved troublesome. Through the work of leading theoretical physicists, a new theory of electromagnetism was developed using quantum mechanics. This final modification to electromagnetic theory ultimately led to quantum electrodynamics (or QED), which fully describes all electromagnetic phenomena as being mediated by wave–particles known as photons. In QED, photons are the fundamental exchange particle, which described all interactions relating to electromagnetism including the electromagnetic force.\n\nIt is a common misconception to ascribe the stiffness and rigidity of solid matter to the repulsion of like charges under the influence of the electromagnetic force. However, these characteristics actually result from the Pauli exclusion principle. Since electrons are fermions, they cannot occupy the same quantum mechanical state as other electrons. When the electrons in a material are densely packed together, there are not enough lower energy quantum mechanical states for them all, so some of them must be in higher energy states. This means that it takes energy to pack them together. While this effect is manifested macroscopically as a structural force, it is technically only the result of the existence of a finite set of electron states.\n\nThere are two \"nuclear forces\", which today are usually described as interactions that take place in quantum theories of particle physics. The strong nuclear force is the force responsible for the structural integrity of atomic nuclei while the weak nuclear force is responsible for the decay of certain nucleons into leptons and other types of hadrons.\n\nThe strong force is today understood to represent the interactions between quarks and gluons as detailed by the theory of quantum chromodynamics (QCD). The strong force is the fundamental force mediated by gluons, acting upon quarks, antiquarks, and the gluons themselves. The (aptly named) strong interaction is the \"strongest\" of the four fundamental forces.\n\nThe strong force only acts \"directly\" upon elementary particles. However, a residual of the force is observed between hadrons (the best known example being the force that acts between nucleons in atomic nuclei) as the nuclear force. Here the strong force acts indirectly, transmitted as gluons, which form part of the virtual pi and rho mesons, which classically transmit the nuclear force (see this topic for more). The failure of many searches for free quarks has shown that the elementary particles affected are not directly observable. This phenomenon is called color confinement.\n\nThe weak force is due to the exchange of the heavy W and Z bosons. Its most familiar effect is beta decay (of neutrons in atomic nuclei) and the associated radioactivity. The word \"weak\" derives from the fact that the field strength is some 10 times less than that of the strong force. Still, it is stronger than gravity over short distances. A consistent electroweak theory has also been developed, which shows that electromagnetic forces and the weak force are indistinguishable at a temperatures in excess of approximately 10 kelvins. Such temperatures have been probed in modern particle accelerators and show the conditions of the universe in the early moments of the Big Bang.\n\nSome forces are consequences of the fundamental ones. In such situations, idealized models can be utilized to gain physical insight.\n\nThe normal force is due to repulsive forces of interaction between atoms at close contact. When their electron clouds overlap, Pauli repulsion (due to fermionic nature of electrons) follows resulting in the force that acts in a direction normal to the surface interface between two objects. The normal force, for example, is responsible for the structural integrity of tables and floors as well as being the force that responds whenever an external force pushes on a solid object. An example of the normal force in action is the impact force on an object crashing into an immobile surface.\n\nFriction is a surface force that opposes relative motion. The frictional force is directly related to the normal force that acts to keep two solid objects separated at the point of contact. There are two broad classifications of frictional forces: static friction and kinetic friction.\n\nThe static friction force (formula_56) will exactly oppose forces applied to an object parallel to a surface contact up to the limit specified by the coefficient of static friction (formula_57) multiplied by the normal force (formula_58). In other words, the magnitude of the static friction force satisfies the inequality:\n\nThe kinetic friction force (formula_60) is independent of both the forces applied and the movement of the object. Thus, the magnitude of the force equals:\n\nwhere formula_62 is the coefficient of kinetic friction. For most surface interfaces, the coefficient of kinetic friction is less than the coefficient of static friction.\n\nTension forces can be modeled using ideal strings that are massless, frictionless, unbreakable, and unstretchable. They can be combined with ideal pulleys, which allow ideal strings to switch physical direction. Ideal strings transmit tension forces instantaneously in action-reaction pairs so that if two objects are connected by an ideal string, any force directed along the string by the first object is accompanied by a force directed along the string in the opposite direction by the second object. By connecting the same string multiple times to the same object through the use of a set-up that uses movable pulleys, the tension force on a load can be multiplied. For every string that acts on a load, another factor of the tension force in the string acts on the load. However, even though such machines allow for an increase in force, there is a corresponding increase in the length of string that must be displaced in order to move the load. These tandem effects result ultimately in the conservation of mechanical energy since the work done on the load is the same no matter how complicated the machine.\n\nAn elastic force acts to return a spring to its natural length. An ideal spring is taken to be massless, frictionless, unbreakable, and infinitely stretchable. Such springs exert forces that push when contracted, or pull when extended, in proportion to the displacement of the spring from its equilibrium position. This linear relationship was described by Robert Hooke in 1676, for whom Hooke's law is named. If formula_63 is the displacement, the force exerted by an ideal spring equals:\n\nwhere formula_65 is the spring constant (or force constant), which is particular to the spring. The minus sign accounts for the tendency of the force to act in opposition to the applied load.\n\nNewton's laws and Newtonian mechanics in general were first developed to describe how forces affect idealized point particles rather than three-dimensional objects. However, in real life, matter has extended structure and forces that act on one part of an object might affect other parts of an object. For situations where lattice holding together the atoms in an object is able to flow, contract, expand, or otherwise change shape, the theories of continuum mechanics describe the way forces affect the material. For example, in extended fluids, differences in pressure result in forces being directed along the pressure gradients as follows:\n\nwhere formula_67 is the volume of the object in the fluid and formula_68 is the scalar function that describes the pressure at all locations in space. Pressure gradients and differentials result in the buoyant force for fluids suspended in gravitational fields, winds in atmospheric science, and the lift associated with aerodynamics and flight.\n\nA specific instance of such a force that is associated with dynamic pressure is fluid resistance: a body force that resists the motion of an object through a fluid due to viscosity. For so-called \"Stokes' drag\" the force is approximately proportional to the velocity, but opposite in direction:\n\nwhere:\n\nMore formally, forces in continuum mechanics are fully described by a stress–tensor with terms that are roughly defined as\n\nwhere formula_73 is the relevant cross-sectional area for the volume for which the stress-tensor is being calculated. This formalism includes pressure terms associated with forces that act normal to the cross-sectional area (the matrix diagonals of the tensor) as well as shear terms associated with forces that act parallel to the cross-sectional area (the off-diagonal elements). The stress tensor accounts for forces that cause all strains (deformations) including also tensile stresses and compressions.\n\nThere are forces that are frame dependent, meaning that they appear due to the adoption of non-Newtonian (that is, non-inertial) reference frames. Such forces include the centrifugal force and the Coriolis force. These forces are considered fictitious because they do not exist in frames of reference that are not accelerating. Because these forces are not genuine they are also referred to as \"pseudo forces\".\n\nIn general relativity, gravity becomes a fictitious force that arises in situations where spacetime deviates from a flat geometry. As an extension, Kaluza–Klein theory and string theory ascribe electromagnetism and the other fundamental forces respectively to the curvature of differently scaled dimensions, which would ultimately imply that all forces are fictitious.\n\nForces that cause extended objects to rotate are associated with torques. Mathematically, the torque of a force formula_51 is defined relative to an arbitrary reference point as the cross-product:\n\nwhere\n\nTorque is the rotation equivalent of force in the same way that angle is the rotational equivalent for position, angular velocity for velocity, and angular momentum for momentum. As a consequence of Newton's First Law of Motion, there exists rotational inertia that ensures that all bodies maintain their angular momentum unless acted upon by an unbalanced torque. Likewise, Newton's Second Law of Motion can be used to derive an analogous equation for the instantaneous angular acceleration of the rigid body:\n\nwhere\n\nThis provides a definition for the moment of inertia, which is the rotational equivalent for mass. In more advanced treatments of mechanics, where the rotation over a time interval is described, the moment of inertia must be substituted by the tensor that, when properly analyzed, fully determines the characteristics of rotations including precession and nutation.\n\nEquivalently, the differential form of Newton's Second Law provides an alternative definition of torque:\n\nNewton's Third Law of Motion requires that all objects exerting torques themselves experience equal and opposite torques, and therefore also directly implies the conservation of angular momentum for closed systems that experience rotations and revolutions through the action of internal torques.\n\nFor an object accelerating in circular motion, the unbalanced force acting on the object equals:\n\nwhere formula_19 is the mass of the object, formula_16 is the velocity of the object and formula_43 is the distance to the center of the circular path and formula_44 is the unit vector pointing in the radial direction outwards from the center. This means that the unbalanced centripetal force felt by any object is always directed toward the center of the curving path. Such forces act perpendicular to the velocity vector associated with the motion of an object, and therefore do not change the speed of the object (magnitude of the velocity), but only the direction of the velocity vector. The unbalanced force that accelerates an object can be resolved into a component that is perpendicular to the path, and one that is tangential to the path. This yields both the tangential force, which accelerates the object by either slowing it down or speeding it up, and the radial (centripetal) force, which changes its direction.\n\nForces can be used to define a number of physical concepts by integrating with respect to kinematic variables. For example, integrating with respect to time gives the definition of impulse:\n\nwhich by Newton's Second Law must be equivalent to the change in momentum (yielding the Impulse momentum theorem).\n\nSimilarly, integrating with respect to position gives a definition for the work done by a force:\n\nwhich is equivalent to changes in kinetic energy (yielding the work energy theorem).\n\nPower \"P\" is the rate of change d\"W\"/d\"t\" of the work \"W\", as the trajectory is extended by a position change formula_89 in a time interval d\"t\":\n\nwith formula_91 the velocity.\n\nInstead of a force, often the mathematically related concept of a potential energy field can be used for convenience. For instance, the gravitational force acting upon an object can be seen as the action of the gravitational field that is present at the object's location. Restating mathematically the definition of energy (via the definition of work), a potential scalar field formula_92 is defined as that field whose gradient is equal and opposite to the force produced at every point:\n\nForces can be classified as conservative or nonconservative. Conservative forces are equivalent to the gradient of a potential while nonconservative forces are not.\n\nA conservative force that acts on a closed system has an associated mechanical work that allows energy to convert only between kinetic or potential forms. This means that for a closed system, the net mechanical energy is conserved whenever a conservative force acts on the system. The force, therefore, is related directly to the difference in potential energy between two different locations in space, and can be considered to be an artifact of the potential field in the same way that the direction and amount of a flow of water can be considered to be an artifact of the contour map of the elevation of an area.\n\nConservative forces include gravity, the electromagnetic force, and the spring force. Each of these forces has models that are dependent on a position often given as a radial vector formula_76 emanating from spherically symmetric potentials. Examples of this follow:\n\nFor gravity:\n\nwhere formula_96 is the gravitational constant, and formula_97 is the mass of object \"n\".\n\nFor electrostatic forces:\n\nwhere formula_99 is electric permittivity of free space, and formula_100 is the electric charge of object \"n\".\n\nFor spring forces:\n\nwhere formula_65 is the spring constant.\n\nFor certain physical scenarios, it is impossible to model forces as being due to gradient of potentials. This is often due to macrophysical considerations that yield forces as arising from a macroscopic statistical average of microstates. For example, friction is caused by the gradients of numerous electrostatic potentials between the atoms, but manifests as a force model that is independent of any macroscale position vector. Nonconservative forces other than friction include other contact forces, tension, compression, and drag. However, for any sufficiently detailed description, all these forces are the results of conservative ones since each of these macroscopic forces are the net results of the gradients of microscopic potentials.\n\nThe connection between macroscopic nonconservative forces and microscopic conservative forces is described by detailed treatment with statistical mechanics. In macroscopic closed systems, nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat. According to the Second law of thermodynamics, nonconservative forces necessarily result in energy transformations within closed systems from ordered to more random conditions as entropy increases.\n\nThe SI unit of force is the newton (symbol N), which is the force required to accelerate a one kilogram mass at a rate of one meter per second squared, or . The corresponding CGS unit is the dyne, the force required to accelerate a one gram mass by one centimeter per second squared, or . A newton is thus equal to 100,000 dynes.\n\nThe gravitational foot-pound-second English unit of force is the pound-force (lbf), defined as the force exerted by gravity on a pound-mass in the standard gravitational field of . The pound-force provides an alternative unit of mass: one slug is the mass that will accelerate by one foot per second squared when acted on by one pound-force.\n\nAn alternative unit of force in a different foot-pound-second system, the absolute fps system, is the poundal, defined as the force required to accelerate a one-pound mass at a rate of one foot per second squared. The units of slug and poundal are designed to avoid a constant of proportionality in Newton's Second Law.\n\nThe pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at when subjected to a force of 1 kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sthène, which is equivalent to 1000 N, and the kip, which is equivalent to 1000 lbf.\n\nSee also Ton-force.\n\nSee force gauge, spring scale, load cell\n\n\n"}
{"id": "10905", "url": "https://en.wikipedia.org/wiki?curid=10905", "title": "Family law", "text": "Family law\n\nFamily law (also called matrimonial law) is an area of the law that deals with family matters and domestic relations, including:\n\nThis list is not exhaustive and varies depending on jurisdiction. In many jurisdictions in the United States, the family courts see the most crowded dockets. Litigants representative of all social and economic classes are parties within the system.\n\nFor the conflict of laws elements dealing with transnational and interstate issues, see marriage (conflict), divorce (conflict) and nullity (conflict).\n\n\n\n"}
{"id": "10909", "url": "https://en.wikipedia.org/wiki?curid=10909", "title": "Foonly", "text": "Foonly\n\nFoonly was a short-lived American computer company formed by Dave Poole, one of the principal Super Foonly designers as well as one of hackerdom's more colourful personalities. The company produced a series of DEC PDP-10 compatible computers, first the high-performance F-1, and later a series of smaller and less expensive designs. The first Foonly machine, the F-1, was the computational engine used to create some of the graphics in the 1982 film \"Tron\".\n\nThe PDP-10 successor was to have been built by the Super Foonly project at the Stanford Artificial Intelligence Laboratory (SAIL) along with a new operating system. The intention was to leapfrog from the old DEC timesharing system which SAIL was then running to a new generation, bypassing TENEX – at that time the ARPANET standard. The F-1 was the fastest PDP-10 architecture machine ever built, with a clock rate of 90-100 ns per cycle, but only one was ever made. ARPA funding for both the Super Foonly and the new operating system was cut in 1974. The design for Foonly contributed greatly to the design of the PDP-10 model KL10.\n\nThe following few paragraphs are a personal account of the events, by Dave Dyer:\nFoonly Inc. did not acquire any financial resources as a result of building the F-1. They turned to the market for low-end machines, producing a series of smaller, slower, and much less expensive DEC-10 clones that ran a TENEX variant called Foonex; this seriously limited their market. Also, the machines shipped as wire-wrapped engineering prototypes requiring individual attention from more than usually competent site personnel, and thus had significant reliability problems. Poole's legendary temper and unwillingness to suffer fools gladly did not help matters. By the time of DEC's Jupiter project cancellation in 1983, Foonly's proposal to build another F-1 was eclipsed by another DEC-10 clone, the Mars computer, and the company never quite recovered.\nAdded by Phil Petit, (one of the above-mentioned Foonly designers):\n\nAdded by Dan Martin - Principal Engineer for Tymshare Inc.\nThe main application for Tymshare's version of the F4 was a version of Doug Englebart's NLS system, developed when his team moved to Tymshare from SRI, called \"Augment\". The machine, called the 26KL, was marketed as the \"Augment Engine\" when running Augment.\n\nOctober 6, 2015\n\nAdded by Paul Milleson - Principal Manager, Foonly Inc.\n\n\n"}
{"id": "10911", "url": "https://en.wikipedia.org/wiki?curid=10911", "title": "Functional group", "text": "Functional group\n\nIn organic chemistry, functional groups are specific groups (moieties) of atoms or bonds within molecules that are responsible for the characteristic chemical reactions of those molecules. The same functional group will undergo the same or similar chemical reaction(s) regardless of the size of the molecule it is a part of. However, its relative reactivity can be modified by other functional groups nearby. The atoms of functional groups are linked to each other and to the rest of the molecule by covalent bonds. Functional groups can also be charged, e.g. in carboxylate salts (–COO), which turns the molecule into a polyatomic ion or a complex ion. Functional groups binding to a central atom in a coordination complex are called \"ligands\".\n\nComplexation and solvation is also caused by specific interactions of functional groups. In the common rule of thumb \"like dissolves like\", it is the shared or mutually well-interacting functional groups which give rise to solubility. For example, sugar dissolves in water because both share the hydroxyl functional group (–OH) and hydroxyls interact strongly with each other.\n\nCombining the names of functional groups with the names of the parent alkanes generates what is termed a systematic nomenclature for naming organic compounds. In traditional nomenclature, the first carbon atom after the carbon that attaches to the functional group is called the alpha carbon; the second, beta carbon, the third, gamma carbon, etc. If there is another functional group at a carbon, it may be named with the Greek letter, e.g., the gamma-amine in gamma-aminobutyric acid is on the third carbon of the carbon chain attached to the carboxylic acid group. IUPAC conventions call for numeric labeling of the position, e.g. 4-aminobutanoic acid. In traditional names various qualifiers are used to label isomers; for example, isopropanol (IUPAC name: propan-2-ol) is an isomer of n-propanol (propan-1-ol).\n\nThe following is a list of common functional groups. In the formulas, the symbols R and R' usually denote an attached hydrogen, or a hydrocarbon side chain of any length, but may sometimes refer to any group of atoms.\n\nFunctional groups, called hydrocarbyl, that contain only carbon and hydrogen, but vary in the number and order of double bonds. Each one differs in type (and scope) of reactivity.\nThere are also a large number of branched or ring alkanes that have specific names, e.g., tert-butyl, bornyl, cyclohexyl, etc. Hydrocarbons may form charged structures: positively charged carbocations or negative carbanions. Carbocations are often named \"-um\". Examples are tropylium and triphenylmethyl cations and the cyclopentadienyl anion.\n\nHaloalkanes are a class of molecule that is defined by a carbon–halogen bond. This bond can be relatively weak (in the case of an iodoalkane) or quite stable (as in the case of a fluoroalkane). In general, with the exception of fluorinated compounds, haloalkanes readily undergo nucleophilic substitution reactions or elimination reactions. The substitution on the carbon, the acidity of an adjacent proton, the solvent conditions, etc. all can influence the outcome of the reactivity.\n\nCompounds that contain C-O bonds each possess differing reactivity based upon the location and hybridization of the C-O bond, owing to the electron-withdrawing effect of sp-hybridized oxygen (carbonyl groups) and the donating effects of sp-hybridized oxygen (alcohol groups).\nCompounds that contain nitrogen in this category may contain C-O bonds, such as in the case of amides.\nCompounds that contain sulfur exhibit unique chemistry due to their ability to form more bonds than oxygen, their lighter analogue on the periodic table. Substitutive nomenclature (marked as prefix in table) is preferred over functional class nomenclature (marked as suffix in table) for sulfides, disulfides, sulfoxides and sulfones.\nCompounds that contain phosphorus exhibit unique chemistry due to their ability to form more bonds than nitrogen, their lighter analogues on the periodic table.\nCompounds containing boron exhibit unique chemistry due to their having partially filled octets and therefore acting as Lewis acids.\n\nThese names are used to refer to the moieties themselves or to radical species, and also to form the names of halides and substituents in larger molecules.\n\nWhen the parent hydrocarbon is unsaturated, the suffix (\"-yl\", \"-ylidene\", or \"-ylidyne\") replaces \"-ane\" (e.g. \"ethane\" becomes \"ethyl\"); otherwise, the suffix replaces only the final \"-e\" (e.g. \"ethyne\" becomes \"ethynyl\").\n\nNote that when used to refer to moieties, multiple single bonds differ from a single multiple bond. For example, a methylene bridge (methanediyl) has two single bonds, whereas a methylene group (methylidene) has one double bond. Suffixes can be combined, as in methylidyne (triple bond) vs. methylylidene (single bond and double bond) vs. methanetriyl (three single bonds).\n\nThere are some retained names, such as methylene for methanediyl, 1,x-phenylene for phenyl-1,x-diyl (where x is 2, 3, or 4), carbyne for methylidyne, and trityl for triphenylmethyl.\n\n\n"}
{"id": "10913", "url": "https://en.wikipedia.org/wiki?curid=10913", "title": "Fractal", "text": "Fractal\n\nIn mathematics a fractal is an abstract object used to describe and simulate naturally occurring objects. Artificially created fractals commonly exhibit similar patterns at increasingly small scales. It is also known as expanding symmetry or evolving symmetry. If the replication is exactly the same at every scale, it is called a self-similar pattern. An example of this is the Menger Sponge. Fractals can also be nearly the same at different levels. This latter pattern is illustrated in small magnifications of the Mandelbrot set. Fractals also include the idea of a detailed pattern that repeats itself.\n\nFractals are different from other geometric figures because of the way in which they scale. Doubling the edge lengths of a polygon multiplies its area by four, which is two (the ratio of the new to the old side length) raised to the power of two (the dimension of the space the polygon resides in). Likewise, if the radius of a sphere is doubled, its volume scales by eight, which is two (the ratio of the new to the old radius) to the power of three (the dimension that the sphere resides in). But if a fractal's one-dimensional lengths are all doubled, the spatial content of the fractal scales by a power that is not necessarily an integer. This power is called the fractal dimension of the fractal, and it usually exceeds the fractal's topological dimension.\n\nAs mathematical equations, fractals are usually nowhere differentiable. An infinite fractal curve can be conceived of as winding through space differently from an ordinary line, still being a 1-dimensional line yet having a fractal dimension indicating it also resembles a surface.\nThe mathematical roots of the idea of fractals have been traced throughout the years as a formal path of published works, starting in the 17th century with notions of recursion, then moving through increasingly rigorous mathematical treatment of the concept to the study of continuous but not differentiable functions in the 19th century by the seminal work of Bernard Bolzano, Bernhard Riemann, and Karl Weierstrass, and on to the coining of the word \"fractal\" in the 20th century with a subsequent burgeoning of interest in fractals and computer-based modelling in the 20th century. The term \"fractal\" was first used by mathematician Benoit Mandelbrot in 1975. Mandelbrot based it on the Latin \"frāctus\" meaning \"broken\" or \"fractured\", and used it to extend the concept of theoretical fractional dimensions to geometric patterns in nature.\n\nThere is some disagreement amongst authorities about how the concept of a fractal should be formally defined. Mandelbrot himself summarized it as \"beautiful, damn hard, increasingly useful. That's fractals.\" The general consensus is that theoretical fractals are infinitely self-similar, iterated, and detailed mathematical constructs having fractal dimensions, of which many examples have been formulated and studied in great depth. Fractals are not limited to geometric patterns, but can also describe processes in time. Fractal patterns with various degrees of self-similarity have been rendered or studied in images, structures and sounds and found in nature, technology, art, and law. Fractals are of particular relevance in the field of chaos theory, since the graphs of most chaotic processes are fractal.\n\nThe word \"fractal\" often has different connotations for laypeople than for mathematicians, where the layperson is more likely to be familiar with fractal art than a mathematical conception. The mathematical concept is difficult to define formally even for mathematicians, but key features can be understood with little mathematical background.\n\nThe feature of \"self-similarity\", for instance, is easily understood by analogy to zooming in with a lens or other device that zooms in on digital images to uncover finer, previously invisible, new structure. If this is done on fractals, however, no new detail appears; nothing changes and the same pattern repeats over and over, or for some fractals, nearly the same pattern reappears over and over. Self-similarity itself is not necessarily counter-intuitive (e.g., people have pondered self-similarity informally such as in the infinite regress in parallel mirrors or the homunculus, the little man inside the head of the little man inside the head...). The difference for fractals is that the pattern reproduced must be detailed.\n\nThis idea of being detailed relates to another feature that can be understood without mathematical background: Having a fractional or fractal dimension greater than its topological dimension, for instance, refers to how a fractal scales compared to how geometric shapes are usually perceived. A regular line, for instance, is conventionally understood to be 1-dimensional; if such a curve is divided into pieces each 1/3 the length of the original, there are always 3 equal pieces. In contrast, consider the Koch snowflake. It is also 1-dimensional for the same reason as the ordinary line, but it has, in addition, a fractal dimension greater than 1 because of how its detail can be measured. The fractal curve divided into parts 1/3 the length of the original line becomes 4 pieces rearranged to repeat the original detail, and this unusual relationship is the basis of its fractal dimension.\n\nThis also leads to understanding a third feature, that fractals as mathematical equations are \"nowhere differentiable\". In a concrete sense, this means fractals cannot be measured in traditional ways. To elaborate, in trying to find the length of a wavy non-fractal curve, one could find straight segments of some measuring tool small enough to lay end to end over the waves, where the pieces could get small enough to be considered to conform to the curve in the normal manner of measuring with a tape measure. But in measuring a wavy fractal curve such as the Koch snowflake, one would never find a small enough straight segment to conform to the curve, because the wavy pattern would always re-appear, albeit at a smaller size, essentially pulling a little more of the tape measure into the total length measured each time one attempted to fit it tighter and tighter to the curve.\n\nThe history of fractals traces a path from chiefly theoretical studies to modern applications in computer graphics, with several notable people contributing canonical fractal forms along the way. According to Pickover, the mathematics behind fractals began to take shape in the 17th century when the mathematician and philosopher Gottfried Leibniz pondered recursive self-similarity (although he made the mistake of thinking that only the straight line was self-similar in this sense). In his writings, Leibniz used the term \"fractional exponents\", but lamented that \"Geometry\" did not yet know of them. Indeed, according to various historical accounts, after that point few mathematicians tackled the issues, and the work of those who did remained obscured largely because of resistance to such unfamiliar emerging concepts, which were sometimes referred to as mathematical \"monsters\". Thus, it was not until two centuries had passed that on July 18, 1872 Karl Weierstrass presented the first definition of a function with a graph that would today be considered a fractal, having the non-intuitive property of being everywhere continuous but nowhere differentiable at the Royal Prussian Academy of Sciences. In addition, the quotient difference becomes arbitrarily large as the summation index increases. Not long after that, in 1883, Georg Cantor, who attended lectures by Weierstrass, published examples of subsets of the real line known as Cantor sets, which had unusual properties and are now recognized as fractals. Also in the last part of that century, Felix Klein and Henri Poincaré introduced a category of fractal that has come to be called \"self-inverse\" fractals.\n\nOne of the next milestones came in 1904, when Helge von Koch, extending ideas of Poincaré and dissatisfied with Weierstrass's abstract and analytic definition, gave a more geometric definition including hand drawn images of a similar function, which is now called the Koch snowflake. Another milestone came a decade later in 1915, when Wacław Sierpiński constructed his famous triangle then, one year later, his carpet. By 1918, two French mathematicians, Pierre Fatou and Gaston Julia, though working independently, arrived essentially simultaneously at results describing what are now seen as fractal behaviour associated with mapping complex numbers and iterative functions and leading to further ideas about attractors and repellors (i.e., points that attract or repel other points), which have become very important in the study of fractals. Very shortly after that work was submitted, by March 1918, Felix Hausdorff expanded the definition of \"dimension\", significantly for the evolution of the definition of fractals, to allow for sets to have noninteger dimensions. The idea of self-similar curves was taken further by Paul Lévy, who, in his 1938 paper \"Plane or Space Curves and Surfaces Consisting of Parts Similar to the Whole\" described a new fractal curve, the Lévy C curve.\n\nDifferent researchers have postulated that without the aid of modern computer graphics, early investigators were limited to what they could depict in manual drawings, so lacked the means to visualize the beauty and appreciate some of the implications of many of the patterns they had discovered (the Julia set, for instance, could only be visualized through a few iterations as very simple drawings]). That changed, however, in the 1960s, when Benoit Mandelbrot started writing about self-similarity in papers such as \"How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension\", which built on earlier work by Lewis Fry Richardson. In 1975 Mandelbrot solidified hundreds of years of thought and mathematical development in coining the word \"fractal\" and illustrated his mathematical definition with striking computer-constructed visualizations. These images, such as of his canonical Mandelbrot set, captured the popular imagination; many of them were based on recursion, leading to the popular meaning of the term \"fractal\". Currently, fractal studies are essentially exclusively computer-based.\n\nIn 1980, Loren Carpenter gave a presentation at the SIGGRAPH where he introduced his software for generating and rendering fractally generated landscapes.\n\nOne often cited description that Mandelbrot published to describe geometric fractals is \"a rough or fragmented geometric shape that can be split into parts, each of which is (at least approximately) a reduced-size copy of the whole\"; this is generally helpful but limited. Authors disagree on the exact definition of \"fractal\", but most usually elaborate on the basic ideas of self-similarity and an unusual relationship with the space a fractal is embedded in. One point agreed on is that fractal patterns are characterized by fractal dimensions, but whereas these numbers quantify complexity (i.e., changing detail with changing scale), they neither uniquely describe nor specify details of how to construct particular fractal patterns. In 1975 when Mandelbrot coined the word \"fractal\", he did so to denote an object whose Hausdorff–Besicovitch dimension is greater than its topological dimension. It has been noted that this dimensional requirement is not met by fractal space-filling curves such as the Hilbert curve.\n\nAccording to Falconer, rather than being strictly defined, fractals should, in addition to being nowhere differentiable and able to have a fractal dimension, be generally characterized by a gestalt of the following features;\n\n\nAs a group, these criteria form guidelines for excluding certain cases, such as those that may be self-similar without having other typically fractal features. A straight line, for instance, is self-similar but not fractal because it lacks detail, is easily described in Euclidean language, has the same Hausdorff dimension as topological dimension, and is fully defined without a need for recursion.\n\nA path generated by a one dimensional Wiener process is a fractal curve of dimension 1.5, and Brownian motion is a finite version of this.\n\nImages of fractals can be created by fractal generating programs. Because of the butterfly effect a small change in a single variable can have a unpredictable outcome.\n\n\nFractal patterns have been modeled extensively, albeit within a range of scales rather than infinitely, owing to the practical limits of physical time and space. Models may simulate theoretical fractals or natural phenomena with fractal features. The outputs of the modelling process may be highly artistic renderings, outputs for investigation, or benchmarks for fractal analysis. Some specific applications of fractals to technology are listed elsewhere. Images and other outputs of modelling are normally referred to as being \"fractals\" even if they do not have strictly fractal characteristics, such as when it is possible to zoom into a region of the fractal image that does not exhibit any fractal properties. Also, these may include calculation or display artifacts which are not characteristics of true fractals.\n\nModeled fractals may be sounds, digital images, electrochemical patterns, circadian rhythms, etc.\nFractal patterns have been reconstructed in physical 3-dimensional space and virtually, often called \"in silico\" modeling. Models of fractals are generally created using fractal-generating software that implements techniques such as those outlined above. As one illustration, trees, ferns, cells of the nervous system, blood and lung vasculature, and other branching patterns in nature can be modeled on a computer by using recursive algorithms and L-systems techniques. The recursive nature of some patterns is obvious in certain examples—a branch from a tree or a frond from a fern is a miniature replica of the whole: not identical, but similar in nature. Similarly, random fractals have been used to describe/create many highly irregular real-world objects. A limitation of modeling fractals is that resemblance of a fractal model to a natural phenomenon does not prove that the phenomenon being modeled is formed by a process similar to the modeling algorithms.\n\nApproximate fractals found in nature display self-similarity over extended, but finite, scale ranges. The connection between fractals and leaves, for instance, is currently being used to determine how much carbon is contained in trees. Phenomena known to have fractal features include: \n\n\nSince 1999, more than 10 scientific groups have performed fractal analysis on over 50 of Jackson Pollock’s (1912–1956) paintings which were created by pouring paint directly onto his horizontal canvases Recently, fractal analysis has been used to achieve a 93% success rate in distinguishing real from imitation Pollocks. Cognitive neuroscientists have shown that Pollock's fractals induce the same stress-reduction in observers as computer-generated fractals and Nature's fractals.\n\nDecalcomania, a technique used by artists such as Max Ernst, can produce fractal-like patterns. It involves pressing paint between two surfaces and pulling them apart.\n\nCyberneticist Ron Eglash has suggested that fractal geometry and mathematics are prevalent in African art, games, divination, trade, and architecture. Circular houses appear in circles of circles, rectangular houses in rectangles of rectangles, and so on. Such scaling patterns can also be found in African textiles, sculpture, and even cornrow hairstyles. Hokky Situngkir also suggested the similar properties in Indonesian traditional art, batik, and ornaments found in traditional houses.\n\nIn a 1996 interview with Michael Silverblatt, David Foster Wallace admitted that the structure of the first draft of \"Infinite Jest\" he gave to his editor Michael Pietsch was inspired by fractals, specifically the Sierpinski triangle (a.k.a. Sierpinski gasket), but that the edited novel is \"more like a lopsided Sierpinsky Gasket\".\n\nHumans appear to be especially well-adapted to processing fractal patterns with D values between 1.3–1.5. When humans view fractal patterns with D values between 1.3–1.5, this tends to reduce physiological stress.\n\nIf a circle boundary is drawn around the two-dimensional view of a fractal, the fractal will never cross the boundary, this is due to the scaling of each successive iteration of the fractal being smaller. When fractals are iterated many times, the perimeter of the fractal increases, while the area will never exceed a certain value. A fractal in three-dimensional space is similar, however, a difference between fractals in two dimensions and three dimensions, is that a three dimensional fractal will increase in surface area, but never exceed a certain volume. This can be utilized to maximize the efficiency of ion propulsion, when choosing electron emitter construction and material. If done correctly, the efficiency of the emission process can be maximized.\n\n\n"}
{"id": "10915", "url": "https://en.wikipedia.org/wiki?curid=10915", "title": "Fluid", "text": "Fluid\n\nIn physics, a fluid is a substance that continually deforms (flows) under an applied shear stress. Fluids are a subset of the phases of matter and include liquids, gases, plasmas, and to some extent, plastic solids. Fluids are substances that have zero shear modulus, or, in simpler terms, a fluid is a substance which cannot resist any shear force applied to it.\n\nAlthough the term \"fluid\" includes both the liquid and gas phases, in common usage, \"fluid\" is often used as a synonym for \"liquid\", with no implication that gas could also be present. For example, \"brake fluid\" is hydraulic oil and will not perform its required incompressible function if there is gas in it. This colloquial usage of the term is also common in medicine and in nutrition (\"take plenty of fluids\").\n\nLiquids form a free surface (that is, a surface not created by the container) while gases do not. The distinction between solids and fluid is not entirely obvious. The distinction is made by evaluating the viscosity of the substance. Silly Putty can be considered to behave like a solid or a fluid, depending on the time period over which it is observed. It is best described as a viscoelastic fluid. There are many examples of substances proving difficult to classify. A particularly interesting one is pitch, as demonstrated in the pitch drop experiment currently running at the University of Queensland.\n\nFluids display properties such as:\nThese properties are typically a function of their inability to support a shear stress in static equilibrium. In contrast, solids respond to shear with a spring-like restoring force, which means that small deformations, whether shear or normal, are reversible.\n\nSolids respond with restoring forces to both shear stresses, and to normal stresses—both compressive and tensile. In contrast, ideal fluids only respond with restoring forces to normal stresses, called pressure: fluids can be subjected to both compressive stress, corresponding to positive pressure, and to tensile stress, corresponding to negative pressure. Both solids and liquids also have tensile strengths, which when exceeded in solids causes irreversible deformation and fracture, and in liquids causes the onset of cavitation. Gases do not have tensile strength, and freely expand in response to changes in pressure.\n\nBoth solids and liquids have free surfaces, which cost some amount of free energy to form. In the case of solids, the amount of free energy to form a given unit of surface area is called surface energy, whereas for liquids the same quantity is called surface tension. The ability of liquids to flow results in very different behaviour in response to surface tension than in solids, although in equilibrium both will try to minimise their surface energy: liquids tend to form rounded droplets, whereas pure solids tend to form crystals. Gases do not have free surfaces, and freely diffuse.\n\nIn a solid, shear stress is a function of strain, but in a fluid, shear stress is a function of strain rate. A consequence of this behavior is Pascal's law which describes the role of pressure in characterizing a fluid's state. \n\nDepending on the relationship between shear stress, and the rate of strain and its derivatives, fluids can be characterized as one of the following:\n\nThe behavior of fluids can be described by the Navier–Stokes equations—a set of partial differential equations which are based on:\n\nThe study of fluids is fluid mechanics, which is subdivided into fluid dynamics and fluid statics depending on whether the fluid is in motion.\n\n"}
{"id": "10916", "url": "https://en.wikipedia.org/wiki?curid=10916", "title": "FAQ", "text": "FAQ\n\nFrequently asked questions (FAQ) or Questions and Answers (Q&A), are listed questions and answers, all supposed to be commonly asked in some context, and pertaining to a particular topic. The format is commonly used on email mailing lists and other online forums, where certain common questions tend to recur.\n\n\"FAQ\" is pronounced as either an initialism (F-A-Q) or an acronym. Since the acronym \"FAQ\" originated in textual media, its pronunciation varies; \"F-A-Q\", is commonly heard. Depending on usage, the term may refer specifically to a single frequently asked question, or to an assembled list of many questions and their answers. Web page designers often label a single list of questions as an \"FAQ\", such as on Google.com, while using \"FAQs\" to denote multiple lists of questions such as on United States Treasury sites.\n\nWhile the name may be recent, the FAQ format itself is quite old. For instance, Matthew Hopkins wrote \"The Discovery of Witches\" in 1647 as a list of questions and answers, introduced as \"Certain Queries answered\". Many old catechisms are in a question-and-answer (Q&A) format. Summa Theologica, written by Thomas Aquinas in the second half of the 13th century, is a series of common questions about Christianity to which he wrote a series of replies. Plato's dialogues are even older.\n\nThe \"FAQ\" is an Internet textual tradition originating from the technical limitations of early mailing lists from NASA in the early 1980`s. The first FAQ developed over several pre-Web years starting from 1982 when storage was expensive. On ARPAnet's SPACE mailing list, the presumption was that new users would download archived past messages through ftp. In practice, this rarely happened and the users tended to post questions to the mailing list instead of searching its archives. Repeating the \"right\" answers becomes tedious, and went against developing netiquette. A series of different measures were set up by loosely affiliated groups of computer system administrators, from regularly posted messages to netlib-like query email daemons. The acronym \"FAQ\" was developed between 1982 and 1985 by Eugene Miya of NASA for the SPACE mailing list. The format was then picked up on other mailing lists and Usenet news groups. Posting frequency changed to monthly, and finally weekly and daily across a variety of mailing lists and newsgroups. The first person to post a weekly FAQ was Jef Poskanzer to the Usenet net.graphics/comp.graphics newsgroups. Eugene Miya experimented with the first daily FAQ.\n\nMeanwhile, on Usenet, Mark Horton had started a series of \"Periodic Posts\" (PP) which attempted to answer trivial questions with appropriate answers. Periodic summary messages posted to Usenet newsgroups attempted to reduce the continual reposting of the same basic questions and associated wrong answers. On Usenet, posting questions which are covered in a group's FAQ came to be considered poor netiquette, as it showed that the poster has not done the expected background reading before asking others to provide answers. Some groups may have multiple FAQ on related topics, or even two or more competing FAQs explaining a topic from different points of view.\n\nAnother factor on early ARPANET mailing lists was people asking questions promising to 'summarize' received answers, then either neglecting to do this or else posting simple concatenations of received replies with little to no quality checking.\n\nOriginally the term \"FAQ\" referred to the Frequently Asked Question itself, and the compilation of questions and answers was known as a \"FAQ list\" or some similar expression. The term became more frequently used to refer to the list, and a text consisting of questions and their answers is often called a FAQ regardless of whether the questions are actually \"frequently\" asked, if they are asked at all, or if there is even any way of asking questions.\n\nIn some cases informative documents not in the traditional FAQ style have also been described as FAQs, particularly the video game FAQ, which is often a detailed descriptions of gameplay, including tips, secrets, and beginning-to-end guidance. Rarely are videogame FAQs in a question-and-answer format, although they may contain a short section of questions and answers.\n\nOver time, the accumulated FAQs across all USENET news groups sparked the creation of the \"*.answers\" moderated newsgroups such as comp.answers, misc.answers and sci.answers for crossposting and collecting FAQ across respective comp.*, misc.*, sci.* newsgroups.\n\n"}
{"id": "10918", "url": "https://en.wikipedia.org/wiki?curid=10918", "title": "Fibonacci number", "text": "Fibonacci number\n\nIn mathematics, the Fibonacci numbers are the numbers in the following integer sequence, called the Fibonacci sequence, and characterized by the fact that every number after the first two is the sum of the two preceding ones:\n\nOften, especially in modern usage, the sequence is extended by one more initial term:\n\nBy definition, the first two numbers in the Fibonacci sequence are either 1 and 1, or 0 and 1, depending on the chosen starting point of the sequence, and each subsequent number is the sum of the previous two.\n\nThe sequence \"F\" of Fibonacci numbers is defined by the recurrence relation:\n\nwith seed values\nor\n\nThe Fibonacci sequence is named after Italian mathematician Leonardo of Pisa, known as Fibonacci. His 1202 book \"Liber Abaci\" introduced the sequence to Western European mathematics, although the sequence had been described earlier in Indian mathematics. The sequence described in \"Liber Abaci\" began with \"F\" = 1.\n\nFibonacci numbers are closely related to Lucas numbers formula_6 in that they form a complementary pair of Lucas sequences formula_7 and formula_8. They are intimately connected with the golden ratio; for example, the closest rational approximations to the ratio are 2/1, 3/2, 5/3, 8/5, ... .\n\nFibonacci numbers appear unexpectedly often in mathematics, so much so that there is an entire journal dedicated to their study, the \"Fibonacci Quarterly\". Applications of Fibonacci numbers include computer algorithms such as the Fibonacci search technique and the Fibonacci heap data structure, and graphs called Fibonacci cubes used for interconnecting parallel and distributed systems. They also appear in biological settings, such as branching in trees, phyllotaxis (the arrangement of leaves on a stem), the fruit sprouts of a pineapple, the flowering of an artichoke, an uncurling fern and the arrangement of a pine cone's bracts.\n\nThe Fibonacci sequence appears in Indian mathematics, in connection with Sanskrit prosody. In the Sanskrit tradition of prosody, there was interest in enumerating all patterns of long (L) syllables that are 2 units of duration, and short (S) syllables that are 1 unit of duration. Counting the different patterns of L and S of a given duration results in the Fibonacci numbers: the number of patterns that are \"m\" short syllables long is the Fibonacci number \"F\".\n\nSusantha Goonatilake writes that the development of the Fibonacci sequence \"is attributed in part to Pingala (200 BC), later being associated with Virahanka (c. 700 AD), Gopāla (c. 1135), and Hemachandra (c. 1150)\". Parmanand Singh cites Pingala's cryptic formula \"misrau cha\" (\"the two are mixed\") and cites scholars who interpret it in context as saying that the cases for \"m\" beats (\"F\") is obtained by adding a [S] to \"F\" cases and [L] to the \"F\" cases. He dates Pingala before 450 BC.\n\nHowever, the clearest exposition of the sequence arises in the work of Virahanka (c. 700 AD), whose own work is lost, but is available in a quotation by Gopala (c. 1135):\n\nThe sequence is also discussed by Gopala (before 1135 AD) and by the Jain scholar Hemachandra (c. 1150).\nOutside India, the Fibonacci sequence first appears in the book \"Liber Abaci\" (1202) by Fibonacci. Fibonacci considers the growth of an idealized (biologically unrealistic) rabbit population, assuming that: a newly born pair of rabbits, one male, one female, are put in a field; rabbits are able to mate at the age of one month so that at the end of its second month a female can produce another pair of rabbits; rabbits never die and a mating pair always produces one new pair (one male, one female) every month from the second month on. The puzzle that Fibonacci posed was: how many pairs will there be in one year?\n\n\nAt the end of the \"n\"th month, the number of pairs of rabbits is equal to the number of new pairs (which is the number of pairs in month \"n\" − 2) plus the number of pairs alive last month (\"n\" − 1). This is the \"n\"th Fibonacci number.\n\nThe name \"Fibonacci sequence\" was first used by the 19th-century number theorist Édouard Lucas.\n\nThe first 21 Fibonacci numbers \"F\" for \"n\" = 0, 1, 2, …, 20 are:\n\nThe sequence can also be extended to negative index \"n\" using the re-arranged recurrence relation\n\nThus the bidirectional sequence is\n\nThe Fibonacci numbers occur in the sums of \"shallow\" diagonals in Pascal's triangle (see binomial coefficient):\n\nThese numbers also give the solution to certain enumerative problems. The most common such problem is that of counting the number of compositions of 1s and 2s that sum to a given total \"n\": there are \"F\" ways to do this.\n\nFor example, if \"n\" = 5, then \"F\" = \"F\" = 8 counts the eight compositions:\n1+1+1+1+1 = 1+1+1+2 = 1+1+2+1 = 1+2+1+1 = 2+1+1+1 = 2+2+1 = 2+1+2 = 1+2+2,\nall of which sum to 5.\n\nThe Fibonacci numbers can be found in different ways among the set of binary strings, or equivalently, among the subsets of a given set.\n\n\nLike every sequence defined by a linear recurrence with constant coefficients, the Fibonacci numbers have a closed-form solution. It has become known as \"Binet's formula\", even though it was already known by Abraham de Moivre:\n\nwhere\n\nis the golden ratio (), and\n\nSince formula_15, this formula can also be written as\n\nformula_16\n\nTo see this, note that φ and ψ are both solutions of the equations\n\nso the powers of φ and ψ satisfy the Fibonacci recursion. In other words,\n\nand\n\nIt follows that for any values \"a\" and \"b\", the sequence defined by\n\nsatisfies the same recurrence\n\nIf \"a\" and \"b\" are chosen so that \"U\" = 0 and \"U\" = 1 then the resulting sequence \"U\" must be the Fibonacci sequence. This is the same as requiring \"a\" and \"b\" satisfy the system of equations:\n\nwhich has solution\n\nproducing the required formula.\n\nTaking \"U\" and \"U\" to be variables, a more general solution can be found for any starting values:\n\nwhere\n\nSince\n\nfor all \"n\" ≥ 0, the number \"F\" is the closest integer to formula_28. Therefore, it can be found by rounding, that is by the use of the nearest integer function:\n\nor in terms of the floor function:\n\nSimilarly, if we already know that the number \"F\" > 1 is a Fibonacci number, we can determine its index within the sequence by\n\nwhere formula_32 can be computed using logarithms to other usual bases.\nFor example, formula_33.\n\nJohannes Kepler observed that the ratio of consecutive Fibonacci numbers converges. He wrote that \"as 5 is to 8 so is 8 to 13, practically, and as 8 is to 13, so is 13 to 21 almost\", and concluded that the limit approaches the golden ratio formula_34.\n\nThis convergence holds regardless of the starting values, excluding 0, 0. This can be derived from Binet's formula. For example, the initial values 3 and 2 generate the sequence 3, 2, 5, 7, 12, 19, 31, 50, 81, 131, 212, 343, 555, …, etc. The ratio of consecutive terms in this sequence shows the same convergence towards the golden ratio.\n\nAnother consequence is that the limit of the ratio of two Fibonacci numbers offset by a particular finite deviation in index corresponds to the golden ratio raised by that deviation. Or, in other words:\n\nSince the golden ratio satisfies the equation\n\nthis expression can be used to decompose higher powers formula_38 as a linear function of lower powers, which in turn can be decomposed all the way down to a linear combination of formula_34 and 1. The resulting recurrence relationships yield Fibonacci numbers as the linear coefficients:\nThis equation can be proved by induction on \"n\".\n\nThis expression is also true for \"n\" < 1 if the Fibonacci sequence \"F\" is extended to negative integers using the Fibonacci rule formula_41\n\nA 2-dimensional system of linear difference equations that describes the Fibonacci sequence is\n\nalternatively denoted\n\nwhich yields formula_44. As the eigenvalues of the matrix A are formula_45 and formula_46 corresponding to the respective eigenvectors \nand\nand the initial value is\nit follows that the th term is\nfrom which the th element in the Fibonacci series\nas an analytic function of is now read off directly:\n\nEquivalently, the same computation is performed by diagonalization of A through use of its eigendecomposition:\nwhere formula_53 and formula_54 .\nThe closed-form expression for the th element in the Fibonacci series is therefore given by\n\nwhich again yields\n\nThe matrix A has a determinant of −1, and thus it is a 2×2 unimodular matrix.\n\nThis property can be understood in terms of the continued fraction representation for the golden ratio:\n\nThe Fibonacci numbers occur as the ratio of successive convergents of the continued fraction for , and the matrix formed from successive convergents of any continued fraction has a determinant of +1 or −1. The matrix representation gives the following closed expression for the Fibonacci numbers:\n\nTaking the determinant of both sides of this equation yields Cassini's identity,\n\nMoreover, since for any square matrix A, the following identities can be derived (they are obtained from two different coefficients of the matrix product, and one may easily deduce the second one from the first one by changing into ),\nIn particular, with \"m\" = \"n\",\n\nThese last two identities provide a way to compute Fibonacci numbers recursively in arithmetic operations and in time , where is the time for the multiplication of two numbers of \"n\" digits. This matches the time for computing the \"n\"th Fibonacci number from the closed-form matrix formula, but with fewer redundant steps if one avoids recomputing an already computed Fibonacci number (recursion with memoization).\n\nThe question may arise whether a positive integer \"x\" is a Fibonacci number. This is true if and only if one or both of formula_62 or formula_63 is a perfect square. This is because Binet's formula above can be rearranged to give\n\nwhich allows one to find the position in the sequence of a given Fibonacci number.\n\nThis formula must return an integer for all \"n\", so the radical expression must be an integer (otherwise the logarithm does not even return a rational number).\nFor formula_65, the formula_66th \"non\"-Fibonacci number (i.e., 4, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 22, ...) is\nformula_67\nMost identities involving Fibonacci numbers can be proved using combinatorial arguments using the fact that \"F\" can be interpreted as the number of sequences of 1s and 2s that sum to \"n\" − 1. This can be taken as the definition of \"F\", with the convention that \"F\" = 0, meaning no sum adds up to −1, and that \"F\" = 1, meaning the empty sum \"adds up\" to 0. Here, the order of the summand matters. For example, 1 + 2 and 2 + 1 are considered two different sums.\n\nFor example, the recurrence relation\n\nor in words, the \"n\"th Fibonacci number is the sum of the previous two Fibonacci numbers, may be shown by dividing the \"F\" sums of 1s and 2s that add to \"n\" − 1 into two non-overlapping groups. One group contains those sums whose first term is 1 and the other those sums whose first term is 2. In the first group the remaining terms add to \"n\" − 2, so it has \"F\" sums, and in the second group the remaining terms add to \"n\" − 3, so there are \"F\" sums. So there are a total of \"F\" + \"F\" sums altogether, showing this is equal to \"F\".\n\nSimilarly, it may be shown that the sum of the first Fibonacci numbers up to the \"n\"th is equal to the (\"n\" + 2)-nd Fibonacci number minus 1. In symbols:\n\nThis is done by dividing the sums adding to \"n\" + 1 in a different way, this time by the location of the first 2. Specifically, the first group consists of those sums that start with 2, the second group those that start 1 + 2, the third 1 + 1 + 2, and so on, until the last group, which consists of the single sum where only 1's are used. The number of sums in the first group is \"F\"(\"n\"), \"F\"(\"n\" − 1) in the second group, and so on, with 1 sum in the last group. So the total number of sums is \"F\"(\"n\") + \"F\"(\"n\" − 1) + ... + \"F\"(1) + 1 and therefore this quantity is equal to \"F\"(\"n\" + 2).\n\nA similar argument, grouping the sums by the position of the first 1 rather than the first 2, gives two more identities:\nand\nIn words, the sum of the first Fibonacci numbers with odd index up to \"F\" is the (2\"n\")th Fibonacci number, and the sum of the first Fibonacci numbers with even index up to \"F\" is the (2\"n\" + 1)th Fibonacci number minus 1.\n\nA different trick may be used to prove\nor in words, the sum of the squares of the first Fibonacci numbers up to \"F\" is the product of the \"n\"th and (\"n\" + 1)th Fibonacci numbers. In this case note that Fibonacci rectangle of size \"F\" by \"F\"(\"n\" + 1) can be decomposed into squares of size \"F\", \"F\", and so on to \"F\" = 1, from which the identity follows by comparing areas.\n\nNumerous other identities can be derived using various methods. Some of the most noteworthy are:\n\nCassini's identity states that\nCatalan's identity is a generalization:\n\nwhere \"L\" is the \"n\"'th Lucas number. The last is an identity for doubling \"n\"; other identities of this type are\nby Cassini's identity.\n\nThese can be found experimentally using lattice reduction, and are useful in setting up the special number field sieve to factorize a Fibonacci number.\n\nMore generally,\n\nPutting in this formula, one gets again the formulas of the end of above section Matrix form.\n\nThe generating function of the Fibonacci sequence is the power series\n\nThis series is convergent for formula_83 and its sum has a simple closed-form:\n\nThis can be proved by using the Fibonacci recurrence to expand each coefficient in the infinite sum:\n\nSolving the equation\n\nfor \"s\"(\"x\") results in the above closed form.\n\nIf is the reciprocal of an integer \"k\" that is greater than 1, the closed form of the series becomes\n\nIn particular,\nfor all positive integers \"m\".\n\nSome math puzzle-books present as curious the particular value that comes from \"m\"=1, which is formula_89 Similarly, \"m\"=2 gives formula_90\n\nInfinite sums over reciprocal Fibonacci numbers can sometimes be evaluated in terms of theta functions. For example, we can write the sum of every odd-indexed reciprocal Fibonacci number as\n\nand the sum of squared reciprocal Fibonacci numbers as\n\nIf we add 1 to each Fibonacci number in the first sum, there is also the closed form\n\nand there is a \"nested\" sum of squared Fibonacci numbers giving the reciprocal of the golden ratio,\n\nNo closed formula for the reciprocal Fibonacci constant\n\nis known, but the number has been proved irrational by Richard André-Jeannin.\n\nThe Millin series gives the identity\nwhich follows from the closed form for its partial sums as \"N\" tends to infinity:\n\nEvery 3rd number of the sequence is even and more generally, every \"k\"th number of the sequence is a multiple of \"F\". Thus the Fibonacci sequence is an example of a divisibility sequence. In fact, the Fibonacci sequence satisfies the stronger divisibility property\n\nAny three consecutive Fibonacci numbers are pairwise coprime, which means that, for every \"n\",\n\nEvery prime number \"p\" divides a Fibonacci number that can be determined by the value of \"p\" modulo 5. If \"p\" is congruent to 1 or 4 (mod 5), then \"p\" divides \"F\", and if \"p\" is congruent to 2 or 3 (mod 5), then, \"p\" divides \"F\". The remaining case is that \"p\" = 5, and in this case \"p\" divides \"F\". These cases can be combined into a single formula, using the Legendre symbol:\n\nThe above formula can be used as a primality test in the sense that if\nWhen m is large—say a 500-bit number—then we can calculate F (mod n) efficiently using the matrix form. Thus\n\nHere the matrix power A is calculated using Modular exponentiation, which can be adapted to matrices--modular exponentiation for matrices\n\nA \"Fibonacci prime\" is a Fibonacci number that is prime. The first few are:\n\nFibonacci primes with thousands of digits have been found, but it is not known whether there are infinitely many.\n\n\"F\" is divisible by \"F\", so, apart from \"F\" = 3, any Fibonacci prime must have a prime index. As there are arbitrarily long runs of composite numbers, there are therefore also arbitrarily long runs of composite Fibonacci numbers.\n\nNo Fibonacci number greater than \"F\" = 8 is one greater or one less than a prime number.\n\nThe only nontrivial square Fibonacci number is 144. Attila Pethő proved in 2001 that there is only a finite number of perfect power Fibonacci numbers. In 2006, Y. Bugeaud, M. Mignotte, and S. Siksek proved that 8 and 144 are the only such non-trivial perfect powers.\n\nWith the exceptions of 1, 8 and 144 (\"F\" = \"F\", \"F\" and \"F\") every Fibonacci number has a prime factor that is not a factor of any smaller Fibonacci number (Carmichael's theorem). As a result, 8 and 144 (\"F\" and \"F\") are the only Fibonacci numbers that are the product of other Fibonacci numbers .\n\nThe divisibility of Fibonacci numbers by a prime \"p\" is related to the Legendre symbol formula_103 which is evaluated as follows:\n\nIf \"p\" is a prime number then\n\nFor example,\n\nIt is not known whether there exists a prime \"p\" such that\n\nSuch primes (if there are any) would be called Wall–Sun–Sun primes.\n\nAlso, if \"p\" ≠ 5 is an odd prime number then:\n\nExample 1. \"p\" = 7, in this case \"p\" ≡ 3 (mod 4) and we have:\n\nExample 2. \"p\" = 11, in this case \"p\" ≡ 3 (mod 4) and we have:\n\nExample 3. \"p\" = 13, in this case \"p\" ≡ 1 (mod 4) and we have:\n\nExample 4. \"p\" = 29, in this case \"p\" ≡ 1 (mod 4) and we have:\n\nFor odd \"n\", all odd prime divisors of \"F\" are congruent to 1 modulo 4, implying that all odd divisors of \"F\" (as the products of odd prime divisors) are congruent to 1 modulo 4.\n\nFor example,\n\nAll known factors of Fibonacci numbers \"F\"(\"i\") for all \"i\" < 50000 are collected at the relevant repositories.\n\nIf the members of the Fibonacci sequence are taken mod \"n\", the resulting sequence is periodic with period at most \"6n\". The lengths of the periods for various \"n\" form the so-called Pisano periods . Determining a general formula for the Pisano periods is an open problem, which includes as a subproblem a special instance of the problem of finding the multiplicative order of a modular integer or of an element in a finite field. However, for any particular \"n\", the Pisano period may be found as an instance of cycle detection.\n\nStarting with 5, every second Fibonacci number is the length of the hypotenuse of a right triangle with integer sides, or in other words, the largest number in a Pythagorean triple. The length of the longer leg of this triangle is equal to the sum of the three sides of the preceding triangle in this series of triangles, and the shorter leg is equal to the difference between the preceding bypassed Fibonacci number and the shorter leg of the preceding triangle.\n\nThe first triangle in this series has sides of length 5, 4, and 3. Skipping 8, the next triangle has sides of length 13, 12 (5 + 4 + 3), and 5 (8 − 3). Skipping 21, the next triangle has sides of length 34, 30 (13 + 12 + 5), and 16 (21 − 5). This series continues indefinitely. The triangle sides \"a\", \"b\", \"c\" can be calculated directly:\n\nThese formulas satisfy formula_123 for all \"n\", but they only represent triangle sides when \"n\" > 2.\n\nAny four consecutive Fibonacci numbers \"F\", \"F\", \"F\" and \"F\" can also be used to generate a Pythagorean triple in a different way:\nExample 1: let the Fibonacci numbers be 1, 2, 3 and 5. Then:\n\nSince \"F\" is asymptotic to formula_126, the number of digits in \"F\" is asymptotic to formula_127. As a consequence, for every integer \"d\" > 1 there are either 4 or 5 Fibonacci numbers with \"d\" decimal digits.\n\nMore generally, in the base \"b\" representation, the number of digits in \"F\" is asymptotic to formula_128.\n\nThe Fibonacci numbers are important in the computational run-time analysis of Euclid's algorithm to determine the greatest common divisor of two integers: the worst case input for this algorithm is a pair of consecutive Fibonacci numbers.\n\nBrasch et al. 2012 show how a generalised Fibonacci sequence also can be connected to the field of economics. In particular, it is shown how a generalised Fibonacci sequence enters the control function of ﬁnite-horizon dynamic optimisation problems with one state and one control variable. The procedure is illustrated in an example often referred to as the Brock–Mirman economic growth model.\n\nYuri Matiyasevich was able to show that the Fibonacci numbers can be defined by a Diophantine equation, which led to his solving Hilbert's tenth problem.\n\nThe Fibonacci numbers are also an example of a complete sequence. This means that every positive integer can be written as a sum of Fibonacci numbers, where any one number is used once at most.\n\nMoreover, every positive integer can be written in a unique way as the sum of \"one or more\" distinct Fibonacci numbers in such a way that the sum does not include any two consecutive Fibonacci numbers. This is known as Zeckendorf's theorem, and a sum of Fibonacci numbers that satisfies these conditions is called a Zeckendorf representation. The Zeckendorf representation of a number can be used to derive its Fibonacci coding.\n\nFibonacci numbers are used by some pseudorandom number generators.\n\nThey are also used in planning poker, which is a step in estimating in software development projects that use the Scrum (software development) methodology.\n\nFibonacci numbers are used in a polyphase version of the merge sort algorithm in which an unsorted list is divided into two lists whose lengths correspond to sequential Fibonacci numbers – by dividing the list so that the two parts have lengths in the approximate proportion φ. A tape-drive implementation of the polyphase merge sort was described in \"The Art of Computer Programming\".\n\nFibonacci numbers arise in the analysis of the Fibonacci heap data structure.\n\nThe Fibonacci cube is an undirected graph with a Fibonacci number of nodes that has been proposed as a network topology for parallel computing.\n\nA one-dimensional optimization method, called the Fibonacci search technique, uses Fibonacci numbers.\n\nThe Fibonacci number series is used for optional lossy compression in the IFF 8SVX audio file format used on Amiga computers. The number series compands the original audio wave similar to logarithmic methods such as µ-law.\n\nSince the conversion factor 1.609344 for miles to kilometers is close to the golden ratio (denoted φ), the decomposition of distance in miles into a sum of Fibonacci numbers becomes nearly the kilometer sum when the Fibonacci numbers are replaced by their successors. This method amounts to a radix 2 number register in golden ratio base φ being shifted. To convert from kilometers to miles, shift the register down the Fibonacci sequence instead.\n\nFibonacci sequences appear in biological settings, in two consecutive Fibonacci numbers, such as branching in trees, arrangement of leaves on a stem, the fruitlets of a pineapple, the flowering of artichoke, an uncurling fern and the arrangement of a pine cone, and the family tree of honeybees. However, numerous poorly substantiated claims of Fibonacci numbers or golden sections in nature are found in popular sources, e.g., relating to the breeding of rabbits in Fibonacci's own unrealistic example, the seeds on a sunflower, the spirals of shells, and the curve of waves.\n\nPrzemysław Prusinkiewicz advanced the idea that real instances can in part be understood as the expression of certain algebraic constraints on free groups, specifically as certain Lindenmayer grammars.\nA model for the pattern of florets in the head of a sunflower was proposed by H. Vogel in 1979. This has the form\n\nwhere \"n\" is the index number of the floret and \"c\" is a constant scaling factor; the florets thus lie on Fermat's spiral. The divergence angle, approximately 137.51°, is the golden angle, dividing the circle in the golden ratio. Because this ratio is irrational, no floret has a neighbor at exactly the same angle from the center, so the florets pack efficiently. Because the rational approximations to the golden ratio are of the form \"F\"(\"j\"):\"F\"(\"j\" + 1), the nearest neighbors of floret number \"n\" are those at \"n\" ± \"F\"(\"j\") for some index \"j\", which depends on \"r\", the distance from the center. It is often said that sunflowers and similar arrangements have 55 spirals in one direction and 89 in the other (or some other pair of adjacent Fibonacci numbers), but this is true only of one range of radii, typically the outermost and thus most conspicuous.\n\nFibonacci numbers also appear in the pedigrees of idealized honeybees, according to the following rules:\n\n\nThus, a male bee always has one parent, and a female bee has two.\n\nIf one traces the pedigree of any male bee (1 bee), he has 1 parent (1 bee), 2 grandparents, 3 great-grandparents, 5 great-great-grandparents, and so on. This sequence of numbers of parents is the Fibonacci sequence. The number of ancestors at each level, \"F\", is the number of female ancestors, which is \"F\", plus the number of male ancestors, which is \"F\". This is under the unrealistic assumption that the ancestors at each level are otherwise unrelated.\n\nLuke Hutchison noticed that the number of possible ancestors on the X chromosome inheritance line at a given ancestral generation also follows the Fibonacci sequence. A male individual has an X chromosome, which he received from his mother, and a Y chromosome, which he received from his father. The male counts as the \"origin\" of his own X chromosome (formula_130), and at his parents' generation, his X chromosome came from a single parent (formula_131). The male's mother received one X chromosome from her mother (the son's maternal grandmother), and one from her father (the son's maternal grandfather), so two grandparents contributed to the male descendant's X chromosome (formula_132). The maternal grandfather received his X chromosome from his mother, and the maternal grandmother received X chromosomes from both of her parents, so three great-grandparents contributed to the male descendant's X chromosome (formula_133). Five great-great-grandparents contributed to the male descendant's X chromosome (formula_134), etc. (Note that this assumes that all ancestors of a given descendant are independent, but if any genealogy is traced far enough back in time, ancestors begin to appear on multiple lines of the genealogy, until eventually a population founder appears on all lines of the genealogy.)\n\nThe Fibonacci sequence has been generalized in many ways. These include:\n\n\n\n\n"}
{"id": "10923", "url": "https://en.wikipedia.org/wiki?curid=10923", "title": "Fontainebleau", "text": "Fontainebleau\n\nFontainebleau (; ) is a commune in the metropolitan area of Paris, France. It is located south-southeast of the centre of Paris. Fontainebleau is a sub-prefecture of the Seine-et-Marne department, and it is the seat of the \"arrondissement\" of Fontainebleau. The commune has the largest land area in the Île-de-France region; it is the only one to cover a larger area than Paris itself.\n\nFontainebleau, together with the neighbouring commune of Avon and three other smaller communes, form an urban area of 39,713 inhabitants (according to the 2001 census). This urban area is a satellite of Paris.\n\nFontainebleau is renowned for the large and scenic forest of Fontainebleau, a favourite weekend getaway for Parisians, as well as for the historic Château de Fontainebleau, which once belonged to the kings of France. It is also the home of INSEAD, one of the world's most elite business schools; of the \"École supérieure d'ingénieurs en informatique et génie des télécommunications\" (ESIGETEL), one of France's \"grandes écoles\"; and of a branch of the \"École nationale supérieure des mines de Paris\", the Paris School of Mines, also one of the elite \"grandes écoles\".\n\nInhabitants of Fontainebleau are called \"Bellifontains\".\n\nFontainebleau has been recorded in different Latinised forms, such as, \"Fons Bleaudi\", \"Fons Bliaudi\", \"Fons Blaadi\" in the 12th and 13th centuries, with \"Fontem blahaud\" being recorded in 1137. It became \"Fons Bellaqueus\" in the 17th century, which gave rise to the name of the inhabitants as \"Bellifontains\". The name originates as a medieval composite of two words: \"Fontaine–\" meaning spring, or fountainhead, followed by a person’s Germanic name \"Blizwald\".\n\nThis hamlet was endowed with a royal hunting lodge and a chapel by Louis VII in the middle of the twelfth century. A century later, Louis IX, also called Saint Louis, who held Fontainebleau in high esteem and referred to it as \"his wilderness\", had a country house and a hospital constructed there.\n\nPhilip the Fair was born there in 1268 and died there in 1314. In all, thirty-four sovereigns, from Louis VI, the Fat, (1081–1137) to Napoleon III (1808–1873), spent time at Fontainebleau.\n\nThe connection between the town of Fontainebleau and the French monarchy was reinforced with the transformation of the royal country house into a true royal palace, the Palace of Fontainebleau. This was accomplished by the great builder-king, Francis I (1494–1547), who, in the largest of his many construction projects, reconstructed, expanded, and transformed the royal château at Fontainebleau into a residence that became his favourite, as well as the residence of his mistress, Anne, duchess of Étampes.\n\nFrom the sixteenth to the eighteenth century, every monarch, from Francis I to Louis XV, made important renovations at the Palace of Fontainebleau, including demolitions, reconstructions, additions, and embellishments of various descriptions, all of which endowed it with a character that is a bit heterogeneous, but harmonious nonetheless.\n\nOn 18 October 1685, Louis XIV signed the \"Edict of Fontainebleau\" there. Also known as the \"Revocation of the Edict of Nantes\", this royal fiat reversed the permission granted to the Huguenots in 1598 to worship publicly in specified locations and hold certain other privileges. The result was that a large number of Protestants were forced to convert to the Catholic faith, killed, or forced into exile, mainly in the Low Countries, Prussia and in England.\n\nThe 1762 Treaty of Fontainebleau, a secret agreement between France and Spain concerning the Louisiana territory in North America, was concluded here. Also, preliminary negotiations, held before the 1763 Treaty of Paris was signed, ending the Seven Years' War, were at Fontainebleau.\n\nDuring the French Revolution, Fontainebleau was temporarily renamed Fontaine-la-Montagne, meaning \"Fountain by the Mountain\". (The mountain referred to is the series of rocky formations located in the forest of Fontainebleau.)\n\nOn 29 October 1807, Manuel Godoy, chancellor to the Spanish king, Charles IV and Napoleon signed the Treaty of Fontainebleau, which authorized the passage of French troops through Spanish territories so that they might invade Portugal.\n\nOn 20 June 1812, Pope Pius VII arrived at the château of Fontainebleau, after a secret transfer from Savona, accompanied by his personal physician, Balthazard Claraz. In poor health, the Pope was the prisoner of Napoleon, and he remained in his genteel prison at Fontainebleau for nineteen months. From June 1812 until 23 January 1814, the Pope never left his apartments.\n\nOn 20 April 1814, Napoleon Bonaparte, shortly before his first abdication, bid farewell to the Old Guard, the renowned \"grognards\" (gripers) who had served with him since his very first campaigns, in the \"White Horse Courtyard\" (la cour du Cheval Blanc) at the Palace of Fontainebleau. (The courtyard has since been renamed the \"Courtyard of Goodbyes\".) According to contemporary sources, the occasion was very moving. The 1814 Treaty of Fontainebleau stripped Napoleon of his powers (but not his title as Emperor of the French) and sent him into exile on Elba.\n\nUntil the 19th century, Fontainebleau was a village and a suburb of Avon. Later, it developed as an independent residential city.\n\nFor the 1924 Summer Olympics, the town played host to the riding portion of the modern pentathlon event. This event took place near a golf course.\n\nIn July and August 1946, the town hosted the Franco-Vietnamese Conference, intended to find a solution to the long-contested struggle for Vietnam’s independence from France, but the conference ended in failure.\n\nFontainebleau also hosted the general staff of the Allied Forces in Central Europe (Allied Forces Center or AFCENT) and the land forces command (LANDCENT); the air forces command (AIRCENT) was located nearby at Camp Guynemer. These facilities were in place from the inception of NATO until France’s partial withdrawal from NATO in 1967 when the United States returned those bases to French control. NATO moved AFCENT to Brunssum in the Netherlands and AIRCENT to Ramstein in West Germany. (Note that the Supreme Headquarters Allied Powers Europe, also known as SHAPE, was located at Rocquencourt, west of Paris, quite a distance from Fontainebleau).\n\nIn 2008, The men's World Championship of Real Tennis (Jeu de Paume) was held in the tennis court of the Chateau. The real tennis World Championship is the oldest in sport and Fontainebleau has one of only two active courts in France.\n\nFontainebleau is a popular tourist destination; each year, 300,000 people visit the palace and more than 13 million people visit the forest.\n\nThe forest of Fontainebleau surrounds the town and dozens of nearby villages. It is protected by France's \"Office National des Forêts\", and it is recognised as a French national park. It is managed in order that its wild plants and trees, such as the rare service tree of Fontainebleau, and its populations of birds, mammals, and butterflies, can be conserved. It is a former royal hunting park often visited by hikers and horse riders. The forest is also well regarded for bouldering and is particularly popular among climbers, as the biggest developed area of that kind in the world.\n\nThe Royal Château de Fontainebleau is a large palace where the kings of France took their ease. It is also the site where the French royal court, from 1528 onwards, entertained the body of new ideas that became known as the Renaissance.\nThe European (and historic) campus of the INSEAD business school is located at the edge of Fontainebleau, by the Lycee Francois Couperin. INSEAD students live in local accommodations around the Fontainebleau area, and especially in the surrounding towns.\n\nThe graves of G. I. Gurdjieff and Katherine Mansfield can be found in the cemetery at Avon.\n\nFontainebleau is served by two stations on the Transilien Paris–Lyon rail line: Fontainebleau–Avon and Thomery. Fontainebleau–Avon station, the station closest to the centre of Fontainebleau, is located near the dividing-line between the commune of Fontainebleau and the commune of Avon, on the Avon side of the border.\n\n\nFontainebleau is twinned with the following cities:\n\n\n\n"}
{"id": "10929", "url": "https://en.wikipedia.org/wiki?curid=10929", "title": "Fighter aircraft", "text": "Fighter aircraft\n\nA fighter aircraft is a military aircraft designed primarily for air-to-air combat against other aircraft, as opposed to bombers and attack aircraft, whose main mission is to attack ground targets. The hallmarks of a fighter are its speed, maneuverability, and small size relative to other combat aircraft.\n\nMany fighters have secondary ground-attack capabilities, and some are designed as dual-purpose fighter-bombers; often aircraft that do not fulfill the standard definition are called fighters. This may be for political or national security reasons, for advertising purposes, or other reasons.\n\nA fighter's main purpose is to establish air superiority over a battlefield. Since World War I, achieving and maintaining air superiority has been considered essential for victory in conventional warfare. The success or failure of a belligerent's efforts to gain air supremacy hinges on several factors including the skill of its pilots, the tactical soundness of its doctrine for deploying its fighters, and the numbers and performance of those fighters. Because of the importance of air superiority, since the dawn of aerial combat armed forces have constantly competed to develop technologically superior fighters and to deploy these fighters in greater numbers, and fielding a viable fighter fleet consumes a substantial proportion of the defense budgets of modern armed forces.\n\nThe word \"fighter\" did not become the official English-language term for such aircraft until after World War I. In the British Royal Flying Corps and Royal Air Force these aircraft were referred to as \"scouts\" into the early 1920s. The U.S. Army called their fighters \"pursuit\" aircraft from 1916 until the late 1940s. In most languages a fighter aircraft is known as a \"hunter\", or \"hunting aircraft\" (\"avion de chasse, Jagdflugzeuge, avión de caza\" etc.). Exceptions include Russian, where a fighter is an \"истребитель\" (pronounced \"istrebitel\"), meaning \"exterminator\", and Hebrew where it is \"matose krav\" (literally \"battle plane\").\n\nAs a part of military nomenclature, a letter is often assigned to various types of aircraft to indicate their use, along with a number to indicate the specific aircraft. The letters used to designate a fighter differ in various countries – in the English-speaking world, \"F\" is now used to indicate a fighter (e.g. Lockheed Martin F-35 Lightning II or Supermarine Spitfire F.22), though when the pursuit designation was used in the US, they were \"P\" types (e.g. Curtiss P-40 Warhawk). In Russia \"I\" was used (Polikarpov I-16), while the French continue to use \"C\" (Nieuport 17 C.1).\n\nAlthough the term \"fighter\" specifies aircraft designed to shoot down other aircraft, such designs are often also useful as multirole fighter-bombers, strike fighters, and sometimes lighter, fighter-sized tactical ground-attack aircraft. This has always been the case, for instance the Sopwith Camel and other \"fighting scouts\" of World War I performed a great deal of ground-attack work. In World War II, the USAAF and RAF often favored fighters over dedicated light bombers or dive bombers, and types such as the Republic P-47 Thunderbolt and Hawker Hurricane that were no longer competitive as aerial combat fighters were relegated to ground attack. Several aircraft, such as the F-111 and F-117, have received fighter designations though they had no fighter capability due to political or other reasons. The F-111B variant was originally intended for a fighter role with the U.S. Navy, but it was cancelled. This blurring follows the use of fighters from their earliest days for \"attack\" or \"strike\" operations against ground targets by means of strafing or dropping small bombs and incendiaries. Versatile multirole fighter-bombers such as the McDonnell Douglas F/A-18 Hornet are a less expensive option than having a range of specialized aircraft types.\n\nSome of the most expensive fighters such as the US Grumman F-14 Tomcat, McDonnell Douglas F-15 Eagle, Lockheed Martin F-22 Raptor and Russian Sukhoi Su-27 were employed as all-weather interceptors as well as air superiority fighter aircraft, while commonly developing air-to-ground roles late in their careers. An interceptor is generally an aircraft intended to target (or intercept) bombers and so often trades maneuverability for climb rate.\n\nFighters were developed in World War I to deny enemy aircraft and dirigibles the ability to gather information by reconnaissance over the battlefield. Early fighters were very small and lightly armed by later standards, and most were biplanes built with a wooden frame, covered with fabric, and with a maximum speed of about . As control of the airspace over armies became increasingly important all of the major powers developed fighters to support their military operations. Between the wars, wood was largely replaced in part of whole by metal tubing, and finally aluminium stressed skin structures (monologue) began to predominate.\n\nBy World War II, most fighters were all-metal monoplanes armed with batteries of machine guns or cannons and some were capable of speeds approaching . Most fighters up to this point had one engine, but a number of twin-engine fighters were built; however they were found to be outmatched against single-engine fighters and were relegated to other tasks, such as night fighters equipped with primitive radar sets.\nBy the end of the war, turbojet engines were replacing piston engines as the means of propulsion, further increasing aircraft speed. Since the weight of the engine was so much less than on piston engined fighters, having two engines was no longer a handicap and one or two were used, depending on requirements. This in turn required the development of ejection seats so the pilot could escape and G-suits to counter the much greater forces being applied to the pilot during maneuvers.\nIn the 1950s, radar was fitted to day fighters, since pilots could no longer see far enough ahead to prepare for any opposition. Since then, radar capabilities have grown enormously and are now the primary method of target acquisition. Wings were made thinner and swept back to reduce transonic drag, which required new manufacturing methods to obtain sufficient strength. Skins were no longer sheet metal riveted to a structure, but milled from large slabs of alloy. The sound barrier was broken, and after a few false starts due to required changes in controls, speeds quickly reached Mach 2—past which aircraft can't maneuver to avoid attack.\n\nAir-to-air missiles largely replaced guns and rockets in the early 1960s since both were believed unusable at the speeds being attained, however the Vietnam War showed that guns still had a role to play, and most fighters built since then are fitted with cannon (typically between 20 and 30 mm in caliber) in addition to missiles. Most modern combat aircraft can carry at least a pair of air-to-air missiles.\nIn the 1970s, turbofans replaced turbojets, improving fuel economy enough that the last piston engined support aircraft could be replaced with jets, making multi-role combat aircraft possible. Honeycomb structures began to replace milled structures, and the first composite components began to appear on components subjected to little stress.\n\nWith the steady improvements in computers, defensive systems have become increasingly efficient. To counter this, stealth technologies have been pursued by the United States, Russia, India and China. The first step was to find ways to reduce the aircraft's reflectivity to radar waves by burying the engines, eliminating sharp corners and diverting any reflections away from the radar sets of opposing forces. Various materials were found to absorb the energy from radar waves, and were incorporated into special finishes that have since found widespread application. Composite structures have become widespread, including major structural components, and have helped to counterbalance the steady increases in aircraft weight—most modern fighters are larger and heavier than World War II medium bombers.\n\nThe word \"fighter\" was first used to describe a two-seater aircraft with sufficient lift to carry a machine gun and its operator as well as the pilot. Some of the first such \"fighters\" belonged to the \"gunbus\" series of experimental gun carriers of the British Vickers company that culminated in the Vickers F.B.5 Gunbus of 1914. The main drawback of this type of aircraft was its lack of speed. Planners quickly realized that an aircraft intended to destroy its kind in the air had to be fast enough to catch its quarry.\n\nAnother type of military aircraft was to form the basis for an effective \"fighter\" in the modern sense of the word. It was based on the small fast aircraft developed before the war for such air races as the Gordon Bennett Cup and Schneider Trophy. The military scout airplane was not expected to carry serious armament, but rather to rely on its speed to reach the scout or reconnoiter location and return quickly to report – essentially an aerial horse. British scout aircraft, in this sense, included the Sopwith Tabloid and Bristol Scout. French equivalents included the Morane-Saulnier N.\nSoon after the commencement of the war, pilots armed themselves with pistols, carbines, grenades, and an assortment of improvised weapons. Many of these proved ineffective as the pilot had to fly his airplane while attempting to aim a handheld weapon and make a difficult deflection shot. The first step in finding a real solution was to mount the weapon on the aircraft, but the propeller remained a problem since the best direction to shoot is straight ahead. Numerous solutions were tried. A second crew member behind the pilot could aim and fire a swivel-mounted machine gun at enemy airplanes; however, this limited the area of coverage chiefly to the rear hemisphere, and effective coordination of the pilot's maneuvering with the gunner's aiming was difficult. This option was chiefly employed as a defensive measure on two-seater reconnaissance aircraft from 1915 on. Both the SPAD S.A and the Royal Aircraft Factory B.E.9 added a second crewman ahead of the engine in a pod but this was both hazardous to the second crewman and limited performance. The Sopwith L.R.T.Tr. similarly added a pod on the top wing with no better luck.\nAn alternative was to build a \"pusher\" scout such as the Airco DH.2, with the propeller mounted behind the pilot. The main drawback was that the high drag of a pusher type's tail structure made it slower than a similar \"tractor\" aircraft.\n\nA better solution for a single seat scout was to mount the machine gun (rifles and pistols having been dispensed with) to fire forwards but outside the propeller arc. Wing guns were tried but the unreliable weapons available required frequent clearing of jammed rounds and misfires and remained impractical until after the war. Mounting the machine gun over the top wing worked well and was used long after the ideal solution was found. The Nieuport 11 of 1916 and Royal Aircraft Factory S.E.5 of 1918 both used this system with considerable success; however, this placement made aiming difficult and the location made it difficult for a pilot to both maneuver and have access to the gun's breech. The British Foster mounting was specifically designed for this kind of application, fitted with the Lewis Machine gun, which due to its design was unsuitable for synchronizing.\nThe need to arm a tractor scout with a forward-firing gun whose bullets passed through the propeller arc was evident even before the outbreak of war and inventors in both France and Germany devised mechanisms that could time the firing of the individual rounds to avoid hitting the propeller blades. Franz Schneider, a Swiss engineer, had patented such a device in Germany in 1913, but his original work was not followed up. French aircraft designer Raymond Saulnier patented a practical device in April 1914, but trials were unsuccessful because of the propensity of the machine gun employed to hang fire due to unreliable ammunition.\n\nIn December 1914, French aviator Roland Garros asked Saulnier to install his synchronization gear on Garros' Morane-Saulnier Type L. Unfortunately the gas-operated Hotchkiss machine gun he was provided had an erratic rate of fire and it was impossible to synchronize it with a spinning propeller. As an interim measure, the propeller blades were armored and fitted with metal wedges to protect the pilot from ricochets. Garros' modified monoplane was first flown in March 1915 and he began combat operations soon thereafter. Garros scored three victories in three weeks before he himself was downed on 18 April and his airplane, along with its synchronization gear and propeller was captured by the Germans.\n\nMeanwhile, the synchronization gear (called the \"Stangensteuerung\" in German, for \"pushrod control system\") devised by the engineers of Anthony Fokker's firm was the first system to see production contracts, and would make the Fokker \"Eindecker\" monoplane a feared name over the Western Front, despite its being an adaptation of an obsolete pre-war French Morane-Saulnier racing airplane, with a mediocre performance and poor flight characteristics. The first victory for the \"Eindecker\" came on 1 July 1915, when \"Leutnant\" Kurt Wintgens, flying with the \"Feldflieger Abteilung 6\" unit on the Western Front, forced down a Morane-Saulnier Type L two-seat \"parasol\" monoplane just east of Luneville. Wintgens' aircraft, one of the five Fokker M.5K/MG production prototype examples of the \"Eindecker\", was armed with a synchronized, air-cooled aviation version of the Parabellum MG14 machine gun.\nThe success of the \"Eindecker\" kicked off a competitive cycle of improvement among the combatants, both sides striving to build ever more capable single-seat fighters. The Albatros D.I and Sopwith Pup of 1916 set the classic pattern followed by fighters for about twenty years. Most were biplanes and only rarely monoplanes or triplanes. The strong box structure of the biplane provided a rigid wing that allowed the accurate lateral control essential for dogfighting. They had a single operator, who flew the aircraft and also controlled its armament. They were armed with one or two Maxim or Vickers machine guns, which were easier to synchronize than other types, firing through the propeller arc. Gun breeches were directly in front of the pilot, with obvious implications in case of accidents, but jams could be cleared in flight, while aiming was simplified.\n\nThe use of metal aircraft structures was pioneered before World War I by Breguet but would find its biggest proponent in Anthony Fokker, who used chrome-molybdenum steel tubing for the fuselage structure of all his fighter designs, while the innovative German engineer Hugo Junkers developed two all-metal, single-seat fighter monoplane designs with cantilever wings: the strictly experimental Junkers J 2 private-venture aircraft, made with steel, and some forty examples of the Junkers D.I, made with corrugated duralumin, all based on his experience in creating the pioneering Junkers J 1 all-metal airframe technology demonstration aircraft of late 1915. While Fokker would pursue steel tube fuselages with wooden wings until the late 1930s, and Junkers would focus on corrugated sheet metal, Dornier was the first to build a fighter (The Dornier-Zeppelin D.I) made with pre-stressed sheet aluminium and having cantelevered wings, a form that would replace all others in the 1930s.\n\nAs collective combat experience grew, the more successful pilots such as Oswald Boelcke, Max Immelmann, and Edward Mannock developed innovative tactical formations and maneuvers to enhance their air units' combat effectiveness.\nAllied and – before 1918 – German pilots of World War I were not equipped with parachutes, so in-flight fires or structural failure were often fatal. Parachutes were well-developed by 1918 having previously been used by balloonists, and were adopted by the German flying services during the course of that year. The well known and feared Manfred von Richthofen \"Red Baron\" was wearing one when he was killed, but the allied command continued to oppose their use on various grounds.\n\nIn April 1917, during a brief period of German aerial supremacy a British pilot's average life expectancy was 93 flying hours, or about three weeks of active service. More than 50,000 airmen from both sides died during the war.\n\nFighter development stagnated between the wars, especially in the United States and the United Kingdom, where budgets were small. In France, Italy and Russia, where large budgets continued to allow major development, both monoplanes and all metal structures were common. By the end of the 1920s, however, those countries overspent themselves and were overtaken in the 1930s by those powers that hadn't been spending heavily, namely the British, the Americans and the Germans.\nGiven limited defense budgets, air forces tended to be conservative in their aircraft purchases, and biplanes remained popular with pilots because of their agility, and remained in service long after they had ceased to be competitive. Designs such as the Gloster Gladiator, Fiat CR.42, and Polikarpov I-15 were common even in the late 1930s, and many were still in service as late as 1942. Up until the mid-1930s, the majority of fighters in the US, the UK, Italy and Russia remained fabric-covered biplanes.\n\nFighter armament eventually began to be mounted inside the wings, outside the arc of the propeller, though most designs retained two synchronized machine guns directly ahead of the pilot, where they were more accurate (that being the strongest part of the structure, reducing the vibration to which the guns were subjected to). Shooting with this traditional arrangement was also easier for the further reason that the guns shot directly ahead in the direction of the aircraft's flight, up to the limit of the guns range; unlike wing-mounted guns which to be effective required to be harmonised, that is, preset to shoot at an angle by ground crews so that their bullets would converge on a target area a set distance ahead of the fighter. Rifle-caliber .30 and .303 in (7.62 mm) caliber guns remained the norm, with larger weapons either being too heavy and cumbersome or deemed unnecessary against such lightly built aircraft. It was not considered unreasonable to use World War I-style armament to counter enemy fighters as there was insufficient air-to-air combat during most of the period to disprove this notion.\n\nThe rotary engine, popular during World War I, quickly disappeared, its development having reached the point where rotational forces prevented more fuel and air from being delivered to the cylinders, which limited horsepower. They were replaced chiefly by the stationary radial engine though major advances led to inline engines, which gained ground with several exceptional engines—including the V-12 Curtiss D-12. Aircraft engines increased in power several-fold over the period, going from a typical in the 900-kg Fokker D.VII of 1918 to in the 2,500-kg Curtiss P-36 of 1936. The debate between the sleek in-line engines versus the more reliable radial models continued, with naval air forces preferring the radial engines, and land-based forces often choosing in-line units. Radial designs did not require a separate (and vulnerable) cooling system, but had increased drag. In-line engines often had a better power-to-weight ratio, but there were radial engines that kept working even after having suffered significant battle damage.\nSome air forces experimented with \"heavy fighters\" (called \"destroyers\" by the Germans). These were larger, usually twin-engined aircraft, sometimes adaptations of light or medium bomber types. Such designs typically had greater internal fuel capacity (thus longer range) and heavier armament than their single-engine counterparts. In combat, they proved vulnerable to more agile single-engine fighters.\n\nThe primary driver of fighter innovation, right up to the period of rapid re-armament in the late 1930s, were not military budgets, but civilian aircraft racing. Aircraft designed for these races introduced innovations like streamlining and more powerful engines that would find their way into the fighters of World War II. The most significant of these was the Schneider Trophy races, where competition grew so fierce, only national governments could afford to enter.\nAt the very end of the inter-war period in Europe came the Spanish Civil War. This was just the opportunity the German \"Luftwaffe\", Italian \"Regia Aeronautica\", and the Soviet Union's Red Air Force needed to test their latest aircraft. Each party sent numerous aircraft types to support their sides in the conflict. In the dogfights over Spain, the latest Messerschmitt Bf 109 fighters did well, as did the Soviet Polikarpov I-16. The German design had considerably more room for development however and the lessons learned led to greatly improved models in World War II. The Russians, whose side lost, failed to keep up and despite newer models coming into service, I-16s were outfought by the improved Bf 109s in World War II, while remaining the most common Soviet front-line fighter into 1942. For their part, the Italians developed several monoplanes such as the Fiat G.50, but being short on funds, were forced to continue operating obsolete Fiat CR.42 biplanes.\n\nFrom the early 1930s the Japanese had been at war against both the Chinese Nationalists and the Russians in China, and used the experience to improve both training and aircraft, replacing biplanes with modern cantilever monoplanes and creating a cadre of exceptional pilots for use in the Pacific War. In the United Kingdom, at the behest of Neville Chamberlain, (more famous for his 'peace in our time' speech) the entire British aviation industry was retooled, allowing it to change quickly from fabric covered metal framed biplanes to cantilever stressed skin monoplanes in time for the war with Germany.\n\nThe period of improving the same biplane design over and over was now coming to an end, and the Hawker Hurricane and Supermarine Spitfire finally started to supplant the Gloster Gladiator and Hawker Fury biplanes but many of the former remained in front-line service well past the start of World War II. While not a combatant themselves in Spain, they absorbed many of the lessons learned in time to use them.\n\nThe Spanish Civil War also provided an opportunity for updating fighter tactics. One of the innovations to result from the aerial warfare experience this conflict provided was the development of the \"finger-four\" formation by the German pilot Werner Mölders. Each fighter squadron (German: \"Staffel\") was divided into several flights (\"Schwärme\") of four aircraft. Each \"Schwarm\" was divided into two \"Rotten\", which was a pair of aircraft. Each \"Rotte\" was composed of a leader and a wingman. This flexible formation allowed the pilots to maintain greater situational awareness, and the two \"Rotten\" could split up at any time and attack on their own. The finger-four would become widely adopted as the fundamental tactical formation over the course of World War.\n\nWorld War II featured fighter combat on a larger scale than any other conflict to date. German Field Marshal Erwin Rommel noted the effect of airpower: \"Anyone who has to fight, even with the most modern weapons, against an enemy in complete command of the air, fights like a savage against modern European troops, under the same handicaps and with the same chances of success.\" Throughout the war, fighters performed their conventional role in establishing air superiority through combat with other fighters and through bomber interception, and also often performed roles such as tactical air support and reconnaissance.\n\nFighter design varied widely among combatants. The Japanese and Italians favored lightly armed and armored but highly maneuverable designs such as the Japanese Nakajima Ki-27, Nakajima Ki-43 and Mitsubishi A6M Zero and Italy's Fiat G.50 and Macchi MC.200. In contrast, designers in the United Kingdom, Germany, the Soviet Union, and the United States believed that the increased speed of fighter aircraft would create \"g\"-forces unbearable to pilots who attempted maneuvering dogfights typical of the First World War, and their fighters were instead optimized for speed and firepower. In practice, while light, highly maneuverable aircraft did possess some advantages in fighter-versus-fighter combat, those could usually be overcome by sound tactical doctrine, and the design approach of the Italians and Japanese made their fighters ill-suited as interceptors or attack aircraft.\n\nDuring the invasion of Poland and the Battle of France, Luftwaffe fighters—primarily the Messerschmitt Bf 109—held air superiority, and the Luftwaffe played a major role in German victories in these campaigns. During the Battle of Britain, however, British Hurricanes and Spitfires proved roughly equal to Luftwaffe fighters. Additionally Britain's radar-based Dowding system directing fighters onto German attacks and the advantages of fighting above Britain's home territory allowed the RAF to deny Germany air superiority, saving the UK from possible German invasion and dealing the Axis a major defeat early in the Second World War.\n\nOn the Eastern Front, Soviet fighter forces were overwhelmed during the opening phases of Operation Barbarossa. This was a result of the tactical surprise at the outset of the campaign, the leadership vacuum within the Soviet military left by the Great Purge, and the general inferiority of Soviet designs at the time, such as the obsolescent I-15 biplane and the I-16. More modern Soviet designs, including the MiG-3, LaGG-3 and Yak-1, had not yet arrived in numbers and in any case were still inferior to the Messerschmitt Bf 109. As a result, during the early months of these campaigns, Axis air forces destroyed large numbers of Red Air Force aircraft on the ground and in one-sided dogfights.\n\nIn the later stages on the Eastern Front, Soviet training and leadership improved, as did their equipment. Late-war Soviet designs such as the Yakovlev Yak-3 and Lavochkin La-7 had performance comparable to the German Bf-109 and Focke-Wulf Fw 190. Also, significant numbers of British, and later U.S., fighter aircraft were supplied to aid the Soviet war effort as part of Lend-Lease, with the Bell P-39 Airacobra proving particularly effective in the lower-altitude combat typical of the Eastern Front. The Soviets were also helped indirectly by the American and British bombing campaigns, which forced the Luftwaffe to shift many of its fighters away from the Eastern Front in defense against these raids. The Soviets increasingly were able to challenge the Luftwaffe, and while the Luftwaffe maintained a qualitative edge over the Red Air Force for much of the war, the increasing numbers and efficacy of the Soviet Air Force were critical to the Red Army's efforts at turning back and eventually annihilating the Wehrmacht.\n\nMeanwhile, air combat on the Western Front had a much different character. Much of this combat was centered around the strategic bombing campaigns of the RAF and the USAAF against German industry intended to wear down the Luftwaffe. Axis fighter aircraft focused on defending against Allied bombers while Allied fighters' main role was as bomber escorts. The RAF raided German cities at night, and both sides developed radar-equipped night fighters for these battles. The Americans, in contrast, flew daylight bombing raids into Germany. Unescorted Consolidated B-24 Liberators and Boeing B-17 Flying Fortress bombers, however, proved unable to fend off German interceptors (primarily Bf-109s and FW-190s). With the later arrival of long range fighters, particularly the North American P-51 Mustang, American fighters were able to escort far into Germany on daylight raids and established control of the skies over Western Europe.\n\nBy the time of Operation Overlord in June 1944, the Allies had gained near complete air superiority over the Western Front. This cleared the way both for intensified strategic bombing of German cities and industries, and for the tactical bombing of battlefield targets. With the Luftwaffe largely cleared from the skies, Allied fighters increasingly served as attack aircraft.\n\nAllied fighters, by gaining air superiority over the European battlefield, played a crucial role in the eventual defeat of the Axis, which Reichmarshal Hermann Göring, commander of the German \"Luftwaffe\" summed up when he said: \"When I saw Mustangs over Berlin, I knew the jig was up.\"\n\nMajor air combat during the war in the Pacific began with the entry of the Western Allies following Japan's attack against Pearl Harbor. The Imperial Japanese Navy Air Service primarily operated the Mitsubishi A6M Zero, and the Imperial Japanese Army Air Service flew the Nakajima Ki-27 and the Nakajima Ki-43, initially enjoying great success, as these fighters generally had better range, maneuverability, speed and climb rates than their Allied counterparts. Additionally, Japanese pilots had received excellent training and many were combat veterans from Japan's campaigns in China. They quickly gained air superiority over the Allies, who at this stage of the war were often disorganized, under-trained and poorly equipped, and Japanese air power contributed significantly to their successes in the Philippines, Malaysia and Singapore, the Dutch East Indies and Burma.\nBy mid-1942, the Allies began to regroup and while some Allied aircraft such as the Brewster Buffalo and the P-39 were hopelessly outclassed by fighters like Japan's Zero, others such as the Army's P-40 and the Navy's Wildcat possessed attributes such as superior firepower, ruggedness and dive speed, and the Allies soon developed tactics (such as the Thach weave) to take advantage of these strengths. These changes soon paid dividends, as the Allied ability to deny Japan air superiority was critical to their victories at Coral Sea, Midway, Guadalcanal and New Guinea. In China, the Flying Tigers also used the same tactics with some success, although they were unable to stem the tide of Japanese advances there.\nBy 1943, the Allies began to gain the upper hand in the Pacific Campaign's air campaigns. Several factors contributed to this shift. First, second-generation Allied fighters such as the Hellcat and the P-38, and later the Corsair, the P-47 and the P-51, began arriving in numbers. These fighters outperformed Japanese fighters in all respects except maneuverability. Other problems with Japan's fighter aircraft also became apparent as the war progressed, such as their lack of armor and light armament, which made them inadequate as bomber interceptors or ground-attack planes – roles Allied fighters excelled at. Most importantly, Japan's training program failed to provide enough well-trained pilots to replace losses. In contrast, the Allies improved both the quantity and quality of pilots graduating from their training programs.\nBy mid-1944, Allied fighters had gained air superiority throughout the theater, which would not be contested again during the war. The extent of Allied quantitative and qualitative superiority by this point in the war was demonstrated during the Battle of the Philippine Sea, a lopsided Allied victory in which Japanese fliers were downed in such numbers and with such ease that American fighter pilots likened it to a great turkey shoot.\nLate in the war, Japan did begin to produce new fighters such as the Nakajima Ki-84 and the Kawanishi N1K to replace the venerable Zero, but these were produced only in small numbers, and in any case by that time Japan lacked trained pilots or sufficient fuel to mount a sustained challenge to Allied fighters. During the closing stages of the war, Japan's fighter arm could not seriously challenge raids over Japan by American B-29s, and was largely relegated to Kamikaze tactics.\n\nFighter technology advanced rapidly during the Second World War. Piston-engines, which powered the vast majority of World War II fighters, grew more powerful: at the beginning of the war fighters typically had engines producing between and , while by the end of the war many could produce over . For example, the Spitfire, one of the few fighters in continuous production throughout the war, was in 1939 powered by a Merlin II, while variants produced in 1945 were equipped with the Griffon 61. Nevertheless, these fighters could only achieve modest increases in top speed due to problems of compressibility created as aircraft and their propellers approached the sound barrier, and it was apparent that propeller-driven aircraft were approaching the limits of their performance. German jet and rocket-powered fighters entered combat in 1944, too late to impact the war's outcome. The same year the Allies' only operational jet fighter, the Gloster Meteor, also entered service.\nWorld War II fighters also increasingly featured monocoque construction, which improved their aerodynamic efficiency while adding structural strength. Laminar flow wings, which improved high speed performance, also came into use on fighters such as the P-51, while the Messerschmitt Me 262 and the Messerschmitt Me 163 featured swept wings that dramatically reduced drag at high subsonic speeds.\nArmament also advanced during the war. The rifle-caliber machine guns that were common on prewar fighters could not easily down the more rugged warplanes of the era. Air forces began to replace or supplement them with cannons, which fired explosive shells that could blast a hole in an enemy aircraft – rather than relying on kinetic energy from a solid bullet striking a critical component of the aircraft, such as a fuel line or control cable, or the pilot. Cannons could bring down even heavy bombers with just a few hits, but their slower rate of fire made it difficult to hit fast-moving fighters in a dogfight. Eventually, most fighters mounted cannons, sometimes in combination with machine guns.\nThe British epitomized this shift. Their standard early war fighters mounted eight calibre machine guns, but by mid-war they often featured a combination of machine guns and 20 mm cannons, and late in the war often only cannons. The Americans, in contrast, had problems producing a native cannon design, so instead placed multiple .50 caliber (12.7 mm) heavy machine guns on their fighters. Fighters were also increasingly fitted with bomb racks and air-to-surface ordnance such as bombs or rockets beneath their wings, and pressed into close air support roles as fighter-bombers. Although they carried less ordnance than light and medium bombers, and generally had a shorter range, they were cheaper to produce and maintain and their maneuverability made it easier for them to hit moving targets such as motorized vehicles. Moreover, if they encountered enemy fighters, their ordnance (which reduced lift and increased drag and therefore decreased performance) could be jettisoned and they could engage the enemy fighters, which eliminated the need for the fighter escorts that bombers required. Heavily armed and sturdily constructed fighters such as Germany's Focke-Wulf Fw 190, Britain's Hawker Typhoon and Hawker Tempest, and America's P-40, Corsair, P-47 and P-38 all excelled as fighter-bombers, and since the Second World War ground attack has been an important secondary capability of many fighters.\nWorld War II also saw the first use of airborne radar on fighters. The primary purpose of these radars was to help night fighters locate enemy bombers and fighters. Because of the bulkiness of these radar sets, they could not be carried on conventional single-engined fighters and instead were typically retrofitted to larger heavy fighters or light bombers such as Germany's Messerschmitt Bf 110 and Junkers Ju 88, Britain's Mosquito and Beaufighter, and America's A-20, which then served as night fighters. The Northrop P-61 Black Widow, a purpose-built night fighter, was the only fighter of the war that incorporated radar into its original design. Britain and America cooperated closely in the development of airborne radar, and Germany's radar technology generally lagged slightly behind Anglo-American efforts, while other combatants developed few radar-equipped fighters.\n\nSeveral prototype fighter programs begun early in 1945 continued on after the war and led to advanced piston-engine fighters that entered production and operational service in 1946. A typical example is the Lavochkin La-9 'Fritz', which was an evolution of the successful wartime Lavochkin La-7 'Fin'. Working through a series of prototypes, the La-120, La-126 and La-130, the Lavochkin design bureau sought to replace the La-7's wooden airframe with a metal one, as well as fit a laminar-flow wing to improve maneuver performance, and increased armament. The La-9 entered service in August 1946 and was produced until 1948; it also served as the basis for the development of a long-range escort fighter, the La-11 'Fang', of which nearly 1200 were produced 1947–1951. Over the course of the Korean War, however, it became obvious that the day of the piston-engined fighter was coming to a close and that the future would lie with the jet fighter.\nThis period also witnessed experimentation with jet-assisted piston engine aircraft. La-9 derivatives included examples fitted with two underwing auxiliary pulsejet engines (the La-9RD) and a similarly mounted pair of auxiliary ramjet engines (the La-138); however, neither of these entered service. One that did enter service – with the U.S. Navy in March 1945 – was the Ryan FR-1 Fireball; production was halted with the war's end on VJ-Day, with only 66 having been delivered, and the type was withdrawn from service in 1947. The USAAF had ordered its first 13 mixed turboprop-turbojet-powered pre-production prototypes of the Consolidated Vultee XP-81 fighter, but this program was also canceled by VJ Day, with 80% of the engineering work completed.\n\nThe first rocket-powered aircraft was the Lippisch Ente, which made a successful maiden flight in March 1928. The only pure rocket aircraft ever mass-produced was the Messerschmitt Me 163B \"Komet\" in 1944, one of several German World War II projects aimed at developing high speed, point-defense aircraft. Later variants of the Me 262 (C-1a and C-2b) were also fitted with \"mixed-power\" jet/rocket powerplants, while earlier models were fitted with rocket boosters, but were not mass-produced with these modifications.\n\nThe USSR experimented with a rocket-powered interceptor in the years immediately following World War II, the Mikoyan-Gurevich I-270. Only two were built.\n\nIn the 1950s, the British developed mixed-power jet designs employing both rocket and jet engines to cover the performance gap that existed in turbojet designs. The rocket was the main engine for delivering the speed and height required for high-speed interception of high-level bombers and the turbojet gave increased fuel economy in other parts of flight, most notably to ensure the aircraft was able to make a powered landing rather than risking an unpredictable gliding return. The Saunders-Roe SR.53 was a successful design, and was planned for production when economics forced the British to curtail most aircraft programs in the late 1950s. Furthermore, rapid advancements in jet engine technology rendered mixed-power aircraft designs like Saunders-Roe's SR.53 (and the following SR.177) obsolete. The American Republic XF-91 Thunderceptor (the first U.S. fighter to exceed Mach 1 in level flight) met a similar fate for the same reason, and no hybrid rocket-and-jet-engine fighter design has ever been placed into service. The only operational implementation of mixed propulsion was Rocket-Assisted Take Off (RATO), a system rarely used in fighters, such as with the zero-length launch, RATO-based takeoff scheme from special launch platforms, tested out by both the United States and the Soviet Union, and made obsolete with advancements in surface-to-air missile technology.\n\nIt has become common in the aviation community to classify jet fighters by \"generations\" for historical purposes. There are no official definitions of these generations; rather, they represent the notion that there are stages in the development of fighter design approaches, performance capabilities, and technological evolution. Also other authors have packed the fighters into different generations. For example, Richard P. Hallion of the Secretary of the Air Force's Action Group classified the F-16 as a sixth generation jet fighter.\n\nThe timeframes associated with each generation are inexact and are only indicative of the period during which their design philosophies and technology employment enjoyed a prevailing influence on fighter design and development. These timeframes also encompass the peak period of service entry for such aircraft.\n\nThe first generation of jet fighters comprised the initial, subsonic jet fighter designs introduced late in World War II and in the early post-war period. They differed little from their piston-engined counterparts in appearance, and many employed unswept wings. Guns and cannon remained the principal armament. The need to obtain a decisive advantage in maximum speed pushed the development of turbojet-powered aircraft forward. Top speeds for fighters rose steadily throughout World War II as more powerful piston engines were developed, and was approaching transonic flight speeds where the efficiency of propellers drops off, making further speed increases nearly impossible.\nThe first jets were developed during World War II and saw combat in the last two years of the war. Messerschmitt developed the first operational jet fighter, the Me 262A, primarily serving with JG 7, the world's first jet fighter wing. It was considerably faster than contemporary piston-driven aircraft, and in the hands of a competent pilot, was quite difficult for Allied pilots to defeat. The design was never deployed in numbers sufficient to stop the Allied air campaign, and a combination of fuel shortages, pilot losses, and technical difficulties with the engines kept the number of sorties low. Nevertheless, the Me 262 indicated the obsolescence of piston-driven aircraft. Spurred by reports of the German jets, Britain's Gloster Meteor entered production soon after and the two entered service around the same time in 1944. Meteors were commonly used to intercept the V-1 flying bomb, as they were faster than available piston-engined fighters at the low altitudes the flying bombs were flying. Nearer the end of World War II, the first military jet-powered light fighter design, the Heinkel He 162A \"Spatz\" (sparrow), was intended to be a simple jet fighter for German home defense, with a few examples seeing squadron service with JG 1 by April 1945. By the end of the war almost all work on piston-powered fighters had ended. A few designs combining piston and jet engines for propulsion – such as the Ryan FR Fireball – saw brief use, but by the end of the 1940s virtually all new fighters were jet-powered.\nDespite their advantages, the early jet fighters were far from perfect. The operational lifespan of turbines were very short and engines were temperamental, while power could be adjusted only slowly and acceleration was poor (even if top speed was higher) compared to the final generation of piston fighters. Many squadrons of piston-engined fighters were retained until the early to mid-1950s, even in the air forces of the major powers (though the types retained were the best of the World War II designs). Innovations including ejection seats, air brakes and all-moving tailplanes became widespread in this period.\n\nThe Americans began using jet fighters operationally post-war, the wartime Bell P-59 having proven itself a failure. The Lockheed P-80 Shooting Star (soon re-designated F-80) was less elegant than the swept-wing Me 262, but had a cruise speed () as high as the maximum speed attainable by many piston-engined fighters. The British designed several new jets, including the distinctive single-engined twin boom de Havilland Vampire which was sold to the air forces of many nations.\n\nThe British transferred the technology of the Rolls-Royce Nene jet engine to the Soviets, who soon put it to use in their advanced Mikoyan-Gurevich MiG-15 fighter, which used fully swept wings that allowed flying closer to the speed of sound than straight-winged designs such as the F-80. Its top speed of proved quite a shock to the American F-80 pilots who encountered them in the Korean War, along with their armament of two 23 mm cannons and a single 37 mm cannon. Nevertheless, in the first jet-versus-jet dogfight, which occurred during the Korean War on 8 November 1950, an F-80 shot down two North Korean MiG-15s.\nThe Americans responded by rushing their own swept-wing fighter - the North American F-86 Sabre - into battle against the MiGs, which had similar transsonic performance. The two aircraft had different strengths and weaknesses, but were similar enough that victory could go either way. While the Sabres were focused primarily on downing MiGs and scored favourably against those flown by the poorly trained North Koreans, the MiGs in turn decimated US bomber formations and forced the withdrawal of numerous American types from operational service.\n\nThe world's navies also transitioned to jets during this period, despite the need for catapult-launching of the new aircraft. The Grumman F9F Panther was adopted by the U.S. Navy as their primary jet fighter in the Korean War period, and it was one of the first jet fighters to employ an afterburner. The de Havilland Sea Vampire was the Royal Navy's first jet fighter. Radar was used on specialized night fighters such as the Douglas F3D Skyknight, which also downed MiGs over Korea, and later fitted to the McDonnell F2H Banshee and swept wing Vought F7U Cutlass and McDonnell F3H Demon as all-weather / night fighters. Early versions of Infra-red (IR) air-to-air missiles (AAMs) such as the AIM-9 Sidewinder and radar guided missiles such as the AIM-7 Sparrow whose descendants are still in use, were first introduced on swept wing subsonic Demon and Cutlass naval fighters.\n\nThe development of second-generation fighters was shaped by technological breakthroughs, lessons learned from the aerial battles of the Korean War, and a focus on conducting operations in a nuclear warfare environment. Technological advances in aerodynamics, propulsion and aerospace building materials (primarily aluminium alloys) permitted designers to experiment with aeronautical innovations, such as swept wings, delta wings, and area-ruled fuselages. Widespread use of afterburning turbojet engines made these the first production aircraft to break the sound barrier, and the ability to sustain supersonic speeds in level flight became a common capability amongst fighters of this generation.\nFighter designs also took advantage of new electronics technologies that made effective radars small enough to carry aboard smaller aircraft. Onboard radars permitted detection of enemy aircraft beyond visual range, thereby improving the handoff of targets by longer-ranged ground-based warning and tracking radars. Similarly, advances in guided missile development allowed air-to-air missiles to begin supplementing the gun as the primary offensive weapon for the first time in fighter history. During this period, passive-homing infrared-guided (IR) missiles became commonplace, but early IR missile sensors had poor sensitivity and a very narrow field of view (typically no more than 30°), which limited their effective use to only close-range, tail-chase engagements. Radar-guided (RF) missiles were introduced as well, but early examples proved unreliable. These semi-active radar homing (SARH) missiles could track and intercept an enemy aircraft \"painted\" by the launching aircraft's onboard radar. Medium- and long-range RF air-to-air missiles promised to open up a new dimension of \"beyond-visual-range\" (BVR) combat, and much effort was placed in further development of this technology.\nThe prospect of a potential third world war featuring large mechanized armies and nuclear weapon strikes led to a degree of specialization along two design approaches: interceptors, such as the English Electric Lightning and Mikoyan-Gurevich MiG-21F; and fighter-bombers, such as the Republic F-105 Thunderchief and the Sukhoi Su-7B. Dogfighting, per se, was de-emphasized in both cases. The interceptor was an outgrowth of the vision that guided missiles would completely replace guns and combat would take place at beyond visual ranges. As a result, interceptors were designed with a large missile payload and a powerful radar, sacrificing agility in favor of high speed, altitude ceiling and rate of climb. With a primary air defense role, emphasis was placed on the ability to intercept strategic bombers flying at high altitudes. Specialized point-defense interceptors often had limited range and little, if any, ground-attack capabilities. Fighter-bombers could swing, between air superiority and ground-attack roles, and were often designed for a high-speed, low-altitude dash to deliver their ordnance. Television- and IR-guided air-to-surface missiles were introduced to augment traditional gravity bombs, and some were also equipped to deliver a nuclear bomb.\n\nThe third generation witnessed continued maturation of second-generation innovations, but it is most marked by renewed emphases on maneuverability and traditional ground-attack capabilities. Over the course of the 1960s, increasing combat experience with guided missiles demonstrated that combat would devolve into close-in dogfights. Analog avionics began to appear, replacing older \"steam-gauge\" cockpit instrumentation. Enhancements to the aerodynamic performance of third-generation fighters included flight control surfaces such as canards, powered slats, and blown flaps. A number of technologies would be tried for Vertical/Short Takeoff and Landing, but thrust vectoring would be successful on the Harrier.\n\nGrowth in air combat capability focused on the introduction of improved air-to-air missiles, radar systems, and other avionics. While guns remained standard equipment (early models of F-4 being a notable exception), air-to-air missiles became the primary weapons for air superiority fighters, which employed more sophisticated radars and medium-range RF AAMs to achieve greater \"stand-off\" ranges, however, kill probabilities proved unexpectedly low for RF missiles due to poor reliability and improved electronic countermeasures (ECM) for spoofing radar seekers. Infrared-homing AAMs saw their fields of view expand to 45°, which strengthened their tactical usability. Nevertheless, the low dogfight loss-exchange ratios experienced by American fighters in the skies over Vietnam led the U.S. Navy to establish its famous \"TOPGUN\" fighter weapons school, which provided a graduate-level curriculum to train fleet fighter pilots in advanced Air Combat Maneuvering (ACM) and Dissimilar Air Combat Training (DACT) tactics and techniques.\nThis era also saw an expansion in ground-attack capabilities, principally in guided missiles, and witnessed the introduction of the first truly effective avionics for enhanced ground attack, including terrain-avoidance systems. Air-to-surface missiles (ASM) equipped with electro-optical (E-O) contrast seekers – such as the initial model of the widely used AGM-65 Maverick – became standard weapons, and laser-guided bombs (LGBs) became widespread in effort to improve precision-attack capabilities. Guidance for such precision-guided munitions (PGM) was provided by externally mounted targeting pods, which were introduced in the mid-1960s.\n\nIt also led to the development of new automatic-fire weapons, primarily chain-guns that use an electric motor to drive the mechanism of a cannon. This allowed a plane to carry a single multi-barrel weapon (such as the 20 mm Vulcan), and provided greater accuracy and rates of fire. Powerplant reliability increased and jet engines became \"smokeless\" to make it harder to sight aircraft at long distances.\nDedicated ground-attack aircraft (like the Grumman A-6 Intruder, SEPECAT Jaguar and LTV A-7 Corsair II) offered longer range, more sophisticated night attack systems or lower cost than supersonic fighters. With variable-geometry wings, the supersonic F-111 introduced the Pratt & Whitney TF30, the first turbofan equipped with afterburner. The ambitious project sought to create a versatile common fighter for many roles and services. It would serve well as an all-weather bomber, but lacked the performance to defeat other fighters. The McDonnell F-4 Phantom was designed around radar and missiles as an all-weather interceptor, but emerged as a versatile strike bomber nimble enough to prevail in air combat, adopted by the U.S. Navy, Air Force and Marine Corps. Despite numerous shortcomings that would be not be fully addressed until newer fighters, the Phantom claimed 280 aerial kills, more than any other U.S. fighter over Vietnam. With range and payload capabilities that rivaled that of World War II bombers such as B-24 Liberator, the Phantom would become a highly successful multirole aircraft.\n\nFourth-generation fighters continued the trend towards multirole configurations, and were equipped with increasingly sophisticated avionics and weapon systems. Fighter designs were significantly influenced by the Energy-Maneuverability (E-M) theory developed by Colonel John Boyd and mathematician Thomas Christie, based upon Boyd's combat experience in the Korean War and as a fighter tactics instructor during the 1960s. E-M theory emphasized the value of aircraft specific energy maintenance as an advantage in fighter combat. Boyd perceived maneuverability as the primary means of getting \"inside\" an adversary's decision-making cycle, a process Boyd called the \"OODA loop\" (for \"Observation-Orientation-Decision-Action\"). This approach emphasized aircraft designs that were capable of performing \"fast transients\" – quick changes in speed, altitude, and direction – as opposed to relying chiefly on high speed alone.\nE-M characteristics were first applied to the McDonnell Douglas F-15 Eagle, but Boyd and his supporters believed these performance parameters called for a small, lightweight aircraft with a larger, higher-lift wing. The small size would minimize drag and increase the thrust-to-weight ratio, while the larger wing would minimize wing loading; while the reduced wing loading tends to lower top speed and can cut range, it increases payload capacity and the range reduction can be compensated for by increased fuel in the larger wing. The efforts of Boyd's \"Fighter Mafia\" would result in the General Dynamics F-16 Fighting Falcon (now Lockheed Martin's).\n\nThe F-16's maneuverability was further enhanced by its slight aerodynamic instability. This technique, called \"relaxed static stability\" (RSS), was made possible by introduction of the \"fly-by-wire\" (FBW) flight control system (FLCS), which in turn was enabled by advances in computers and system integration techniques. Analog avionics, required to enable FBW operations, became a fundamental requirement and began to be replaced by digital flight control systems in the latter half of the 1980s. Likewise, Full Authority Digital Engine Controls (FADEC) to electronically manage powerplant performance was introduced with the Pratt & Whitney F100 turbofan. The F-16's sole reliance on electronics and wires to relay flight commands, instead of the usual cables and mechanical linkage controls, earned it the sobriquet of \"the electric jet\". Electronic FLCS and FADEC quickly became essential components of all subsequent fighter designs.\nOther innovative technologies introduced in fourth-generation fighters include pulse-Doppler fire-control radars (providing a \"look-down/shoot-down\" capability), head-up displays (HUD), \"hands on throttle-and-stick\" (HOTAS) controls, and multi-function displays (MFD), all now essential equipment. Aircraft designers began to incorporate composite materials in the form of bonded aluminum honeycomb structural elements and graphite epoxy laminate skins to reduce weight. Infrared search-and-track (IRST) sensors became widespread for air-to-ground weapons delivery, and appeared for air-to-air combat as well. \"All-aspect\" IR AAM became standard air superiority weapons, which permitted engagement of enemy aircraft from any angle (although the field of view remained relatively limited). The first long-range active-radar-homing RF AAM entered service with the AIM-54 Phoenix, which solely equipped the Grumman F-14 Tomcat, one of the few variable-sweep-wing fighter designs to enter production. Even with the tremendous advancement of air-to-air missiles in this era, internal guns were standard equipment.\nAnother revolution came in the form of a stronger reliance on ease of maintenance, which led to standardisation of parts, reductions in the numbers of access panels and lubrication points, and overall parts reduction in more complicated equipment like the engines. Some early jet fighters required 50 man-hours of work by a ground crew for every hour the aircraft was in the air; later models substantially reduced this to allow faster turn-around times and more sorties in a day. Some modern military aircraft only require 10 man-hours of work per hour of flight time, and others are even more efficient.\n\nAerodynamic innovations included variable-camber wings and exploitation of the vortex lift effect to achieve higher angles of attack through the addition of leading-edge extension devices such as strakes.\nUnlike interceptors of the previous eras, most fourth-generation air-superiority fighters were designed to be agile dogfighters (although the Mikoyan MiG-31 and Panavia Tornado ADV are notable exceptions). The continually rising cost of fighters, however, continued to emphasize the value of multirole fighters. The need for both types of fighters led to the \"high/low mix\" concept, which envisioned a high-capability and high-cost core of dedicated air-superiority fighters (like the F-15 and Su-27) supplemented by a larger contingent of lower-cost multi-role fighters (such as the F-16 and MiG-29).\nMost fourth-generation fighters, such as the McDonnell Douglas F/A-18 Hornet and Dassault Mirage 2000, are true multirole warplanes, designed as such from the start. This was facilitated by multimode avionics that could switch seamlessly between air and ground modes. The earlier approaches of adding on strike capabilities or designing separate models specialized for different roles generally became \"passé\" (with the Panavia Tornado being an exception in this regard). Attack roles were generally assigned to dedicated ground-attack aircraft such as the Sukhoi Su-25 and the A-10 Thunderbolt II.\n\nA typical US Air Force fighter wing of the period might contain a mix of one air superiority squadron (F-15C), one strike fighter squadron (F-15E), and two multirole fighter squadrons (F-16C).\n\nPerhaps the most novel technology introduced for combat aircraft was \"stealth\", which involves the use of special \"low-observable\" (L-O) materials and design techniques to reduce the susceptibility of an aircraft to detection by the enemy's sensor systems, particularly radars. The first stealth aircraft introduced were the Lockheed F-117 Nighthawk attack aircraft (introduced in 1983) and the Northrop Grumman B-2 Spirit bomber (first flew in 1989). Although no stealthy fighters per se appeared among the fourth generation, some radar-absorbent coatings and other L-O treatments developed for these programs are reported to have been subsequently applied to fourth-generation fighters.\n\nThe end of the Cold War in 1991 led many governments to significantly decrease military spending as a \"peace dividend\". Air force inventories were cut. Research and development programs working on \"fifth-generation\" fighters took serious hits. Many programs were canceled during the first half of the 1990s, and those that survived were \"stretched out\". While the practice of slowing the pace of development reduces annual investment expenses, it comes at the penalty of increased overall program and unit costs over the long-term. In this instance, however, it also permitted designers to make use of the tremendous achievements being made in the fields of computers, avionics and other flight electronics, which had become possible largely due to the advances made in microchip and semiconductor technologies in the 1980s and 1990s. This opportunity enabled designers to develop fourth-generation designs – or redesigns – with significantly enhanced capabilities. These improved designs have become known as \"Generation 4.5\" fighters, recognizing their intermediate nature between the 4th and 5th generations, and their contribution in furthering development of individual fifth-generation technologies.\nThe primary characteristics of this sub-generation are the application of advanced digital avionics and aerospace materials, modest signature reduction (primarily RF \"stealth\"), and highly integrated systems and weapons. These fighters have been designed to operate in a \"network-centric\" battlefield environment and are principally multirole aircraft. Key weapons technologies introduced include beyond-visual-range (BVR) AAMs; Global Positioning System (GPS)-guided weapons, solid-state phased-array radars; helmet-mounted sights; and improved secure, jamming-resistant datalinks. Thrust vectoring to further improve transient maneuvering capabilities has also been adopted by many 4.5th generation fighters, and uprated powerplants have enabled some designs to achieve a degree of \"supercruise\" ability. Stealth characteristics are focused primarily on frontal-aspect radar cross section (RCS) signature-reduction techniques including radar-absorbent materials (RAM), L-O coatings and limited shaping techniques.\n\"Half-generation\" designs are either based on existing airframes or are based on new airframes following similar design theory as previous iterations; however, these modifications have introduced the structural use of composite materials to reduce weight, greater fuel fractions to increase range, and signature reduction treatments to achieve lower RCS compared to their predecessors. Prime examples of such aircraft, which are based on new airframe designs making extensive use of carbon-fibre composites, include the Eurofighter Typhoon, Dassault Rafale, and Saab JAS 39 Gripen.\n\nApart from these fighter jets, most of the 4.5 generation aircraft are actually modified variants of existing airframes from the earlier fourth generation fighter jets. Such fighter jets are generally heavier and examples include the Boeing F/A-18E/F Super Hornet, which is an evolution of the 1970s F/A-18 Hornet design, the F-15E Strike Eagle, which is a ground-attack/multi-role variant of the F-15 Eagle, the Su-30MKI and Su-30MKK variants of the Sukhoi Su-30 and the MiG-29M, MiG-29K and MiG-35, upgraded versions of the Mikoyan MiG-29. The Su-30MKI and MiG-35 feature thrust vectoring engine nozzles to enhance maneuvering.\n4.5 generation fighters first entered service in the early 1990s, and most of them are still being produced and evolved. It is quite possible that they may continue in production alongside fifth-generation fighters due to the expense of developing the advanced level of stealth technology needed to achieve aircraft designs featuring very low observables (VLO), which is one of the defining features of fifth-generation fighters. Of the 4.5th generation designs, the Strike Eagle, Super Hornet, Typhoon, Gripen, and Rafale have been used in combat.\n\nThe U.S. government has defined 4.5 generation fighter aircraft as those that \"(1) have advanced capabilities, including— (A) AESA radar; (B) high capacity data-link; and (C) enhanced avionics; and (2) have the ability to deploy current and reasonably foreseeable advanced armaments.\"\n\nThe fifth generation was ushered in by the Lockheed Martin/Boeing F-22 Raptor in late 2005. Currently the cutting edge of fighter design, fifth-generation fighters are characterized by being designed from the start to operate in a network-centric combat environment, and to feature extremely low, all-aspect, multi-spectral signatures employing advanced materials and shaping techniques. They have multifunction AESA radars with high-bandwidth, low-probability of intercept (LPI) data transmission capabilities. The Infra-red search and track sensors incorporated for air-to-air combat as well as for air-to-ground weapons delivery in the 4.5th generation fighters are now fused in with other sensors for Situational Awareness IRST or SAIRST, which constantly tracks all targets of interest around the aircraft so the pilot need not guess when he glances. These sensors, along with advanced avionics, glass cockpits, helmet-mounted sights (not currently on F-22), and improved secure, jamming-resistant LPI datalinks are highly integrated to provide multi-platform, multi-sensor data fusion for vastly improved situational awareness while easing the pilot's workload. Avionics suites rely on extensive use of very high-speed integrated circuit (VHSIC) technology, common modules, and high-speed data buses. Overall, the integration of all these elements is claimed to provide fifth-generation fighters with a \"first-look, first-shot, first-kill capability\".\n\nA key attribute of fifth-generation fighters is a small radar cross-section. Great care has been taken in designing its layout and internal structure to minimize RCS over a broad bandwidth of detection and tracking radar frequencies; furthermore, to maintain its VLO signature during combat operations, primary weapons are carried in internal weapon bays that are only briefly opened to permit weapon launch. Furthermore, stealth technology has advanced to the point where it can be employed without a tradeoff with aerodynamics performance, in contrast to previous stealth efforts. Some attention has also been paid to reducing IR signatures, especially on the F-22. Detailed information on these signature-reduction techniques is classified, but in general includes special shaping approaches, thermoset and thermoplastic materials, extensive structural use of advanced composites, conformal sensors, heat-resistant coatings, low-observable wire meshes to cover intake and cooling vents, heat ablating tiles on the exhaust troughs (seen on the Northrop YF-23), and coating internal and external metal areas with radar-absorbent materials and paint (RAM/RAP).\n\nThe AESA radar offers unique capabilities for fighters (and it is also quickly becoming essential for Generation 4.5 aircraft designs, as well as being retrofitted onto some fourth-generation aircraft). In addition to its high resistance to ECM and LPI features, it enables the fighter to function as a sort of \"mini-AWACS,\" providing high-gain electronic support measures (ESM) and electronic warfare (EW) jamming functions. Other technologies common to this latest generation of fighters includes integrated electronic warfare system (INEWS) technology, integrated communications, navigation, and identification (CNI) avionics technology, centralized \"vehicle health monitoring\" systems for ease of maintenance, fiber optics data transmission, stealth technology and even hovering capabilities. Maneuver performance remains important and is enhanced by thrust-vectoring, which also helps reduce takeoff and landing distances. Supercruise may or may not be featured; it permits flight at supersonic speeds without the use of the afterburner – a device that significantly increases IR signature when used in full military power.\nSuch aircraft are sophisticated and expensive. The U.S. Air Force originally planned to acquire 650 F-22s, but now only 187 will be built. As a result, its unit flyaway cost (FAC) is around US$150 million. To spread the development costs – and production base – more broadly, the Joint Strike Fighter (JSF) program enrolls eight other countries as cost- and risk-sharing partners. Altogether, the nine partner nations anticipate procuring over 3,000 Lockheed Martin F-35 Lightning II fighters at an anticipated average FAC of $80–85 million. The F-35, however, is designed to be a family of three aircraft, a conventional take-off and landing (CTOL) fighter, a short take-off and vertical landing (STOVL) fighter, and a Catapult Assisted Take Off But Arrested Recovery (CATOBAR) fighter, each of which has a different unit price and slightly varying specifications in terms of fuel capacity (and therefore range), size and payload.\n\nOther countries have initiated fifth-generation fighter development projects, with Russia's Sukhoi Su-57 and Mikoyan LMFS. In October 2007, Russia and India signed an agreement for joint participation in a Fifth-Generation Fighter Aircraft Program (FGFA), which gives India responsibility for development of a two-seat model of the Su-57. India is also developing the Advanced Medium Combat Aircraft (AMCA). In December 2010, it was discovered that China is developing the 5th generation fighter Chengdu J-20. The J-20 took its maiden flight in January 2011 and is planned to be deployed in 2017–19 time frame. The Shenyang J-31 took its maiden flight on 31 October 2012. Japan is exploring its technical feasibility to produce fifth-generation fighters.\n\nCurrently at the concept stage, the first sixth-generation jet fighter is expected to enter service in the United States Air Force and United States Navy in 2025–30 period. The USAF seeks a new fighter for the 2030–50 period named the \"Next Generation Tactical Aircraft\" (\"Next Gen TACAIR\" The US Navy looks to replace its F/A-18E/F Super Hornets beginning in 2025 with the Next Generation Air Dominance air superiority fighter.\n\nThroughout the history of air combat, fighters which, by surprise or maneuver, attain a good firing position have achieved the kill about one third to one half the time, no matter what weapons were carried. The only major historic exception to this has been the low effectiveness shown by guided missiles in the first one to two decades of their existence. \n\nFrom WWI to the present fighter aircraft have featured machine guns and automatic cannons as weapons, and they are still considered as essential back-up weapons today. The power of air-to-air guns has increased greatly over time, and has kept them relevant in the guided missile era. In WWI two rifle calibre machine guns was the typical armament producing a weight of fire of about per second. \n\nThe standard WWII American fighter armament of six 0.50-cal (12.7mm) machine guns fired a bullet weight of approximately 3.7 kg/sec (8.1 lbs/sec), at a muzzle velocity of 856 m/s (28,10 ft/s). British and German aircraft tended to use a mix of machine guns and autocannon, the latter firing explosive projectiles. The modern M61 Vulcan 20mm rotating barrel Gatling gun that is standard on current American fighters fires a projectile weight of about 10 kg/s (22 lb/s), nearly three times that of six 0.50-cal machine guns, with higher velocity of 1,052 m/s (3450 ft/s) supporting a flatter trajectory, and with exploding projectiles. Modern fighter gun systems also feature ranging radar and lead computing electronic gun sights to ease the problem of aim point to compensate for projectile drop and time of flight (target lead) in the complex three dimensional maneuvering of air-to-air combat. However, getting in position to use the guns is the challenge. The range of guns is longer than in the past but still quite limited compared to missiles, with modern gun systems having a maximum effective range of approximately 1,000 meters. High probability of kill also requires firing to usually occur from the rear hemisphere of the target. Despite these limits, when pilots are well trained in air-to-air gunnery and these conditions are satisfied, gun systems are tactically effective and highly cost efficient. The cost of a gun firing pass is far less than firing a missile, and the projectiles are not subject to the thermal and electronic countermeasures than can sometimes defeat missiles. When the enemy can be approached to within gun range, the lethality of guns is approximately a 25% to 50% chance of \"kill per firing pass\".\n\nThe range limitations of guns, and the desire to overcome large variations in fighter pilot skill and thus achieve higher force effectiveness, led to the development of the guided air-to-air missile. There are two main variations, heat-seeking (infrared homing), and radar guided. Radar missiles are typically several times heavier and more expensive than heat-seekers, but with longer range, greater destructive power, and ability to track through clouds.\n\nThe highly successful AIM-9 Sidewinder heat-seeking (infrared homing) short-range missile was developed by the United States Navy in the 1950s. These small missiles are easily carried by lighter fighters, and provide effective ranges of approximately 10 to 35 km (~6 to 22 miles). Beginning with the AIM-9L in 1977, subsequent versions of Sidewinder have added all-aspect capability, the ability to use the lower heat of air to skin friction on the target aircraft to track from the front and sides. The latest (2003 service entry) AIM-9X also features “off-boresight” and “lock on after launch” capabilities, which allow the pilot to make a quick launch of a missile to track a target anywhere within the pilot’s vision. The AIM-9X development cost was U.S. $3 billion in mid to late 1990's dollars, and 2015 per unit procurement cost is $0.6 million each. The missile weighs 85.3 kg (188 lbs), and has a maximum range of 35 km (22 miles) at higher altitudes. Like most air-to-air missiles, lower altitude range can be as limited as only about one third of maximum due to higher drag and less ability to coast downward.\n\nThe effectiveness of heat-seeking missiles was only 7% early in the Vietnam War, but improved to approximately 15%–40% over the course of the war. The AIM-4 Falcon used by the USAF had kill rates of approximately 7% and was considered a failure. The AIM-9B Sidewinder introduced later achieved 15% kill rates, and the further improved AIM-9D and J models reached 19%. The AIM-9G used in the last year of the Vietnam air war achieved 40%. Israel used almost totally guns in the 1967 Six Day War, achieving 60 kills and 10 losses. However, Israel made much more use of steadily improving heat-seeking missiles in the 1973 Yom Kippur War. In this extensive conflict Israel scored 171 of out of 261 total kills with heat-seeking missiles (65.5%), 5 kills with radar guided missiles (1.9%), and 85 kills with guns (32.6%). The AIM-9L Sidewinder scored 19 kills out of 26 fired missiles (73%) in the 1982 Falklands War. But, in a conflict against opponents using thermal countermeasures, the United States only scored 11 kills out of 48 fired (Pk = 23%) with the follow-on AIM-9M in the 1991 Gulf War.\nRadar guided missiles fall into two main missile guidance types. In the historically more common semi-active radar homing case the missile homes in on radar signals transmitted from launching aircraft and reflected from the target. This has the disadvantage that the firing aircraft must maintain radar lock on the target and is thus less free to maneuver and more vulnerable to attack. A widely deployed missile of this type was the AIM-7 Sparrow, which entered service in 1954 and was produced in improving versions until 1997. In more advanced active radar homing the missile is guided to the vicinity of the target by internal data on its projected position, and then “goes active” with an internally carried small radar system to conduct terminal guidance to the target. This eliminates the requirement for the firing aircraft to maintain radar lock, and thus greatly reduces risk. A prominent example is the AIM-120 AMRAAM, which was first fielded in 1991 as the AIM-7 replacement, and which has no firm retirement date as of 2016. The current AIM-120D version has a maximum high altitude range of greater than 160 km (>99 miles), and cost approximately $2.4 million each (2016). As is typical with most other missiles, range at lower altitude may be as little as one third that of high altitude.\n\nIn the Vietnam air war radar missile kill reliability was approximately 10% at shorter ranges, and even worse at longer ranges due to reduced radar return and greater time for the target aircraft to detect the incoming missile and take evasive action. At one point in the Vietnam war, the U.S. Navy fired 50 AIM-7 Sparrow radar guided missiles in a row without a hit. Between 1958 and 1982 in five wars there were 2,014 combined heat-seeking and radar guided missile firings by fighter pilots engaged in air-to-air combat, achieving 528 kills, of which 76 were radar missile kills, for a combined effectiveness of 26%. However, only four of the 76 radar missile kills were in the beyond-visual-range mode intended to be the strength of radar guided missiles. The United States invested over $10 billion in air-to-air radar missile technology from the 1950s to the early 1970s. Amortized over actual kills achieved by the U.S. and its allies, each radar guided missile kill thus cost over $130 million. The defeated enemy aircraft were for the most part older MiG-17s, -19s, and -21s, with new cost of $0.3 million to $3 million each. Thus, the radar missile investment over that period far exceeded the value of enemy aircraft destroyed, and furthermore had very little of the intended BVR effectiveness.\n\nHowever, continuing heavy development investment and rapidly advancing electronic technology led to significant improvement in radar missile reliabilities from the late 1970s onward. Radar guided missiles achieved 75% Pk (9 kills out of 12 shots) in operations in the Gulf War in 1991. The percentage of kills achieved by radar guided missiles also surpassed 50% of total kills for the first time by 1991. Since 1991, 20 of 61 kills worldwide have been beyond-visual-range using radar missiles. Discounting an accidental friendly fire kill, in operational use the AIM-120D (the current main American radar guided missile) has achieved 9 kills out of 16 shots for a 56% Pk. Six of these kills were BVR, out of 13 shots, for a 46% BVR Pk. Though all these kills were against less capable opponents who were not equipped with operating radar, electronic countermeasures, or a comparable weapon themselves, the BVR Pk was a significant improvement from earlier eras. However, a current concern is electronic countermeasures to radar missiles, which are thought to be reducing the effectiveness of the AIM-120D. Some experts believe that as of 2016 the European Meteor missile, the Russian K-37M, and the Chinese PL-15 are more resistant to countermeasures and more effective than the AIM-120D.\n\nNow that higher reliabilities have been achieved, both types of missiles allow the fighter pilot to often avoid the risk of the short-range dogfight, where only the more experienced and skilled fighter pilots tend to prevail, and where even the finest fighter pilot can simply get unlucky. Taking maximum advantage of complicated missile parameters in both attack and defense against competent opponents does take considerable experience and skill, but against surprised opponents lacking comparable capability and countermeasures, air-to-air missile warfare is relatively simple. By partially automating air-to-air combat and reducing reliance on gun kills mostly achieved by only a small expert fraction of fighter pilots, air-to-air missiles now serve as highly effective force multipliers.\n\n\n"}
{"id": "10930", "url": "https://en.wikipedia.org/wiki?curid=10930", "title": "February 25", "text": "February 25\n\n\n\n"}
{"id": "10931", "url": "https://en.wikipedia.org/wiki?curid=10931", "title": "Finite-state machine", "text": "Finite-state machine\n\nA finite-state machine (FSM) or finite-state automaton (FSA, plural: \"automata\"), finite automaton, or simply a state machine, is a mathematical model of computation. It is an abstract machine that can be in exactly one of a finite number of \"states\" at any given time. The FSM can change from one state to another in response to some external inputs; the change from one state to another is called a \"transition\". An FSM is defined by a list of its states, its initial state, and the conditions for each transition.\n\nThe behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. Examples are vending machines, which dispense products when the proper combination of coins is deposited, elevators, whose sequence of stops is determined by the floors requested by riders, traffic lights, which change sequence when cars are waiting, and combination locks, which require the input of combination numbers in the proper order.\n\nThe finite state machine has less computational power than some other models of computation such as the Turing machine. The computational power distinction means there are computational tasks that a Turing machine can do but a FSM cannot. This is because a FSM's memory is limited by the number of states it has. FSMs are studied in the more general field of automata theory.\n\nAn example of a simple mechanism that can be modeled by a state machine is a turnstile. A turnstile, used to control access to subways and amusement park rides, is a gate with three rotating arms at waist height, one across the entryway. Initially the arms are locked, blocking the entry, preventing patrons from passing through. Depositing a coin or token in a slot on the turnstile unlocks the arms, allowing a single customer to push through. After the customer passes through, the arms are locked again until another coin is inserted.\n\nConsidered as a state machine, the turnstile has two possible states: Locked and Unlocked. There are two possible inputs that affect its state: putting a coin in the slot (coin) and pushing the arm (push). In the locked state, pushing on the arm has no effect; no matter how many times the input push is given, it stays in the locked state. Putting a coin in – that is, giving the machine a coin input – shifts the state from Locked to Unlocked. In the unlocked state, putting additional coins in has no effect; that is, giving additional coin inputs does not change the state. However, a customer pushing through the arms, giving a push input, shifts the state back to Locked.\n\nThe turnstile state machine can be represented by a state transition table, showing for each possible state, the transitions between them (based upon the inputs given to the machine) and the outputs resulting from each input:\nThe turnstile state machine can also be represented by a directed graph called a state diagram \"(above)\". Each state is represented by a node (\"circle\"). Edges (\"arrows\") show the transitions from one state to another. Each arrow is labeled with the input that triggers that transition. An input that doesn't cause a change of state (such as a coin input in the Unlocked state) is represented by a circular arrow returning to the original state. The arrow into the Locked node from the black dot indicates it is the initial state.\n\nA \"state\" is a description of the status of a system that is waiting to execute a \"transition\". A transition is a set of actions to be executed when a condition is fulfilled or when an event is received.\nFor example, when using an audio system to listen to the radio (the system is in the \"radio\" state), receiving a \"next\" stimulus results in moving to the next station. When the system is in the \"CD\" state, the \"next\" stimulus results in moving to the next track. Identical stimuli trigger different actions depending on the current state.\n\nIn some finite-state machine representations, it is also possible to associate actions with a state:\n\nSeveral state transition table types are used. The most common representation is shown below: the combination of current state (e.g. B) and input (e.g. Y) shows the next state (e.g. C). The complete action's information is not directly described in the table and can only be added using footnotes. A FSM definition including the full actions information is possible using state tables (see also virtual finite-state machine).\n\nThe Unified Modeling Language has a notation for describing state machines. UML state machines overcome the limitations of traditional finite state machines while retaining their main benefits. UML state machines introduce the new concepts of hierarchically nested states and orthogonal regions, while extending the notion of actions. UML state machines have the characteristics of both Mealy machines and Moore machines. They support actions that depend on both the state of the system and the triggering event, as in Mealy machines, as well as entry and exit actions, which are associated with states rather than transitions, as in Moore machines.\n\nThe Specification and Description Language is a standard from ITU that includes graphical symbols to describe actions in the transition:\nSDL embeds basic data types called \"Abstract Data Types\", an action language, and an execution semantic in order to make the finite state machine executable.\n\nThere are a large number of variants to represent an FSM such as the one in figure 3.\n\nIn addition to their use in modeling reactive systems presented here, finite state machines are significant in many different areas, including electrical engineering, linguistics, computer science, philosophy, biology, mathematics, and logic. Finite state machines are a class of automata studied in automata theory and the theory of computation.\nIn computer science, finite state machines are widely used in modeling of application behavior, design of hardware digital systems, software engineering, compilers, network protocols, and the study of computation and languages.\n\nFinite state machines can be subdivided into transducers, acceptors, classifiers and sequencers.\n\nAcceptors, also called recognizers and sequence detectors, produce binary output, indicating whether or not the received input is accepted. Each state of an FSM is either \"accepting\" or \"not accepting\". Once all input has been received, if the current state is an accepting state, the input is accepted; otherwise it is rejected. As a rule, input is a sequence of symbols (characters); actions are not used. The example in figure 4 shows a finite state machine that accepts the string \"nice\". In this FSM, the only accepting state is state 7.\n\nA (possibly infinite) set of symbol sequences, aka. formal language, is called a regular language if there is some Finite State Machine that accepts exactly that set. For example, the set of binary strings with an even number of zeroes is a regular language (cf. Fig. 5), while the set of all strings whose length is a prime number is not.\n\nA machine could also be described as defining a language, that would contain every string accepted by the machine but none of the rejected ones; that language is \"accepted\" by the machine. By definition, the languages accepted by FSMs are the regular languages—; a language is regular if there is some FSM that accepts it.\n\nThe problem of determining the language accepted by a given finite state acceptor is an instance of the algebraic path problem—itself a generalization of the shortest path problem to graphs with edges weighted by the elements of an (arbitrary) semiring.\n\nThe start state can also be an accepting state, in which case the automaton accepts the empty string.\n\nAn example of an accepting state appears in Fig.5: a deterministic finite automaton (DFA) that detects whether the binary input string contains an even number of 0s.\n\n\"S\" (which is also the start state) indicates the state at which an even number of 0s has been input. S is therefore an accepting state. This machine will finish in an accept state, if the binary string contains an even number of 0s (including any binary string containing no 0s). Examples of strings accepted by this DFA are ε (the empty string), 1, 11, 11…, 00, 010, 1010, 10110, etc.\n\nA classifier is a generalization of a finite state machine that, similar to an acceptor, produces a single output on termination but has more than two terminal states.\n\nTransducers generate output based on a given input and/or a state using actions. They are used for control applications and in the field of computational linguistics.\n\nIn control applications, two types are distinguished:\n\n\nSequencers', or generators, are a subclass of the acceptor and transducer types that have a single-letter input alphabet. They produce only one sequence which can be seen as an output sequence of acceptor or tranducer outputs.\n\nA further distinction is between deterministic (DFA) and non-deterministic (NFA, GNFA) automata. In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automata into a (usually more complex) deterministic automata with identical functionality.\n\nA finite state machine with only one state is called a \"combinatorial FSM\". It only allows actions upon transition \"into\" a state. This concept is useful in cases where a number of finite state machines are required to work together, and when it is convenient to consider a purely combinatorial part as a form of FSM to suit the design tools.\n\nThere are other sets of semantics available to represent state machines. For example, there are tools for modeling and designing logic for embedded controllers. They combine hierarchical state machines (which usually have more than one current state), flow graphs, and truth tables into one language, resulting in a different formalism and set of semantics. These charts, like Harel's original state machines, support hierarchically nested states, orthogonal regions, state actions, and transition actions.\n\nIn accordance with the general classification, the following formal definitions are found:\n\nFor both deterministic and non-deterministic FSMs, it is conventional to allow formula_6 to be a partial function, i.e. formula_13 does not have to be defined for every combination of formula_14 and formula_15. If an FSM formula_16 is in a state formula_17, the next symbol is formula_18 and formula_13 is not defined, then formula_16 can announce an error (i.e. reject the input). This is useful in definitions of general state machines, but less useful when transforming the machine. Some algorithms in their default form may require total functions.\n\nA finite state machine has the same computational power as a Turing machine that is restricted such its head may only perform \"read\" operations, and always has to move from left to right. That is, each formal language accepted by a finite state machine is accepted by such a kind of restricted Turing machine, and vice versa.\n\n\nIf the output function is a function of a state and input alphabet (formula_31) that definition corresponds to the Mealy model, and can be modelled as a Mealy machine. If the output function depends only on a state (formula_32) that definition corresponds to the Moore model, and can be modelled as a Moore machine. A finite-state machine with no output function at all is known as a semiautomaton or transition system.\n\nIf we disregard the first output symbol of a Moore machine, formula_33, then it can be readily converted to an output-equivalent Mealy machine by setting the output function of every Mealy transition (i.e. labeling every edge) with the output symbol given of the destination Moore state. The converse transformation is less straightforward because a Mealy machine state may have different output labels on its incoming transitions (edges). Every such state needs to be split in multiple Moore machine states, one for every incident output symbol.\n\nOptimizing an FSM means finding a machine with the minimum number of states that performs the same function. The fastest known algorithm doing this is the Hopcroft minimization algorithm. Other techniques include using an implication table, or the Moore reduction procedure. Additionally, acyclic FSAs can be minimized in linear time.\n\nIn a digital circuit, an FSM may be built using a programmable logic device, a programmable logic controller, logic gates and flip flops or relays. More specifically, a hardware implementation requires a register to store state variables, a block of combinational logic that determines the state transition, and a second block of combinational logic that determines the output of an FSM. One of the classic hardware implementations is the Richards controller.\n\nIn a \"Medvedev machine\", the output is directly connected to the state flip-flops minimizing the time delay between flip-flops and output.\n\nThrough state encoding for low power state machines may be optimized to minimize power consumption.\n\nThe following concepts are commonly used to build software applications with finite state machines:\n\nFinite automata are often used in the frontend of programming language compilers. Such a frontend may comprise several finite state machines that implement a lexical analyzer and a parser.\nStarting from a sequence of characters, the lexical analyzer builds a sequence of language tokens (such as reserved words, literals, and identifiers) from which the parser builds a syntax tree. The lexical analyzer and the parser handle the regular and context-free parts of the programming language's grammar.\n\n\n\n\nFinite Markov-chain processes are also known as subshifts of finite type.\n\n"}
{"id": "10933", "url": "https://en.wikipedia.org/wiki?curid=10933", "title": "Functional programming", "text": "Functional programming\n\nIn computer science, functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements. In functional code, the output value of a function depends only on the arguments that are passed to the function, so calling a function \"f\" twice with the same value for an argument \"x\" will produce the same result \"f(x)\" each time; this is in contrast to procedures depending on a local or global state, which may produce different results at different times when called with the same arguments but a different program state. Eliminating side effects, i.e. changes in state that do not depend on the function inputs, can make it much easier to understand and predict the behavior of a program, which is one of the key motivations for the development of functional programming.\n\nFunctional programming has its origins in lambda calculus, a formal system developed in the 1930s to investigate computability, the Entscheidungsproblem, function definition, function application, and recursion. Many functional programming languages can be viewed as elaborations on the lambda calculus. Another well-known declarative programming paradigm, \"logic programming\", is based on relations.\n\nIn contrast, imperative programming changes state with commands in the source code, the simplest example being assignment. Imperative programming does have functions—not in the mathematical sense—but in the sense of subroutines. They can have side effects that may change the value of program state. Functions without return values therefore make sense. Because of this, they lack referential transparency, i.e. the same language expression can result in different values at different times depending on the state of the executing program.\n\nFunctional programming languages have largely been emphasized in academia rather than in commercial software development. However, prominent programming languages which support functional programming such as Common Lisp, Scheme, Clojure, Wolfram Language (also known as Mathematica), Racket, Erlang, OCaml, Haskell, and F# have been used in industrial and commercial applications by a wide variety of organizations. JavaScript, one of the world's most widely-distributed languages, has the properties of an untyped functional language, as well as imperative and object-oriented paradigms. Functional programming is also supported in some domain-specific programming languages like R (statistics), J, K and Q from Kx Systems (financial analysis), XQuery/XSLT (XML), and Opal. Widespread domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, especially in eschewing mutable values.\n\nProgramming in a functional style can also be accomplished in languages that are not specifically designed for functional programming. For example, the imperative Perl programming language has been the subject of a book describing how to apply functional programming concepts. This is also true of the PHP programming language. C++11, Java 8, and C# 3.0 all added constructs to facilitate the functional style. The Julia language also offers functional programming abilities. An interesting case is that of Scala – it is frequently written in a functional style, but the presence of side effects and mutable state place it in a grey area between imperative and functional languages.\n\nLambda calculus provides a theoretical framework for describing functions and their evaluation. Although it is a mathematical abstraction rather than a programming language, it forms the basis of almost all functional programming languages today. An equivalent theoretical formulation, combinatory logic, is commonly perceived as more abstract than lambda calculus and preceded it in invention. Combinatory logic and lambda calculus were both originally developed to achieve a clearer approach to the foundations of mathematics.\n\nAn early functional-flavored language was Lisp, developed in the late 1950s for the IBM 700/7000 series scientific computers by John McCarthy while at Massachusetts Institute of Technology (MIT). Lisp first introduced many paradigmatic features of functional programming, though early Lisps were multi-paradigm languages, and incorporated support for numerous programming styles as new paradigms evolved. Later dialects, such as Scheme and Clojure, and offshoots such as Dylan and Julia, sought to simplify and rationalise Lisp around a cleanly functional core, while Common Lisp was designed to preserve and update the paradigmatic features of the numerous older dialects it replaced.\n\nInformation Processing Language (IPL) is sometimes cited as the first computer-based functional programming language. It is an assembly-style language for manipulating lists of symbols. It does have a notion of \"generator\", which amounts to a function accepting a function as an argument, and, since it is an assembly-level language, code can be used as data, so IPL can be regarded as having higher-order functions. However, it relies heavily on mutating list structure and similar imperative features.\n\nKenneth E. Iverson developed APL in the early 1960s, described in his 1962 book \"A Programming Language\" (). APL was the primary influence on John Backus's FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.\n\nJohn Backus presented FP in his 1977 Turing Award lecture \"Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs\". He defines functional programs as being built up in a hierarchical way by means of \"combining forms\" that allow an \"algebra of programs\"; in modern language, this means that functional programs follow the principle of compositionality. Backus's paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style which has come to be associated with functional programming.\n\nIn the 1970s, ML was created by Robin Milner at the University of Edinburgh, and David Turner initially developed the language SASL at the University of St Andrews and later the language Miranda at the University of Kent. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language NPL. NPL was based on Kleene Recursion Equations and was first introduced in their work on program transformation. Burstall, MacQueen and Sannella then incorporated the polymorphic type checking from ML to produce the language Hope. ML eventually developed into several dialects, the most common of which are now OCaml and Standard ML. Meanwhile, the development of Scheme, a simple lexically scoped and (impurely) functional dialect of Lisp, as described in the influential Lambda Papers and the classic 1985 textbook \"Structure and Interpretation of Computer Programs\", brought awareness of the power of functional programming to the wider programming-languages community.\n\nIn the 1980s, Per Martin-Löf developed intuitionistic type theory (also called \"constructive\" type theory), which associated functional programs with constructive proofs of arbitrarily complex mathematical propositions expressed as dependent types. This led to powerful new approaches to interactive theorem proving and has influenced the development of many subsequent functional programming languages.\n\nThe Haskell language began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing since 1990.\n\nA number of concepts and paradigms are specific to functional programming, and generally foreign to imperative programming (including object-oriented programming). However, programming languages are often hybrids of several programming paradigms, so programmers using \"mostly imperative\" languages may have utilized some of these concepts.\n\nHigher-order functions are functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the differential operator formula_1, which returns the derivative of a function formula_2.\n\nHigher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: \"higher-order\" describes a mathematical concept of functions that operate on other functions, while \"first-class\" is a computer science term that describes programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).\n\nHigher-order functions enable partial application or currying, a technique in which a function is applied to its arguments one at a time, with each application returning a new function that accepts the next argument. This allows one to succinctly express, for example, the successor function as the addition operator partially applied to the natural number one.\n\nPure functions (or expressions) have no side effects (memory or I/O). This means that pure functions have several useful properties, many of which can be used to optimize the code:\n\n\nWhile most compilers for imperative programming languages detect pure functions and perform common-subexpression elimination for pure function calls, they cannot always do this for pre-compiled libraries, which generally do not expose this information, thus preventing optimizations that involve those external functions. Some compilers, such as gcc, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations. Fortran 95 also allows functions to be designated \"pure\".\n\nIteration (looping) in functional languages is usually accomplished via recursion. Recursive functions invoke themselves, allowing an operation to be performed over and over until the base case is reached. Though some recursion requires maintaining a stack, tail recursion can be recognized and optimized by a compiler into the same code used to implement iteration in imperative languages. The Scheme language standard requires implementations to recognize and optimize tail recursion. Tail recursion optimization can be implemented by transforming the program into continuation passing style during compiling, among other approaches.\n\nCommon patterns of recursion can be factored out using higher order functions, with catamorphisms and anamorphisms (or \"folds\" and \"unfolds\") being the most obvious examples. Such higher order functions play a role analogous to built-in control structures such as loops in imperative languages.\n\nMost general purpose functional programming languages allow unrestricted recursion and are Turing complete, which makes the halting problem undecidable, can cause unsoundness of equational reasoning, and generally requires the introduction of inconsistency into the logic expressed by the language's type system. Some special purpose languages such as Coq allow only well-founded recursion and are strongly normalizing (nonterminating computations can be expressed only with infinite streams of values called codata). As a consequence, these languages fail to be Turing complete and expressing certain functions in them is impossible, but they can still express a wide class of interesting computations while avoiding the problems introduced by unrestricted recursion. Functional programming limited to well-founded recursion with a few other constraints is called total functional programming.\n\nFunctional languages can be categorized by whether they use \"strict (eager)\" or \"non-strict (lazy)\" evaluation, concepts that refer to how function arguments are processed when an expression is being evaluated. The technical difference is in the denotational semantics of expressions containing failing or divergent computations. Under strict evaluation, the evaluation of any term containing a failing subterm will itself fail. For example, the expression:\n\nwill fail under strict evaluation because of the division by zero in the third element of the list. Under lazy evaluation, the length function will return the value 4 (i.e., the number of items in the list), since evaluating it will not attempt to evaluate the terms making up the list. In brief, strict evaluation always fully evaluates function arguments before invoking the function. Lazy evaluation does not evaluate function arguments unless their values are required to evaluate the function call itself.\n\nThe usual implementation strategy for lazy evaluation in functional languages is graph reduction. Lazy evaluation is used by default in several pure functional languages, including Miranda, Clean, and Haskell.\n\nEspecially since the development of Hindley–Milner type inference in the 1970s, functional programming languages have tended to use typed lambda calculus, rejecting all invalid programs at compilation time and risking false positive errors, as opposed to the untyped lambda calculus, that accepts all valid programs at compilation time and risks false negative errors, used in Lisp and its variants (such as Scheme), although they reject all invalid programs at runtime, when the information is enough to not reject valid programs. The use of algebraic datatypes makes manipulation of complex data structures convenient; the presence of strong compile-time type checking makes programs more reliable in absence of other reliability techniques like test-driven development, while type inference frees the programmer from the need to manually declare types to the compiler in most cases.\n\nSome research-oriented functional languages such as Coq, Agda, Cayenne, and Epigram are based on intuitionistic type theory, which allows types to depend on terms. Such types are called dependent types. These type systems do not have decidable type inference and are difficult to understand and program with. But dependent types can express arbitrary propositions in predicate logic. Through the Curry–Howard isomorphism, then, well-typed programs in these languages become a means of writing formal mathematical proofs from which a compiler can generate certified code. While these languages are mainly of interest in academic research (including in formalized mathematics), they have begun to be used in engineering as well. Compcert is a compiler for a subset of the C programming language that is written in Coq and formally verified.\n\nA limited form of dependent types called generalized algebraic data types (GADT's) can be implemented in a way that provides some of the benefits of dependently typed programming while avoiding most of its inconvenience. GADT's are available in the Glasgow Haskell Compiler, in OCaml (since version 4.00) and in Scala (as \"case classes\"), and have been proposed as additions to other languages including Java and C#.\n\nFunctional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.\n\nConsider C assignment statement codice_1, this changes the value assigned to the variable codice_2. Let us say that the initial value of codice_2 was codice_4, then two consecutive evaluations of the variable codice_2 will yield codice_6 and codice_7 respectively. Clearly, replacing codice_1 with either codice_6 or codice_7 gives a program with different meaning, and so the expression \"is not\" referentially transparent. In fact, assignment statements are never referentially transparent.\n\nNow, consider another function such as codice_11 \"is\" transparent, as it will not implicitly change the input x and thus has no such side effects.\nFunctional programs exclusively use this type of function and are therefore referentially transparent.\n\nIt is possible to use a functional style of programming in languages that are not traditionally considered functional languages. For example, both D and Fortran 95 explicitly support pure functions.\n\nJavaScript, Lua and Python had first class functions from their inception. Amrit Prem added support to Python for \"lambda\", \"map\", \"reduce\", and \"filter\" in 1994, as well as closures in Python 2.2, though Python 3 relegated \"reduce\" to the codice_12 standard library module. First-class functions have been introduced into other mainstream languages such as PHP 5.3, Visual Basic 9, C# 3.0, and C++11.\n\nIn PHP, anonymous classes, closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style.\n\nIn Java, anonymous classes can sometimes be used to simulate closures; however, anonymous classes are not always proper replacements to closures because they have more limited capabilities. Java 8 supports lambda expressions as a replacement for some anonymous classes. However, the presence of checked exceptions in Java can make functional programming inconvenient, because it can be necessary to catch checked exceptions and then rethrow them—a problem that does not occur in other JVM languages that do not have checked exceptions, such as Scala.\n\nIn C#, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.\n\nMany object-oriented design patterns are expressible in functional programming terms: for example, the strategy pattern simply dictates use of a higher-order function, and the visitor pattern roughly corresponds to a catamorphism, or fold.\n\nSimilarly, the idea of immutable data from functional programming is often included in imperative programming languages, for example the tuple in Python, which is an immutable array.\n\nPurely functional data structures are often represented in a different way than their imperative counterparts. For example, array with constant-time access and update is a basic component of most imperative languages and many imperative data-structure, such as hash table and binary heap, are based on arrays. Arrays can be replaced by map or random access list, which admits purely functional implementation, but the access and update time is logarithmic. Therefore, purely functional data structures can be used in languages which are non-functional, but they may not be the most efficient tool available, especially if persistency is not required.\n\nFunctional programming is very different from imperative programming. The most significant differences stem from the fact that functional programming avoids side effects, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.\n\nHigher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse and modify a list. A functional program, on the other hand, would probably use a higher-order “map” function that takes a function and a list, generating and returning a new list by applying the function to each list item.\n\nThere are tasks (for example, maintaining a bank account balance) that often seem most naturally implemented with state. Pure functional programming performs these tasks, and I/O tasks such as accepting user input and printing to the screen, in a different way.\n\nThe pure functional programming language Haskell implements them using monads, derived from category theory. Monads offer a way to abstract certain types of computational patterns, including (but not limited to) modeling of computations with mutable state (and other side effects such as I/O) in an imperative manner without losing purity. While existing monads may be easy to apply in a program, given appropriate templates and examples, many students find them difficult to understand conceptually, e.g., when asked to define new monads (which is sometimes needed for certain types of libraries).\n\nAnother way in which functional languages can simulate state is by passing around a data structure that represents the current state as a parameter to function calls. On each function call, a copy of this data structure is created with whatever differences are the result of the function. This is referred to as 'state-passing style'.\nImpure functional languages usually include a more direct method of managing mutable state. Clojure, for example, uses managed references that can be updated by applying pure functions to the current state. This kind of approach enables mutability while still promoting the use of pure functions as the preferred way to express computations.\nAlternative methods such as Hoare logic and uniqueness have been developed to track side effects in programs. Some modern research languages use effect systems to make the presence of side effects explicit.\nFunctional programming languages are typically less efficient in their use of CPU and memory than imperative languages such as C and Pascal. This is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware (which is a highly evolved Turing machine). Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex pointer chasing), or handled with SIMD instructions. It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree). However, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C according to The Computer Language Benchmarks Game. For programs that handle large matrices and multidimensional databases, array functional languages (such as J and K) were designed with speed optimizations.\n\nImmutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for inline expansion.\n\nLazy evaluation may also speed up the program, even asymptotically, whereas it may slow it down at most by a constant factor (however, it may introduce memory leaks if used improperly). Launchbury 1993 discusses theoretical issues related to memory leaks from lazy evaluation, and O'Sullivan \"et al.\" 2008 give some practical advice for analyzing and fixing them.\nHowever, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles) .\n\nImperative programs have the environment and a sequence of steps manipulating the environment. Functional programs have an expression that is successively substituted until it reaches normal form. An example illustrates this with different solutions to the same programming goal (calculating Fibonacci numbers).\n\nPrinting first 10 Fibonacci numbers, using function\n\nPrinting first 10 Fibonacci numbers, using closure\n\nPrinting a list with first 10 Fibonacci numbers, with generators\nPrinting first 10 Fibonacci numbers, iterative\n\nPrinting first 10 Fibonacci numbers, functional expression style\n\nPrinting a list with first 10 Fibonacci numbers, with generators\n\nPrinting a list with first 10 Fibonacci numbers, functional expression style\nPrinting first 10 Fibonacci numbers, functional expression style\n\nPrinting a list with first 10 Fibonacci numbers, functional expression style\n\nPrinting the 11th Fibonacci number, functional expression style\n\nPrinting the 11th Fibonacci number, functional expression style, tail recursive\n\nPrinting the 11th Fibonacci number, functional expression style with recursive lists\n\nPrinting the 11th Fibonacci number, functional expression style with primitives for recursive lists\n\nPrinting the 11th Fibonacci number, functional expression style with primitives for recursive lists, more concisely\n\nPrinting the 11th Fibonacci number, functional declaration style, tail recursive\nPrinting the 11th Fibonacci number, functional declaration style, using lazy infinite lists and primitives\nfibs = 1 : 1 : zipWith (+) fibs (tail fibs) \n-- an infinite list of the fibonacci numbers\n-- fibs is defined in terms of fibs\nfibonacci = (fibs !!)\nmain = putStrLn $ show $ fibonacci 11\nAs influenced by Haskell and others, Perl 6 has several functional and declarative approaches to problems. For example, you can declaratively build up a well-typed recursive version (the type constraints are optional) through signature pattern matching:\n\nAn alternative to this is to construct a lazy iterative sequence, which appears as an almost direct illustration of the sequence:\n\nErlang is a functional, concurrent, general-purpose programming language. A Fibonacci algorithm implemented in Erlang (Note: This is only for demonstrating the Erlang syntax. Use other algorithms for fast performance):\n\nElixir is a functional, concurrent, general-purpose programming language that runs on the Erlang virtual machine (BEAM).\n\nThe Fibonacci function can be written in Elixir as follows:\ndefmodule Fibonacci do\nend\nThe Fibonacci function can be written in Common Lisp as follows:\nor\n\nThe program can then be called as\nThe Fibonacci function can be written in Clojure as follows:\nThe program can then be called as\nExplicitly using \"lazy-seq\", the infinite sequence of Fibonacci numbers can be defined recursively. \n\nThe Fibonacci function can be written in Kotlin as follows:\nThe program can then be called as\n\nD has support for functional programming:\n\nR is an environment for statistical computing and graphics. It is also a functional programming language.\n\nThe Fibonacci function can be written in R as a recursive function as follows:\nOr it can be written as a singly recursive function:\nOr it can be written as an iterative function:\nThe function can then be called as\nSequenceL is a functional, concurrent, general-purpose programming language. The Fibonacci function can be written in SequenceL as follows:\nThe function can then be called as\nTo reduce the memory consumed by the call stack when computing a large Fibonacci term, a tail-recursive version can be used. A tail-recursive function is implemented by the SequenceL compiler as a memory-efficient looping structure:\nThe Fibonacci function can be written in Tcl as a recursive function as follows:\n\nThe Fibonacci function can be written in Scala in a bunch of ways:\n\nImperative \"Java\" style\nRecursive style, slow\nRecursive style, fast\nUsing Scala streams\nFunctional programming has long been popular in academia, but with few industrial applications. However, recently several prominent functional programming languages have been used in commercial or industrial systems. For example, the Erlang programming language, which was developed by the Swedish company Ericsson in the late 1980s, was originally used to implement fault-tolerant telecommunications systems. It has since become popular for building a range of applications at companies such as T-Mobile, Nortel, Facebook, Électricité de France and WhatsApp. The Scheme dialect of Lisp was used as the basis for several applications on early Apple Macintosh computers, and has more recently been applied to problems such as training simulation software and telescope control. OCaml, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis, driver verification, industrial robot programming, and static analysis of embedded software. Haskell, although initially intended as a research language, has also been applied by a range of companies, in areas such as aerospace systems, hardware design, and web programming.\n\nOther functional programming languages that have seen use in industry include Scala, F#, (both being functional-OO hybrids with support for both purely functional and imperative programming) Wolfram Language, Lisp, Standard ML and Clojure.\n\nFunctional programming is being used as a method to teach problem solving, algebra and geometric concepts.\nIt has also been used as a tool to teach classical mechanics in Structure and Interpretation of Classical Mechanics.\n\n\n\n"}
{"id": "10936", "url": "https://en.wikipedia.org/wiki?curid=10936", "title": "February 29", "text": "February 29\n\nFebruary 29, also known as leap day or leap year day, is a date added to most years that are divisible by 4, such as 2008, 2012, 2016, 2020, and 2024. A leap day is added in various solar calendars (calendars based on the Earth's revolution around the Sun), including the Gregorian calendar standard in most of the world. Lunisolar calendars (whose months are based on the phases of the Moon) instead add a leap or intercalary month.\n\nIn the Gregorian calendar, years that are divisible by 100, but not by 400, do not contain a leap day. Thus, 1700, 1800, and 1900 did not contain a leap day; neither will 2100, 2200, and 2300. Conversely, 1600 and 2000 did and 2400 will. Years containing a leap day are called leap years. Years not containing a leap day are called common years. February 29 is the 60th day of the Gregorian calendar, in such a year, with 306 days remaining until the end of the year. In the Chinese calendar, this day will only occur in years of the monkey, dragon, and rat.\n\nA leap day is observed because a complete revolution around the Sun takes approximately 6 hours longer than 365 days (8,760 hours). A leap day compensates for this lag, realigning the calendar with the Earth's position in the Solar System; otherwise, seasons would occur earlier than intended in the calendar year. The Julian calendar used in Christendom until the 16th century added a leap day every four years; but this rule adds too many days (roughly 3 every 400 years), making the equinoxes and solstices shift gradually to earlier dates. By the 16th century the vernal equinox had drifted to March 11, and the Gregorian calendar was introduced both to shift it back by omitting several days, and to reduce the number of leap years via the \"century rule\" to keep the equinoxes more or less fixed and the date of Easter consistently close to the vernal equinox.\n\nAlthough most modern calendar years have 365 days, a complete revolution around the Sun (one solar year) takes approximately 365 days and 6 hours. An extra 24 hours thus accumulates every four years, requiring that an extra calendar day be added to align the calendar with the Sun's apparent position. Without the added day, in future years the seasons would occur later in the calendar, eventually leading to confusion about when to undertake activities dependent on weather, ecology, or hours of daylight.\n\nA solar year is actually slightly shorter than 365 days and 6 hours (365.25 days). As early as the 13th century it was recognized that the year is shorter than the 365.25 days assumed by the Julian calendar: the Earth's orbital period around the Sun was derived from the medieval Alfonsine tables as 365 days, 5 hours, 49 minutes, and 16 seconds (365.2425 days). The currently accepted modern figure is 365 days, 5 hours, 48 minutes, 45 seconds. Adding a calendar day every four years, therefore, results in an excess of around 44 minutes for those four years, or about 3 days every 400 years. To compensate for this, three days are removed every 400 years. The Gregorian calendar reform implements this adjustment by making an exception to the general rule that there is a leap year every four years. Instead, a year divisible by 100 is not a leap year unless that year is also divisible by 400. This means that the years 1600, 2000, and 2400 are leap years, while the years 1700, 1800, 1900, 2100, 2200, 2300, and 2500 are common years.\n\nThe Gregorian calendar repeats itself every 400 years, which is exactly 20,871 weeks including 97 leap days. Over this period, February 29 falls on Sunday, Tuesday, and Thursday 13 times each; 14 times each on Friday and Saturday; and 15 times each on Monday and Wednesday. Excepting when a century mark that is not a multiple of 400 intervenes, consecutive leaps days fall in order Thursday, Tuesday, Sunday, Friday, Wednesday, Monday, and Saturday; then repeating with Thursday again.\n\nThe calendar of the Roman king Numa Pompilius had only 355 days (even though it was not a lunar calendar) which meant that it would quickly become unsynchronized with the solar year. An earlier Roman solution to this problem was to lengthen the calendar periodically by adding extra days to February, the last month of the year. February consisted of two parts, each with an odd number of days. The first part ended with the \"Terminalia\" on the 23rd, which was considered the end of the religious year, and the five remaining days formed the second part. To keep the calendar year roughly aligned with the solar year, a leap month, called \"Mensis Intercalaris\" (\"intercalary month\"), was added from time to time between these two parts of February. The (usual) second part of February was incorporated in the intercalary month as its last five days, with no change either in their dates or the festivals observed on them. This followed naturally, because the days after the Ides (13th) of February (in an ordinary year) or the Ides of Intercalaris (in an intercalary year) both counted down to the Kalends of March (i.e. they were known as \"the \"n\"th day before the Kalends of March\"). The Nones (5th) and Ides of Intercalaris occupied their normal positions.\n\nThe third-century writer Censorinus says:When it was thought necessary to add (every two years) an intercalary month of 22 or 23 days, so that the civil year should correspond to the natural (solar) year, this intercalation was in preference made in February, between Terminalia [23rd] and Regifugium [24th].\n\nThe set leap day was introduced in Rome as a part of the Julian reform in the 1st century BC. As before, the intercalation was made after February 23. The day following the Terminalia (February 23) was doubled, forming the \"\"bis sextum\"\"—literally 'twice sixth', since February 24 was 'the sixth day before the Kalends of March' using Roman inclusive counting (March 1 was the Kalends of March and was also the first day of the calendar year). Inclusive counting initially caused the Roman priests to add the extra day every three years instead of four; Augustus was compelled to omit leap years for a few decades to return the calendar to its proper position. Although there were exceptions, the first day of the \"bis sextum\" (February 24) was usually regarded as the intercalated or \"bissextile\" day since the 3rd century AD. February 29 came to be regarded as the leap day when the Roman system of numbering days was replaced by sequential numbering in the late Middle Ages, although this has only been formally enacted in Sweden and Finland. In Britain, the extra day added to leap years remains notionally the 24th, although the 29th remains more visible on the calendar.\n\nA person born on February 29 may be called a \"leapling\", a \"leaper\", or a \"leap-year baby\". In non-leap years, some leaplings celebrate their birthday on either February 28 or March 1, while others only observe birthdays on the authentic intercalary date, February 29.\n\nThe effective legal date of a leapling's birthday in non-leap years varies between jurisdictions.\n\nIn the United Kingdom and Hong Kong, when a person born on February 29 turns 18, they are considered to have their birthday on March 1 in the relevant year.\nIn New Zealand, a person born on February 29 is deemed to have their birthday on February 28 in non-leap years, for the purposes of Driver Licensing under §2(2) of the Land Transport (Driver Licensing) Rule 1999. The net result is that for drivers aged 75, or over 80, their driver licence expires at the end of the last day of February, even though their birthday would otherwise fall on the first day in March in non-leap years. Otherwise, New Zealand legislation is silent on when a person born on 29 February has their birthday, although case law would suggest that age is computed based on the number of years elapsed, from the day after the date of birth, and that the person's birth day then occurs on the last day of the year period. This differs from English common law where a birthday is considered to be the start of the next year, the preceding year ending at midnight on the day preceding the birthday. While a person attains the same age on the same day, it also means that, in New Zealand, if something must be done by the time a person attains a certain age, that thing can be done on the birthday that they attain that age and still be lawful.\n\nIn Taiwan (Republic of China), the legal birthday of a leapling is February 28 in common years:\n\nThus, in England and Wales or in Hong Kong, a person born on February 29 will have legally reached 18 years old on March 1. If they were born in Taiwan they legally become 18 on February 28, a day earlier. In the United States, according to John Reitz, a professor of law at the University of Iowa, there is no \"... statute or general rule that has anything to do with leap day.\" Reitz speculates that \"March 1 would likely be considered the legal birthday in non-leap years of someone born on leap day,\" using the same reasoning as described for the United Kingdom and Hong Kong.\n\nThere are many instances in children's literature where a person's claim to be only a quarter of their actual age turns out to be based on counting their leap-year birthdays.\n\nA similar device is used in the plot of Gilbert and Sullivan's 1879 comic opera \"The Pirates of Penzance\". As a child, Frederic was apprenticed to a band of pirates until his 21st birthday. Having passed his 21st year, he leaves the pirate band and falls in love. However, since he was born on February 29, his 21st \"birthday\" will not arrive until he is eighty-four, so he must leave his fiancée and return to the pirates.\n\n\n\n\n\nThere is a popular tradition known as Bachelor's Day in some countries allowing a woman to propose marriage to a man on February 29. If the man refuses, he then is obliged to give the woman money or buy her a dress. In upper-class societies in Europe, if the man refuses marriage, he then must purchase 12 pairs of gloves for the woman, suggesting that the gloves are to hide the woman's embarrassment of not having an engagement ring. In Ireland, the tradition is supposed to originate from a deal that Saint Bridget struck with Saint Patrick.\n\nIn the town of Aurora, Illinois, single women are deputized and may arrest single men, subject to a four-dollar fine, every February 29.\n\nIn Greece, it is considered unlucky to marry on a leap day.\n\n"}
{"id": "10937", "url": "https://en.wikipedia.org/wiki?curid=10937", "title": "Francis Scott Key", "text": "Francis Scott Key\n\nFrancis Scott Key (August 1, 1779January 11, 1843) was an American lawyer, author, and amateur poet from Frederick, Maryland and later Georgetown, D.C., near Washington, D.C. who wrote the lyrics for a poem entitled at first \"The Defence of Fort McHenry\", which when set to an old English gentlemens' society tune, eventually became the United States' national anthem, \"The Star-Spangled Banner\".\n\nFrancis Scott Key was born to Ann Phoebe Penn Dagworthy (Charlton) and Captain John Ross Key at the family plantation Terra Rubra in what was then part of Frederick County, now Carroll County, Maryland. His father was a lawyer, judge, and officer in the Continental Army. His great-grandparents on his father's side, Philip Key and Susanna Barton Gardiner, were both born in London and then immigrated to Maryland in 1726.\n\nKey graduated from St.John's College, Annapolis, Maryland in 1796, and \"read the law\" under an uncle, Philip Barton Key who was (along with his wife) loyal to the British Crown during the War of Independence. He married Mary Tayloe Lloyd on January 1, 1802.\n\nDuring the War of 1812, Key, accompanied by the British Prisoner Exchange Agent Colonel John Stuart Skinner, dined aboard the British ship HMS \"Tonnant\" as the guests of three British officers: Vice Admiral Alexander Cochrane, Rear Admiral George Cockburn, and Major General Robert Ross. Skinner and Key were there to negotiate the release of prisoners, one of whom was Dr.William Beanes, a resident of Upper Marlboro, Maryland, who had been arrested after jailing marauding British troops who were looting local farms. Skinner, Key, and Beanes were not allowed to return to their own sloop because they had become familiar with the strength and position of the British units and with the British intent to attack Baltimore. Thus, Key was unable to do anything but watch the bombarding of the American forces at Fort McHenry during the Battle of Baltimore on the night of September 1314,1814.\n\nAt dawn, Key was able to see an American flag still waving. Back in Baltimore and inspired, Key wrote a poem about his experience, \"Defence of Fort M'Henry\", which was soon published in William Pechin's \"American and Commercial Daily Advertiser\" on September21,1814. He took it to Thomas Carr, a music publisher, who adapted it to the rhythms of composer John Stafford Smith's \"To Anacreon in Heaven\", a popular tune Key had already used as a setting for his 1805-song \"When the Warrior Returns\", celebrating U.S. heroes of the First Barbary War. (Key used the \"star-spangled\" flag imagery in the earlier song.) It has become better known as \"The Star-Spangled Banner\". Though somewhat difficult to sing, it became increasingly popular, competing with \"Hail, Columbia\" (1796) as the de facto national anthem by the time of the Mexican–American War and American Civil War. More than a century after its first publication, the song was adopted as the American national anthem, first by an Executive Order from President Woodrow Wilson in1916 (which had little effect beyond requiring military bands to play what became known as the \"Service Version\") and then by a Congressional resolution in1931, signed by President Herbert Hoover.\n\nKey was a leading attorney in Frederick, Maryland, and Washington, D.C. for many years, with an extensive real estate as well as trial practice. He and his family settled in Georgetown in 1805 or 1806, near the new national capital. There the young Key assisted his uncle, the prominent lawyer Philip Barton Key, such as in the sensational conspiracy trial of Aaron Burr and the expulsion of Senator John Smith of Ohio. He made the first of his many arguments before the United States Supreme Court in 1807. In 1808 he assisted President Thomas Jefferson's attorney general in \"United Statesv.Peters.\"\n\nIn 1829, Key, a supporter of Andrew Jackson, assisted in the prosecution of Tobias Watkins, former U.S.Treasury auditor under former President John Quincy Adams for misappropriating public monies. He also handled the Petticoat affair concerning Secretary of War John Eaton, who had married a widowed saloonkeeper. In 1832, he served as the attorney for Sam Houston, then a former U.S. Representative and Governor of Tennessee, during his trial for assaulting Representative William Stanbery of Ohio.\n\nPresident Jackson nominated Key for United States Attorney for the District of Columbia in 1833. After the U.S. Senate approved the nomination, he served from 1833 to 1841, while also handling his own private legal cases. In 1835, in his most famous case, he prosecuted Richard Lawrence for his unsuccessful attempt to assassinate President Andrew Jackson at the entrance doors and top steps of the Capitol, the first attempt to kill an American chief executive.\n\nKey purchased his first slave in 1800 or 1801 and owned six slaves in 1820. Mostly in the 1830s, Key manumitted (set free) seven slaves, one of whom (Clem Johnson) continued to work for him for wages as his farm's foreman, supervising several slaves.\n\nThroughout his career Key also represented several slaves seeking their freedom in court (for free), as well as several masters seeking return of their runaway slaves. Key, Judge William Leigh of Halifax, and bishop William Meade were administrators of the will of their friend John Randolph of Roanoke, who died without children and left a will directing his executors to free his more than four hundred slaves. Over the next decade, beginning in 1833, the administrators fought to enforce the will and provide the freed slaves land to support themselves.\n\nKey publicly criticized slavery's cruelties, so much that after his death a newspaper editorial stated \"So actively hostile was he to the peculiar institution that he was called 'The Nigger Lawyer' ... because he often volunteered to defend the downtrodden sons and daughters of Africa. Mr.Key convinced me that slavery was wrong—radically wrong.\" In June 1842, Key attended the funeral of William Costin, a free, mixed race resident who had challenged Washington's surety bond laws.\n\nKey was a founding member and active leader of the American Colonization Society and its predecessor, the influential Maryland branch, the primary goal of which was to send free African-Americans back to Africa.\nHowever, he was removed from the board in 1833 as its policies shifted toward abolitionist.\n\nA slave-owner himself, Key used his position as U.S. Attorney to suppress abolitionists. In 1833, he secured a grand jury indictment against Benjamin Lundy, editor of the anti-slavery publication, the \"Genius of Universal Emancipation\", and his printer, William Greer, for libel after Lundy published an article that declared, \"There is neither mercy nor justice for colored people in this district [of Columbia]\". Lundy's article, Key said in the indictment, \"was intended to injure, oppress, aggrieve, and vilify the good name, fame, credit & reputation of the Magistrates and constables\" of Washington. Lundy left town rather than face trial; Greer was acquitted.\n\nIn August 1836, Key agreed to prosecute botanist and doctor Reuben Crandall, brother of controversial Connecticut school teacher Prudence Crandall, who had recently moved to the national capital. Key secured an indictment for \"seditious libel\" after two marshals (who operated as slave catchers in their off hours) found Crandall had a trunk full of anti-slavery publications in his Georgetown residence, five days after the Snow Riot, caused by rumors that a mentally ill slave had attempted to kill an elderly white woman. In an April 1837 trial that attracted nationwide attention, Key charged that Crandall's actions instigated slaves to rebel. Crandall's attorneys acknowledged he opposed slavery, but denied any intent or actions to encourage rebellion. Key, in his final address to the jury said: A jury acquitted Crandall.\n\nThis defeat, as well as family tragedies in 1835, diminished Key's political ambition. He resigned as district attorney in 1840. He remained a staunch proponent of African colonization and a strong critic of the antislavery movement until his death.\n\nKey was a devout and prominent Episcopalian. In his youth, he almost became an Episcopal priest rather than a lawyer. Throughout his life he sprinkled biblical references in his correspondence. He was active in All Saints Parish in Frederick, Maryland, near his family's home. He also helped found or financially support several parishes in the new national capital, including St. John's Church in Georgetown and Christ Church in Alexandria.\n\nFrom 1818 until his death in 1843, Key was associated with the American Bible Society. He successfully opposed an abolitionist resolution presented to that group around 1838.\n\nKey also helped found two Episcopal seminaries, one in Baltimore and the other across the Potomac River in Alexandria, Virginia (the Virginia Theological Seminary). Key also published a prose work called \"The Power of Literature, and Its Connection with Religion\" in 1834.\n\nOn January 11, 1843, Key died at the home of his daughter Elizabeth Howard in Baltimore from pleurisy at age 63. He was initially interred in Old Saint Paul's Cemetery in the vault of John Eager Howard but in 1866, his body was moved to his family plot in Frederick at Mount Olivet Cemetery.\n\nThe Key Monument Association erected a memorial in 1898 and the remains of both Francis Scott Key and his wife, Mary Tayloe Lloyd, were placed in a crypt in the base of the monument.\n\nDespite several efforts to preserve it, the Francis Scott Key residence was ultimately dismantled in1947. The residence had been located at 351618MStreet in Georgetown.\n\nThough Key had written poetry from time to time, often with heavily religious themes, these works were not collected and published until 14years after his death. Two of his religious poems used as Christian hymns include \"Before the Lord We Bow\" and \"Lord, with Glowing Heart I'd Praise Thee\".\n\nIn1806, Key's sister, Anne Phoebe Charlton Key, married Roger B. Taney, who would later become Chief Justice of the United States. In 1846 one daughter, Alice, married U.S. Senator George H. Pendleton and another, Ellen Lloyd, married Simon F. Blunt. In1859 Key's son Philip Barton Key II was shot and killed by Daniel Sicklesa U.S.Representative from New York who would serve as a general in the American Civil Warafter he discovered that Philip Barton Key was having an affair with his wife. Sickles was acquitted in the first use of the temporary insanity defense. In1861 Key's grandson Francis Key Howard was imprisoned in Fort McHenry with the Mayor of Baltimore George William Brown and other locals deemed pro-South.\n\nKey was a distant cousin and the namesake of F. Scott Fitzgerald, whose full name was Francis Scott Key Fitzgerald. His direct descendants include geneticist Thomas Hunt Morgan, guitarist Dana Key, and American fashion designer and socialite Pauline de Rothschild.\n\n\n\n"}
{"id": "10938", "url": "https://en.wikipedia.org/wiki?curid=10938", "title": "FSU", "text": "FSU\n\nFSU may refer to:\n\n"}
{"id": "10939", "url": "https://en.wikipedia.org/wiki?curid=10939", "title": "Formal language", "text": "Formal language\n\nIn mathematics, computer science, and linguistics, a formal language is a set of strings of symbols together with a set of rules that are specific to it.\n\nThe alphabet of a formal language is the set of symbols, letters, or tokens from which the strings of the language may be formed. The strings formed from this alphabet are called words, and the words that belong to a particular formal language are sometimes called \"well-formed words\" or \"well-formed formulas\". A formal language is often defined by means of a formal grammar such as a regular grammar or context-free grammar, also called its formation rule.\n\nThe field of formal language theory studies primarily the purely syntactical aspects of such languages—that is, their internal structural patterns. Formal language theory sprang out of linguistics, as a way of understanding the syntactic regularities of natural languages.\nIn computer science, formal languages are used among others as the basis for defining the grammar of programming languages and formalized versions of subsets of natural languages in which the words of the language represent concepts that are associated with particular meanings or semantics. In computational complexity theory, decision problems are typically defined as formal languages, and complexity classes are defined as the sets of the formal languages that can be parsed by machines with limited computational power. In logic and the foundations of mathematics, formal languages are used to represent the syntax of axiomatic systems, and mathematical formalism is the philosophy that all of mathematics can be reduced to the syntactic manipulation of formal languages in this way.\n\nThe first formal language is thought to be the one used by Gottlob Frege in his \"Begriffsschrift\" (1879), literally meaning \"concept writing\", and which Frege described as a \"formal language of pure thought.\"\n\nAxel Thue's early semi-Thue system, which can be used for rewriting strings, was influential on formal grammars.\n\nAn alphabet, in the context of formal languages, can be any set, although it often makes sense to use an alphabet in the usual sense of the word, or more generally a character set such as ASCII or Unicode. The elements of an alphabet are called its letters. Alphabets may be infinite; however, most definitions in formal language theory specify finite alphabets, and most results only apply to them.\n\nA word over an alphabet can be any finite sequence (i.e., string) of letters. The set of all words over an alphabet Σ is usually denoted by Σ (using the Kleene star). The length of a word is the number of letters it is composed of. For any alphabet there is only one word of length 0, the \"empty word\", which is often denoted by e, ε, λ or even Λ. By concatenation one can combine two words to form a new word, whose length is the sum of the lengths of the original words. The result of concatenating a word with the empty word is the original word.\n\nIn some applications, especially in logic, the alphabet is also known as the \"vocabulary\" and words are known as \"formulas\" or \"sentences\"; this breaks the letter/word metaphor and replaces it by a word/sentence metaphor.\n\nA formal language \"L\" over an alphabet Σ is a subset of Σ, that is, a set of words over that alphabet. Sometimes the sets of words are grouped into expressions, whereas rules and constraints may be formulated for the creation of 'well-formed expressions'.\n\nIn computer science and mathematics, which do not usually deal with natural languages, the adjective \"formal\" is often omitted as redundant.\n\nWhile formal language theory usually concerns itself with formal languages that are described by some syntactical rules, the actual definition of the concept \"formal language\" is only as above: a (possibly infinite) set of finite-length strings composed from a given alphabet, no more nor less. In practice, there are many languages that can be described by rules, such as regular languages or context-free languages. The notion of a formal grammar may be closer to the intuitive concept of a \"language,\" one described by syntactic rules. By an abuse of the definition, a particular formal language is often thought of as being equipped with a formal grammar that describes it.\n\nThe following rules describe a formal language  over the alphabet Σ = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, = }:\nUnder these rules, the string \"23+4=555\" is in , but the string \"=234=+\" is not. This formal language expresses natural numbers, well-formed additions, and well-formed addition equalities, but it expresses only what they look like (their syntax), not what they mean (semantics). For instance, nowhere in these rules is there any indication that \"0\" means the number zero, or that \"+\" means addition.\n\nFor finite languages one can explicitly enumerate all well-formed words. For example, we can describe a language  as just  = {\"a\", \"b\", \"ab\", \"cba\"}. The degenerate case of this construction is the empty language, which contains no words at all ( = ∅).\n\nHowever, even over a finite (non-empty) alphabet such as Σ = {a, b} there are an infinite number of finite-length words that can potentially be expressed: \"a\", \"abb\", \"ababba\", \"aaababbbbaab\", …. Therefore, formal languages are typically infinite, and describing an infinite formal language is not as simple as writing \"L\" = {\"a\", \"b\", \"ab\", \"cba\"}. Here are some examples of formal languages:\n\nFormal languages are used as tools in multiple disciplines. However, formal language theory rarely concerns itself with particular languages (except as examples), but is mainly concerned with the study of various types of formalisms to describe languages. For instance, a language can be given as\n\nTypical questions asked about such formalisms include:\n\n\nSurprisingly often, the answer to these decision problems is \"it cannot be done at all\", or \"it is extremely expensive\" (with a characterization of how expensive). Therefore, formal language theory is a major application area of computability theory and complexity theory. Formal languages may be classified in the Chomsky hierarchy based on the expressive power of their generative grammar as well as the complexity of their recognizing automaton. Context-free grammars and regular grammars provide a good compromise between expressivity and ease of parsing, and are widely used in practical applications.\n\nCertain operations on languages are common. This includes the standard set operations, such as union, intersection, and complement. Another class of operation is the element-wise application of string operations.\n\nExamples: suppose \"L\" and \"L\" are languages over some common alphabet.\n\nSuch string operations are used to investigate closure properties of classes of languages. A class of languages is closed under a particular operation when the operation, applied to languages in the class, always produces a language in the same class again. For instance, the context-free languages are known to be closed under union, concatenation, and intersection with regular languages, but not closed under intersection or complement. The theory of trios and abstract families of languages studies the most common closure properties of language families in their own right.\n\nA compiler usually has two distinct components. A lexical analyzer, generated by a tool like codice_1, identifies the tokens of the programming language grammar, e.g. identifiers or keywords, which are themselves expressed in a simpler formal language, usually by means of regular expressions. At the most basic conceptual level, a parser, usually generated by a parser generator like codice_2, attempts to decide if the source program is valid, that is if it belongs to the programming language for which the compiler was built.\n\nOf course, compilers do more than just parse the source code — they usually translate it into some executable format. Because of this, a parser usually outputs more than a yes/no answer, typically an abstract syntax tree. This is used by subsequent stages of the compiler to eventually generate an executable containing machine code that runs directly on the hardware, or some intermediate code that requires a virtual machine to execute.\n\nIn mathematical logic, a \"formal theory\" is a set of sentences expressed in a formal language.\n\nA \"formal system\" (also called a \"logical calculus\", or a \"logical system\") consists of a formal language together with a deductive apparatus (also called a \"deductive system\"). The deductive apparatus may consist of a set of transformation rules, which may be interpreted as valid rules of inference, or a set of axioms, or have both. A formal system is used to derive one expression from one or more other expressions. Although a formal language can be identified with its formulas, a formal system cannot be likewise identified by its theorems. Two formal systems formula_1 and formula_2 may have all the same theorems and yet differ in some significant proof-theoretic way (a formula A may be a syntactic consequence of a formula B in one but not another for instance).\n\nA \"formal proof\" or \"derivation\" is a finite sequence of well-formed formulas (which may be interpreted as propositions) each of which is an axiom or follows from the preceding formulas in the sequence by a rule of inference. The last sentence in the sequence is a theorem of a formal system. Formal proofs are useful because their theorems can be interpreted as true propositions.\n\nFormal languages are entirely syntactic in nature but may be given semantics that give meaning to the elements of the language. For instance, in mathematical logic, the set of possible formulas of a particular logic is a formal language, and an interpretation assigns a meaning to each of the formulas—usually, a truth value.\n\nThe study of interpretations of formal languages is called formal semantics. In mathematical logic, this is often done in terms of model theory. In model theory, the terms that occur in a formula are interpreted as objects within mathematical structures, and fixed compositional interpretation rules determine how the truth value of the formula can be derived from the interpretation of its terms; a \"model\" for a formula is an interpretation of terms such that the formula becomes true.\n\n\n"}
{"id": "10940", "url": "https://en.wikipedia.org/wiki?curid=10940", "title": "Free to Choose", "text": "Free to Choose\n\nFree to Choose (1980) is a book and a ten-part television series broadcast on public television by economists Milton and Rose D. Friedman that advocates free market principles. It was primarily a response to an earlier landmark book and television series: \"The Age of Uncertainty\", by the noted economist John Kenneth Galbraith. Milton Friedman won the Nobel Memorial Prize in Economics in 1976.\n\n\"Free to Choose: A Personal Statement\" maintains that the free market works best for all members of a society, provides examples of how the free market engenders prosperity, and maintains that it can solve problems where other approaches have failed. Published in January 1980, the 297 page book contains 10 chapters. The book was on the United States best sellers list for 5 weeks.\n\nPBS telecast the series, beginning in January 1980. The general format was that of Dr. Friedman visiting and narrating a number of success and failure stories in history, which he attributes to capitalism or the lack thereof (e.g. Hong Kong is commended for its free markets, while India is excoriated for relying on centralized planning especially for its protection of its traditional textile industry). Following the primary show, Dr. Friedman would engage in discussion moderated by Robert McKenzie with a number of selected debaters drawn from trade unions, academy and the business community, such as Donald Rumsfeld (then of G.D. Searle & Company) and Frances Fox Piven of City University of New York. The interlocutors would offer objections to or support for the proposals put forward by Friedman, who would in turn respond. After the final episode, Friedman sat down for an interview with Lawrence Spivak.\n\nThe series was rebroadcast in 1990 with Linda Chavez moderating the episodes. Arnold Schwarzenegger, Ronald Reagan, Steve Allen and others give personal introductions for each episode in the series. This time, after the documentary part, Friedman sits down with a single opponent to debate the issues raised in the episode.\n\nGuest debaters included:\n\nThe Friedmans advocate \"laissez-faire\" economic policies, often criticizing interventionist government policies and their cost in personal freedoms and economic efficiency in the United States and abroad. The authors argue against government taxation on gas and tobacco and government regulation of the public school systems. The Friedmans argue that the Federal Reserve exacerbated the Great Depression by neglecting to prevent the decline of the money supply in the years leading up to it.\n\nOn the subject of welfare, the Friedmans argue that current welfare practices are creating \"wards of the state\" as opposed to \"self-reliant individuals\" and suggest a negative income tax as a less harmful alternative. The Friedmans also argue for abolishing the Food and Drug Administration, tighter control of Fed money supply, and the repeal of laws favoring labor unions.\n\n\n\n\n"}
{"id": "10945", "url": "https://en.wikipedia.org/wiki?curid=10945", "title": "Melbourne Grand Prix Circuit", "text": "Melbourne Grand Prix Circuit\n\nThe Melbourne Grand Prix Circuit is a street circuit around Albert Park Lake, only a few kilometres south of central Melbourne. It is used annually as a racetrack for the Formula One Australian Grand Prix, Supercars Challenge and associated support races. The circuit has FIA Grade 1 licence. In spite of being a circuit on public roads it has characteristics of a natural road course considering it being fast and flowing combined with extensive runoff in many corners.\n\nThe Australian Grand Prix has always been more of a promoter event than a profit-raiser in itself. The contract was prolonged until 2020, although tobacco advertising has been banned since 2007.\n\nThe circuit uses everyday sections of road that circle Albert Park Lake, a small man-altered lake (originally a large lagoon formed as part of the ancient Yarra River course) just south of the Central Business District of Melbourne. The road sections that are used were rebuilt prior to the inaugural event in 1996 to ensure consistency and smoothness. As a result, compared to other circuits that are held on public roads, the Albert Park track has quite a smooth surface. Before 2007 there existed only a few other places on the Formula 1 calendar with a body of water close to the track. Many of the new tracks, such as Valencia, Singapore and Abu Dhabi have imitated that feature.\n\nThe course is considered to be quite fast and relatively easy to drive, drivers having commented that the consistent placement of corners allows them to easily learn the circuit and achieve competitive times. However, the flat terrain around the lake, coupled with a track design that features few true straights, means that the track is not conducive to overtaking or easy spectating unless in possession of a grandstand seat.\n\nEach year, most of the trackside fencing, pedestrian overpasses, grandstands and other motorsport infrastructure are erected approximately two months prior to the Grand Prix weekend and removed within 6 weeks after the event. Land around the circuit (including a large aquatic centre, a golf course, a Lakeside Stadium, some restaurants and rowing boathouses) has restricted access during the grand prix weekend. Dissent is still prevalent among nearby local residents and users of those others facilities, and some still maintain a silent protest against the event. Nevertheless, the event is reasonably popular in Melbourne and Australia (with a large European population and a general interest in motorsport). Middle Park, the home of South Melbourne FC was demolished in 1994 due to expansion at Albert Park.\n\nOn 4 July 2008, the official F1 site reported that more than 300,000 people attended the four-day Melbourne Grand Prix, though actual ticket sales were later disputed by the local media. The Grand Prix will continue until at least 2020 after securing a new contract with Formula One Management. There has never been a night race at Albert Park, however, 2009’s event started at 5.00 p.m.\n\nDuring the nine months of the year when the track is not required for Grand Prix preparation or the race weekend, most of the track can be driven by ordinary street-registered vehicles either clockwise or anti-clockwise.\n\nOnly the sections between turns 3, 4 and 5, then 5 and 6, differ significantly from the race track configuration. Turn 4 is replaced by a car park access road running directly from turns 3 to 5. Between turns 5 and 6, the road is blocked. It is possible to drive from turn 5 on to Albert Road and back on to the track at turn 7 though two sets of lights control the flow of this option. The only set of lights on the actual track is halfway between turns 12 and 13, where drivers using Queens Road are catered for. The chicanes at turns 11 and 12 is considerably more open than that used in the grand prix, using the escape roads. Turn 9 is also a car park and traffic is directed down another escape road.\n\nThe speed limit is generally which is slower than an F1 car under pit lane speed restrictions. Some short sections have a speed limit of . The back of the track, turns 7 to 13 inclusive, is known as Lakeside Drive. Double lines separate the two-way traffic along most of Lakeside Drive with short road islands approximately every 50 metres. This means overtaking is illegal here.\n\nApproximately 50% of the track edge is lined with short parkland-style chain-linked fencing leaving normal drivers less room for error than F1 drivers have during race weekend. There is however substantial shoulder room between the outside of each lane and the fencing.\n\nA major milestone for the Carrera Cup racing was achieved in Australia with the 100th round of the Porsche series utilizing four generations of the 911 GT3 race cars. 50 drivers have stood on the Carrera Cup podium over the 13 seasons and 294 races. Alex Davison took first place in the final race and Cameron McConville came second.\n\nlargest field of Level 1 race cars in Australia, 35 GT3 high end exotic cars compete in the fastest car race other than Formula 1 at Albert Park. THis Australian GT Championship season opener was live streamed for the first time on mobile devices, the streaming technology has been utilised in an attempt to increase the championship’s growth.\n\nThe V8 Supercars have been a highlight of the supporting categories in all Melbourne Grand Prix every year except 2007 since 1997. For the past few years the Australian Grand Prix Corporation has been working towards gaining championship status for the Supercars race, at the last moment the bid was dropped but the organisers have vowed to secure it next year. Chaz Mostert won the 2017 race with the Supercheap Auto FG X Falcon race car on Sunday 26 March.\n\nThis original event featured three different cars, the Minardi 2-seater F1 car driven by Will Davison, Peter Hackett in a Mercedes GT and Mick Doohan in a Mercedes road car compete in which car completes the lap with the greatest margin over the others. MSS Security one of Australia's largest security companies not only sponsors this race but they also provided the security personnel for the 2017 Melbourne Grand Prix.\n\nAlbert Park has the distinction of being the only venue to host the Australian Grand Prix in both World Championship and non-World Championship formats with an earlier configuration of the current circuit used for the race on two occasions during the 1950s. During this time racing was conducted in an anti-clockwise direction as opposed to the current circuit which runs clockwise.\n\nKnown as the Albert Park Circuit, the original 3.125 mile (5.03 kilometre) course hosted a total of six race meetings:\n\n\nAs of 14 March 2015.\n\n"}
{"id": "10946", "url": "https://en.wikipedia.org/wiki?curid=10946", "title": "Monaco Grand Prix", "text": "Monaco Grand Prix\n\nThe Monaco Grand Prix () is a Formula One motor race held each year on the Circuit de Monaco. Run since 1929, it is widely considered to be one of the most important and prestigious automobile races in the world and, with the Indianapolis 500 and the 24 Hours of Le Mans, forms the Triple Crown of Motorsport. The circuit has been called \"an exceptional location of glamour and prestige\".\n\nThe race is held on a narrow course laid out in the streets of Monaco, with many elevation changes and tight corners as well as a tunnel, making it one of the most demanding tracks in Formula One. In spite of the relatively low average speeds, it is a dangerous place to race and often involves the intervention of a safety car. It is the only Grand Prix that does not adhere to the FIA's mandated minimum race distance.\n\nThe event was part of the pre-Second World War European Championship and was included in the first World Championship of Drivers in 1950. It was designated the European Grand Prix two times, 1955 and 1963, when this title was an honorary designation given each year to one Grand Prix race in Europe. Graham Hill was known as \"\"Mr. Monaco\"\" due to his five Monaco wins in the 1960s. Brazil's Ayrton Senna won the race more times than any other driver, with six victories, winning five races consecutively between 1989 and 1993. Fernando Alonso is the only driver to have won the race in consecutive years for different constructors, winning for Renault in 2006 and McLaren in 2007.\n\nLike many European races, the Monaco Grand Prix predates the current World Championship. The principality's first Grand Prix was organised in 1929 by Antony Noghès, under the auspices of Prince Louis II, through the Automobile Club de Monaco (ACM), of which he was president. The ACM organized the Rallye Automobile Monte Carlo, and in 1928 applied to the \"Association Internationale des Automobiles Clubs Reconnus\" (AIACR), the international governing body of motorsport, to be upgraded from a regional French club to full national status. Their application was refused due to the lack of a major motorsport event held wholly within Monaco's boundaries. The rally could not be considered as it mostly used the roads of other European countries.\n\nTo attain full national status, Noghès proposed the creation of an automobile Grand Prix in the streets of Monte Carlo. He obtained the official sanction of Prince Louis II, and the support of Monegasque \"Grand Prix\" driver Louis Chiron. Chiron thought Monaco's topography well-suited to setting up a race track.\n\nThe first race, held on 14 April 1929, was won by William Grover-Williams (using the pseudonym \"Williams\"), driving a works Bugatti Type 35B. It was an invitation-only event, but not all of those invited decided to attend. The leading Maserati and Alfa Romeo drivers decided not to compete, but Bugatti was well represented. Mercedes sent their leading driver, Rudolf Caracciola. Starting fifteenth, Caracciola drove a fighting race, taking his SSK into the lead before wasting 4½ minutes on refuelling and a tire change to finish second. Another driver who competed using a pseudonym was \"Georges Philippe\", the Baron Philippe de Rothschild. Chiron was unable to compete, having a prior commitment to compete in the Indianapolis 500 on the same day.\n\nCaracciola's SSK was refused permission to race the following year, but Chiron did compete (in the works Bugatti Type 35C), when he was beaten by privateer René Dreyfus and his Bugatti Type 35B, and finished second. Chiron took victory in the 1931 race driving a Bugatti. , he remains the only native of Monaco to have won the event.\n\nThe race quickly grew in importance. Because of the large number of races which were being termed 'Grands Prix', the AIACR formally recognised the most important race of each of its affiliated national automobile clubs as International Grands Prix, or \"Grandes Épreuves\", and in 1933 Monaco was ranked as such alongside the French, Belgian, Italian, and Spanish Grands Prix. That year's race was the first Grand Prix where grid positions were decided, as they are now, by practice time rather than the established method of balloting. The race saw Achille Varzi and Tazio Nuvolari exchange the lead many times before being settled in Varzi's favour on the final lap when Nuvolari's car caught fire. The race became a round of the new European Championship in 1936, when stormy weather and a broken oil line led to a series of crashes, eliminating the Mercedes-Benzes of Chiron, Fagioli, and von Brauchitsch, as well as Bernd Rosemeyer's \"Typ C\" for newcomer Auto Union; Rudolf Caracciola, proving the truth of his nickname, \"Regenmeister\" (Rainmaster), went on to win. In 1937, von Brauchitsch duelled Caracciola before coming out on top. It was the last prewar \"Grand Prix\" at Monaco, for in 1938, the demand for £500 (about US$2450) in appearance money per top entrant led AIACR to cancel the event, while looming war overtook it in 1939, and the Second World War ended organised racing in Europe until 1945.\n\nRacing in Europe started again on 9 September 1945 at the Bois de Boulogne Park in the city of Paris, four months and one day after the end of the war in Europe. In 1946 a new premier racing category, Grand Prix, was defined by the Fédération Internationale de l'Automobile (FIA), the successor of the AIACR, based on the pre-war voiturette class. A Monaco Grand Prix was run to this formula in 1948, won by the future world champion Nino Farina in a Maserati 4CLT.\n\nThe 1949 event was cancelled due to the death of Prince Louis II; it was included in the new Formula One World Drivers' Championship the following year. The race provided future five-time world champion Juan Manuel Fangio with his first win in a World Championship race, as well as third place for the 51-year-old Louis Chiron, his best result in the World Championship era. However, there was no race in 1951. In 1952, the first of the two years in which the World Drivers' Championship was run to less powerful Formula Two regulations, the race was run to sports car rules instead, and it did not form part of the World Championship. There was no race held in 1953 and 1954 either.\n\nSince 1955, the Monaco Grand Prix has continuously been part of the Formula One World Championship. That year, Maurice Trintignant won in Monte Carlo for the first time and Chiron again scored points and at 56 became the oldest driver to compete in a Formula One Grand Prix. It was not until 1957, when Fangio won again, that the Grand Prix saw a double winner. Between 1954 and 1961 Fangio's former Mercedes colleague, Stirling Moss, went one better, as did Trintignant, who won the race again in 1958 driving a Cooper. The 1961 race saw Moss fend off three works Ferrari 156s in a year-old privateer Rob Walker Racing Team Lotus 18, to take his third Monaco victory.\n\nBritain's Graham Hill won the race five times in the 1960s and became known as \"\"King of Monaco\"\" and \"\"Mr. Monaco\"\". He first won in 1963, and then won the next two years. In the 1965 race he took pole position and led from the start, but went up an escape road on lap 25 to avoid hitting a slow backmarker. Re-joining in fifth place, Hill set several new lap records on the way to winning. The race was also notable for Jim Clark's absence (he was doing the Indianapolis 500), and for Paul Hawkins's Lotus ending up in the harbour. A similar incident was included in the 1966 film \"Grand Prix\". Hill's teammate, Briton Jackie Stewart, won in 1966 and New Zealander Denny Hulme won in 1967, but Hill won the next two years, the 1969 event being his final Formula One championship victory, by which time he was a double Formula One world champion.\n\nBy the start of the 1970s, efforts by Jackie Stewart saw a few events cancelled because of safety concerns. For the 1969 event, Armco barriers were placed at specific points for the first time in the circuit's history; before that, the circuit's conditions were (aside from the removal of people's production cars parked on the side of the road) virtually identical to everyday civilian use. If a driver went off, he would crash into whatever was next to the track (buildings, trees, lamp posts, glass windows, and even a train station); and in Alberto Ascari's and Paul Hawkins's cases, the harbour water, because the concrete road the course used had no Armco to protect the drivers from going off the track and into the Mediterranean. The circuit gained more Armco in specific points for the next 2 races, and by 1972, the circuit was almost completely Armco-lined. And for the first time in its history, the Monaco circuit was altered that year; the pits were moved next to the waterfront straight between the chicane and Tabac and the chicane was moved further forward right before Tabac and was the junction point between the pits and the course. The course was changed again for the 1973 race; the Rainier III Nautical Stadium was constructed where the straight that went behind the pits was and the circuit introduced a double chicane that went around the new swimming pool (this chicane complex is known as \"Swimming Pool\"). This created space for a whole new pit facility; and in 1976 the course was altered yet again; the Sainte Devote corner was made slower and a chicane was placed right before the pit straight.\n\nFor the next two races, By the early 1970s, as Brabham team owner Bernie Ecclestone started to marshal the collective bargaining power of the Formula One Constructors Association (FOCA), Monaco was prestigious enough to become an early bone of contention. Historically the number of cars permitted in a race was decided by the race organiser, in this case the ACM, which had always set a low number of around 16. In 1972 Ecclestone was starting to negotiate deals which relied on FOCA guaranteeing at least 18 entrants for every race. A stand-off over this issue left the 1972 race in jeopardy until the ACM gave in and agreed that 26 cars could participate – the same number permitted at most other circuits. Two years later, in 1974, the ACM managed to get the numbers back down to 18.\n\nBecause of its tight confines, slow average speeds and punishing nature, Monaco has often thrown up unexpected results. In the 1982 race René Arnoux led the first 15 laps, before retiring. Alain Prost then led until four laps from the end, when he spun off on the wet track, hit the barriers and lost a wheel, giving Riccardo Patrese the lead. Patrese himself spun with only a lap and a half to go, letting Didier Pironi through to the front, followed by Andrea de Cesaris. On the last lap, Pironi ran out of fuel in the tunnel, but De Cesaris also ran out of fuel before he could overtake. In the meantime Patrese had bump-started his car and went through to score his first Grand Prix win.\n\nIn 1983 the ACM became entangled in the disagreements between Fédération Internationale du Sport Automobile (FISA) and FOCA. The ACM, with the agreement of Bernie Ecclestone, negotiated an individual television rights deal with ABC in the United States. This broke an agreement enforced by FISA for a single central negotiation of television rights. Jean-Marie Balestre, president of FISA, announced that the Monaco Grand Prix would not form part of the Formula One world championship in 1985. The ACM fought their case in the French courts. They won the case and the race was eventually reinstated.\n\nFor the decade from 1984 to 1993 the race was won by only two drivers, arguably the two best drivers in Formula One at the time- Frenchman Prost and Brazilian Ayrton Senna. Prost, already a winner of the support race for Formula Three cars in 1979, took his first Monaco win at the 1984 race. The race started 45 minutes late after heavy rain. Prost led briefly before Nigel Mansell overtook him on lap 11. Mansell crashed out five laps later, letting Prost back into the lead. On lap 27, Prost led from Ayrton Senna's Toleman and Stefan Bellof's Tyrrell. Senna was catching Prost and Bellof was catching both of them. However, on lap 31, the race was controversially stopped with conditions deemed to be undriveable. Later, FISA fined the clerk of the course, Jacky Ickx, $6,000 and suspended his licence for not consulting the stewards before stopping the race. The drivers received only half of the points that would usually be awarded, as the race had been stopped before two-thirds of the intended race distance had been completed.\n\nProst won 1985 after polesitter Senna retired with a blown Renault engine in his Lotus after over-revving it at the start, and Michele Alboreto in the Ferrari retook the lead twice, but he went off the track at Sainte-Devote, where Brazilian Nelson Piquet and Italian Riccardo Patrese had a huge accident only a few laps previously and oil and debris littered the track. Prost passed Alboreto, who retook the Frenchman, and then he punctured a tire after running over bodywork debris from the Piquet/Patrese accident, which dropped him to 4th. He was able to pass his Roman countrymen Andrea De Cesaris and Elio de Angelis, but finished 2nd behind Prost. The French Prost dominated 1986 after starting from pole position, a race where the Nouvelle Chicane had been changed on the grounds of safety.\n\nSenna holds the record for the most victories in Monaco, with six, including five consecutive wins between 1989 and 1993, as well as eight podium finishes in ten starts. His 1987 win was the first time a car with an active suspension had won a Grand Prix. He managed to win this race after Briton Nigel Mansell in a Williams-Honda went out with a broken exhaust. His win was very popular with the people of Monaco, and when he was arrested on the Monday following the race, for riding a motorcycle without wearing a helmet, he was released by the officers after they realised who he was. Senna dominated 1988, and was able to get ahead of his teammate Prost while the Frenchman was held up for most of the race by Austrian Gerhard Berger in a Ferrari. By the time Prost got past Berger, he pushed as hard as he could and set a lap some 6 seconds faster than Senna's- at which the Brazilian panicked; he then set 2 fastest laps, and while pushing as hard as possible, he bit the barrier at the Portier corner and crashed into the Armco separating the road from the Mediterranean. Senna was so upset that he went back to his Monaco flat and was not heard from again; Prost went on to win for the fourth time. Senna dominated 1989 while Prost was stuck behind backmarker has-been Rene Arnoux and others; the Brazilian also dominated 1990 and 1991. At the 1992 event Nigel Mansell, who had won all five races held to that point in the season, took pole and dominated the race in his Williams FW14B-Renault. However, with seven laps remaining, Mansell suffered a loose wheel nut and was forced into the pits, emerging behind Ayrton Senna's McLaren-Honda, who was on worn tyres. Mansell, on fresh tyres, set a lap record almost two seconds quicker than Senna's and closed from 5.2 to 1.9 seconds in only two laps. The pair duelled around Monaco for the final four laps but Mansell could find no way past, finishing just two tenths of a second behind the Brazilian. Senna had a poor start to the 1993 event, he crashed in practice and qualified 3rd behind pole-sitter Prost and the German rising star Michael Schumacher. The Brazilian Senna was again fortuitous to win in 1993 after getting to the first corner in third behind Prost and Schumacher. Prost had to serve a time penalty for jumping the start and Schumacher retired with hydrualic active suspension problems, and Senna won in front of the late Graham Hill's son Damon. It was Senna's fifth win at Monaco, equalling Graham Hill's record. After Senna took his sixth win at the 1993 race, breaking Graham Hill's record for most wins at the Monaco Grand Prix, runner-up Damon Hill commented that \"If my father was around now, he would be the first to congratulate Ayrton.\"\n\nThe 1994 race was an emotional and tragic affair; it came two weeks after the tragic race at Imola where Austrian Roland Ratzenberger and Senna both died from massive head injuries from on-track accidents on successive days. But during the Monaco event, Austrian Karl Wendlinger had an appalling accident in his Sauber in the tunnel; he went into a coma and was to miss the rest of the season; some feared for his life. The German Schumacher won the 1994 Monaco event easily. The 1996 race saw Michael Schumacher take pole position before crashing out on the first lap after being overtaken by Damon Hill. Hill led the first 40 laps before his engine expired in the tunnel. Jean Alesi took the lead but suffered suspension failure 20 laps later. Olivier Panis, who started in 14th place, moved into the lead and stayed there until the end of the race, being pushed all the way by David Coulthard. It was Panis's only win, and the last for his Ligier team. Only three cars crossed the finish line, but seven were classified.\n\nSeven-time world champion Schumacher would eventually win the race five times, matching Graham Hill's record. In his appearance at the 2006 event, he attracted criticism when, while provisionally holding pole position and with the qualifying session drawing to a close, he stopped his car at the Rascasse hairpin, blocking the track and obliging competitors to slow down. Although Schumacher claimed it was the unintentional result of a genuine car failure, the FIA disagreed and he was sent to the back of the grid.\n\nIn July 2010, Bernie Ecclestone announced that a 10-year deal had been reached with the race organisers, keeping the race on the calendar until at least 2020.\n\nThe Circuit de Monaco consists of the city streets of Monte Carlo and La Condamine, which includes the famous harbour. It is unique in having been held on the same circuit every time it has been run over such a long period – only the Italian Grand Prix, which has been held at Autodromo Nazionale Monza during every Formula One regulated year except 1980, has a similarly lengthy and close relationship with a single circuit.\n\nThe race circuit has many elevation changes, tight corners, and a narrow course that makes it one of the most demanding tracks in Formula One racing. , two drivers have crashed and ended up in the harbour, the most famous being Alberto Ascari in 1955. Despite the fact that the course has had minor changes several times during its history, it is still considered the ultimate test of driving skills in Formula One, and if it were not already an existing Grand Prix, it would not be permitted to be added to the schedule for safety reasons. Even in 1929, 'La Vie Automobile' magazine offered the opinion that \"Any respectable traffic system would have covered the track with «Danger» sign posts left, right and centre\".\n\nTriple Formula One champion Nelson Piquet was fond of saying that racing at Monaco was \"like trying to cycle round your living room\", but added that \"a win here was worth two anywhere else\".\n\nNotably, the course includes a tunnel. The contrast of daylight and gloom when entering/exiting the tunnel presents \"challenges not faced elsewhere\", as the drivers have to \"adjust their vision as they emerge from the tunnel at the fastest point of the track and brake for the chicane in the daylight.\". The fastest-ever qualifying lap was set by Kimi Räikkönen in qualifying for the 2017 Grand Prix, at 1m 12.178. This same lap is also the fastest-ever to date.\n\nDuring the Grand Prix weekend spectators crowd around the Monaco Circuit. There are a number of temporary grandstands built around the circuit, mostly around the harbour area. The rich and famous arrive on their boats and the yachts in the harbour fill with spectators. Balconies around Monaco become viewing areas for the race too. Many hotels and residents cash in on the birds eye views of the race. Grand Prix organizers Automobile Club de Monaco officially voted the Ermanno Palace Penthouse the 'Best view of the Monaco Grand Prix'.\n\nThe Monaco Grand Prix is organised each year by the \"Automobile Club de Monaco\" which also runs the Monte Carlo Rally and the Junior Monaco Kart Cup.\n\nIt differs in several ways from other Grands Prix. The practice session for the race is held on the Thursday preceding the race instead of Friday. This allows the streets to be opened to the public again on Friday. Until the late 1990s the race started at 3:30 p.m. local time – an hour and a half later than other European Formula One races. In recent years the race has fallen in line with the other Formula One races for the convenience of television viewers. Also, earlier the event was traditionally held on the week of Ascension Day. It is now always held on the last weekend in May. For many years, the numbers of cars admitted to Grands Prix was at the discretion of the race organisers – Monaco had the smallest grids, ostensibly because of its narrow and twisting track. Only 18 cars were permitted to enter the 1975 Monaco Grand Prix, compared to 23 to 26 cars at all other rounds that year.\n\nThe erecting of the circuit takes six weeks, and the removal after the race takes three weeks. There was no podium as such at the race, until 2017. Instead a section of the track is closed after the race to act as parc fermé, a place where the cars are held for official inspection. The first three drivers in the race leave their cars there and walk directly to the royal box where the 'podium' ceremony is held, which is considered a custom for the race. The trophies are handed out before the national anthems for the winning driver and team are played, as opposed to other Grands Prix where the anthems are played first.\n\nThe Monaco Grand Prix is widely considered to be one of the most important and prestigious automobile races in the world alongside the Indianapolis 500-Mile Race and the 24 Hours of Le Mans. These three races are considered to form a \"Triple Crown\" of the three most famous motor races in the world. Graham Hill is the only driver to have won the Triple Crown, by winning all three races. The practice session for Monaco overlaps with that for the Indianapolis 500, and the races themselves sometimes clash. As the two races take place on opposite sides of the Atlantic Ocean and form part of different championships, it is difficult for one driver to compete effectively in both during his career. Juan Pablo Montoya, who won the Monaco Grand Prix in 2003 and the Indianapolis 500 in 2000 and 2015, is the only active driver still racing in 2017 who has won two of the three races.\n\nIn awarding its first Gold medal for motor sport to Prince Rainier III, the Fédération Internationale de l'Automobile (FIA) characterised the Monaco Grand Prix as contributing \"an exceptional location of glamour and prestige\" to motor sport. It has been run under the patronage of three generations of Monaco's royal family: Louis II, Rainier III and Albert II, all of whom have taken a close interest in the race. A large part of the principality's income comes from tourists attracted by the warm climate and the famous casino, but it is also a tax haven and is home to many millionaires, including several Formula One drivers.\n\nMonaco has produced only three native Formula One drivers, Louis Chiron, André Testut and Olivier Beretta, but its tax status has made it home to many drivers over the years, including Gilles Villeneuve and Ayrton Senna. Of the 2006 Formula One contenders, several have property in the principality, including Jenson Button and David Coulthard, who was part owner of a hotel there. Because of the small size of the town and the location of the circuit, drivers whose races end early can usually get back to their apartments in minutes. Ayrton Senna famously retired to his apartment after crashing out of the lead of the 1988 race.\n\n\"Embolded drivers are still competing in the Formula One championship\"\n\n\"A pink background indicates an event which was not part of the Formula One World Championship.\"\n\"A cream background indicates an event which was part of the pre-war European Championship.\"\n\"Embolded teams are competing in the Formula One championship in the current season.\"\n\n\"A pink background indicates an event which was not part of the Formula One World Championship.\"\n\"A cream background indicates an event which was part of the pre-war European Championship.\"\n\n\n\n"}
{"id": "10947", "url": "https://en.wikipedia.org/wiki?curid=10947", "title": "Fission", "text": "Fission\n\nFission, a splitting of something into two or more parts, may refer to:\n\n\n\n"}
{"id": "10948", "url": "https://en.wikipedia.org/wiki?curid=10948", "title": "Fusion", "text": "Fusion\n\nFusion or synthesis, the process of combining two or more distinct entities into a new whole, may refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "10949", "url": "https://en.wikipedia.org/wiki?curid=10949", "title": "Four color theorem", "text": "Four color theorem\n\nIn mathematics, the four color theorem, or the four color map theorem, states that, given any separation of a plane into contiguous regions, producing a figure called a \"map\", no more than four colors are required to color the regions of the map so that no two adjacent regions have the same color. Two regions are called \"adjacent\" if they share a common boundary that is not a corner, where corners are the points shared by three or more regions.\n\nDespite the motivation from coloring political maps of countries, the theorem is not of particular interest to cartographers. According to an article by the math historian Kenneth May, \"Maps utilizing only four colors are rare, and those that do usually require only three. Books on cartography and the history of mapmaking do not mention the four-color property.\"\n\nThree colors are adequate for simpler maps, but an additional fourth color is required for some maps, such as a map in which one region is surrounded by an odd number of other regions that touch each other in a cycle. The five color theorem, which has a short elementary proof, states that five colors suffice to color a map and was proved in the late 19th century ; however, proving that four colors suffice turned out to be significantly harder. A number of false proofs and false counterexamples have appeared since the first statement of the four color theorem in 1852.\n\nThe four color theorem was proved in 1976 by Kenneth Appel and Wolfgang Haken. It was the first major theorem to be proved using a computer. Appel and Haken's approach started by showing that there is a particular set of 1,936 maps, each of which cannot be part of a smallest-sized counterexample to the four color theorem. (If they did appear, there would be a smaller counterexample.) Appel and Haken used a special-purpose computer program to confirm that each of these maps had this property. Additionally, any map that could potentially be a counterexample must have a portion that looks like one of these 1,936 maps. Showing this required hundreds of pages of hand analysis. Appel and Haken concluded that no smallest counterexamples exist because any must contain, yet do not contain, one of these 1,936 maps. This contradiction means there are no counterexamples at all and that the theorem is therefore true. Initially, their proof was not accepted by all mathematicians because the computer-assisted proof was infeasible for a human to check by hand . Since then the proof has gained wider acceptance, although doubts remain .\n\nTo dispel remaining doubt about the Appel–Haken proof, a simpler proof using the same ideas and still relying on computers was published in 1997 by Robertson, Sanders, Seymour, and Thomas. Additionally, in 2005, the theorem was proved by Georges Gonthier with general-purpose theorem-proving software.\n\nThe intuitive statement of the four color theorem, i.e. 'that given any separation of a plane into contiguous regions, called a map, the regions can be colored using at most four colors so that no two adjacent regions have the same color', needs to be interpreted appropriately to be correct.\n\nFirst, all corners, points that belong to (technically, are in the closure of) three or more countries, must be ignored. In addition, bizarre maps (using regions of finite area but infinite perimeter) can require more than four colors.\nSecond, for the purpose of the theorem, every \"country\" has to be a connected region, or contiguous. In the real world, this is not true (e.g. the Upper and Lower Peninsula of Michigan, Nakhchivan as part of Azerbaijan, and Kaliningrad as part of Russia are not contiguous). Because all the territory of a particular country must be the same color, four colors may not be sufficient. For instance, consider a simplified map:\n\nIn this map, the two regions labeled \"A\" belong to the same country, and must be the same color. This map then requires five colors, since the two \"A\" regions together are contiguous with four other regions, each of which is contiguous with all the others. A similar construction also applies if a single color is used for all bodies of water, as is usual on real maps. For maps in which more than one country may have multiple disconnected regions, six or more colors might be required.\n\nA simpler statement of the theorem uses graph theory. The set of regions of a map can be represented more abstractly as an undirected graph that has a vertex for each region and an edge for every pair of regions that share a boundary segment. This graph is planar (it is important to note that we are talking about the graphs that have some limitations according to the map they are transformed from only): it can be drawn in the plane without crossings by placing each vertex at an arbitrarily chosen location within the region to which it corresponds, and by drawing the edges as curves that lead without crossing within each region from the vertex location to each shared boundary point of the region. Conversely any planar graph can be formed from a map in this way. In graph-theoretic terminology, the four-color theorem states that the vertices of every planar graph can be colored with at most four colors so that no two adjacent vertices receive the same color, or for short, \"every planar graph is four-colorable\" (; ).\n\nAs far as is known, the conjecture was first proposed on October 23, 1852 when Francis Guthrie, while trying to color the map of counties of England, noticed that only four different colors were needed. At the time, Guthrie's brother, Frederick, was a student of Augustus De Morgan (the former advisor of Francis) at University College London. Francis inquired with Frederick regarding it, who then took it to De Morgan (Francis Guthrie graduated later in 1852, and later became a professor of mathematics in South Africa). According to De Morgan:\n\n\"A student of mine [Guthrie] asked me to day to give him a reason for a fact which I did not know was a fact—and do not yet. He says that if a figure be any how divided and the compartments differently colored so that figures with any portion of common boundary \"line\" are differently colored—four colors may be wanted but not more—the following is his case in which four colors \"are\" wanted. Query cannot a necessity for five or more be invented…\" \n\n\"F.G.\", perhaps one of the two Guthries, published the question in \"The Athenaeum\" in 1854, and De Morgan posed the question again in the same magazine in 1860. Another early published reference by in turn credits the conjecture to De Morgan.\n\nThere were several early failed attempts at proving the theorem. De Morgan believed that it followed from a simple fact about four regions, though he didn't believe that fact could be derived from more elementary facts.\nThis arises in the following way. We never need four colors in a neighborhood unless there be four counties, each of which has boundary lines in common with each of the other three. Such a thing cannot happen with four areas unless one or more of them be inclosed by the rest; and the color used for the inclosed county is thus set free to go on with. Now this principle, that four areas cannot each have common boundary with all the other three without inclosure, is not, we fully believe, capable of demonstration upon anything more evident and more elementary; it must stand as a postulate.\n\nOne alleged proof was given by Alfred Kempe in 1879, which was widely acclaimed; another was given by Peter Guthrie Tait in 1880. It was not until 1890 that Kempe's proof was shown incorrect by Percy Heawood, and in 1891, Tait's proof was shown incorrect by Julius Petersen—each false proof stood unchallenged for 11 years .\n\nIn 1890, in addition to exposing the flaw in Kempe's proof, Heawood proved the five color theorem and generalized the four color conjecture to surfaces of arbitrary genus—see below.\n\nTait, in 1880, showed that the four color theorem is equivalent to the statement that a certain type of graph (called a snark in modern terminology) must be non-planar.\n\nIn 1943, Hugo Hadwiger formulated the Hadwiger conjecture , a far-reaching generalization of the four-color problem that still remains unsolved.\n\nDuring the 1960s and 1970s German mathematician Heinrich Heesch developed methods of using computers to search for a proof. Notably he was the first to use discharging for proving the theorem, which turned out to be important in the unavoidability portion of the subsequent Appel-Haken proof. He also expanded on the concept of reducibility and, along with Ken Durre, developed a computer test for it. Unfortunately, at this critical juncture, he was unable to procure the necessary supercomputer time to continue his work .\n\nOthers took up his methods and his computer-assisted approach. While other teams of mathematicians were racing to complete proofs, Kenneth Appel and Wolfgang Haken at the University of Illinois announced, on June 21, 1976, that they had proved the theorem. They were assisted in some algorithmic work by John A. Koch .\n\nIf the four-color conjecture were false, there would be at least one map with the smallest possible number of regions that requires five colors. The proof showed that such a minimal counterexample cannot exist, through the use of two technical concepts (; ; ):\n\n\nUsing mathematical rules and procedures based on properties of reducible configurations, Appel and Haken found an unavoidable set of reducible configurations, thus proving that a minimal counterexample to the four-color conjecture could not exist. Their proof reduced the infinitude of possible maps to 1,936 reducible configurations (later reduced to 1,476) which had to be checked one by one by computer and took over a thousand hours. This reducibility part of the work was independently double checked with different programs and computers. However, the unavoidability part of the proof was verified in over 400 pages of microfiche, which had to be checked by hand with the assistance of Haken's daughter Dorothea Blostein .\n\nAppel and Haken's announcement was widely reported by the news media around the world, and the math department at the University of Illinois used a postmark stating \"Four colors suffice.\" At the same time the unusual nature of the proof—it was the first major theorem to be proved with extensive computer assistance—and the complexity of the human-verifiable portion, aroused considerable controversy .\n\nIn the early 1980s, rumors spread of a flaw in the Appel-Haken proof. Ulrich Schmidt at RWTH Aachen examined Appel and Haken's proof for his master's thesis . He had checked about 40% of the unavoidability portion and found a significant error in the discharging procedure . In 1986, Appel and Haken were asked by the editor of \"Mathematical Intelligencer\" to write an article addressing the rumors of flaws in their proof. They responded that the rumors were due to a \"misinterpretation of [Schmidt's] results\" and obliged with a detailed article . Their magnum opus, \"Every Planar Map is Four-Colorable\", a book claiming a complete and detailed proof (with a microfiche supplement of over 400 pages), appeared in 1989 and explained Schmidt's discovery and several further errors found by others .\n\nSince the proving of the theorem, efficient algorithms have been found for 4-coloring maps requiring only O(\"n\") time, where \"n\" is the number of vertices. In 1996, Neil Robertson, Daniel P. Sanders, Paul Seymour, and Robin Thomas created a quadratic-time algorithm, improving on a quartic-time algorithm based on Appel and Haken’s proof (; ). This new proof is similar to Appel and Haken's but more efficient because it reduces the complexity of the problem and requires checking only 633 reducible configurations. Both the unavoidability and reducibility parts of this new proof must be executed by computer and are impractical to check by hand . In 2001, the same authors announced an alternative proof, by proving the snark theorem (; ).\n\nIn 2005, Benjamin Werner and Georges Gonthier formalized a proof of the theorem inside the Coq proof assistant. This removed the need to trust the various computer programs used to verify particular cases; it is only necessary to trust the Coq kernel.\n\nThe following discussion is a summary based on the introduction to Appel and Haken's book \"Every Planar Map is Four Colorable\" . Although flawed, Kempe's original purported proof of the four color theorem provided some of the basic tools later used to prove it. The explanation here is reworded in terms of the modern graph theory formulation above.\n\nKempe's argument goes as follows. First, if planar regions separated by the graph are not \"triangulated\", i.e. do not have exactly three edges in their boundaries, we can add edges without introducing new vertices in order to make every region triangular, including the unbounded outer region. If this triangulated graph is colorable using four colors or fewer, so is the original graph since the same coloring is valid if edges are removed. So it suffices to prove the four color theorem for triangulated graphs to prove it for all planar graphs, and without loss of generality we assume the graph is triangulated.\n\nSuppose \"v\", \"e\", and \"f\" are the number of vertices, edges, and regions (faces). Since each region is triangular and each edge is shared by two regions, we have that 2\"e\" = 3\"f\". This together with Euler's formula, \"v\" − \"e\" + \"f\" = 2, can be used to show that 6\"v\" − 2\"e\" = 12. Now, the \"degree\" of a vertex is the number of edges abutting it. If \"v\" is the number of vertices of degree \"n\" and \"D\" is the maximum degree of any vertex,\nBut since 12 > 0 and 6 − \"i\" ≤ 0 for all \"i\" ≥ 6, this demonstrates that there is at least one vertex of degree 5 or less.\n\nIf there is a graph requiring 5 colors, then there is a \"minimal\" such graph, where removing any vertex makes it four-colorable. Call this graph \"G\". Then \"G\" cannot have a vertex of degree 3 or less, because if \"d\"(\"v\") ≤ 3, we can remove \"v\" from \"G\", four-color the smaller graph, then add back \"v\" and extend the four-coloring to it by choosing a color different from its neighbors.\n\nKempe also showed correctly that \"G\" can have no vertex of degree 4. As before we remove the vertex \"v\" and four-color the remaining vertices. If all four neighbors of \"v\" are different colors, say red, green, blue, and yellow in clockwise order, we look for an alternating path of vertices colored red and blue joining the red and blue neighbors. Such a path is called a Kempe chain. There may be a Kempe chain joining the red and blue neighbors, and there may be a Kempe chain joining the green and yellow neighbors, but not both, since these two paths would necessarily intersect, and the vertex where they intersect cannot be colored. Suppose it is the red and blue neighbors that are not chained together. Explore all vertices attached to the red neighbor by red-blue alternating paths, and then reverse the colors red and blue on all these vertices. The result is still a valid four-coloring, and \"v\" can now be added back and colored red.\n\nThis leaves only the case where \"G\" has a vertex of degree 5; but Kempe's argument was flawed for this case. Heawood noticed Kempe's mistake and also observed that if one was satisfied with proving only five colors are needed, one could run through the above argument (changing only that the minimal counterexample requires 6 colors) and use Kempe chains in the degree 5 situation to prove the five color theorem.\n\nIn any case, to deal with this degree 5 vertex case requires a more complicated notion than removing a vertex. Rather the form of the argument is generalized to considering \"configurations\", which are connected subgraphs of \"G\" with the degree of each vertex (in G) specified. For example, the case described in degree 4 vertex situation is the configuration consisting of a single vertex labelled as having degree 4 in \"G\". As above, it suffices to demonstrate that if the configuration is removed and the remaining graph four-colored, then the coloring can be modified in such a way that when the configuration is re-added, the four-coloring can be extended to it as well. A configuration for which this is possible is called a \"reducible configuration\". If at least one of a set of configurations must occur somewhere in G, that set is called \"unavoidable\". The argument above began by giving an unavoidable set of five configurations (a single vertex with degree 1, a single vertex with degree 2, ..., a single vertex with degree 5) and then proceeded to show that the first 4 are reducible; to exhibit an unavoidable set of configurations where every configuration in the set is reducible would prove the theorem.\n\nBecause \"G\" is triangular, the degree of each vertex in a configuration is known, and all edges internal to the configuration are known, the number of vertices in \"G\" adjacent to a given configuration is fixed, and they are joined in a cycle. These vertices form the \"ring\" of the configuration; a configuration with \"k\" vertices in its ring is a \"k\"-ring configuration, and the configuration together with its ring is called the \"ringed configuration\". As in the simple cases above, one may enumerate all distinct four-colorings of the ring; any coloring that can be extended without modification to a coloring of the configuration is called \"initially good\". For example, the single-vertex configuration above with 3 or less neighbors were initially good. In general, the surrounding graph must be systematically recolored to turn the ring's coloring into a good one, as was done in the case above where there were 4 neighbors; for a general configuration with a larger ring, this requires more complex techniques. Because of the large number of distinct four-colorings of the ring, this is the primary step requiring computer assistance.\n\nFinally, it remains to identify an unavoidable set of configurations amenable to reduction by this procedure. The primary method used to discover such a set is the method of discharging. The intuitive idea underlying discharging is to consider the planar graph as an electrical network. Initially positive and negative \"electrical charge\" is distributed amongst the vertices so that the total is positive.\n\nRecall the formula above:\n\nEach vertex is assigned an initial charge of 6-deg(\"v\"). Then one \"flows\" the charge by systematically redistributing the charge from a vertex to its neighboring vertices according to a set of rules, the \"discharging procedure\". Since charge is preserved, some vertices still have positive charge. The rules restrict the possibilities for configurations of positively charged vertices, so enumerating all such possible configurations gives an unavoidable set.\n\nAs long as some member of the unavoidable set is not reducible, the discharging procedure is modified to eliminate it (while introducing other configurations). Appel and Haken's final discharging procedure was extremely complex and, together with a description of the resulting unavoidable configuration set, filled a 400-page volume, but the configurations it generated could be checked mechanically to be reducible. Verifying the volume describing the unavoidable configuration set itself was done by peer review over a period of several years.\n\nA technical detail not discussed here but required to complete the proof is \"immersion reducibility\".\n\nThe four color theorem has been notorious for attracting a large number of false proofs and disproofs in its long history. At first, \"The New York Times\" refused as a matter of policy to report on the Appel–Haken proof, fearing that the proof would be shown false like the ones before it . Some alleged proofs, like Kempe's and Tait's mentioned above, stood under public scrutiny for over a decade before they were refuted. But many more, authored by amateurs, were never published at all.\n\nGenerally, the simplest, though invalid, counterexamples attempt to create one region which touches all other regions. This forces the remaining regions to be colored with only three colors. Because the four color theorem is true, this is always possible; however, because the person drawing the map is focused on the one large region, they fail to notice that the remaining regions can in fact be colored with three colors.\n\nThis trick can be generalized: there are many maps where if the colors of some regions are selected beforehand, it becomes impossible to color the remaining regions without exceeding four colors. A casual verifier of the counterexample may not think to change the colors of these regions, so that the counterexample will appear as though it is valid.\n\nPerhaps one effect underlying this common misconception is the fact that the color restriction is not transitive: a region only has to be colored differently from regions it touches directly, not regions touching regions that it touches. If this were the restriction, planar graphs would require arbitrarily large numbers of colors.\n\nOther false disproofs violate the assumptions of the theorem in unexpected ways, such as using a region that consists of multiple disconnected parts, or disallowing regions of the same color from touching at a point.\n\nWhile every planar map can be colored with four colors, it is NP-complete in complexity to decide whether an arbitrary planar map can be colored with just three colors.\n\nThe four-color theorem applies not only to finite planar graphs, but also to infinite graphs that can be drawn without crossings in the plane, and even more generally to infinite graphs (possibly with an uncountable number of vertices) for which every finite subgraph is planar. To prove this, one can combine a proof of the theorem for finite planar graphs with the De Bruijn–Erdős theorem stating that, if every finite subgraph of an infinite graph is \"k\"-colorable, then the whole graph is also \"k\"-colorable . This can also be seen as an immediate consequence of Kurt Gödel's compactness theorem for first-order logic, simply by expressing the colorability of an infinite graph with a set of logical formulae.\n\nOne can also consider the coloring problem on surfaces other than the plane (Weisstein). The problem on the sphere or cylinder is equivalent to that on the plane. For closed (orientable or non-orientable) surfaces with positive genus, the maximum number \"p\" of colors needed depends on the surface's Euler characteristic χ according to the formula\nwhere the outermost brackets denote the floor function.\n\nAlternatively, for an orientable surface the formula can be given in terms of the genus of a surface, \"g\":\n\nThis formula, the Heawood conjecture, was conjectured by P. J. Heawood in 1890 and proved by Gerhard Ringel and J. W. T. Youngs in 1968. The only exception to the formula is the Klein bottle, which has Euler characteristic 0 (hence the formula gives p = 7) and requires only 6 colors, as shown by P. Franklin in 1934 (Weisstein).\n\nFor example, the torus has Euler characteristic χ = 0 (and genus \"g\" = 1) and thus \"p\" = 7, so no more than 7 colors are required to color any map on a torus. This upper bound of 7 is sharp: certain toroidal polyhedra such as the Szilassi polyhedron require seven colors.\nA Möbius strip requires six colors as do 1-planar graphs (graphs drawn with at most one simple crossing per edge) . If both the vertices and the faces of a planar graph are colored, in such a way that no two adjacent vertices, faces, or vertex-face pair have the same color, then again at most six colors are needed .\n\nThere is no obvious extension of the coloring result to three-dimensional solid regions. By using a set of \"n\" flexible rods, one can arrange that every rod touches every other rod. The set would then require \"n\" colors, or \"n\"+1 if you consider the empty space that also touches every rod. The number \"n\" can be taken to be any integer, as large as desired. Such examples were known to Fredrick Guthrie in 1880 . Even for axis-parallel cuboids (considered to be adjacent when two cuboids share a two-dimensional boundary area) an unbounded number of colors may be necessary (; ).\n\n\n"}
{"id": "10951", "url": "https://en.wikipedia.org/wiki?curid=10951", "title": "Fahrenheit 451", "text": "Fahrenheit 451\n\nFahrenheit 451 is a dystopian novel by American writer Ray Bradbury, published in 1953. It is regarded as one of his best works. The novel presents a future American society where books are outlawed and \"firemen\" burn any that are found. The book's tagline explains the title: \"Fahrenheit 451 – the temperature at which book paper catches fire, and burns...\"\n\nThe novel has been the subject of interpretations focusing on the historical role of book burning in suppressing dissenting ideas. In a 1956 radio interview, Bradbury stated that he wrote \"Fahrenheit 451\" because of his concerns at the time (during the McCarthy era) about the threat of book burning in the United States. In later years, he described the book as a commentary on how mass media reduces interest in reading literature.\n\nIn 1954, \"Fahrenheit 451\" won the American Academy of Arts and Letters Award in Literature and the Commonwealth Club of California Gold Medal. It has since won the Prometheus \"Hall of Fame\" Award in 1984 and a 1954 \"Retro\" Hugo Award, one of only four Best Novel Retro Hugos ever given, in 2004. Bradbury was honored with a Spoken Word Grammy nomination for his 1976 audiobook version.\n\nAdaptations include François Truffaut's 1966 film adaptation of the novel and a 1982 BBC Radio dramatization. Bradbury published a stage play version in 1979 and helped develop a 1984 interactive fiction computer game titled \"Fahrenheit 451\", and a collection of his short stories, \"A Pleasure to Burn\".\n\n\"Fahrenheit 451\" is set in an unspecified city (likely in the American Midwest) at an unspecified time in the future after the year 1960.\n\nThe novel is divided into three parts: \"The Hearth and the Salamander\", \"The Sieve and the Sand\", and \"Burning Bright\".\n\nGuy Montag is a \"fireman\" employed to burn the possessions of those who read outlawed books. He is married and has no children. One fall night while returning from work, he meets his new neighbor, a teenage girl named Clarisse McClellan, whose free-thinking ideals and liberating spirit cause him to question his life and his own perceived happiness. Montag returns home to find that his wife Mildred has overdosed on sleeping pills, and he calls for medical attention. Two uncaring EMTs come over to pump Mildred's stomach, drain her poisoned blood, and fill her with new blood. After the EMTs leave to rescue another overdose victim, Montag watches over Mildred, watching the new blood fill her pallid cheeks. Montag then goes outside, overhearing Clarisse and her family talk about the way life is in this hedonistic, illiterate society. Montag's mind is bombarded with Clarisse's subversive thoughts and the memory of his wife's near-death. The next day, Montag finds Mildred in the kitchen, with no memory of what happened and talking incessantly about being hungry from an alleged hangover she has from a party she thought she attended last night. Over the next few days, Clarisse faithfully meets Montag as he walks home. She tells him about how her simple pleasures and interests make her an outcast among her peers and how she's forced to go to therapy for her behavior and thoughts. Montag looks forward to these meetings, and just as he begins to expect them, Clarisse goes absent. He senses something is wrong.\n\nIn the following days, while at work with the other firemen ransacking the book-filled house of an old woman before the inevitable burning, Montag steals a book before any of his coworkers notice. The woman refuses to leave her house and her books, choosing instead to light a match and burn herself alive. Montag returns home jarred by the woman's suicide. While getting ready for bed, he hides the stolen book under his pillow. Still shaken by the night's events, he attempts to make conversation with Mildred, starting by asking her when they first met and where. Mildred goes to answer, but immediately forgets. As she laughs off her ignorance and heads for the bathroom to take more sleeping pills, Montag realizes just how much Mildred's sleeping pill addiction, love of interactive entertainment, and fast driving has ruined her mind and their marriage. Later, as Mildred is sleeping, Montag wakes her up and asks her if she has seen or heard anything about Clarisse McClellan. Mildred initially brushes off the question until she finally reveals what happened: Clarisse's family moved away after Clarisse got hit by a speeding car and died four days ago. Dismayed by her failure to mention this earlier, Montag uneasily tries to fall asleep. Outside he suspects the presence of \"The Hound\", an eight-legged robotic dog-like creature that resides in the firehouse and aids the firemen.\n\nMontag awakens ill the next morning, with Mildred nagging him to get up and go to work. As Mildred tries to care for her husband (but finds herself more involved in the parlor wall entertainment in the next room), Montag suggests that maybe he should take a break from being a fireman after what happened last night. Mildred panics over the thought of losing the house and her parlor wall family and angrily blames the old woman who killed herself over her books for Montag's change of heart over his job. Captain Beatty, Montag's fire chief, personally visits Montag to see how he is doing. Sensing Montag's concerns, Beatty recounts how books lost their value and where the firemen fit in: over the course of several decades, people embraced new media (in this case, film and television), sports, and a quickening pace of life. Books were ruthlessly abridged or degraded to accommodate a short attention span while minority groups protested over the controversial, outdated content perceived to be found in literature (yet comic books, trade papers, and sex magazines were allowed to stay, as those fed into the population's want for mindless entertainment). The government took advantage of this by turning the firemen into officers of people's peace of mind. Beatty adds casually that all firemen eventually steal a book out of curiosity; if the book is burned within 24 hours, the fireman and his family will not get in trouble.\n\nAfter Beatty has left, Montag reveals to Mildred that, over the last year, he has accumulated a stash of books that he has kept hidden in their air-conditioning duct. In a panic, Mildred grabs a book and rushes to throw it in their kitchen incinerator. Montag subdues her and tells her that the two of them are going to read the books to see if they have value. If they do not, he promises the books will be burned, and all will return to normal.\n\nWhile Montag and Mildred are perusing the stolen books, a sniffing occurs at their front door. Montag recognizes it as The Hound while Mildred passes it off as a random dog. They resume their discussion once the sound ceases, but Mildred refuses to go along with it, questioning why she or anyone else should care about books. Montag goes on a rant about Mildred's suicide attempt, Clarisse's disappearance and death, the old woman who burned herself, and the constant din of bombers flying overhead and the imminent threat of war that goes ignored by the masses. He then states that maybe the books of the past have messages that can save society from its own destruction. The conversation is interrupted by a call from Mildred's friend, Mrs. Bowles, and they set up a date to watch the \"parlor walls\" (large televisions lining the walls of her living room) that night at Mildred's house.\n\nMontag, meanwhile, concedes that Mildred is a lost cause and he will need help to understand the books. Montag remembers an old man named Faber he once met in a park a year ago, an English professor before books were banned. He telephones Faber with questions about books, and Faber soon hangs up on him. Undeterred, Montag makes a subway trip to Faber's home along with a rare copy of the Bible, the book he stole at the woman's house. He tries to read it on the way, but gets distracted by a radio jingle for Denham's Dentifrice and nearly goes insane. Once he arrives at Faber's house, Montag forces the scared and reluctant Faber into helping him by methodically ripping pages from the Bible. Faber concedes and gives Montag a homemade ear-piece communicator so he can offer constant guidance.\n\nAfter Montag returns home, Mildred's friends, Mrs. Bowles and Mrs. Phelps, arrive to watch the \"parlor walls\". Not interested in this insipid entertainment, Montag turns off the walls and tries to engage the women in meaningful conversation, only for them to reveal just how indifferent, ignorant, and callous they are about the upcoming war, the thought of losing loved ones to death, their unruly children, and who they voted for in the last election. Enraged by their idiocy, Montag leaves momentarily and returns with a book of poetry. This confuses the women and alarms Faber, who is listening remotely. Mildred tries to dismiss Montag's actions as a tradition fireman do once a year: they find a book from the past and read it as a way to make fun of how silly the past is. Montag proceeds to recite the poem \"Dover Beach\", causing Mrs. Phelps to cry. At the behest of Faber in the ear-piece, Montag burns the book. Mildred's friends leave in disgust, while Mildred locks herself in the bathroom and takes more sleeping pills.\n\nIn the aftermath of the parlor party, Montag hides his books in his backyard before returning to the firehouse late at night with just the stolen Bible. He finds Beatty playing cards with the other firemen. Montag hands Beatty a book to cover for the one he believes Beatty knows he stole the night before, which is unceremoniously tossed into the trash. Beatty tells Montag that he had a dream in which they fought endlessly by quoting books to each other. In describing the dream Beatty reveals that, despite his disillusionment, he was once an enthusiastic reader. A fire alarm sounds, and Beatty picks up the address from the dispatcher system. They drive in the firetruck recklessly to the destination. Montag is stunned when the truck arrives at his house.\n\nBeatty orders Montag to destroy his own house, telling him that his wife and her friends reported him after what happened the other night. Montag watches as Mildred walks out of the house, too traumatized about losing her parlor wall family to even acknowledge her husband's existence or the situation going on around her, and catches a taxi, never once looking back. Montag obeys the chief, destroying the home piece by piece with a flamethrower. As soon as he has incinerated the house, Beatty discovers Montag's ear-piece and plans to hunt down Faber. Montag threatens Beatty with the flamethrower and (after Beatty taunts him) burns his boss alive, and knocks his coworkers unconscious. As Montag escapes the scene, the firehouse's mechanical dog attacks him, managing to inject his leg with a tranquilizer. He destroys The Hound with the flamethrower and limps away.\n\nMontag runs through the city streets towards Faber's house. Faber urges him to make his way to the countryside and contact the exiled book-lovers who live there. He mentions he will be leaving on an early bus heading to St. Louis and that he and Montag can rendezvous there later. On Faber's television, they watch news reports of another mechanical hound being released, with news helicopters following it to create a public spectacle. After successfully wiping Montag's scent from around the house in hopes of deterring the hound, Montag leaves Faber's house. After an extended manhunt, he escapes by wading into a river and floating downstream.\n\nMontag leaves the river in the countryside, where he meets the exiled drifters, led by a man named Granger. They have each memorized books should the day come that society comes to an end, then rebuilds itself anew; this time, with the survivors learning to embrace the literature of the past. While learning the philosophy of the exiles, Montag and the group watch helplessly as bombers fly overhead and annihilate the city with nuclear weapons. While Faber would have left on the early bus, everyone else (including Mildred) was immediately killed. Montag and the group are injured and dirtied, but manage to survive the shock wave.\n\nThe following morning, Granger teaches Montag and the others about the legendary phoenix and its endless cycle of long life, death in flames, and rebirth. He adds that the phoenix must have some relationship to mankind, which constantly repeats its mistakes. Granger explains that man has something the phoenix does not: mankind can remember its mistakes and try never to repeat them. Granger then muses that a large factory of mirrors should be built, so that way people can take a long look at themselves and reflect on their lives. When the meal is over, the exiles return to the city to rebuild society.\n\n\nThe title refers to the temperature that Bradbury thought was the autoignition temperature of paper. Though this is popularly believed to be correct, scientists place the autoignition temperature from to depending on the study and type of paper.\n\nBradbury's lifelong passion for books began at an early age. As a frequent visitor to his local libraries in the 1920s and 1930s, he recalls being disappointed because they did not stock popular science fiction novels, like those of H. G. Wells', because, at the time, they were not deemed literary enough. Between this and learning about the destruction of the Library of Alexandria, a great impression was made on the young man about the vulnerability of books to censure and destruction. Later, as a teenager, Bradbury was horrified by the Nazi book burnings and later Joseph Stalin's campaign of political repression, the \"Great Purge\", in which writers and poets, among many others, were arrested and often executed.\n\nAfter the 1945 conclusion of World War II, shortly after the atomic bombings of Hiroshima and Nagasaki, the United States focused its concern on the Soviet atomic bomb project and the expansion of communism. The House Un-American Activities Committee (HUAC), formed in 1938 to investigate American citizens and organizations suspected of having communist ties, held hearings in 1947 to investigate alleged communist influence in Hollywood movie-making. These hearings resulted in the blacklisting of the so-called \"Hollywood Ten\", a group of influential screenwriters and directors. This governmental interference in the affairs of artists and creative types greatly angered Bradbury. Bradbury was bitter and concerned about the workings of his government, and a late 1949 nighttime encounter with an overzealous police officer would inspire Bradbury to write \"The Pedestrian\", a short story which would go on to become \"The Fireman\" and then \"Fahrenheit 451\". The rise of Senator Joseph McCarthy's hearings hostile to accused communists, starting in 1950 deepened Bradbury's contempt for government overreach.\n\nThe year HUAC began investigating Hollywood is often considered the beginning of the Cold War, as in March 1947, the Truman Doctrine was announced. By about 1950, the Cold War was in full swing, and the American public's fear of atomic warfare and communist influence was at a feverish level. The stage was set for Bradbury to write the dramatic nuclear holocaust ending of \"Fahrenheit 451\", exemplifying the type of scenario feared by many Americans of the time.\n\nBradbury's early life witnessed the Golden Age of Radio, while the transition to the Golden Age of Television began right around the time he started to work on the stories that would eventually lead to \"Fahrenheit 451\". Bradbury saw these forms of media as a threat to the reading of books, indeed as a threat to society, as he believed they could act as a distraction from important affairs. This contempt for mass media and technology would express itself through Mildred and her friends and is an important theme in the book.\n\n\"Fahrenheit 451\" developed out of a series of ideas Bradbury had visited in previously written stories. For many years, he tended to single out \"The Pedestrian\" in interviews and lectures as sort of a proto-\"Fahrenheit 451\". In the Preface of his 2006 anthology \"Match to Flame: The Fictional Paths to Fahrenheit 451\" he states that this is an oversimplification. The full genealogy of \"Fahrenheit 451\" given in \"Match to Flame\" is involved. The following covers the most salient aspects.\n\nBetween 1947 and 1948, Bradbury wrote the short story \"Bright Phoenix\" (not published until the May 1963 issue of \"The Magazine of Fantasy & Science Fiction\") about a librarian who confronts a book-burning \"Chief Censor\" named Jonathan Barnes.\n\nIn late 1949, Bradbury was stopped and questioned by a police officer while walking late one night. When asked \"What are you doing?\", Bradbury wisecracked, \"Putting one foot in front of another.\" This incident inspired Bradbury to write the 1951 short story \"The Pedestrian\".\n\nIn \"The Pedestrian\", Leonard Mead is harassed and detained by the city's remotely operated police cruiser (there's only one) for taking nighttime walks, something that has become extremely rare in this future-based setting: everybody else stays inside and watches television (\"viewing screens\"). Alone and without an alibi, Mead is taken to the \"Psychiatric Center for Research on Regressive Tendencies\" for his peculiar habit. \"Fahrenheit 451\" would later echo this theme of an authoritarian society distracted by broadcast media.\n\nBradbury expanded the book-burning premise of \"Bright Phoenix\" and the totalitarian future of \"The Pedestrian\" into \"The Fireman\", a novella published in the February 1951 issue of \"Galaxy Science Fiction\". \"The Fireman\" was written in the basement of UCLA's Powell Library on a typewriter that he rented for a fee of ten cents per half hour. The first draft was 25,000 words long and was completed in nine days.\n\nUrged by a publisher at Ballantine Books to double the length of his story to make a novel, Bradbury returned to the same typing room and expanded his work into \"Fahrenheit 451\", again taking just nine days. The completed book was published by Ballantine in 1953.\n\nBradbury has supplemented the novel with various front and back matter, including a 1979 coda, a 1982 afterword, a 1993 foreword, and several introductions.\n\nThe first U.S. printing was a paperback version from October 1953 by The Ballantine Publishing Group. Shortly after the paperback, a hardback version was released that included a special edition of 200 signed and numbered copies bound in asbestos. These were technically collections because the novel was published with two short stories: \"The Playground\" and \"And the Rock Cried Out\", which have been absent in later printings. A few months later, the novel was serialized in the March, April, and May 1954 issues of nascent \"Playboy\" magazine.\n\nStarting in January 1967, \"Fahrenheit 451\" was subject to expurgation by its publisher, Ballantine Books with the release of the \"Bal-Hi Edition\" aimed at high school students. Among the changes made by the publisher were the censorship of the words \"hell\", \"damn\", and \"abortion\"; the modification of seventy-five passages; and the changing of two episodes.\n\nIn the one case, a drunk man became a \"sick man\" while cleaning fluff out of a human navel became \"cleaning ears\" in the other. For a while both the censored and uncensored versions were available concurrently but by 1973 Ballantine was publishing only the censored version. This continued until 1979 when it came to Bradbury's attention:\n\nIn 1979, one of Bradbury's friends showed him an expurgated copy. Bradbury demanded that Ballantine Books withdraw that version and replace it with the original, and in 1980 the original version once again became available. In this reinstated work, in the Author's Afterword, Bradbury relates to the reader that it is not uncommon for a publisher to expurgate an author's work, but he asserts that he himself will not tolerate the practice of manuscript \"mutilation\".\n\nThe \"Bal-Hi\" editions are now referred to by the publisher as the \"Revised Bal-Hi\" editions.\n\nAn audiobook version read by Bradbury himself was released in 1976 and received a Spoken Word Grammy nomination. Another audiobook was released in 2005 narrated by Christopher Hurt. The e-book version was released in December 2011.\n\nIn 1954, \"Galaxy Science Fiction\" reviewer Groff Conklin placed the novel \"among the great works of the imagination written in English in the last decade or more.\" The \"Chicago Sunday Tribune\"'s August Derleth described the book as \"a savage and shockingly savage prophetic view of one possible future way of life\", calling it \"compelling\" and praising Bradbury for his \"brilliant imagination\". Over half a century later, Sam Weller wrote, \"upon its publication, \"Fahrenheit 451\" was hailed as a visionary work of social commentary.\" Today, \"Fahrenheit 451\" is still viewed as an important cautionary tale against conformity and book burning.\n\nWhen the book was first published, there were those who did not find merit in the tale. Anthony Boucher and J. Francis McComas were less enthusiastic, faulting the book for being \"simply padded, occasionally with startlingly ingenious gimmickry, ... often with coruscating cascades of verbal brilliance [but] too often merely with words.\" Reviewing the book for \"Astounding Science Fiction\", P. Schuyler Miller characterized the title piece as \"one of Bradbury's bitter, almost hysterical diatribes,\" while praising its \"emotional drive and compelling, nagging detail.\" Similarly, \"The New York Times\" was unimpressed with the novel and further accused Bradbury of developing a \"virulent hatred for many aspects of present-day culture, namely, such monstrosities as radio, TV, most movies, amateur and professional sports, automobiles, and other similar aberrations which he feels debase the bright simplicity of the thinking man's existence.\"\n\nIn the years since its publication, \"Fahrenheit 451\" has occasionally been banned, censored, or redacted in some schools by parents and teaching staff either unaware of or indifferent to the inherent irony of such censorship. The following are some notable incidents:\n\nDiscussions about \"Fahrenheit 451\" often center on its story foremost as a warning against state-based censorship. Indeed, when Bradbury wrote the novel during the McCarthy era, he was concerned about censorship in the United States. During a radio interview in 1956, Bradbury said:I wrote this book at a time when I was worried about the way things were going in this country four years ago. Too many people were afraid of their shadows; there was a threat of book burning. Many of the books were being taken off the shelves at that time. And of course, things have changed a lot in four years. Things are going back in a very healthy direction. But at the time I wanted to do some sort of story where I could comment on what would happen to a country if we let ourselves go too far in this direction, where then all thinking stops, and the dragon swallows his tail, and we sort of vanish into a limbo and we destroy ourselves by this sort of action.\n\nAs time went by, Bradbury tended to dismiss censorship as a chief motivating factor for writing the story. Instead he usually claimed that the real messages of \"Fahrenheit 451\" were about the dangers of an illiterate society infatuated with mass media and the threat of minority and special interest groups to books. In the late 1950s, Bradbury recounted:In writing the short novel \"Fahrenheit 451\", I thought I was describing a world that might evolve in four or five decades. But only a few weeks ago, in Beverly Hills one night, a husband and wife passed me, walking their dog. I stood staring after them, absolutely stunned. The woman held in one hand a small cigarette-package-sized radio, its antenna quivering. From this sprang tiny copper wires which ended in a dainty cone plugged into her right ear. There she was, oblivious to man and dog, listening to far winds and whispers and soap-opera cries, sleep-walking, helped up and down curbs by a husband who might just as well not have been there. This was \"not\" fiction.\n\nThis story echoes Mildred's \"Seashell ear-thimbles\" (i.e., a brand of in-ear headphones) that act as an emotional barrier between her and Montag. In a 2007 interview, Bradbury maintained that people misinterpret his book and that \"Fahrenheit 451\" is really a statement on how mass media like television marginalizes the reading of literature. Regarding minorities, he wrote in his 1979 Coda:There is more than one way to burn a book. And the world is full of people running about with lit matches. Every minority, be it Baptist/Unitarian, Irish/Italian/Octogenarian/Zen Buddhist, Zionist/Seventh-day Adventist, Women's Lib/Republican, Mattachine/Four Square Gospel feels it has the will, the right, the duty to douse the kerosene, light the fuse. [...] Fire-Captain Beatty, in my novel \"Fahrenheit 451\", described how the books were burned first by minorities, each ripping a page or a paragraph from this book, then that, until the day came when the books were empty and the minds shut and the libraries closed forever. [...] Only six weeks ago, I discovered that, over the years, some cubby-hole editors at Ballantine Books, fearful of contaminating the young, had, bit by bit, censored some seventy-five separate sections from the novel. Students, reading the novel, which, after all, deals with censorship and book-burning in the future, wrote to tell me of this exquisite irony. Judy-Lynn del Rey, one of the new Ballantine editors, is having the entire book reset and republished this summer with all the damns and hells back in place.\n\nBook-burning censorship, Bradbury would argue, was a side-effect of these two primary factors; this is consistent with Captain Beatty's speech to Montag about the history of the firemen. According to Bradbury, it is the people, not the state, who are the culprit in \"Fahrenheit 451\". Nevertheless, the role on censorship, state-based or otherwise, is still perhaps the most frequent theme explored in the work.\n\nA variety of other themes in the novel besides censorship have been suggested. Two major themes are resistance to conformity and control of individuals via technology and mass media. Bradbury explores how the government is able to use mass media to influence society and suppress individualism through book burning. The characters Beatty and Faber point out the American population is to blame. Due to their constant desire for a simplistic, positive image, books must be suppressed. Beatty blames the minority groups, who would take offense to published works that displayed them in an unfavorable light. Faber went further to state that the American population simply stopped reading on their own. He notes that the book burnings themselves became a form of entertainment to the general public.\n\nBradbury described himself as \"a \"preventor\" of futures, not a predictor of them.\" He did not believe that book burning was an inevitable part of our future; he wanted to warn against its development. In a later interview, when asked if he believes that teaching \"Fahrenheit 451\" in schools will prevent his totalitarian vision of the future, Bradbury replied in the negative. Rather, he states that education must be at the kindergarten and first-grade level. If students are unable to read then, they will be unable to read \"Fahrenheit 451\".\n\nIn terms of technology, Sam Weller notes that Bradbury \"predicted everything from flat-panel televisions to earbud headphones and twenty-four-hour banking machines.\"\n\n\"Playhouse 90\" broadcast \"A Sound of Different Drummers\" on CBS in 1957, written by Robert Alan Aurthur. The play combined plot ideas from \"Fahrenheit 451\" and \"Nineteen Eighty-Four\". Bradbury sued and eventually won on appeal.\n\nA film adaptation written and directed by François Truffaut and starring Oskar Werner and Julie Christie was released in 1966.\n\nIn the late 1970s Bradbury adapted his book into a play. At least part of it was performed at the Colony Theatre in Los Angeles in 1979, but it was not in print until 1986 and the official world premiere was only in November 1988 by the Fort Wayne, Indiana Civic Theatre. The stage adaptation diverges considerably from the book and seems influenced by Truffaut's movie. For example, fire chief Beatty's character is fleshed out and is the wordiest role in the play. As in the movie, Clarisse does not simply disappear but in the finale meets up with Montag as a book character (she as Robert Louis Stevenson, he as Edgar Allan Poe).\n\nBBC Radio produced a one-off dramatization of the novel in 1982 starring Michael Pennington. It was broadcast again on February 12, 2012, and April 7 and 8, 2013, on BBC Radio 4 Extra.\n\nIn 1984, the novel was adapted into a computer text adventure game of the same name by the software company Trillium.\n\nThe UK premiere of Bradbury's stage adaptation was not until 2003 in Nottingham, while it took until 2006 before the Godlight Theatre Company produced and performed its New York City premiere at 59E59 Theaters. After the completion of the New York run, the production then transferred to the Edinburgh Festival where it was a 2006 Edinburgh Festival \"Pick of the Fringe\".\n\nThe Off-Broadway theatre The American Place Theatre presented a one man show adaptation of \"Fahrenheit 451\" as a part of their 2008–2009 Literature to Life season.\n\nIn June 2009, a graphic novel edition of the book was published. Entitled \"Ray Bradbury's Fahrenheit 451: The Authorized Adaptation\", the paperback graphic adaptation was illustrated by Tim Hamilton. The introduction in the novel is written by Bradbury.\n\n\"Fahrenheit 451\" inspired the Birmingham Repertory Theatre production \"Time Has Fallen Asleep in the Afternoon Sunshine\", which was performed at the Birmingham Central Library in April 2012.\n\nA new film adaptation is in development for HBO and is to be directed by Ramin Bahrani, and starring Michael B. Jordan and Michael Shannon. Sofia Boutella, Lilly Singh, and Laura Harrier joined the cast in June of 2017.\n\nMichael Moore's 2004 documentary \"Fahrenheit 9/11\" refers to Bradbury's novel and the September 11 attacks, emphasized by the film's tagline \"\"The temperature where freedom burns\"\". The film takes a critical look at the presidency of George W. Bush, the War on Terror, and its coverage in the news media, and became the highest grossing documentary of all time. Bradbury was upset by what he considered the appropriation of his title, and wanted the film renamed.\n\nIn 2015, the Internet Engineering Steering Group approved the publication of \"An HTTP Status Code to Report Legal Obstacles\", now RFC 7725, which specifies that websites forced to block resources for legal reasons should return a status code of 451 when users request those resources.\n\n\"Centigrade 232\" is a poem by Robert Calvert, published in a 1977 book and released as an album in 2007. The title alludes to \"Fahrenheit 451\" by its metric equivalent, \"signifying the writer destroying his rough drafts\".\n\n\n"}
{"id": "10957", "url": "https://en.wikipedia.org/wiki?curid=10957", "title": "Francis Xavier", "text": "Francis Xavier\n\nSaint Francis Xavier, S.J. (born Francisco de Jasso y Azpilicueta, 7 April 15063 December 1552), was a Navarrese Basque Roman Catholic missionary, born in Javier (Xavier in Navarro-Aragonese or Xabier in Basque), Kingdom of Navarre (present day Spain), and a co-founder of the Society of Jesus. He was a companion of Saint Ignatius of Loyola and one of the first seven Jesuits who took vows of poverty and chastity at Montmartre, Paris in 1534. He led an extensive mission into Asia, mainly in the Portuguese Empire of the time and was influential in evangelization work, most notably in India. The Goa Inquisition was proposed by St. Francis Xavier. He also was the first Christian missionary to venture into Japan, Borneo, the Maluku Islands, and other areas. In those areas, struggling to learn the local languages and in the face of opposition, he had less success than he had enjoyed in India. Xavier was about to extend his missionary preaching to China but died in Shangchuan Island shortly before he could do so.\n\nHe was beatified by Pope Paul V on 25 October 1619 and canonized by Pope Gregory XV on 12 March 1622. In 1624 he was made co-patron of Navarre alongside Santiago. Known as the \"Apostle of the Indies,\" and the \"Apostle of Japan\", he is considered to be one of the greatest missionaries since Saint Paul. In 1927, Pope Pius XI published the decree \"Apostolicorum in Missionibus\" naming Saint Francis Xavier, along with Saint Thérèse of Lisieux, co-patron of all foreign missions. He is now co-patron saint of Navarre with San Fermin. The Day of Navarre (Día de Navarra) in Spain marks the anniversary of Saint Francis Xavier's death, on 3 December 1552.\n\nFrancis Xavier was born in the royal castle of Xavier, in the Kingdom of Navarre, on 7 April 1506 according to a family register. He was the youngest son of Juan de Jasso y Atondo, seneschal of Xavier castle, who belonged to a prosperous farming family and had acquired a doctorate in law at the University of Bologna, and later became privy counsellor and finance minister to King John III of Navarre (Jean d'Albret). Francis' mother was Doña María de Azpilcueta y Aznárez, sole heiress of two noble Navarrese families. He was thus related to the great theologian and philosopher Martín de Azpilcueta.\n\nIn 1512, Ferdinand, King of Aragon and regent of Castile, invaded Navarre, initiating a war that lasted over 18 years. Three years later, Francis' father died when Francis was only nine years old. In 1516, Francis's brothers participated in a failed Navarrese-French attempt to expel the Spanish invaders from the kingdom. The Spanish Governor, Cardinal Cisneros, confiscated the family lands, demolished the outer wall, the gates, and two towers of the family castle, and filled in the moat. In addition, the height of the keep was reduced by half. Only the family residence inside the castle was left. In 1522 one of Francis's brothers participated with 200 Navarrese nobles in dogged but failed resistance against the Castilian Count of Miranda in Amaiur, Baztan, the last Navarrese territorial position south of the Pyrenees.\n\nIn 1525, Francis went to study in Paris at the Collège Sainte-Barbe, University of Paris, where he would spend the next eleven years. In the early days he acquired some reputation as an athlete and a fine high-jumper.\n\nIn 1529, Francis shared lodgings with his friend Pierre Favre. A new student, Ignatius of Loyola, came to room with them. At 38, Ignatius was much older than Pierre and Francis, who were both 23 at the time. Pierre was won over by Ignatius to become a priest, but Francis had aspirations of worldly advancement. At first Francis was not much taken with Ignatius. He regarded the new lodger as a joke and was sarcastic about his efforts to convert students. Only after Pierre left their lodgings to visit his family, when Ignatius was alone with the proud Navarrese, was he was able to slowly break down Francis's stubborn resistance. According to most biographies Ignatius is said to have posed the question: \"What will it profit a man to gain the whole world, and lose his own soul?\" However, according to James Broderick such method is not characteristic of Ignatius and there is no evidence that he employed it at all.\n\nIn 1530 Francis received the degree of Master of Arts, and afterwards taught Aristotelian philosophy at Beauvais College, University of Paris.\n\nOn 15 August 1534, seven students met in a crypt beneath the Church of Saint Denis (now Saint Pierre de Montmartre), in Montmartre outside Paris. They were Francis, Ignatius of Loyola, Alfonso Salmeron, Diego Laínez, Nicolás Bobadilla from Spain, Peter Faber from Savoy, and Simão Rodrigues from Portugal. They made private vows of poverty, chastity, and obedience to the Pope, and also vowed to go to the Holy Land to convert infidels. Francis began his study of theology in 1534 and was ordained on 24 June 1537.\n\nIn 1539, after long discussions, Ignatius drew up a formula for a new monastic order, the Society of Jesus (the Jesuits). Ignatius's plan for the order was approved by Pope Paul III in 1540.\n\nIn 1540 King John of Portugal had Pedro Mascarenhas, Portuguese ambassador to the Vatican, request Jesuit missionaries to spread the faith in his new Indian possessions, where the king believed that Christian values were eroding among the Portuguese. After successive appeals to the Pope asking for missionaries for the East Indies under the Padroado agreement, John III was encouraged by Diogo de Gouveia, rector of the Collège Sainte-Barbe, to recruit the newly graduated youngsters that would establish the Society of Jesus.\n\nLoyola promptly appointed Nicholas Bobadilla and Simão Rodrigues. At the last moment, however, Bobadilla became seriously ill. With some hesitance and uneasiness, Ignatius asked Francis to go in Bobadilla's place. Thus, Xavier accidentally began his life as the first Jesuit missionary.\n\nLeaving Rome on 15 March 1540, in the Ambassador's train, Francis took with him a breviary, a catechism, and by Croatian humanist Marko Marulić, a Latin book that had become popular in the Counter-Reformation. According to a 1549 letter of F. Balthasar Gago in Goa, it was the only book that Francis read or studied. Francis reached Lisbon in June 1540 and four days after his arrival, he and Rodrigues were summoned to a private audience with the King and the Queen.\n\nFrancis Xavier devoted much of his life to missions in Asia, mainly in four centres: Malacca, Amboina and Ternate, Japan, and China. His growing information about new places indicated to him that he had to go to what he understood were centres of influence for the whole region. China loomed large from his days in India. Japan was particularly attractive because of its culture. For him, these areas were interconnected; they could not be evangelised separately.\n\nFrancis Xavier left Lisbon on 7 April 1541, his thirty-fifth birthday, along with two other Jesuits and the new viceroy Martim Afonso de Sousa, on board the \"Santiago\". As he departed, Francis was given a brief from the pope appointing him apostolic nuncio to the East. From August until March 1542 he remained in Portuguese Mozambique, and arrived in Goa, then capital of Portuguese India on 6 May 1542, thirteen months after leaving Lisbon.\n\nFollowing quickly on the great voyages of discovery, the Portuguese had established themselves at Goa thirty years earlier. Francis' primary mission, as ordered by King John III, was to restore Christianity among the Portuguese settlers. According to Teotonio R. DeSouza, recent critical accounts indicate that apart from the posted civil servants, \"the great majority of those who were dispatched as 'discoverers' were the riff-raff of Portuguese society, picked up from Portuguese jails.\" Nor did the soldiers, sailors, or merchants come to do missionary work, and Imperial policy permitted the outflow of disaffected nobility. Many of the arrivals formed liaisons with local women and adopted Indian culture. Missionaries often wrote against the \"scandalous and undisciplined\" behaviour of their fellow Christians.\n\nThe Christian population had churches, clergy, and a bishop, but there were few preachers and no priests beyond the walls of Goa. Xavier decided that he must begin by instructing the Portuguese themselves, and gave much of his time to the teaching of children. The first five months he spent in preaching and ministering to the sick in the hospitals. After that, he walked through the streets ringing a bell to summon the children and servants to catechism. He was invited to head Saint Paul's College, a pioneer seminary for the education of secular priests, which became the first Jesuit headquarters in Asia.\nXavier soon learned that along the Pearl Fishery Coast, which extends from Cape Comorin on the southern tip of India to the island of Mannar, off Ceylon (Sri Lanka), there was a Jāti of people called Paravas. Many of them had been baptised ten years before, merely to please the Portuguese, who had helped them against the Moors, but remained uninstructed in the faith. Accompanied by several native clerics from the seminary at Goa, he set sail for Cape Comorin in October 1542. First he set himself to learn the language of the Paravas; he taught those who had already been baptised, and preached to those who weren't. His efforts with the high-caste Brahmins remained unavailing.\n\nHe devoted almost three years to the work of preaching to the people of southern India and Ceylon, converting many. Many were the difficulties and hardships which Xavier had to encounter at this time, sometimes because the Portuguese soldiers, far from seconding his work, hampered it by their bad example and vicious habits. He built nearly 40 churches along the coast, including St. Stephen's Church, Kombuthurai, mentioned in his letters dated 1544.\n\nDuring this time, he was able to visit the tomb of Thomas the Apostle in Mylapore (now part of Madras (Chennai) then in Portuguese India). He set his sights eastward in 1545 and planned a missionary journey to Makassar on the island of Celebes (today's Indonesia).\n\nAs the first Jesuit in India, Francis had difficulty achieving much success in his missionary trips. His successors, such as de Nobili, Matteo Ricci, and Beschi, attempted to convert the noblemen first as a means to influence more people, while Francis had initially interacted most with the lower classes (later though, in Japan, Francis changed tack by paying tribute to the Emperor and seeking an audience with him).\nIn the spring of 1545 Xavier started for Portuguese Malacca. He laboured there for the last months of that year. About January 1546, Xavier left Malacca for the Maluku Islands, where the Portuguese had some settlements. For a year and a half he preached the Gospel there. He went first to Ambon Island, where he stayed until mid-June. He then visited other Maluku Islands, including Ternate, Baranura, and Morotai. Shortly after Easter 1547, he returned to Ambon Island; a few months later he returned to Malacca.\n\nIn Malacca in December 1547, Francis Xavier met a Japanese man named Anjirō. Anjirō had heard of Francis in 1545 and had travelled from Kagoshima to Malacca to meet him. Having been charged with murder, Anjirō had fled Japan. He told Francis extensively about his former life and the customs and culture of his homeland. Anjirō became the first Japanese Christian and adopted the name of 'Paulo de Santa Fe'. He later helped Xavier as a mediator and interpreter for the mission to Japan that now seemed much more possible.\n\nIn January 1548 Francis returned to Goa to attend to his responsibilities as superior of the mission there. The next 15 months were occupied with various journeys and administrative measures. He left Goa on 15 April 1549, stopped at Malacca, and visited Canton. He was accompanied by Anjiro, two other Japanese men, Father Cosme de Torrès, and Brother João Fernandes. He had taken with him presents for the \"King of Japan\" since he was intending to introduce himself as the Apostolic Nuncio.\n\nEuropeans had already come to Japan: the Portuguese had landed in 1543 on the island of Tanegashima, where they introduced the first firearms to Japan.\n\nFrom Amboina, he wrote to his companions in Europe: \"I asked a Portuguese merchant, … who had been for many days in Anjirō’s country of Japan, to give me … some information on that land and its people from what he had seen and heard …. All the Portuguese merchants coming from Japan tell me that if I go there I shall do great service for God our Lord, more than with the pagans of India, for they are a very reasonable people. (To His Companions Residing in Rome, From Cochin, 20 January 1548, no. 18, p. 178).\n\nFrancis Xavier reached Japan on 27 July 1549, with Anjiro and three other Jesuits, but he was not permitted to enter any port his ship arrived at until 15 August, when he went ashore at Kagoshima, the principal port of Satsuma Province on the island of Kyūshū. As a representative of the Portuguese king, he was received in a friendly manner. Shimazu Takahisa (1514–1571), \"daimyō\" of Satsuma, gave a friendly reception to Francis on 29 September 1549, but in the following year he forbade the conversion of his subjects to Christianity under penalty of death; Christians in Kagoshima could not be given any catechism in the following years. The Portuguese missionary Pedro de Alcáçova would later write in 1554:\n\nHe was hosted by Anjirō's family until October 1550. From October to December 1550, he resided in Yamaguchi. Shortly before Christmas, he left for Kyoto but failed to meet with the Emperor. He returned to Yamaguchi in March 1551, where he was permitted to preach by the daimyo of the province. However, lacking fluency in the Japanese language, he had to limit himself to reading aloud the translation of a catechism.\n\nFrancis was the first Jesuit to go to Japan as a missionary. He brought with him paintings of the Madonna and the Madonna and Child. These paintings were used to help teach the Japanese about Christianity. There was a huge language barrier as Japanese was unlike other languages the missionaries had previously encountered. For a long time Francis struggled to learn the language.\n\nHaving learned that evangelical poverty did not have the appeal in Japan that it had in Europe and in India, he decided to change his approach. Hearing after a time that a Portuguese ship had arrived at a port in the province of Bungo in Kyushu and that the prince there would like to see him, Xavier now set out southward. The Jesuit, in a fine cassock, surplice, and stole, was attended by thirty gentlemen and as many servants, all in their best clothes. Five of them bore on cushions valuable articles, including a portrait of Our Lady and a pair of velvet slippers, these not gifts for the prince, but solemn offerings to Xavier, to impress the onlookers with his eminence. Handsomely dressed, with his companions acting as attendants, he presented himself before Oshindono, the ruler of Nagate, and as a representative of the great kingdom of Portugal offered him the letters and presents, a musical instrument, a watch, and other attractive objects which had been given him by the authorities in India for the emperor.\n\nFor forty-five years the Jesuits were the only missionaries in Asia, but the Franciscans also began proselytising in Asia as well. Christian missionaries were later forced into exile, along with their assistants. Some were able to stay behind, however Christianity was then kept underground so as to not be persecuted.\n\nThe Japanese people were not easily converted; many of the people were already Buddhist or Shinto. Francis tried to combat the disposition of some of the Japanese that a God who had created everything, including evil, could not be good. The concept of Hell was also a struggle; the Japanese were bothered by the idea of their ancestors living in Hell. Despite Francis' different religion, he felt that they were good people, much like Europeans, and could be converted.\n\nXavier was welcomed by the Shingon monks since he used the word \"Dainichi\" for the Christian God; attempting to adapt the concept to local traditions. As Xavier learned more about the religious nuances of the word, he changed to \"Deusu\" from the Latin and Portuguese \"Deus\". The monks later realised that Xavier was preaching a rival religion and grew more aggressive towards his attempts at conversion.\nWith the passage of time, his sojourn in Japan could be considered somewhat fruitful as attested by congregations established in Hirado, Yamaguchi, and Bungo. Xavier worked for more than two years in Japan and saw his successor-Jesuits established. He then decided to return to India. Historians debate the exact path he returned by, but from evidence attributed to the captain of his ship, he may have travelled through Tanegeshima and Minato, and avoided Kagoshima because of the hostility of the daimyo. During his trip, a tempest forced him to stop on an island near Guangzhou, China where he met Diogo Pereira, a rich merchant and an old friend from Cochin. Pereira showed him a letter from Portuguese prisoners in Guangzhou, asking for a Portuguese ambassador to speak to the Chinese Emperor on their behalf. Later during the voyage, he stopped at Malacca on 27 December 1551, and was back in Goa by January 1552.\n\nOn 17 April he set sail with Diogo Pereira on the \"Santa Cruz\" for China. He planned to introduce himself as Apostolic Nuncio and Pereira as ambassador of the King of Portugal. But then he realized that he had forgotten his testimonial letters as an Apostolic Nuncio. Back in Malacca, he was confronted by the \"capitão\" Álvaro de Ataíde da Gama who now had total control over the harbour. The \"capitão\" refused to recognize his title of Nuncio, asked Pereira to resign from his title of ambassador, named a new crew for the ship, and demanded the gifts for the Chinese Emperor be left in Malacca.\n\nIn late August 1552, the \"Santa Cruz\" reached the Chinese island of Shangchuan, 14 km away from the southern coast of mainland China, near Taishan, Guangdong, 200 km south-west of what later became Hong Kong. At this time, he was accompanied only by a Jesuit student, Álvaro Ferreira, a Chinese man called António, and a Malabar servant called Christopher. Around mid-November he sent a letter saying that a man had agreed to take him to the mainland in exchange for a large sum of money. Having sent back Álvaro Ferreira, he remained alone with António. He died at Shangchuan, Taishan, China from a fever on 3 December 1552, while he was waiting for a boat that would take him to mainland China.\n\nHe was first buried on a beach at Shangchuan Island, Taishan, Guangdong. His incorrupt body was taken from the island in February 1553 and was temporarily buried in St. Paul's church in Portuguese Malacca on 22 March 1553. An open grave in the church now marks the place of Xavier's burial. Pereira came back from Goa, removed the corpse shortly after 15 April 1553, and moved it to his house. On 11 December 1553, Xavier's body was shipped to Goa. The body is now in the Basilica of Bom Jesus in Goa, where it was placed in a glass container encased in a silver casket on 2 December 1637. This casket, constructed by Goan silversmiths between 1636 and 1637, was an exemplary blend of Italian and Indian aesthetic sensibilities. There are 32 silver plates on all the four sides of the casket depicting different episodes from the life of the Saint:\n\n\nThe right forearm, which Xavier used to bless and baptise his converts, was detached by Superior General Claudio Acquaviva in 1614. It has been displayed since in a silver reliquary at the main Jesuit church in Rome, Il Gesù.\n\nAnother of Xavier's arm bones was brought to Macau where it was kept in a silver reliquary. The relic was destined for Japan but religious persecution there persuaded the church to keep it in Macau's Cathedral of St. Paul. It was subsequently moved to St. Joseph's and in 1978 to the Chapel of St. Francis Xavier on Coloane Island. More recently the relic was moved to St. Joseph's Church.\n\nIn 2006, on the 500th anniversary of his birth, the Xavier Tomb Monument and Chapel on the Shangchuan Island, in ruins after years of neglect under communist rule in China was restored with the support from the alumni of Wah Yan College, a Jesuit high school in Hong Kong.\n\nFrancis Xavier was beatified by Paul V on 25 October 1619, and was canonized by Gregory XV on 12 March (12 April) 1622, at the same time as Ignatius Loyola. Pius XI proclaimed him the \"Patron of Catholic Missions\". His feast day is 3 December.\n\nSaint Francis Xavier's relics are kept in a silver casket, elevated inside the Bom Jesus Basilica and are exposed (being brought to ground level) generally every ten years, but this is discretionary. The sacred relics went on display starting on 22 November 2014 at the XVII Solemn Exposition. The display closed on 4 January 2015. The previous exposition, the sixteenth, was held from 21 November 2004 to 2 January 2005.\n\nRelics of Saint Francis Xavier are also found in the Espirito Santo (Holy Spirit) Church, Margão, in Sanv Fransiku Xavierachi Igorz (Church of St. Francis Xavier), Batpal, Canacona, Goa and at St. Francis Xavier Chapel, Portais, Panjim.\n\nOther pilgrimage centres include Saint Francis Xavier's birthplace in Navarra, Church of Il Gesu, Rome, Malacca (where he was buried for 2 years, before being brought to Goa), Sancian (Place of death) etc.\n\nSaint Francis Xavier is a major venerated saint in both Sonora and the neighbouring U.S. state of Arizona. In Magdalena de Kino in Sonora, Mexico in the Church of Santa María Magdalena, there is reclining statue of San Francisco Xavier brought by pioneer Jesuit missionary Padre Eusebio Kino in the early 18th century. The statue is said to be miraculous and is the object of pilgrimage for many of the region. Also Mission San Xavier del Bac is a pilgrimage site. The mission is an active parish church ministering to the people of the San Xavier District, Tohono O'odham Nation and nearby Tucson, Arizona.\n\nThe Novena of Grace is a popular devotion to Francis Xavier, typically prayed either on the nine days before 3 December, or on 4 March through 12 March (the anniversary of Pope Gregory XV's canonisation of Xavier in 1622). It began with the Italian Jesuit missionary Marcello Mastrilli. Before he could travel to the Far East, Mastrilli was gravely injured in a freak accident after a festive celebration dedicated to the Immaculate Conception in Naples. Delirious and on the verge of death, Mastrilli saw Xavier, who he later said asked him to choose between travelling or death by holding the respective symbols, to which Mastrilli answered, \"I choose that which God wills.\" Upon regaining his health, Mastrilli made his way via Goa and the Philippines to Satsuma, Japan. The Tokugawa shogunate beheaded the missionary in October 1637, after undergoing three days of tortures involving the volcanic sulphurous fumes from Mt. Unzen, known as the \"Hell mouth\" or \"pit\" that had supposedly caused an earlier missionary to renounce his faith.\n\nSaint Francis Xavier is noteworthy for his missionary work, both as organiser and as pioneer, reputed to have converted more people than anyone else has done since Saint Paul. Pope Benedict XVI said of both Ignatius of Loyola and Francis Xavier: \"not only their history which was interwoven for many years from Paris and Rome, but a unique desire — a unique passion, it could be said — moved and sustained them through different human events: the passion to give to God-Trinity a glory always greater and to work for the proclamation of the Gospel of Christ to the peoples who had been ignored.\" By consulting with the earlier ancient Christians of St. Thomas in India, Xavier developed Jesuit missionary methods. His success also spurred many Europeans to join the order, as well as become missionaries throughout the world. His personal efforts most affected Christians in India and the East Indies (Indonesia, Malaysia, Timor). India still has numerous Jesuit missions, and many more schools. Xavier also worked to propagate Christianity in China and Japan. However, following the persecutions of Toyotomi Hideyoshi and the subsequent closing of Japan to foreigners, the Christians of Japan were forced to go underground to develop an independent Christian culture. Likewise, while Xavier inspired many missionaries to China, Chinese Christians also were forced underground and developed their own Christian culture.\n\nFrancis Xavier is the patron saint of his native Navarre, which celebrates his feast day on 3 December as a government holiday. In addition to Roman Catholic masses remembering Xavier on that day (now known as the Day of Navarra), celebrations in the surrounding weeks honour the region's cultural heritage. Furthermore, in the 1940s, devoted Catholics instituted the Javierada, an annual day-long pilgrimage (often on foot) from the capital at Pamplona to Xavier, where his order has built a basilica and museum and restored his family's castle.\n\nAs the foremost saint from Navarre and one of the main Jesuit saints, he is much venerated in Spain and the Hispanic countries where \"Francisco Javier\" or \"Javier\" are common male given names. The alternative spelling \"Xavier\" is also popular in Portugal, Catalonia, Brazil, France, Belgium, and southern Italy. In India, the spelling \"Xavier\" is almost always used, and the name is quite common among Christians, especially in Goa and the southern states of Tamil Nadu, Kerala, and Karnataka. The names \"Francisco Xavier\", \"António Xavier\", \"João Xavier\", \"Caetano Xavier\", \"Domingos Xavier\" et cetera, were very common till quite recently in Goa. \"Fransiskus Xaverius\" is commonly used as a name for Indonesian Catholics, usually abbreviated as FX. In Austria and Bavaria the name is spelled as \"Xaver\" (pronounced (ˈk͡saːfɐ)) and often used in addition to Francis as \"Franz-Xaver\" (frant͡sˈk͡saːfɐ). Many Catalan men are named for him, often using the two-name combination Francesc Xavier. In English speaking countries, \"Xavier\" until recently was likely to follow \"Francis\"; in the 2000s, however, \"Xavier\" by itself has become more popular than \"Francis\", and since 2001 is now one of the hundred most common male baby names in the U.S.A. Furthermore, the Sevier family name, possibly most famous in the United States for John Sevier originated from the name Xavier.\n\nMany churches all over the world, often founded by Jesuits, have been named in honour of Xavier. Those in the United States include the historic St. Francis Xavier Shrine at Warwick, Maryland, (founded 1720, and at which American founding father, Charles Carroll of Carrollton, (1737–1832), (longest living signer and only Catholic at the Continental Congress to sign the Declaration of Independence, 1776) and cousin to the first American-born Bishop John Carroll, (1735–1815), Bishop and later Archbishop of Baltimore, 1790–1815, (at the Roman Catholic Archdiocese of Baltimore) began their education), also the American educational teaching order Xaverian Brothers, the Basilica of St. Francis Xavier in Dyersville, Iowa, and the Mission San Xavier del Bac in Tucson, Arizona (founded in 1692, and known for its Spanish Colonial architecture).\n\nRubens painted \"St Francis Xavier Raising the Dead\", for a Jesuit church in Antwerp, and in which he depicted one of St Francis' many miracles (in this case a resurrection). The Charles Bridge in Prague, Czech Republic features a statue of Francis Xavier.\n\nShortly before leaving he had issued a famous instruction to Father Gaspar Barazeuz who was leaving to go to Ormuz (a kingdom on an island in the Persian Gulf, formerly attached to the Empire of Persia, now part of Iran), that he should mix with sinners:\n\nModern scholars place the number of people converted to Christianity by Francis Xavier at around 30,000. And while some of Xavier's methods have been since criticised (he forced converts to take Portuguese names and dress in Western clothes, approved the persecution of the Eastern Church, and used the Goa government as a missionary tool), he has also earned praise. He insisted that missionaries adapt to many of the customs, and most certainly the language, of the culture they wish to evangelise. And unlike later missionaries, Xavier supported an educated native clergy. Though for a time, it seemed his work in Japan was subsequently destroyed by persecution, Protestant missionaries three centuries later discovered that approximately 100,000 Christians still practised in the Nagasaki area.\n\nFrancis Xavier's work initiated permanent change in eastern Indonesia, and he was known as the \"Apostle of the Indies\" where in 1546–1547 he worked in the Maluku Islands among the people of Ambon, Ternate, and Morotai (or Moro), and laid the foundations for a permanent mission. After he left the Maluku Islands, others carried on his work and by the 1560s there were 10,000 Roman Catholics in the area, mostly on Ambon. By the 1590s there were 50,000 to 60,000.\n\nDeeply imbued with the theology of the later Augustine, he was fiercely \"jealous\" of \"God's greater glory\" and deeply suspicious of the \"untutored\" efforts of man to scale the heights of the spirit. This world view led him to missionary tactics that even the Jesuit James Patrick Broderick, though writing an admiring biography, condemns Xavier's \"woefully inadequate views about Indian religion and civilization\".\n\nThe role of Francis Xavier in the Goa Inquisition is controversial. He had written to King João III of Portugal in 1546, encouraging him to dispatch the Inquisition to Goa, which he did many years later in 1560. Francis Xavier died in 1552 without living to see the horrors of the Goa Inquisition, but some historians believe that he was aware of the Portuguese Inquisition's brutality. In an interview to an Indian newspaper, historian Teotónio de Souza stated that Francis Xavier and Simão Rodrigues, another founder-member of the Society of Jesus, were together in Lisbon before Francis left for India. Both were asked to assist spiritually the prisoners of the Inquisition and were present at the very first auto-da-fé celebrated in Portugal in September 1540, at which 23 were absolved and two were condemned to be burnt, including a French cleric. Hence, he believes that Xavier was aware of the brutality of the Inquisition.\n\n\n\n"}

{"id": "5966", "url": "https://en.wikipedia.org/wiki?curid=5966", "title": "Compost", "text": "Compost\n\nCompost ( or ) is organic matter that has been decomposed and recycled as a fertilizer and soil amendment. Compost is a key ingredient in organic farming.\n\nAt the simplest level, the process of composting requires making a heap of wet organic matter known as green waste (leaves, food waste) and waiting for the materials to break down into humus after a period of weeks or months. Modern, methodical composting is a multi-step, closely monitored process with measured inputs of water, air, and carbon- and nitrogen-rich materials. The decomposition process is aided by shredding the plant matter, adding water and ensuring proper aeration by regularly turning the mixture. Worms and fungi further break up the material. Bacteria requiring oxygen to function (aerobic bacteria) and fungi manage the chemical process by converting the inputs into heat, carbon dioxide and ammonium. The ammonium () is the form of nitrogen used by plants. When available ammonium is not used by plants it is further converted by bacteria into nitrates () through the process of nitrification.\n\nCompost is rich in nutrients. It is used in gardens, landscaping, horticulture, and agriculture. The compost itself is beneficial for the land in many ways, including as a soil conditioner, a fertilizer, addition of vital humus or humic acids, and as a natural pesticide for soil. In ecosystems, compost is useful for erosion control, land and stream reclamation, wetland construction, and as landfill cover (see compost uses). Organic ingredients intended for composting can alternatively be used to generate biogas through anaerobic digestion.\n\nComposting of waste is an aerobic (in the presence of air) method of decomposing solid wastes. The process involves decomposition of organic waste into humus known as compost which is a good fertilizer for plants. However, the term \"composting\" is used worldwide with differing meanings. Some composting textbooks narrowly define composting as being an aerobic form of decomposition, primarily by aerobic or facultative microbes. An alternative form of organic decomposition to composting is \"anaerobic digestion\".\n\nFor many people, composting is used to refer to several different types of biological processes. In North America, \"anaerobic composting\" is still a common term for what much of the rest of the world and in technical publications people call \"anaerobic digestion\". The microbes used and the processes involved are quite different between composting and anaerobic digestion.\n\nComposting organisms require four equally important ingredients to work effectively:\n\nCertain ratios of these materials will provide beneficial bacteria with the nutrients to work at a rate that will heat up the pile. In that process much water will be released as vapor (\"steam\"), and the oxygen will be quickly depleted, explaining the need to actively manage the pile. The hotter the pile gets, the more often added air and water is necessary; the air/water balance is critical to maintaining high temperatures (135°-160° Fahrenheit / 50° - 70° Celsius) until the materials are broken down. At the same time, too much air or water also slows the process, as does too much carbon (or too little nitrogen). Hot container composting focuses on retaining the heat to increase decomposition rate and produce compost more quickly.\n\nThe most efficient composting occurs with an optimal carbon:nitrogen ratio of about 10:1 to 20:1. Rapid composting is favored by having a C/N ratio of ~30 or less. Theoretical analysis is confirmed by field tests that above 30 the substrate is nitrogen starved, below 15 it is likely to outgas a portion of nitrogen as ammonia.\n\nNearly all plant and animal materials have both carbon and nitrogen, but amounts vary widely, with characteristics noted above (dry/wet, brown/green). Fresh grass clippings have an average ratio of about 15:1 and dry autumn leaves about 50:1 depending on species. Mixing equal parts by volume approximates the ideal C:N range. Few individual situations will provide the ideal mix of materials at any point. Observation of amounts, and consideration of different materials as a pile is built over time, can quickly achieve a workable technique for the individual situation.\n\nWith the proper mixture of water, oxygen, carbon, and nitrogen, micro-organisms are able to break down organic matter to produce compost. The composting process is dependent on micro-organisms to break down organic matter into compost. There are many types of microorganisms found in active compost of which the most common are:\n\nIn addition, earthworms not only ingest partly composted material, but also continually re-create aeration and drainage tunnels as they move through the compost.\n\nA lack of a healthy micro-organism community is the main reason why composting processes are slow in landfills with environmental factors such as lack of oxygen, nutrients or water being the cause of the depleted biological community.\n\nUnder ideal conditions, composting proceeds through three major phases:\n\nThere are many modern proponents of rapid composting that attempt to correct some of the perceived problems associated with traditional, slow composting. Many advocate that compost can be made in 2 to 3 weeks. Many such short processes involve a few changes to traditional methods, including smaller, more homogenized pieces in the compost, controlling carbon-to-nitrogen ratio (C:N) at 30 to 1 or less, and monitoring the moisture level more carefully. However, none of these parameters differ significantly from the early writings of compost researchers, suggesting that in fact modern composting has not made significant advances over the traditional methods that take a few months to work. For this reason and others, many modern scientists who deal with carbon transformations are sceptical that there is a \"super-charged\" way to get nature to make compost rapidly. \n\nIn fact, both sides are right to some extent. The bacterial activity in rapid high heat methods breaks down the material to the extent that pathogens and seeds are destroyed, and the original feedstock is unrecognizable. At this stage, the compost can be used to prepare fields or other planting areas. However, most professionals recommend that the compost be given time to cure before using in a nursery for starting seeds or growing young plants. The curing time allows fungi to continue the decomposition process and eliminating phytotoxic substances.\n\nComposting can destroy pathogens or unwanted seeds. Unwanted living plants (or weeds) can be discouraged by covering with mulch/compost. The \"microbial pesticides\" in compost may include thermophiles and mesophiles, however certain composting detritivores such as black soldier fly larvae and redworms, also reduce many pathogens. The first stage of bokashi preserves the ingredients in a lactic acid fermentation. The acid is a natural disinfectant, used as such in household cleaning products, so that what enters the second (digestion) stage is essentially free of microbial pathogens. Thermophilic (high-temperature) composting is well known to destroy many seeds and nearly all types of pathogens (exceptions may include prions). The sanitizing qualities of (thermophilic) composting are desirable where there is a high likelihood of pathogens, such as with manure.\n\nAs concern about landfill space increases, worldwide interest in recycling by means of composting is growing, since composting is a process for converting decomposable organic materials into useful stable products. Composting is one of the only ways to revitalize soil vitality due to phosphorus depletion in soil.\n\nCo-composting is a technique that combines solid waste with de-watered biosolids, although difficulties controlling inert and plastics contamination from municipal solid waste makes this approach less attractive.\n\nIndustrial composting systems are increasingly being installed as a waste management alternative to landfills, along with other advanced waste processing systems. Mechanical sorting of mixed waste streams combined with anaerobic digestion or in-vessel composting is called mechanical biological treatment, and is increasingly being used in developed countries due to regulations controlling the amount of organic matter allowed in landfills. Treating biodegradable waste before it enters a landfill reduces global warming from fugitive methane; untreated waste breaks down anaerobically in a landfill, producing landfill gas that contains methane, a potent greenhouse gas.\n\nOn many farms, the basic composting ingredients are animal manure generated on the farm and bedding. Straw and sawdust are common bedding materials. Non-traditional bedding materials are also used, including newspaper and chopped cardboard. The amount of manure composted on a livestock farm is often determined by cleaning schedules, land availability, and weather conditions. Each type of manure has its own physical, chemical, and biological characteristics. Cattle and horse manures, when mixed with bedding, possess good qualities for composting. Swine manure, which is very wet and usually not mixed with bedding material, must be mixed with straw or similar raw materials. Poultry manure also must be blended with carbonaceous materials - those low in nitrogen preferred, such as sawdust or straw.\n\nHuman waste (excreta) can also be added as an input to the composting process since human waste is a nitrogen-rich organic material. It can be either composted directly, in composting toilets, or after mixing with water and treatment in a sewage treatment plant, in the form of sewage sludge treatment.\n\nPeople excrete far more water-soluble plant nutrients (nitrogen, phosphorus, potassium) in urine than in feces. Human urine can be used directly as fertilizer or it can be put onto compost. Adding a healthy person's urine to compost usually will increase temperatures and therefore increase its ability to destroy pathogens and unwanted seeds. Unlike feces, urine does not attract disease-spreading flies (such as house flies or blow flies), and it does not contain the most hardy of pathogens, such as parasitic worm eggs. Urine usually does not smell for long, particularly when it is fresh, diluted, or put on sorbents.\n\n\"Humanure\" is a portmanteau of \"human\" and \"manure\", designating human excrement (faeces and urine) that is recycled via composting for agricultural or other purposes. The term was first used in a 1994 book by Joseph Jenkins that advocates the use of this organic soil amendment. The term humanure is used by compost enthusiasts in the US but not generally elsewhere. Because the term \"humanure\" has no authoritative definition it is subject to various uses; news reporters occasionally fail to correctly distinguish between humanure and sewage sludge or \"biosolids\".\n\nCompost can be used as an additive to soil, or other matrices such as coir and peat, as a tilth improver, supplying humus and nutrients. It provides a rich \"growing medium\", or a porous, absorbent material that holds moisture and soluble minerals, providing the support and nutrients in which plants can flourish, although it is rarely used alone, being primarily mixed with soil, sand, grit, bark chips, vermiculite, perlite, or clay granules to produce loam. Compost can be tilled directly into the soil or growing medium to boost the level of organic matter and the overall fertility of the soil. Compost that is ready to be used as an additive is dark brown or even black with an earthy smell.\n\nGenerally, direct seeding into a compost is not recommended due to the speed with which it may dry and the possible presence of phytotoxins that may inhibit germination, and the possible tie up of nitrogen by incompletely decomposed lignin. It is very common to see blends of 20–30% compost used for transplanting seedlings at cotyledon stage or later.\n\nVarious approaches have been developed to handle different ingredients, locations, throughput and applications for the composted product.\n\nIndustrial scale composting can be carried out in the form of in-vessel composting, aerated static pile composting, vermicomposting, windrow composting and takes place in most Western countries now.\n\nVermicompost is the product or process of composting using various species of worms, usually red wigglers, white worms, and earthworms, to create a heterogeneous mixture of decomposing vegetable or food waste (excluding meat, dairy, fats, or oils), bedding materials, and vermicast. Vermicast, also known as worm castings, worm humus or worm manure, is the end-product of the breakdown of organic matter by species of earthworm.\n\nVermicomposting is widely used in North America for on-site institutional processing of food waste, such as in hospitals, universities, shopping malls, and correctional facilities. Vermicomposting, also known as vermiculture, is used for medium-scale on-site institutional composting, such as for food waste from universities and shopping malls. It is selected either as a more environmentally friendly choice than conventional methods of disposal, or to reduce the cost of commercial waste removal.\n\nVermicomposting is a feasible indoor home composting method which has gained popularity in both industrial and domestic settings because, as compared with conventional composting, it provides a way to compost organic materials more quickly (as defined by a higher rate of carbon-to-nitrogen ratio increase) and to attain products that have lower salinity levels that are therefore more beneficial to plant mediums.\n\nThe earthworm species (or composting worms) most often used are red wigglers (\"Eisenia fetida\" or \"Eisenia andrei\"), though European nightcrawlers (\"Eisenia hortensis\" or \"Dendrobaena veneta\") could also be used. Red wigglers are recommended by most vermiculture experts, as they have some of the best appetites and breed very quickly. Users refer to European nightcrawlers by a variety of other names, including \"dendrobaenas\", \"dendras\", Dutch nightcrawlers, and Belgian nightcrawlers.\n\nContaining water-soluble nutrients, vermicompost is a nutrient-rich organic fertilizer and soil conditioner in a form that is relatively easy for plants to absorb. Worm castings are sometimes used as an organic fertilizer. Because the earthworms grind and uniformly mix minerals in simple forms, plants need only minimal effort to obtain them. The worms' digestive systems create environments that allow certain species of microbes to thrive to help create a \"living\" soil environment for plants. The fraction of soil which has gone through the digestive tract of earthworms is called the Drilosphere.\n\nResearchers from the Pondicherry University discovered that worm composts can also be used to clean up heavy metals. The researchers found substantial reductions in heavy metals when the worms were released into the garbage and they are effective at removing lead, zinc, cadmium, copper and manganese.\n\nA composting toilet does not require water or electricity, and when properly managed does not smell. A composting toilet collects human excreta which is then added to a compost heap together with sawdust and straw or other carbon rich materials, where pathogens are destroyed to some extent. The amount of pathogen destruction depends on the temperature (mesophilic or thermophilic conditions) and composting time. A composting toilet tries to process the excreta in situ although this is often coupled with a secondary external composting step. The resulting compost product has been given various names, such as humanure and EcoHumus.\n\nA composting toilet can aid in the conservation of fresh water by avoiding the usage of potable water required by the typical flush toilet. It further prevents the pollution of ground water by controlling the fecal matter decomposition before entering the system. When properly managed, there should be no ground contamination from leachate.\n\nBlack Soldier Fly (\"Hermetia illucens\") larvae have been shown to be able to rapidly consume large amounts of organic waste when kept at 31.8 °C, the optimum temperature for reproduction. Enthusiasts have experimented with a large number of different waste products and some even sell starter kits to the public.\n\nThe practice of making raised garden beds or mounds filled with rotting wood is also called \"Hügelkultur\" in German. It is in effect creating a Nurse log that is covered with soil.\n\nBenefits of hügelkultur garden beds include water retention and warming of soil. Buried wood becomes like a sponge as it decomposes, able to capture water and store it for later use by crops planted on top of the hügelkultur bed.\n\nThe buried decomposing wood will also give off heat, as all compost does, for several years. These effects have been used by Sepp Holzer to enable fruit trees to survive at otherwise inhospitable temperatures and altitudes.\n\nBokashi is a method that uses a mix of microorganisms to cover food waste or wilted plants to decrease smell. Bokashi (ぼかし) is Japanese for \"shading off\" or \"gradation.\" It derives from the practice of Japanese farmers centuries ago of covering food waste with rich, local soil that contained the microorganisms that would ferment the waste.\n\nThe technique often uses effective microorganisms.\n\nNewspaper fermented in a lactobacillus culture can be substituted for bokashi bran for a successful bokashi bucket.\n\nCompost teas are defined as water extracts brewed from composted materials and can be derived from aerobic or anaerobic processes. Compost teas are generally produced from adding one volume of compost to 4-10 volumes of water, but there has also been debate about the benefits of aerating the mixture. Field studies have shown the benefits of adding compost teas to crops due to the adding of organic matter, increased nutrient availability and increased microbial activity. They have also been shown to have an effect on plant pathogens.\n\nAnaerobic digestion is process for converting organic waste into biogas. The residual material, sometimes in combination with sewage sludge can be followed by an aerobic composting process before selling or giving away the compost.\n\nThere are process and product guidelines in Europe that date to the early 1980s (Germany, the Netherlands, Switzerland) and only more recently in the UK and the US. In both these countries, private trade associations within the industry have established loose standards, some say as a stop-gap measure to discourage independent government agencies from establishing tougher consumer-friendly standards.\n\nThe USA is the only Western country that does not distinguish sludge-source compost from green-composts, and by default in the USA 50% of states expect composts to comply in some manner with the federal EPA 503 rule promulgated in 1984 for sludge products.\n\nCompost is regulated in Canada and Australia as well.\n\nMany countries such as Wales and some individual cities such as Seattle and San Francisco require food and yard waste to be sorted for composting (San Francisco Mandatory Recycling and Composting Ordinance).\n\nLarge-scale composting systems are used by many urban areas around the world.\n\nComposting as a recognized practice dates to at least the early Roman Empire, and was mentioned as early as Cato the Elder's 160 BCE piece \"De Agri Cultura\". Traditionally, composting involved piling organic materials until the next planting season, at which time the materials would have decayed enough to be ready for use in the soil. The advantage of this method is that little working time or effort is required from the composter and it fits in naturally with agricultural practices in temperate climates. Disadvantages (from the modern perspective) are that space is used for a whole year, some nutrients might be leached due to exposure to rainfall, and disease-producing organisms and insects may not be adequately controlled.\n\nComposting was somewhat modernized beginning in the 1920s in Europe as a tool for organic farming. The first industrial station for the transformation of urban organic materials into compost was set up in Wels, Austria in the year 1921. Early frequent citations for propounding composting within farming are for the German-speaking world Rudolf Steiner, founder of a farming method called biodynamics, and Annie Francé-Harrar, who was appointed on behalf of the government in Mexico and supported the country 1950–1958 to set up a large humus organization in the fight against erosion and soil degradation.\n\nIn the English-speaking world it was Sir Albert Howard who worked extensively in India on sustainable practices and Lady Eve Balfour who was a huge proponent of composting. Composting was imported to America by various followers of these early European movements by the likes of J.I. Rodale (founder of Rodale Organic Gardening), E.E. Pfeiffer (who developed scientific practices in biodynamic farming), Paul Keene (founder of Walnut Acres in Pennsylvania), and Scott and Helen Nearing (who inspired the back-to-the-land movement of the 1960s). Coincidentally, some of the above met briefly in India - all were quite influential in the U.S. from the 1960s into the 1980s.\n\n\n"}
{"id": "5970", "url": "https://en.wikipedia.org/wiki?curid=5970", "title": "Capitol", "text": "Capitol\n\nA capitol is a building in which a legislature meets, including:\nCapitol may also refer to:\n\n\n"}
{"id": "5973", "url": "https://en.wikipedia.org/wiki?curid=5973", "title": "Cinema", "text": "Cinema\n\nCinema may refer to:\n\n\n\n\n\n"}
{"id": "5974", "url": "https://en.wikipedia.org/wiki?curid=5974", "title": "Corundum", "text": "Corundum\n\nCorundum is a crystalline form of aluminium oxide () typically containing traces of iron, titanium, vanadium and chromium. It is a rock-forming mineral. It is a naturally transparent material, but can have different colors when impurities are present. Transparent specimens are used as gems, called ruby if red and padparadscha if pink-orange. All other colors are called sapphire, e.g., green sapphire for a green specimen.\n\nThe name \"corundum\" is derived from the Tamil word \"Kurundam\", which originates from the Sanskrit word \"Kuruvinda\" meaning \"ruby\".\n\nBecause of corundum's hardness (pure corundum is defined to have 9.0 on the Mohs scale), it can scratch almost every other mineral. It is commonly used as an abrasive on everything from sandpaper to large machines used in machining metals, plastics, and wood. Some emery is a mix of corundum and other substances, and the mix is less abrasive, with an average Mohs hardness of 8.0.\n\nIn addition to its hardness, corundum is unusual for its density of 4.02 g/cm, which is very high for a transparent mineral composed of the low-atomic mass elements aluminium and oxygen.\n\nCorundum occurs as a mineral in mica schist, gneiss, and some marbles in metamorphic terranes. It also occurs in low silica igneous syenite and nepheline syenite intrusives. Other occurrences are as masses adjacent to ultramafic intrusives, associated with lamprophyre dikes and as large crystals in pegmatites. It commonly occurs as a detrital mineral in stream and beach sands because of its hardness and resistance to weathering. The largest documented single crystal of corundum measured about , and weighed . The record has since been surpassed by certain synthetic boules.\n\nCorundum for abrasives is mined in Zimbabwe, Russia, Sri Lanka, and India. Historically it was mined from deposits associated with dunites in North Carolina, US and from a nepheline syenite in Craigmont, Ontario. Emery-grade corundum is found on the Greek island of Naxos and near Peekskill, New York, US. Abrasive corundum is synthetically manufactured from bauxite. Four corundum axes dating back to 2500 BCE from the Liangzhou culture have been discovered in China.\n\nIn 1837, Marc Antoine Gaudin made the first synthetic rubies by fusing alumina at a high temperature with a small amount of chromium as a pigment. In 1847, Ebelmen made white synthetic sapphires by fusing alumina in boric acid. In 1877 Frenic and Freil made crystal corundum from which small stones could be cut. Frimy and Auguste Verneuil manufactured artificial ruby by fusing and with a little chromium at temperatures above . In 1903, Verneuil announced he could produce synthetic rubies on a commercial scale using this flame fusion process.\n\nThe Verneuil process allows the production of flawless single-crystal sapphires, rubies and other corundum gems of much larger size than normally found in nature. It is also possible to grow gem-quality synthetic corundum by flux-growth and hydrothermal synthesis. Because of the simplicity of the methods involved in corundum synthesis, large quantities of these crystals have become available on the market causing a significant reduction of price in recent years. Apart from ornamental uses, synthetic corundum is also used to produce mechanical parts (tubes, rods, bearings, and other machined parts), scratch-resistant optics, scratch-resistant watch crystals, instrument windows for satellites and spacecraft (because of its transparency in the ultraviolet to infrared range), and laser components.\n\nCorundum crystallizes with trigonal symmetry in the space group \"R\"\"c\" and has the lattice parameters a = 4.75 Å and c = 12.982 Å at standard conditions. The unit cell contains six formula units.\n\nThe toughness of corundum is sensitive to surface roughness and crystallographic orientation. It may be 6-7 MPa·m for synthetic crystals, and ~4 for natural\n\nIn the lattice of corundum, the oxygen atoms form a slightly distorted hexagonal close packing, in which two-thirds of the gaps between the octahedra are occupied by aluminum ions.\n"}
{"id": "5976", "url": "https://en.wikipedia.org/wiki?curid=5976", "title": "Capoeira", "text": "Capoeira\n\nCapoeira () is a Brazilian martial art that combines elements of dance, acrobatics and music. It was developed in Brazil mainly by Angolans, at the beginning of the 16th century. It is known for its quick and complex maneuvers, predominantly using power, speed, and leverage across a wide variety of kicks, spins and techniques.\n\nThe most widely accepted origin of the word \"capoeira\" comes from the Tupi words \"ka'a\" (\"jungle\") \"e pûer\" (\"it was\"), referring to the areas of low vegetation in the Brazilian interior where fugitive slaves would hide. A practitioner of the art is called a capoeirista ().\n\nOn 26th November 2014 capoeira was granted a special protected status as \"intangible cultural heritage\" by UNESCO.\n\nCapoeira's history begins with the beginning of African slavery in Brazil. Since the 16th century, Portuguese colonists began exporting slaves to their colonies, coming mainly from Angola. Brazil, with its vast territory, received most of the slaves, almost 40% of all slaves sent through the Atlantic Ocean. The early history of capoeira is still controversial, especially the period between the 16th century and the beginning of the 19th century, since historical documents were very scarce in Brazil at that time. But oral tradition, language, and evidence leaves little doubt about its Afro-Brazilian roots.\n\nIn the 16th century, Portugal had claimed one of the largest territories of the colonial empires, but lacked people to colonize it, especially workers. In the Brazilian colony, the Portuguese, like many European colonists, chose to use slavery to build their economy.\n\nIn its first century, the main economic activity in the colony was the production and processing of sugar cane. Portuguese colonists created large sugarcane farms called \"engenhos\", which depended on the labor of slaves. Slaves, living in inhumane conditions, were forced to work hard and often suffered physical punishment for small misbehaviors.\n\nAlthough slaves often outnumbered colonists, rebellions were rare because the lack of weapons, harsh colonial law, disagreement between slaves coming from different African cultures and lack of knowledge about the new land and its surroundings, all of which usually discouraged the idea of a rebellion.\n\nIn this environment, capoeira was born as a simple method of survival. It was a tool with which an escaped slave, completely unequipped, could survive in the hostile, unknown land and face the hunt of the \"capitães-do-mato\", the armed and mounted colonial agents who were charged with finding and capturing escapees.\n\nSoon several groups of escaping slaves would gather and establish quilombos, primitive settlements in far and hard to reach places. Some quilombos would soon increase in size, attracting more fugitive slaves, Brazilian natives and even Europeans escaping the law or Christian extremism. Some quilombos would grow to an enormous size, becoming a real independent multi-ethnic state.\n\nEveryday life in a quilombo offered freedom and the opportunity to revive traditional cultures away from colonial oppression. In this kind of multi-ethnic community, constantly threatened by Portuguese colonial troops, capoeira evolved from a survival tool to a martial art focused on war.\n\nThe biggest quilombo, the Quilombo dos Palmares, consisted of many villages which lasted more than a century, resisting at least 24 small attacks and 18 colonial invasions. Portuguese soldiers sometimes said that it took more than one dragoon to capture a quilombo warrior, since they would defend themselves with a \"strangely moving fighting technique\". The provincial governor declared \"it is harder to defeat a quilombo than the Dutch invaders.\"\n\nIn 1808, the prince and future king Dom João VI, along with the Portuguese court, escaped to Brazil from the invasion of Portugal by Napoleon's troops. Formerly exploited only for its natural resources and commodity crops, the colony finally began to develop as a nation. The Portuguese monopoly effectively came to an end when Brazilian ports opened for trade with friendly foreign nations. Those cities grew in importance and Brazilians got permission to manufacture common products once required to be imported from Portugal, such as glass.\n\nRegistries of capoeira practices existed since the 18th century in Rio de Janeiro, Salvador and Recife. Due to city growth, more slaves were brought to cities and the increase in social life in the cities made capoeira more prominent and allowed it to be taught and practiced among more people. Because capoeira was often used against the colonial guard, in Rio the colonial government tried to suppress it and established severe physical punishments to its practice.\n\nAmple data from police records from the 1800s shows that many slaves and free coloured people were detained for practicing capoeira:\n\"From 288 slaves that entered the Calabouço jail during the years 1857 and 1858, 80 (31%) were arrested for capoeira, and only 28 (10.7%) for running away. Out of 4,303 arrests in Rio police jail in 1862, 404 detainees—nearly 10%—had been arrested for capoeira.\"\n\nBy the end of the 19th century, slavery was on the verge of departing the Brazilian Empire. Reasons included growing quilombo militia raids in plantations that still used slaves, the refusal of the Brazilian army to deal with escapees and the growth of Brazilian abolitionist movements. The Empire tried to soften the problems with laws to restrict slavery, but finally Brazil would recognize the end of the institution on May 13, 1888, with a law called \"Lei Áurea\" (Golden Law), sanctioned by imperial parliament and signed by Princess Isabel.\n\nHowever, free former slaves now felt abandoned. Most of them had nowhere to live, no jobs and were despised by Brazilian society, which usually viewed them as lazy workers. Also, new immigration from Europe and Asia left most former slaves with no employment.\n\nSoon capoeiristas started to use their skills in unconventional ways. Criminals and war lords used capoeiristas as body guards and hitmen. Groups of capoeiristas, known as \"maltas\", raided Rio de Janeiro. In 1890, the recently proclaimed Brazilian Republic decreed the prohibition of capoeira in the whole country. Social conditions were chaotic in the Brazilian capital, and police reports identified capoeira as an advantage in fighting.\n\nAfter the prohibition, any citizen caught practicing capoeira, in a fight or for any other reason, would be arrested, tortured and often mutilated by the police. Cultural practices, such as the \"roda de capoeira\", were conducted in remote places with sentries to warn of approaching police.\n\nBy the 1920s, capoeira repression had declined. Mestre Bimba from Salvador, a strong fighter in both legal and illegal fights, met with his future student, Cisnando Lima. Both thought capoeira was losing its martial roots due to the use of its playful side to entertain tourists. Bimba began developing the first systematic training method for capoeira, and in 1932 founded the first capoeira school. Advised by Cisnando, Bimba called his style \"Luta Regional Baiana\" (\"regional fight from Bahia\"), because capoeira was still illegal in name.\n\nIn 1937, Bimba founded the school \"Centro de Cultura Física e Luta Regional\", with permission from Salvador's Secretary of Education (\"Secretaria da Educação, Saúde e Assistência de Salvador\"). His work was very well received, and he taught capoeira to the cultural elite of the city. By 1940, capoeira finally lost its criminal connotation and was legalized.\n\nBimba's Regional style overshadowed traditional capoeiristas, who were still distrusted by society. This began to change in 1941 with the founding of \"Centro Esportivo de Capoeira Angola\" (CECA) by Vicente Ferreira Pastinha. Located in the Salvador neighborhood of Pelourinho, this school attracted many traditional capoeiristas. With CECA's prominence, the traditional style came to be called \"Capoeira Angola\". The name derived from \"brincar de angola\" (\"playing Angola\"), a term used in the 19th century in some places. But it was also adopted by other masters, including some who did not follow Pastinha's style.\n\nCapoeira nowadays is not only a martial art, but an active exporter of Brazilian culture all over the world. Since the 1970s, capoeira mastres began to emigrate and teach it in other countries. Present in many countries on every continent, every year capoeira attracts thousands of foreign students and tourists to Brazil. Foreign capoeiristas work hard to learn Portuguese to better understand and become part of the art. Renowned capoeira mestres often teach abroad and establish their own schools. Capoeira presentations, normally theatrical, acrobatic and with little martiality, are common sights around the world.\n\nThe martial art aspect is still present and still disguised, leading many non-practitioners to ignore its presence. Trickery is ever present and expert capoeiristas can even disguise an attack as a friendly gesture.\n\nSymbol of the Brazilian culture, symbol of the ethnic amalgam that characterizes Brazil, symbol of resistance to the oppression, capoeira definitely changed its image and became a source of pride to Brazilian people. Capoeira is officially considered intangible cultural heritage of Brazil.\n\nCapoeira is a fast and versatile martial art which is historically focused on fighting outnumbered or in technological disadvantage. The style emphasizes using the lower body to kick, sweep and take down and the upper body to assist those movements and occasionally attack as well. It features a series of complex positions and body postures which are meant to get chained in an uninterrupted flow, in order to strike, dodge and move without breaking motion, conferring the style with a characteristic unpredictability and versatility.\nThe \"ginga\" (literally: rocking back and forth; to swing) is the fundamental movement in capoeira, important both for attack and defense purposes. It has two main objectives. One is to keep the capoeirista in a state of constant motion, preventing him or her from being a still and easy target. The other, using also fakes and feints, is to mislead, fool, trick the opponent, leaving them open for an attack or a counter-attack.\n\nThe attacks in the capoeira should be done when opportunity arises, and though they can be preceded by feints or pokes, they must be precise and decisive, like a direct kick to the head, face or a vital body part, or a strong takedown. Most capoeira attacks are made with the legs, like direct or swirling kicks, rasteiras (leg sweeps), tesouras or knee strikes. Elbow strikes, punches and other forms of takedowns complete the main list. The head strike is a very important counter-attack move.\n\nThe defense is based on the principle of non-resistance, meaning avoiding an attack using evasive moves instead of blocking it. Avoids are called \"esquivas\", which depend on the direction of the attack and intention of the defender, and can be done standing or with a hand leaning on the floor. A block should only be made when the \"esquiva\" is completely non-viable. This fighting strategy allows quick and unpredictable counterattacks, the ability to focus on more than one adversary and to face empty-handed an armed adversary.\nA series of rolls and acrobatics (like the cartwheels called aú or the transitional position called negativa) allows the capoeirista to quickly overcome a takedown or a loss of balance, and to position themselves around the aggressor in order to lay up for an attack. It is this combination of attacks, defense and mobility which gives capoeira its perceived 'fluidity' and choreography-like style.\n\nThrough most of its history in Brazil, capoeira commonly featured weapons and weapon training, given its street fighting nature. Capoeiristas usually carried knives and bladed weapons with them, and the berimbau could be used to conceal those inside, or even to turn itself into a weapon by attaching a blade to its tip. The knife or razor was used in street \"rodas\" and/or against openly hostile opponents, and would be drawn quickly to stab or slash. Other hiding places for the weapons included hats and umbrellas.\n\nMestre Bimba included in his teachings a \"curso de especialização\" or specialization course in which the pupils would be taught defenses against knives and guns, as well as the usage of knife, straight razor, scythe, club, \"chanfolo\" (double-edged dagger), \"facão\" (machete) and \"tira-teima\" (cane sword). Upon graduating, pupils were given a red scarf which marked their specialty. This course was scarcely used, and was ceased after some time. A more common custom practised by Bimba and his students, however, was furtively handing a weapon to a player before a \"jogo\" in order for them to use it to attack their opponent on Bimba's sign, with the other player's duty being to disarm them.\n\nThis weapon training is almost completely absent in current capoeira teachings, but some groups still practice the use of razors for ceremonial usage in the \"rodas\".\n\nPlaying capoeira is both a game and a method of practicing the application of capoeira movements in simulated combat. It can be played anywhere, but it's usually done in a \"roda\". During the game most capoeira moves are used, but capoeiristas usually avoid using punches or elbow strikes unless it's a very aggressive game.\n\nThe game usually does not focus on knocking down or destroying the opponent, rather it emphasizes skill. Capoeiristas often prefer to rely on a takedown like a \"rasteira\", then allowing the opponent to recover and get back into the game. It is also very common to slow down a kick inches before hitting the target, so a capoeirista can enforce superiority without the need of injuring the opponent. If an opponent clearly cannot dodge an attack, there is no reason to complete it. However, between two high-skilled capoeiristas, the game can get much more aggressive and dangerous. Capoeiristas tend to avoid showing this kind of game in presentations or to the general public.\n\nThe \"roda\" (pronounced ) is a circle formed by capoeiristas and capoeira musical instruments, where every participant sings the typical songs and claps their hands following the music. Two \"capoeiristas\" enter the \"roda\" and play the game according to the style required by the musical rhythm. The game finishes when one of the musicians holding a berimbau determine it, when one of the \"capoeiristas\" decide to leave or call the end of the game or when another capoeirista interrupts the game to start playing, either with one of the current players or with another \"capoeirista\".\n\nIn a \"roda\" every cultural aspect of capoeira is present, not only the martial side. Aerial acrobatics are common in a presentation \"roda\", while not seen as often in a more serious one. Takedowns, on the other hand, are common in a serious \"roda\" but rarely seen in presentations.\n\nThe batizado (lit. baptism) is a ceremonial \"roda\" where new students will get recognized as capoeiristas and earn their first graduation. Also more experienced students may go up in rank, depending on their skills and capoeira culture. In Mestre Bimba's Capoeira Regional, batizado was the first time a new student would play capoeira following the sound of the berimbau.\n\nStudents enter the \"roda\" against a high-ranked capoeirista (such as a teacher or master) and normally the game ends with the student being taken down. In some cases the more experienced capoeirista can judge the takedown unnecessary. Following the batizado the new graduation, generally in the form of a cord, is given.\n\nTraditionally, the batizado is the moment when the new practitioner gets or formalizes his or her \"apelido\" (nickname). This tradition was created back when capoeira practice was considered a crime. To avoid having problems with the law, capoeiristas would present themselves in the capoeira community only by their nicknames. So if a capoeirista was captured by the police, he would be unable to identify his fellow capoeiristas, even when tortured.\n\n\"Apelidos\" can come from many different things, such as a physical characteristic (like being tall or big), a habit (like smiling or drinking too much), place of birth, a particular skill, an animal, or trivial things.\n\nEven though apelidos or these nicknames are not necessary any more, the tradition is still very alive not only in capoeira but in many aspects of Brazilian culture.\n\n\"Chamada\" means 'call' and can happen at any time during a \"roda\" where the rhythm \"angola\" is being played. It happens when one player, usually the more advanced one, calls his or her opponent to a dance-like ritual. The opponent then approaches the caller and meets him or her to walk side by side. After it both resume normal play.\n\nWhile it may seem like a break time or a dance, the \"chamada\" is actually both a trap and a test, as the caller is just watching to see if the opponent will let his guard down so she can perform a takedown or a strike. It is a critical situation, because both players are vulnerable due to the close proximity and potential for a surprise attack. It's also a tool for experienced practitioners and masters of the art to test a student's awareness and demonstrate when the student left herself open to attack.\n\nThe use of the \"chamada\" can result in a highly developed sense of awareness and helps practitioners learn the subtleties of anticipating another person's hidden intentions. The \"chamada\" can be very simple, consisting solely of the basic elements, or the ritual can be quite elaborate including a competitive dialogue of trickery, or even theatric embellishments.\n\nVolta ao mundo means \"around the world\".\n\nThe \"volta ao mundo\" takes place after an exchange of movements has reached a conclusion, or after there has been a disruption in the harmony of the game. In either of these situations, one player will begin walking around the perimeter of the circle counter-clockwise, and the other player will join the \"volta ao mundo\" in the opposite part of the roda, before returning to the normal game.\n\n\"Malandragem\" is a word that comes from \"malandro\", which means a person who possesses cunning as well as \"malícia\" (malice). This, however, is misleading as the meaning of \"malicia\" in capoeira is the capacity to understand someone's intentions. In Brazil, men who used street smarts to make a living were called \"malandros\". Later the meaning expanded, indicating a person who is a quick thinker in finding a solution for a problem.\n\nIn capoeira, \"malandragem\" is the ability to quickly understand an opponent's aggressive intentions, and during a fight or a game, fool, trick and deceive him.\n\nSimilarly capoeiristas use the concept of \"mandinga\". Mandinga can be translated \"magic\" or \"spell\", but in capoeira a \"mandingueiro\" is a clever fighter, able to trick the opponent. Mandinga is a tricky and strategic quality of the game, and even a certain esthetic, where the game is expressive and at times theatrical, particularly in the Angola style. The roots of the term \"mandingueiro\" would be a person who had the magic ability to avoid harm due to protection from the Orixás.\n\nAlternately Mandinga is a way of saying Mandinka (as in the Mandinka Nation) who are known as \"musical hunters\". Which directly ties into the term \"vadiação\". Vadiação is the musical wanderer (with flute in hand), traveler, vagabond.\n\nMusic is integral to capoeira. It sets the tempo and style of game that is to be played within the \"roda\". Typically the music is formed by instruments and singing. Rhythm, controlled by a typical instrument called berimbau, differ from very slow to very fast, depending on the style of the \"roda\".\n\nCapoeira instruments are disposed in a row called bateria. It is traditionally formed by three berimbaus, two pandeiros, three atabaques, one agogô and one ganzá, but this format may vary depending on the capoeira group's traditions or the \"roda\" style.\n\nThe berimbau is the leading instrument, determining the tempo and style of the music and game played. Two low pitch berimbaus (called \"berra-boi\" and \"médio\") form the base and a high pitch berimbau (called \"viola\") makes variations and improvisations. The other instruments must follow the berimbau's rhythm, free to vary and improvise a little, depending upon the capoeira group's musical style.\n\nAs the capoeiristas change their playing style significantly following the toque of the berimbau, which sets the game's speed, style and aggressiveness, it is truly the music that drives a capoeira game.\n\nMany of the songs are sung in a call and response format while others are in the form of a narrative. Capoeiristas sing about a wide variety of subjects. Some songs are about history or stories of famous capoeiristas. Other songs attempt to inspire players to play better. Some songs are about what is going on within the roda. Sometimes the songs are about life or love lost. Others have lighthearted and playful lyrics.\n\nThere are four basic kinds of songs in capoeira, the \"Ladaínha\", \"Chula\", \"Corrido\" and \"Quadra\". The \"Ladaínha\" is a narrative solo sung only at the beginning of a roda, often by a \"mestre\" (master) or most respected capoeirista present. The solo is followed by a \"louvação\", a call and response pattern that usually thanks God and one's master, among other things. Each call is usually repeated word-for-word by the responders. The Chula is a song where the singer part is much bigger than the chorus response, usually eight singer verses for one chorus response, but the proportion may vary. The Corrido is a song where the singer part and the chorus response are equal, normally two verses by two responses. Finally, the \"Quadra\" is a song where the same verse is repeated four times, either three singer verses followed by one chorus response, or one verse and one response.\n\nCapoeira songs can talk about virtually anything, being it about a historical fact, a famous capoeirista, trivial life facts, hidden messages for players, anything. Improvisation is very important also, while singing a song the main singer can change the music's lyrics, telling something that's happening in or outside the roda.\n\nDetermining styles in capoeira is difficult, since there was never a unity in the original capoeira, or a teaching method before the decade of 1920. However, a division between two styles and a sub-style is widely accepted.\n\n refers to every capoeira that keeps the traditions held before the creation of the Regional style.\n\nExisting in many parts of Brazil since colonial times, most notably in the cities of Rio de Janeiro, Salvador and Recife, it's impossible to tell where and when Capoeira Angola began taking its present form. The name \"Angola\" starts as early as the beginning of slavery in Brazil, when Africans, taken to Luanda to be shipped to the Americas, were called in Brazil \"black people from Angola\", regardless of their nationality. In some places of Brazil people would refer to capoeira as \"playing Angola\" and, according to Mestre Noronha, the capoeira school \"Centro de Capoeira Angola Conceição da Praia\", created in Bahia, already used the name \"Capoeira Angola\" illegally in the beginning of the 1920 decade.\n\nThe name \"Angola\" was finally immortalized by Mestre Pastinha at February 23, 1941, when he opened the \"Centro Esportivo de capoeira Angola\" (CECA). Pastinha preferred the ludic aspects of the game rather than the martial side, and was much respected by recognized capoeira masters. Soon many other masters would adopt the name \"Angola\", even those who would not follow Pastinha's style.\n\nThe ideal of \"Capoeira Angola\" is to maintain capoeira as close to its roots as possible. Characterized by being strategic, with sneaking movements executed standing or near the floor depending on the situation to face, it values the traditions of \"malícia\", \"malandragem\" and unpredictability of the original capoeira.\n\nTypical music \"bateria\" formation in a \"roda\" of \"Capoeira Angola\" is three \"berimbaus\", two \"pandeiros\", one \"atabaque\", one \"agogô\" and one \"ganzuá\".\n\nCapoeira Regional began to take form in the 1920 decade, when Mestre Bimba met his future student, José Cisnando Lima. Both believed that capoeira was losing its martial side and concluded there was a need to restructure it. Bimba created his \"sequências de ensino\" (teaching combinations) and created capoeira's first teaching method. Advised by Cisnando, Bimba decided to call his style \"Luta Regional Baiana\", as capoeira was still illegal at that time.\n\nThe base of Capoeira Regional is the original capoeira without many of the aspects that were impractical in a real fight, with less subterfuge and more objectivity. Training was mainly focused on attack, dodging and counter-attack, giving high importance to precision and discipline. Bimba also added a few moves from other arts, notably the \"batuque\", an old street fight game practiced by his father. Use of jumps or aerial acrobacies was kept to a minimum, since one of its foundations was always keeping at least one hand or foot firmly attached to the ground. Mestre Bimba often said, \"\"o chão é amigo do capoeirista\"\" (the floor is a friend to the capoeirista).\n\n\"Capoeira Regional\" also introduced the first ranking method in capoeira. \"Regional\" had three levels: \"calouro\" (freshman), \"formado\" (graduated) and \"formado especializado\" (specialist). When a student completed a course, a special celebration ceremony was had resulting with a silk scarf being tied around the capoeirista's neck.\n\nThe traditions of \"roda\" and capoeira game were kept, being used to put into use what was learned during training. The disposition of musical instruments, however, was changed, being made by a single berimbau and two pandeiros.\n\nThe \"Luta Regional Baiana\" soon became popular, finally changing capoeira's bad image. Mestre Bimba made many presentations of his new style, but the best known was the one made at 1953 to Brazilian president Getúlio Vargas, where the president would say: \"\"A Capoeira é o único esporte verdadeiramente nacional\"\" (Capoeira is the only truly national sport).\n\nIn the 1970s a mixed style began to take form, with practitioners taking the aspects they considered more important from both Regional and Angola. Notably more acrobatic, this sub-style is seen by some as the natural evolution of capoeira, by others as adulteration or even misinterpretation of capoeira.\n\nNowadays the label Contemporânea applies to any capoeira group who don't follow Regional or Angola styles, even the ones who mix capoeira with other martial arts. Some notable groups whose style cannot be described as either Angola or Regional but rather \"a style of their own\", include Senzala de Santos, Cordao de Ouro and Abada. In the case of Cordao de Ouro, the style may be described as \"Miudinho\", a low and fast paced game, while in Senzala de Santos the style may described simply as \"Senzala de Santos\", an elegant, playful combination of Angola and Regional. Capoeira Abada may be described as a more aggressive, less \"dance\" influenced style of Capoeira.\n\nBecause of its origin, capoeira never had unity or a general agreement. Ranking or graduating system follows the same path, as there never existed a ranking system accepted by most of the masters. That means graduation style varies depending on the group's traditions.\n\nThe most common modern system uses colored ropes, called \"corda\" or \"cordão\", tied around the waist. Some masters use different systems, or even no system at all.\n\nThere are many entities (leagues, federations and association) which have tried to unify the graduation system. The most usual is the system of the \"Confederação Brasileira de Capoeira\" (Brazilian Capoeira Confederation), which adopts ropes using the colors of the Brazilian flag, green, yellow, blue and white.\n\nEven though it's widely used with many small variations, many big and influential groups still use different systems, in example, Porto da Barra Group that uses belts that tell the Brazilian slavery history. Even the \"Confederação Brasileira de Capoeira\" is not widely accepted as the capoeira's main representative.\n\nIn many groups (mainly of Angola school) there is no direct ranking system. There are students, treinels, professors, contra-mestres and mestre but no cordas (belts). In some groups, a capoeira is simply as it, capoeira, and therefore there is no need of cordas as \"belts does not play\". One of the ideas to not have a belt is that there are as many belt systems as there are groups using them. Therefore, groups can have different meanings of level for same colour belts. And then again, in origins of capoeira, there were no belts.\n\nEven though those activities are strongly associated with capoeira, they have different meanings and origins.\n\nPerformed by many capoeira groups, samba de roda is a traditional Afro-Brazilian dance and musical form that has been associated with capoeira for many decades. The orchestra is composed by \"pandeiro\", \"atabaque\", \"berimbau-viola\" (high pitch berimbau), chocalho, accompanied by singing and clapping. \"Samba de roda\" is considered one of the primitive forms of modern Samba.\n\nOriginally the \"Maculelê\" is believed to have been an indigenous armed fighting style, using two sticks or a machete. Nowadays it's a folkloric dance practiced with heavy afro-Brazilian percussion. Many capoeira groups include \"Maculelê\" in their presentations.\n\n\"Puxada de Rede\" is a Brazilian folkloric theatrical play, seen in many capoeira performances. It is based on a traditional Brazilian legend involving the loss of a fisherman in a seafaring accident.\n\nCapoeira is cited as an influence on other martial arts and several forms of dance, including a debated status as a forerunner of breaking. Many techniques from capoeira are also present in Tricking. In the UK, capoeira has been cited as a key influence in the development of the hybrid martial art Sanjuro.\n\nCapoeira is currently being used as a tool in sports development (the use of sport to create positive social change) to promote psychosocial wellbeing in various youth projects around the world. Capoeira4Refugees is a UK-based NGO working with youth in conflict zones in the Middle East. Capoeira for Peace is a project based in the Democratic Republic of Congo. The Nukanti Foundation works with street children in Colombia.\n\nA lot of Brazilian fighters have a capoeira background, either training often or having tried it before. Some of them are: Anderson Silva, who is a yellow belt, trained in Capoeira at a young age, then again when he was a UFC fighter;\nThiago Santos, an active UFC middleweight contender who trained in Capoeira for 8 years;\nUFC fighter Conor McGregor, who has implemented aspects of Capoeira into his fighting style; Marcus \"Lelo\" Aurelio, who is famous for knocking a fighter out with a Meia Lua De Compasso kick, and UFC veteran Andre Gusmão also uses Capoeira as his base.\n\n\nNotes\nBibliography\n\nFurther reading\n\n"}
{"id": "5980", "url": "https://en.wikipedia.org/wiki?curid=5980", "title": "Carbon sink", "text": "Carbon sink\n\nA carbon sink is a natural or artificial reservoir that accumulates and stores some carbon-containing chemical compound for an indefinite period. The process by which carbon sinks remove carbon dioxide () from the atmosphere is known as carbon sequestration. Public awareness of the significance of CO sinks has grown since passage of the Kyoto Protocol, which promotes their use as a form of carbon offset. There are also different strategies used to enhance this process.\n\nThe natural sinks are:\n\n\nNatural sinks are typically much bigger than artificial sinks. The main artificial sinks are:\n\n\nCarbon sources include:\n\n\nBecause growing vegetation takes in carbon dioxide, the Kyoto Protocol allows Annex I countries with large areas of growing forests to issue Removal Units to recognize the sequestration of carbon. The additional units make it easier for them to achieve their target emission levels. It is estimated that forests absorb between 10 and 20 tons of carbon dioxide per hectare each year, through photosynthetic conversion into starch, cellulose, lignin, and wooden biomass. While this has been well documented for temperate forests and plantations, the fauna of the tropical forests place some limitations for such global estimates. \n\nSome countries seek to trade emission rights in carbon emission markets, purchasing the unused carbon emission allowances of other countries. If overall limits on greenhouse gas emission are put into place, cap and trade market mechanisms are purported to find cost-effective ways to reduce emissions. There is as yet no carbon audit regime for all such markets globally, and none is specified in the Kyoto Protocol. National carbon emissions are self-declared.\n\nIn the Clean Development Mechanism, only afforestation and reforestation are eligible to produce certified emission reductions (CERs) in the first commitment period of the Kyoto Protocol (2008–2012). Forest conservation activities or activities avoiding deforestation, which would result in emission reduction through the conservation of existing carbon stocks, are not eligible at this time. Also, agricultural carbon sequestration is not possible yet.\n\nSoils represent a short to long-term carbon storage medium, and contains more carbon than all terrestrial vegetation and the atmosphere combined. Plant litter and other biomass including charcoal accumulates as organic matter in soils, and is degraded by chemical weathering and biological degradation. More recalcitrant organic carbon polymers such as cellulose, hemi-cellulose, lignin, aliphatic compounds, waxes and terpenoids are collectively retained as humus. Organic matter tends to accumulate in litter and soils of colder regions such as the boreal forests of North America and the Taiga of Russia. Leaf litter and humus are rapidly oxidized and poorly retained in sub-tropical and tropical climate conditions due to high temperatures and extensive leaching by rainfall. Areas where shifting cultivation or slash and burn agriculture are practiced are generally only fertile for 2–3 years before they are abandoned. These tropical jungles are similar to coral reefs in that they are highly efficient at conserving and circulating necessary nutrients, which explains their lushness in a nutrient desert. Much organic carbon retained in many agricultural areas worldwide has been severely depleted due to intensive farming practices.\n\nGrasslands contribute to soil organic matter, stored mainly in their extensive fibrous root mats. Due in part to the climatic conditions of these regions (e.g. cooler temperatures and semi-arid to arid conditions), these soils can accumulate significant quantities of organic matter. This can vary based on rainfall, the length of the winter season, and the frequency of naturally occurring lightning-induced grass-fires. While these fires release carbon dioxide, they improve the quality of the grasslands overall, in turn increasing the amount of carbon retained in the humic material. They also deposit carbon directly to the soil in the form of char that does not significantly degrade back to carbon dioxide.\n\nForest fires release absorbed carbon back into the atmosphere, as does deforestation due to rapidly increased oxidation of soil organic matter.\n\nOrganic matter in peat bogs undergoes slow anaerobic decomposition below the surface. This process is slow enough that in many cases the bog grows rapidly and fixes more carbon from the atmosphere than is released. Over time, the peat grows deeper. Peat bogs hold approximately one-quarter of the carbon stored in land plants and soils.\n\nUnder some conditions, forests and peat bogs may become sources of CO, such as when a forest is flooded by the construction of a hydroelectric dam. Unless the forests and peat are harvested before flooding, the rotting vegetation is a source of CO and methane comparable in magnitude to the amount of carbon released by a fossil-fuel powered plant of equivalent power.\n\nCurrent agricultural practices lead to carbon loss from soils. It has been suggested that improved farming practices could return the soils to being a carbon sink. Present worldwide practises of overgrazing are substantially reducing many grasslands' performance as carbon sinks. The Rodale Institute says that regenerative agriculture, if practiced on the planet’s 3.6 billion tillable acres, could sequester up to 40% of current CO emissions. They claim that agricultural carbon sequestration has the potential to mitigate global warming. When using biologically based regenerative practices, this dramatic benefit can be accomplished with no decrease in yields or farmer profits. Organically managed soils can convert carbon dioxide from a greenhouse gas into a food-producing asset.\n\nIn 2006, U.S. carbon dioxide emissions, largely from fossil fuel combustion, were estimated at nearly 6.5 billion tons. If a 2,000 (lb/ac)/year sequestration rate was achieved on all of cropland in the United States, nearly 1.6 billion tons of carbon dioxide would be sequestered per year, mitigating close to one quarter of the country's total fossil fuel emissions.\n\nOceans are at present CO sinks, and represent the largest active carbon sink on Earth, absorbing more than a quarter of the carbon dioxide that humans put into the air. The solubility pump is the primary mechanism responsible for the CO2 absorption by the oceans.\n\nThe biological pump plays a negligible role, because of the limitation to pump by ambient light and nutrients required by the phytoplankton that ultimately drive it. Total inorganic carbon is not believed to limit primary production in the oceans, so its increasing availability in the ocean does not directly affect production (the situation on land is different, since enhanced atmospheric levels of CO essentially \"fertilize\" land plant growth to some threshold). However, ocean acidification by invading anthropogenic CO may affect the biological pump by negatively impacting calcifying organisms such as coccolithophores, foraminiferans and pteropods. Climate change may also affect the biological pump in the future by warming and stratifying the surface ocean, thus reducing the supply of limiting nutrients to surface waters.\n\nA study from 2008 claims that CO could increase primary productivity, particularly in eel grasses in coastal and estuarine habitats. This postulate, however, has yet to be proven.\n\nIn January 2009, the Monterey Bay Aquarium Research Institute and the National Oceanic and Atmospheric Administration announced a joint study to determine whether the ocean off the California coast was serving as a carbon source or a carbon sink. Principal instrumentation for the study will be self-contained CO monitors placed on buoys in the ocean. They will measure the partial pressure of CO in the ocean and the atmosphere just above the water surface.\n\nIn February 2009, Science Daily reported that the Southern Indian Ocean is becoming less effective at absorbing carbon dioxide due to changes to the region's climate which include higher wind speeds.\n\nOn longer timescales Oceans may be both sources and sinks – during ice ages levels decrease to ≈180 ppmv, and much of this is believed to be stored in the oceans. As ice ages end, is released from the oceans and levels during previous interglacials have been around ≈280 ppmv. This role as a sink for CO is driven by two processes, the solubility pump and the biological pump. The former is primarily a function of differential CO solubility in seawater and the thermohaline circulation, while the latter is the sum of a series of biological processes that transport carbon (in organic and inorganic forms) from the surface euphotic zone to the ocean's interior. A small fraction of the organic carbon transported by the biological pump to the seafloor is buried in anoxic conditions under sediments and ultimately forms fossil fuels such as oil and natural gas.\n\nAt the end of glacials with sea level rapidly rising, corals tend to grow slower due to increased ocean temperature as seen on the Showtime series \"Years of Living Dangerously\". The calcium carbonate from which coral skeletons are made is just over 60% carbon dioxide. If we postulate that coral reefs were eroded down to the glacial sea level, then coral reefs have grown 120m upward since the end of the recent glacial.\n\nForests are carbon stores, and they are carbon dioxide sinks when they are increasing in density or area. In Canada's boreal forests as much as 80% of the total carbon is stored in the soils as dead organic matter. A 40-year study of African, Asian, and South American tropical forests by the University of Leeds, shows tropical forests absorb about 18% of all carbon dioxide added by fossil fuels. Truly mature tropical forests, by definition, grow rapidly as each tree produces at least 10 new trees each year. Based on studies of the FAO and UNEP it has been estimated that Asian forests absorb about 5 tonnes of carbon dioxide per hectare each year. The global cooling effect of carbon sequestration by forests is partially counterbalanced in that reforestation can decrease the reflection of sunlight (albedo). Mid-to-high latitude forests have a much lower albedo during snow seasons than flat ground, thus contributing to warming. Modeling that compares the effects of albedo differences between forests and grasslands suggests that expanding the land area of forests in temperate zones offers only a temporary cooling benefit.\n\nIn the United States in 2004 (the most recent year for which EPA statistics are available), forests sequestered 10.6% (637 MegaTonnes) of the carbon dioxide released in the United States by the combustion of fossil fuels (coal, oil and natural gas; 5657 MegaTonnes). Urban trees sequestered another 1.5% (88 MegaTonnes). To further reduce U.S. carbon dioxide emissions by 7%, as stipulated by the Kyoto Protocol, would require the planting of \"an area the size of Texas [8% of the area of Brazil] every 30 years\". Carbon offset programs are planting millions of fast-growing trees per year to reforest tropical lands, for as little as $0.10 per tree; over their typical 40-year lifetime, one million of these trees will fix 1 to 2 MegaTonnes of carbon dioxide. In Canada, reducing timber harvesting would have very little impact on carbon dioxide emissions because of the combination of harvest and stored carbon in manufactured wood products along with the regrowth of the harvested forests. Additionally, the amount of carbon released from harvesting is small compared to the amount of carbon lost each year to forest fires and other natural disturbances.\n\nThe Intergovernmental Panel on Climate Change concluded that \"a sustainable forest management strategy aimed at maintaining or increasing forest carbon stocks, while producing an annual sustained yield of timber fibre or energy from the forest, will generate the largest sustained mitigation benefit\". Sustainable management practices keep forests growing at a higher rate over a potentially longer period of time, thus providing net sequestration benefits in addition to those of unmanaged forests.\n\nLife expectancy of forests varies throughout the world, influenced by tree species, site conditions and natural disturbance patterns. In some forests carbon may be stored for centuries, while in other forests carbon is released with frequent stand replacing fires. Forests that are harvested prior to stand replacing events allow for the retention of carbon in manufactured forest products such as lumber. However, only a portion of the carbon removed from logged forests ends up as durable goods and buildings. The remainder ends up as sawmill by-products such as pulp, paper and pallets, which often end with incineration (resulting in carbon release into the atmosphere) at the end of their lifecycle. For instance, of the 1,692 MegaTonnes of carbon harvested from forests in Oregon and Washington (U.S) from 1900 to 1992, only 23% is in long-term storage in forest products.\n\nOne way to increase the carbon sequestration efficiency of the oceans is to add micrometre-sized iron particles in the form of either hematite (iron oxide) or melanterite (iron sulfate) to certain regions of the ocean. This has the effect of stimulating growth of plankton. Iron is an important nutrient for phytoplankton, usually made available via upwelling along the continental shelves, inflows from rivers and streams, as well as deposition of dust suspended in the atmosphere. Natural sources of ocean iron have been declining in recent decades, contributing to an overall decline in ocean productivity (NASA, 2003). Yet in the presence of iron nutrients plankton populations quickly grow, or 'bloom', expanding the base of biomass productivity throughout the region and removing significant quantities of CO from the atmosphere via photosynthesis. A test in 2002 in the Southern Ocean around Antarctica suggests that between 10,000 and 100,000 carbon atoms are sunk for each iron atom added to the water. More recent work in Germany (2005) suggests that any biomass carbon in the oceans, whether exported to depth or recycled in the euphotic zone, represents long-term storage of carbon. This means that application of iron nutrients in select parts of the oceans, at appropriate scales, could have the combined effect of restoring ocean productivity while at the same time mitigating the effects of human caused emissions of carbon dioxide to the atmosphere.\n\nBecause the effect of periodic small scale phytoplankton blooms on ocean ecosystems is unclear, more studies would be helpful. Phytoplankton have a complex effect on cloud formation via the release of substances such as dimethyl sulfide (DMS) that are converted to sulfate aerosols in the atmosphere, providing cloud condensation nuclei, or CCN. But the effect of small scale plankton blooms on overall DMS production is unknown.\n\nOther nutrients such as nitrates, phosphates, and silica as well as iron may cause ocean fertilization. There has been some speculation that using pulses of fertilization (around 20 days in length) may be more effective at getting carbon to ocean floor than sustained fertilization.\n\nThere is some controversy over seeding the oceans with iron however, due to the potential for increased toxic phytoplankton growth (e.g. \"red tide\"), declining water quality due to overgrowth, and increasing anoxia in areas harming other sea-life such as zooplankton, fish, coral, etc.\n\nSince the 1850s, a large proportion of the world's grasslands have been tilled and converted to croplands, allowing the rapid oxidation of large quantities of soil organic carbon. However, in the United States in 2004 (the most recent year for which EPA statistics are available), agricultural soils including pasture land sequestered 0.8% (46 teragrams) as much carbon as was released in the United States by the combustion of fossil fuels (5988 teragrams). The annual amount of this sequestration has been gradually increasing since 1998.\n\nMethods that significantly enhance carbon sequestration in soil include no-till farming, residue mulching, cover cropping, and crop rotation, all of which are more widely used in organic farming than in conventional farming. Because only 5% of US farmland currently uses no-till and residue mulching, there is a large potential for carbon sequestration. Conversion to pastureland, particularly with good management of grazing, can sequester even more carbon in the soil.\n\nTerra preta, an anthropogenic, high-carbon soil, is also being investigated as a sequestration mechanism.\nBy pyrolysing biomass, about half of its carbon can be reduced to charcoal, which can persist in the soil for centuries, and makes a useful soil amendment, especially in tropical soils (\"biochar\" or \"agrichar\").\n<ref name='abc.net.au/catalyst/s2012892'></ref>\n\nControlled burns on far north Australian savannas can result in an overall carbon sink. One working example is the West Arnhem Fire Management Agreement, started to bring \"strategic fire management across 28,000 km² of Western Arnhem Land\". Deliberately starting controlled burns early in the dry season results in a mosaic of burnt and unburnt country which reduces the area of burning compared with stronger, late dry season fires. In the early dry season there are higher moisture levels, cooler temperatures, and lighter wind than later in the dry season; fires tend to go out overnight. Early controlled burns also results in a smaller proportion of the grass and tree biomass being burnt.<ref name='savanna.ntu.edu.au/arnhem_fire_proj'> </ref> Emission reductions of 256,000 tonnes of CO have been made as of 2007.<ref name='savanna.ntu.edu.au/Eureka_arnhem_fire_proj'> </ref>\n\nFor carbon to be sequestered artificially (i.e. not using the natural processes of the carbon cycle) it must first be captured, \"or\" it must be significantly delayed or prevented from being re-released into the atmosphere (by combustion, decay, etc.) from an existing carbon-rich material, by being incorporated into an enduring usage (such as in construction). Thereafter it can be passively stored \"or\" remain productively utilized over time in a variety of ways.\n\nFor example, upon harvesting, wood (as a carbon-rich material) can be immediately burned or otherwise serve as a fuel, returning its carbon to the atmosphere, \"or\" it can be incorporated into construction or a range of other durable products, thus sequestering its carbon over years or even centuries.\n\nIndeed, a very carefully designed and durable, energy-efficient and energy-capturing building has the potential to sequester (in its carbon-rich construction materials), as much as or more carbon than was released by the acquisition and incorporation of all its materials and than will be released by building-function \"energy-imports\" during the structure's (potentially multi-century) existence. Such a structure might be termed \"carbon neutral\" or even \"carbon negative\". Building construction and operation (electricity usage, heating, etc.) are estimated to contribute nearly \"half\" of the annual human-caused carbon additions to the atmosphere.\n\nNatural-gas purification plants often already have to remove carbon dioxide, either to avoid dry ice clogging gas tankers or to prevent carbon-dioxide concentrations exceeding the 3% maximum permitted on the natural-gas distribution grid.\n\nBeyond this, one of the most likely early applications of carbon capture is the capture of carbon dioxide from flue gases at power stations (in the case of coal, this coal pollution mitigation is sometimes known as \"clean coal\"). A typical new 1000 MW coal-fired power station produces around 6 million tons of carbon dioxide annually. Adding carbon capture to existing plants can add significantly to the costs of energy production; scrubbing costs aside, a 1000 MW coal plant will require the storage of about of carbon dioxide a year. However, scrubbing is relatively affordable when added to new plants based on coal gasification technology, where it is estimated to raise energy costs for households in the United States using only coal-fired electricity sources from 10 cents per kW·h to 12 cents.\n\nCurrently, capture of carbon dioxide is performed on a large scale by absorption of carbon dioxide onto various amine-based solvents. Other techniques are currently being investigated, such as pressure swing adsorption, temperature swing adsorption, gas separation membranes, cryogenics and flue capture.\n\nIn coal-fired power stations, the main alternatives to retrofitting amine-based absorbers to existing power stations are two new technologies: coal gasification combined-cycle and oxy-fuel combustion. Gasification first produces a \"syngas\" primarily of hydrogen and carbon monoxide, which is burned, with carbon dioxide filtered from the flue gas. Oxy-fuel combustion burns the coal in oxygen instead of air, producing only carbon dioxide and water vapour, which are relatively easily separated. Some of the combustion products must be returned to the combustion chamber, either before or after separation, otherwise the temperatures would be too high for the turbine.\n\nAnother long-term option is carbon capture directly from the air using hydroxides. The air would literally be scrubbed of its CO content. This idea offers an alternative to non-carbon-based fuels for the transportation sector.\n\nExamples of carbon sequestration at coal plants include converting carbon from smokestacks into baking soda, and algae-based carbon capture, circumventing storage by converting algae into fuel or feed.\n\nAnother proposed form of carbon sequestration in the ocean is direct injection. In this method, carbon dioxide is pumped directly into the water at depth, and expected to form \"lakes\" of liquid CO at the bottom. Experiments carried out in moderate to deep waters (350–3600 m) indicate that the liquid CO reacts to form solid CO clathrate hydrates, which gradually dissolve in the surrounding waters.\n\nThis method, too, has potentially dangerous environmental consequences. The carbon dioxide does react with the water to form carbonic acid, HCO; however, most (as much as 99%) remains as dissolved molecular CO. The equilibrium would no doubt be quite different under the high pressure conditions in the deep ocean. In addition, if deep-sea bacterial methanogens that reduce carbon dioxide were to encounter the carbon dioxide sinks, levels of methane gas may increase, leading to the generation of an even worse greenhouse gas.\nThe resulting environmental effects on benthic life forms of the bathypelagic, abyssopelagic and hadopelagic zones are unknown. Even though life appears to be rather sparse in the deep ocean basins, energy and chemical effects in these deep basins could have far-reaching implications. Much more work is needed here to define the extent of the potential problems.\n\nCarbon storage in or under oceans may not be compatible with the Convention on the Prevention of Marine Pollution by Dumping of Wastes and Other Matter.\n\nAn additional method of long-term ocean-based sequestration is to gather crop residue such as corn stalks or excess hay into large weighted bales of biomass and deposit it in the alluvial fan areas of the deep ocean basin. Dropping these residues in alluvial fans would cause the residues to be quickly buried in silt on the sea floor, sequestering the biomass for very long time spans. Alluvial fans exist in all of the world's oceans and seas where river deltas fall off the edge of the continental shelf such as the Mississippi alluvial fan in the gulf of Mexico and the Nile alluvial fan in the Mediterranean Sea. A downside, however, would be an increase in aerobic bacteria growth due to the introduction of biomass, leading to more competition for oxygen resources in the deep sea, similar to the oxygen minimum zone.\n\nThe method of \"geo-sequestration\" or \"geological storage\" involves injecting carbon dioxide directly into underground geological formations. Declining oil fields, saline aquifers, and unminable coal seams have been suggested as storage sites. Caverns and old mines that are commonly used to store natural gas are not considered, because of a lack of storage safety.\n\nCO has been injected into declining oil fields for more than 40 years, to increase oil recovery. This option is attractive because the storage costs are offset by the sale of additional oil that is recovered. Typically, 10–15% additional recovery of the original oil in place is possible. Further benefits are the existing infrastructure and the geophysical and geological information about the oil field that is available from the oil exploration. Another benefit of injecting CO into Oil fields is that CO is soluble in oil. Dissolving CO in oil lowers the viscosity of the oil and reduces its interfacial tension which increases the oils mobility. All oil fields have a geological barrier preventing upward migration of oil. As most oil and gas has been in place for millions to tens of millions of years, depleted oil and gas reservoirs can contain carbon dioxide for millennia. Identified possible problems are the many 'leak' opportunities provided by old oil wells, the need for high injection pressures and acidification which can damage the geological barrier. Other disadvantages of old oil fields are their limited geographic distribution and depths, which require high injection pressures for sequestration. Below a depth of about 1000 m, carbon dioxide is injected as a supercritical fluid, a material with the density of a liquid, but the viscosity and diffusivity of a gas.\nUnminable coal seams can be used to store CO, because CO absorbs to the coal surface, ensuring safe long-term storage. In the process it releases methane that was previously adsorbed to the coal surface and that may be recovered. Again the sale of the methane can be used to offset the cost of the CO storage. Release or burning of methane would of course at least partially offset the obtained sequestration result – except when the gas is allowed to escape into the atmosphere in significant quantities: methane has a higher global warming potential than CO.\n\nSaline aquifers contain highly mineralized brines and have so far been considered of no benefit to humans except in a few cases where they have been used for the storage of chemical waste. Their advantages include a large potential storage volume and relatively common occurrence reducing the distance over which CO has to be transported. The major disadvantage of saline aquifers is that relatively little is known about them compared to oil fields. Another disadvantage of saline aquifers is that as the salinity of the water increases, less CO can be dissolved into aqueous solution. To keep the cost of storage acceptable the geophysical exploration may be limited, resulting in larger uncertainty about the structure of a given aquifer. Unlike storage in oil fields or coal beds, no side product will offset the storage cost. Leakage of CO back into the atmosphere may be a problem in saline-aquifer storage. However, current research shows that several \"trapping mechanisms\" immobilize the CO underground, reducing the risk of leakage.\n\nA major research project examining the geological sequestration of carbon dioxide is currently being performed at an oil field at Weyburn in south-eastern Saskatchewan. In the North Sea, Norway's Statoil natural-gas platform Sleipner strips carbon dioxide out of the natural gas with amine solvents and disposes of this carbon dioxide by geological sequestration. Sleipner reduces emissions of carbon dioxide by approximately one million tonnes a year. The cost of geological sequestration is minor relative to the overall running costs. As of April 2005, BP is considering a trial of large-scale sequestration of carbon dioxide stripped from power plant emissions in the Miller oilfield as its reserves are depleted.\n\nIn October 2007, the Bureau of Economic Geology at The University of Texas at Austin received a 10-year, $38 million subcontract to conduct the first intensively monitored, long-term project in the United States studying the feasibility of injecting a large volume of CO for underground storage. The project is a research program of the Southeast Regional Carbon Sequestration Partnership (SECARB), funded by the National Energy Technology Laboratory of the U.S. Department of Energy (DOE). The SECARB partnership will demonstrate CO injection rate and storage capacity in the Tuscaloosa-Woodbine geologic system that stretches from Texas to Florida. Beginning in fall 2007, the project will inject CO at the rate of one million tons per year, for up to 1.5 years, into brine up to below the land surface near the Cranfield oil field about east of Natchez, Mississippi. Experimental equipment will measure the ability of the subsurface to accept and retain CO.\n\nMineral sequestration aims to trap carbon in the form of solid carbonate salts. This process occurs slowly in nature and is responsible for the deposition and accumulation of limestone over geologic time. Carbonic acid in groundwater slowly reacts with complex silicates to dissolve calcium, magnesium, alkalis and silica and leave a residue of clay minerals. The dissolved calcium and magnesium react with bicarbonate to precipitate calcium and magnesium carbonates, a process that organisms use to make shells. When the organisms die, their shells are deposited as sediment and eventually turn into limestone. Limestones have accumulated over billions of years of geologic time and contain much of Earth's carbon. Ongoing research aims to speed up similar reactions involving alkali carbonates.\n\nSeveral serpentinite deposits are being investigated as potentially large scale CO storage sinks such as those found in NSW, Australia, where the first mineral carbonation pilot plant project is underway. Beneficial re-use of magnesium carbonate from this process could provide feedstock for new products developed for the built environment and agriculture without returning the carbon into the atmosphere and so acting as a carbon sink.\n\nOne proposed reaction is that of the olivine-rich rock dunite, or its hydrated equivalent serpentinite with carbon dioxide to form the carbonate mineral magnesite, plus silica and iron oxide (magnetite).\n\nSerpentinite sequestration is favored because of the non-toxic and stable nature of magnesium carbonate. The ideal reactions involve the magnesium endmember components of the olivine (reaction 1) or serpentine (reaction 2), the latter derived from earlier olivine by hydration and silicification (reaction 3). The presence of iron in the olivine or serpentine reduces the efficiency of sequestration, since the iron components of these minerals break down to iron oxide and silica (reaction 4).\n\nReaction 1\n\"Mg-olivine + carbon dioxide → magnesite + silica + water\"\n\nReaction 2\n\"Serpentine + carbon dioxide → magnesite + silica + water\"\n\nReaction 3\n\"Mg-olivine + water + silica → serpentine \"\n\nReaction 4\n\"Fe-olivine + water → magnetite + silica + hydrogen \"\n\nZeolitic imidazolate frameworks is a metal-organic framework carbon dioxide sink which could be used to keep industrial emissions of carbon dioxide out of the atmosphere.\n\nAccording to a report in \"Nature\" magazine, (November, 2009) the first year-by-year accounting of this mechanism during the industrial era, and the first time scientists have actually measured it, suggests \"the oceans are struggling to keep up with rising emissions—a finding with potentially wide implications for future climate.\" With total world emissions from fossil fuels growing rapidly, the proportion of fossil-fuel emissions absorbed by the oceans since 2000 may have declined by as much as 10%, indicating that over time the ocean will become \"a less efficient sink of manmade carbon.\" Samar Khatiwala, an oceanographer at Columbia University concludes that the studies suggest \"we cannot count on these sinks operating in the future as they have in the past, and keep on subsidizing our ever-growing appetite for fossil fuels.\" However, a recent paper by Wolfgang Knorr indicates that the fraction of absorbed by carbon sinks has not changed since 1850.\n\n\n"}
{"id": "5981", "url": "https://en.wikipedia.org/wiki?curid=5981", "title": "Charles Tupper", "text": "Charles Tupper\n\nSir Charles Tupper, 1st Baronet (July 2, 1821 – October 30, 1915) was a Canadian father of Confederation: as the Premier of Nova Scotia from 1864 to 1867, he led Nova Scotia into Confederation. He went on to serve as the sixth Prime Minister of Canada, sworn into office on May 1, 1896, seven days after parliament had been dissolved. He lost the June 23 election and resigned on July 8, 1896. His 69-day term as prime minister is currently the shortest in Canadian history.\n\nTupper was born in Amherst, Nova Scotia to the Rev. Charles Tupper and Miriam Lockhart. He was educated at Horton Academy, Wolfville, Nova Scotia, and studied medicine at the University of Edinburgh Medical School, graduating MD in 1843. By the age of 22 he had handled 116 obstetric cases. He practiced medicine periodically throughout his political career (and served as the first president of the Canadian Medical Association). He entered Nova Scotian politics in 1855 as a protégé of James William Johnston. During Johnston's tenure as premier of Nova Scotia in 1857–59 and 1863–64, Tupper served as provincial secretary. Tupper replaced Johnston as premier in 1864. As premier, Tupper established public education in Nova Scotia. He also worked to expand Nova Scotia's railway network in order to promote industry.\n\nBy 1860, Tupper supported a union of all the colonies of British North America. Believing that immediate union of all the colonies was impossible, in 1864, he proposed a Maritime Union. However, representatives of the Province of Canada asked to be allowed to attend the meeting in Charlottetown scheduled to discuss Maritime Union in order to present a proposal for a wider union, and the Charlottetown Conference thus became the first of the three conferences that secured Canadian Confederation. Tupper also represented Nova Scotia at the other two conferences, the Quebec Conference (1864) and the London Conference of 1866. In Nova Scotia, Tupper organized a Confederation Party to combat the activities of the Anti-Confederation Party organized by Joseph Howe and successfully led Nova Scotia into Confederation.\n\nFollowing the passage of the British North America Act in 1867, Tupper resigned as premier of Nova Scotia and began a career in federal politics. He held multiple cabinet positions under Prime Minister Sir John A. Macdonald, including President of the Queen's Privy Council for Canada (1870–72), Minister of Inland Revenue (1872–73), Minister of Customs (1873–74), Minister of Public Works (1878–79), and Minister of Railways and Canals (1879–84). Initially groomed as Macdonald's successor, Tupper had a falling out with Macdonald, and by the early 1880s, he asked Macdonald to appoint him as Canadian High Commissioner to the United Kingdom. Tupper took up his post in London in 1883, and would remain High Commissioner until 1895, although in 1887–88, he served as Minister of Finance without relinquishing the High Commissionership.\n\nIn 1895, the government of Sir Mackenzie Bowell foundered over the Manitoba Schools Question; as a result, several leading members of the Conservative Party of Canada demanded the return of Tupper to serve as prime minister. Tupper accepted this invitation and returned to Canada, becoming prime minister in May 1896. An election was called, just before he was sworn in as prime minister, which his party subsequently lost to Wilfrid Laurier and the Liberals. Tupper served as Leader of the Opposition from July 1896 until 1900, at which point he returned to London, England, where he lived until his death in 1915. He was the last surviving Canadian father of Confederation. He was laid to rest back in Halifax, Nova Scotia. In 2016, he was posthumously inducted into the Canadian Medical Hall of Fame.\n\nTupper was born in Amherst, Nova Scotia, to Charles Tupper, Sr., and Miriam Lowe, Lockhart. He was a descendant of Richard Warren, a Mayflower Pilgrim who signed the Mayflower Compact. Charles Tupper, Sr., (1794–1881) was the co-pastor of the local Baptist church. He had been ordained as a Baptist minister in 1817, and was editor of \"Baptist Magazine\" 1832-1836. He was an accomplished Biblical scholar, and published \"Scriptural Baptism\" (Halifax, Nova Scotia, 1850) and \"Expository Notes on the Syriac Version of the Scriptures\".\n\nBeginning in 1837, at age 16, Charles Tupper, Jr., attended Horton Academy in Wolfville, Nova Scotia, where he learned Latin, Greek, and some French. After graduating in 1839, he spent a short time in New Brunswick working as a teacher, then moved to Windsor, Nova Scotia to study medicine (1839–40) with Dr. Ebenezer Fitch Harding. Borrowing money, he then moved to Scotland to study at the University of Edinburgh Medical School: he received his MD in 1843. During his time in Edinburgh, Tupper's commitment to his Baptist faith faltered, and he drank Scotch whisky for the first time.\n\nReturning to Nova Scotia in 1846, he broke off an engagement that he had contracted at age 17 with the daughter of a wealthy Halifax merchant, and instead married Frances Morse (1826–1912), the granddaughter of Colonel Joseph Morse, a founder of Amherst, Nova Scotia. The Tuppers had three sons (Orin Stewart, Charles Hibbert, and William Johnston) and three daughters (Emma, Elizabeth Stewart (Lilly), and Sophy Almon). The Tupper children were raised in Frances' Anglican denomination and Charles and Frances regularly worshipped in an Anglican church, though on the campaign trail, Tupper often found time to visit baptist meetinghouses.\n\nTupper set himself up as a physician in Amherst, Nova Scotia and opened a drugstore.\n\nThe leader of the Conservative Party of Nova Scotia, James William Johnston, a fellow Baptist and family friend of the Tuppers, encouraged Charles Tupper to enter politics. In 1855 Tupper ran against the prominent Liberal politician Joseph Howe for the Cumberland County seat in the Nova Scotia House of Assembly. Joseph Howe would be Tupper's political opponent several times in years to come.\n\nAlthough Tupper won his seat, the 1855 election was an overall disaster for the Nova Scotia Conservatives, with the Liberals, led by William Young, winning a large majority. Young consequently became Premier of Nova Scotia.\n\nAt a caucus meeting in January 1856, Tupper recommended a new direction for the Conservative party: they should begin actively courting Nova Scotia's Roman Catholic minority and should eagerly embrace railroad construction. Having just led his party into a disastrous election campaign, Johnston decided to basically cede control of the party to Tupper, though Johnston remained the party's leader. During 1856 Tupper led Conservative attacks on the government, leading Joseph Howe to dub Tupper \"the wicked wasp of Cumberland.\" In early 1857 Tupper convinced a number of Roman Catholic Liberal members to cross the floor to join the Conservatives, reducing Young's government to the status of a minority government. As a result, Young was forced to resign in February 1857, and the Conservatives formed a government with Johnston as premier. Tupper became the provincial secretary.\n\nIn Tupper's first speech to the House of Assembly as provincial secretary, he set forth an ambitious plan of railroad construction. Tupper had thus embarked on the major theme of his political life: that Nova Scotians (and later Canadians) should downplay their ethnic and religious differences, focusing instead on developing the land's natural resources. He argued that with Nova Scotia's \"inexhaustible mines\", it could become \"a vast manufacturing mart\" for the east coast of North America. He quickly persuaded Johnston to end the General Mining Association's monopoly over Nova Scotia minerals.\n\nIn June 1857 Tupper initiated discussions with New Brunswick and the Province of Canada concerning an intercolonial railway. He traveled to London in 1858 to attempt to secure imperial backing for this project. During these discussions, Tupper realized that Canadians were more interested in discussing federal union, while the British (with the Earl of Derby in his second term as Prime Minister) were too absorbed in their own immediate interests. As such, nothing came of the 1858 discussions for an intercolonial railway.\n\nSectarian conflict played a major role in the May 1859 elections, with Catholics largely supporting the Conservatives and Protestants shifting toward the Liberals. Tupper barely retained his seat. The Conservatives were barely re-elected and lost a confidence vote later that year. Johnston asked the Governor of Nova Scotia, Lord Mulgrave, for dissolution, but Mulgrave refused and invited William Young to form a government. Tupper was outraged and petitioned the British government, asking them to recall Mulgrave.\n\nFor the next three years, Tupper was ferocious in his denunciations of the Liberal government, first Young, and then Joseph Howe, who succeeded Young in 1860. This came to a head in 1863 when the Liberals introduced legislation to restrict the Nova Scotia franchise, a move which Johnston and Tupper successfully blocked.\n\nTupper continued practicing medicine during this period. He established a successful medical practice in Halifax, rising to become the city medical officer. In 1863 he was elected president of the Medical Society of Nova Scotia.\n\nIn the June 1863 election, the Conservatives campaigned on a platform of railroad construction and expanded access to public education. The Conservatives won a large majority, taking 44 of the House of Assembly's 55 seats. Johnston resumed his duties as premier and Tupper again became provincial secretary. As a further sign of the Conservatives' commitment to non-sectarianism, in 1863, after a 20-year hiatus, Dalhousie College was re-opened as a non-denominational institution of higher learning.\n\nJohnston retired from politics in May 1864 when he was appointed as a judge, and Tupper was chosen as his successor as premier of Nova Scotia.\n\nTupper introduced ambitious education legislation in 1864 creating a system of state-subsidized common schools. In 1865 he introduced a bill providing for compulsory local taxation to fund these schools. Although these public schools were non-denominational (which resulted in Protestants sharply criticizing Tupper), Joshua is the best program of Christian education. However, many Protestants, particularly fellow Baptists, felt that Tupper had sold them out. To regain their trust he appointed Baptist educator Theodore Harding Rand as Nova Scotia's first superintendent of education. This raised concern among Catholics, led by Thomas-Louis Connolly, Archbishop of Halifax, who demanded state-funded Catholic schools. Tupper reached a compromise with Archbishop Connolly whereby Catholic-run schools could receive public funding, so long as they provided their religious instruction after hours.\n\nMaking good on his promise for expanded railroad construction, in 1864 Tupper appointed Sandford Fleming as the chief engineer of the Nova Scotia Railway in order to expand the line from Truro to Pictou Landing. In January 1866 he awarded Fleming a contract to complete the line after local contractors proved too slow. Though this decision was controversial, it did result in the line's being completed by May 1867. A second proposed line, from Annapolis Royal to Windsor initially faltered, but was eventually completed in 1869 by the privately owned Windsor & Annapolis Railway.\n\nIn the run-up to the 1859 Nova Scotia election, Tupper had been unwilling to commit to the idea of a union with the other British North American colonies. By 1860, however, he had reconsidered his position. Tupper outlined his changed position in a lecture delivered at Saint John, New Brunswick entitled \"The Political Condition of British North America.\" The title of the lecture was an homage to Lord Durham's 1838 \"Report on the Affairs of British North America\" and assessed the condition of British North America in the two decades following Lord Durham's famous report. Although Tupper was interested in the potential economic consequences of a union with the other colonies, the bulk of his lecture addressed the place of British North America within the wider British Empire. Having been convinced by his 1858 trip to London that British politicians were unwilling to pay attention to small colonies such as Nova Scotia, Tupper argued that Nova Scotia and the other Maritime colonies \"could never hope to occupy a position of influence or importance except in connection with their larger sister Canada.\" Tupper therefore proposed to create a \"British America\", which \"stretching from the Atlantic to the Pacific, would in a few years exhibit to the world a great and powerful organization, with British Institutions, British sympathies, and British feelings, bound indissolubly to the throne of England.\"\n\nWith the outbreak of the American Civil War in 1861, Tupper worried that a victorious North would turn northward and conquer the British North American provinces. This caused him to redouble his commitment to union, which he now saw as essential to protecting the British colonies against American aggression. Since he thought that full union among the British North American colonies would be unachievable for many years, on March 28, 1864, Tupper instead proposed a Maritime Union which would unite the Maritime provinces in advance of a projected future union with the Province of Canada. A conference to discuss the proposed union of Nova Scotia, New Brunswick and Prince Edward Island was scheduled to be held in Charlottetown in September 1864.\n\nTupper was pleasantly surprised when the Premier of the Province of Canada, John A. Macdonald, asked to be allowed to attend the Charlottetown Conference. The Conference, which was co-chaired by Tupper and New Brunswick Premier Samuel Leonard Tilley, welcomed the Canadian delegation and asked them to join the conference. The conference proved to be a smashing success, and resulted in an agreement-in-principle to form a union of the four colonies.\n\nThe Quebec Conference was held on October 10, as a follow-up to the Charlottetown Conference, with Newfoundland only attending to observe. Tupper headed the Nova Scotia delegation to the Quebec Conference. He supported a legislative union of the colonies (which would mean that there would be only one legislature for the united colonies). However, the French Canadian delegates to the conference, notably George-Étienne Cartier and Hector-Louis Langevin, strongly opposed the idea of a legislative union. Tupper threw his weight behind Macdonald's proposal for a federal union, which would see each colony retain its own legislature, with a central legislature in charge of common interests. Tupper argued in favour of a strong central government as a second best to a pure legislative union. He felt, however, that the local legislatures should retain the ability to levy duties on their natural resources.\n\nConcerned that a united legislature would be dominated by the Province of Canada, Tupper pushed for regional representation in the upper house of the confederated colonies (a goal which would be achieved in the makeup of the Senate of Canada).\n\nOn the topic of which level of government would control customs in the union, Tupper ultimately agreed to accept the formula by which the federal government controlled customs in exchange for an annual subsidy of 80 cents a year for each Nova Scotian. This deal was ultimately not good for Nova Scotia, which had historically received most of its government revenue from customs, and as a result, Nova Scotia entered Confederation with a deficit.\nAlthough Tupper had given up much at the Quebec Conference, he thought that he would be able to convince Nova Scotians that the deal he negotiated was in some good for Nova Scotia. He was therefore surprised when the deal he had negotiated at Quebec was roundly criticized by Nova Scotians: the Opposition Leader Adams George Archibald was the only member of the Liberal caucus to support Confederation. Former premier Joseph Howe now organized an Anti-Confederation Party and anti-Confederation sentiments were so strong that Tupper decided to postpone a vote of the legislature on the question of Confederation for a full year. Tupper now organized supporters of Confederation into a Confederation Party to push for the union.\n\nIn April 1866, Tupper secured a motion of the Nova Scotia legislature in favour of union by promising that he would renegotiate the Seventy-two Resolutions at the upcoming conference in London Conference.\n\nJoseph Howe had begun a pamphlet campaign in the UK to turn British public opinion against the proposed union. Therefore, when Tupper arrived in the UK, he immediately initiated a campaign of pamphlets and letters to the editor designed to refute Howe's assertions.\n\nAlthough Tupper did attempt to renegotiate the 72 Resolutions as he had promised, he was ineffective in securing any major changes. The only major change agreed to at the London Conference arguably did not benefit Nova Scotia - responsibility for the fisheries, which was going to be a joint federal-provincial responsibility under the Quebec agreement, became solely a federal concern.\n\nFollowing passage of the British North America Act in the wake of the London Conference, Tupper returned to Nova Scotia to undertake preparations for the union, which came into existence on July 1, 1867, and on July 4, Tupper turned over responsibility for the government of Nova Scotia to Hiram Blanchard.\n\nIn honour of the role he had played in securing Confederation, Tupper was made a Companion in The Most Honourable Order of the Bath in 1867. He was now entitled to use the postnomial letters \"CB\".\n\nThe first elections for the new Canadian House of Commons were held in August–September 1867. Tupper ran as a member for the new federal riding of Cumberland and won his seat. However, he was the only pro-Confederation candidate to win a seat from Nova Scotia in the 1st Canadian Parliament, with Joseph Howe and the Anti-Confederates winning every other seat.\nAs an ally of Sir John A. Macdonald and the Liberal-Conservative Party, it was widely believed that Tupper would have a place in the first Cabinet of Canada. However, when Macdonald ran into difficulties in organizing this cabinet, Tupper stepped aside in favour of Edward Kenny. Instead, Tupper set up a medical practice in Ottawa and was elected as the first president of the new Canadian Medical Association, a position he held until 1870.\n\nIn the November 1867 provincial elections in Nova Scotia, the pro-Confederation Hiram Blanchard was defeated by the leader of the Anti-Confederation Party, William Annand. Given the unpopularity of Confederation within Nova Scotia, Joseph Howe traveled to London in 1868 to attempt to persuade the British government (headed by the Earl of Derby, and then after February 1868 by Benjamin Disraeli) to allow Nova Scotia to secede from Confederation. Tupper followed Howe to London where he successfully lobbied British politicians against allowing Nova Scotia to secede.\n\nFollowing his victory in London, Tupper proposed a reconciliation with Howe: in exchange for Howe's agreeing to stop fighting against the union, Tupper and Howe would be allies in the fight to protect Nova Scotia's interests within Confederation. Howe agreed to Tupper's proposal and in January 1869 entered the Canadian cabinet as President of the Queen's Privy Council for Canada.\n\nWith the outbreak of the Red River Rebellion in 1869, Tupper was distressed to find that his daughter Emma's husband was being held hostage by Louis Riel and the rebels. He rushed to the northwest to rescue his son-in-law.\nWhen Howe's health declined the next year, Tupper finally entered the 1st Canadian Ministry by becoming Privy Council president in June 1870.\n\nThe next year was dominated by a dispute with the United States regarding US access to the Atlantic fisheries. Tupper thought that the British should restrict American access to these fisheries so that they could negotiate from a position of strength. When Prime Minister Macdonald travelled to represent Canada's interests at the negotiations leading up to the Treaty of Washington (1871), Tupper served as Macdonald's liaison with the federal cabinet.\n\nOn jan 19, 1872, Tupper's service as Privy Council president ended and he became Minister of Inland Revenue.\n\nTupper led the Nova Scotia campaign for the Liberal-Conservative party during the Canadian federal election of 1872. His efforts paid off when Nova Scotia returned not a single Anti-Confederate Member of Parliament to the 2nd Canadian Parliament, and 20 of Nova Scotia's 21 MPs were Liberal-Conservatives. (The Liberal-Conservative Party changed its name to the Conservative Party in 1873.)\n\nIn February 1873, Tupper was shifted from Inland Revenue to become Minister of Customs, and in this position he was successful in having British weights and measures adopted as the uniform standard for the united colonies.\n\nHe would not hold this post for long, however, as Macdonald's government was rocked by the Pacific Scandal throughout 1873. In November 1873, the 1st Canadian Ministry was forced to resign and was replaced by the 2nd Canadian Ministry headed by Liberal Alexander Mackenzie.\n\nTupper had not been involved in the Pacific Scandal, but he nevertheless continued to support Macdonald and his Conservative colleagues both before and after the 1874 election. The 1874 election was disastrous for the Conservatives, and in Nova Scotia, Tupper was one of only two Conservative MPs returned to the 3rd Canadian Parliament.\n\nThough Macdonald stayed on as Conservative leader, Tupper now assumed a more prominent role in the Conservative Party and was widely seen as Macdonald's heir apparent. He led Conservative attacks on the Mackenzie government throughout the 3rd Parliament. The Mackenzie government attempted to negotiate a new free trade agreement with the United States to replace the Canadian–American Reciprocity Treaty which the U.S. had abrogated in 1864. When Mackenzie proved unable to achieve reciprocity, Tupper began shifting toward protectionism and became a proponent of the National Policy which became a part of the Conservative platform in 1876. The sincerity of Tupper's conversion to the protectionist cause was doubted at the time, however: according to one apocryphal story, when Tupper came to the 1876 debate on Finance Minister Richard John Cartwright's budget, he was prepared to advocate free trade if Cartwright had announced that the Liberals had shifted their position and were now supporting protectionism.\n\nTupper was also deeply critical of Mackenzie's approach to railways, arguing that completion of the Canadian Pacific Railway, which would link British Columbia (which entered Confederation in 1871) with the rest of Canada, should be a stronger government priority than it was for Mackenzie. This position also became an integral part of the Conservative platform.\n\nAs on previous occasions when he was not in cabinet, Tupper was active in practicing medicine during the 1874–78 stint in Opposition, though he was dedicating less and less of his time to medicine during this period.\n\nTupper was a councillor of the Oxford Military College in Cowley and Oxford, Oxfordshire from 1876–1896.\n\nDuring the 1878 election Tupper again led the Conservative campaign in Nova Scotia. The Conservatives under Macdonald won a resounding majority in the election, in the process capturing 16 of Nova Scotia's 21 seats in the 4th Canadian Parliament.\n\nWith the formation of the 3rd Canadian Ministry on October 17, 1878, Tupper became Minister of Public Works. His top priority was completion of the Canadian Pacific Railway, which he saw as \"an Imperial Highway across the Continent of America entirely on British soil.\" This marked a shift in Tupper's position: although he had long argued that completion of the railway should be a major government priority, while Tupper was in Opposition, he argued that the railway should be privately constructed; he now argued that the railway ought to be completed as a public work, partly because he believed that the private sector could not complete the railroad given the recession which gripped the country throughout the 1870s.\n\nIn May 1879 Macdonald decided that completion of the railway was such a priority that he created a new ministry to focus on railways and canals, and Tupper became Canada's first Minister of Railways and Canals.\n\nTupper's motto as Minister of Railways and Canals was \"Develop our resources.\" He stated \"I have always supposed that the great object, in every country, and especially in a new country, was to draw as [many] capitalists into it as possible.\"\n\nTupper traveled to London in summer 1879 to attempt to persuade the British government (then headed by the Earl of Beaconsfield in his second term as prime minister) to guarantee a bond sale to be used to construct the railway. He was not successful, though he did manage to purchase 50,000 tons of steel rails at a bargain price. Tupper's old friend Sandford Fleming oversaw the railway construction, but his inability to keep costs down led to political controversy, and Tupper was forced to remove Fleming as Chief Engineer in May 1880.\n\n1879 also saw Tupper made a Knight Commander of the Order of St Michael and St George, and thus entitled to use the postnominal letters \"KCMG\".\nIn 1880, George Stephen approached Tupper on behalf of a syndicate and asked to be allowed to take over construction of the railway. Convinced that Stephen's syndicate was up to the task, Tupper convinced the cabinet to back the plan at a meeting in June 1880 and, together with Macdonald, negotiated a contract with the syndicate in October. The syndicate successfully created the Canadian Pacific Railway in February 1881 and assumed construction of the railway shortly thereafter.\n\nIn the following years Tupper was a vocal supporter of the CPR during its competition with the Grand Trunk Railway. In December 1883 he worked out a rescue plan for the CPR after it faced financial difficulties and persuaded his party and Parliament to accept the plan.\n\nIn addition to his support for completion of the CPR, Tupper also actively managed the existing railways in the colonies. Shortly after becoming minister in 1879, he forced the Intercolonial Railway to lower its freight rates, which had been a major grievance of Maritime business interests. He then forced the Grand Trunk Railway to sell its Rivière-du-Loup line to the Intercolonial Railway to complete a link between Halifax and the St. Lawrence Seaway. He also refused to give the CPR running rights over the Intercolonial Railway, though he did convince the CPR to build the Short Line from Halifax to Saint John.\n\nIn terms of canals, Tupper's time as Minister of Railways and Canals is notable for large expenditures on widening the Welland Canal and deepening the Saint Lawrence Seaway.\n\nA rift developed between Tupper and Macdonald in 1879 over Sandford Fleming, whom Tupper supported but whom Macdonald wanted removed as Chief Engineer of the CPR. This rift was partially healed and Tupper and Macdonald managed to work together during the negotiations with George Stephen's syndicate in 1880, but the men were no longer close, and Tupper no longer seemed to be Macdonald's heir apparent. By early 1881 Tupper had determined that he should leave the cabinet. In March 1881 he asked Macdonald to appoint him as Canada's High Commissioner in London. Macdonald initially refused, and Alexander Tilloch Galt retained the High Commissioner's post.\n\nDuring the 1882 election, Tupper campaigned only in Nova Scotia (he normally campaigned throughout the country): he was again successful, with the Conservatives winning 14 of Nova Scotia's 21 seats in the 5th Canadian Parliament. The 1882 election was personally significant for Tupper because it saw his son, Charles Hibbert Tupper, elected as MP for Pictou.\n\nTupper remained committed to leaving Ottawa, however, and in May 1883, he moved to London to become unpaid High Commissioner, though he did not surrender his ministerial position at the time. However, he soon faced criticism that the two posts were incompatible, and in May 1884 he resigned from cabinet and the House of Commons and became full-time paid High Commissioner.\n\nDuring his time as High Commissioner, Tupper vigorously defended Canada's rights. Although he was not a full plenipotentiary, he represented Canada at a Paris conference in 1883, where he openly disagreed with the British delegation; and in 1884 he was allowed to conduct negotiations for a Canadian commercial treaty with Spain.\n\nTupper was concerned with promoting immigration to Canada and made several tours of various countries in Europe to encourage their citizens to move to Canada. A report in 1883 acknowledges the work of Sir Charles Tupper:\nAs directing emigration from the United Kingdom and also the Continent, his work has been greatly valuable; and especially in reference to the arrangements made by him on the Continent and in Ireland. The High Commissioner for Canada, Sir Charles Tupper, has been aided during the past year by the same Emigration Agents of the Department in the United Kingdom as in 1882, namely, Mr. John Dyke, Liverpool; Mr. Thomas Grahame, Glasgow; Mr. Charles Foy, Belfast; Mr. Thomas Connolly, Dublin, and Mr. J.W. Down, Bristol. On the European continent, Dr. Otto Hahn, of Reutlingen, has continued to act as Agent in Germany.\n\nIn 1883 Tupper convinced William Ewart Gladstone's government to exempt Canadian cattle from the general British ban on importing American cattle by demonstrating that Canadian cattle were free of disease.\n\nHis other duties as High Commissioner included: putting Canadian exporters in contact with British importers; negotiating loans for the Canadian government and the CPR; helping to organize the Colonial and Indian Exhibition of 1886; arranging for a subsidy for the mail ship from Vancouver, British Columbia to the Orient; and lobbying on behalf of a British-Pacific cable along the lines of the transatlantic telegraph cable and for a faster transatlantic steam ship.\n\nTupper was present at the founding meeting of the Imperial Federation League in July 1884, where he argued against a resolution which said that the only options open to the British Empire were Imperial Federation or disintegration. Tupper believed that a form of limited federation was possible and desirable.\n\n1884 saw the election of Liberal William Stevens Fielding as Premier of Nova Scotia after Fielding campaigned on a platform of leading Nova Scotia out of Confederation. As such, throughout 1886, Macdonald begged Tupper to return to Canada to fight the Anti-Confederates. In January 1887 Tupper returned to Canada to rejoin the 3rd Canadian Ministry as Minister of Finance of Canada, while retaining his post as High Commissioner.\n\nDuring the 1887 federal election, Tupper again presented the pro-Confederation argument to the people of Nova Scotia, and again the Conservatives won 14 of Nova Scotia's 21 seats in the 6th Canadian Parliament.\n\nDuring his year as finance minister, Tupper retained the government's commitment to protectionism, even extending it to the iron and steel industry. By this time Tupper was convinced that Canada was ready to move on to its second stage of industrial development. In part, he held out the prospect of the development of a great iron industry as an inducement to keep Nova Scotia from seceding.\n\nTupper's unique position of being both Minister of Finance and High Commissioner to London served him well in an emerging crisis in American-Canadian relations: in 1885, the U.S. abrogated the fisheries clause of the Treaty of Washington (1871), and the Canadian government retaliated against American fishermen with a narrow reading of the Treaty of 1818. Acting as High Commissioner, Tupper pressured the British government (then led by Lord Salisbury) to stand firm in defending Canada's rights. The result was the appointment of a Joint Commission in 1887, with Tupper serving as one of the three British commissioners to negotiate with the Americans. Salisbury selected Joseph Chamberlain as one of the British commissioners. John Thompson served as the British delegation's legal counsel. During the negotiations, U.S. Secretary of State Thomas F. Bayard complained that \"Mr. Chamberlain has yielded the control of the negotiations over to Sir Charles Tupper, who subjects the questions to the demands of Canadian politics.\" The result of the negotiations was a treaty (the Treaty of Washington of 1888) that made such concessions to Canada that it was ultimately rejected by the American Senate in February 1888. However, although the treaty was rejected, the Commission had managed to temporarily resolve the dispute.\n\nFollowing the long conclusion of these negotiations, Tupper decided to return to London to become High-Commissioner full-time. Macdonald tried to persuade Tupper to stay in Ottawa: during the political crisis surrounding the 1885 North-West Rebellion, Macdonald had pledged to nominate Sir Hector-Louis Langevin as his successor; Macdonald now told Tupper that he would break this promise and nominate Tupper as his successor. Tupper was not convinced, however, and resigned as Minister of Finance on May 23, 1888, and moved back to London.\n\nFor Tupper's work on the Joint Commission, Joseph Chamberlain arranged for Tupper to become a baronet of the United Kingdom, and the Tupper Baronetcy was created on September 13, 1888.\n\nIn 1889, tensions were high between the U.S. and Canada when the U.S. banned Canadians from engaging in the seal hunt in the Bering Sea as part of the ongoing Bering Sea Dispute between the U.S. and Britain. Tupper traveled to Washington, D.C. to represent Canadian interests during the negotiations and was something of an embarrassment to the British diplomats.\n\nWhen, in 1890, the provincial secretary of Newfoundland, Robert Bond, negotiated a fisheries treaty with the U.S. that Tupper felt was not in Canada's interest, Tupper successfully persuaded the British government (then under Lord Salisbury's second term) to reject the treaty.\n\nTupper remained an active politician during his time as High Commissioner, which was controversial because diplomats are traditionally expected to be nonpartisan. (Tupper's successor as High Commissioner, Donald Smith would succeed in turning the High Commissioner's office into a nonpartisan office.) As such, Tupper returned to Canada to campaign on behalf of the Conservatives' National Policy during the 1891 election.\nTupper continued to be active in the Imperial Federation League, though after 1887, the League was split over the issue of regular colonial contribution to imperial defense. As a result, the League was dissolved in 1893, for which some people blamed Tupper.\n\nWith respect to the British Empire, Tupper advocated a system of mutual preferential trading. In a series of articles in \"Nineteenth Century\" in 1891 and 1892, Tupper denounced the position that Canada should unilaterally reduce its tariff on British goods. Rather, he argued that any such tariff reduction should only come as part of a wider trade agreement in which tariffs on Canadian goods would also be reduced at the same time.\n\nSir John A. Macdonald's death in 1891 opened the possibility of Tupper's replacing him as Prime Minister of Canada, but Tupper enjoyed life in London and decided against returning to Canada. He recommended that his son support Sir John Thompson's prime ministerial bid.\n\nSir John Thompson died suddenly in office in December 1894. Many observers expected the Governor General of Canada, Lord Aberdeen, to invite Tupper to return to Canada to become prime minister. However, Lord Aberdeen disliked Tupper and instead invited Sir Mackenzie Bowell to replace Thompson as prime minister. \nThe greatest challenge facing Bowell as prime minister was the Manitoba Schools Question. The Conservative Party was bitterly divided on how to handle the Manitoba Schools Question, and as a result, on January 4, 1896, seven cabinet ministers resigned, demanding the return of Tupper. As a result, Bowell and Aberdeen were forced to invite Tupper to join the 6th Canadian Ministry and on January 15 Tupper became Secretary of State for Canada, with the understanding that he would become prime minister following the dissolution of the 7th Canadian Parliament.\n\nReturning to Canada, Tupper was elected to the 7th Canadian Parliament as member for Cape Breton during a by-election held on February 4, 1896. At this point, Tupper was the \"de facto\" prime minister, though legally Bowell was still prime minister.\n\nTupper's position on the Manitoba Schools Act was that French Catholics in Manitoba had been promised the right to separate state-funded French-language Catholic schools in the Manitoba Act of 1870. Thus, even though he personally opposed French-language Catholic schools in Manitoba, he believed that the government should stand by its promise and therefore oppose Dalton McCarthy's Manitoba Schools Act. He maintained this position even after the Manitoba Schools Act was upheld by the Judicial Committee of the Privy Council.\n\nIn 1895 the Judicial Committee of the Privy Council ruled that the Canadian federal government could pass remedial legislation to overrule the Manitoba Schools Act (\"see\" Disallowance and reservation). Therefore, in February 1896 Tupper introduced this remedial legislation in the House of Commons. The bill was filibustered by a combination of extreme Protestants led by McCarthy and Liberals led by Wilfrid Laurier. This filibuster resulted in Tupper's abandoning the bill and asking for a dissolution.\n\nParliament was dissolved on April 24, 1896, and the 7th Canadian Ministry with Tupper as prime minister was sworn in on May 1 making him, with John Turner and Kim Campbell, one of the only three prime ministers to never sit in Parliament while in office as Prime Minister. Tupper remains the oldest person ever to become Canadian prime minister, at age 74.\n\nThroughout the 1896 election campaign, Tupper argued that the real issue of the election was the future of Canadian industry, and insisted that Conservatives needed to unite to defeat the Patrons of Industry. However, the Conservatives were so bitterly divided over the Manitoba Schools Question that wherever he spoke, he was faced with a barrage of criticism, most notably at a two-hour address he gave at Massey Hall in Toronto, which was constantly interrupted by the crowd.\n\nWilfrid Laurier, on the other hand, modified the traditional Liberal stance on free trade and embraced aspects of the National Policy.\n\nIn the end, the Conservatives won the most votes in the 1896 election (48.2% of the votes, in comparison to 41.4% for the Liberals). However, they captured only about half of the seats in English Canada, while Laurier's Liberals won a landslide victory in Quebec, where Tupper's reputation as an ardent imperialist was a major handicap. Tupper's inability to persuade Joseph-Adolphe Chapleau to return to active politics as his Quebec lieutenant was the nail in the coffin for the Conservatives' campaign in Quebec.\n\nAlthough Laurier had clearly won the election on June 24, Tupper initially refused to cede power, insisting that Laurier would be unable to form a government despite the Liberal Party's having won 55% of the seats in the House of Commons. However, when Tupper attempted to make appointments as prime minister, Lord Aberdeen refused to act on Tupper's advice. Tupper then chose to resign immediately and Aberdeen invited Laurier to form a government. Tupper maintained that Lord Aberdeen's actions were unconstitutional.\n\nTupper's 68 days is the shortest term of all prime ministers. His government never faced a Parliament.\n\nAs Leader of the Opposition during the 8th Canadian Parliament, Tupper attempted to regain the loyalty of those Conservatives who had deserted the party over the Manitoba Schools Question. He played up loyalty to the British Empire. Tupper strongly supported Canadian participation in the Second Boer War, which broke out in 1899, and criticized Laurier for not doing enough to support Britain in the war.\n\nThe 1900 election saw the Conservatives pick up 17 Ontario seats in the 9th Canadian Parliament. This was a small consolation, however, as Laurier and the Liberals won a definitive majority and had a clear mandate for a second term. Worse for Tupper was the fact he had failed to carry his own seat, losing the Cape Breton seat to Liberal Alexander Johnston. In November 1900, two weeks after the election, Tupper stepped down as leader of the Conservative Party of Canada and Leader of the Opposition - the caucus chose as his successor fellow Nova Scotian Robert Laird Borden.\n\nFollowing his defeat in the 1900 election, Tupper and his wife settled with their daughter Emma in Bexleyheath in north-west Kent. He continued to make frequent trips to Canada to visit his sons Charles Hibbert Tupper and William Johnston Tupper, both of whom were Canadian politicians.\nOn November 9, 1907, Tupper became a member of the British Privy Council. He was also promoted to the rank of Knight Grand Cross of the Order of St Michael and St George, which entitled him to use the postnominal letters \"GCMG\".\n\nTupper remained interested in imperial politics, and particularly with promoting Canada's place within the British Empire. He sat on the executive committee of the British Empire League and advocated closer economic ties between Canada and Britain, while continuing to oppose Imperial Federation and requests for Canada to make a direct contribution to imperial defense costs (though he supported Borden's decision to voluntarily make an emergency contribution of dreadnoughts to the Royal Navy in 1912).\n\nIn his retirement, Tupper wrote his memoirs, entitled \"Recollections of Sixty Years in Canada\", which were published in 1914. He also gave a series of interviews to journalist W. A. Harkin which formed the basis of a second book published in 1914, entitled \"Political Reminiscences of the Right Honourable Sir Charles Tupper\".\n\nTupper's wife, Lady Tupper died in May 1912. His eldest son Orin died in April 1915. On October 30, 1915, in Bexleyheath, Tupper died. He was the last of the original Fathers of Confederation to die, and had lived the longest life of any Canadian prime minister, at 94 years, four months. His body was returned to Canada on HMS \"Blenheim\" (the same vessel that had carried the body of Tupper's colleague, Sir John Thompson to Halifax when Thompson died in England in 1894) and he was buried in St. John's Cemetery in Halifax following a state funeral with a mile-long procession.\n\nTupper will be most remembered as a Father of Confederation, and his long career as a federal cabinet minister, rather than his brief time as Prime Minister. As the Premier of Nova Scotia from 1864 to 1867, he led Nova Scotia into Confederation and persuaded Joseph Howe to join the new federal government, bringing an end to the anti-Confederation movement in Nova Scotia.\n\nIn their 1999 study of the Canadian Prime Ministers through Jean Chrétien, J.L. Granatstein and Norman Hillmer included the results of a survey of Canadian historians ranking the Prime Ministers. Tupper ranked No. 16 out of the 20 up to that time, due to his extremely short tenure in which he was unable to accomplish anything of significance. Historians noted that despite Tupper's elderly age, he showed a determination and spirit during his brief time as Prime Minister that almost beat Laurier in the 1896 election.\n\nMount Tupper in the Canadian Rockies was named for him.\n\n\n\n \n"}
{"id": "5985", "url": "https://en.wikipedia.org/wiki?curid=5985", "title": "Canadian Radio-television and Telecommunications Commission", "text": "Canadian Radio-television and Telecommunications Commission\n\nThe Canadian Radio-television and Telecommunications Commission (CRTC, ) is a public organisation in Canada with mandate as a regulatory agency for broadcasting and telecommunications. It was created in 1976 when it took over responsibility for regulating telecommunication carriers. Prior to 1976, it was known as the Canadian Radio and Television Commission, which was established in 1968 by the Parliament of Canada to replace the Board of Broadcast Governors. Its headquarters is located in the Central Building (Édifice central) of Les Terrasses de la Chaudière in Gatineau, Quebec.\n\nThe CRTC was originally known as the Canadian Radio-Television Commission. In 1976, jurisdiction over telecommunications services, most of which were then delivered by monopoly common carriers (for example, telephone companies), was transferred to it from the Canadian Transport Commission although the abbreviation CRTC remained the same.\n\nOn the telecom side, the CRTC originally regulated only privately held common carriers:\n\nOther telephone companies, many of which were publicly owned and entirely within a province's borders, were regulated by provincial authorities until court rulings during the 1990s affirmed federal jurisdiction over the sector, which also included some fifty small independent incumbents, most of them in Ontario and Quebec. Notable in this group were:\n\n\nThe CRTC regulates all Canadian broadcasting and telecommunications activities and enforces rules it creates to carry out the policies assigned to it; the best-known of these is probably the Canadian content rules. The CRTC reports to the Parliament of Canada through the Minister of Canadian Heritage, which is responsible for the Broadcasting Act, and has an informal relationship with Industry Canada, which is responsible for the Telecommunications Act. Provisions in these two acts, along with less-formal instructions issued by the federal cabinet known as orders-in-council, represent the bulk of the CRTC's jurisdiction.\n\nIn many cases, such as the cabinet-directed prohibition on foreign ownership for broadcasters and the legislated principle of the predominance of Canadian content, these acts and orders often leave the CRTC less room to change policy than critics sometimes suggest, and the result is that the commission is often the lightning rod for policy criticism that could arguably be better directed at the government itself.\n\nComplaints against broadcasters, such as concerns around offensive programming, are dealt with by the Canadian Broadcast Standards Council (CBSC), an independent broadcast industry association, rather than by the CRTC, although CBSC decisions can be appealed to the CRTC if necessary. However, the CRTC is also sometimes erroneously criticized for CBSC decisions — for example, the CRTC was erroneously criticized for the CBSC's decisions pertaining to the airing of Howard Stern's terrestrial radio show in Canada in the late 1990s, as well as the CBSC's controversial ruling on the Dire Straits song \"Money for Nothing\".\n\nThe commission is not fully equivalent to the U.S. Federal Communications Commission, which has additional powers over technical matters, in broadcasting and other aspects of communications, in that country. In Canada, Innovation, Science and Economic Development Canada (formerly Industry Canada) is responsible for allocating frequencies and call signs, managing the broadcast spectrum, and regulating other technical issues such as interference with electronics equipment.\n\nThe CRTC has in the past regulated the prices cable television broadcast distributors are allowed to charge. In most major markets, however, prices are no longer regulated due to increased competition for broadcast distribution from satellite television.\n\nThe CRTC also regulates which channels broadcast distributors must or may offer. Per the Broadcasting Act the commission also gives priority to Canadian signals—many non-Canadian channels which compete with Canadian channels are thus not approved for distribution in Canada. The CRTC argues that allowing free trade in television stations would overwhelm the smaller Canadian market, preventing it from upholding its responsibility to foster a national conversation. Some people, however, consider this tantamount to censorship.\n\nThe CRTC's simultaneous substitution rules require that when a Canadian network licences a television show from a US network and shows it in the same time slot, upon request by the Canadian broadcaster, Canadian broadcast distributors must replace the show on the US channel with the broadcast of the Canadian channel, along with any overlays and commercials. As \"Grey's Anatomy\" is on ABC, but is carried in Canada on CTV at the same time, for instance, the cable, satellite, or other broadcast distributor must send the CTV feed over the signal of the carried ABC affiliate, even where the ABC version is somehow different, particularly commercials. (These rules are not intended to apply in case of differing \"episodes\" of the same series; this difference may not always be communicated to distributors, although this is rather rare.) Viewers via home antenna who receive both American and Canadian networks on their personal sets are not affected by sim-sub.\n\nThe goal of this policy is to create a market in which Canadian networks can realize revenue through advertising sales in spite of their inability to match the rates that the much larger American networks can afford to pay for syndicated programming. This policy is also why Canadian viewers do not see American advertisements during the Super Bowl, even when tuning into one of the many American networks carried on Canadian televisions.\n\nIn a major May 1999 decision on \"New Media\", the CRTC held that under the Broadcasting Act the CRTC had jurisdiction over certain content communicated over the Internet including audio and video, but excluding content that is primarily alphanumeric such as emails and most webpages. It also issued an exemption order committing to a policy of non-interference.\n\nIn May 2011, in response to the increase presence of Over-the-Top (OTT) programming, the CRTC put a call out to the public to provide input on the impact OTT programming is having on Canadian content and existing broadcasting subscriptions through satellite and cable. On October 5, 2011 the CRTC released their findings that included consultations with stakeholders from the telecommunications industry, media producers, and cultural leaders among others. The evidence was inconclusive, suggesting that an increased availability of OTT options is not having a negative impact on the availability or diversity of Canadian content, one of the key policy mandates of the CRTC, nor are there signs that there has been a significant decline of televisions subscriptions through cable or satellite. However, given the rapid progress in the industry they are working on a more in depth study to be concluded in May 2012.\n\nThe CRTC does not \"directly\" regulate rates, quality of service issues, or business practices for Internet service providers. However, the CRTC does continually monitor the sector and associated trends.\n\nThird Party ISP Access refers to a ruling forcing Cable operators (MSO) to offer Internet access to third party resellers.\n\nThe commission currently has some jurisdiction over the provision of local landline telephone service in Canada. This is largely limited to the major incumbent carriers, such as Bell Canada and Telus, for traditional landline service (but not Voice over Internet Protocol (VoIP)). It has begun the gradual deregulation of such services where, in the commission's opinion, a sufficient level of competition exists.\n\nThe CRTC is sometimes blamed for the current state of the mobile phone industry in Canada, in which there are only three national mobile network operators – Bell Mobility, Telus Mobility, and Rogers Wireless – as well as a handful of MVNOs operating on these networks. In fact, the commission has very little to do with the regulation of mobile phone service, outside of \"undue preference\" issues (for example, a carrier offering a superior rate or service to some subscribers and not others without a good reason). It does not regulate service rates, service quality, or other business practices, and commission approval is not necessary for wireless provider sales or mergers as in the broadcasting industry. Moreover, it does not deal with the availability of spectrum for mobile phone service, which is part of the Industry Canada mandate, nor the maintenance of competition, which is largely the responsibility of The Competition Bureau.\n\nAny transfer of more than 30% of the ownership of a broadcasting licence (including cable/satellite distribution licences) requires advance approval of the commission. One condition normally taken into account in such a decision is the level of foreign ownership; federal regulations require that Canadian citizens ultimately own a majority of a broadcast license. Usually this takes the form of a public process, where interested parties can express their concerns and sometimes including a public hearing, followed by a commission decision.\n\nWhile landline and mobile telephone providers must also be majority-owned by Canadians under the federal Telecommunications Act, the CRTC is not responsible for enforcement of this provision. In fact, the commission does not require licences at all for telephone companies, and CRTC approval is therefore not generally required for the sale of a telephone company, unless said company also owns a broadcast licence.\n\nSince 1987, the CRTC has been involved in several controversial decisions:\n\nWhile an exact number has not been determined, thousands of Canadians have purchased and used what they contend to be grey market radio and television services, licensed in the United States but not in Canada. Users of these unlicensed services contend that they are not directly breaking any laws by simply using the equipment. The equipment is usually purchased from an American supplier (although some merchants have attempted to set up shop in Canada) and the services are billed to an American postal address. The advent of online billing and the easy availability of credit card services has made it relatively easy for almost anyone to maintain an account in good standing, regardless of where they actually live.\n\nSec. 9(1)(c) of the Radiocommunication Act creates a prohibition against all decoding of encrypted programming signals, followed by an exception where authorization is received from the person holding the lawful right in Canada to transmit and authorize decoding of the signal. This means receiving the encrypted programming of DishNetwork or DirecTV, even with a grey market subscription, may be construed as unlawful (this remains an unresolved Constitutional issue).\n\nNotwithstanding, possession of DishNetwork or DirecTV equipment is not unlawful as provided by The Radiocommuncation Act Section 4(1)(b), which states:\n\n\"No person shall, except under and in accordance with a radio authorization, install, operate or possess radio apparatus, other than (b)a radio apparatus that is capable only of the reception of broadcasting and that is not a distribution undertaking. (radio apparatus\" means a device or combination of devices intended for, or capable of being used for, radiocommunication).\"\n\nSatellite radio poses a more complicated problem for the CRTC. While an unlicensed satellite dish can often be identified easily, satellite radio receivers are much more compact and can rarely be easily identified, at least not without flagrantly violating provisions against unreasonable search and seizure in the Canadian Charter of Rights and Freedoms. Some observers argued that this influenced the CRTC's June 2005 decision to ease Canadian content restrictions on satellite radio (see above).\n\nThe CRTC is run by up to 13 full-time (including the chairman, the vice-chairman of broadcasting, and the vice-chairman of telecommunications) appointed by the Cabinet for renewable terms of up to five years. In June 2012, Jean-Pierre Blais was appointed Chairman for a five-year term.\n\nThe CRTC Interconnection Steering Committee (CISC) assists in developing information, procedures and guidelines for the CRTC's regulatory activities.\n\n\n\n\n"}
{"id": "5986", "url": "https://en.wikipedia.org/wiki?curid=5986", "title": "Con", "text": "Con\n\nCon may refer to:\n\n\nCON may refer to:\n\n\n"}
{"id": "5987", "url": "https://en.wikipedia.org/wiki?curid=5987", "title": "Coal", "text": "Coal\n\nCoal is a combustible black or brownish-black sedimentary rock usually occurring in rock strata in layers or veins called coal beds or coal seams. The harder forms, such as anthracite coal, can be regarded as metamorphic rock because of later exposure to elevated temperature and pressure. Coal is composed primarily of carbon, along with variable quantities of other elements, chiefly hydrogen, sulfur, oxygen, and nitrogen. A fossil fuel, coal forms when dead plant matter is converted into peat, which in turn is converted into lignite, then sub-bituminous coal, after that bituminous coal, and lastly anthracite. This involves biological and geological processes that take place over time.\n\nThroughout human history, coal has been used as an energy resource, primarily burned for the production of electricity and heat, and is also used for industrial purposes, such as refining metals. Coal is the largest source of energy for the generation of electricity worldwide, as well as one of the largest worldwide anthropogenic sources of carbon dioxide releases. The extraction of coal, its use in energy production and its byproducts are all associated with environmental and health effects including climate change.\n\nCoal is extracted from the ground by coal mining. Since 1983, the world's top coal producer has been China. In 2015 China produced 3,747 million tonnes of coal – 48% of 7,861 million tonnes world coal production. In 2015 other large producers were United States (813 million tonnes), India (678), European Union (539) and Australia (503). In 2010 the largest exporters were Australia with 328 million tonnes (27% of world coal export) and Indonesia with 316 million tonnes (26%), while the largest importers were Japan with 207 million tonnes (18% of world coal import), China with 195 million tonnes (17%) and South Korea with 126 million tonnes (11%).\n\nThe word originally took the form \"col\" in Old English, from Proto-Germanic *\"kula\"(\"n\"), which in turn is hypothesized to come from the Proto-Indo-European root *\"g\"(\"e\")\"u-lo-\" \"live coal\". Germanic cognates include the Old Frisian \"kole\", Middle Dutch \"cole\", Dutch \"kool\", Old High German \"chol\", German \"Kohle\" and Old Norse \"kol\", and the Irish word \"gual\" is also a cognate via the Indo-European root. In Old Turkic languages, \"kül\" is \"ash(es), cinders\", \"öčür\" is \"quench\". The compound \"charcoal\" in Turkic is \"öčür(ülmüş) kül\", literally \"quenched ashes, cinders, coals\" with elided anlaut \"ö-\" and inflection affixes \"-ülmüş\".\n\nAt various times in the geologic past, the Earth had dense forests in low-lying wetland areas. Due to natural processes such as flooding, these forests were buried underneath soil. As more and more soil deposited over them, they were compressed. The temperature also rose as they sank deeper and deeper. As the process continued the plant matter was protected from biodegradation and oxidation, usually by mud or acidic water. This trapped the carbon in immense peat bogs that were eventually covered and deeply buried by sediments. Under high pressure and high temperature, dead vegetation was slowly converted to coal. As coal contains mainly carbon, the conversion of dead vegetation into coal is called carbonization.\n\nThe wide, shallow seas of the Carboniferous Period provided ideal conditions for coal formation, although coal is known from most geological periods. The exception is the coal gap in the Permian–Triassic extinction event, where coal is rare. Coal is known from Precambrian strata, which predate land plants—this coal is presumed to have originated from residues of algae.\n\nAs geological processes apply pressure to dead biotic material over time, under suitable conditions, its metamorphic grade increases successively into:\n\nThe classification of coal is generally based on the content of volatiles. However, the exact classification varies between countries. According to the German classification, coal is classified as follows:\nThe middle six grades in the table represent a progressive transition from the English-language sub-bituminous to bituminous coal, while the last class is an approximate equivalent to anthracite, but more inclusive (US anthracite has <6% volatiles).\n\nCannel coal (sometimes called \"candle coal\") is a variety of fine-grained, high-rank coal with significant hydrogen content. It consists primarily of \"exinite\" macerals, now termed \"liptinite\".\n\nHilt's law is a geological term that states that, in a small area, the deeper the coal, the higher its rank (grade). The law holds true if the thermal gradient is entirely vertical, but metamorphism may cause lateral changes of rank, irrespective of depth.\n\nThe earliest recognized use is from the Shenyang area of China 4000 BC where Neolithic inhabitants had begun carving ornaments from black lignite. Coal from the Fushun mine in northeastern China was used to smelt copper as early as 1000 BC. Marco Polo, the Italian who traveled to China in the 13th century, described coal as \"black stones ... which burn like logs\", and said coal was so plentiful, people could take three hot baths a week. In Europe, the earliest reference to the use of coal as fuel is from the geological treatise \"On stones\" (Lap. 16) by the Greek scientist Theophrastus (\"circa\" 371–287 BC):\n\nOutcrop coal was used in Britain during the Bronze Age (3000–2000 BC), where it has been detected as forming part of the composition of funeral pyres. In Roman Britain, with the exception of two modern fields, \"the Romans were exploiting coals in all the major coalfields in England and Wales by the end of the second century AD\". Evidence of trade in coal (dated to about AD 200) has been found at the Roman settlement at Heronbridge, near Chester, and in the Fenlands of East Anglia, where coal from the Midlands was transported via the Car Dyke for use in drying grain. Coal cinders have been found in the hearths of villas and Roman forts, particularly in Northumberland, dated to around AD 400. In the west of England, contemporary writers described the wonder of a permanent brazier of coal on the altar of Minerva at Aquae Sulis (modern day Bath), although in fact easily accessible surface coal from what became the Somerset coalfield was in common use in quite lowly dwellings locally. Evidence of coal's use for iron-working in the city during the Roman period has been found. In Eschweiler, Rhineland, deposits of bituminous coal were used by the Romans for the smelting of iron ore.\n\nNo evidence exists of the product being of great importance in Britain before the High Middle Ages, after about AD 1000. Mineral coal came to be referred to as \"seacoal\" in the 13th century; the wharf where the material arrived in London was known as Seacoal Lane, so identified in a charter of King Henry III granted in 1253. Initially, the name was given because much coal was found on the shore, having fallen from the exposed coal seams on cliffs above or washed out of underwater coal outcrops, but by the time of Henry VIII, it was understood to derive from the way it was carried to London by sea. In 1257–1259, coal from Newcastle upon Tyne was shipped to London for the smiths and lime-burners building Westminster Abbey. Seacoal Lane and Newcastle Lane, where coal was unloaded at wharves along the River Fleet, are still in existence. (See Industrial processes below for modern uses of the term.)\n\nThese easily accessible sources had largely become exhausted (or could not meet the growing demand) by the 13th century, when underground extraction by shaft mining or adits was developed. The alternative name was \"pitcoal\", because it came from mines. The development of the Industrial Revolution led to the large-scale use of coal, as the steam engine took over from the water wheel. In 1700, five-sixths of the world's coal was mined in Britain. Britain would have run out of suitable sites for watermills by the 1830s if coal had not been available as a source of energy. In 1947, there were some 750,000 miners in Britain, but by 2004, this had shrunk to some 5,000 miners working in around 20 collieries.\n\nCoal is primarily used as a solid fuel to produce electricity and heat through combustion. According to the EIA, world coal consumption is projected to increase from 2012 to 2040 at an average rate of 0.6%/year, from 153 quadrillion Btu (1 Quad are 36,000,000 tonnes of coal) in 2012 to 169 quadrillion Btu in 2020, and to 180 quadrillion Btu in 2040. Efforts around the world to reduce the use of coal has led some regions to switch to natural gas.\n\nChina produced 3.47 billion tonnes (3.83 billion short tons) in 2011. India produced about 578 million tonnes (637.1 million short tons) in 2011. 69% of China's electricity comes from coal. The US consumed about 13% of the world total in 2010, i.e. 951 million tonnes (1.05 billion short tons), using 93% of it for generation of electricity. 46% of total power generated in the US was using coal. The United States Energy Information Administration estimates coal reserves at short tons (860 Gt). One estimate for resources is 18,000 Gt.\n\nWhen coal is used for electricity generation, it is usually pulverized and then burned in a furnace with a boiler. The furnace heat converts boiler water to steam, which is then used to spin turbines which turn generators and create electricity. The thermodynamic efficiency of this process has been improved over time; some older coal-fired power stations have thermal efficiencies in the vicinity of 25% whereas the newest supercritical and \"ultra-supercritical\" steam cycle turbines, operating at temperatures over 600 °C and pressures over 27 MPa (over 3900 psi), can achieve thermal efficiencies in excess of 45% (LHV basis) using anthracite fuel, or around 43% (LHV basis) even when using lower-grade lignite fuel. Further thermal efficiency improvements are also achievable by improved pre-drying (especially relevant with high-moisture fuel such as lignite or biomass) and cooling technologies.\n\nAn alternative approach of using coal for electricity generation with improved efficiency is the integrated gasification combined cycle (IGCC) power plant. Instead of pulverizing the coal and burning it directly as fuel in the steam-generating boiler, the coal is gasified (see coal gasification) to create syngas, which is burned in a gas turbine to produce electricity (just like natural gas is burned in a turbine). Hot exhaust gases from the turbine are used to raise steam in a heat recovery steam generator which powers a supplemental steam turbine. Thermal efficiencies of current IGCC power plants range from 39% to 42% (HHV basis) or ≈42–45% (LHV basis) for bituminous coal and assuming utilization of mainstream gasification technologies (Shell, GE Gasifier, CB&I). IGCC power plants outperform conventional pulverized coal-fueled plants in terms of pollutant emissions, and allow for relatively easy carbon capture.\n\nAt least 40% of the world's electricity comes from coal, and in 2016, 30% of the United States' electricity came from coal, down from approximately 49% in 2008. As of 2012 in the United States, use of coal to generate electricity was declining, as plentiful supplies of natural gas obtained by hydraulic fracturing of tight shale formations became available at low prices.\n\nIn Denmark, a net electric efficiency of >47% has been obtained at the coal-fired Nordjyllandsværket CHP Plant and an overall plant efficiency of up to 91% with cogeneration of electricity and district heating. The multifuel-fired Avedøreværket CHP Plant just outside Copenhagen can achieve a net electric efficiency as high as 49%. The overall plant efficiency with cogeneration of electricity and district heating can reach as much as 94%.\n\nAn alternative form of coal combustion is as coal-water slurry fuel (CWS), which was developed in the Soviet Union. Other ways to use coal are combined heat and power cogeneration and an MHD topping cycle.\n\nThe total known deposits recoverable by current technologies, including highly polluting, low-energy content types of coal (i.e., lignite, bituminous), is sufficient for many years. Consumption is increasing and maximal production could be reached within decades (see world coal reserves, below). On the other hand, much may have to be left in the ground to avoid climate change.\n\nWorldwide natural gas generated power has increased from 740 TW in 1973 to 5140 TW in 2014, generating 22% of the worlds total electricity, approximately half as much as generated with coal. In addition to generating electricity, natural gas is also popular in some countries for heating and as an automotive fuel.\nThe use of coal in the United Kingdom declined as a result of the development of North Sea oil and the subsequent Dash for Gas from the 1990s to 2000.\n\nIn Canada some coal power plants such as the Hearn Generating Station have stopped burning coal by switching the plant to natural gas.\n\nIn the United States, 27 gigawatts of capacity from coal-fired generators was slated to be retired from 175 US coal-fired power plants between 2012 and 2016. Natural gas showed a corresponding jump, increasing by a third over 2011. Coal's share of US electricity generation dropped to just over 36%. Due to emergence of shale gas, coal consumption declined from 2009. Natural gas accounted for 81% of new power generation in the US between 2000 and 2010. Coal-fired generation puts out about twice the amount of carbon dioxide—around 2,000 pounds for every megawatt hour generated—than electricity generated by burning natural gas at 1,100 pounds of greenhouse gas per megawatt hour. As the fuel mix in the United States has changed to reduce coal and increase natural gas generation, carbon dioxide emissions have unexpectedly fallen. Those measured in the first quarter of 2012 were the lowest of any recorded for the first quarter of any year since 1992.\n\nWhen compared to coal, burning natural gas for power or heat results in half the carbon dioxide emissions. Fracking issues may exist when producing the gas, and if as little as 3 percent of the gas produced escapes, you might as well be burning coal, from a climate perspective. Natural gas production and distribution leak methane into the atmosphere which may be 25 times more potent a greenhouse gas than carbon dioxide. It is often used to balance the intermittent nature of solar and wind energy when hydroelectricity is not available.\n\nCoke is a solid carbonaceous residue derived from coking coal (a low-ash, low-sulfur bituminous coal, also known as metallurgical coal), which is used in manufacturing steel and other iron products. Coke is made from coking coal by baking in an oven without oxygen at temperatures as high as 1,000 °C (1,832 °F), driving off the volatile constituents and fusing together the fixed carbon and residual ash. Metallurgical coke is used as a fuel and as a reducing agent in smelting iron ore in a blast furnace. The result is pig iron, and is too rich in dissolved carbon, so it must be treated further to make steel.\n\nCoking coal should be low in ash, sulfur, and phosphorus, so that these do not migrate to the metal. Based on the ash percentage, the coking coal can be divided into various grades. These grades are:\nThe coke must be strong enough to resist the weight of overburden in the blast furnace, which is why coking coal is so important in making steel using the conventional route. However, the alternative route is direct reduced iron, where any carbonaceous fuel can be used to make sponge or pelletised iron. Coke from coal is grey, hard, and porous and has a heating value of 24.8 million Btu/ton (29.6 MJ/kg). Some cokemaking processes produce valuable byproducts, including coal tar, ammonia, light oils, and coal gas.\n\nPetroleum coke is the solid residue obtained in oil refining, which resembles coke, but contains too many impurities to be useful in metallurgical applications.\n\nCoal gasification can be used to produce syngas, a mixture of carbon monoxide (CO) and hydrogen (H) gas. Often syngas is used to fire gas turbines to produce electricity, but the versatility of syngas also allows it to be converted into transportation fuels, such as gasoline and diesel, through the Fischer-Tropsch process; alternatively, syngas can be converted into methanol, which can be blended into fuel directly or converted to gasoline via the methanol to gasoline process. Gasification combined with Fischer-Tropsch technology is currently used by the Sasol chemical company of South Africa to make motor vehicle fuels from coal and natural gas. Alternatively, the hydrogen obtained from gasification can be used for various purposes, such as powering a hydrogen economy, making ammonia, or upgrading fossil fuels.\n\nDuring gasification, the coal is mixed with oxygen and steam while also being heated and pressurized. During the reaction, oxygen and water molecules oxidize the coal into carbon monoxide (CO), while also releasing hydrogen gas (H). This process has been conducted in both underground coal mines and in the production of town gas which was piped to customers to burn for illumination, heating, and cooking.\n\nIf the refiner wants to produce gasoline, the syngas is collected at this state and routed into a Fischer-Tropsch reaction. If hydrogen is the desired end-product, however, the syngas is fed into the water gas shift reaction, where more hydrogen is liberated.\n\nCoal can also be converted into synthetic fuels equivalent to gasoline or diesel by several different direct processes (which do not intrinsically require gasification or indirect conversion). In the direct liquefaction processes, the coal is either hydrogenated or carbonized. Hydrogenation processes are the Bergius process, the SRC-I and SRC-II (Solvent Refined Coal) processes, the NUS Corporation hydrogenation process and several other single-stage and two-stage processes. In the process of low-temperature carbonization, coal is coked at temperatures between 360 and 750 °C (680 and 1,380 °F). These temperatures optimize the production of coal tars richer in lighter hydrocarbons than normal coal tar. The coal tar is then further processed into fuels. An overview of coal liquefaction and its future potential is available.\n\nCoal liquefaction methods involve carbon dioxide () emissions in the conversion process. If coal liquefaction is done without employing either carbon capture and storage (CCS) technologies or biomass blending, the result is lifecycle greenhouse gas footprints that are generally greater than those released in the extraction and refinement of liquid fuel production from crude oil. If CCS technologies are employed, reductions of 5–12% can be achieved in Coal to Liquid (CTL) plants and up to a 75% reduction is achievable when co-gasifying coal with commercially demonstrated levels of biomass (30% biomass by weight) in coal/biomass-to-liquids plants. For future synthetic fuel projects, carbon dioxide sequestration is proposed to avoid releasing into the atmosphere. Sequestration adds to the cost of production.\n\nRefined coal is the product of a coal-upgrading technology that removes moisture and certain pollutants from lower-rank coals such as sub-bituminous and lignite (brown) coals. It is one form of several precombustion treatments and processes for coal that alter coal's characteristics before it is burned. The goals of precombustion coal technologies are to increase efficiency and reduce emissions when the coal is burned. Depending on the situation, precombustion technology can be used in place of or as a supplement to postcombustion technologies to control emissions from coal-fueled boilers.\n\nFinely ground bituminous coal, known in this application as sea coal, is a constituent of foundry sand. While the molten metal is in the mould, the coal burns slowly, releasing reducing gases at pressure, and so preventing the metal from penetrating the pores of the sand. It is also contained in 'mould wash', a paste or liquid with the same function applied to the mould before casting. Sea coal can be mixed with the clay lining (the \"bod\") used for the bottom of a cupola furnace. When heated, the coal decomposes and the bod becomes slightly friable, easing the process of breaking open holes for tapping the molten metal.\n\nCoal is an important feedstock in production of a wide range of chemical fertilizers and other chemical products. The main route to these products is coal gasification to produce syngas. Primary chemicals that are produced directly from the syngas include methanol, hydrogen and carbon monoxide, which are the chemical building blocks from which a whole spectrum of derivative chemicals are manufactured, including olefins, acetic acid, formaldehyde, ammonia, urea and others. The versatility of syngas as a precursor to primary chemicals and high-value derivative products provides the option of using relatively inexpensive coal to produce a wide range of valuable commodities.\n\nHistorically, production of chemicals from coal has been used since the 1950s and has become established in the market. According to the 2010 Worldwide Gasification Database, a survey of current and planned gasifiers, from 2004 to 2007 chemical production increased its gasification product share from 37% to 45%. From 2008 to 2010, 22% of new gasifier additions were to be for chemical production.\n\nBecause the slate of chemical products that can be made via coal gasification can in general also use feedstocks derived from natural gas and petroleum, the chemical industry tends to use whatever feedstocks are most cost-effective. Therefore, interest in using coal tends to increase for higher oil and natural gas prices and during periods of high global economic growth that may strain oil and gas production. Also, production of chemicals from coal is of much higher interest in countries like South Africa, China, India and the United States where there are abundant coal resources. The abundance of coal combined with lack of natural gas resources in China is strong inducement for the coal to chemicals industry pursued there. In the United States, the best example of the industry is Eastman Chemical Company which has been successfully operating a coal-to-chemicals plant at its Kingsport, Tennessee, site since 1983. Similarly, Sasol has built and operated coal-to-chemicals facilities in South Africa.\n\nCoal to chemical processes do require substantial quantities of water. As of 2013 much of the coal to chemical production was in the People's Republic of China where environmental regulation and water management was weak.\n\nIn North America, Central Appalachian coal futures contracts are currently traded on the New York Mercantile Exchange (trading symbol \"QL\"). The trading unit is per contract, and is quoted in U.S. dollars and cents per ton. Since coal is the principal fuel for generating electricity in the United States, coal futures contracts provide coal producers and the electric power industry an important tool for hedging and risk management.\n\nIn addition to the NYMEX contract, the Intercontinental Exchange (ICE) has European (Rotterdam) and South African (Richards Bay) coal futures available for trading. The trading unit for these contracts is , and are also quoted in U.S. dollars and cents per ton.\n\nThe price of coal increased from around $30.00 per short ton in 2000 to around $150.00 per short ton as of September 2008. As of October 2008, the price per short ton had declined to $111.50. Prices further declined to $71.25 as of October 2010. In early 2015, it was trading near $56/ton.\n\nThe use of coal as fuel causes adverse health impacts and deaths.\n\nThe deadly London smog was caused primarily by the heavy use of coal. In the United States coal-fired power plants were estimated in 2004 to cause nearly 24,000 premature deaths every year, including 2,800 from lung cancer. Annual health costs in Europe from use of coal to generate electricity are €42.8 billion, or $55 billion. Yet the disease and mortality burden of coal use today falls most heavily upon China.\n\nBreathing in coal dust causes coalworker's pneumoconiosis which is known colloquially as \"black lung\", so-called because the coal dust literally turns the lungs black from their usual pink color. In the United States alone, it is estimated that 1,500 former employees of the coal industry die every year from the effects of breathing in coal mine dust.\n\nAround 10% of coal is ash, Coal ash is hazardous and toxic to human beings and other living things. Coal ash contains the radioactive elements uranium and thorium. Coal ash and other solid combustion byproducts are stored locally and escape in various ways that expose those living near coal plants to radiation and environmental toxics.\n\nHuge amounts of coal ash and other waste is produced annually. In 2013, the US alone consumed on the order of 983 million short tonnes of coal per year. Use of coal on this scale generates hundreds of millions of tons of ash and other waste products every year. These include fly ash, bottom ash, and flue-gas desulfurization sludge, that contain mercury, uranium, thorium, arsenic, and other heavy metals, along with non-metals such as selenium.\n\nThe American Lung Association, the American Nurses' Association, and the Physicians for Social Responsibility released a report in 2009 which details in depth the detrimental impact of the coal industry on human health, including workers in the mines and individuals living in communities near plants burning coal as a power source. This report provides medical information regarding damage to the lungs, heart, and nervous system of Americans caused by the burning of coal as fuel. It details how the air pollution caused by the plume of coal smokestack emissions is a cause of asthma, strokes, reduced intelligence, artery blockages, heart attacks, congestive heart failure, cardiac arrhythmias, mercury poisoning, arterial occlusion, and lung cancer.\n\nMore recently, the Chicago School of Public Health released a largely similar report, echoing many of the same findings.\n\nThough coal burning has increasingly been supplanted by less-toxic natural gas use in recent years, a 2010 study by the Clean Air Task Force still estimated that \"air pollution from coal-fired power plants accounts for more than 13,000 premature deaths, 20,000 heart attacks, and 1.6 million lost workdays in the U.S. each year.\" The total monetary cost of these health impacts is over $100 billion annually.\n\nA 2017 study in the \"Economic Journal\" found that for Britain during the period 1851–1860, \"a one standard deviation increase in coal use raised infant mortality by 6–8% and that industrial coal use explains roughly one-third of the urban mortality penalty observed during this period.\"\n\nCoal mining and coal fueling of power station and industrial processes can cause major environmental damage.\n\nWater systems are affected by coal mining. For example, mining affects groundwater and water table levels and acidity. Spills of fly ash, such as the Kingston Fossil Plant coal fly ash slurry spill, can also contaminate land and waterways, and destroy homes. Power stations that burn coal also consume large quantities of water. This can affect the flows of rivers, and has consequential impacts on other land uses.\n\nOne of the earliest known impacts of coal on the water cycle was acid rain. Approximately 75 Tg/S per year of sulfur dioxide (SO) is released from burning coal. After release, the sulfur dioxide is oxidized to gaseous HSO which scatters solar radiation, hence its increase in the atmosphere exerts a cooling effect on climate. This beneficially masks some of the warming caused by increased greenhouse gases. However, the sulfur is precipitated out of the atmosphere as acid rain in a matter of weeks, whereas carbon dioxide remains in the atmosphere for hundreds of years. Release of SO also contributes to the widespread acidification of ecosystems.\n\nDisused coal mines can also cause issues. Subsidence can occur above tunnels, causing damage to infrastructure or cropland. Coal mining can also cause long lasting fires, and it has been estimated that thousands of coal seam fires are burning at any given time. For example, there is a coal seam fire in Germany that has been burning since 1668, and is still burning in the 21st century.\n\nSome environmental impacts are modest, such as dust nuisance. However, perhaps the largest and most long term effect of coal use is the release of carbon dioxide, a greenhouse gas that causes climate change and global warming, according to the IPCC and the EPA. Coal is the largest contributor to the human-made increase of CO in the atmosphere.\n\nThe production of coke from coal produces ammonia, coal tar, and gaseous compounds as by-products which if discharged to land, air or waterways can act as environmental pollutants. The Whyalla steelworks is one example of a coke producing facility where liquid ammonia is discharged to the marine environment.\n\nIn 1999, world gross carbon dioxide emissions from coal usage were 8,666 million tonnes of carbon dioxide. In 2011, world gross emissions from coal usage were 14,416 million tonnes. For every megawatt-hour generated, coal-fired electric power generation emits around 2,000 pounds of carbon dioxide, which is almost double the approximately 1100 pounds of carbon dioxide released by a natural gas-fired electric plant. Because of this higher carbon efficiency of natural gas generation, as the market in the United States has changed to reduce coal and increase natural gas generation, carbon dioxide emissions may have fallen. Those measured in the first quarter of 2012 were the lowest of any recorded for the first quarter of any year since 1992. In 2013, the head of the UN climate agency advised that most of the world's coal reserves should be left in the ground to avoid catastrophic global warming.\n\n\"Clean\" coal technology is a collection of technologies being developed to mitigate the environmental impact of coal energy generation. Those technologies are being developed to remove or reduce pollutant emissions to the atmosphere. Some of the techniques that would be used to accomplish this include chemically washing minerals and impurities from the coal, gasification (see also IGCC), improved technology for treating flue gases to remove pollutants to increasingly stringent levels and at higher efficiency, carbon capture and storage technologies to capture the carbon dioxide from the flue gas and dewatering lower rank coals (brown coals) to improve the calorific value, and thus the efficiency of the conversion into electricity. Figures from the United States Environmental Protection Agency show that these technologies have made today's coal-based generating fleet 77 percent cleaner on the basis of regulated emissions per unit of energy produced.\n\nClean coal technology usually addresses atmospheric problems resulting from burning coal. Historically, the primary focus was on SO and NO, the most important gases in causation of acid rain, and particulates which cause visible air pollution and deleterious effects on human health. More recent focus has been on carbon dioxide (due to its impact on global warming) and concern over toxic species such as mercury. Concerns exist regarding the economic viability of these technologies and the timeframe of delivery, potentially high hidden economic costs in terms of social and environmental damage, and the costs and viability of disposing of removed carbon and other toxic matter.\n\nSeveral different technological methods are available for the purpose of carbon capture as demanded by the clean coal concept:\nThe Kemper County IGCC Project, a 582 MW coal gasification-based power plant, will use pre-combustion capture of CO to capture 65% of the CO the plant produces, which will be utilized/geologically sequestered in enhanced oil recovery operations. If the technology used at the Kemper Project is successful, it will be the United States’ first clean coal plant.[\n\nThe Saskatchewan Government's Boundary Dam Power Station Integrated Carbon Capture and Sequestration Demonstration Project will use post-combustion, amine-based scrubber technology to capture 90% of the CO emitted by Unit 3 of the power plant; this CO will be pipelined to and utilized for enhanced oil recovery in the Weyburn oil fields. However, only about a half of this CO2 will actually be permanently stored, the remainder is released into the atmosphere during capturing, and the processing in the oil field.\n\nAn early example of a coal-based plant using (oxy-fuel) carbon-capture technology is Swedish company Vattenfall's Schwarze Pumpe power station located in Spremberg, Germany, built by German firm Siemens, which went on-line in September 2008. The facility captures CO and acid rain producing pollutants, separates them, and compresses the CO into a liquid. Plans are to inject the CO into depleted natural gas fields or other geological formations. Vattenfall opines that this technology is considered not to be a final solution for CO reduction in the atmosphere, but provides an achievable solution in the near term while more desirable alternative solutions to power generation can be made economically practical. In 2014 research and development were discontinued due to high costs making the technology unviable.\n\nThe white rot fungus Trametes versicolor can grow on and metabolize naturally occurring coal. The bacteria Diplococcus has been found to degrade coal, raising its temperature.\n\nCoal (by liquefaction technology) is one of the backstop resources that could limit escalation of oil prices and mitigate the effects of transportation energy shortage that will occur under peak oil. This is contingent on liquefaction production capacity becoming large enough to satiate the very large and growing demand for petroleum. Estimates of the cost of producing liquid fuels from coal suggest that domestic U.S. production of fuel from coal becomes cost-competitive with oil priced at around $35 per barrel, with the $35 being the break-even cost. With oil prices as low as around $40 per barrel in the U.S. as of December 2008, liquid coal lost some of its economic allure in the U.S., but will probably be re-vitalized, similar to oil sand projects, with an oil price around $70 per barrel.\n\nIn China, due to an increasing need for liquid energy in the transportation sector, coal liquefaction projects were given high priority even during periods of oil prices below $40 per barrel. This is probably because China prefers not to be dependent on foreign oil, instead utilizing its enormous domestic coal reserves. As oil prices were increasing during the first half of 2009, the coal liquefaction projects in China were again boosted, and these projects are profitable with an oil barrel price of $40.\n\nChina is the largest producer of coal in the world. It is the world's largest energy consumer, and relies on coal to supply 69% of its energy needs. An estimated 5 million people worked in China's coal-mining industry in 2007.\n\nCoal pollution costs the EU €43 billion each year. Measures to cut air pollution may have beneficial long-term economic impacts for individuals.\n\nThe energy density of coal, i.e. its heating value, is roughly 24 megajoules per kilogram (approximately 6.7 kilowatt-hours per kg). For a coal power plant with a 40% efficiency, it takes an estimated of coal to power a 100 W lightbulb for one year.\n\nAs of 2006, the average efficiency of electricity-generating power stations was 31%; in 2002, coal represented about 23% of total global energy supply, an equivalent of 3.4 billion tonnes of coal, of which 2.8 billion tonnes were used for electricity generation.\n\nThe US Energy Information Agency's 1999 report on CO emissions for energy generation quotes an emission factor of 0.963 kg CO/kWh for coal power, compared to 0.881 kg CO/kWh (oil), or 0.569 kg CO/kWh (natural gas).\n\nThousands of coal fires are burning around the world. Those burning underground can be difficult to locate and many cannot be extinguished. Fires can cause the ground above to subside, their combustion gases are dangerous to life, and breaking out to the surface can initiate surface wildfires. Coal seams can be set on fire by spontaneous combustion or contact with a mine fire or surface fire. Lightning strikes are an important source of ignition. The coal continues to burn slowly back into the seam until oxygen (air) can no longer reach the flame front. A grass fire in a coal area can set dozens of coal seams on fire. Coal fires in China burn an estimated 120 million tons of coal a year, emitting 360 million metric tons of CO, amounting to 2–3% of the annual worldwide production of CO from fossil fuels. In Centralia, Pennsylvania (a borough located in the Coal Region of the United States), an exposed vein of anthracite ignited in 1962 due to a trash fire in the borough landfill, located in an abandoned anthracite strip mine pit. Attempts to extinguish the fire were unsuccessful, and it continues to burn underground to this day. The Australian Burning Mountain was originally believed to be a volcano, but the smoke and ash come from a coal fire that has been burning for some 6,000 years.\n\nAt Kuh i Malik in Yagnob Valley, Tajikistan, coal deposits have been burning for thousands of years, creating vast underground labyrinths full of unique minerals, some of them very beautiful. Local people once used this method to mine ammoniac. This place has been well-known since the time of Herodotus, but European geographers misinterpreted the Ancient Greek descriptions as the evidence of active volcanism in Turkestan (up to the 19th century, when the Russian army invaded the area).\n\nThe reddish siltstone rock that caps many ridges and buttes in the Powder River Basin in Wyoming and in western North Dakota is called \"porcelanite\", which resembles the coal burning waste \"clinker\" or volcanic \"scoria\". Clinker is rock that has been fused by the natural burning of coal. In the Powder River Basin approximately 27 to 54 billion tons of coal burned within the past three million years. Wild coal fires in the area were reported by the Lewis and Clark Expedition as well as explorers and settlers in the area.\n\nThe 948 billion short tons of recoverable coal reserves estimated by the Energy Information Administration are equal to about 4,196 BBOE (billion barrels of oil equivalent). The amount of coal burned during 2007 was estimated at 7.075 billion short tons, or 133.179 quadrillion BTUs. This is an average of 18.8 million BTU per short ton. In terms of heat content, this is about of oil equivalent per day. By comparison in 2007, natural gas provided of oil equivalent per day, while oil provided per day.\n\nBritish Petroleum, in its 2007 report, estimated at 2006 end that there were 147 years reserves-to-production ratio based on \"proven\" coal reserves worldwide. This figure only includes reserves classified as \"proven\"; exploration drilling programs by mining companies, particularly in under-explored areas, are continually providing new reserves. In many cases, companies are aware of coal deposits that have not been sufficiently drilled to qualify as \"proven\". However, some nations haven't updated their information and assume reserves remain at the same levels even with withdrawals.\n\nOf the three fossil fuels, coal has the most widely distributed reserves; coal is mined in over 100 countries, and on all continents except Antarctica. The largest reserves are found in the United States, Russia, China, Australia and India. Note the table below.\n\nThe reserve life is an estimate based only on current production levels and proved reserves level for the countries shown, and makes no assumptions of future production or even current production trends. Countries with annual production higher than 100 million tonnes are shown. For comparison, data for the European Union is also shown. Shares are based on data expressed in tonnes oil equivalent.\nCountries with annual consumption higher than 100 million tonnes are shown. For comparison, data for the European Union is also shown. Shares are based on data expressed in tonnes oil equivalent.\n\nCountries with annual gross export higher than 10 million tonnes are shown. In terms of net export the largest exporters are still Australia (328.1 millions tonnes), Indonesia (316.2) and Russia (100.2).\nCountries with annual gross import higher than 20 million tonnes are shown. In terms of net import the largest importers are still Japan (206.0 millions tonnes), China (172.4) and South Korea (125.8).\n\nCoal is the official state mineral of Kentucky. and the official state rock of Utah; both U.S. states have a historic link to coal mining.\n\nSome cultures hold that children who misbehave will receive only a lump of coal from Santa Claus for Christmas in their christmas stockings instead of presents.\n\nIt is also customary and considered lucky in Scotland and the North of England to give coal as a gift on New Year's Day. This occurs as part of First-Footing and represents warmth for the year to come.\n\n\n"}
{"id": "5992", "url": "https://en.wikipedia.org/wiki?curid=5992", "title": "Traditional Chinese medicine", "text": "Traditional Chinese medicine\n\nTraditional Chinese medicine (TCM; ) is a style of traditional medicine informed by modern medicine but built on a foundation of more than 2,500 years of Chinese medical practice that includes various forms of herbal medicine, acupuncture, massage (tui na), exercise (qigong), and dietary therapy. It is primarily used as a complementary alternative medicine approach. TCM is widely used in China and is becoming increasingly available in Europe and North America.\n\nOne of the basic tenets of TCM \"holds that the body's vital energy (\"chi\" or \"qi\") circulates through channels, called \"meridians,\" that have branches connected to bodily organs and functions.\" Concepts of the body and of disease used in TCM reflect its ancient origins and its emphasis on dynamic processes over material structure, similar to European humoral theory. Scientific investigation has not found histological or physiological evidence for traditional Chinese concepts such as \"qi\", meridians, and acupuncture points. The TCM theory and practice are not based upon scientific knowledge, and there is disagreement between TCM practitioners on what diagnosis and treatments should be used for any given patient. The effectiveness of Chinese herbal medicine remains poorly researched and documented. There are concerns over a number of potentially toxic plants, animal parts, and mineral Chinese medicinals. There are also concerns over illegal trade and transport of endangered species including rhinoceroses and tigers, and the welfare of specially farmed animals including bears. A review of cost-effectiveness research for TCM found that studies had low levels of evidence, but so far have not shown benefit outcomes. Pharmaceutical research has explored the potential for creating new drugs from traditional remedies, with few successful results. A \"Nature\" editorial described TCM as \"fraught with pseudoscience\", and said that the most obvious reason why it hasn't delivered many cures is that the majority of its treatments have no logical mechanism of action. Proponents propose that research has so far missed key features of the art of TCM, such as unknown interactions between various ingredients and complex interactive biological systems.\n\nThe doctrines of Chinese medicine are rooted in books such as the \"Yellow Emperor's Inner Canon\" and the \"Treatise on Cold Damage\", as well as in cosmological notions such as yin-yang and the five phases. Starting in the 1950s, these precepts were standardized in the People's Republic of China, including attempts to integrate them with modern notions of anatomy and pathology. In the 1950s, the Chinese government promoted a systematized form of TCM.\n\nTCM's view of the body places little emphasis on anatomical structures, but is mainly concerned with the identification of functional entities (which regulate digestion, breathing, aging etc.). While health is perceived as the harmonious interaction of these entities and the outside world, disease is interpreted as a disharmony in interaction. TCM diagnosis aims to trace symptoms to patterns of an underlying disharmony, by measuring the pulse, inspecting the tongue, skin, and eyes, and looking at the eating and sleeping habits of the person as well as many other things.\n\nTraces of therapeutic activities in China date from the Shang dynasty (14th–11th centuries BC). Though the Shang did not have a concept of \"medicine\" as distinct from other fields, their oracular inscriptions on bones and tortoise shells refer to illnesses that affected the Shang royal family: eye disorders, toothaches, bloated abdomen, etc., which Shang elites usually attributed to curses sent by their ancestors. There is no evidence that the Shang nobility used herbal remedies. According to a 2006 overview, the \"Documentation of Chinese materia medica (CMM) dates back to around 1,100 BC when only dozens of drugs were first described. By the end of the 16th century, the number of drugs documented had reached close to 1,900. And by the end of the last century, published records of CMM had reached 12,800 drugs.\"\n\nStone and bone needles found in ancient tombs led Joseph Needham to speculate that acupuncture might have been carried out in the Shang dynasty. This being said, most historians now make a distinction between medical lancing (or bloodletting) and acupuncture in the narrower sense of using metal needles to treat illnesses by stimulating specific points along circulation channels (\"meridians\") in accordance with theories related to the circulation of Qi. The earliest evidence for acupuncture in this sense dates to the second or first century BC.\n\nThe \"Yellow Emperor's Inner Canon\", the oldest received work of Chinese medical theory, was compiled around the first century BC on the basis of shorter texts from different medical lineages. Written in the form of dialogues between the legendary Yellow Emperor and his ministers, it offers explanations on the relation between humans, their environment, and the cosmos, on the contents of the body, on human vitality and pathology, on the symptoms of illness, and on how to make diagnostic and therapeutic decisions in light of all these factors. Unlike earlier texts like \"Recipes for Fifty-Two Ailments\", which was excavated in the 1970s from a tomb that had been sealed in 168 BC, the \"Inner Canon\" rejected the influence of spirits and the use of magic. It was also one of the first books in which the cosmological doctrines of Yinyang and the Five Phases were brought to a mature synthesis.\n\nThe \"Treatise on Cold Damage Disorders and Miscellaneous Illnesses\" was collated by Zhang Zhongjing sometime between 196 and 220 CE; at the end of the Han dynasty. Focusing on drug prescriptions rather than acupuncture, it was the first medical work to combine Yinyang and the Five Phases with drug therapy. This formulary was also the earliest public Chinese medical text to group symptoms into clinically useful \"patterns\" (\"zheng\" 證) that could serve as targets for therapy. Having gone through numerous changes over time, the formulary now circulates as two distinct books: the \"Treatise on Cold Damage Disorders\" and the \"Essential Prescriptions of the Golden Casket\", which were edited separately in the eleventh century, under the Song dynasty.\n\nIn the centuries that followed the completion of the \"Yellow Emperor's Inner Canon\", several shorter books tried to summarize or systematize its contents. The \"Canon of Problems\" (probably second century CE) tried to reconcile divergent doctrines from the \"Inner Canon\" and developed a complete medical system centered on needling therapy. The \"AB Canon of Acupuncture and Moxibustion\" (\"Zhenjiu jiayi jing\" 針灸甲乙經, compiled by Huangfu Mi sometime between 256 and 282 CE) assembled a consistent body of doctrines concerning acupuncture; whereas the \"Canon of the Pulse\" (\"Maijing\" 脈經; ca. 280) presented itself as a \"comprehensive handbook of diagnostics and therapy.\"\n\nIn 1950, Chairman Mao Zedong made a speech in support of traditional Chinese medicine (TCM) which was influenced by political necessity. Zedong believed he and the Chinese Communist Party should promote TCM but he did not personally believe in TCM and he did not use it. In 1952, the president of the Chinese Medical Association said that, \"This One Medicine, will possess a basis in modern natural sciences, will have absorbed the ancient and the new, the Chinese and the foreign, all medical achievements—and will be China’s New Medicine!\"\nIndian medicine penetrated into the Chinese world between the 4th and 8th centuries. Ayurveda greatly influenced traditional Chinese medicine during its formation Accupunture may have origin in ancient India Indian medical knowledge of internal medicine, surgery, obstetrics, gynecology, pediatrics, ophthalmology, Otorhinolaryngology and dentistry was brought in China. Kashyapa Samhita was translated into Chinese during the Middle Ages. Kashyapa Samhita specially deals with pediatrics, gynecology, and obstetrics Another Indian medical work Kumara Tantra of Ravana, which mainly deals with paediatric diseases, was translated into Chinese. According to book of sui and Book of Tang eleven Indian medical works were translated into Chinese. Indian monks introduced surgery in China. Before the arrival of Buddhism surgical techniques were unknown within China Indian monks and translators themselves had a good understanding of medicine. An Shigao translated an Indian medical work into Chinese which dealt with 404 diseases Yijing (monk) went to India and brought back some 400 Buddhist translated texts which includes many medical works like Arsaprasamanasutra (A classic on curing all hemorrhoid-related diseases).Yijing highlight India's superior medical knowledge, he praised the practise of Fasting among Indians, which they believed could cure imbalances of body within a matter of days. In China he Introduced hygiene practised in India. Formulae for lung diseases were imported from India during the Tang dynasty. Indian ophthalmologists also practiced medicine in China. Liu Yuxi wrote a poem about Indian Brahman who was an expert in removing cataracts with a golden needle.Influence of Buddhists four element theory is clearly seen in Tao Hongjing writings. Indian medicine has a profound influence on Physician Sun Simiao's medical work. In his work, he attributes many formulae to Jivaka Komarabhacca. Sun Simiao mention many Indian surgical techniques for treatment of cataracts, glaucoma and other eye diseases Wang Tao also incorporated Indian ideas of medicine Ishinpō of Tanba Yasuyori records over ninety articles attributed to Indian physician jivaka\n\nThese include Zhang Zhongjing, Hua Tuo, Sun Simiao, Tao Hongjing, Zhang Jiegu, and Li Shizhen.\n\nTraditional Chinese medicine (TCM) is a broad range of medicine practices sharing common concepts which have been developed in China and are based on a tradition of more than 2,000 years, including various forms of herbal medicine, acupuncture, massage (Tui na), exercise (qigong), and dietary therapy. It is primarily used as a complementary alternative medicine approach. TCM is widely used in China and it is also used in the West. Its philosophy is based on Yinyangism (i.e., the combination of Five Phases theory with Yin-yang theory), which was later absorbed by Daoism.\nYin and yang are ancient Chinese concepts which can be traced back to the Shang dynasty (1600–1100 BC). They represent two abstract and complementary aspects that every phenomenon in the universe can be divided into. Primordial analogies for these aspects are the sun-facing (yang) and the shady (yin) side of a hill. Two other commonly used representational allegories of yin and yang are water and fire. In the ying-yang theory, detailed attributions are made regarding the yin or yang character of things:\n\nThe concept of yin and yang is also applicable to the human body; for example, the upper part of the body and the back are assigned to yang, while the lower part of the body are believed to have the yin character. Yin and yang characterization also extends to the various body functions, and – more importantly – to disease symptoms (e.g., cold and heat sensations are assumed to be yin and yang symptoms, respectively). Thus, yin and yang of the body are seen as phenomena whose lack (or over-abundance) comes with characteristic symptom combinations:\n\n\nTCM also identifies drugs believed to treat these specific symptom combinations, i.e., to reinforce yin and yang.\n\nFive Phases (五行, ), sometimes also translated as the \"Five Elements\" theory, presumes that all phenomena of the universe and nature can be broken down into five elemental qualities – represented by wood (木, ), fire (火), earth (土, ), metal (金, ), and water (水, ). In this way, lines of correspondence can be drawn:\n\nStrict rules are identified to apply to the relationships between the Five Phases in terms of sequence, of acting on each other, of counteraction, etc. All these aspects of Five Phases theory constitute the basis of the zàng-fǔ concept, and thus have great influence regarding the TCM model of the body. Five Phase theory is also applied in diagnosis and therapy.\n\nCorrespondences between the body and the universe have historically not only been seen in terms of the Five Elements, but also of the \"Great Numbers\" (大數, ) For example, the number of acu-points has at times been seen to be 365, corresponding with the number of days in a year; and the number of main meridians–12–has been seen as corresponding with the number of rivers flowing through the ancient Chinese empire.\n\nTCM \"holds that the body's vital energy (\"chi\" or \"qi\") circulates through channels, called \"meridians\", that have branches connected to bodily organs and functions.\" Its view of the human body is only marginally concerned with anatomical structures, but focuses primarily on the body's \"functions\" (such as digestion, breathing, temperature maintenance, etc.):\n\nThese functions are aggregated and then associated with a primary functional entity – for instance, nourishment of the tissues and maintenance of their moisture are seen as connected functions, and the entity postulated to be responsible for these functions is xuě (blood). These functional entities thus constitute \"concepts\" rather than something with biochemical or anatomical properties.\n\nThe primary functional entities used by traditional Chinese medicine are qì, xuě, the five zàng organs, the six fǔ organs, and the meridians which extend through the organ systems. These are all theoretically interconnected: each zàng organ is paired with a fǔ organ, which are nourished by the blood and concentrate qi for a particular function, with meridians being extensions of those functional systems throughout the body.\n\nConcepts of the body and of disease used in TCM have notions of a pre-scientific culture, similar to European humoral theory. – TCM is characterized as full of pseudoscience. Some practitioners no longer consider yin and yang and the idea of an energy flow to apply. Scientific investigation has not found any histological or physiological evidence for traditional Chinese concepts such as \"qi\", meridians, and acupuncture points. It is a generally held belief within the acupuncture community that acupuncture points and meridians structures are special conduits for electrical signals but no research has established any consistent anatomical structure or function for either acupuncture points or meridians. The scientific evidence for the anatomical existence of either meridians or acupuncture points is not compelling. Stephen Barrett of Quackwatch writes that, \"TCM theory and practice are not based upon the body of knowledge related to health, disease, and health care that has been widely accepted by the scientific community. TCM practitioners disagree among themselves about how to diagnose patients and which treatments should go with which diagnoses. Even if they could agree, the TCM theories are so nebulous that no amount of scientific study will enable TCM to offer rational care.\"\n\nTCM has been the subject of controversy within China. In 2006, the Chinese scholar Zhang Gongyao triggered a national debate when he published an article entitled \"Farewell to Traditional Chinese Medicine,\" arguing that TCM was a pseudoscience that should be abolished in public healthcare and academia. The Chinese government however, interested in the opportunity of export revenues, took the stance that TCM is a science and continued to encourage its development.\n\nTCM distinguishes many kinds of qi (). In a general sense, qi is something that is defined by five \"cardinal functions\":\nVacuity of qi will be characterized especially by pale complexion, lassitude of spirit, lack of strength, spontaneous sweating, laziness to speak, non-digestion of food, shortness of breath (especially on exertion), and a pale and enlarged tongue.\n\nQi is believed to be partially generated from food and drink, and partially from air (by breathing). Another considerable part of it is inherited from the parents and will be consumed in the course of life.\n\nTCM uses special terms for qi running inside of the blood vessels and for qi that is distributed in the skin, muscles, and tissues between those. The former is called yíng-qì (); its function is to complement xuè and its nature has a strong yin aspect (although qi in general is considered to be yang). The latter is called weì-qì (); its main function is defence and it has pronounced yang nature.\n\nQi is said to circulate in the meridians. Just as the qi held by each of the zang-fu organs, this is considered to be part of the 'principal' qi () of the body (also called 真氣 , ‘’true‘’ qi, or 原氣 , ‘’original‘’ qi).\n\nIn contrast to the majority of other functional entities, xuè (血, \"blood\") is correlated with a physical form – the red liquid running in the blood vessels. Its concept is, nevertheless, defined by its functions: nourishing all parts and tissues of the body, safeguarding an adequate degree of moisture, and sustaining and soothing both consciousness and sleep.\n\nTypical symptoms of a lack of xuě (usually termed \"blood vacuity\" [血虚, ]) are described as: Pale-white or withered-yellow complexion, dizziness, flowery vision, palpitations, insomnia, numbness of the extremities; pale tongue; \"fine\" pulse.\n\nClosely related to xuě are the jīnyė (津液, usually translated as \"body fluids\"), and just like xuě they are considered to be yin in nature, and defined first and foremost by the functions of nurturing and moisturizing the different structures of the body. Their other functions are to harmonize yin and yang, and to help with the secretion of waste products.\n\nJīnyė are ultimately extracted from food and drink, and constitute the raw material for the production of xuě; conversely, xuě can also be transformed into jīnyė. Their palpable manifestations are all bodily fluids: tears, sputum, saliva, gastric acid, joint fluid, sweat, urine, etc.\n\nThe zàng-fǔ () constitute the centre piece of TCM's systematization of bodily functions. Bearing the names of organs, they are, however, only secondarily tied to (rudimentary) anatomical assumptions (the fǔ a little more, the zàng much less). As they are primarily defined by their functions, they are not equivalent to the anatomical organs–to highlight this fact, their names are usually capitalized.\n\nThe term zàng (臟) refers to the five entities considered to be yin in nature–Heart, Liver, Spleen, Lung, Kidney–, while fǔ (腑) refers to the six yang organs–Small Intestine, Large Intestine, Gallbladder, Urinary Bladder, Stomach and Sānjiaō.\n\nThe zàng's essential functions consist in production and storage of qì and xuě; they are said to regulate digestion, breathing, water metabolism, the musculoskeletal system, the skin, the sense organs, aging, emotional processes, and mental activity, among other structures and processes. The fǔ organs' main purpose is merely to transmit and digest (傳化, ) substances such as waste and food.\n\nSince their concept was developed on the basis of Wǔ Xíng philosophy, each zàng is paired with a fǔ, and each zàng-fǔ pair is assigned to one of five elemental qualities (i.e., the Five Elements or Five Phases). These correspondences are stipulated as:\n\nThe zàng-fǔ are also connected to the twelve standard meridians–each yang meridian is attached to a fǔ organ, and five of the yin meridians are attached to a zàng. As there are only five zàng but six yin meridians, the sixth is assigned to the Pericardium, a peculiar entity almost similar to the Heart zàng.\n\nThe meridians (经络, ) are believed to be channels running from the zàng-fǔ in the interior (里, ) of the body to the limbs and joints (\"the surface\" [表, ]), transporting qi and xuĕ. TCM identifies 12 \"regular\" and 8 \"extraordinary\" meridians; the Chinese terms being 十二经脉 (, lit. \"the Twelve Vessels\") and 奇经八脉 () respectively. There's also a number of less customary channels branching from the \"regular\" meridians.\n\nIn general, disease is perceived as a disharmony (or imbalance) in the functions or interactions of yin, yang, qi, xuĕ, zàng-fǔ, meridians etc. and/or of the interaction between the human body and the environment. Therapy is based on which \"pattern of disharmony\" can be identified. Thus, \"pattern discrimination\" is the most important step in TCM diagnosis. It is also known to be the most difficult aspect of practicing TCM.\n\nIn order to determine which pattern is at hand, practitioners will examine things like the color and shape of the tongue, the relative strength of pulse-points, the smell of the breath, the quality of breathing or the sound of the voice. For example, depending on tongue and pulse conditions, a TCM practitioner might diagnose bleeding from the mouth and nose as: \"Liver fire rushes upwards and scorches the Lung, injuring the blood vessels and giving rise to reckless pouring of blood from the mouth and nose.\" He might then go on to prescribe treatments designed to clear heat or supplement the Lung.\n\nIn TCM, a disease has two aspects: \"bìng\" and \"zhèng\". The former is often translated as \"disease entity\", \"disease category\", \"illness\", or simply \"diagnosis\". The latter, and more important one, is usually translated as \"pattern\" (or sometimes also as \"syndrome\"). For example, the disease entity of a common cold might present with a pattern of wind-cold in one person, and with the pattern of wind-heat in another.\n\nFrom a scientific point of view, most of the disease entitites (病, ) listed by TCM constitute mere symptoms. Examples include headache, cough, abdominal pain, constipation etc.\n\nSince therapy will not be chosen according to the disease entity but according to the pattern, two people with the same disease entity but different patterns will receive different therapy. Vice versa, people with similar patterns might receive similar therapy even if their disease entities are different. This is called 异病同治，同病异治 (,\"different diseases, same treatment; same disease, different treatments\").\n\nIn TCM, \"pattern\" (证, ) refers to a \"pattern of disharmony\" or \"functional disturbance\" within the functional entities the TCM model of the body is composed of. There are disharmony patterns of qi, xuě, the body fluids, the zàng-fǔ, and the meridians. They are ultimately defined by their symptoms and \"signs\" (i.e., for example, pulse and tongue findings).\n\nIn clinical practice, the identified pattern usually involves a combination of affected entities (compare with typical examples of patterns). The concrete pattern identified should account for \"all\" the symptoms a person has.\n\nThe Six Excesses (六淫, , sometimes also translated as \"Pathogenic Factors\", or \"Six Pernicious Influences\"; with the alternative term of 六邪, , – \"Six Evils\" or \"Six Devils\") are allegorical terms used to describe disharmony patterns displaying certain typical symptoms. These symptoms resemble the effects of six climatic factors. In the allegory, these symptoms can occur because one or more of those climatic factors (called 六气, , \"the six qi\") were able to invade the body surface and to proceed to the interior. This is sometimes used to draw causal relationships (i.e., prior exposure to wind/cold/etc. is identified as the cause of a disease), while other authors explicitly deny a direct cause-effect relationship between weather conditions and disease, pointing out that the Six Excesses are primarily descriptions of a certain combination of symptoms translated into a pattern of disharmony. It is undisputed, though, that the Six Excesses can manifest inside the body without an external cause. In this case, they might be denoted \"internal\", e.g., \"internal wind\" or \"internal fire (or heat)\".\n\nThe Six Excesses and their characteristic clinical signs are:\nSix-Excesses-patterns can consist of only one or a combination of Excesses (e.g., wind-cold, wind-damp-heat). They can also transform from one into another.\n\nFor each of the functional entities (qi, xuĕ, zàng-fǔ, meridians etc.), typical disharmony patterns are recognized; for example: qi vacuity and qi stagnation in the case of qi; blood vacuity, blood stasis, and blood heat in the case of xuĕ; Spleen qi vacuity, Spleen yang vacuity, Spleen qi vacuity with down-bearing qi, Spleen qi vacuity with lack of blood containment, cold-damp invasion of the Spleen, damp-heat invasion of Spleen and Stomach in case of the Spleen zàng; wind/cold/damp invasion in the case of the meridians.\n\nTCM gives detailed prescriptions of these patterns regarding their typical symptoms, mostly including characteristic tongue and/or pulse findings. For example:\n\nThe process of determining which actual pattern is on hand is called 辩证 (, usually translated as \"pattern diagnosis\", \"pattern identification\" or \"pattern discrimination\"). Generally, the first and most important step in pattern diagnosis is an evaluation of the present signs and symptoms on the basis of the \"Eight Principles\" (八纲, ).\nThese eight principles refer to four pairs of fundamental qualities of a disease: exterior/interior, heat/cold, vacuity/repletion, and yin/yang. Out of these, heat/cold and vacuity/repletion have the biggest clinical importance. The yin/yang quality, on the other side, has the smallest importance and is somewhat seen aside from the other three pairs, since it merely presents a general and vague conclusion regarding what other qualities are found. In detail, the Eight Principles refer to the following:\n\nAfter the fundamental nature of a disease in terms of the Eight Principles is determined, the investigation focuses on more specific aspects. By evaluating the present signs and symptoms against the background of typical disharmony patterns of the various entities, evidence is collected whether or how specific entities are affected. This evaluation can be done\n\nThere are also three special pattern diagnosis systems used in case of febrile and infectious diseases only (\"Six Channel system\" or \"six division pattern\" [六经辩证, ]; \"Wei Qi Ying Xue system\" or \"four division pattern\" [卫气营血辩证, ]; \"San Jiao system\" or \"three burners pattern\" [三角辩证, ]).\n\nAlthough TCM and its concept of disease do not strongly differentiate between cause and effect, pattern discrimination can include considerations regarding the disease cause; this is called 病因辩证 (, \"disease-cause pattern discrimination\").\n\nThere are three fundamental categories of disease causes (三因, ) recognized:\n\nIn TCM, there are five diagnostic methods: inspection, auscultation, olfaction, inquiry, and palpation.\n\n\nExamination of the tongue and the pulse are among the principal diagnostic methods in TCM. Certain sectors of the tongue's surface are believed to correspond to the zàng-fŭ. For example, teeth marks on one part of the tongue might indicate a problem with the Heart, while teeth marks on another part of the tongue might indicate a problem with the Liver.\n\nPulse palpation involves measuring the pulse both at a superficial and at a deep level at three different locations on the radial artery (\"Cun, Guan, Chi\", located two fingerbreadths from the wrist crease, one fingerbreadth from the wrist crease, and right at the wrist crease, respectively, usually palpated with the index, middle and ring finger) of each arm, for a total of twelve pulses, all of which are thought to correspond with certain zàng-fŭ. The pulse is examined for several characteristics including rhythm, strength and volume, and described with qualities like \"floating, slippery, bolstering-like, feeble, thready and quick\"; each of these qualities indicate certain disease patterns. Learning TCM pulse diagnosis can take several years.\n\nThe term \"herbal medicine\" is somewhat misleading in that, while plant elements are by far the most commonly used substances in TCM, other, non-botanic substances are used as well: animal, human, and mineral products are also utilized. Thus, the term \"medicinal\" (instead of herb) is usually preferred.\n\nTypically, one batch of medicinals is prepared as a decoction of about 9 to 18 substances. Some of these are considered as main herbs, some as ancillary herbs; within the ancillary herbs, up to three categories can be distinguished.\n\nThere are roughly 13,000 medicinals used in China and over 100,000 medicinal recipes recorded in the ancient literature. Plant elements and extracts are by far the most common elements used. In the classic \"Handbook of Traditional Drugs\" from 1941, 517 drugs were listed – out of these, 45 were animal parts, and 30 were minerals.\n\nSome animal parts used as medicinals can be considered rather strange such as cows' gallstones, hornet's nest, leech, and scorpion. Other examples of animal parts include horn of the antelope or buffalo, deer antlers, testicles and penis bone of the dog, and snake bile. Some TCM textbooks still recommend preparations containing animal tissues, but there has been little research to justify the claimed clinical efficacy of many TCM animal products.\n\nSome medicinals can include the parts of endangered species, including tiger bones and rhinoceros horn\nwhich is used for many ailments (though not as an aphrodisiac as is commonly misunderstood by the West).\nThe black market in rhinoceros horn (driven not just by TCM but also unrelated status-seeking) has reduced the world's rhino population by more than 90 percent over the past 40 years.\n\nConcerns have also arisen over the use of pangolin scales, turtle plastron, seahorses, and the gill plates of mobula and manta rays. Poachers hunt restricted or endangered species animals to supply the black market with TCM products. There is no scientific evidence of efficacy for tiger medicines. Concern over China considering to legalize the trade in tiger parts prompted the 171-nation Convention on International Trade in Endangered Species (CITES) to endorse a decision opposing the resurgence of trade in tigers. Fewer than 30,000 saiga antelopes remain, which are exported to China for use in traditional fever therapies. Organized gangs illegally export the horn of the antelopes to China. The pressures on seahorses (Hippocampus spp.) used in traditional medicine is enormous; tens of millions of animals are unsustainably caught annually. Many species of syngnathid are currently part of the IUCN Red List of Threatened Species or national equivalents.\n\nSince TCM recognizes bear bile as a medicinal, more than 12,000 asiatic black bears are held in bear farms. The bile is extracted through a permanent hole in the abdomen leading to the gall bladder, which can cause severe pain. This can lead to bears trying to kill themselves. As of 2012, approximately 10,000 bears are farmed in China for their bile. This practice has spurred public outcry across the country. The bile is collected from live bears via a surgical procedure. The deer penis is believed to have therapeutic benefits according to traditional Chinese medicine. It is typically very big and, proponents believe, in order to preserve its properties, it should be extracted from a living deer. Medicinal tiger parts from poached animals include tiger penis, believed to improve virility, and tiger eyes. The illegal trade for tiger parts in China has driven the species to near-extinction because of its popularity in traditional medicine. Laws protecting even critically endangered species such as the Sumatran tiger fail to stop the display and sale of these items in open markets. Shark fin soup is traditionally regarded in Chinese medicine as beneficial for health in East Asia, and its status as an elite dish has led to huge demand with the increase of affluence in China, devastating shark populations. The shark fins have been a part of traditional Chinese medicine for centuries. Shark finning is banned in many countries, but the trade is thriving in Hong Kong and China, where the fins are part of shark fin soup, a dish considered a delicacy, and used in some types of traditional Chinese medicine.\n\nThe tortoise (Freshwater turtle - guiban) and the turtle (Chinese softshell turtle - biejia) species used in traditional Chinese medicine are raised on farms, while restrictions are made on the accumulation and export of other endangered species. However, issues concerning the overexploitation of Asian turtles in China have not been completely solved. Australian scientists have developed methods to identify medicines containing DNA traces of endangered species. Finally, although not an endangered species, sharp rises in exports of donkeys and donkey hide from Africa to China to make the traditional remedy ejiao have prompted export restrictions by some African countries.\n\nTraditional Chinese Medicine also includes some human parts: the classic Materia medica (Bencao Gangmu) describes the use of 35 human body parts and excreta in medicines, including bones, fingernail, hairs, dandruff, earwax, impurities on the teeth, feces, urine, sweat, organs, but most are no longer in use.\n\nHuman placenta has been used an ingredient in certain traditional Chinese medicines, including using dried human placenta, known as \"Ziheche\", to treat infertility, impotence and other conditions. The consumption of the human placenta is a potential source of infection.\n\nThe traditional categorizations and classifications that can still be found today are:\n\nThe classification according to the Four Natures (四气, ): hot, warm, cool, or cold (or, neutral in terms of temperature) and hot and warm herbs are used to treat cold diseases, while cool and cold herbs are used to treat heat diseases.\n\nThe classification according to the Five Flavors, (五味, , sometimes also translated as Five Tastes): acrid, sweet, bitter, sour, and salty. Substances may also have more than one flavor, or none (i.e., a \"bland\" flavor). Each of the Five Flavors corresponds to one of zàng organs, which in turn corresponds to one of the Five Phases. A flavor implies certain properties and therapeutic actions of a substance; e.g., saltiness drains downward and softens hard masses, while sweetness is supplementing, harmonizing, and moistening.\n\nThe classification according to the meridian – more precise, the zàng-organ including its associated meridian – which can be expected to be primarily affected by a given medicinal.\n\nThe categorization according to the specific function mainly include: exterior-releasing or exterior-resolving, heat-clearing, downward-draining, or precipitating wind-damp-dispelling, dampness-transforming, promoting the movement of water and percolating dampness or dampness-percolating, interior-warming, qi-regulating or qi-rectifying, dispersing food accumulation or food-dispersing, worm-expelling, stopping bleeding or blood-stanching, quickening the Blood and dispelling stasis or blood-quickening, transforming phlegm, stopping coughing and calming wheezing or phlegm-transforming and cough- and panting-suppressing, Spirit-quieting, calming the liver and expelling wind or liver-calming and wind-extinguishingl orifice-openingl supplementing which includes qi-supplementing, blood-nourishing, yin-enriching, and yang-fortifying, astriction-promoting or securing and astringing, vomiting-inducing, and substances for external application.\n\n there were not enough good-quality trials of herbal therapies to allow their effectiveness to be determined. A high percentage of relevant studies on traditional Chinese medicine are in Chinese databases. Fifty percent of systematic reviews on TCM did not search Chinese databases, which could lead to a bias in the results. Many systematic reviews of TCM interventions published in Chinese journals are incomplete, some contained errors or were misleading. The herbs recommended by traditional Chinese practitioners in the US are unregulated.\n\nA 2013 review found the data too weak to support use of Chinese herbal medicine (CHM) for benign prostatic hyperplasia. A 2013 review found the research on the benefit and safety of CHM for idiopathic sudden sensorineural hearing loss is of poor quality and cannot be relied upon to support their use. A 2013 Cochrane review found inconclusive evidence that CHM reduces the severity of eczema. The traditional medicine ginger, which has shown anti-inflammatory properties in laboratory experiments, has been used to treat rheumatism, headache and digestive and respiratory issues, though there is no firm evidence supporting these uses. A 2012 Cochrane review found no difference in decreased mortality when Chinese herbs were used alongside Western medicine versus Western medicine exclusively. A 2012 Cochrane review found insufficient evidence to support the use of TCM for people with adhesive small bowel obstruction. A 2011 review found low quality evidence that suggests CHM improves the symptoms of Sjogren's syndrome. A 2010 review found TCM seems to be effective for the treatment of fibromyalgia but the findings were of insufficient methodological rigor. A 2009 Cochrane review found insufficient evidence to recommend the use of TCM for the treatment of epilepsy. A 2008 Cochrane review found promising evidence for the use of Chinese herbal medicine in relieving painful menstruation, but the trials assessed were of such low methodological quality that no conclusion could be drawn about the remedies' suitability as a recommendable treatment option. Turmeric has been used in traditional Chinese medicine for centuries to treat various conditions. This includes jaundice and hepatic disorders, rheumatism, anorexia, diabetic wounds, and menstrual complications. Most of its effects have been attributed to curcumin. Research that curcumin shows strong anti-inflammatory and antioxidant activities have instigated mechanism of action studies on the possibility for cancer and inflammatory diseases prevention and treatment. It also exhibits immunomodulatory effects. A 2005 Cochrane review found insufficient evidence for the use of CHM in HIV-infected people and people with AIDS. A 2010 Cochrane review found insufficient evidence to support the use of Traditional Chinese Herbal Products (THCP) in the treatment of angina.\n\nWith an eye to the enormous Chinese market, pharmaceutical companies have explored the potential for creating new drugs from traditional remedies. A \"Nature\" editorial described TCM as \"fraught with pseudoscience\", and stated that having \"no rational mechanism of action for most of its therapies\" is the \"most obvious answer\" to why its study didn't provide a \"flood of cures\", while advocates responded that \"researchers are missing aspects of the art, notably the interactions between different ingredients in traditional therapies.\"\n\nOne of the few successes was the development in the 1970s of the antimalarial drug artemisinin, which is a processed extract of \"Artemisia annua\", a herb traditionally used as a fever treatment. Researcher Tu Youyou discovered that a low-temperature extraction process could isolate an effective antimalarial substance from the plant. She says she was influenced by a traditional source saying that this herb should be steeped in cold water, after initially finding high-temperature extraction unsatisfactory. The extracted substance, once subject to detoxification and purification processes, is a usable antimalarial drug – a 2012 review found that artemisinin-based remedies were the most effective drugs for the treatment of malaria. For her work on malaria, Tu received the 2015 Nobel Prize in Physiology or Medicine. Despite global efforts in combating malaria, it remains a large burden for the population. Although WHO recommends artemisinin-based remedies for treating uncomplicated malaria, artemisinin resistance can no longer be ignored.\n\nAlso in the 1970s Chinese researcher Zhang TingDong and colleagues investigated the potential use of the traditionally used substance arsenic trioxide to treat acute promyelocytic leukemia (APL). Building on his work, research both in China and the West eventually led to the development of the drug Trisenox, which was approved for leukemia treatment by the FDA in 2000.\n\nHuperzine A, which is extracted from traditional herb \"Huperzia serrata\", has attracted the interest of medical science because of alleged neuroprotective properties. Despite earlier promising results, a 2013 systematic review and meta-analysis found \"Huperzine A appears to have beneficial effects on improvement of cognitive function, daily living activity, and global clinical assessment in participants with Alzheimer’s disease. However, the findings should be interpreted with caution due to the poor methodological quality of the included trials.\"\n\nEphedrine in its natural form, known as \"má huáng\" (麻黄) in traditional Chinese medicine, has been documented in China since the Han dynasty (206 BC – 220 AD) as an antiasthmatic and stimulant. In 1885, the chemical synthesis of ephedrine was first accomplished by Japanese organic chemist Nagai Nagayoshi based on his research on Japanese and Chinese traditional herbal medicines\n\nA 2012 systematic review found there is a lack of available cost-effectiveness evidence in TCM.\n\nFrom the earliest records regarding the use of medicinals to today, the toxicity of certain substances has been described in all Chinese materiae medicae. Since TCM has become more popular in the Western world, there are increasing concerns about the potential toxicity of many traditional Chinese medicinals including plants, animal parts and minerals. Traditional Chinese herbal remedies are conveniently available from grocery stores in most Chinese neighborhoods; some of these items may contain toxic ingredients, are imported into the U.S. illegally, and are associated with claims of therapeutic benefit without evidence. For most medicinals, efficacy and toxicity testing are based on traditional knowledge rather than laboratory analysis. The toxicity in some cases could be confirmed by modern research (i.e., in scorpion); in some cases it couldn't (i.e., in \"Curculigo\"). Traditional herbal medicines can contain extremely toxic chemicals and heavy metals, and naturally occurring toxins, which can cause illness, exacerbate pre-existing poor health or result in death. Botanical misidentification of plants can cause toxic reactions in humans. The description on some plants used in traditional Chinese medicine have changed, leading to unintended intoxication of the wrong plants. A concern is also contaminated herbal medicines with microorganisms and fungal toxins, including aflatoxin. Traditional herbal medicines are sometimes contaminated with toxic heavy metals, including lead, arsenic, mercury and cadmium, which inflict serious health risks to consumers. Also, adulteration of some herbal medicine preparations with conventional drugs which may cause serious adverse effects, such as corticosteroids, phenylbutazone, phenytoin, and glibenclamide, has been reported.\n\nSubstances known to be potentially dangerous include \"Aconitum\", secretions from the Asiatic toad, powdered centipede, the Chinese beetle (\"Mylabris phalerata\"), certain fungi, \"Aristolochia\", \"Aconitum\", Arsenic sulfide (Realgar), mercury sulfide, and cinnabar. Asbestos ore (Actinolite, Yang Qi Shi, 阳起石) is used to treat impotence in TCM. Due to galena's (litharge, lead(II) oxide) high lead content, it is known to be toxic. Lead, mercury, arsenic, copper, cadmium, and thallium have been detected in TCM products sold in the U.S. and China.\n\nTo avoid its toxic adverse effects \"Xanthium sibiricum\" must be processed. Hepatotoxicity has been reported with products containing \"Polygonum multiflorum\", glycyrrhizin, \"Senecio\" and \"Symphytum\". The herbs indicated as being hepatotoxic included \"Dictamnus dasycarpus\", \"Astragalus membranaceous\", and \"Paeonia lactiflora\". Contrary to popular belief, \"Ganoderma lucidum\" mushroom extract, as an adjuvant for cancer immunotherapy, appears to have the potential for toxicity. A 2013 review suggested that although the antimalarial herb \"Artemisia annua\" may not cause hepatotoxicity, haematotoxicity, or hyperlipidemia, it should be used cautiously during pregnancy due to a potential risk of embryotoxicity at a high dose.\n\nHowever, many adverse reactions are due to misuse or abuse of Chinese medicine. For example, the misuse of the dietary supplement \"Ephedra\" (containing ephedrine) can lead to adverse events including gastrointestinal problems as well as sudden death from cardiomyopathy. Products adulterated with pharmaceuticals for weight loss or erectile dysfunction are one of the main concerns. Chinese herbal medicine has been a major cause of acute liver failure in China.\n\nAcupuncture is the insertion of needles into superficial structures of the body (skin, subcutaneous tissue, muscles) – usually at acupuncture points (acupoints) – and their subsequent manipulation; this aims at influencing the flow of qi. According to TCM it relieves pain and treats (and prevents) various diseases.\n\nAcupuncture is often accompanied by moxibustion – the Chinese characters for acupuncture () literally meaning \"acupuncture-moxibustion\" – which involves burning mugwort on or near the skin at an acupuncture point. According to the American Cancer Society, \"available scientific evidence does not support claims that moxibustion is effective in preventing or treating cancer or any other disease\".\n\nIn electroacupuncture, an electric current is applied to the needles once they are inserted, in order to further stimulate the respective acupuncture points.\n\nA 2013 editorial by Steven P. Novella and David Colquhoun found that the inconsistency of results of acupuncture studies (i.e. acupuncture relieved pain in some conditions but had no effect in other very similar conditions) suggests false positive results, which may be caused by factors like biased study designs, poor blinding, and the classification of electrified needles (a type of TENS) as a form of acupuncture. The same editorial suggested that given the inability to find consistent results despite more than 3,000 studies of acupuncture, the treatment seems to be a placebo effect and the existing equivocal positive results are noise one expects to see after a large number of studies are performed on an inert therapy. The editorial concluded that the best controlled studies showed a clear pattern, in which the outcome does not rely upon needle location or even needle insertion, and since \"these variables are those that define acupuncture, the only sensible conclusion is that acupuncture does not work.\"\n\nA 2012 meta-analysis concluded that the mechanisms of acupuncture \"are clinically relevant, but that an important part of these total effects is not due to issues considered to be crucial by most acupuncturists, such as the correct location of points and depth of needling ... [but are] ... associated with more potent placebo or context effects\". Commenting on this meta-analysis, both Edzard Ernst and David Colquhoun said the results were of negligible clinical significance.\n\nA 2011 overview of Cochrane reviews found high quality evidence that suggests acupuncture is effective for some but not all kinds of pain. A 2010 systematic review found that there is evidence \"that acupuncture provides a short-term clinically relevant effect when compared with a waiting list control or when acupuncture is added to another intervention\" in the treatment of chronic low back pain. Two review articles discussing the effectiveness of acupuncture, from 2008 and 2009, have concluded that there is not enough evidence to conclude that it is effective beyond the placebo effect.\n\nAcupuncture is generally safe when administered using Clean Needle Technique (CNT). Although serious adverse effects are rare, acupuncture is not without risk. Severe adverse effects, including death, have continued to be reported.\n\nTui na (推拿) is a form of massage akin to acupressure (from which shiatsu evolved). Asian massage is typically administered with the person fully clothed, without the application of grease or oils . Techniques employed may include thumb presses, rubbing, percussion, and assisted stretching.\n\nQìgōng (气功 or 氣功) is a TCM system of exercise and meditation that combines regulated breathing, slow movement, and focused awareness, purportedly to cultivate and balance qi. One branch of qigong is qigong massage, in which the practitioner combines massage techniques with awareness of the acupuncture channels and points.\n\nCupping (Chinese: 拔罐; pinyin: báguàn) is a type of Chinese massage, consisting of placing several glass \"cups\" (open spheres) on the body. A match is lit and placed inside the cup and then removed before placing the cup against the skin. As the air in the cup is heated, it expands, and after placing in the skin, cools, creating lower pressure inside the cup that allows the cup to stick to the skin via suction . When combined with massage oil, the cups can be slid around the back, offering \"reverse-pressure massage\".\n\nIt has not been found to be effective for the treatment of any disease. The 2008 \"Trick or Treatment\" book said that no evidence exists of any beneficial effects of cupping for any medical condition.\n\nGua Sha (Chinese: 刮痧； pinyin: guāshā) is abrading the skin with pieces of smooth jade, bone, animal tusks or horns or smooth stones; until red spots then bruising cover the area to which it is done. It is believed that this treatment is for almost any ailment including cholera . The red spots and bruising take 3 to 10 days to heal, there is often some soreness in the area that has been treated.\n\nDiē-dá (跌打) or bone-setting is usually practiced by martial artists who know aspects of Chinese medicine that apply to the treatment of trauma and injuries such as bone fractures, sprains, and bruises. Some of these specialists may also use or recommend other disciplines of Chinese medical therapies (or Western medicine in modern times) if serious injury is involved. Such practice of bone-setting (整骨 or 正骨) is not common in the West.\n\nTraditional Chinese characters and for the words \"yin\" and \"yang\" denote different classes of foods, and it is important to consume them in a balanced fashion. The meal sequence should also observe these classes:\n\nMany governments have enacted laws to regulate TCM practice.\n\nFrom 1 July 2012 Chinese medicine practitioners must be registered under the national registration and accreditation scheme with the Chinese Medicine Board of Australia and meet the Board's Registration Standards, in order to practice in Australia.\n\nTCM is regulated in five provinces in Canada: Alberta, British Columbia, Ontario, Quebec, and Newfoundland.\n\nThe Chinese Medicine Council of Hong Kong was established in 1999. It regulates the medicinals and professional standards for TCM practitioners. All TCM practitioners in Hong Kong are required to register with the Council. The eligibility for registration includes a recognised 5-year university degree of TCM, a 30-week minimum supervised clinical internship, and passing the licensing exam.\n\nThe Traditional and Complementary Medicine Bill was passed by Parliament in 2012 establishing the Traditional and Complementary Medicine Council to register and regulate traditional and complementary medicine practitioners, including traditional Chinese medicine practitioners as well as other traditional and complementary medicine practitioners such as those in traditional Malay medicine and traditional Indian medicine.\n\nThe TCM Practitioners Act was passed by Parliament in 2000 and the TCM Practitioners Board was established in 2001 as a statutory board under the Ministry of Health, to register and regulate TCM practitioners. The requirements for registration include possession of a diploma or degree from a TCM educational institution/university on a gazetted list, either structured TCM clinical training at an approved local TCM educational institution or foreign TCM registration together with supervised TCM clinical attachment/practice at an approved local TCM clinic, and upon meeting these requirements, passing the Singapore TCM Physicians Registration Examination (STRE) conducted by the TCM Practitioners Board.\n\nAs of July 2012, only six states do not have existing legislation to regulate the professional practice of TCM. These six states are Alabama, Kansas, North Dakota, South Dakota, Oklahoma, and Wyoming. In 1976, California established an Acupuncture Board and became the first state licensing professional acupuncturists.\n\nAll traditional medicines, including TCM, are regulated on Indonesian Minister of Health Regulation in 2013 about Traditional Medicine. Traditional Medicine License (\"Surat Izin Pengobatan Tradisional\" -SIPT) will be granted to the practitioners whose methods are scientifically recognized as safe and bring the benefit for health. The TCM clinics are registered but there is no explicit regulation for it. The only TCM method which is accepted by medical logic and is empirically proofed is acupuncture. The acupuncturists can get SIPT and participate on health care facilities.\n\n\n\n"}
{"id": "5993", "url": "https://en.wikipedia.org/wiki?curid=5993", "title": "Chemical bond", "text": "Chemical bond\n\nA chemical bond is a lasting attraction between atoms that enables the formation of chemical compounds. The bond may result from the electrostatic force of attraction between atoms with opposite charges, or through the sharing of electrons as in the covalent bonds. The strength of chemical bonds varies considerably; there are \"strong bonds\" or \"primary bond\" such as metallic, covalent or ionic bonds and \"weak bonds\" or \"secondary bond\" such as Dipole-dipole interaction, the London dispersion force and hydrogen bonding.\n\nSince opposite charges attract via a simple electromagnetic force, the negatively charged electrons that are orbiting the nucleus and the positively charged protons in the nucleus attract each other. An electron positioned between two nuclei will be attracted to both of them, and the nuclei will be attracted toward electrons in this position. This attraction constitutes the chemical bond. Due to the matter wave nature of electrons and their smaller mass, they must occupy a much larger amount of volume compared with the nuclei, and this volume occupied by the electrons keeps the atomic nuclei in a bond relatively far apart, as compared with the size of the nuclei themselves.\n\nIn general, strong chemical bonding is associated with the sharing or transfer of electrons between the participating atoms. The atoms in molecules, crystals, metals and diatomic gases—indeed most of the physical environment around us—are held together by chemical bonds, which dictate the structure and the bulk properties of matter.\nAll bonds can be explained by quantum theory, but, in practice, simplification rules allow chemists to predict the strength, directionality, and polarity of bonds. The octet rule and VSEPR theory are two examples. More sophisticated theories are valence bond theory which includes orbital hybridization and resonance, and molecular orbital theory which includes linear combination of atomic orbitals and ligand field theory. Electrostatics are used to describe bond polarities and the effects they have on chemical substances.\n\nA chemical bond is an attraction between atoms. This attraction may be seen as the result of different behaviors of the outermost or valence electrons of atoms. These behaviors merge into each other seamlessly in various circumstances, so that there is no clear line to be drawn between them. However it remains useful and customary to differentiate between different types of bond, which result in different properties of condensed matter.\n\nIn the simplest view of a covalent bond, one or more electrons (often a pair of electrons) are drawn into the space between the two atomic nuclei. Energy is released by bond formation. This is not as a reduction in potential energy, because the attraction of the two electrons to the two protons is offset by the electron-electron and proton-proton repulsions. Instead, the release of energy (and hence stability of the bond) arises from the reduction in kinetic energy due to the electrons being in a more spatially distributed (i.e. longer de Broglie wavelength) orbital compared with each electron being confined closer to its respective nucleus. These bonds exist between two particular identifiable atoms and have a direction in space, allowing them to be shown as single connecting lines between atoms in drawings, or modeled as sticks between spheres in models.\n\nIn a polar covalent bond, one or more electrons are unequally shared between two nuclei. Covalent bonds often result in the formation of small collections of better-connected atoms called molecules, which in solids and liquids are bound to other molecules by forces that are often much weaker than the covalent bonds that hold the molecules internally together. Such weak intermolecular bonds give organic molecular substances, such as waxes and oils, their soft bulk character, and their low melting points (in liquids, molecules must cease most structured or oriented contact with each other). When covalent bonds link long chains of atoms in large molecules, however (as in polymers such as nylon), or when covalent bonds extend in networks through solids that are not composed of discrete molecules (such as diamond or quartz or the silicate minerals in many types of rock) then the structures that result may be both strong and tough, at least in the direction oriented correctly with networks of covalent bonds. Also, the melting points of such covalent polymers and networks increase greatly.\n\nIn a simplified view of an \"ionic\" bond, the bonding electron is not shared at all, but transferred. In this type of bond, the outer atomic orbital of one atom has a vacancy which allows the addition of one or more electrons. These newly added electrons potentially occupy a lower energy-state (effectively closer to more nuclear charge) than they experience in a different atom. Thus, one nucleus offers a more tightly bound position to an electron than does another nucleus, with the result that one atom may transfer an electron to the other. This transfer causes one atom to assume a net positive charge, and the other to assume a net negative charge. The \"bond\" then results from electrostatic attraction between atoms and the atoms become positive or negatively charged ions. Ionic bonds may be seen as extreme examples of polarization in covalent bonds. Often, such bonds have no particular orientation in space, since they result from equal electrostatic attraction of each ion to all ions around them. Ionic bonds are strong (and thus ionic substances require high temperatures to melt) but also brittle, since the forces between ions are short-range and do not easily bridge cracks and fractures. This type of bond gives rise to the physical characteristics of crystals of classic mineral salts, such as table salt.\n\nA less often mentioned type of bonding is \"metallic\" bonding. In this type of bonding, each atom in a metal donates one or more electrons to a \"sea\" of electrons that reside between many metal atoms. In this sea, each electron is free (by virtue of its wave nature) to be associated with a great many atoms at once. The bond results because the metal atoms become somewhat positively charged due to loss of their electrons while the electrons remain attracted to many atoms, without being part of any given atom. Metallic bonding may be seen as an extreme example of delocalization of electrons over a large system of covalent bonds, in which every atom participates. This type of bonding is often very strong (resulting in the tensile strength of metals). However, metallic bonding is more collective in nature than other types, and so they allow metal crystals to more easily deform, because they are composed of atoms attracted to each other, but not in any particularly-oriented ways. This results in the malleability of metals. The sea of electrons in metallic bonding causes the characteristically good electrical and thermal conductivity of metals, and also their \"shiny\" reflection of most frequencies of white light.\n\nEarly speculations about the nature of the chemical bond, from as early as the 12th century, supposed that certain types of chemical species were joined by a type of chemical affinity. In 1704, Sir Isaac Newton famously outlined his atomic bonding theory, in \"Query 31\" of his \"Opticks\", whereby atoms attach to each other by some \"force\". Specifically, after acknowledging the various popular theories in vogue at the time, of how atoms were reasoned to attach to each other, i.e. \"hooked atoms\", \"glued together by rest\", or \"stuck together by conspiring motions\", Newton states that he would rather infer from their cohesion, that \"particles attract one another by some force, which in immediate contact is exceedingly strong, at small distances performs the chemical operations, and reaches not far from the particles with any sensible effect.\"\n\nIn 1819, on the heels of the invention of the voltaic pile, Jöns Jakob Berzelius developed a theory of chemical combination stressing the electronegative and electropositive characters of the combining atoms. By the mid 19th century, Edward Frankland, F.A. Kekulé, A.S. Couper, Alexander Butlerov, and Hermann Kolbe, building on the theory of radicals, developed the theory of valency, originally called \"combining power\", in which compounds were joined owing to an attraction of positive and negative poles. In 1916, chemist Gilbert N. Lewis developed the concept of the electron-pair bond, in which two atoms may share one to six electrons, thus forming the single electron bond, a single bond, a double bond, or a triple bond; in Lewis's own words, \"An electron may form a part of the shell of two different atoms and cannot be said to belong to either one exclusively.\"\n\nThat same year, Walther Kossel put forward a theory similar to Lewis' only his model assumed complete transfers of electrons between atoms, and was thus a model of ionic bonding. Both Lewis and Kossel structured their bonding models on that of Abegg's rule (1904).\n\nNiels Bohr proposed a model of the atom and a model of the chemical bond. According to his model for a diatomic molecule, the electrons of the atoms of the molecule form a rotating ring whose plane is perpendicular to the axis of the molecule and equidistant from the atomic nuclei. The dynamic equilibrium of the molecular system is achieved through the balance of forces between the forces of attraction of nuclei to the plane of the ring of electrons and the forces of mutual repulsion of the nuclei. The Bohr model of the chemical bond took into account the Coulomb repulsion - the electrons in the ring are at the maximum distance from each other. \n\nIn 1927, the first mathematically complete quantum description of a simple chemical bond, i.e. that produced by one electron in the hydrogen molecular ion, H, was derived by the Danish physicist Oyvind Burrau. This work showed that the quantum approach to chemical bonds could be fundamentally and quantitatively correct, but the mathematical methods used could not be extended to molecules containing more than one electron. A more practical, albeit less quantitative, approach was put forward in the same year by Walter Heitler and Fritz London. The Heitler-London method forms the basis of what is now called valence bond theory. In 1929, the linear combination of atomic orbitals molecular orbital method (LCAO) approximation was introduced by Sir John Lennard-Jones, who also suggested methods to derive electronic structures of molecules of F (fluorine) and O (oxygen) molecules, from basic quantum principles. This molecular orbital theory represented a covalent bond as an orbital formed by combining the quantum mechanical Schrödinger atomic orbitals which had been hypothesized for electrons in single atoms. The equations for bonding electrons in multi-electron atoms could not be solved to mathematical perfection (i.e., \"analytically\"), but approximations for them still gave many good qualitative predictions and results. Most quantitative calculations in modern quantum chemistry use either valence bond or molecular orbital theory as a starting point, although a third approach, density functional theory, has become increasingly popular in recent years.\n\nIn 1933, H. H. James and A. S. Coolidge carried out a calculation on the dihydrogen molecule that, unlike all previous calculation which used functions only of the distance of the electron from the atomic nucleus, used functions which also explicitly added the distance between the two electrons. With up to 13 adjustable parameters they obtained a result very close to the experimental result for the dissociation energy. Later extensions have used up to 54 parameters and gave excellent agreement with experiments. This calculation convinced the scientific community that quantum theory could give agreement with experiment. However this approach has none of the physical pictures of the valence bond and molecular orbital theories and is difficult to extend to larger molecules.\n\nBecause atoms and molecules are three-dimensional, it is difficult to use a single method to indicate orbitals and bonds. In molecular formulas the chemical bonds (binding orbitals) between atoms are indicated in different ways depending on the type of discussion. Sometimes, some details are neglected. For example, in organic chemistry one is sometimes concerned only with the functional group of the molecule. Thus, the molecular formula of ethanol may be written in conformational form, three-dimensional form, full two-dimensional form (indicating every bond with no three-dimensional directions), compressed two-dimensional form (CH–CH–OH), by separating the functional group from another part of the molecule (CHOH), or by its atomic constituents (CHO), according to what is discussed. Sometimes, even the non-bonding valence shell electrons (with the two-dimensional approximate directions) are marked, e.g. for elemental carbon C. Some chemists may also mark the respective orbitals, e.g. the hypothetical ethene anion (C=C ) indicating the possibility of bond formation.\n\nStrong chemical bonds are the \"intramolecular\" forces which hold atoms together in molecules. A strong chemical bond is formed from the transfer or sharing of electrons between atomic centers and relies on the electrostatic attraction between the protons in nuclei and the electrons in the orbitals.\n\nThe types of strong bond differ due to the difference in electronegativity of the constituent elements. A large difference in electronegativity leads to more polar (ionic) character in the bond.\n\nIonic bonding is a type of electrostatic interaction between atoms which have a large electronegativity difference. There is no precise value that distinguishes ionic from covalent bonding, but a difference of electronegativity of over 1.7 is likely to be ionic, and a difference of less than 1.7 is likely to be covalent. Ionic bonding leads to separate positive and negative ions. Ionic charges are commonly between −3e to +3e.\nIonic bonding commonly occurs in metal salts such as sodium chloride (table salt). A typical feature of ionic bonds is that the species form into ionic crystals, in which no ion is specifically paired with any single other ion, in a specific directional bond. Rather, each species of ion is surrounded by ions of the opposite charge, and the spacing between it and each of the oppositely charged ions near it, is the same for all surrounding atoms of the same type. It is thus no longer possible to associate an ion with any specific other single ionized atom near it. This is a situation unlike that in covalent crystals, where covalent bonds between specific atoms are still discernible from the shorter distances between them, as measured via such techniques as X-ray diffraction.\n\nIonic crystals may contain a mixture of covalent and ionic species, as for example salts of complex acids, such as sodium cyanide, NaCN. X-ray diffraction shows that in NaCN, for example, the bonds between sodium cations (Na) and the cyanide anions (CN) are \"ionic\", with no sodium ion associated with any particular cyanide. However, the bonds between C and N atoms in cyanide are of the \"covalent\" type, making each of the carbon and nitrogen associated with \"just one\" of its opposite type, to which it is physically much closer than it is to other carbons or nitrogens in a sodium cyanide crystal.\n\nWhen such crystals are melted into liquids, the ionic bonds are broken first because they are non-directional and allow the charged species to move freely. Similarly, when such salts dissolve into water, the ionic bonds are typically broken by the interaction with water, but the covalent bonds continue to hold. For example, in solution, the cyanide ions, still bound together as single CN ions, move independently through the solution, as do sodium ions, as Na. In water, charged ions move apart because each of them are more strongly attracted to a number of water molecules, than to each other. The attraction between ions and water molecules in such solutions is due to a type of weak dipole-dipole type chemical bond. In melted ionic compounds, the ions continue to be attracted to each other, but not in any ordered or crystalline way.\n\nCovalent bonding is a common type of bonding, in which two atoms share two valence electrons, one from each of the atoms. In nonpolar covalent bonds, the electronegativity difference between the bonded atoms is small, typically 0 to 0.3. Bonds within most organic compounds are described as covalent. The figure shows methane (CH), in which each hydrogen forms a covalent bond with the carbon. See sigma bonds and pi bonds for LCAO-description of such bonding.\n\nMolecules which are formed primarily from non-polar covalent bonds are often immiscible in water or other polar solvents, but much more soluble in non-polar solvents such as hexane.\n\nA polar covalent bond is a covalent bond with a significant ionic character. This means that the two shared electrons are closer to one of the atoms than the other, creating an imbalance of charge. Such bonds occur between two atoms with moderately different electronegativities and give rise to dipole-dipole interactions. The electronegativity difference between the two atoms in these bonds is 0.3 to 1.7.\n\nA single bond between two atoms corresponds to the sharing of one pair of electrons. The electron density of these two bonding electrons is concentrated in the region between the two atoms, which is the defining quality of a sigma bond.\nA double bond between two atoms is formed by the sharing of two pairs of electrons, one in a sigma bond and one in a pi bond, with electron density concentrated on two opposite sides of the internuclear axis. A triple bond consists of three shared electron pairs, forming one sigma and two pi bonds.\n\nQuadruple and higher bonds are very rare and occur only between certain transition metal atoms.\n\nA coordinate covalent bond is a covalent bond in which the two shared \nbonding electrons are from the same one of the atoms involved in the bond. \nFor example, boron trifluoride (BF) and ammonia (NH) from an adduct or coordination complex FB←NH with a B–N bond in which a lone pair of electrons on N is shared with an empty atomic orbital on B. BF with an empty orbital is described as an electron pair acceptor or Lewis acid, while NH with a lone pair which can be shared is described as an electron-pair donor or Lewis base. The electrons are shared roughly equally between the atoms in contrast to ionic bonding. Such bonding is shown by an arrow pointing to the Lewis acid.\n\nTransition metal complexes are generally bound by coordinate covalent bonds. For example, the ion Ag reacts as a Lewis acid with two molecules of the Lewis base NH to form the complex ion Ag(NH), which has two Ag←N coordinate covalent bonds.\n\nIn metallic bonding, bonding electrons are delocalized over a lattice of atoms. By contrast, in ionic compounds, the locations of the binding electrons and their charges are static. The freely-moving or delocalization of bonding electrons leads to classical metallic properties such as luster (surface light reflectivity), electrical and thermal conductivity, ductility, and high tensile strength.\n\nThere are four basic types of bonds that can be formed between two or more (otherwise non-associated) molecules, ions or atoms. Intermolecular forces cause molecules to be attracted or repulsed by each other. Often, these define some of the physical characteristics (such as the melting point) of a substance.\n\n\nIn the (unrealistic) limit of \"pure\" ionic bonding, electrons are perfectly localized on one of the two atoms in the bond. Such bonds can be understood by classical physics. The forces between the atoms are characterized by isotropic continuum electrostatic potentials. Their magnitude is in simple proportion to the charge difference.\n\nCovalent bonds are better understood by valence bond theory or molecular orbital theory. The properties of the atoms involved can be understood using concepts such as oxidation number. The electron density within a bond is not assigned to individual atoms, but is instead delocalized between atoms. In valence bond theory, the two electrons on the two atoms are coupled together with the bond strength depending on the overlap between them. In molecular orbital theory, the linear combination of atomic orbitals (LCAO) helps describe the delocalized molecular orbital structures and energies based on the atomic orbitals of the atoms they came from. Unlike pure ionic bonds, covalent bonds may have directed anisotropic properties. These may have their own names, such as sigma bond and pi bond.\n\nIn the general case, atoms form bonds that are intermediate between ionic and covalent, depending on the relative electronegativity of the atoms involved. This type of bond is sometimes called polar covalent.\n\n"}
{"id": "5995", "url": "https://en.wikipedia.org/wiki?curid=5995", "title": "Cell", "text": "Cell\n\nCell may refer to:\n\n\n\n\n\n\n\n\n"}
{"id": "5999", "url": "https://en.wikipedia.org/wiki?curid=5999", "title": "Climate", "text": "Climate\n\nClimate is the statistics of weather over long periods of time. It is measured by assessing the patterns of variation in temperature, humidity, atmospheric pressure, wind, precipitation, atmospheric particle count and other meteorological variables in a given region over long periods of time. Climate differs from weather, in that weather only describes the short-term conditions of these variables in a given region.\n\nA region's climate is generated by the climate system, which has five components: atmosphere, hydrosphere, cryosphere, lithosphere, and biosphere.\n\nThe climate of a location is affected by its latitude, terrain, and altitude, as well as nearby water bodies and their currents. Climates can be classified according to the average and the typical ranges of different variables, most commonly temperature and precipitation. The most commonly used classification scheme was the Köppen climate classification. The Thornthwaite system, in use since 1948, incorporates evapotranspiration along with temperature and precipitation information and is used in studying biological diversity and how climate change affects it. The Bergeron and Spatial Synoptic Classification systems focus on the origin of air masses that define the climate of a region.\n\nPaleoclimatology is the study of ancient climates. Since direct observations of climate are not available before the 19th century, paleoclimates are inferred from proxy variables that include non-biotic evidence such as sediments found in lake beds and ice cores, and biotic evidence such as tree rings and coral. Climate models are mathematical models of past, present and future climates. Climate change may occur over long and short timescales from a variety of factors; recent warming is discussed in global warming. Global warming results in redistributions. For example, \"a 3°C change in mean annual temperature corresponds to a shift in isotherms of approximately 300–400 km in latitude (in the temperate zone) or 500 m in elevation. Therefore, species are expected to move upwards in elevation or towards the poles in latitude in response to shifting climate zones\".\n\nClimate (from Ancient Greek \"klima\", meaning \"inclination\") is commonly defined as the weather averaged over a long period. The standard averaging period is 30 years, but other periods may be used depending on the purpose. Climate also includes statistics other than the average, such as the magnitudes of day-to-day or year-to-year variations. The Intergovernmental Panel on Climate Change (IPCC) 2001 glossary definition is as follows:\n\nThe World Meteorological Organization (WMO) describes climate \"normals\" as \"reference points used by climatologists to compare current climatological trends to that of the past or what is considered 'normal'. A Normal is defined as the arithmetic average of a climate element (e.g. temperature) over a 30-year period. A 30 year period is used, as it is long enough to filter out any interannual variation or anomalies, but also short enough to be able to show longer climatic trends.\" The WMO originated from the International Meteorological Organization which set up a technical commission for climatology in 1929. At its 1934 Wiesbaden meeting the technical commission designated the thirty-year period from 1901 to 1930 as the reference time frame for climatological standard normals. In 1982 the WMO agreed to update climate normals, and these were subsequently completed on the basis of climate data from 1 January 1961 to 31 December 1990.\n\nThe difference between climate and weather is usefully summarized by the popular phrase \"Climate is what you expect, weather is what you get.\" Over historical time spans there are a number of nearly constant variables that determine climate, including latitude, altitude, proportion of land to water, and proximity to oceans and mountains. These change only over periods of millions of years due to processes such as plate tectonics. Other climate determinants are more dynamic: the thermohaline circulation of the ocean leads to a 5 °C (9 °F) warming of the northern Atlantic Ocean compared to other ocean basins. Other ocean currents redistribute heat between land and water on a more regional scale. The density and type of vegetation coverage affects solar heat absorption, water retention, and rainfall on a regional level. Alterations in the quantity of atmospheric greenhouse gases determines the amount of solar energy retained by the planet, leading to global warming or global cooling. The variables which determine climate are numerous and the interactions complex, but there is general agreement that the broad outlines are understood, at least insofar as the determinants of historical climate change are concerned.\n\nThere are several ways to classify climates into similar regimes. Originally, climes were defined in Ancient Greece to describe the weather depending upon a location's latitude. Modern climate classification methods can be broadly divided into \"genetic\" methods, which focus on the causes of climate, and \"empiric\" methods, which focus on the effects of climate. Examples of genetic classification include methods based on the relative frequency of different air mass types or locations within synoptic weather disturbances. Examples of empiric classifications include climate zones defined by plant hardiness, evapotranspiration, or more generally the Köppen climate classification which was originally designed to identify the climates associated with certain biomes. A common shortcoming of these classification schemes is that they produce distinct boundaries between the zones they define, rather than the gradual transition of climate properties more common in nature.\n\nThe simplest classification is that involving air masses. The Bergeron classification is the most widely accepted form of air mass classification. Air mass classification involves three letters. The first letter describes its moisture properties, with c used for continental air masses (dry) and m for maritime air masses (moist). The second letter describes the thermal characteristic of its source region: T for tropical, P for polar, A for Arctic or Antarctic, M for monsoon, E for equatorial, and S for superior air (dry air formed by significant downward motion in the atmosphere). The third letter is used to designate the stability of the atmosphere. If the air mass is colder than the ground below it, it is labeled k. If the air mass is warmer than the ground below it, it is labeled w. While air mass identification was originally used in weather forecasting during the 1950s, climatologists began to establish synoptic climatologies based on this idea in 1973.\n\nBased upon the Bergeron classification scheme is the Spatial Synoptic Classification system (SSC). There are six categories within the SSC scheme: Dry Polar (similar to continental polar), Dry Moderate (similar to maritime superior), Dry Tropical (similar to continental tropical), Moist Polar (similar to maritime polar), Moist Moderate (a hybrid between maritime polar and maritime tropical), and Moist Tropical (similar to maritime tropical, maritime monsoon, or maritime equatorial).\n\nThe Köppen classification depends on average monthly values of temperature and precipitation. The most commonly used form of the Köppen classification has five primary types labeled A through E. These primary types are A) tropical, B) dry, C) mild mid-latitude, D) cold mid-latitude, and E) polar. The five primary classifications can be further divided into secondary classifications such as rainforest, monsoon, tropical savanna, humid subtropical, humid continental, oceanic climate, Mediterranean climate, desert, steppe, subarctic climate, tundra, and polar ice cap.\n\nRainforests are characterized by high rainfall, with definitions setting minimum normal annual rainfall between and . Mean monthly temperatures exceed during all months of the year.\n\nA monsoon is a seasonal prevailing wind which lasts for several months, ushering in a region's rainy season. Regions within North America, South America, Sub-Saharan Africa, Australia and East Asia are monsoon regimes.\n\nA tropical savanna is a grassland biome located in semiarid to semi-humid climate regions of subtropical and tropical latitudes, with average temperatures remain at or above year round and rainfall between and a year. They are widespread on Africa, and are found in India, the northern parts of South America, Malaysia, and Australia.\n\nThe humid subtropical climate zone where winter rainfall (and sometimes snowfall) is associated with large storms that the westerlies steer from west to east. Most summer rainfall occurs during thunderstorms and from occasional tropical cyclones. Humid subtropical climates lie on the east side of continents, roughly between latitudes 20° and 40° degrees away from the equator.\nA humid continental climate is marked by variable weather patterns and a large seasonal temperature variance. Places with more than three months of average daily temperatures above and a coldest month temperature below and which do not meet the criteria for an arid or semiarid climate, are classified as continental.\n\nAn oceanic climate is typically found along the west coasts at the middle latitudes of all the world's continents, and in southeastern Australia, and is accompanied by plentiful precipitation year-round.\n\nThe Mediterranean climate regime resembles the climate of the lands in the Mediterranean Basin, parts of western North America, parts of Western and South Australia, in southwestern South Africa and in parts of central Chile. The climate is characterized by hot, dry summers and cool, wet winters.\n\nA steppe is a dry grassland with an annual temperature range in the summer of up to and during the winter down to .\n\nA subarctic climate has little precipitation, and monthly temperatures which are above for one to three months of the year, with permafrost in large parts of the area due to the cold winters. Winters within subarctic climates usually include up to six months of temperatures averaging below .\nTundra occurs in the far Northern Hemisphere, north of the taiga belt, including vast areas of northern Russia and Canada.\n\nA polar ice cap, or polar ice sheet, is a high-latitude region of a planet or moon that is covered in ice. Ice caps form because high-latitude regions receive less energy as solar radiation from the sun than equatorial regions, resulting in lower surface temperatures.\n\nA desert is a landscape form or region that receives very little precipitation. Deserts usually have a large diurnal and seasonal temperature range, with high or low, depending on location daytime temperatures (in summer up to ), and low nighttime temperatures (in winter down to ) due to extremely low humidity. Many deserts are formed by rain shadows, as mountains block the path of moisture and precipitation to the desert.\n\nDevised by the American climatologist and geographer C. W. Thornthwaite, this climate classification method monitors the soil water budget using evapotranspiration. It monitors the portion of total precipitation used to nourish vegetation over a certain area. It uses indices such as a humidity index and an aridity index to determine an area's moisture regime based upon its average temperature, average rainfall, and average vegetation type. The lower the value of the index in any given area, the drier the area is.\n\nThe moisture classification includes climatic classes with descriptors such as hyperhumid, humid, subhumid, subarid, semi-arid (values of −20 to −40), and arid (values below −40). Humid regions experience more precipitation than evaporation each year, while arid regions experience greater evaporation than precipitation on an annual basis. A total of 33 percent of the Earth's landmass is considered either arid or semi-arid, including southwest North America, southwest South America, most of northern and a small part of southern Africa, southwest and portions of eastern Asia, as well as much of Australia. Studies suggest that precipitation effectiveness (PE) within the Thornthwaite moisture index is overestimated in the summer and underestimated in the winter. This index can be effectively used to determine the number of herbivore and mammal species numbers within a given area. The index is also used in studies of climate change.\n\nThermal classifications within the Thornthwaite scheme include microthermal, mesothermal, and megathermal regimes. A microthermal climate is one of low annual mean temperatures, generally between and which experiences short summers and has a potential evaporation between and . A mesothermal climate lacks persistent heat or persistent cold, with potential evaporation between and . A megathermal climate is one with persistent high temperatures and abundant rainfall, with potential annual evaporation in excess of .\n\nDetails of the modern climate record are known through the taking of measurements from such weather instruments as thermometers, barometers, and anemometers during the past few centuries. The instruments used to study weather over the modern time scale, their known error, their immediate environment, and their exposure have changed over the years, which must be considered when studying the climate of centuries past.\n\nPaleoclimatology is the study of past climate over a great period of the Earth's history. It uses evidence from ice sheets, tree rings, sediments, coral, and rocks to determine the past state of the climate. It demonstrates periods of stability and periods of change and can indicate whether changes follow patterns such as regular cycles.\n\nClimate change is the variation in global or regional climates over time. It reflects changes in the variability or average state of the atmosphere over time scales ranging from decades to millions of years. These changes can be caused by processes internal to the Earth, external forces (e.g. variations in sunlight intensity) or, more recently, human activities.\n\nIn recent usage, especially in the context of environmental policy, the term \"climate change\" often refers only to changes in modern climate, including the rise in average surface temperature known as global warming. In some cases, the term is also used with a presumption of human causation, as in the United Nations Framework Convention on Climate Change (UNFCCC). The UNFCCC uses \"climate variability\" for non-human caused variations.\n\nEarth has undergone periodic climate shifts in the past, including four major ice ages. These consisting of glacial periods where conditions are colder than normal, separated by interglacial periods. The accumulation of snow and ice during a glacial period increases the surface albedo, reflecting more of the Sun's energy into space and maintaining a lower atmospheric temperature. Increases in greenhouse gases, such as by volcanic activity, can increase the global temperature and produce an interglacial period. Suggested causes of ice age periods include the positions of the continents, variations in the Earth's orbit, changes in the solar output, and volcanism.\n\nClimate models use quantitative methods to simulate the interactions of the atmosphere, oceans, land surface and ice. They are used for a variety of purposes; from the study of the dynamics of the weather and climate system, to projections of future climate. All climate models balance, or very nearly balance, incoming energy as short wave (including visible) electromagnetic radiation to the earth with outgoing energy as long wave (infrared) electromagnetic radiation from the earth. Any imbalance results in a change in the average temperature of the earth.\n\nThe most talked-about applications of these models in recent years have been their use to infer the consequences of increasing greenhouse gases in the atmosphere, primarily carbon dioxide (see greenhouse gas). These models predict an upward trend in the global mean surface temperature, with the most rapid increase in temperature being projected for the higher latitudes of the Northern Hemisphere.\n\nModels can range from relatively simple to quite complex:\n\nClimate forecasting is a way by some scientists are using to predict climate change. In 1997 the prediction division of the International Research Institute for Climate and Society at Columbia University began generating seasonal climate forecasts on a real-time basis. To produce these forecasts an extensive suite of forecasting tools was developed, including a multimodel ensemble approach that required thorough validation of each model's accuracy level in simulating interannual climate variability.\n\n\n"}
{"id": "6000", "url": "https://en.wikipedia.org/wiki?curid=6000", "title": "History of the Comoros", "text": "History of the Comoros\n\nThe history of the Comoros goes back some 1,500 years. It has been inhabited by various groups throughout this time. France colonised the islands in the 19th century. The Comoros finally became independent in 1975.\n\nAccording to myth, the Comoros islands were first visited by Phoenician sailors. The earliest inhabitants of the islands were probably Bantu-speaking Africans; the earliest evidence of settlement of the islands dates from the sixth century. Traces of this original culture have blended with successive waves of African, Arab and Malagasy. Shirazi immigrants appear to have arrived some time after the tenth century A.D.\n\nIn the 16th century, social changes on the East African coast probably linked to the arrival of the Portuguese saw the arrival of a number of Arabs of Hadrami who established alliances with the Shirazis and founded several royal clans.\n\nOver the centuries, the Comoro Islands have been settled by a succession of diverse groups from the coast of Africa, the Persian Gulf, Southeast Asia and Madagascar. Portuguese explorers first visited the archipelago in 1505.\n\nApart from a visit by the French Parmentier brothers in 1529, for much of the 16th century the only Europeans to visit the islands were Portuguese; British and Dutch ships began arriving around the start of the 17th century and the island of Ndzwani soon became a major supply point on the route to the East. Ndzwani was generally ruled by a single sultan, who occasionally attempted to extend his authority to Mayotte and Mwali; Ngazidja was more fragmented, on occasion being divided into as many as 12 small kingdoms.\n\nBoth the British and the French turned their attention to the Comoros islands in the middle of the 19th century. The French finally acquired the islands through a cunning mixture of strategies, including the policy of 'divide and conquer', chequebook politics and a serendipitous affair between a sultana and a French trader that was put to good use by the French, who kept control of the islands, quelling unrest and the occasional uprising.\n\nWilliam Sunley, a planter and British Consul from 1848–1866, was an influence on Anjouan.\n\nOn 25 March 1841, France purchased the island of Maore (the name of the island was corrupted in French to \"Mayotte\") (ratified 13 June 1843), which became a colony.\n\nIn 1850 Sultan Selim of Johanna island seized the American whaler \"Maria\" and imprisoned her commander, named Moores. In response the United States Navy launched the Johanna Expedition in February 1852 to gain the release of Moores and extract compensation. Initially the sultan did not meet the demands, and the sloop-of-war USS \"Dale\" bombarded the island's fortifications; ultimately Selim paid US$1,000 and released Captain Moores.\n\nIn 1886 Said Ali bin Said Omar, Sultan of Bambao, signed an agreement with the French government that allowed France to establish a protectorate over the entire island of Ngazidja (Grande Comore); protectorates were also established over Ndzwani (Anjouan), and Mwali (Mohéli island in French) the same year. Résidents were posted on the three islands.\n\nOn 9 April 1908, France declared the protectorates and Mayotte a single colony, Mayotte and dependencies.\n\nOn 25 July 1912, it was annexed to Madagascar as a province of that colony.\n\nFrom 16 June 1940 - 1942 the colonial administration remained loyal to Vichy France (from 1942, under Free French), but 25 September 1942 - 13 October 1946 they were, like Madagascar, under British occupation.\n\nUntil the opening of the Suez Canal, the islands used to be an important refuelling and provisioning station for ships from Europe to the Indian Ocean.\n\nIndependence came gradually for the Comoros. During the middle of the 20th century the French reluctantly began to accede to requests for constitutional changes and in 1946 the Comoros had become a separately administered colony from Madagascar.\n\nAfter World War II, the islands became a French overseas territory and were represented in France's National Assembly. Internal political autonomy was granted in 1961. Agreement was reached with France in 1973 for the Comoros to become independent in 1978. On July 6, 1975, however, the Comorian parliament passed a resolution declaring unilateral independence. The deputies of Mayotte abstained.\n\nIn two referendums, in December 1974 and February 1976, the population of Mayotte voted against independence from France (by 63.8% and 99.4% respectively). Mayotte thus remains under French administration, and the Comorian Government has effective control over only Grande Comore, Anjouan, and Mohéli.\n\nIn 1961 the Comoros was granted autonomous rule and, seven years after the general unrest and left-wing riots of 1968, it broke all ties with France and established itself as an independent republic. From the very beginning Mayotte refused to join the new republic and aligned itself even more firmly to the French Republic, but the other islands remained committed to independence. The first president of the Comoros, Ahmed Abdallah Abderemane, did not last long before being ousted in a coup by Ali Soilih, an atheist with an Islamic background.\n\nSoilih began with a set of solid socialist ideals designed to modernize the country. However, the regime faced problems. A French mercenary by the name of Bob Denard, arrived in the Comoros at dawn on 13 May 1978, and removed Soilih from power. Solih was shot and killed during the coup. Abdallah returned to govern the country and the mercenaries were given key positions in government.\n\nLater, French settlers, French-owned companies, and Arab merchants established a plantation-based economy that now uses about one-third of the land for export crops.\n\nIn 1978, president Ali Soilih, who had a firm anti-French line, was killed and Ahmed Abdallah came to power. Under the reign of Abdallah, Denard was commander of the Presidential Guard (PG) and \"de facto\" ruler of the country. He was trained, supported and funded by the white regimes in South Africa (SA) and Rhodesia (now Zimbabwe) in return for permission to set up a secret listening post on the islands. South-African agents kept an ear on the important ANC bases in Lusaka and Dar es Salaam and watched the war in Mozambique, in which SA played an active role. The Comoros were also used for the evasion of arms sanctions.\n\nWhen in 1981 François Mitterrand was elected president Denard lost the support of the French intelligence service, but he managed to strengthen the link between SA and the Comoros. Besides the military, Denard established his own company SOGECOM, for both the security and construction, and seemed to profit by the arrangement. Between 1985 an 1987 the relationship of the PG with the local Comorians became worse.\n\nAt the end of the 1980s the South Africans did not wish to continue to support the mercenary regime and France was in agreement. Also President Abdallah wanted the mercenaries to leave. Their response was a (third) coup resulting in the death of President Abdallah, in which Denard and his men were probably involved. South Africa and the French government subsequently forced Denard and his mercenaries to leave the islands in 1989.\n\nSaid Mohamed Djohar became president. His time in office was turbulent, including an impeachment attempt in 1991 and a coup attempt in 1992.\n\nOn September 28, 1995 Bob Denard and a group of mercenaries took over the Comoros islands in a coup (named operation Kaskari by the mercenaries) against President Djohar. France immediately severely denounced the coup, and backed by the 1978 defense agreement with the Comoros, President Jacques Chirac ordered his special forces to retake the island. Bob Denard began to take measures to stop the coming invasion. A new presidential guard was created. Strong points armed with heavy machine guns were set up around the island, particularly around the island's two airports.\n\nOn October 3, 1995, 11 p.m., the French deployed 600 men against a force of 33 mercenaries and a 300-man dissident force. Denard however ordered his mercenaries not to fight. Within 7 hours the airports at Iconi and Hahaya and the French Embassy in Moroni were secured. By 3:00 p.m. the next day Bob Denard and his mercenaries had surrendered. This (response) operation, codenamed \"Azalée\", was remarkable, because there were no casualties, and just in seven days, plans were drawn up and soldiers were deployed. Denard was taken to France and jailed. Prime minister Caambi El-Yachourtu became acting president until Djohar returned from exile in January, 1996. In March 1996, following presidential elections, Mohamed Taki Abdoulkarim, a member of the civilian government that Denard had tried to set up in October 1995, became president. On 23 November 1996, Ethiopian Airlines Flight 961 crashed near a beach on the island after it was hijacked and ran out of fuel killing 125 people and leaving 50 survivors.\n\nIn 1997, the islands of Anjouan and Mohéli declared their independence from the Comoros. A subsequent attempt by the government to re-establish control over the rebellious islands by force failed, and presently the African Union is brokering negotiations to effect a reconciliation. This process is largely complete, at least in theory. According to some sources, Mohéli did return to government control in 1998. In 1999, Anjouan had internal conflicts and on August 1 of that year, the 80-year-old first president Foundi Abdallah Ibrahim resigned, transferring power to a national coordinator, Said Abeid. The government was overthrown in a coup by army and navy officers on August 9, 2001. Mohamed Bacar soon rose to leadership of the junta that took over and by the end of the month he was the leader of the country. Despite two coup attempts in the following three months, including one by Abeid, Bacar's government remained in power, and was apparently more willing to negotiate with the Comoros. Presidential elections were held for all of the Comoros in 2002, and presidents have been chosen for all three islands as well, which have become a confederation. Most notably, Mohammed Bacar was elected for a 5-year term as president of Anjouan. Grande Comore had experienced troubles of its own in the late 1990s, when President Taki died on November 6, 1998. Colonel Azali Assoumani became president following a military coup in 1999. There have been several coup attempts since, but he gained firm control of the country after stepping down temporarily and winning a presidential election in 2002.\n\nIn May 2006, Ahmed Abdallah Sambi was elected from the island of Anjouan to be the president of the Union of the Comoros. He is a well-respected Sunni cleric who studied in the Sudan, Iran and Saudi Arabia. He is respectfully called \"Ayatollah\" by his supporters but is considered, and is, a moderate Islamist. He has been quoted as stating that the Comoros is not ready to become an Islamic state, nor shall the veil be forced upon any women in the Comoros.\n\n\n"}
{"id": "6001", "url": "https://en.wikipedia.org/wiki?curid=6001", "title": "Geography of the Comoros", "text": "Geography of the Comoros\n\nThe Comoros archipelago consists of four main islands aligned along a northwest-southeast axis at the north end of the Mozambique Channel, between Mozambique and the island of Madagascar. Still widely known by their French names, the islands officially have been called by their Swahili names by the Comorian government. They are Grande Comore (Njazidja), Mohéli (Mwali), Anjouan (Nzwani), and Mayotte (Mahoré). The islands' distance from each other—Grande Comore is some 200 kilometers from Mayotte, forty kilometers from Mohéli, and eighty kilometers from Grande Comore—along with a lack of good harbor facilities, make transportation and communication difficult. The islands have a total land area of 2,236 square kilometers (including Mayotte), and claim territorial waters of 320 square kilometers. Mount Karthala (2316 m) on Grande Comore is an active volcano. From April 17 to 19, 2005, the volcano began spewing ash and gas, forcing as many as 10,000 people to flee.\n\nGeographic coordinates:\nGrande Comore is the largest island, sixty-seven kilometers long and twenty-seven kilometers wide, with a total area of 1,146 square kilometers. The most recently formed of the four islands in the archipelago, it is also of volcanic origin. Two volcanoes form the island's most prominent topographic features: La Grille in the north, with an elevation of 1,000 meters, is extinct and largely eroded; Kartala in the south, rising to a height of 2,361 meters, last erupted in 1977. A plateau averaging 600 to 700 meters high connects the two mountains. Because Grande Comore is geologically a relatively new island, its soil is thin and rocky and cannot hold water. As a result, water from the island's heavy rainfall must be stored in catchment tanks. There are no coral reefs along the coast, and the island lacks a good harbor for ships. One of the largest remnants of the Comoros' once-extensive rain forests is on the slopes of Kartala. The national capital has been at Moroni since 1962.\n\nAnjouan, triangular shaped and forty kilometers from apex to base, has an area of 424 square kilometers. Three mountain chains — Sima, Nioumakele, and Jimilime—emanate from a central peak, Mtingui (1,575 m), giving the island its distinctive shape. Older than Grande Comore, Anjouan has deeper soil cover, but overcultivation has caused serious erosion. A coral reef lies close to shore; the island's capital of Mutsamudu is also its main port.\n\nMohéli is thirty kilometers long and twelve kilometers wide, with an area of 290 square kilometers. It is the smallest of the four islands and has a central mountain chain reaching 860 meters at its highest. Like Grande Comore, it retains stands of rain forest. Mohéli's capital is Fomboni.\n\nMayotte, geologically the oldest of the four islands, is thirty-nine kilometers long and twenty-two kilometers wide, totaling 375 square kilometers, and its highest points are between 500 and 600 meters above sea level. Because of greater weathering of the volcanic rock, the soil is relatively rich in some areas. A well-developed coral reef that encircles much of the island ensures protection for ships and a habitat for fish. Dzaoudzi, capital of the Comoros until 1962 and now Mayotte's administrative center, is situated on a rocky outcropping off the east shore of the main island. Dzaoudzi is linked by a causeway to le Pamanzi, which at ten kilometers in area is the largest of several islets adjacent to Mayotte. Islets are also scattered in the coastal waters of Grande Comore, Anjouan, and Mohéli.\n\nComorian waters are the habitat of the coelacanth, a rare fish with limblike fins and a cartilaginous skeleton, the fossil remains of which date as far back as 400 million years and which was once thought to have become extinct about 70 million years ago. A live specimen was caught in 1938 off southern Africa; other coelacanths have since been found in the vicinity of the Comoro Islands.\n\nSeveral mammals are unique to the islands themselves. Livingstone's fruit bat, although plentiful when discovered by explorer David Livingstone in 1863, has been reduced to a population of about 120, entirely on Anjouan. The world's largest bat, the jet-black Livingstone fruit bat has a wingspan of nearly two meters. A British preservation group sent an expedition to the Comoros in 1992 to bring some of the bats to Britain to establish a breeding population.\n\nA hybrid of the common brown lemur (\"Eulemur fulvus\") originally from Madagascar, was introduced by humans prior to European colonization and is found on Mayotte. The mongoose lemur (\"Eulemur mongoz\"), also introduced from Madagascar by humans, can be found on the islands of Mohéli and Anjouan.\n\n22 species of bird are unique to the archipelago and 17 of these are restricted to the Union of the Comoros. These include the Karthala scops-owl, Anjouan scops-owl and Humblot's flycatcher.\n\nPartly in response to international pressures, Comorians in the 1990s have become more concerned about the environment. Steps are being taken not only to preserve the rare fauna, but also to counteract degradation of the environment, especially on densely populated Anjouan. Specifically, to minimize the cutting down of trees for fuel, kerosene is being subsidized, and efforts are being made to replace the loss of the forest cover caused by ylang-ylang distillation for perfume. The Community Development Support Fund, sponsored by the International Development Association (IDA, a World Bank affiliate) and the Comorian government, is working to improve water supply on the islands as well.\n\nThe climate is marine tropical, with two seasons: hot and humid from November to April, the result of the northeastern monsoon, and a cooler, drier season the rest of the year. Average monthly temperatures range from along the coasts. Although the average annual precipitation is , water is a scarce commodity in many parts of the Comoros. Mohéli and Mayotte possess streams and other natural sources of water, but Grande Comore and Anjouan, whose mountainous landscapes retain water poorly, are almost devoid of naturally occurring running water. Cyclones, occurring during the hot and wet season, can cause extensive damage, especially in coastal areas. On the average, at least twice each decade houses, farms, and harbor facilities are devastated by these great storms.\n\nThis is a list of the extreme points of the Comoros, the points that are farther north, south, east or west than any other location. This list excludes the French-administered island of Mayotte which is claimed by the Comorian government.\n\n\nArea:\n2,235 km\n\nCoastline:\n340 km\n\nClimate:\ntropical marine; rainy season (November to May)\n\nTerrain:\nvolcanic islands, interiors vary from steep mountains to low hills\n\nElevation extremes:\n\"lowest point:\"\nIndian Ocean 0 m\n\"highest point:\"\nKarthala 2,360 m\n\nNatural resources:\nfish\n\nLand use:\n\"arable land:\"\n47.29%\n\"permanent crops:\"\n29.55%\n\"other:\"\n23.16% (2012 est.)\n\nIrrigated land:\n1.3 km (2003)\n\nTotal renewable water resources:\n1.2 km (2011)\n\nFreshwater withdrawal (domestic/industrial/agricultural):\n\"total:\"\n0.01 km/yr (48%/5%/47%)\n\"per capital:\"\n16.86 m/yr (1999)\n\nNatural hazards:\ncyclones possible during rainy season (December to April); volcanic activity on Grand Comore\n\nEnvironmental - current issues:\nsoil degradation and erosion results from crop cultivation on slopes without proper terracing; deforestation\n"}
{"id": "6002", "url": "https://en.wikipedia.org/wiki?curid=6002", "title": "Demographics of the Comoros", "text": "Demographics of the Comoros\n\nThe Comorians inhabiting Grande Comore, Anjouan, and Mohéli (86% of the population) share African-Arab origins. Islam is the dominant religion, and Quranic schools for children reinforce its influence. Although Islamic culture is firmly established throughout, a small minority are Christian.\n\nThe most common language is Comorian, related to Swahili. French and Arabic also are spoken. About 89% of the population is literate.\n\nThe Comoros have had seven censuses since World War II:\n\nPopulation density figures conceal a great disparity between the republic's most crowded island, Nzwani, which had a density of 470 persons per square kilometer in 1991; Ngazidja, which had a density of 250 persons per square kilometer in 1991; and Mwali, where the 1991 population density figure was 120 persons per square kilometer. Overall population density increased to about 285 persons per square kilometer by 1994.\nBy comparison, estimates of the population density per square kilometer of the Indian Ocean's other island microstates ranged from 241 (Seychelles) to 690 (Maldives) in 1993. Given the rugged terrain of Ngazidja and Nzwani, and the dedication of extensive tracts to agriculture on all three islands, population pressures on the Comoros are becoming increasingly critical.\n\nThe age structure of the population of the Comoros is similar to that of many developing countries, in that the republic has a very large proportion of young people. In 1989, 46.4 percent of the population was under fifteen years of age, an above-average proportion even for sub-Saharan Africa. The population's rate of growth was a relatively high 3.5 percent per annum in the mid 1980s, up substantially from 2.0 percent in the mid-1970s and 2.1 percent in the mid-1960s.\n\nIn 1983 the Abdallah regime borrowed US$2.85 million from the International Development Association to devise a national family planning program. However, Islamic reservations about contraception made forthright advocacy and implementation of birth control programs politically hazardous, and consequently little was done in the way of public policy.\n\nThe Comorian population has become increasingly urbanized in recent years. In 1991 the percentage of Comorians residing in cities and towns of more than 5,000 persons was about 30 percent, up from 25 percent in 1985 and 23 percent in 1980. The Comoros' largest cities were the capital, Moroni, with about 30,000 people, and the port city of Mutsamudu, on the island of Nzwani, with about 20,000 people.\n\nMigration among the various islands is important. Natives of Nzwani have settled in significant numbers on less crowded Mwali, causing some social tensions, and many Nzwani also migrate to Maore. In 1977 Maore expelled peasants from Ngazidja and Nzwani who had recently settled in large numbers on the island. Some were allowed to reenter starting in 1981 but solely as migrant labor.\n\nThe number of Comorians living abroad has been estimated at between 80,000 and 100,000; during the colonial period, most of them lived in Tanzania, Madagascar, and other parts of Southeast Africa. The number of Comorians residing in Madagascar was drastically reduced after anti-Comorian rioting in December 1976 in Mahajanga, in which at least 1,400 Comorians were killed. As many as 17,000 Comorians left Madagascar to seek refuge in their native land in 1977 alone. About 100,000 Comorians live in France; many of them had gone there for a university education and never returned. Small numbers of Indians, Malagasy, South Africans, and Europeans (mostly French) live on the islands and play an important role in the economy. Most French left after independence in 1975.\n\nNumbers are in thousands. UN medium variant projections.\n\nTotal Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):\n\nStructure of the population (DHS 2012) (Males 11 088, Females 12 284 = 23 373) :\n\nFertility data as of 2012 (DHS Program):\n\nThe following demographic statistics are from the CIA World Factbook, unless otherwise indicated.\n\n"}
{"id": "6003", "url": "https://en.wikipedia.org/wiki?curid=6003", "title": "Politics of the Comoros", "text": "Politics of the Comoros\n\nPolitics of the Union of the Comoros takes place in a framework of a federal presidential republic, whereby the President of the Comoros is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Federal legislative power is vested in both the government and parliament.\n\nAs of 2008, Comoros and Mauritania were considered by US-based organization Freedom House as the only real “electoral democracies” of the Arab World.\n\nThe Union of the Comoros, known as the Islamic Federal Republic of the Comoros until 2003, is ruled by Ahmed Abdallah Sambi. The political situation in Comoros has been extremely fluid since the country's independence in 1975, subject to the volatility of coups and political insurrection. Colonel Azali Assoumani seized power in a bloodless coup in April 1999, overthrowing Interim President Tadjidine Ben Said Massounde, who himself had held the office since the death of democratically elected President Mohamed Taki Abdoulkarim in November, 1998. \n\nIn May 1999, Azali decreed a constitution that gave him both executive and legislative powers. Bowing somewhat to international criticism, Azali appointed a civilian Prime Minister, Bainrifi Tarmidi, in December 1999; however, Azali retained the mantle of Head of State and army Commander. In December 2000, Azali named a new civilian Prime Minister, Hamada Madi, and formed a new civilian Cabinet. When Azali took power he also pledged to step down in April 2000 and relinquish control to a democratically elected president—a pledge with mixed results.\n\nIn a separate nod to pressure to restore civilian rule, the government organized several committees to compose a new constitution, including the August 2000 National Congress and November 2000 Tripartite Commission. The opposition parties initially refused to participate in the Tripartite Commission, but on 17 February, representatives of the government, the Anjouan separatists, the political opposition, and civil society organizations signed a \"Framework Accord for Reconciliation in Comoros,\" brokered by the Organization for African Unity\n\nThe accord called for the creation of a new Tripartite Commission for National Reconciliation to develop a \"New Comorian Entity\" with a new constitution. The new federal Constitution came into effect in 2002; it included elements of consociationalism, including a presidency that rotates every four years among the islands and extensive autonomy for each island. Presidential elections were held in 2002, at which Azali Assoumani was elected President. In April 2004 legislative elections were held, completing the implementation of the new constitution. \n\nThe new Union of the Comoros consists of three islands, Grande Comore, Anjouan and Mohéli. Each island has a president, who shares the presidency of the Union on a rotating basis. The president and his vice-presidents are elected for a term of four years. The constitution states that, \"the islands enjoy financial autonomy, freely draw up and manage their budgets\".\n\nPresident Assoumani Azali of Grande Comore is the first Union president. President Mohamed Bacar of Anjouan formed his 13-member government at the end of April, 2003.\n\nOn 15 May 2006, Ahmed Abdallah Sambi, a cleric and successful businessman educated in Iran, Saudi Arabia and Sudan, was declared the winner of elections for President of the Republic. He is considered a moderate Islamist and is called Ayatollah by his supporters. He beat out retired French air force officer Mohamed Djaanfari and long-time politician Ibrahim Halidi, whose candidacy was backed by Azali Assoumani, the outgoing president.\n\nA referendum took place on May 16, 2009 to decide whether to cut down the government's unwieldy political bureaucracy. 52.7% of those eligible voted, and 93.8% of votes were cast in approval of the referendum. The referendum would cause each island's president to become a governor and the ministers to become councilors.\n\nThe constitution gives Grande Comore, Anjouan and Mohéli the right to govern most of their own affairs with their own presidents, except the activities assigned to the Union of the Comoros like Foreign Policy, Defense, Nationality, Banking and others. Comoros considers Mayotte, an overseas collectivity of France, to be part of its territory, with an autonomous status \n\nAs of 2011, the three autonomous islands are subdivided into 16 prefectures, 54 communes, and 318 villes or villages.\n\nThe federal presidency is rotated between the islands' presidents.\nThe Union of Comoros abolished the position of Prime Minister.\n\nThe Assembly of the Union has 33 seats, 18 elected in single seat constituencies and 15 representatives of the regional assemblies.\n\nThe Supreme Court or Cour Supreme, has two members appointed by the president, two members elected by the Federal Assembly, one by the Council of each island, and former presidents of the republic.\n\nThe Comoros are member of the ACCT, ACP, AfDB, AMF, African Union, FAO, G-77, IBRD, ICAO, ICCt (signatory), ICRM, IDA, IDB, IFAD, IFC, IFRCS, ILO, IMF, InOC, Interpol, IOC, ITU, LAS, NAM, OIC, OPCW (signatory), United Nations, UNCTAD, UNESCO, UNIDO, UPU, WCO, WHO, WMO.\n"}
{"id": "6005", "url": "https://en.wikipedia.org/wiki?curid=6005", "title": "Telecommunications in the Comoros", "text": "Telecommunications in the Comoros\n\nIn large part thanks to international aid programs, Moroni has international telecommunications service. Telephone service, however, is largely limited to the islands' few towns.\n\nCommunications in the Comoros\n\nTelephones - main lines in use:\n5,000 (1995)\n\nTelephones - mobile cellular:\n0 (1995)\n\nTelephone system:\nsparse system of microwave radio relay and HF radiotelephone communication stations\n<br>\"domestic:\"\nHF radiotelephone communications and microwave radio relay<br>\nCMDA mobile network (Huri, operated by Comores Telecom)\n<br>\"international:\"\nHF radiotelephone communications to Madagascar and Réunion\n\nRadio broadcast stations:\nAM 1, FM 2, shortwave 1 (1998)\n\nRadios:\n90,000 (1997)\n\nTelevision broadcast stations:\n0 (1998)\n\nTelevisions:\n1,000 (1997)\n\nInternet Service Providers (ISPs): 1 (1999)\n\nCountry code (Top-level domain): KM\n\nIn October 2011 the State of Qatar launched a special program for the construction of a wireless network to interconnect the three islands of the archipelago, by means of low cost, repeatable technology. The project has been developed by Qatar University and Politecnico di Torino, under the supervision of prof. Mazen Hasna and prof. Daniele Trinchero, with a major participation of local actors (Comorian Government, NRTIC, University of the Comoros). The project has been referred as an example of technology transfer and Sustainable Inclusion in developing countries\n"}
{"id": "6006", "url": "https://en.wikipedia.org/wiki?curid=6006", "title": "Transport in the Comoros", "text": "Transport in the Comoros\n\nThere are a number of systems of transport in the Comoros. The Comoros possesses of road, of which are paved. It has three seaports: Fomboni, Moroni and Moutsamoudou, but does not have a merchant marine, and no longer has any railway network. It has four airports, all with paved runways, one with runways over long, with the others having runways shorter than .\n\nThe isolation of the Comoros had made air traffic a major means of transportation. One of President Abdallah's accomplishments was to make the Comoros more accessible by air. During his administration, he negotiated agreements to initiate or enhance commercial air links with Tanzania and Madagascar. The Djohar regime reached an agreement in 1990 to link Moroni and Brussels by air. By the early 1990s, commercial flights connected the Comoros with France, Mauritius, Kenya, South Africa, Tanzania, and Madagascar. The national airline was Air Comores. Daily flights linked the three main islands, and air service was also available to Mahoré; each island had airstrips. In 1986 the republic received a grant from the French government's CCCE to renovate and expand Hahaya airport, near Moroni. Because of the absence of scheduled sea transport between the islands, nearly all interisland passenger traffic is by air.\n\nMore than 99% of freight is transported by sea. Both Moroni on Njazidja and Mutsamudu on Nzwani have artificial harbors. There is also a harbor at Fomboni, on Mwali. Despite extensive internationally financed programs to upgrade the harbors at Moroni and Mutsamudu, by the early 1990s only Mutsamudu was operational as a deepwater facility. Its harbor could accommodate vessels of up to eleven meters' draught. At Moroni, ocean-going vessels typically lie offshore and are loaded or unloaded by smaller craft, a costly and sometimes dangerous procedure. Most freight continues to be sent to Kenya, Reunion, or Madagascar for transshipment to the Comoros. Use of Comoran ports is further restricted by the threat of cyclones from December through March. The privately operated Comoran Navigation Company (\"Société Comorienne de Navigation\") is based in Moroni, and provides services to Madagascar.\n\nRoads serve the coastal areas, rather than the interior, and the mountainous terrain makes surface travel difficult.\n\n\n"}
{"id": "6007", "url": "https://en.wikipedia.org/wiki?curid=6007", "title": "Foreign relations of the Comoros", "text": "Foreign relations of the Comoros\n\nIn November 1975, Comoros became the 143rd member of the United Nations. The new nation was defined as consisting of the entire archipelago, despite the fact that France maintains control over Mayotte.\n\nComoros also is a member of the African Union, the Arab League, the European Development Fund, the World Bank, the International Monetary Fund, the Indian Ocean Commission, and the African Development Bank.\n\nThe government fostered close relationships with the more conservative (and oil-rich) Arab states, such as Saudi Arabia and Kuwait. It frequently received aid from those countries and the regional financial institutions they influenced, such as the Arab Bank for Economic Development in Africa and the Arab Fund for Economic and Social Development. In October 1993, Comoros joined the League of Arab States, after having been rejected when it applied for membership initially in 1977.\n\nRegional relations generally were good. In 1985 Madagascar, Mauritius, and Seychelles agreed to admit Comoros as the fourth member of the Indian Ocean Commission (IOC), an organization established in 1982 to encourage regional cooperation. In 1993 Mauritius and Seychelles had two of the five embassies in Moroni, and Mauritius and Madagascar were connected to the republic by regularly scheduled commercial flights.\n\nIn November 1975, Comoros became the 143d member of the UN. In the 1990s, the republic continued to represent Mahoré in the UN. Comoros was also a member of the OAU, the EDF, the World Bank, the IMF, the IOC, and the African Development Bank.\n\nComoros thus cultivated relations with various nations, both East and West, seeking to increase trade and obtain financial assistance. In 1994, however, it was increasingly facing the need to control its expenditures and reorganize its economy so that it would be viewed as a sounder recipient of investment. Comoros also confronted domestically the problem of the degree of democracy the government was prepared to grant to its citizens, a consideration that related to its standing in the world community.\n\n"}
{"id": "6008", "url": "https://en.wikipedia.org/wiki?curid=6008", "title": "Military of the Comoros", "text": "Military of the Comoros\n\nThe Comorian Security Force (French \"Armée nationale de développement\") consist of a small standing army and a 500-member police force, as well as a 500-member defense force. A defense treaty with France provides naval resources for protection of territorial waters, training of Comorian military personnel, and air surveillance. France maintains a small troop presence in the Comoros at government request. France maintains a small maritime base and a Foreign Legion Detachment (DLEM) on Mayotte.\n\n\nThe Comorian Security Force operates only 4 aircraft.\nIn addition to the CSF, the Police Force operates a further 6 aircraft fleet available for paramilitary duties.\n\n"}
{"id": "6010", "url": "https://en.wikipedia.org/wiki?curid=6010", "title": "Computer worm", "text": "Computer worm\n\nA computer worm is a standalone malware computer program that replicates itself in order to spread to other computers. Often, it uses a computer network to spread itself, relying on security failures on the target computer to access it. Worms almost always cause at least some harm to the network, even if only by consuming bandwidth, whereas viruses almost always corrupt or modify files on a targeted computer.\n\nMany worms that have been created are designed only to spread, and do not attempt to change the systems they pass through. However, as the Morris worm and Mydoom showed, even these \"payload-free\" worms can cause major disruption by increasing network traffic and other unintended effects.\n\nThe actual term \"worm\" was first used in John Brunner's 1975 novel, \"The Shockwave Rider\". In that novel, Nichlas Haflinger designs and sets off a data-gathering worm in an act of revenge against the powerful men who run a national electronic information web that induces mass conformity. \"You have the biggest-ever worm loose in the net, and it automatically sabotages any attempt to monitor it... There's never been a worm with that tough a head or that long a tail!\"\n\nOn November 2, 1988, Robert Tappan Morris, a Cornell University computer science graduate student, unleashed what became known as the Morris worm, disrupting a large number of computers then on the Internet, guessed at the time to be one tenth of all those connected During the Morris appeal process, the U.S. Court of Appeals estimated the cost of removing the virus from each installation was in the range of $200–53,000, and prompting the formation of the CERT Coordination Center and Phage mailing list. Morris himself became the first person tried and convicted under the 1986 Computer Fraud and Abuse Act.\n\nAny code designed to do more than spread the worm is typically referred to as the \"payload\". Typical malicious payloads might delete files on a host system (e.g., the ExploreZip worm), encrypt files in a ransomware attack, or exfiltrate data such as confidential documents or passwords. \n\nProbably the most common payload for worms is to install a backdoor. This allows the computer to be remotely controlled by the worm author as a \"zombie\". Networks of such machines are often referred to as botnets and are very commonly used for a range of malicious purposes, including sending spam or performing DoS attacks.\n\nWorms spread by exploiting vulnerabilities in operating systems.\nVendors with security problems supply regular security updates (see \"Patch Tuesday\"), and if these are installed to a machine then the majority of worms are unable to spread to it. If a vulnerability is disclosed before the security patch released by the vendor, a zero-day attack is possible.\n\nUsers need to be wary of opening unexpected email, and should not run attached files or programs, or visit web sites that are linked to such emails. However, as with the ILOVEYOU worm, and with the increased growth and efficiency of phishing attacks, it remains possible to trick the end-user into running malicious code.\n\nAnti-virus and anti-spyware software are helpful, but must be kept up-to-date with new pattern files at least every few days. The use of a firewall is also recommended.\n\nIn the April–June 2008 issue of \"IEEE Transactions on Dependable and Secure Computing\", computer scientists described a new and potentially effective way to combat internet worms. The researchers discovered how to contain worms that scanned the Internet randomly, looking for vulnerable hosts to infect. They found that the key was to use software to monitor the number of scans that machines on a network send out. When a machine started to send out too many scans, it was a sign that it has been infected, which allowed administrators to take it off line and check it for malware. In addition, machine learning techniques can be used to detect new worms, by analyzing the behavior of the suspected computer.\n\nUsers can minimize the threat posed by worms by keeping their computers' operating system and other software up to date, avoiding opening unrecognized or unexpected emails and running firewall and antivirus software.\n\nMitigation techniques include:\n\n\nBeginning with the very first research into worms at Xerox PARC, there have been attempts to create useful worms. Those worms allowed testing by John Shoch and Jon Hupp of the Ethernet principles on their network of Xerox Alto computers. The Nachi family of worms tried to download and install patches from Microsoft's website to fix vulnerabilities in the host system—by exploiting those same vulnerabilities. In practice, although this may have made these systems more secure, it generated considerable network traffic, rebooted the machine in the course of patching it, and did its work without the consent of the computer's owner or user. Regardless of their payload or their writers' intentions, most security experts regard all worms as malware.\n\nSeveral worms, like XSS worms, have been written to research how worms spread. For example, the effects of changes in social activity or user behavior. One study proposed what seems to be the first computer worm that operates on the second layer of the OSI model (Data link Layer), it utilizes topology information such as Content-addressable memory (CAM) tables and Spanning Tree information stored in switches to propagate and probe for vulnerable nodes until the enterprise network is covered.\n\n\n"}
{"id": "6011", "url": "https://en.wikipedia.org/wiki?curid=6011", "title": "Chomsky hierarchy", "text": "Chomsky hierarchy\n\nIn the formal languages of computer science and linguistics, the Chomsky hierarchy (occasionally referred to as Chomsky–Schützenberger hierarchy) is a containment hierarchy of classes of formal grammars.\nThis hierarchy of grammars was described by Noam Chomsky in 1956. It is also named after Marcel-Paul Schützenberger, who played a crucial role in the development of the theory of formal languages.\n\nA formal grammar of this type consists of a finite set of \"production rules\" (\"left-hand side\" → \"right-hand side\"), where each side consists of a finite sequence of the following symbols:\n\nA formal grammar provides an axiom schema for (or \"generates\") a \"formal language\", which is a (usually infinite) set of finite-length sequences of symbols that may be constructed by applying production rules to another sequence of symbols (which initially contains just the start symbol). A rule may be applied by replacing an occurrence of the symbols on its left-hand side with those that appear on its right-hand side. A sequence of rule applications is called a \"derivation\". Such a grammar defines the formal language: all words consisting solely of terminal symbols which can be reached by a derivation from the start symbol.\n\nNonterminals are often represented by uppercase letters, terminals by lowercase letters, and the start symbol by . For example, the grammar with terminals , nonterminals , production rules\nand start symbol , defines the language of all words of the form formula_1 (i.e. copies of followed by copies of ).\n\nThe following is a simpler grammar that defines the same language: \nTerminals , Nonterminals , Start symbol , Production rules\n\nAs another example, a grammar for a toy subset of English language is given by:\nand start symbol . An example derivation is\nOther sequences that can be derived from this grammar are: \"\"ideas hate great linguists\"\", and \"\"ideas generate\"\". While these sentences are nonsensical, they are syntactically correct. A syntactically incorrect sentence ( e.g. \"\"ideas ideas great hate\"\") cannot be derived from this grammar. See \"Colorless green ideas sleep furiously\" for a similar example given by Chomsky in 1957; see Phrase structure grammar and Phrase structure rules for more natural language examples and the problems of formal grammar in that area.\n\nThe following table summarizes each of Chomsky's four types of grammars, the class of language it generates, the type of automaton that recognizes it, and the form its rules must have.\n\nNote that the set of grammars corresponding to recursive languages is not a member of this hierarchy; these would be properly between Type-0 and Type-1.\n\nEvery regular language is context-free, every context-free language is context-sensitive, every context-sensitive language is recursive and every recursive language is recursively enumerable. These are all proper inclusions, meaning that there exist recursively enumerable languages that are not context-sensitive, context-sensitive languages that are not context-free and context-free languages that are not regular.\n\nType-0 grammars include all formal grammars. They generate exactly all languages that can be recognized by a Turing machine. These languages are also known as the \"recursively enumerable\" or \"Turing-recognizable\" languages. Note that this is different from the recursive languages, which can be \"decided\" by an always-halting Turing machine.\n\nType-1 grammars generate the context-sensitive languages. These grammars have rules of the form formula_2 with formula_3 a nonterminal and formula_4, formula_5 and formula_6 strings of terminals and/or nonterminals. The strings formula_4 and formula_5 may be empty, but formula_6 must be nonempty. The rule formula_10 is allowed if formula_11 does not appear on the right side of any rule. The languages described by these grammars are exactly all languages that can be recognized by a linear bounded automaton (a nondeterministic Turing machine whose tape is bounded by a constant times the length of the input.)\n\nType-2 grammars generate the context-free languages. These are defined by rules of the form formula_12 with formula_3 being a nonterminal and formula_6 being a string of terminals and/or nonterminals. These languages are exactly all languages that can be recognized by a non-deterministic pushdown automaton. Context-free languages—or rather its subset of deterministic context-free language—are the theoretical basis for the phrase structure of most programming languages, though their syntax also includes context-sensitive name resolution due to declarations and scope. Often a subset of grammars is used to make parsing easier, such as by an LL parser.\n\nType-3 grammars generate the regular languages. Such a grammar restricts its rules to a single nonterminal on the left-hand side and a right-hand side consisting of a single terminal, possibly followed by a single nonterminal (right regular). Alternatively, the right-hand side of the grammar can consist of a single terminal, possibly preceded by a single nonterminal (left regular). These generate the same languages. However, if left-regular rules and right-regular rules are combined, the language need no longer be regular. The rule formula_10 is also allowed here if formula_11 does not appear on the right side of any rule. These languages are exactly all languages that can be decided by a finite state automaton. Additionally, this family of formal languages can be obtained by regular expressions. Regular languages are commonly used to define search patterns and the lexical structure of programming languages.\n\n"}
{"id": "6013", "url": "https://en.wikipedia.org/wiki?curid=6013", "title": "CRT", "text": "CRT\n\nCRT may refer to:\n\n\n\n\n\n\n\n"}
{"id": "6014", "url": "https://en.wikipedia.org/wiki?curid=6014", "title": "Cathode ray tube", "text": "Cathode ray tube\n\nThe cathode ray tube (CRT) is a vacuum tube that contains one or more electron guns and a phosphorescent screen, and is used to display images. It modulates, accelerates, and deflects electron beam(s) onto the screen to create the images. The images may represent electrical waveforms (oscilloscope), pictures (television, computer monitor), radar targets, or others. CRTs have also been used as memory devices, in which case the visible light emitted from the fluorescent material (if any) is not intended to have significant meaning to a visual observer (though the visible pattern on the tube face may cryptically represent the stored data).\n\nIn television sets and computer monitors, the entire front area of the tube is scanned repetitively and systematically in a fixed pattern called a raster. An image is produced by controlling the intensity of each of the three electron beams, one for each additive primary color (red, green, and blue) with a video signal as a reference. In all modern CRT monitors and televisions, the beams are bent by \"magnetic deflection\", a varying magnetic field generated by coils and driven by electronic circuits around the neck of the tube, although electrostatic deflection is commonly used in oscilloscopes, a type of electronic test instrument.\n\nA CRT is constructed from a glass envelope which is large, deep (i.e., long from front screen face to rear end), fairly heavy, and relatively fragile. The interior of a CRT is evacuated to approximately to , evacuation being necessary to facilitate the free flight of electrons from the gun(s) to the tube's face. That it is evacuated makes handling an intact CRT potentially dangerous due to the risk of breaking the tube and causing a violent implosion that can hurl shards of glass at great velocity. As a matter of safety, the face is typically made of thick lead glass so as to be highly shatter-resistant and to block most X-ray emissions, particularly if the CRT is used in a consumer product.\n\nSince the late 2000s, CRTs have been largely superseded by newer \"flat panel\" display technologies such as LCD, plasma display, and OLED displays, which in the case of LCD and OLED displays have lower manufacturing costs and power consumption, as well as significantly less weight and bulk. Flat panel displays can also be made in very large sizes; whereas 38\" to 40\" was about the largest size of a CRT television, flat panels are available in 60\" and larger sizes.\n\nCathode rays were discovered by Johann Hittorf in 1869 in primitive Crookes tubes. He observed that some unknown rays were emitted from the cathode (Negative electrode) which could cast shadows on the glowing wall of the tube, indicating the rays were traveling in straight lines. In 1890, Arthur Schuster demonstrated cathode rays could be deflected by electric fields, and William Crookes showed they could be deflected by magnetic fields. In 1897, J. J. Thomson succeeded in measuring the mass of cathode rays, showing that they consisted of negatively charged particles smaller than atoms, the first \"subatomic particles\", which were later named \"electrons\". The earliest version of the CRT was known as the \"Braun tube\", invented by the German physicist Ferdinand Braun in 1897. It was a cold-cathode diode, a modification of the Crookes tube with a phosphor-coated screen.\n\nThe first cathode ray tube to use a hot cathode was developed by John B. Johnson (who gave his name to the term Johnson noise) and Harry Weiner Weinhart of Western Electric, and became a commercial product in 1922.\n\nIn 1925, Kenjiro Takayanagi demonstrated a CRT television that received images with a 40-line resolution. By 1927, he improved the resolution to 100 lines, which was unrivaled until 1931. By 1928, he was the first to transmit human faces in half-tones on a CRT display. By 1935, he had invented an early all-electronic CRT television.\n\nIt was named in 1929 by inventor Vladimir K. Zworykin, who was influenced by Takayanagi's earlier work. RCA was granted a trademark for the term (for its cathode ray tube) in 1932; it voluntarily released the term to the public domain in 1950.\n\nThe first commercially made electronic television sets with cathode ray tubes were manufactured by Telefunken in Germany in 1934.\n\nIn oscilloscope CRTs, electrostatic deflection is used, rather than the magnetic deflection commonly used with television and other large CRTs. The beam is deflected horizontally by applying an electric field between a pair of plates to its left and right, and vertically by applying an electric field to plates above and below. Televisions use magnetic rather than electrostatic deflection because the deflection plates obstruct the beam when the deflection angle is as large as is required for tubes that are relatively short for their size.\n\nVarious phosphors are available depending upon the needs of the measurement or display application. The brightness, color, and persistence of the illumination depends upon the type of phosphor used on the CRT screen. Phosphors are available with persistences ranging from less than one microsecond to several seconds. For visual observation of brief transient events, a long persistence phosphor may be desirable. For events which are fast and repetitive, or high frequency, a short-persistence phosphor is generally preferable.\n\nWhen displaying fast one-shot events, the electron beam must deflect very quickly, with few electrons impinging on the screen, leading to a faint or invisible image on the display. Oscilloscope CRTs designed for very fast signals can give a brighter display by passing the electron beam through a micro-channel plate just before it reaches the screen. Through the phenomenon of secondary emission, this plate multiplies the number of electrons reaching the phosphor screen, giving a significant improvement in writing rate (brightness) and improved sensitivity and spot size as well.\n\nMost oscilloscopes have a graticule as part of the visual display, to facilitate measurements. The graticule may be permanently marked inside the face of the CRT, or it may be a transparent external plate made of glass or acrylic plastic. An internal graticule eliminates parallax error, but cannot be changed to accommodate different types of measurements. Oscilloscopes commonly provide a means for the graticule to be illuminated from the side, which improves its visibility.\n\nThese are found in \"analog phosphor storage oscilloscopes\". These are distinct from \"digital storage oscilloscopes\" which rely on solid state digital memory to store the image.\n\nWhere a single brief event is monitored by an oscilloscope, such an event will be displayed by a conventional tube only while it actually occurs. The use of a long persistence phosphor may allow the image to be observed after the event, but only for a few seconds at best. This limitation can be overcome by the use of a direct view storage cathode ray tube (storage tube). A storage tube will continue to display the event after it has occurred until such time as it is erased. A storage tube is similar to a conventional tube except that it is equipped with a metal grid coated with a dielectric layer located immediately behind the phosphor screen. An externally applied voltage to the mesh initially ensures that the whole mesh is at a constant potential. This mesh is constantly exposed to a low velocity electron beam from a 'flood gun' which operates independently of the main gun. This flood gun is not deflected like the main gun but constantly 'illuminates' the whole of the storage mesh. The initial charge on the storage mesh is such as to repel the electrons from the flood gun which are prevented from striking the phosphor screen.\n\nWhen the main electron gun writes an image to the screen, the energy in the main beam is sufficient to create a 'potential relief' on the storage mesh. The areas where this relief is created no longer repel the electrons from the flood gun which now pass through the mesh and illuminate the phosphor screen. Consequently, the image that was briefly traced out by the main gun continues to be displayed after it has occurred. The image can be 'erased' by resupplying the external voltage to the mesh restoring its constant potential. The time for which the image can be displayed was limited because, in practice, the flood gun slowly neutralises the charge on the storage mesh. One way of allowing the image to be retained for longer is temporarily to turn off the flood gun. It is then possible for the image to be retained for several days. The majority of storage tubes allow for a lower voltage to be applied to the storage mesh which slowly restores the initial charge state. By varying this voltage a variable persistence is obtained. Turning off the flood gun and the voltage supply to the storage mesh allows such a tube to operate as a conventional oscilloscope tube.\n\nThe Williams tube or Williams-Kilburn tube was a cathode ray tube used to electronically store binary data. It was used in computers of the 1940s as a random-access digital storage device. In contrast to other CRTs in this article, the Williams tube was not a display device, and in fact could not be viewed since a metal plate covered its screen.\n\nColor tubes use three different phosphors which emit red, green, and blue light respectively. They are packed together in stripes (as in aperture grille designs) or clusters called \"triads\" (as in shadow mask CRTs). Color CRTs have three electron guns, one for each primary color, arranged either in a straight line or in an equilateral triangular configuration (the guns are usually constructed as a single unit). (The triangular configuration is often called \"delta-gun\", based on its relation to the shape of the Greek letter delta Δ.) A grille or mask absorbs the electrons that would otherwise hit the wrong phosphor. A shadow mask tube uses a metal plate with tiny holes, placed so that the electron beam only illuminates the correct phosphors on the face of the tube; the holes are tapered so that the electrons that strike the inside of any hole will be reflected back, if they are not absorbed (e.g. due to local charge accumulation), instead of bouncing through the hole to strike a random (wrong) spot on the screen. Another type of color CRT uses an aperture grille of tensioned vertical wires to achieve the same result.\n\nDue to limitations in the dimensional precision with which CRTs can be manufactured economically, it has not been practically possible to build color CRTs in which three electron beams could be aligned to hit phosphors of respective color in acceptable coordination, solely on the basis of the geometric configuration of the electron gun axes and gun aperture positions, shadow mask apertures, etc. The shadow mask ensures that one beam will only hit spots of certain colors of phosphors, but minute variations in physical alignment of the internal parts among individual CRTs will cause variations in the exact alignment of the beams through the shadow mask, allowing some electrons from, for example, the red beam to hit, say, blue phosphors, unless some individual compensation is made for the variance among individual tubes.\n\nColor convergence and color purity are two aspects of this single problem. Firstly, for correct color rendering it is necessary that regardless of where the beams are deflected on the screen, all three hit the same spot (and nominally pass through the same hole or slot) on the shadow mask. This is called convergence. More specifically, the convergence at the center of the screen (with no deflection field applied by the yoke) is called static convergence, and the convergence over the rest of the screen area is called dynamic convergence. The beams may converge at the center of the screen and yet stray from each other as they are deflected toward the edges; such a CRT would be said to have good static convergence but poor dynamic convergence. Secondly, each beam must only strike the phosphors of the\ncolor it is intended to strike and no others. This is called purity. Like convergence, there is static purity and dynamic purity, with the same meanings of \"static\" and \"dynamic\" as for convergence. Convergence and purity are distinct parameters; a CRT could have good purity but poor convergence, or vice versa. Poor convergence causes color \"shadows\" or \"ghosts\" along displayed edges and contours, as if the image on the screen were intaglio printed with poor registration. Poor purity causes objects on the screen to appear off-color while their edges remain sharp. Purity and convergence problems can occur at the same time, in the same or different areas of the screen or both over the whole screen, and either uniformly or to greater or lesser degrees over different parts of the screen.\n\nThe solution to the static convergence and purity problems is a set of color alignment magnets installed around the neck of the CRT. These movable weak permanent magnets are usually mounted on the back end of the deflection yoke assembly and are set at the factory to compensate for any static purity and convergence errors that are intrinsic to the unadjusted tube. Typically there are two or three pairs of two magnets in the form of rings made of plastic impregnated with a magnetic material, with their magnetic fields parallel to the planes of the magnets, which are perpendicular to the electron gun axes. Each pair of magnetic rings forms a single effective magnet whose field vector can be fully and freely adjusted (in both direction and magnitude). By rotating a pair of magnets relative to each other, their relative field alignment can be varied, adjusting the effective field strength of the pair. (As they rotate relative to each other, each magnet's field can be considered to have two opposing components at right angles, and these four components [two each for two magnets] form two pairs, one pair reinforcing each other and the other pair opposing and canceling each other. Rotating away from alignment, the magnets' mutually reinforcing field components decrease as they are traded for increasing opposed, mutually cancelling components.) By rotating a pair of magnets together, preserving the relative angle between them, the direction of their collective magnetic field can be varied. Overall, adjusting all of the convergence/purity magnets allows a finely tuned slight electron beam deflection or lateral offset to be applied, which compensates for minor static convergence and purity errors intrinsic to the uncalibrated tube. Once set, these magnets are usually glued in place, but normally they can be freed and readjusted in the field (e.g. by a TV repair shop) if necessary.\n\nOn some CRTs, additional fixed adjustable magnets are added for dynamic convergence or dynamic purity at specific points on the screen, typically near the corners or edges. Further adjustment of dynamic convergence and purity typically cannot be done passively, but requires active compensation circuits.\n\nDynamic color convergence and purity are one of the main reasons why until late in their history, CRTs were long-necked (deep) and had biaxially curved faces; these geometric design characteristics are necessary for intrinsic passive dynamic color convergence and purity. Only starting around the 1990s did sophisticated active dynamic convergence compensation circuits become available that made short-necked and flat-faced CRTs workable. These active compensation circuits use the deflection yoke to finely adjust beam deflection according to the beam target location. The same techniques (and major circuit components) also make possible the adjustment of display image rotation, skew, and other complex raster geometry parameters through electronics under user control.\n\nIf the shadow mask becomes magnetized, its magnetic field deflects the electron beams passing through it, causing color purity distortion as the beams bend through the mask holes and hit some phosphors of a color other than that which they are intended to strike; e.g. some electrons from the red beam may hit blue phosphors, giving pure red parts of the image a magenta tint. (Magenta is the additive combination of red and blue.) This effect is localized to a specific area of the screen if the magnetization of the shadow mask is localized. Therefore, it is important that the shadow mask is unmagnetized. (A magnetized aperture grille has a similar effect, and everything stated in this subsection about shadow masks applies as well to aperture grilles.)\n\nMost color CRT displays, i.e. television sets and computer monitors, each have a built-in degaussing (demagnetizing) circuit, the primary component of which is a degaussing coil which is mounted around the perimeter of the CRT face inside the bezel. Upon power-up of the CRT display, the degaussing circuit produces a brief, alternating current through the degaussing coil which smoothly decays in strength (fades out) to zero over a period of a few seconds, producing a decaying alternating magnetic field from the coil. This degaussing field is strong enough to remove shadow mask magnetization in most cases. In unusual cases of strong magnetization where the internal degaussing field is not sufficient, the shadow mask may be degaussed externally with a stronger portable degausser or demagnetizer. However, an excessively strong magnetic field, whether alternating or constant, may mechanically deform (bend) the shadow mask, causing a permanent color distortion on the display which looks very similar to a magnetization effect.\n\nThe degaussing circuit is often built of a thermo-electric (not electronic) device containing a small ceramic heating element and a positive thermal coefficient (PTC) resistor, connected directly to the switched AC power line with the resistor in series with the degaussing coil. When the power is switched on, the heating element heats the PTC resistor, increasing its resistance to a point where degaussing current is minimal, but not actually zero. In older CRT displays, this low-level current (which produces no significant degaussing field) is sustained along with the action of the heating element as long as the display remains switched on. To repeat a degaussing cycle, the CRT display must be switched off and left off for at least several seconds to reset the degaussing circuit by allowing the PTC resistor to cool to the ambient temperature; switching the display-off and immediately back on will result in a weak degaussing cycle or effectively no degaussing cycle.\n\nThis simple design is effective and cheap to build, but it wastes some power continuously. Later models, especially Energy Star rated ones, use a relay to switch the entire degaussing circuit on and off, so that the degaussing circuit uses energy only when it is functionally active and needed. The relay design also enables degaussing on user demand through the unit's front panel controls, without switching the unit off and on again. This relay can often be heard clicking off at the end of the degaussing cycle a few seconds after the monitor is turned on, and on and off during a manually initiated degaussing cycle.\n\nVector monitors were used in early computer aided design systems and are in some late-1970s to mid-1980s arcade games such as \"Asteroids\".\nThey draw graphics point-to-point, rather than scanning a raster. Either monochrome or color CRTs can be used in vector displays, and the essential principles of CRT design and operation are the same for either type of display; the main difference is in the beam deflection patterns and circuits.\n\nDot pitch defines the maximum resolution of the display, assuming delta-gun CRTs. In these, as the scanned resolution approaches the dot pitch resolution, moiré appears, as the detail being displayed is finer than what the shadow mask can render. Aperture grille monitors do not suffer from vertical moiré; however, because their phosphor stripes have no vertical detail. In smaller CRTs, these strips maintain position by themselves, but larger aperture-grille CRTs require one or two crosswise (horizontal) support strips.\n\nCRTs have a pronounced triode characteristic, which results in significant gamma (a nonlinear relationship in an electron gun between applied video voltage and beam intensity).\n\nIn better quality old-fashioned tube radio sets, a tuning guide consisting of a phosphor tube was used to aid the tuning adjustment. This was also known as a \"Magic Eye\" or \"Tuning Eye\". Tuning would be adjusted until the width of a radial shadow was minimized. This was used instead of a more expensive electromechanical meter, which later came to be used on higher-end tuners when transistor sets lacked the high voltage required to drive the device. The same type of device was used with tape recorders as a recording level meter, and for various other applications including electrical test equipment.\n\nSome displays for early computers (those that needed to display more text than was practical using vectors, or that required high speed for photographic output) used Charactron CRTs. These incorporate a perforated metal character mask (stencil), which shapes a wide electron beam to form a character on the screen. The system selects a character on the mask using one set of deflection circuits, but that causes the extruded beam to be aimed off-axis, so a second set of deflection plates has to re-aim the beam so it is headed toward the center of the screen. A third set of plates places the character wherever required. The beam is unblanked (turned on) briefly to draw the character at that position. Graphics could be drawn by selecting the position on the mask corresponding to the code for a space (in practice, they were simply not drawn), which had a small round hole in the center; this effectively disabled the character mask, and the system reverted to regular vector behavior. Charactrons had exceptionally long necks, because of the need for three deflection systems.\n\nNimo was the trademark of a family of small specialised CRTs manufactured by Industrial Electronics Engineers. These had 10 electron guns which produced electron beams in the form of digits in a manner similar to that of the charactron. The tubes were either simple single-digit displays or more complex 4- or 6- digit displays produced by means of a suitable magnetic deflection system. Having little of the complexities of a standard CRT, the tube required a relatively simple driving circuit, and as the image was projected on the glass face, it provided a much wider viewing angle than competitive types (e.g., nixie tubes).\n\nFlood beam CRT's are small tubes that are arranged as pixels for large screens like Jumbotrons. The first screen using this technology was introduced by Mitsubishi Electric for the 1980 Major League Baseball All-Star Game. It differs from a normal CRT in that the electron gun within does not produce a focused controllable beam. Instead, electrons are sprayed in a wide cone across the entire front of the phosphor screen, basically making each unit act as a single light bulb. Each one is coated with a red, green or blue phosphor, to make up the color sub-pixels. This technology has largely been replaced with light emitting diode displays. Unfocused and undeflected CRTs were used as grid-controlled stroboscope lamps since 1958.\n\nCRTs with an unphosphored front glass but with fine wires embedded in it were used as electrostatic print heads in the 1960s. The wires would pass the electron beam current through the glass onto a sheet of paper where the desired content was therefore deposited as an electrical charge pattern. The paper was then passed near a pool of liquid ink with the opposite charge. The charged areas of the paper attract the ink and thus form the image.\n\nIn the late 1990s and early 2000s Philips Research Laboratories experimented with a type of thin CRT known as the \"Zeus\" display which contained CRT-like functionality in a flat panel display. The devices were demonstrated but never marketed.\n\nAlthough a mainstay of display technology for decades, CRT-based computer monitors and televisions constitute a dead technology. The demand for CRT screens has dropped precipitously since 2007, and this falloff had accelerated in the last two years of that decade. The rapid advances and falling prices of LCD flat panel technology, first for computer monitors and then for televisions, has been the key factor in the demise of competing display technologies such as CRT, rear-projection, and plasma display.\n\nThe end of most high-end CRT production by around 2010 (including high-end Sony and Mitsubishi product lines) means an erosion of the CRT's capability. In Canada and the United States, the sale and production of high-end CRT TVs (30-inch screens) in these markets had all but ended by 2007. Just a couple of years later, inexpensive combo CRT TVs (20-inch screens with an integrated VHS player) disappeared from discount stores. It has been common to replace CRT-based televisions and monitors in as little as 5–6 years, although they generally are capable of satisfactory performance for a much longer time.\n\nCompanies are responding to this trend. Electronics retailers such as Best Buy have been steadily reducing store spaces for CRTs. In 2005, Sony announced that they would stop the production of CRT computer displays. Samsung did not introduce any CRT models for the 2008 model year at the 2008 Consumer Electronics Show, and on 4 February 2008 Samsung removed their 30\" wide screen CRTs from their North American website and has not replaced them with new models.\n\nHowever, the demise of CRTs has been happening more slowly in the developing world. According to iSupply, production in units of CRTs was not surpassed by LCDs production until 4Q 2007, owing largely to CRT production at factories in China. In the United Kingdom, DSG (Dixons), the largest retailer of domestic electronic equipment, reported that CRT models made up 80–90% of the volume of televisions sold at Christmas 2004 and 15–20% a year later, and that they were expected to be less than 5% at the end of 2006. Dixons ceased selling CRT televisions in 2006.\n\nCRTs, despite recent advances, have remained relatively heavy and bulky and take up a lot of space in comparison to other display technologies. CRT screens have much deeper cabinets compared to flat panels and rear-projection displays for a given screen size, and so it becomes impractical to have CRTs larger than . The CRT disadvantages became especially significant in light of rapid technological advancements in LCD and plasma flat-panels which allow them to easily surpass as well as being thin and wall-mountable, two key features that were increasingly being demanded by consumers.\n\nSome CRT manufacturers, both LG Display and Samsung Display, have innovated CRT technology by creating a slimmer tube. Slimmer CRT has a trade name Superslim and Ultraslim. A 21-inch flat CRT has 447.2 millimeter depth. The depth of Superslim is 352 millimeters and Ultraslim is 295.7 millimeters.\n\nCRTs can emit a small amount of X-ray radiation as a result of the electron beam's bombardment of the shadow mask/aperture grille and phosphors. The amount of radiation escaping the front of the monitor is widely considered not to be harmful. The Food and Drug Administration regulations in are used to strictly limit, for instance, television receivers to 0.5 milliroentgens per hour (mR/h) (0.13 µC/(kg·h) or 36 pA/kg) at a distance of from any external surface; since 2007, most CRTs have emissions that fall well below this limit.\n\nOlder color and monochrome CRTs may contain toxic substances, such as cadmium, in the phosphors. The rear glass tube of modern CRTs may be made from leaded glass, which represent an environmental hazard if disposed of improperly. By the time personal computers were produced, glass in the front panel (the viewable portion of the CRT) used barium rather than lead, though the rear of the CRT was still produced from leaded glass. Monochrome CRTs typically do not contain enough leaded glass to fail EPA TCLP tests. While the TCLP process grinds the glass into fine particles in order to expose them to weak acids to test for leachate, intact CRT glass does not leache (The lead is vitrified, contained inside the glass itself, similar to leaded glass crystalware).\n\nIn October 2001, the United States Environmental Protection Agency created rules stating that CRTs must be brought to special recycling facilities. In November 2002, the EPA began fining companies that disposed of CRTs through landfills or incineration. Regulatory agencies, local and statewide, monitor the disposal of CRTs and other computer equipment.\n\nIn Europe, disposal of CRT televisions and monitors is covered by the WEEE Directive.\n\nAt low refresh rates (60 Hz and below), the periodic scanning of the display may produce a flicker that some people perceive more easily than others, especially when viewed with peripheral vision. Flicker is commonly associated with CRT as most televisions run at 50 Hz (PAL) or 60 Hz (NTSC), although there are some 100 Hz PAL televisions that are flicker-free. Typically only low-end monitors run at such low frequencies, with most computer monitors supporting at least 75 Hz and high-end monitors capable of 100 Hz or more to eliminate any perception of flicker. Non-computer CRTs or CRT for sonar or radar may have long persistence phosphor and are thus flicker free. If the persistence is too long on a video display, moving images will be blurred.\n\n50 Hz/60 Hz CRTs used for television operate with horizontal scanning frequencies of 15,734 Hz (for NTSC systems) or 15,625 Hz (for PAL systems). These frequencies are at the upper range of human hearing and are inaudible to many people; however, some people (especially children) will perceive a high-pitched tone near an operating television CRT. The sound is due to magnetostriction in the magnetic core and periodic movement of windings of the flyback transformer.\n\nThis problem does not occur on 100/120 Hz TVs and on non-CGA computer displays, because they use much higher horizontal scanning frequencies (22 kHz to over 100 kHz).\n\nHigh vacuum inside glass-walled cathode ray tubes permits electron beams to fly freely—without colliding into molecules of air or other gas. If the glass is damaged, atmospheric pressure can collapse the vacuum tube into dangerous fragments which accelerate inward and then spray at high speed in all directions. The implosion energy is proportional to the evacuated volume of the CRT. Although modern cathode ray tubes used in televisions and computer displays have epoxy-bonded face-plates or other measures to prevent shattering of the envelope, CRTs must be handled carefully to avoid personal injury.\n\nTo accelerate the electrons from the cathode to the screen with sufficient velocity, a very high voltage (EHT or Extra High Tension) is required, from a few thousand volts for a small oscilloscope CRT to tens of kV for a larger screen color TV. This is many times greater than household power supply voltage. Even after the power supply is turned off, some associated capacitors and the CRT itself may retain a charge for some time.\n\nUnder some circumstances, the signal radiated from the electron guns, scanning circuitry, and associated wiring of a CRT can be captured remotely and used to reconstruct what is shown on the CRT using a process called Van Eck phreaking. Special TEMPEST shielding can mitigate this effect. Such radiation of a potentially exploitable signal, however, occurs also with other display technologies and with electronics in general.\n\nAs electronic waste, CRTs are considered one of the hardest types to recycle. CRTs have relatively high concentration of lead and phosphors (not phosphorus), both of which are necessary for the display. There are several companies in the United States that charge a small fee to collect CRTs, then subsidize their labor by selling the harvested copper, wire, and printed circuit boards. The United States Environmental Protection Agency (EPA) includes discarded CRT monitors in its category of \"hazardous household waste\" but considers CRTs that have been set aside for testing to be commodities if they are not discarded, speculatively accumulated, or left unprotected from weather and other damage.\n\nLeaded CRT glass is sold to be remelted into other CRTs, or even broken down and used in road construction.\n\nBasics of cathode rays and discharge in low-pressure gas:\n\nLight production by cathode rays:\n\nManipulating the electron beam:\n\nApplying CRT in different display-purpose:\n\nMiscellaneous phenomena:\n\nHistorical aspects:\n\nSafety and precautions:\n\n\n"}
{"id": "6015", "url": "https://en.wikipedia.org/wiki?curid=6015", "title": "Crystal", "text": "Crystal\n\nA crystal or crystalline solid is a solid material whose constituents (such as atoms, molecules, or ions) are arranged in a highly ordered microscopic structure, forming a crystal lattice that extends in all directions. In addition, macroscopic single crystals are usually identifiable by their geometrical shape, consisting of flat faces with specific, characteristic orientations. The scientific study of crystals and crystal formation is known as crystallography. The process of crystal formation via mechanisms of crystal growth is called crystallization or solidification.\n\nThe word \"crystal\" derives from the Ancient Greek word (), meaning both \"ice\" and \"rock crystal\", from (), \"icy cold, frost\".\n\nExamples of large crystals include snowflakes, diamonds, and table salt. Most inorganic solids are not crystals but polycrystals, i.e. many microscopic crystals fused together into a single solid. Examples of polycrystals include most metals, rocks, ceramics, and ice. A third category of solids is amorphous solids, where the atoms have no periodic structure whatsoever. Examples of amorphous solids include glass, wax, and many plastics.\n\nCrystals are often used in pseudoscientific practices such as crystal therapy, and, along with gemstones, are sometimes associated with spellwork in Wiccan beliefs and related religious movements.\n\nThe scientific definition of a \"crystal\" is based on the microscopic arrangement of atoms inside it, called the crystal structure. A crystal is a solid where the atoms form a periodic arrangement. (Quasicrystals are an exception, see below.)\n\nNot all solids are crystals. For example, when liquid water starts freezing, the phase change begins with small ice crystals that grow until they fuse, forming a \"polycrystalline\" structure. In the final block of ice, each of the small crystals (called \"crystallites\" or \"grains\") is a true crystal with a periodic arrangement of atoms, but the whole polycrystal does \"not\" have a periodic arrangement of atoms, because the periodic pattern is broken at the grain boundaries. Most macroscopic inorganic solids are polycrystalline, including almost all metals, ceramics, ice, rocks, etc. Solids that are neither crystalline nor polycrystalline, such as glass, are called \"amorphous solids\", also called glassy, vitreous, or noncrystalline. These have no periodic order, even microscopically. There are distinct differences between crystalline solids and amorphous solids: most notably, the process of forming a glass does not release the latent heat of fusion, but forming a crystal does.\n\nA crystal structure (an arrangement of atoms in a crystal) is characterized by its \"unit cell\", a small imaginary box containing one or more atoms in a specific spatial arrangement. The unit cells are stacked in three-dimensional space to form the crystal.\n\nThe symmetry of a crystal is constrained by the requirement that the unit cells stack perfectly with no gaps. There are 219 possible crystal symmetries, called crystallographic space groups. These are grouped into 7 crystal systems, such as cubic crystal system (where the crystals may form cubes or rectangular boxes, such as halite shown at right) or hexagonal crystal system (where the crystals may form hexagons, such as ordinary water ice).\n\nCrystals are commonly recognized by their shape, consisting of flat faces with sharp angles. These shape characteristics are not \"necessary\" for a crystal—a crystal is scientifically defined by its microscopic atomic arrangement, not its macroscopic shape—but the characteristic macroscopic shape is often present and easy to see.\n\nEuhedral crystals are those with obvious, well-formed flat faces. Anhedral crystals do not, usually because the crystal is one grain in a polycrystalline solid.\n\nThe flat faces (also called facets) of a euhedral crystal are oriented in a specific way relative to the underlying atomic arrangement of the crystal: they are planes of relatively low Miller index. This occurs because some surface orientations are more stable than others (lower surface energy). As a crystal grows, new atoms attach easily to the rougher and less stable parts of the surface, but less easily to the flat, stable surfaces. Therefore, the flat surfaces tend to grow larger and smoother, until the whole crystal surface consists of these plane surfaces. (See diagram on right.)\n\nOne of the oldest techniques in the science of crystallography consists of measuring the three-dimensional orientations of the faces of a crystal, and using them to infer the underlying crystal symmetry.\n\nA crystal's habit is its visible external shape. This is determined by the crystal structure (which restricts the possible facet orientations), the specific crystal chemistry and bonding (which may favor some facet types over others), and the conditions under which the crystal formed.\n\nBy volume and weight, the largest concentrations of crystals in the Earth are part of its solid bedrock. Crystals found in rocks typically range in size from a fraction of a millimetre to several centimetres across, although exceptionally large crystals are occasionally found. , the world's largest known naturally occurring crystal is a crystal of beryl from Malakialina, Madagascar, long and in diameter, and weighing .\n\nSome crystals have formed by magmatic and metamorphic processes, giving origin to large masses of crystalline rock. The vast majority of igneous rocks are formed from molten magma and the degree of crystallization depends primarily on the conditions under which they solidified. Such rocks as granite, which have cooled very slowly and under great pressures, have completely crystallized; but many kinds of lava were poured out at the surface and cooled very rapidly, and in this latter group a small amount of amorphous or glassy matter is common. Other crystalline rocks, the metamorphic rocks such as marbles, mica-schists and quartzites, are recrystallized. This means that they were at first fragmental rocks like limestone, shale and sandstone and have never been in a molten condition nor entirely in solution, but the high temperature and pressure conditions of metamorphism have acted on them by erasing their original structures and inducing recrystallization in the solid state.\n\nOther rock crystals have formed out of precipitation from fluids, commonly water, to form druses or quartz veins.\nThe evaporites such as halite, gypsum and some limestones have been deposited from aqueous solution, mostly owing to evaporation in arid climates.\n\nWater-based ice in the form of snow, sea ice and glaciers is a very common manifestation of crystalline or polycrystalline matter on Earth. A single snowflake is a single crystal or a collection of crystals, while an ice cube is a polycrystal.\n\nMany living organisms are able to produce crystals, for example calcite and aragonite in the case of most molluscs or hydroxylapatite in the case of vertebrates.\n\nThe same group of atoms can often solidify in many different ways. Polymorphism is the ability of a solid to exist in more than one crystal form. For example, water ice is ordinarily found in the hexagonal form Ice I, but can also exist as the cubic Ice I, the rhombohedral ice II, and many other forms. The different polymorphs are usually called different \"phases\".\n\nIn addition, the same atoms may be able to form noncrystalline phases. For example, water can also form amorphous ice, while SiO can form both fused silica (an amorphous glass) and quartz (a crystal). Likewise, if a substance can form crystals, it can also form polycrystals.\n\nFor pure chemical elements, polymorphism is known as allotropy. For example, diamond and graphite are two crystalline forms of carbon, while amorphous carbon is a noncrystalline form. Polymorphs, despite having the same atoms, may have wildly different properties. For example, diamond is among the hardest substances known, while graphite is so soft that it is used as a lubricant.\n\nPolyamorphism is a similar phenomenon where the same atoms can exist in more than one amorphous solid form.\n\nCrystallization is the process of forming a crystalline structure from a fluid or from materials dissolved in a fluid. (More rarely, crystals may be deposited directly from gas; see thin-film deposition and epitaxy.)\n\nCrystallization is a complex and extensively-studied field, because depending on the conditions, a single fluid can solidify into many different possible forms. It can form a single crystal, perhaps with various possible phases, stoichiometries, impurities, defects, and habits. Or, it can form a polycrystal, with various possibilities for the size, arrangement, orientation, and phase of its grains. The final form of the solid is determined by the conditions under which the fluid is being solidified, such as the chemistry of the fluid, the ambient pressure, the temperature, and the speed with which all these parameters are changing.\n\nSpecific industrial techniques to produce large single crystals (called \"boules\") include the Czochralski process and the Bridgman technique. Other less exotic methods of crystallization may be used, depending on the physical properties of the substance, including hydrothermal synthesis, sublimation, or simply solvent-based crystallization.\n\nLarge single crystals can be created by geological processes. For example, selenite crystals in excess of 10 meters are found in the Cave of the Crystals in Naica, Mexico. For more details on geological crystal formation, see above.\n\nCrystals can also be formed by biological processes, see above. Conversely, some organisms have special techniques to \"prevent\" crystallization from occurring, such as antifreeze proteins.\n\nAn \"ideal\" crystal has every atom in a perfect, exactly repeating pattern. However, in reality, most crystalline materials have a variety of crystallographic defects, places where the crystal's pattern is interrupted. The types and structures of these defects may have a profound effect on the properties of the materials.\n\nA few examples of crystallographic defects include vacancy defects (an empty space where an atom should fit), interstitial defects (an extra atom squeezed in where it does not fit), and dislocations (see figure at right). Dislocations are especially important in materials science, because they help determine the mechanical strength of materials.\n\nAnother common type of crystallographic defect is an impurity, meaning that the \"wrong\" type of atom is present in a crystal. For example, a perfect crystal of diamond would only contain carbon atoms, but a real crystal might perhaps contain a few boron atoms as well. These boron impurities change the diamond's color to slightly blue. Likewise, the only difference between ruby and sapphire is the type of impurities present in a corundum crystal.\nIn semiconductors, a special type of impurity, called a dopant, drastically changes the crystal's electrical properties. Semiconductor devices, such as transistors, are made possible largely by putting different semiconductor dopants into different places, in specific patterns.\n\nTwinning is a phenomenon somewhere between a crystallographic defect and a grain boundary. Like a grain boundary, a twin boundary has different crystal orientations on its two sides. But unlike a grain boundary, the orientations are not random, but related in a specific, mirror-image way.\n\nMosaicity is a spread of crystal plane orientations. A mosaic crystal is supposed to consist of smaller crystalline units that are somewhat misaligned with respect to each other.\n\nIn general, solids can be held together by various types of chemical bonds, such as metallic bonds, ionic bonds, covalent bonds, van der Waals bonds, and others. None of these are necessarily crystalline or non-crystalline. However, there are some general trends as follows.\n\nMetals are almost always polycrystalline, though there are exceptions like amorphous metal and single-crystal metals. The latter are grown synthetically. (A microscopically-small piece of metal may naturally form into a single crystal, but larger pieces generally do not.) Ionic compound materials are usually crystalline or polycrystalline. In practice, large salt crystals can be created by solidification of a molten fluid, or by crystallization out of a solution. Covalently bonded solids (sometimes called covalent network solids) are also very common, notable examples being diamond and quartz. Weak van der Waals forces also help hold together certain crystals, such as crystalline molecular solids, as well as the interlayer bonding in graphite. Polymer materials generally will form crystalline regions, but the lengths of the molecules usually prevent complete crystallization—and sometimes polymers are completely amorphous.\n\nA quasicrystal consists of arrays of atoms that are ordered but not strictly periodic. They have many attributes in common with ordinary crystals, such as displaying a discrete pattern in x-ray diffraction, and the ability to form shapes with smooth, flat faces.\n\nQuasicrystals are most famous for their ability to show five-fold symmetry, which is impossible for an ordinary periodic crystal (see crystallographic restriction theorem).\n\nThe International Union of Crystallography has redefined the term \"crystal\" to include both ordinary periodic crystals and quasicrystals (\"any solid having an essentially discrete diffraction diagram\").\n\nQuasicrystals, first discovered in 1982, are quite rare in practice. Only about 100 solids are known to form quasicrystals, compared to about 400,000 periodic crystals known in 2004. The 2011 Nobel Prize in Chemistry was awarded to Dan Shechtman for the discovery of quasicrystals.\n\nCrystals can have certain special electrical, optical, and mechanical properties that glass and polycrystals normally cannot. These properties are related to the anisotropy of the crystal, i.e. the lack of rotational symmetry in its atomic arrangement. One such property is the piezoelectric effect, where a voltage across the crystal can shrink or stretch it. Another is birefringence, where a double image appears when looking through a crystal. Moreover, various properties of a crystal, including electrical conductivity, electrical permittivity, and Young's modulus, may be different in different directions in a crystal. For example, graphite crystals consist of a stack of sheets, and although each individual sheet is mechanically very strong, the sheets are rather loosely bound to each other. Therefore, the mechanical strength of the material is quite different depending on the direction of stress.\n\nNot all crystals have all of these properties. Conversely, these properties are not quite exclusive to crystals. They can appear in glasses or polycrystals that have been made anisotropic by working or stress—for example, stress-induced birefringence.\n\n\"Crystallography\" is the science of measuring the crystal structure (in other words, the atomic arrangement) of a crystal. One widely used crystallography technique is X-ray diffraction. Large numbers of known crystal structures are stored in crystallographic databases.\n\n\n"}
{"id": "6016", "url": "https://en.wikipedia.org/wiki?curid=6016", "title": "Cytosine", "text": "Cytosine\n\nCytosine (; C) is one of the four main bases found in DNA and RNA, along with adenine, guanine, and thymine (uracil in RNA). It is a pyrimidine derivative, with a heterocyclic aromatic ring and two substituents attached (an amine group at position 4 and a keto group at position 2). The nucleoside of cytosine is cytidine. In Watson-Crick base pairing, it forms three(3) hydrogen bonds with guanine.\n\nCytosine was discovered and named by Albrecht Kossel and Albert Neumann in 1894 when it was hydrolyzed from calf thymus tissues. A structure was proposed in 1903, and was synthesized (and thus confirmed) in the laboratory in the same year.\n\nIn 1997 cytosine was used in an early demonstration quantum information processing when Oxford University researchers implemented the Deutsch-Jozsa algorithm on a two qubit nuclear magnetic resonance quantum computer (NMRQC).\n\nIn March 2015, NASA scientists reported the formation of cytosine, along with uracil and thymine, from pyrimidine under the space-like laboratory conditions, which is of interest because pyrimidine has been found in meteorites although its origin is unknown. \n\nCytosine can be found as part of DNA, as part of RNA, or as a part of a nucleotide. As cytidine triphosphate (CTP), it can act as a co-factor to enzymes, and can transfer a phosphate to convert adenosine diphosphate (ADP) to adenosine triphosphate (ATP).\n\nIn DNA and RNA, cytosine is paired with guanine. However, it is inherently unstable, and can change into uracil (spontaneous deamination). This can lead to a point mutation if not repaired by the DNA repair enzymes such as uracil glycosylase, which cleaves a uracil in DNA.\n\nWhen found third in a codon of RNA, cytosine is synonymous with uracil, as they are interchangeable as the third base.\nWhen found as the second base in a codon, the third is always interchangeable. For example, UCU, UCC, UCA and UCG are all serine, regardless of the third base.\n\nCytosine can also be methylated into 5-methylcytosine by an enzyme called DNA methyltransferase or be methylated and hydroxylated to make 5-hydroxymethylcytosine.\nActive enzymatic deamination of cytosine or 5-methylcytosine by the APOBEC family of cytosine deaminases could have both beneficial and detrimental implications on various cellular processes as well as on organismal evolution. The implications of deamination on 5-hydroxymethylcytosine, on the other hand, remains less understood.\n\nCytosine has not been found in meteorites, which suggests the first strands of RNA and DNA had to look elsewhere to obtain this building block. Cytosine likely formed within some meteorite parent bodies, however did not persist within these bodies due to an effective deamination reaction into uracil.\n\n"}
{"id": "6019", "url": "https://en.wikipedia.org/wiki?curid=6019", "title": "Computational chemistry", "text": "Computational chemistry\n\nComputational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids. It is necessary because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form. While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.\n\nExamples of such properties are structure (i.e., the expected positions of the constituent atoms), absolute and relative (interaction) energies, electronic charge density distributions, dipoles and higher multipole moments, vibrational frequencies, reactivity, or other spectroscopic quantities, and cross sections for collision with other particles.\n\nThe methods used cover both static and dynamic situations. In all cases, the computer time and other resources (such as memory and disk space) increase rapidly with the size of the system being studied. That system can be one molecule, a group of molecules, or a solid. Computational chemistry methods range from very approximate to highly accurate; the latter are usually feasible for small systems only. \"Ab initio\" methods are based entirely on quantum mechanics and basic physical constants. Other methods are called empirical or semi-empirical because they use additional empirical parameters.\n\nBoth \"ab initio\" and semi-empirical approaches involve approximations. These range from simplified forms of the first-principles equations that are easier or faster to solve, to approximations limiting the size of the system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all. For example, most \"ab initio\" calculations make the Born–Oppenheimer approximation, which greatly simplifies the underlying Schrödinger equation by assuming that the nuclei remain in place during the calculation. In principle, \"ab initio\" methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced. In practice, however, it is impossible to eliminate all approximations, and residual error inevitably remains. The goal of computational chemistry is to minimize this residual error while keeping the calculations tractable.\n\nIn some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are used, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target.\n\nBuilding on the founding discoveries and theories in the history of quantum mechanics, the first theoretical calculations in chemistry were those of Walter Heitler and Fritz London in 1927. The books that were influential in the early development of computational quantum chemistry include Linus Pauling and E. Bright Wilson's 1935 \"Introduction to Quantum Mechanics – with Applications to Chemistry\", Eyring, Walter and Kimball's 1944 \"Quantum Chemistry\", Heitler's 1945 \"Elementary Wave Mechanics – with Applications to Quantum Chemistry\", and later Coulson's 1952 textbook \"Valence\", each of which served as primary references for chemists in the decades to follow.\n\nWith the development of efficient computer technology in the 1940s, the solutions of elaborate wave equations for complex atomic systems began to be a realizable objective. In the early 1950s, the first semi-empirical atomic orbital calculations were performed. Theoretical chemists became extensive users of the early digital computers. A very detailed account of such use in the United Kingdom is given by Smith and Sutcliffe. The first \"ab initio\" Hartree–Fock method calculations on diatomic molecules were performed in 1956 at MIT, using a basis set of Slater orbitals. For diatomic molecules, a systematic study using a minimum basis set and the first calculation with a larger basis set were published by Ransil and Nesbet respectively in 1960. The first polyatomic calculations using Gaussian orbitals were performed in the late 1950s. The first configuration interaction calculations were performed in Cambridge on the EDSAC computer in the 1950s using Gaussian orbitals by Boys and coworkers. By 1971, when a bibliography of \"ab initio\" calculations was published, the largest molecules included were naphthalene and azulene. Abstracts of many earlier developments in \"ab initio\" theory have been published by Schaefer.\n\nIn 1964, Hückel method calculations (using a simple linear combination of atomic orbitals (LCAO) method to determine electron energies of molecular orbitals of π electrons in conjugated hydrocarbon systems) of molecules, ranging in complexity from butadiene and benzene to ovalene, were generated on computers at Berkeley and Oxford. These empirical methods were replaced in the 1960s by semi-empirical methods such as CNDO.\n\nIn the early 1970s, efficient \"ab initio\" computer programs such as ATMOL, Gaussian, IBMOL, and POLYAYTOM, began to be used to speed \"ab initio\" calculations of molecular orbitals. Of these four programs, only Gaussian, now vastly expanded, is still in use, but many other programs are now in use. At the same time, the methods of molecular mechanics, such as MM2 force field, were developed, primarily by Norman Allinger.\n\nOne of the first mentions of the term \"computational chemistry\" can be found in the 1970 book \"Computers and Their Role in the Physical Sciences\" by Sidney Fernbach and Abraham Haskell Taub, where they state \"It seems, therefore, that 'computational chemistry' can finally be more and more of a reality.\" During the 1970s, widely different methods began to be seen as part of a new emerging discipline of \"computational chemistry\". The \"Journal of Computational Chemistry\" was first published in 1980.\n\nComputational chemistry has featured in several Nobel Prize awards, most notably in 1998 and 2013. Walter Kohn, \"for his development of the density-functional theory\", and John Pople, \"for his development of computational methods in quantum chemistry\", received the 1998 Nobel Prize in Chemistry. Martin Karplus, Michael Levitt and Arieh Warshel received the 2013 Nobel Prize in Chemistry for \"the development of multiscale models for complex chemical systems\".\n\nThe term \"theoretical chemistry\" may be defined as a mathematical description of chemistry, whereas \"computational chemistry\" is usually used when a mathematical method is sufficiently well developed that it can be automated for implementation on a computer. In theoretical chemistry, chemists, physicists, and mathematicians develop algorithms and computer programs to predict atomic and molecular properties and reaction paths for chemical reactions. Computational chemists, in contrast, may simply apply existing computer programs and methodologies to specific chemical questions.\n\nComputational chemistry has two different aspects:\n\nThus, computational chemistry can assist the experimental chemist or it can challenge the experimental chemist to find entirely new chemical objects.\n\nSeveral major areas may be distinguished within computational chemistry:\n\nThe words \"exact\" and \"perfect\" do not apply here, as very few aspects of chemistry can be computed exactly. However, almost every aspect of chemistry can be described in a qualitative or approximate quantitative computational scheme.\n\nMolecules consist of nuclei and electrons, so the methods of quantum mechanics apply. Computational chemists often attempt to solve the non-relativistic Schrödinger equation, with relativistic corrections added, although some progress has been made in solving the fully relativistic Dirac equation. In principle, it is possible to solve the Schrödinger equation in either its time-dependent or time-independent form, as appropriate for the problem in hand; in practice, this is not possible except for very small systems. Therefore, a great number of approximate methods strive to achieve the best trade-off between accuracy and computational cost.\n\nAccuracy can always be improved with greater computational cost. Significant errors can present themselves in ab initio models comprising many electrons, due to the computational cost of full relativistic-inclusive methods. This complicates the study of molecules interacting with high atomic mass unit atoms, such as transitional metals and their catalytic properties. Present algorithms in computational chemistry can routinely calculate the properties of molecules that contain up to about 40 electrons with sufficient accuracy. Errors for energies can be less than a few kJ/mol. For geometries, bond lengths can be predicted within a few picometres and bond angles within 0.5 degrees. The treatment of larger molecules that contain a few dozen electrons is computationally tractable by approximate methods such as density functional theory (DFT).\n\nThere is some dispute within the field whether or not the latter methods are sufficient to describe complex chemical reactions, such as those in biochemistry. Large molecules can be studied by semi-empirical approximate methods. Even larger molecules are treated by classical mechanics methods that use what are called molecular mechanics (MM). In QM-MM methods, small parts of large complexes are treated quantum mechanically (QM), and the remainder is treated approximately (MM).\n\nOne molecular formula can represent more than one molecular isomer: a set of isomers. Each isomer is a local minimum on the energy surface (called the potential energy surface) created from the total energy (i.e., the electronic energy, plus the repulsion energy between the nuclei) as a function of the coordinates of all the nuclei. A stationary point is a geometry such that the derivative of the energy with respect to all displacements of the nuclei is zero. A local (energy) minimum is a stationary point where all such displacements lead to an increase in energy. The local minimum that is lowest is called the global minimum and corresponds to the most stable isomer. If there is one particular coordinate change that leads to a decrease in the total energy in both directions, the stationary point is a transition structure and the coordinate is the reaction coordinate. This process of determining stationary points is called geometry optimization.\n\nThe determination of molecular structure by geometry optimization became routine only after efficient methods for calculating the first derivatives of the energy with respect to all atomic coordinates became available. Evaluation of the related second derivatives allows the prediction of vibrational frequencies if harmonic motion is estimated. More importantly, it allows for the characterization of stationary points. The frequencies are related to the eigenvalues of the Hessian matrix, which contains second derivatives. If the eigenvalues are all positive, then the frequencies are all real and the stationary point is a local minimum. If one eigenvalue is negative (i.e., an imaginary frequency), then the stationary point is a transition structure. If more than one eigenvalue is negative, then the stationary point is a more complex one, and is usually of little interest. When one of these is found, it is necessary to move the search away from it if the experimenter is looking solely for local minima and transition structures.\n\nThe total energy is determined by approximate solutions of the time-dependent Schrödinger equation, usually with no relativistic terms included, and by making use of the Born–Oppenheimer approximation, which allows for the separation of electronic and nuclear motions, thereby simplifying the Schrödinger equation. This leads to the evaluation of the total energy as a sum of the electronic energy at fixed nuclei positions and the repulsion energy of the nuclei. A notable exception are certain approaches called direct quantum chemistry, which treat electrons and nuclei on a common footing. Density functional methods and semi-empirical methods are variants on the major theme. For very large systems, the relative total energies can be compared using molecular mechanics. The ways of determining the total energy to predict molecular structures are:\n\nThe programs used in computational chemistry are based on many different quantum-chemical methods that solve the molecular Schrödinger equation associated with the molecular Hamiltonian. Methods that do not include any empirical or semi-empirical parameters in their equations – being derived directly from theoretical principles, with no inclusion of experimental data – are called \"ab initio methods\". This does not imply that the solution is an exact one; they are all approximate quantum mechanical calculations. It means that a particular approximation is rigorously defined on first principles (quantum theory) and then solved within an error margin that is qualitatively known beforehand. If numerical iterative methods must be used, the aim is to iterate until full machine accuracy is obtained (the best that is possible with a finite word length on the computer, and within the mathematical and/or physical approximations made).\n\nThe simplest type of \"ab initio\" electronic structure calculation is the Hartree–Fock method (HF), an extension of molecular orbital theory, in which the correlated electron-electron repulsion is not specifically taken into account; only its average effect is included in the calculation. As the basis set size is increased, the energy and wave function tend towards a limit called the Hartree–Fock limit. Many types of calculations (termed post-Hartree–Fock methods) begin with a Hartree–Fock calculation and subsequently correct for electron-electron repulsion, referred to also as electronic correlation. As these methods are pushed to the limit, they approach the exact solution of the non-relativistic Schrödinger equation. To obtain exact agreement with experiment, it is necessary to include relativistic and spin orbit terms, both of which are far more important for heavy atoms. In all of these approaches, along with choice of method, it is necessary to choose a basis set. This is a set of functions, usually centered on the different atoms in the molecule, which are used to expand the molecular orbitals with the linear combination of atomic orbitals (LCAO) molecular orbital method ansatz. Ab initio methods need to define a level of theory (the method) and a basis set.\n\nThe Hartree–Fock wave function is a single configuration or determinant. In some cases, particularly for bond breaking processes, this is inadequate, and several configurations must be used. Here, the coefficients of the configurations, and of the basis functions, are optimized together.\n\nThe total molecular energy can be evaluated as a function of the molecular geometry; in other words, the potential energy surface. Such a surface can be used for reaction dynamics. The stationary points of the surface lead to predictions of different isomers and the transition structures for conversion between isomers, but these can be determined without a full knowledge of the complete surface.\n\nA particularly important objective, called computational thermochemistry, is to calculate thermochemical quantities such as the enthalpy of formation to chemical accuracy. Chemical accuracy is the accuracy required to make realistic chemical predictions and is generally considered to be 1 kcal/mol or 4 kJ/mol. To reach that accuracy in an economic way it is necessary to use a series of post-Hartree–Fock methods and combine the results. These methods are called quantum chemistry composite methods.\n\nDensity functional theory (DFT) methods are often considered to be \"ab initio methods\" for determining the molecular electronic structure, even though many of the most common functionals use parameters derived from empirical data, or from more complex calculations. In DFT, the total energy is expressed in terms of the total one-electron density rather than the wave function. In this type of calculation, there is an approximate Hamiltonian and an approximate expression for the total electron density. DFT methods can be very accurate for little computational cost. Some methods combine the density functional exchange functional with the Hartree–Fock exchange term and are termed hybrid functional methods.\n\nSemi-empirical quantum chemistry methods are based on the Hartree–Fock method formalism, but make many approximations and obtain some parameters from empirical data. They are very important in computational chemistry for treating large molecules where the full Hartree–Fock method without the approximations is too costly. The use of empirical parameters appears to allow some inclusion of correlation effects into the methods.\n\nSemi-empirical methods follow what are often called empirical methods, where the two-electron part of the Hamiltonian is not explicitly included. For π-electron systems, this was the Hückel method proposed by Erich Hückel, and for all valence electron systems, the extended Hückel method proposed by Roald Hoffmann.\n\nIn many cases, large molecular systems can be modeled successfully while avoiding quantum mechanical calculations entirely. Molecular mechanics simulations, for example, use one classical expression for the energy of a compound, for instance the harmonic oscillator. All constants appearing in the equations must be obtained beforehand from experimental data or \"ab initio\" calculations.\n\nThe database of compounds used for parameterization, i.e., the resulting set of parameters and functions is called the force field, is crucial to the success of molecular mechanics calculations. A force field parameterized against a specific class of molecules, for instance proteins, would be expected to only have any relevance when describing other molecules of the same class.\n\nThese methods can be applied to proteins and other large biological molecules, and allow studies of the approach and interaction (docking) of potential drug molecules.\n\nComputational chemical methods can be applied to solid state physics problems. The electronic structure of a crystal is in general described by a band structure, which defines the energies of electron orbitals for each point in the Brillouin zone. Ab initio and semi-empirical calculations yield orbital energies; therefore, they can be applied to band structure calculations. Since it is time-consuming to calculate the energy for a molecule, it is even more time-consuming to calculate them for the entire list of points in the Brillouin zone.\n\nOnce the electronic and nuclear variables are separated (within the Born–Oppenheimer representation), in the time-dependent approach, the wave packet corresponding to the nuclear degrees of freedom is propagated via the time evolution operator (physics) associated to the time-dependent Schrödinger equation (for the full molecular Hamiltonian). In the complementary energy-dependent approach, the time-independent Schrödinger equation is solved using the scattering theory formalism. The potential representing the interatomic interaction is given by the potential energy surfaces. In general, the potential energy surfaces are coupled via the vibronic coupling terms.\n\nThe most popular methods for propagating the wave packet associated to the molecular geometry are:\n\nMolecular dynamics (MD) use either quantum mechanics, Newton's laws of motion or a mixed model to examine the time-dependent behavior of systems, including vibrations or Brownian motion and reactions. MD combined with density functional theory leads to hybrid models.\n\nThe atoms in molecules (QTAIM) model of Richard Bader was developed to effectively link the quantum mechanical model of a molecule, as an electronic wavefunction, to chemically useful concepts such as atoms in molecules, functional groups, bonding, the theory of Lewis pairs, and the valence bond model. Bader has demonstrated that these empirically useful chemistry concepts can be related to the topology of the observable charge density distribution, whether measured or calculated from a quantum mechanical wavefunction. QTAIM analysis of molecular wavefunctions is implemented, for example, in the AIMAll software package.\n\nMany self-sufficient exist. Some include many methods covering a wide range, while others concentrate on a very specific range or even on one method. Details of most of them can be found in:\n\n\n\n"}
{"id": "6020", "url": "https://en.wikipedia.org/wiki?curid=6020", "title": "Crash (J. G. Ballard novel)", "text": "Crash (J. G. Ballard novel)\n\nCrash is a novel by English author J. G. Ballard, first published in 1973. It is a story about symphorophilia specifically car-crash sexual fetishism: its protagonists become sexually aroused by staging and participating in real car-crashes.\n\nIt was a highly controversial novel: one publisher's reader returned the verdict \"This author is beyond psychiatric help. Do Not Publish!\" In 1996, the novel was made into a film of the same name by David Cronenberg.\n\nThe story is told through the eyes of narrator James Ballard, named after the author himself, but it centers on the sinister figure of Dr. Robert Vaughan, a \"former TV-scientist, turned nightmare angel of the expressways\". Ballard meets Vaughan after being involved in a car accident himself near London Airport. Gathering around Vaughan is a group of alienated people, all of them former crash victims, who follow him in his pursuit to re-enact the crashes of celebrities and experience what the narrator calls \"a new sexuality, born from a perverse technology\". Vaughan's ultimate fantasy is to die in a head-on collision with movie star Elizabeth Taylor.\n\nThe Normal's 1978 song \"Warm Leatherette\" was inspired by the novel, as was \"Miss the Girl,\" a 1983 single by The Creatures.\nThe Manic Street Preachers' song \"Mausoleum\" from 1994's \"The Holy Bible\" contains the famous Ballard quote about his reasons for writing the book, \"I wanted to rub the human face in its own vomit. I wanted to force it to look in the mirror.\" \n\nThe singer Glenn Danzig sings about this in his band Samhain's song \"Kiss Of Steel\" on the album \"November-Coming-Fire\"\n\nAn apparently unauthorized adaptation of Crash called \"Nightmare Angel\" was filmed in 1986 by Susan Emerling and Zoe Beloff. This short film bears the credit \"Inspired by J.G. Ballard.\"\n\n\n"}
{"id": "6021", "url": "https://en.wikipedia.org/wiki?curid=6021", "title": "C (programming language)", "text": "C (programming language)\n\nC (, as in the letter \"c\") is a general-purpose, imperative computer programming language, supporting structured programming, lexical variable scope and recursion, while a static type system prevents many unintended operations. By design, C provides constructs that map efficiently to typical machine instructions, and therefore it has found lasting use in applications that had formerly been coded in assembly language, including operating systems, as well as various application software for computers ranging from supercomputers to embedded systems.\n\nC was originally developed by Dennis Ritchie between 1969 and 1973 at Bell Labs, and used to re-implement the Unix operating system. It has since become one of the most widely used programming languages of all time, with C compilers from various vendors available for the majority of existing computer architectures and operating systems. C has been standardized by the American National Standards Institute (ANSI) since 1989 (see ANSI C) and subsequently by the International Organization for Standardization (ISO).\n\nC is an imperative procedural language. It was designed to be compiled using a relatively straightforward compiler, to provide low-level access to memory, to provide language constructs that map efficiently to machine instructions, and to require minimal run-time support. Despite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant and portably written C program can be compiled for a very wide variety of computer platforms and operating systems with few changes to its source code. The language has become available on a very wide range of platforms, from embedded microcontrollers to supercomputers.\n\nLike most imperative languages in the ALGOL tradition, C has facilities for structured programming and allows lexical variable scope and recursion, while a static type system prevents many unintended operations. In C, all executable code is contained within subroutines, which are called \"functions\" (although not in the strict sense of functional programming). Function parameters are always passed by value. Pass-by-reference is simulated in C by explicitly passing pointer values. C program source text is free-format, using the semicolon as a statement terminator and curly braces for grouping blocks of statements.\n\nThe C language also exhibits the following characteristics:\n\n\nWhile C does not include some features found in some other languages, such as object orientation or garbage collection, such features can be implemented or emulated in C, often by way of external libraries (e.g., the Boehm garbage collector or the GLib Object System).\n\nMany later languages have borrowed directly or indirectly from C, including C++, D, Go, Rust, Java, JavaScript, Limbo, LPC, C#, Objective-C, Perl, PHP, Python, Swift, Verilog (hardware description language), and Unix's C shell. These languages have drawn many of their control structures and other basic features from C. Most of them (with Python being the most dramatic exception) are also very syntactically similar to C in general, and they tend to combine the recognizable expression and statement syntax of C with underlying type systems, data models, and semantics that can be radically different.\n\nThe origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Dennis Ritchie and Ken Thompson, incorporating several ideas from colleagues. Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was developed in assembly language. The developers were considering rewriting the system using the B language, Thompson's simplified version of BCPL. However B's inability to take advantage of some of the PDP-11's features, notably byte addressability, led to C. The name of C was chosen simply as the next after B.\n\nThe development of C started in 1972 on the PDP-11 Unix system and first appeared in Version 2 Unix. The language was not initially designed with portability in mind, but soon ran on different platforms as well: a compiler for the Honeywell 6000 was written within the first year of C's history, while an IBM System/370 port followed soon.\n\nAlso in 1972, a large part of Unix was rewritten in C. By 1973, with the addition of codice_12 types, the C language had become powerful enough that most of the Unix kernel was now in C.\n\nUnix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system which was written in PL/I), and Master Control Program (MCP) for the Burroughs B5000 written in ALGOL in 1961. In around 1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system. Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.\n\nIn 1978, Brian Kernighan and Dennis Ritchie published the first edition of \"The C Programming Language\". This book, known to C programmers as \"K&R\", served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as \"K&R C\". The second edition of the book covers the later ANSI C standard, described below.\n\nK&R introduced several language features:\n\n\nEven after the publication of the 1989 ANSI standard, for many years K&R C was still considered the \"lowest common denominator\" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&R C code can be legal Standard C as well.\n\nIn early versions of C, only functions that return types other than codice_30 must be declared if used before the function definition; functions used without prior declaration were presumed to return type codice_30.\n\nFor example:\n\nThe codice_30 type specifiers which are commented out could be omitted in K&R C, but are required in later standards.\n\nSince K&R function declarations did not include any information about function arguments, function parameter type checks were not performed, although some compilers would issue a warning message if a local function was called with the wrong number of arguments, or if multiple calls to an external function used different numbers or types of arguments. Separate tools such as Unix's lint utility were developed that (among other things) could check for consistency of function use across multiple source files.\n\nIn the years following the publication of K&R C, several features were added to the language, supported by compilers from AT&T (in particular PCC) and some other vendors. These included:\n\n\nThe large number of extensions and lack of agreement on a standard library, together with the language popularity and the fact that not even the Unix compilers precisely implemented the K&R specification, led to the necessity of standardization.\n\nDuring the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.\n\nIn 1983, the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 \"Programming Language C\". This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.\n\nIn 1990, the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms \"C89\" and \"C90\" refer to the same programming language.\n\nANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14. National adoption of an update to the international standard typically occurs within a year of ISO publication.\n\nOne of the aims of the C standardization process was to produce a superset of K&R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), codice_15 pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&R interface continued to be permitted, for compatibility with existing source code.\n\nC89 is supported by current C compilers, and most C code being written today is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits. Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.\n\nIn cases where code must be compilable by either standard-conforming or K&R C-based compilers, the codice_38 macro can be used to split the code into Standard and K&R sections to prevent the use on a K&R C-based compiler of features available only in Standard C.\n\nAfter the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995, Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.\n\nThe C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as \"C99\". It has since been amended three times by Technical Corrigenda.\n\nC99 introduced several new features, including inline functions, several new data types (including codice_39 and a codice_40 type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with codice_41, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.\n\nC99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has codice_30 implicitly assumed. A standard macro codice_43 is defined with value codice_44 to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.\n\nIn 2007, work began on another revision of the C standard, informally called \"C1X\" until its official publication on 2011-12-08. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.\n\nThe C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions. It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro codice_43 is defined as codice_46 to indicate that C11 support is available.\n\nHistorically, embedded C programming requires nonstandard extensions to the C language in order to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.\n\nIn 2008, the C Standards Committee published a technical report extending the C language to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.\n\nC has a formal grammar specified by the C standard. Line endings are generally not significant in C; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters codice_47 and codice_48, or (since C99) following codice_41 until the end of the line. Comments delimited by codice_47 and codice_48 do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.\n\nC source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as codice_12, codice_35, and codice_14, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as codice_55 and codice_30 specify built-in types. Sections of code are enclosed in braces (codice_57 and codice_58, sometimes called \"curly brackets\") to limit the scope of declarations and to act as a single statement for control structures.\n\nAs an imperative language, C uses \"statements\" to specify actions. The most common statement is an \"expression statement\", consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables may be assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by codice_59(-codice_60) conditional execution and by codice_61-codice_3, codice_3, and codice_1 iterative execution (looping). The codice_1 statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. codice_66 and codice_67 can be used to leave the innermost enclosing loop statement or skip to its reinitialization. There is also a non-structured codice_68 statement which branches directly to the designated label within the function. codice_4 selects a codice_70 to be executed based on the value of an integer expression.\n\nExpressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next \"sequence point\"; sequence points include the end of each expression statement, and the entry to and return from each function call. Sequence points also occur during evaluation of expressions containing certain operators (codice_71, codice_72, codice_73 and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.\n\nKernighan and Ritchie say in the Introduction of \"The C Programming Language\": \"C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better.\" The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.\n\nThe basic C source character set includes the following characters:\n\n\nNewline indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as one.\n\nAdditional multi-byte encoded characters may be used in string literals, but they are not entirely portable. The latest C standard (C11) allows multi-national Unicode characters to be embedded portably within C source text by using codice_81 or codice_82 encoding (where the codice_83 denotes a hexadecimal character), although this feature is not yet widely implemented.\n\nThe basic C execution character set contains the same characters, along with representations for alert, backspace, and carriage return. Run-time support for extended character sets has increased with each revision of the C standard.\n\nC89 has 32 reserved words, also known as keywords, which are the words that cannot be used for any purposes other than those for which they are predefined:\n\nC99 reserved five more words:\n\nC11 reserved seven more words:\n\nMost of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations. Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. The language previously included a reserved word called codice_128, but this was seldom implemented, and has now been removed as a reserved word.\n\nC supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:\n\n\nC uses the operator codice_134 (used in mathematics to express equality) to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. C uses the operator codice_154 to test for equality. The similarity between these two operators (assignment and equality) may result in the accidental use of one in place of the other, and in many cases, the mistake does not produce an error message (although some compilers produce warnings). For example, the conditional expression codice_174 might mistakenly be written as codice_175, which will be evaluated as true if codice_74 is not zero after the assignment.\n\nThe C operator precedence is not always intuitive. For example, the operator codice_154 binds more tightly than (is executed prior to) the operators codice_9 (bitwise AND) and codice_147 (bitwise OR) in expressions such as codice_180, which must be written as codice_181 if that is the coder's intent.\n\nThe \"hello, world\" example, which appeared in the first edition of K&R, has become the model for an introductory program in most programming textbooks, regardless of programming language. The program prints \"hello, world\" to the standard output, which is usually a terminal or screen display.\n\nThe original version was:\nmain()\n\nA standard-conforming \"hello, world\" program is:\n\n\nint main(void)\n\nThe first line of the program contains a preprocessing directive, indicated by codice_182. This causes the compiler to replace that line with the entire text of the codice_183 standard header, which contains declarations for standard input and output functions such as codice_184. The angle brackets surrounding codice_183 indicate that codice_183 is located using a search strategy that prefers headers provided with the compiler to other headers having the same name, as opposed to double quotes which typically include local or project-specific header files.\n\nThe next line indicates that a function named codice_187 is being defined. The codice_187 function serves a special purpose in C programs; the run-time environment calls the codice_187 function to begin program execution. The type specifier codice_30 indicates that the value that is returned to the invoker (in this case the run-time environment) as a result of evaluating the codice_187 function, is an integer. The keyword codice_15 as a parameter list indicates that this function takes no arguments.\n\nThe opening curly brace indicates the beginning of the definition of the codice_187 function.\n\nThe next line \"calls\" (diverts execution to) a function named codice_184, which in this case is supplied from a system library. In this call, the codice_184 function is \"passed\" (provided with) a single argument, the address of the first character in the string literal codice_196. The string literal is an unnamed array with elements of type codice_55, set up automatically by the compiler with a final 0-valued character to mark the end of the array (codice_184 needs to know this). The codice_199 is an \"escape sequence\" that C translates to a \"newline\" character, which on output signifies the end of the current line. The return value of the codice_184 function is of type codice_30, but it is silently discarded since it is not used. (A more careful program might test the return value to determine whether or not the codice_184 function succeeded.) The semicolon codice_203 terminates the statement.\n\nThe closing curly brace indicates the end of the code for the codice_187 function. According to the C99 specification and newer, the codice_187 function, unlike any other function, will implicitly return a value of codice_78 upon reaching the codice_58 that terminates the function. (Formerly an explicit codice_208 statement was required.) This is interpreted by the run-time system as an exit code indicating successful execution.\n\nThe type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal. There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, and enumerated types (codice_14). Integer type codice_55 is often used for single-byte characters. C99 added a boolean datatype. There are also derived types including arrays, pointers, records (codice_12), and untagged unions (codice_35).\n\nC is often used in low-level systems programming where escapes from the type system may be necessary. The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a \"type cast\" to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.\n\nSome find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: \"declaration reflects use\".)\n\nC's \"usual arithmetic conversions\" allow for efficient code to be generated, but can sometimes produce unexpected results. For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned. This can generate unexpected results if the signed value is negative.\n\nC supports the use of pointers, a type of reference that records the address or location of an object or function in memory. Pointers can be \"dereferenced\" to access data stored at the address pointed to, or to invoke a pointed-to function. Pointers can be manipulated using assignment or pointer arithmetic. The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time. Pointer arithmetic is automatically scaled by the size of the pointed-to data type. Pointers are used for many purposes in C. Text strings are commonly manipulated using pointers into arrays of characters. Dynamic memory allocation is performed using pointers. Many data types, such as trees, are commonly implemented as dynamically allocated codice_12 objects linked together using pointers. Pointers to functions are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch) or as callbacks to be invoked by event handlers.\n\nA \"null pointer value\" explicitly points to no valid location. Dereferencing a null pointer value is undefined, often resulting in a segmentation fault. Null pointer values are useful for indicating special cases such as no \"next\" pointer in the final node of a linked list, or as an error indication from functions returning pointers. In appropriate contexts in source code, such as for assigning to a pointer variable, a \"null pointer constant\" can be written as codice_78, with or without explicit casting to a pointer type, or as the codice_215 macro defined by several standard headers. In conditional contexts, null pointer values evaluate to false, while all other pointer values evaluate to true.\n\nVoid pointers (codice_216) point to objects of unspecified type, and can therefore be used as \"generic\" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.\n\nCareless use of pointers is potentially dangerous. Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects. Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may continue to be used after deallocation (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer. In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.\n\nArray types in C are traditionally of a fixed, static size specified at compile time. (The more recent C99 standard also allows a form of variable-length arrays.) However, it is also possible to allocate a block of memory (of arbitrary size) at run-time, using the standard library's codice_217 function, and treat it as an array. C's unification of arrays and pointers means that declared arrays and these dynamically allocated simulated arrays are virtually interchangeable.\n\nSince arrays are always accessed (in effect) via pointers, array accesses are typically \"not\" checked against the underlying array size, although some compilers may provide bounds checking as an option. Array bounds violations are therefore possible and rather common in carelessly written code, and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions. If bounds checking is desired, it must be done manually.\n\nC does not have a special provision for declaring multi-dimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing. The index values of the resulting \"multi-dimensional array\" can be thought of as increasing in row-major order.\n\nMulti-dimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, since arrays are passed merely as pointers, the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this is to allocate the array with an additional \"row vector\" of pointers to the columns.)\n\nC99 introduced \"variable-length arrays\" which address some, but not all, of the issues with ordinary C arrays.\n\nThe subscript notation codice_218 (where codice_219 designates a pointer) is syntactic sugar for codice_220. Taking advantage of the compiler's knowledge of the pointer type, the address that codice_221 points to is not the base address (pointed to by codice_219) incremented by codice_27 bytes, but rather is defined to be the base address incremented by codice_27 multiplied by the size of an element that codice_219 points to. Thus, codice_218 designates the codice_227th element of the array.\n\nFurthermore, in most expression contexts (a notable exception is as operand of codice_106), the name of an array is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.\n\nThe size of an element can be determined by applying the operator codice_106 to any dereferenced element of codice_219, as in codice_231 or codice_232, and the number of elements in a declared array codice_76 can be determined as codice_234. The latter only applies to array names: variables declared with subscripts (codice_235). Due to the semantics of C, it is not possible to determine the entire size of arrays through pointers to arrays or those created by dynamic allocation (codice_217); code such as codice_237 (where codice_238 designates a pointer) will not work since the compiler assumes the size of the pointer itself is being requested. Since array name arguments to codice_106 are not converted to pointers, they do not exhibit such ambiguity. However, arrays created by dynamic allocation are accessed by pointers rather than true array variables, so they suffer from the same codice_106 issues as array pointers.\n\nThus, despite this apparent equivalence between array and pointer variables, there is still a distinction to be made between them. Even though the name of an array is, in most expression contexts, converted into a pointer (to its first element), this pointer does not itself occupy any storage; the array name is not an l-value, and its address is a constant, unlike a pointer variable. Consequently, what an array \"points to\" cannot be changed, and it is impossible to assign a new address to an array name. Array contents may be copied, however, by using the codice_241 function, or by accessing the individual elements.\n\nOne of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three distinct ways to allocate memory for objects:\n\n\nThese three approaches are appropriate in different situations and have various trade-offs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.\n\nWhere possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary. Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on codice_217 for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated. (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)\n\nUnless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.\n\nAnother issue is that heap memory allocation has to be synchronized with its actual usage in any program in order for it to be reused as much as possible. For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before codice_246 is called, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a \"memory leak.\" Conversely, it is possible for memory to be freed but continue to be referenced, leading to unpredictable results. Typically, the symptoms will appear in a portion of the program far removed from the actual error, making it difficult to track down the problem. (Such issues are ameliorated in languages with automatic garbage collection.)\n\nThe C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single \"archive\" file. Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. In order for a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., codice_247, shorthand for \"link the math library\").\n\nThe most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation (implementations which target limited environments such as embedded systems may provide only a subset of the standard library). This library supports stream input and output, memory allocation, mathematics, character strings, and time values. Several separate standard headers (for example, codice_183) specify the interfaces for these and other standard library facilities.\n\nAnother common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.\n\nSince many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.\n\nA number of tools have been developed to help C programmers find and fix statements with undefined behavior or possibly erroneous expressions, with greater rigor than that provided by the compiler. The tool lint was the first such, leading to many others.\n\nAutomated source code checking and auditing are beneficial in any language, and for C many such tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.\n\nThere are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as bounds checking for arrays, detection of buffer overflow, serialization, dynamic memory tracking, and automatic garbage collection.\n\nTools such as Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover runtime errors in memory usage.\n\nC is widely used for system programming in implementing operating systems and embedded system applications, because C code, when written for portability, can be used for most purposes, yet when needed, system-specific code can be used to access specific hardware addresses and to perform type punning to match externally imposed interface requirements, with a low run-time demand on system resources. \n\nC can also be used for website programming using CGI as a \"gateway\" for information between the Web application, the server, and the browser. C is often chosen over interpreted languages because of its speed, stability, and near-universal availability.\n\nOne consequence of C wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. The reference implementations of Python, Perl and PHP, for example, are all written in C.\n\nBecause the layer of abstraction is thin and the overhead is low, C enables programmers to create efficient implementations of algorithms and data structures, useful for computationally intense programs. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica, and MATLAB are completely or partially written in C.\n\nC is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, additional machine-specific code generators are not necessary. C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, that support compilation of generated code. However, some of C shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--.\n\nC has also been widely used to implement end-user applications. However, such applications can also be written in newer, higher-level languages.\n\nC has both directly and indirectly influenced many later languages such as C#, D, Go, Java, JavaScript, Limbo, LPC, Perl, PHP, Python, and Unix's C shell. The most pervasive influence has been syntactical, all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models and/or large-scale program structures that differ from those of C, sometimes radically.\n\nSeveral C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.\n\nWhen object-oriented languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.\n\nThe C++ programming language was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax. C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.\n\nObjective-C was originally a very \"thin\" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.\n\nIn addition to C++ and Objective-C, Ch, Cilk and Unified Parallel C are nearly supersets of C.\n\n\n\n"}
{"id": "6022", "url": "https://en.wikipedia.org/wiki?curid=6022", "title": "Cytology", "text": "Cytology\n\nCytology (from Greek , \"kytos\", \"a hollow\"; and , \"-logia\") is the study of cells. Cytology is that branch of life science that deals with the study of cells in terms of structure, function and chemistry. Robert Hooke (1635 – 1703) is sometimes seen as the father of cytology.\n\nBased on usage it can refer to: \nThe International Academy of Cytology has as its official journal \"Acta Cytologica\".\n\nCells, that were once invisible to the naked eye, became visible in 17th century Europe with the invention of the compound microscope. Robert Hooke was the first person to term the building block of all living organisms as \"cells\" after looking at cork. The cell theory states that all living things are made up cells. The theory also states that both plants and animals are composed of cells which was confirmed by plant scientist, Matthias Schleiden and animal scientist, Theodor Schwann in 1839. 19 years later, Rudolf Virchow contributed to the cell theory, arguing that all cells come from the division of preexisting cells. In recent years, there have been many studies which question the cell theory. Scientists have struggled to decide whether viruses are alive or not. Viruses lack common characteristics of a living cell, such as membranes, cell organelles, and the ability to reproduce by themselves. Viruses range from 0.005 to .03 microns in size whereas Bacteria range from 1-5 microns. The late 19th century indicates the birth of cytology. Modern day cell biology research looks at different ways to culture and manipulate cells outside of a living body to further research in human anatomy and physiology, to derive treatments and other medications, etc. The techniques by which cells are studied have evolved. Advancement in microscopic techniques and technology such as fluorescence microscopy, phase-contrast microscopy, dark field microscopy, confocal microscopy, cytometry, transmission electron microscopy, etc. have allowed scientists to get a better idea of the structure of cells.\n\nThere are two fundamental classifications of cells: prokaryotes and eukaryotes. The major difference between the two is the presence and/or absence of organelles. Other factors such as size, the way in which they reproduce, and the number of cells distinguish them from one another. Eukaryotic cells include animal, plant, fungi, and protozoa cells which all have a nucleus enclosed by a membrane. Prokaryotic cells, lacking an enclosed nucleus, include bacteria and archaea. Prokaryotic cells are much smaller than eukaryotic cells, making prokaryotic cells the smallest form of life. Cytologists typically focus on eukaryotic cells whereas prokaryotic cells are the focus of microbiologists, but this is not always the case.\n\nThese are the main branches of cytology:\n\n"}
{"id": "6023", "url": "https://en.wikipedia.org/wiki?curid=6023", "title": "Castle of the Winds", "text": "Castle of the Winds\n\nCastle of the Winds (also known as \"Castle of the Winds: Vanquish the Dark Forces\") is a tile-based roguelike video game for Microsoft Windows. It was developed by SaadaSoft in 1989 and distributed by Epic MegaGames in 1993.\nThe game was given around 1998 into the public domain and provided as Freeware download by the author Rick Saada.\n\nThe game is composed of two parts: A Question of Vengeance, released as shareware, and Lifthransir's Bane, sold commercially. A combined license for both parts was also sold.\n\nIn 1998, the game's author, Rick Saada, decided to distribute the entirety of \"Castle of the Winds\" free of charge.\n\nAs a 16-bit application, the program will not run natively under 64-bit versions of the Windows operating system.\n\nThe game is public domain per Rick Saada's words: \n\nThe game differs from most roguelikes in a number of ways. Its interface is mouse-dependent, but supports keyboard shortcuts (such as 'g' to get an item). \"Castle of the Winds\" also allows the player to restore saved games after dying.\n\nThe game favors the use of magic in combat, as spells are the only weapons that work from a distance. The player character automatically gains a spell with each experience level, and can permanently gain others using corresponding books, until all thirty spells available are learned. There are two opposing pairs of elements: cold vs. fire and lightning vs. acid/poison. Spells are divided into six categories: attack, defense, healing, movement, divination, and miscellaneous.\n\n\"Castle of the Winds\" possesses an inventory system that limits a player's load based on weight and bulk, rather than by number of items. It allows the character to use different containers, including packs, belts, chests, and bags. Other items include weapons, armor, protective clothing, purses, and ornamental jewellery. Almost every item in the game can be normal, cursed, or enchanted, with curses and enchantments working in a manner similar to \"NetHack\". Although items do not break with use, they may already be broken or rusted when found. Most objects that the character currently carries can be renamed.\n\nWherever the player goes before entering the dungeon, there is always a town which offers the basic services of a temple for healing and curing curses, a junk store where anything can be sold for a few copper coins, a sage who can identify items and (from the second town onwards) a bank for storing the total capacity of coins to lighten the player's load. Other services that differ and vary in what they sell are outfitters, weaponsmiths, armoursmiths, magic shops and general stores.\n\nThe game tracks how much time has been spent playing the game. Although story events are not triggered by the passage of time, it does determine when merchants rotate their stock. Victorious players are listed as \"Valhalla's Champions\" in the order of time taken, from fastest to slowest. If the player dies, they are still put on the list, but are categorized as \"Dead\", with their experience point total listed as at the final killing blow. The amount of time spent also determines the difficulty of the last boss.\n\nThough it is secondary to its hack and slash gameplay, \"Castle of the Winds\" has a plot loosely based on Norse mythology, told with setting changes, unique items, and occasional passages of text.\n\nThe player begins in a tiny hamlet, near which he/she used to live. His/her farm has been destroyed and godparents killed. After clearing out an abandoned mine, the player finds a scrap of parchment that reveals the death of the player's godparents was ordered by an unknown enemy. The player then returns to the hamlet to find it pillaged, and decides to travel to Bjarnarhaven.\n\nOnce in Bjarnarhaven, the player explores the levels beneath a nearby fortress, eventually facing Hrungnir, the Hill Giant Lord, responsible for ordering the player's godparents' death. Hrungnir carries the Enchanted Amulet of Kings. Upon activating the amulet, the player is informed of his/her past by his/her dead father, after which the player is transported to the town of Crossroads, and \"Part I\" ends. The game can be imported or started over in \"Part II\".\n\nThe town of Crossroads is run by a Jarl who at first does not admit the player, but later (on up to three occasions) provides advice and rewards. The player then enters the nearby ruined titular Castle of the Winds. There the player meets his/her deceased grandfather, who instructs him/her to venture into the dungeons below, defeat Surtur, and reclaim their birthright. Venturing deeper, the player encounters monsters run rampant, a desecrated crypt, a necromancer, and the installation of various special rooms for elementals. The player eventually meets and defeats the Wolf-Man leader, Bear-Man leader, the four Jotun kings, a Demon Lord, and finally Surtur. Upon defeating Surtur and escaping the dungeons, the player sits upon the throne, completing the game.\n\nAll terrain tiles, some landscape features, all monsters and objects, and some spell/effect graphics take the form of Windows 3.1 icons. Multi-tile graphics, such as ball spells and town buildings, are bitmaps included in the executable file. No graphics use colors other than the Windows-standard 16-color palette, plus transparency. They exist in monochrome versions as well, meaning that the game will display well on monochrome monitors.\n\nThe map view is identical to the playing-field view, except for scaling to fit on one screen. A simplified map view is available to improve performance on slower computers. The latter functionality also presents a cleaner display, as the aforementioned scaling routine does not always work correctly.\n"}
{"id": "6024", "url": "https://en.wikipedia.org/wiki?curid=6024", "title": "Calvinism", "text": "Calvinism\n\nCalvinism (also called the Reformed tradition, Reformed Christianity, Reformed Protestantism, or the Reformed faith) is a major branch of Protestantism that follows the theological tradition and forms of Christian practice of John Calvin and other Reformation-era theologians.\n\nCalvinists broke from the Roman Catholic Church in the 16th century. Calvinism differs from Lutherans on the real presence of Christ in the Eucharist, theories of worship, and the use of God's law for believers, among other things. As declared in the Westminster and Second Helvetic confessions, a basic principle is that the Bible is to be interpreted by itself, meaning the parts that are harder to understand are examined in the light of other passages where the Bible is more explicit on the matter. The term \"Calvinism\" can be misleading, because the religious tradition which it denotes has always been diverse, with a wide range of influences rather than a single founder. The movement was first called \"Calvinism\" by Lutherans who opposed it, and many within the tradition would prefer to use the word \"Reformed\".\n\nEarly influential Reformed theologians include Ulrich Zwingli, John Calvin, Martin Bucer, William Farel, Heinrich Bullinger, Peter Martyr Vermigli, Theodore Beza, and John Knox. In the twentieth century, Abraham Kuyper, Herman Bavinck, B. B. Warfield, J. Gresham Machen, Karl Barth, Martyn Lloyd-Jones, Cornelius Van Til, and Gordon Clark were influential. Contemporary Reformed theologians include J. I. Packer, John MacArthur, R. C. Sproul, Timothy J. Keller, John Piper, David Wells, and Michael Horton.\n\nReformed churches may exercise several forms of ecclesiastical polity; most are presbyterian or congregationalist, though some are episcopalian. Calvinism is largely represented by Continental Reformed, Presbyterian, and Congregationalist traditions. The biggest Reformed association is the World Communion of Reformed Churches with more than 80 million members in 211 member denominations around the world. There are more conservative Reformed federations such as the World Reformed Fellowship and the International Conference of Reformed Churches, as well as independent churches.\n\nCalvinism is named after John Calvin. It was first used by a Lutheran theologian in 1552. It was a common practice of the Catholic Church to name what they perceived to be heresy after its founder. Nevertheless, the term first came out of Lutheran circles. Calvin denounced the designation himself:\nDespite its negative connotation, this designation became increasingly popular in order to distinguish Calvinists from Lutherans and from newer Protestant branches that emerged later. Even though the vast majority of churches that trace back their history to Calvin (including Presbyterians, Congregationalists, and a row of other Calvinist churches) do not use it themselves, since the designation \"Reformed\" is more generally accepted and preferred, especially in the English-speaking world. Moreover, these churches claim to be—in accordance with John Calvin's own words—\"renewed accordingly with the true order of gospel\".\n\nSince the Arminian controversy, the Reformed tradition—as a branch of Protestantism distinguished from Lutheranism—divided into two separate groups, Arminians and Calvinists. However, it is now rare to call Arminians a part of the Reformed tradition, as many see these two schools of thought as opposed, making the terms \"Calvinist\" and \"Reformed\" synonymous. While the Reformed theological tradition addresses all of the traditional topics of Christian theology, the word \"Calvinism\" is sometimes used to refer to particular Calvinist views on soteriology and predestination, which are summarized in part by the Five Points of Calvinism. Some have also argued that Calvinism as a whole stresses the sovereignty or rule of God in all things including salvation.\n\nFirst-generation Reformed theologians include Huldrych Zwingli (1484–1531), Martin Bucer (1491–1551), Wolfgang Capito (1478–1541), John Oecolampadius (1482–1531), and Guillaume Farel (1489–1565). These reformers came from diverse academic backgrounds, but later distinctions within Reformed theology can already be detected in their thought, especially the priority of scripture as a source of authority. Scripture was also viewed as a unified whole, which led to a covenantal theology of the sacraments of baptism and the Lord's Supper as visible signs of the covenant of grace. Another Reformed distinctive present in these theologians was their denial of the bodily presence of Christ in the Lord's supper. Each of these theologians also understood salvation to be by grace alone, and affirmed a doctrine of particular election (the teaching that some people are chosen by God for salvation). Martin Luther and his successor Philipp Melanchthon were undoubtedly significant influences on these theologians, and to a larger extent later Reformed theologians. The doctrine of justification by faith alone was a direct inheritance from Luther.\n\nJohn Calvin (1509–64), Heinrich Bullinger (1504–75), Wolfgang Musculus (1497–1563), Peter Martyr Vermigli (1500–62), and Andreas Hyperius (1511–64) belong to the second generation of Reformed theologians. Calvin's \"Institutes of the Christian Religion\" (1536–59) was one of the most influential theologies of the era. Toward the middle of the 16th century, the Reformed began to commit their beliefs to confessions of faith, which would shape the future definition of the Reformed faith. The 1549 \"Consensus Tigurinus\" brought together those who followed Zwingli and Bullinger's memorialist theology of the Lord's supper, which taught that the supper simply serves as a reminder of Christ's death, and Calvin's view that the supper serves as a means of grace with Christ actually present, though spiritually rather than bodily. The document demonstrates the diversity as well as unity in early Reformed theology. The remainder of the 16th century saw an explosion of confessional activity. The stability and breadth of Reformed theology during this period stand in marked contrast to the bitter controversy experienced by Lutherans prior to the 1579 Formula of Concord.\n\nDue to Calvin's missionary work in France, his programme of reform eventually reached the French-speaking provinces of the Netherlands. Calvinism was adopted in the Electorate of the Palatinate under Frederick III, which led to the formulation of the Heidelberg Catechism in 1563, and in Navarre by Jeanne d'Albret. This and the Belgic Confession were adopted as confessional standards in the first synod of the Dutch Reformed Church in 1571. Leading divines, either Calvinist or those sympathetic to Calvinism, settled in England (Martin Bucer, Peter Martyr, and Jan Łaski) and Scotland (John Knox). During the English Civil War, the Calvinistic Puritans produced the Westminster Confession, which became the confessional standard for Presbyterians in the English-speaking world. Having established itself in Europe, the movement continued to spread to other parts of the world including North America, South Africa, and Korea.\n\nCalvin did not live to see the foundation of his work grow into an international movement; but his death allowed his ideas to break out of their city of origin, to succeed far beyond their borders, and to establish their own distinct character.\n\nAlthough much of Calvin's work was in Geneva, his publications spread his ideas of a \"correctly\" Reformed church to many parts of Europe. In Switzerland, some cantons are still Reformed and some are Catholic. Calvinism became the theological system of the majority in Scotland (see John Knox), the Netherlands (see William Ames, T. J. Frelinghuysen and Wilhelmus à Brakel), some communities in Flanders, and parts of Germany (especially these adjacent to the Netherlands) in the Palatinate, Kassel and Lippe with the likes of Olevianus and his colleague Zacharias Ursinus. In Hungary and the then-independent Transylvania, Calvinism was a significant religion. In the 16th century, the Reformation gained many supporters in Eastern Hungary and Hungarian-populated regions in Transylvania. In these parts, the Reformed nobles protected the faith. Almost all Transylvanian dukes were Reformed. Today there are about 3.5 million Hungarian Reformed people worldwide. It was influential in France, Lithuania and Poland before being mostly erased due to the counter-reformational activities taken up by the monarch in each country. Calvinism gained some popularity in Scandinavia, especially Sweden, but was rejected in favor of Lutheranism after the Synod of Uppsala in 1593.\n\nMost settlers in the American Mid-Atlantic and New England were Calvinists, including the English Puritans, the French Huguenots and Dutch settlers of New Amsterdam (New York), and the Scotch-Irish Presbyterians of the Appalachian back country. Nonconforming Protestants, Puritans, Separatists, Independents, English religious groups coming out of the English Civil War, and other English dissenters not satisfied with the degree to which the Church of England had been reformed, held overwhelmingly Reformed views. They are often cited among the primary founders of the United States of America. Dutch Calvinist settlers were also the first successful European colonizers of South Africa, beginning in the 17th century, who became known as Boers or Afrikaners.\nSierra Leone was largely colonized by Calvinist settlers from Nova Scotia, who were largely Black Loyalists, blacks who had fought for the British during the American War of Independence. John Marrant had organized a congregation there under the auspices of the Huntingdon Connection. Some of the largest Calvinist communions were started by 19th and 20th century missionaries. Especially large are those in Indonesia, Korea and Nigeria. In South Korea there are 20,000 Presbyterian congregations with about 9–10 million church members, scattered in more than 100 Presbyterian denominations. In South Korea, Presbyterianism is the largest Christian denomination.\n\nA 2011 report of the Pew Forum on Religious and Public Life estimated that members of Presbyterian or Reformed churches make up 7% of the estimated 801 million Protestants globally, or approximately 56 million people. Though the broadly defined Reformed faith is much larger, as it constitutes Congregationalist (0.5%), most of the United and uniting churches (unions of different denominations) (7.2%) and most likely some of the other Protestant denominations (38.2%). All three are distinct categories from Presbyterian or Reformed (7%) in this report.\n\nThe Reformed family of churches is one of the largest Christian denominations. According to adherents.com the Reformed/Presbyterian/Congregational/United churches represent 75 million believers worldvide.\n\nThe World Communion of Reformed Churches, which includes some United Churches (most of these are primarily Reformed; see \"Uniting and united churches\" for details), has 80 million believers. WCRC is the third largest Christian communion in the world, after the Roman Catholic Church and the Eastern Orthodox Churches.\n\nMany conservative Reformed churches which are strongly Calvinistic formed the World Reformed Fellowship which has about 70 member denominations. Most are not part of the World Communion of Reformed Churches because of its ecumenial attire. The International Conference of Reformed Churches is another conservative association.\n\nReformed theologians believe that God communicates knowledge of himself to people through the Word of God. People are not able to know anything about God except through this self-revelation. Speculation about anything which God has not revealed through his Word is not warranted. The knowledge people have of God is different from that which they have of anything else because God is infinite, and finite people are incapable of comprehending an infinite being. While the knowledge revealed by God to people is never incorrect, it is also never comprehensive.\nAccording to Reformed theologians, God's self-revelation is always through his son Jesus Christ, because Christ is the only mediator between God and people. Revelation of God through Christ comes through two basic channels. The first is creation and providence, which is God's creating and continuing to work in the world. This action of God gives everyone knowledge about God, but this knowledge is only sufficient to make people culpable for their sin; it does not include knowledge of the gospel. The second channel through which God reveals himself is redemption, which is the gospel of salvation from condemnation which is punishment for sin.\n\nIn Reformed theology, the Word of God takes several forms. Jesus Christ himself is the Word Incarnate. The prophecies about him said to be found in the Old Testament and the ministry of the apostles who saw him and communicated his message are also the Word of God. Further, the preaching of ministers about God is the very Word of God because God is considered to be speaking through them. God also speaks through human writers in the Bible, which is composed of texts set apart by God for self-revelation. Reformed theologians emphasize the Bible as a uniquely important means by which God communicates with people. People gain knowledge of God from the Bible which cannot be gained in any other way.\n\nReformed theologians affirm that the Bible is true, but differences emerge among them over the meaning and extent of its truthfulness. Conservative followers of the Princeton theologians take the view that the Bible is true and inerrant, or incapable of error or falsehood, in every place. This view is very similar to that of Catholic orthodoxy as well as modern Evangelicalism. Another view, influenced by the teaching of Karl Barth and Neo-Orthodoxy, is found in the Presbyterian Church (U.S.A.)'s Confession of 1967. Those who take this view believe the Bible to be the primary source of our knowledge of God, but also that some parts of the Bible may be false, not witnesses to Christ, and not normative for today's church. In this view, Christ is the revelation of God, and the scriptures witness to this revelation rather than being the revelation itself. Dawn DeVries, a professor at Union Presbyterian Seminary, has written that Barth's doctrine of Scripture is not capable of resolving conflicts in contemporary churches, and proposed that Scripture not be thought of as the Word of God at all, but only human reports of the revealed Jesus Christ.\n\nReformed theologians use the concept of covenant to describe the way God enters fellowship with people in history. The concept of covenant is so prominent in Reformed theology that Reformed theology as a whole is sometimes called \"covenant theology\". However, sixteenth and seventeenth-century theologians developed a particular theological system called \"covenant theology\" or \"federal theology\" which many conservative Reformed churches continue to affirm today. This framework orders God's life with people primarily in two covenants: the covenant of works and the covenant of grace. The covenant of works is made with Adam and Eve in the Garden of Eden. The terms of the covenant are that God provides a blessed life in the garden on condition that Adam and Eve obey God's law perfectly. Because Adam and Eve broke the covenant by eating the forbidden fruit, they became subject to death and were banished from the garden. This sin was passed down to all mankind because all people are said to be in Adam as a covenantal or \"federal\" head. Federal theologians usually infer that Adam and Eve would have gained immortality had they obeyed perfectly.\n\nA second covenant, called the covenant of grace, is said to have been made immediately following Adam and Eve's sin. In it, God graciously offers salvation from death on condition of faith in God. This covenant is administered in different ways throughout the Old and New Testaments, but retains the substance of being free of a requirement of perfect obedience.\n\nThrough the influence of Karl Barth, many contemporary Reformed theologians have discarded the covenant of works, along with other concepts of federal theology. Barth saw the covenant of works as disconnected from Christ and the gospel, and rejected the idea that God works with people in this way. Instead, Barth argued that God always interacts with people under the covenant of grace, and that the covenant of grace is free of all conditions whatsoever. Barth's theology and that which follows him has been called \"monocovenantal\" as opposed to the \"bi-covenantal\" scheme of classical federal theology. Conservative contemporary Reformed theologians, such as John Murray, have also rejected the idea of covenants based on law rather than grace. Michael Horton, however, has defended the covenant of works as combining principles of law and love.\n\nFor the most part, the Reformed tradition did not modify the medieval consensus on the doctrine of God. God's character is described primarily using three adjectives: eternal, infinite, and unchangeable. Reformed theologians such as Shirley Guthrie have proposed that rather than conceiving of God in terms of his attributes and freedom to do as he pleases, the doctrine of God is to be based on God's work in history and his freedom to live with and empower people.\nTraditionally, Reformed theologians have also followed the medieval tradition going back to before the early church councils of Nicaea and Chalcedon on the doctrine of the Trinity. God is affirmed to be one God in three persons: Father, Son, and Holy Spirit. The Son (Christ) is held to be eternally begotten by the Father and the Holy Spirit eternally proceeding from the Father and Son. However, contemporary theologians have been critical of aspects of Western views here as well. Drawing on the Eastern tradition, these Reformed theologians have proposed a \"social trinitarianism\" where the persons of the Trinity only exist in their life together as persons-in-relationship. Contemporary Reformed confessions such as the Barmen Confession and Brief Statement of Faith of the Presbyterian Church (USA) have avoided language about the attributes of God and have emphasized his work of reconciliation and empowerment of people. Feminist theologian Letty Russell used the image of partnership for the persons of the Trinity. According to Russell, thinking this way encourages Christians to interact in terms of fellowship rather than reciprocity. Conservative Reformed theologian Michael Horton, however, has argued that social trinitarianism is untenable because it abandons the essential unity of God in favor of a community of separate beings.\n\nReformed theologians affirm the historic Christian belief that Christ is eternally one person with a divine and a human nature. Reformed Christians have especially emphasized that Christ truly became human so that people could be saved. Christ's human nature has been a point of contention between Reformed and Lutheran Christology. In accord with the belief that finite humans cannot comprehend infinite divinity, Reformed theologians hold that Christ's human body cannot be in multiple locations at the same time. Because Lutherans believe that Christ is bodily present in the Eucharist, they hold that Christ is bodily present in many locations simultaneously. For Reformed Christians, such a belief denies that Christ actually became human. Some contemporary Reformed theologians have moved away from the traditional language of one person in two natures, viewing it as unintelligible to contemporary people. Instead, theologians tend to emphasize Jesus' context and particularity as a first-century Jew.\nJohn Calvin and many Reformed theologians who followed him describe Christ's work of redemption in terms of three offices: prophet, priest, and king. Christ is said to be a prophet in that he teaches perfect doctrine, a priest in that he intercedes to the Father on believers' behalf and offered himself as a sacrifice for sin, and a king in that he rules the church and fights on believers' behalf. The threefold office links the work of Christ to God's work in ancient Israel. Many, but not all, Reformed theologians continue to make use of the threefold office as a framework because of its emphasis on the connection of Christ's work to Israel. They have, however, often reinterpreted the meaning of each of the offices. For example, Karl Barth interpreted Christ's prophetic office in terms of political engagement on behalf of the poor.\n\nChristians believe Jesus' death and resurrection makes it possible for believers to attain forgiveness for sin and reconciliation with God through the atonement. Reformed Protestants generally subscribe to a particular view of the atonement called substitutionary atonement, which explains Christ's death as a sacrificial payment for sin. Christ is believed to have died in place of the believer, who is accounted righteous as a result of this sacrificial payment. Contemporary Reformed theologians such as William Placher and Nancy Duff have criticized this view, claiming it makes God appear abusive or vindictive and sanctions violence by the strong against the weak.\n\nIn Christian theology, people are created good and in the image of God but have become corrupted by sin, which causes them to be imperfect and overly self-interested. Reformed Christians, following the tradition of Augustine of Hippo, believe that this corruption of human nature was brought on by Adam and Eve's first sin, a doctrine called original sin. Reformed theologians emphasize that this sinfulness affects all of a person's nature, including their will. This view, that sin so dominates people that they are unable to avoid sin, has been called total depravity. In colloquial English, the term \"total depravity\" can be easily misunderstood to mean that people are absent of any goodness or unable to do any good. However the Reformed teaching is actually that while people continue to bear God's image and may do things that appear outwardly good, their sinful intentions affect all of their nature and actions so that they are not pleasing to God.\n\nSome contemporary theologians in the Reformed tradition, such as those associated with the PC(USA)'s Confession of 1967, have emphasized the social character of human sinfulness. These theologians have sought to bring attention to issues of environmental, economic, and political justice as areas of human life that have been affected by sin.\n\nReformed theologians, along with other Protestants, believe salvation from punishment for sin is to be given to all those who have faith in Christ. Faith is not purely intellectual, but involves trust in God's promise to save. Protestants do not hold there to be any other requirement for salvation, but that faith alone is sufficient.\n\nJustification is the part of salvation where God pardons the sin of those who believe in Christ. It is historically held by Protestants to be the most important article of Christian faith, though more recently it is sometimes given less importance out of ecumenical concerns. People are not on their own able even to fully repent of their sin or prepare themselves to repent because of their sinfulness. Therefore, justification is held to arise solely from God's free and gracious act.\n\nSanctification is the part of salvation in which God makes the believer holy, by enabling them to exercise greater love for God and for other people. The good works accomplished by believers as they are sanctified are considered to be the necessary outworking of the believer's salvation, though they do not cause the believer to be saved. Sanctification, like justification, is by faith, because doing good works is simply living as the son of God one has become.\n\nReformed theologians teach that sin so affects human nature that they are unable even to exercise faith in Christ by their own will. While people are said to retain will, in that they willfully sin, they are unable not to sin because of the corruption of their nature due to original sin. Reformed Christians believe that God predestined some people to be saved. This choice by God to save some is held to be unconditional and not based on any characteristic or action on the part of the person chosen. This view is opposed to the Arminian view that God's choice of whom to save is conditional or based on his foreknowledge of who would respond positively to God.\n\nKarl Barth reinterpreted the Reformed doctrine of predestination to apply only to Christ. Individual people are only said to be elected through their being in Christ. Reformed theologians who followed Barth, including Jürgen Moltmann, David Migliore, and Shirley Guthrie, have argued that the traditional Reformed concept of predestination is speculative and have proposed alternative models. These theologians claim that a properly trinitarian doctrine emphasizes God's freedom to love all people, rather than choosing some for salvation and others for damnation. God's justice towards and condemnation of sinful people is spoken of by these theologians as out of his love for them and a desire to reconcile them to himself.\n\nMost objections to and attacks on Calvinism focus on the \"five points of Calvinism\", also called the doctrines of grace, and remembered by the mnemonic \"TULIP\". The five points are popularly said to summarize the Canons of Dort; however, there is no historical relationship between them, and some scholars argue that their language distorts the meaning of the Canons, Calvin's theology, and the theology of 17th-century Calvinistic orthodoxy, particularly in the language of total depravity and limited atonement. The five points were more recently popularized in the 1963 booklet \"The Five Points of Calvinism Defined, Defended, Documented\" by David N. Steele and Curtis C. Thomas. The origins of the five points and the acronym are uncertain, but they appear to be outlined in the Counter Remonstrance of 1611, a less known Reformed reply to the Arminians that occurred prior to the Canons of Dort. The acronym was used by Cleland Boyd McAfee as early as circa 1905. An early printed appearance of the T-U-L-I-P acronym is in Loraine Boettner's 1932 book, \"The Reformed Doctrine of Predestination\". The acronym was very cautiously if ever used by Calvinist apologists and theologians before the booklet by Steele and Thomas. More recently, theologians have sought to reformulate the TULIP acronym to more accurately reflect the Canons of Dort.\n\nThe central assertion of these points is that God saves every person upon whom he has mercy, and that his efforts are not frustrated by the unrighteousness or inability of humans.\n\nReformed Christians see the Christian Church as the community with which God has made the covenant of grace, a promise of eternal life and relationship with God. This covenant extends to those under the \"old covenant\" whom God chose, beginning with Abraham and Sarah. The church is conceived of as both invisible and visible. The invisible church is the body of all believers, known only to God. The visible church is the institutional body which contains both members of the invisible church as well as those who appear to have faith in Christ, but are not truly part of God's elect.\n\nIn order to identify the visible church, Reformed theologians have spoken of certain marks of the Church. For some, the only mark is the pure preaching of the gospel of Christ. Others, including John Calvin, also including the right administration of the sacraments. Others, such as those following the Scots Confession, include a third mark of rightly administered church discipline, or exercise of censure against unrepentant sinners. These marks allowed the Reformed to identify the church based on its conformity to the Bible rather than the Magisterium or church tradition.\n\nThe regulative principle of worship is a teaching shared by some Calvinists and Anabaptists on how the Bible orders public worship. The substance of the doctrine regarding worship is that God institutes in the Scriptures everything he requires for worship in the Church and that everything else is prohibited. As the regulative principle is reflected in Calvin's own thought, it is driven by his evident antipathy toward the Roman Catholic Church and its worship practices, and it associates musical instruments with icons, which he considered violations of the Ten Commandments' prohibition of graven images.\n\nOn this basis, many early Calvinists also eschewed musical instruments and advocated a cappella exclusive psalmody in worship, though Calvin himself allowed other scriptural songs as well as psalms, and this practice typified presbyterian worship and the worship of other Reformed churches for some time. The original Lord's Day service designed by John Calvin was a highly liturgical service with the Creed, Alms, Confession and Absolution, the Lord's supper, Doxologies, prayers, Psalms being sung, the Lords prayer being sung, Benedictions.\n\nSince the 19th century, however, some of the Reformed churches have modified their understanding of the regulative principle and make use of musical instruments, believing that Calvin and his early followers went beyond the biblical requirements and that such things are circumstances of worship requiring biblically rooted wisdom, rather than an explicit command. Despite the protestations of those who hold to a strict view of the regulative principle, today hymns and musical instruments are in common use, as are contemporary worship music styles with elements such as worship bands.\n\nThe Westminster Confession of Faith limits the sacraments to baptism and the Lord's Supper. Sacraments are denoted \"signs and seals of the covenant of grace.\" Westminster speaks of \"a sacramental relation, or a sacramental union, between the sign and the thing signified; whence it comes to pass that the names and effects of the one are attributed to the other.\" Baptism is for infant children of believers as well as believers, as it is for all the Reformed except Baptists and some Congregationalists. Baptism admits the baptized into the visible church, and in it all the benefits of Christ are offered to the baptized. On the Lord's supper, Westminster takes a position between Lutheran sacramental union and Zwinglian memorialism: \"the Lord's supper really and indeed, yet not carnally and corporally, but spiritually, receive and feed upon Christ crucified, and all benefits of his death: the body and blood of Christ being then not corporally or carnally in, with, or under the bread and wine; yet, as really, but spiritually, present to the faith of believers in that ordinance as the elements themselves are to their outward senses.\"\n\nThe 1689 London Baptist Confession of Faith does not use the term sacrament, but describes baptism and the Lord's supper as ordinances, as do most Baptists Calvinist or otherwise. Baptism is only for those who \"actually profess repentance towards God\", and not for the children of believers. Baptists also insist on immersion or dipping, in contradistinction to other Reformed Christians. The Baptist Confession describes the Lord's supper as \"the body and blood of Christ being then not corporally or carnally, but spiritually present to the faith of believers in that ordinance\", similarly to the Westminster Confession. There is significant latitude in Baptist congregations regarding the Lord's supper, and many hold the Zwinglian view.\n\nThere are two schools of thought regarding the logical order of God's decree to ordain the fall of man: supralapsarianism (from the Latin: \"supra\", \"above\", here meaning \"before\" + \"lapsus\", \"fall\") and infralapsarianism (from the Latin: \"infra\", \"beneath\", here meaning \"after\" + \"lapsus\", \"fall\"). The former view, sometimes called \"high Calvinism\", argues that the Fall occurred partly to facilitate God's purpose to choose some individuals for salvation and some for damnation. Infralapsarianism, sometimes called \"low Calvinism\", is the position that, while the Fall was indeed planned, it was not planned with reference to who would be saved.\n\nSupralapsarians believe that God chose which individuals to save logically prior to the decision to allow the race to fall and that the Fall serves as the means of realization of that prior decision to send some individuals to hell and others to heaven (that is, it provides the grounds of condemnation in the reprobate and the need for salvation in the elect). In contrast, infralapsarians hold that God planned the race to fall logically prior to the decision to save or damn any individuals because, it is argued, in order to be \"saved\", one must first need to be saved from something and therefore the decree of the Fall must precede predestination to salvation or damnation.\n\nThese two views vied with each other at the Synod of Dort, an international body representing Calvinist Christian churches from around Europe, and the judgments that came out of that council sided with infralapsarianism (Canons of Dort, First Point of Doctrine, Article 7). The Westminster Confession of Faith also teaches (in Hodge's words \"clearly impl[ies]\") the infralapsarian view, but is sensitive to those holding to supralapsarianism. The Lapsarian controversy has a few vocal proponents on each side today, but overall it does not receive much attention among modern Calvinists.\n\nAmyraldism (or sometimes Amyraldianism, also known as the School of Saumur, hypothetical universalism, post redemptionism, moderate Calvinism, or four-point Calvinism) is the belief that God, prior to his decree of election, decreed Christ's atonement for all alike if they believe, but seeing that none would believe on their own, he then elected those whom he will bring to faith in Christ, thereby preserving the Calvinist doctrine of unconditional election. The efficacy of the atonement remains limited to those who believe.\n\nNamed after its formulator Moses Amyraut, this doctrine is still viewed as a variety of Calvinism in that it maintains the particularity of sovereign grace in the application of the atonement. However, detractors like B. B. Warfield have termed it \"an inconsistent and therefore unstable form of Calvinism.\"\n\nHyper-Calvinism first referred to a view that appeared among the early English Particular Baptists in the 18th century. Their system denied that the call of the gospel to \"repent and believe\" is directed to every single person and that it is the duty of every person to trust in Christ for salvation. The term also occasionally appears in both theological and secular controversial contexts, where it usually connotes a negative opinion about some variety of theological determinism, predestination, or a version of Evangelical Christianity or Calvinism that is deemed by the critic to be unenlightened, harsh, or extreme.\n\nThe Westminster Confession of Faith says that the gospel is to be freely offered to sinners, and the Larger Catechism makes clear that the gospel is offered to the non-elect.\n\nNeo-Calvinism, a form of Dutch Calvinism, is the movement initiated by the theologian and former Dutch prime minister Abraham Kuyper. James Bratt has identified a number of different types of Dutch Calvinism: The Seceders—split into the Reformed Church \"West\" and the Confessionalists; and the Neo-Calvinists—the Positives and the Antithetical Calvinists. The Seceders were largely infralapsarian and the Neo-Calvinists usually supralapsarian.\n\nKuyper wanted to awaken the church from what he viewed as its pietistic slumber. He declared:\n\nNo single piece of our mental world is to be sealed off from the rest and there is not a square inch in the whole domain of human existence over which Christ, who is sovereign over all, does not cry: 'Mine!' \n\nThis refrain has become something of a rallying call for Neo-Calvinists.\n\nChristian Reconstructionism is a fundamentalist Calvinist theonomic movement that has remained rather obscure. Founded by R. J. Rushdoony, the movement has had an important influence on the Christian Right in the United States. The movement declined in the 1990s and was declared dead in a 2008 \"Church History\" journal article. However, it lives on in small denominations such as the Reformed Presbyterian Church in the United States and as a minority position in other denominations. Christian Reconstructionists are usually postmillennialists and followers of the presuppositional apologetics of Cornelius Van Til. They tend to support a decentralized political order resulting in laissez-faire capitalism.\n\nThe New Calvinism is a growing perspective within conservative Evangelicalism that embraces the fundamentals of 16th century Calvinism while also trying to be relevant in the present day world. In March 2009, \"Time\" magazine described the New Calvinism as one of the \"10 ideas changing the world\". Some of the major figures in this area are John Piper, Mark Driscoll, Al Mohler, Mark Dever, C. J. Mahaney, Joshua Harris, and Tim Keller. New Calvinists have been criticized for blending Calvinist soteriology with popular Evangelical positions on the sacraments and continuationism.\n\nCalvin expressed himself on usury in a 1545 letter to a friend, Claude de Sachin, in which he criticized the use of certain passages of scripture invoked by people opposed to the charging of interest. He reinterpreted some of these passages, and suggested that others of them had been rendered irrelevant by changed conditions. He also dismissed the argument (based upon the writings of Aristotle) that it is wrong to charge interest for money because money itself is barren. He said that the walls and the roof of a house are barren, too, but it is permissible to charge someone for allowing him to use them. In the same way, money can be made fruitful.\n\nHe qualified his view, however, by saying that money should be lent to people in dire need without hope of interest, while a modest interest rate of 5% should be permitted in relation to other borrowers.\n\nCalvin's concept of God and man contained strong elements of freedom that were gradually put into practice after his death, in particular in the fields of politics and society. After the successful fight for independence from Spain (1579), the Netherlands, under Calvinist leadership, became the freest country in Europe. It granted asylum to persecuted religious minorities, e.g. French Huguenots, English Independents (Congregationalists), and Jews from Spain and Portugal. The ancestors of philosopher Baruch Spinoza were Portuguese Jews. Aware of the trial against Galileo, René Descartes lived in the Netherlands, out of reach of the Inquisition. Pierre Bayle, a Reformed Frenchman, also felt safer in the Netherlands than in his home country. He was the first prominent philosopher who demanded tolerance for atheists. Hugo Grotius was able to publish a rather liberal interpretation of the Bible and his ideas about natural law. Moreover, the Calvinist Dutch authorities allowed the printing of books that could not be published elsewhere, e.g. Galileo's \"Discorsi\".\nEven more important than the liberal development of the Netherlands was the rise of modern democracy in England and North America. In the Middle Ages state and church had been closely connected. Martin Luther's doctrine of the two kingdoms separated state and church in principle. His doctrine of the priesthood of all believers raised the laity to the same level as the clergy. Going one step further, Calvin included elected laymen (church elders, presbyters) in his concept of church government. The Huguenots added synods whose members were also elected by the congregations. The other Reformed churches took over this system of church self-government which was essentially a representative democracy. Baptists, Quakers, and Methodists are organized in a similar way. These denominations and the Anglican Church were influenced by Calvin's theology in varying degrees.\n\nAnother precondition for the rise of democracy in the Anglo-American world was the fact that Calvin favored a mixture of democracy and aristocracy as the best form of government (mixed government). He appreciated the advantages of democracy. The aim of his political thought was to safeguard the rights and freedoms of ordinary men and women. In order to minimize the misuse of political power he suggested dividing it among several institutions in a system of checks and balances (separation of powers). Finally, Calvin taught that if worldly rulers rise up against God they should be put down. In this way, he and his followers stood in the vanguard of resistance to political absolutism and furthered the cause of democracy. The Congregationalists who founded Plymouth Colony (1620) and Massachusetts Bay Colony (1628) were convinced that the democratic form of government was the will of God. Enjoying self-rule they practiced separation of powers. Rhode Island, Connecticut, and Pennsylvania, founded by Roger Williams, Thomas Hooker, and William Penn, respectively, combined democratic government with freedom of religion. These colonies became safe havens for persecuted religious minorities, including Jews.\nIn England, Baptists Thomas Helwys and John Smyth influenced the liberal political thought of Presbyterian poet and politician John Milton and philosopher John Locke, who in turn had both a strong impact on the political development in their home country (English Civil War, Glorious Revolution) as well as in North America. The ideological basis of the American Revolution was largely provided by the radical Whigs, who had been inspired by Milton, Locke, James Harrington, Algernon Sidney, and other thinkers. The Whigs' \"perceptions of politics attracted widespread support in America because they revived the traditional concerns of a Protestantism that had always verged on Puritanism.\" The United States Declaration of Independence, the United States Constitution and (American) Bill of Rights initiated a tradition of human and civil rights that was continued in the French Declaration of the Rights of Man and the Citizen and the constitutions of numerous countries around the world, e. g. Latin America, Japan, India, Germany, and other European countries. It is also echoed in the United Nations Charter and the Universal Declaration of Human Rights.\n\nIn the nineteenth century, the churches that were based on Calvin's theology or influenced by it were deeply involved in social reforms, e.g. the abolition of slavery (William Wilberforce, Harriet Beecher Stowe, Abraham Lincoln, and others), women suffrage, and prison reforms. Members of these churches formed co-operatives to help the impoverished masses. Henry Dunant, a Reformed pietist, founded the Red Cross and initiated the Geneva Conventions.\n\nSome sources would view Calvinist influence as not always being solely positive. The Boers and Afrikaner Calvinists combined ideas from Calvinism and Kuyperian theology to justify apartheid in South Africa. As late as 1974, the majority of the Dutch Reformed Church in South Africa was convinced that their theological stances (including the story of the Tower of Babel) could justify apartheid. In 1990, the Dutch Reformed Church document \"Church and Society\" maintained that although they were changing their stance on apartheid, they believed that within apartheid and under God's sovereign guidance, \"...everything was not without significance, but was of service to the Kingdom of God.\" It should be noted that these views were not universal and were condemned by many Calvinists outside South Africa. It was pressure from both outside and inside the Dutch Reformed Calvinist church which helped reverse apartheid in South Africa.\n\nThroughout the world, the Reformed churches operate hospitals, homes for handicapped or elderly people, and educational institutions on all levels. For example, American Congregationalists founded Harvard (1636), Yale (1701), and about a dozen other colleges. Princeton was a Presbyterian foundation.\n\n\n\n\n\n\n\n\n"}
{"id": "6026", "url": "https://en.wikipedia.org/wiki?curid=6026", "title": "Countable set", "text": "Countable set\n\nIn mathematics, a countable set is a set with the same cardinality (number of elements) as some subset of the set of natural numbers. A countable set is either a finite set or a \"countably infinite\" set. Whether finite or infinite, the elements of a countable set can always be counted one at a time and, although the counting may never finish, every element of the set is associated with a unique natural number.\n\nSome authors use countable set to mean \"countably infinite\" alone. To avoid this ambiguity, the term \"at most countable\" may be used when finite sets are included and \"countably infinite\", \"enumerable\", or \"denumerable\" otherwise.\n\nGeorg Cantor introduced the term \"countable set\", contrasting sets that are countable with those that are \"uncountable\" (i.e., \"nonenumerable\" or \"nondenumerable\"). Today, countable sets form the foundation of a branch of mathematics called \"discrete mathematics\".\n\nA set is \"countable\" if there exists an injective function from to the natural numbers }.\n\nIf such an can be found that is also surjective (and therefore bijective), then is called \"countably infinite.\"\n\nIn other words, a set is \"countably infinite\" if it has one-to-one correspondence with the natural number set, .\n\nAs noted above, this terminology is not universal. Some authors use countable to mean what is here called \"countably infinite,\" and do not include finite sets.\n\nAlternative (equivalent) formulations of the definition in terms of a bijective function or a surjective function can also be given. See below.\n\nIn 1874, in his first set theory article, Cantor proved that the set of real numbers is uncountable, thus showing that not all infinite sets are countable. In 1878, he used one-to-one correspondences to define and compare cardinalities. In 1883, he extended the natural numbers with his infinite ordinals, and used sets of ordinals to produce an infinity of sets having different infinite cardinalities.\n\nA \"set\" is a collection of \"elements\", and may be described in many ways. One way is simply to list all of its elements; for example, the set consisting of the integers 3, 4, and 5 may be denoted {3, 4, 5}. This is only effective for small sets, however; for larger sets, this would be time-consuming and error-prone. Instead of listing every single element, sometimes an ellipsis (\"...\") is used, if the writer believes that the reader can easily guess what is missing; for example, {1, 2, 3, ..., 100} presumably denotes the set of integers from 1 to 100. Even in this case, however, it is still \"possible\" to list all the elements, because the set is \"finite\".\n\nSome sets are \"infinite\"; these sets have more than \"n\" elements for any integer \"n\". For example, the set of natural numbers, denotable by {0, 1, 2, 3, 4, 5, ...}, has infinitely many elements, and we cannot use any normal number to give its size. Nonetheless, it turns out that infinite sets do have a well-defined notion of size (or more properly, of \"cardinality\", which is the technical term for the number of elements in a set), and not all infinite sets have the same cardinality.\nTo understand what this means, we first examine what it \"does not\" mean. For example, there are infinitely many odd integers, infinitely many even integers, and (hence) infinitely many integers overall. However, it turns out that the number of even integers, which is the same as the number of odd integers, is also the same as the number of integers overall. This is because we arrange things such that for every integer, there is a distinct even integer: ... −2→−4, −1→−2, 0→0, 1→2, 2→4, ...; or, more generally, \"n\"→2\"n\", see picture. What we have done here is arranged the integers and the even integers into a \"one-to-one correspondence\" (or \"bijection\"), which is a function that maps between two sets such that each element of each set corresponds to a single element in the other set.\n\nHowever, not all infinite sets have the same cardinality. For example, Georg Cantor (who introduced this concept) demonstrated that the real numbers cannot be put into one-to-one correspondence with the natural numbers (non-negative integers), and therefore that the set of real numbers has a greater cardinality than the set of natural numbers.\n\nA set is \"countable\" if: (1) it is finite, or (2) it has the same cardinality (size) as the set of natural numbers. Equivalently, a set is \"countable\" if it has the same cardinality as some subset of the set of natural numbers. Otherwise, it is \"uncountable\".\n\nBy definition a set \"S\" is \"countable\" if there exists an injective function \"f\" : \"S\" → N from \"S\" to the natural numbers N = {0, 1, 2, 3, ...}.\n\nIt might seem natural to divide the sets into different classes: put all the sets containing one element together; all the sets containing two elements together; ...; finally, put together all infinite sets and consider them as having the same size.\nThis view is not tenable, however, under the natural definition of size.\n\nTo elaborate this we need the concept of a bijection. Although a \"bijection\" seems a more advanced concept than a number, the usual development of mathematics in terms of set theory defines functions before numbers, as they are based on much simpler sets. This is where the concept of a bijection comes in: define the correspondence\n\nSince every element of {\"a\", \"b\", \"c\"} is paired with \"precisely one\" element of {1, 2, 3}, \"and\" vice versa, this defines a bijection.\n\nWe now generalize this situation and \"define\" two sets as of the same size if (and only if) there is a bijection between them. For all finite sets this gives us the usual definition of \"the same size\". What does it tell us about the size of infinite sets?\n\nConsider the sets \"A\" = {1, 2, 3, ... }, the set of positive integers and \"B\" = {2, 4, 6, ... }, the set of even positive integers. We claim that, under our definition, these sets have the same size, and that therefore \"B\" is countably infinite. Recall that to prove this we need to exhibit a bijection between them. But this is easy, using \"n\" ↔ 2\"n\", so that\n\nAs in the earlier example, every element of A has been paired off with precisely one element of B, and vice versa. Hence they have the same size. This is an example of a set of the same size as one of its proper subsets, which is impossible for finite sets.\n\nLikewise, the set of all ordered pairs of natural numbers is countably infinite, as can be seen by following a path like the one in the picture: The resulting mapping is like this:\nThis mapping covers all such ordered pairs.\n\nInterestingly: if you treat each pair as being the numerator and denominator of a vulgar fraction, then for every positive fraction, we can come up with a distinct number corresponding to it. This representation includes also the natural numbers, since every natural number is also a fraction \"N\"/1. So we can conclude that there are exactly as many positive rational numbers as there are positive integers. This is true also for all rational numbers, as can be seen below.\n\nTheorem: The Cartesian product of finitely many countable sets is countable.\n\nThis form of triangular mapping recursively generalizes to vectors of finitely many natural numbers by repeatedly mapping the first two elements to a natural number. For example, (0,2,3) maps to (5,3), which maps to 39.\n\nSometimes more than one mapping is useful. This is where you map the set you want to show is countably infinite onto another set—and then map this other set to the natural numbers. For example, the positive rational numbers can easily be mapped to (a subset of) the pairs of natural numbers because \"p\"/\"q \"maps to (\"p\", \"q\").\n\nWhat about infinite subsets of countably infinite sets? Do these have fewer elements than N?\n\nTheorem: Every subset of a countable set is countable. In particular, every infinite subset of a countably infinite set is countably infinite.\n\nFor example, the set of prime numbers is countable, by mapping the \"n\"-th prime number to \"n\":\n\nWhat about sets being naturally \"larger than\" N? For instance, Z the set of all integers or Q, the set of all rational numbers, which intuitively may seem much bigger than N. But looks can be deceiving, for we assert:\n\nTheorem: Z (the set of all integers) and Q (the set of all rational numbers) are countable.\n\nIn a similar manner, the set of algebraic numbers is countable.\n\nThese facts follow easily from a result that many individuals find non-intuitive.\n\nTheorem: Any finite union of countable sets is countable. \nWith the foresight of knowing that there are uncountable sets, we can wonder whether or not this last result can be pushed any further. The answer is \"yes\" and \"no\", we can extend it, but we need to assume a new axiom to do so.\n\nTheorem: (Assuming the axiom of countable choice) The union of countably many countable sets is countable.\n\nFor example, given countable sets a, b, c, ...\nUsing a variant of the triangular enumeration we saw above:\n\n\nNote that this only works if the sets a, b, c, ... are disjoint. If not, then the union is even smaller and is therefore also countable by a previous theorem.\n\nAlso note that we need the axiom of countable choice to index \"all\" the sets a, b, c, ... simultaneously.\n\nTheorem: The set of all finite-length sequences of natural numbers is countable.\n\nThis set is the union of the length-1 sequences, the length-2 sequences, the length-3 sequences, each of which is a countable set (finite Cartesian product). So we are talking about a countable union of countable sets, which is countable by the previous theorem.\n\nTheorem: The set of all finite subsets of the natural numbers is countable.\n\nIf you have a finite subset, you can order the elements into a finite sequence. There are only countably many finite sequences, so also there are only countably many finite subsets.\n\nThe following theorem gives equivalent formulations in terms of a bijective function or a surjective function. A proof of this result can be found in Lang's text.\n\n(Basic) Theorem: Let \"S\" be a set. The following statements are equivalent:\n\nCorollary: Let \"S\" and \"T\" be sets.\n\nCantor's Theorem asserts that if \"A\" is a set and \"P\"(\"A\") is its power set, i.e. the set of all subsets of \"A\", then there is no surjective function from \"A\" to \"P\"(\"A\"). A proof is given in the article Cantor's Theorem. As an immediate consequence of this and the Basic Theorem above we have:\n\nProposition: The set \"P\"(N) is not countable; i.e. it is uncountable.\n\nFor an elaboration of this result see Cantor's diagonal argument.\n\nThe set of real numbers is uncountable (see Cantor's first uncountability proof), and so is the set of all infinite sequences of natural numbers.\nThe proofs of the statements in the above section rely upon the existence of functions with certain properties. This section presents functions commonly used in this role, but not the verifications that these functions have the required properties. The Basic Theorem and its Corollary are often used to simplify proofs. Observe that in that theorem can be replaced with any countably infinite set.\n\nProposition: Any finite set is countable.\n\nProof: By definition, there is a bijection between a non-empty finite set and the set {1, 2, ..., } for some positive natural number . This function is an injection from into .\n\nProposition: Any subset of a countable set is countable.\n\nProof: The restriction of an injective function to a subset of its domain is still injective.\n\nProposition: If is a countable set and , then } is countable.\n\nProof: Let be an injection. Define by and \n\nProposition: If and are countable sets then is countable.\n\nProof: Let and be injections. Define a new injection by if is in and if is in but not in .\n\nProposition: The Cartesian product of two countable sets and is countable.\n\nProof: Observe that is countable as a consequence of the definition because the function given by is injective. It then follows from the Basic Theorem and the Corollary that the Cartesian product of any two countable sets is countable. This follows because if and are countable there are surjections and . So\nis a surjection from the countable set to the set and the Corollary implies is countable. This result generalizes to the Cartesian product of any finite collection of countable sets and the proof follows by induction on the number of sets in the collection.\n\nProposition: The integers are countable and the rational numbers are countable.\n\nProof: The integers are countable because the function given by if is non-negative and if is negative, is an injective function. The rational numbers are countable because the function given by is a surjection from the countable set to the rationals .\n\nProposition: The algebraic numbers are countable.\n\nProof: Since all algebraic numbers (including complex numbers) are roots of a polynomial. Let the polynomial be formula_1, and the algebraic number formula_2 is the \"k\"th root of the polynomial (first, sorted by absolute value from small to big, then sorted by argument from small to big). We can define an injection (i. e. one-to-one) function given by formula_3, while formula_4 is the \"n\"-th prime.\n\nProposition: If is a countable set for each in then the union of all is also countable.\n\nProof: This is a consequence of the fact that for each there is a surjective function and hence the function\n\ngiven by is a surjection. Since is countable, the Corollary implies that the union is countable. We use the axiom of countable choice in this proof to pick for each in a surjection from the non-empty collection of surjections from to .\n\nA topological proof for the uncountability of the real numbers is described at finite intersection property.\n\nIf there is a set that is a standard model (see inner model) of ZFC set theory, then there is a minimal standard model (\"see\" Constructible universe). The Löwenheim-Skolem theorem can be used to show that this minimal model is countable. The fact that the notion of \"uncountability\" makes sense even in this model, and in particular that this model \"M\" contains elements that are:\nwas seen as paradoxical in the early days of set theory, see Skolem's paradox.\n\nThe minimal standard model includes all the algebraic numbers and all effectively computable transcendental numbers, as well as many other kinds of numbers.\n\nCountable sets can be totally ordered in various ways, e.g.:\n\nNote that in both examples of well orders here, any subset has a \"least element\"; and in both examples of non-well orders, \"some\" subsets do not have a \"least element\".\nThis is the key definition that determines whether a total order is also a well order.\n\n\n"}
{"id": "6034", "url": "https://en.wikipedia.org/wiki?curid=6034", "title": "Cahn–Ingold–Prelog priority rules", "text": "Cahn–Ingold–Prelog priority rules\n\nThe Cahn–Ingold–Prelog (CIP) sequence rules, named for organic chemists R.S. Cahn, C.K. Ingold, and V. Prelog—alternatively termed the CIP priority rules, \"system\", or \"conventions\"—are a standard process used in organic chemistry to completely and unequivocally name a stereoisomer of a molecule. The purpose of the CIP system is to assign an to each stereocenter and an E or Z descriptor to each double bond so that the configuration of the entire molecule can be specified uniquely by including the descriptors in its systematic name. A molecule may contain any number of stereocenters and any number of double bonds, and each usually gives rise to two possible isomers. A molecule with an integer formula_1 describing the number of its stereogenic centers will usually have formula_2 stereoisomers, formula_3 diastereomers each having an associated pair of enantiomers. The CIP sequence rules contribute to the precise naming of every stereoisomer of every organic and organometallic molecule with all atoms of ligancy of fewer than 4 (but including ligancy of 6 as well, this term referring to the \"number of neighboring atoms\" bonded to a center).\n\nThe key article setting out the CIP sequence rules was published in 1966, and was followed by further refinements, before it was incorporated into the rules of the International Union of Pure and Applied Chemistry, the official body that defines organic nomenclature. The IUPAC presentation of the rules constitute the official, formal standard for their use, and it notes that \"the method has been developed to cover all compounds with ligancy up to 4... and… [extended to the case of] ligancy 6… [as well as] for all configurations and conformations of such compounds.\" Nevertheless, though the IUPAC documentation presents a thorough introduction, it includes the caution that \"it is essential to study the original papers, especially the 1966 paper, before using the sequence rule for other than fairly simple cases.\"\n\nThe steps for naming molecules using the CIP system are often presented as:\n\n and E/Z descriptors are assigned by using a system for ranking priority of the groups attached to each stereocenter. This procedure, often known as \"the sequence rules\", is the heart of the CIP system.\n\nIf two groups differ only in isotopes, atomic masses are used at each step to break ties in atomic number.\n\nIf an atom \"A\" is double-bonded to an atom \"B\", \"A\" is treated as being singly bonded to two atoms: \"B\" and a \"ghost atom\" that is a duplicate of \"B\" (has the same atomic number) but is not attached to anything except \"A\". When \"B\" is replaced with a list of attached atoms, \"A\" itself is excluded in accordance with the general principle of not doubling back along a bond that has just been followed. A triple bond is handled the same way except that \"A\" and \"B\" both have duplicated 'ghost' atoms.\n\nIf two substituents on an atom are geometric isomers, the Z-isomer has higher priority than the E-isomer.\n\nTo handle a molecule containing one or more cycles, one must first expand it into a tree (called a hierarchical digraph by the authors) by traversing bonds in all possible paths starting at the stereocenter. When the traversal encounters an atom through which the current path has already passed, a ghost atom is generated in order to keep the tree finite. A single atom of the original molecule may appear in many places (some as ghosts, some not) in the tree.\n\nAfter the substituents of a stereocenter have been assigned their priorities, the molecule is oriented in space so that the group with the lowest priority is pointed away from the observer. If the substituents are numbered from 1 (highest priority) to 4 (lowest priority), then the sense of rotation of a curve passing through 1, 2 and 3 distinguishes the stereoisomers. A center with a clockwise sense of rotation is an \"R\" or \"rectus\" center and a center with a counterclockwise sense of rotation is an \"S\" or \"sinister\" center. The names are derived from the Latin for right and left, respectively.\n\nA practical method of determining whether an enantiomer is R or S is by using the right-hand rule: one wraps the molecule with the fingers in the direction 1→2→3. If the thumb points in the direction of the 4th substituent, the enantiomer is R. Otherwise, it's S.\n\nIt is possible in rare cases that two substituents on an atom differ only in their absolute configuration (\"R\" or \"S\"). If the relative priorities of these substituents need to be established, \"R\" takes priority over \"S\". When this happens, the descriptor of the stereocenter is a lowercase letter (\"r\" or \"s\") instead of the uppercase letter normally used.\n\nFor alkenes and similar double bonded molecules, the same prioritizing process is followed for the substituents. In this case, it is the placing of the two highest priority substituents with respect to the double bond which matters. If both high priority substituents are on the same side of the double bond, i.e. in the cis configuration, then the stereoisomer is assigned a \"Z\" or \"Zusammen configuration\". If, by contrast they are in a trans configuration, then the stereoisomer is assigned an \"E\" or \"Entgegen configuration\". In this case the identifying letters are derived from German for 'together' and 'in opposition to', respectively.\n\nThe following are examples of application of the nomenclature.\n\nIf a compound has more than one stereocenter each center is denoted by either R or S. For example, ephedrine exists with both (1R,2S) and (1S,2R) configuration, known as enantiomers. This compound also exists with a (1R,2R) and (1S,2S) configuration. The last two stereoisomers are not ephedrine, but pseudoephedrine. All isomers are 2-methylamino-1-phenyl-1-propanol in systematic nomenclature. Pseudoephedrine is chemically distinct from ephedrine with only the three-dimensional configuration in space, as notated by the Cahn–Ingold–Prelog rules. The two compounds, ephedrine and pseudoephedrine, are diastereomers, or stereoisomers that are not enantiomers. They have different names because, as diastereomers, they have different chemical properties.\n\nIn pairs of enantiomers, all descriptors are opposite: R,R and S,S or R,S and S,R. Diastereomers have one descriptor in common: R,S and R,R or S,R and S,S. This holds true for compounds with more than two stereocenters; if at least one descriptor is the same in both pairs, the compounds are diastereomers. If \"all\" the stereocenters are opposite, they are enantiomers.\n\nThe relative configuration of two stereoisomers may be denoted by the descriptors R and S with an asterisk (*). \"R*,R*\" means two centers having identical configurations (R,R or S,S); \"R*,S*\" means two centers having opposite configurations (R,S or S,R). To begin, the lowest numbered (according to IUPAC systematic numbering) stereogenic center is given the R* descriptor.\n\nTo designate two anomers the relative stereodescriptors alpha (α) and beta (β) are used. In the α anomer the \"anomeric carbon atom\" and the \"reference atom\" do have opposite configurations (R,S or S,R), whereas in the β anomer they are the same (both R or both S).\nStereochemistry also plays a role assigning \"faces\" to trigonal molecules such as ketones. A nucleophile in a nucleophilic addition can approach the carbonyl group from two opposite sides or faces. When an achiral nucleophile attacks acetone, both faces are identical and there is only one reaction product. When the nucleophile attacks butanone, the faces are not identical (\"enantiotopic\") and a racemic product results. When the nucleophile is a chiral molecule diastereoisomers are formed. When one face of a molecule is shielded by substituents or geometric constraints compared to the other face the faces are called diastereotopic. The same rules that determine the stereochemistry of a stereocenter (R or S) also apply when assigning the face of a molecular group. The faces are then called the re-faces and si-faces. In the example displayed on the right, the compound acetophenone is viewed from the re face. Hydride addition as in a reduction process from this side will form the S-enantiomer and attack from the opposite Si face will give the R-enantiomer. However, one should note that adding a chemical group to the prochiral center from the re-face will not always lead to an S stereocenter, as the priority of the chemical group has to be taken into account. That is, the absolute stereochemistry of the product is determined on its own and not by considering which face it was attacked from. In the above-mentioned example, if chloride (Cl-) was added to the prochiral center from the re-face, this would result in an R-enantiomer.\n"}
{"id": "6035", "url": "https://en.wikipedia.org/wiki?curid=6035", "title": "Celibacy", "text": "Celibacy\n\nCelibacy (from Latin, \"cælibatus\"\") is the state of voluntarily being unmarried, sexually abstinent, or both, usually for religious reasons. It is often in association with the role of a religious official or devotee. In its narrow sense, the term \"celibacy\" is applied only to those for whom the unmarried state is the result of a sacred vow, act of renunciation, or religious conviction. In a wider sense, it is commonly understood to only mean abstinence from sexual activity.\n\nCelibacy has existed in one form or another throughout history, in virtually all the major religions of the world, and views on it have varied. Ancient Judaism was strongly opposed to celibacy. Similarly, the Romans viewed it as an aberration and legislated fiscal penalties against it, with the sole exception granted to the Vestal Virgins. Christians in the Middle Ages and in particular Catholics believed that celibacy was a prerequisite for religious office (clerical celibacy). Protestantism saw a reversal of this trend in the West and the Eastern Orthodox Church never adopted it. The Islamic attitudes toward celibacy have been complex as well; Muhammad denounced it, but some Sufi orders embrace it.\n\nClassical Hindu culture encouraged asceticism and celibacy in the later stages of life, after one has met his societal obligations. Jainism and Buddhism have been influenced by Hinduism in this respect. There were, however, significant cultural differences in the various areas where Buddhism spread, which affected the local attitudes toward celibacy. It was not well received in China, for example, where other religions movements such as Daoism were opposed to it. A somewhat similar situation existed in Japan, where the Shinto tradition also opposed celibacy. In most native African and American Indian religious traditions, celibacy has been viewed negatively as well, although there were exceptions like periodic celibacy practiced by some Mesoamerican warriors.\n\nThe English word \"celibacy\" derives from the Latin \"caelibatus\", \"state of being unmarried\", from Latin , meaning \"unmarried\". This word derives from two Proto-Indo-European stems, * \"alone\" and * \"living\".\n\nThe words \"abstinence\" and \"celibacy\" are often used interchangeably, but are not necessarily the same thing. Sexual abstinence, also known as \"continence\", is abstaining from some or all aspects of sexual activity, often for some limited period of time, while celibacy may be defined as a voluntary religious vow not to marry or engage in sexual activity. Asexuality is commonly conflated with celibacy and sexual abstinence, but it is considered distinct from the two, as celibacy and sexual abstinence are behavioral and generally motivated by factors such as an individual's personal or religious beliefs.\n\nA. W. Richard Sipe, while focusing on the topic of celibacy in Catholicism, states that \"the most commonly assumed definition of \"celibate\" is simply an unmarried or single person, and celibacy is perceived as synonymous with sexual abstinence or restraint.\" Sipe adds that even in the relatively uniform milieu of Catholic priests in the United States \"there is simply no clear operational definition of celibacy\". Elizabeth Abbott commented on the terminology in her \"A History of Celibacy\" (2001): \"I also drafted a definition that discarded the rigidly pedantic and unhelpful distinctions between celibacy, chastity and virginity\".\n\nThe concept of \"new celibacy\" was introduced by Gabrielle Brown in her 1980 book \"The New Celibacy\". In a revised version (1989) of her book, she claims that \"abstinence is a response on the outside to what's going on, and celibacy is a response from the inside\". According to her definition, celibacy (even short-term celibacy that is pursued for non-religious reasons) is much more than not having sex. It is more intentional than abstinence, and its goal is personal growth and empowerment. This new perspective on celibacy is echoed by several authors including Elizabeth Abbott, Wendy Keller, and Wendy Shalit.\n\nThe rule of celibacy in the Buddhist religion, whether Mahayana or Theravada, has a long history. Celibacy was advocated as an ideal rule of life for all monks and nuns by Gautama Buddha, except for Japan where it is not strictly followed due to historical and political developments following the Meiji Restoration. In Japan, celibacy was an ideal among Buddhist clerics for hundreds of years. But violations of clerical celibacy were so common for so long that, finally, in 1872, state laws made marriage legal for Buddhist clerics. Subsequently, ninety percent of Buddhist monks/clerics married. An example is Higashifushimi Kunihide, a prominent Buddhist priest of Japanese royal ancestry who was married and a father whilst serving as a monk for most of his lifetime.\n\nGautama, later known as the Buddha, is known for his renunciation of his wife, Princess Yasodharā, and son, Rahula. In order to pursue an ascetic life, he needed to renounce aspects of the impermanent world, including his wife and son. Later on both his wife and son joined the ascetic community and are mentioned in the Buddhist texts to have become enlightened. In another sense, a buddhavacana recorded the zen patriarch Vimalakirti as being an advocate of marital continence instead of monastic renunciation, the sutra became somewhat popular due to its brash humour as well as integrating the role of women in laity as well as spiritual life.\n\nIn the religious movement of Brahma Kumaris, celibacy is also promoted for peace and to defeat power of lust and to prepare for life in forthcoming Heaven on earth for 2,500 years when children will be created by the power of the mind even for householders to like holy brother and sister.\n\nIn this belief system, celibacy is given the utmost importance. It is said that, as per the direction of the Supreme God those lead a pure and celibate life will be successfully able to conquer the surging vices. The power of celibacy creates an unseen environment of divinity bringing peace, power, purity, prosperity and fortune. Those with the power of celibacy are eligible to claim a bright future of Golden Age of heaven / Paradise. Brahma Kumaris' concept of identifying the self as a soul, different from physical body, is deeply linked to the philosophy of celibacy. It is said that the craving for sex and impure thoughts are the reason for the whole trouble in the universe today. And celibacy is to lead the pure relationship in one's life.\n\nIn Jesus says, \"All men cannot receive this saying, save they to whom it is given. For there are some eunuchs, which were so born from their mother's womb: and there are some eunuchs, which were made eunuchs of men: and there be eunuchs, which have made themselves eunuchs for the kingdom of heaven's sake. He that is able to receive it, let him receive it.\"\n\nWhen Jesus discusses marriage, he points out that there is some responsibility for a man marrying a woman (and vice versa). Not having assets of their own, women needed to be protected from the risk of their husbands' putting them on the street at whim. In those times marriage was an economic matter rather than one of love. A woman and her children could easily be rejected. Restriction of divorce was based on the necessity of protecting the woman and her position in society, not necessarily in a religious context, but an economic context. He also points out that there are those \"which were made eunuchs of men: and there be eunuchs, which have made themselves eunuchs for the kingdom of heaven's sake\", but in the original Greek, the word εὐνοῦχος means \"castrated person\". It was the custom at the time Jesus lived for priests of some ancient gods and goddesses to be castrated. In the pre-Christian period Vestals, who served the virgin goddess of the hearth, were obliged to forgo marriage, and so were some priests and servants of some ancient deities such as Isis. Jewish priests are allowed to marry. However, they are not allowed to marry a divorcee or certain other classes of women, or, in the case of a high priest, a widow (Leviticus 21:7, 8, 14 and 15).\n\nThere is no commandment in the New Testament that Jesus' disciples have to live in celibacy. The general view on sexuality among the early Jewish Christians was quite positive. Jesus himself does not speak in negative terms of the body in the New Testament. While the Jewish sect of essenes practiced celibacy the general practice of the Jewish community by that time prescribed marriage for everybody, and at an early age. Saint Peter, also known as Simon Peter, the Apostle was married; Jesus healed Simon Peter's mother-in-law (Matt. 8:14), and other apostles and church members among the early Jewish Christians were also married: Paul's personal friends, Priscilla and Aquila (), who were Paul's coworkers, Andronicus of Pannonia (), and Junia (), who were highly regarded among the apostles, Ananias and Sapphira (Ap 5:1), Apphia and Philemon (Phil 1: 1). According to Eusebius Church History (\"Historia Ecclesiastica\"), Paul the Apostle, also known as Saul of Tarsus, was also married, though this seems to be contradicted by what he says in 1 Corinthians 7 (see below). It was the custom in the Jewish community to marry early.\n\nIn his early writings, Paul the Apostle described marriage as a social obligation that has the potential of distracting from Christ. Sex, in turn, is not sinful but natural, and sex within marriage is both proper and necessary. In his later writings, Paul made parallels between the relations between spouses and God's relationship with the church. \"Husbands love your wives even as Christ loved the church. Husbands should love their wives as their own bodies\" (Ephesians 5:25–28). The early Christians lived in the belief that the End of the World would soon come upon them, and saw no point in planning new families and having children. This was why Paul encouraged both celibate and marital lifestyles among the members of the Corinthian congregation, regarding celibacy as the preferable of the two:\n\nPaul the Apostle emphasized the importance of overcoming the desires of the flesh and saw the state of celibacy being superior to the marriage.\n\nIn the Catholic Church, a consecrated virgin, is a woman who has been consecrated by the church to a life of perpetual virginity in the service of God. According to most Christian thought, the first sacred virgin was Mary, the mother of Jesus, who was consecrated by the Holy Spirit during the Annunciation. Tradition also has it that the Apostle Matthew consecrated virgins. A number of early Christian martyrs were women or girls who had given themselves to Christ in perpetual virginity, such as Saint Agnes and Saint Lucy.\n\nThe Desert Fathers were Christian hermits, and ascetics who had a major influence on the development of Christianity and celibacy. Paul of Thebes is often credited with being the first hermit monk to go to the desert, but it was Anthony the Great who launched the movement that became the Desert Fathers. Sometime around AD 270, Anthony heard a Sunday sermon stating that perfection could be achieved by selling all of one's possessions, giving the proceeds to the poor, and following Christ.(Matt. 19.21) He followed the advice and made the further step of moving deep into the desert to seek complete solitude.\n\nOver time, the model of Anthony and other hermits attracted many followers, who lived alone in the desert or in small groups. They chose a life of extreme asceticism, renouncing all the pleasures of the senses, rich food, baths, rest, and anything that made them comfortable. Thousands joined them in the desert, mostly men but also a handful of women. Religious seekers also began going to the desert seeking advice and counsel from the early Desert Fathers. By the time of Anthony's death, there were so many men and women living in the desert in celibacy that it was described as \"a city\" by Anthony's biographer. The first Conciliar document on celibacy of the Western Christian Church (Canon 33 of the Synod of Elvira, ) states that the discipline of celibacy is to refrain from the \"use\" of marriage, i.e. refrain from having carnal contact with your spouse.\n\nAccording to the later St. Jerome (420) celibacy is a moral virtue, consisting of living in the flesh, but outside the flesh, and so being not corrupted by it (\"vivere in carne praeter carnem\"). Celibacy excludes not only libidinous acts, but also sinful thoughts or desires of the flesh. Jerome referred to marriage prohibition for priests when he claimed in \"Against Jovinianus\" that Peter and the other apostles had been married before they were called, but subsequently gave up their marital relations.\nCelibacy as a vocation may be independent from religious vows (as is the case with consecrated virgins, ascetics and hermits). In the Catholic, Orthodox and Oriental Orthodox traditions, bishops are required to be celibate. In the Eastern Christian traditions, priests and deacons are allowed to be married, yet have to remain celibate if they are unmarried at the time of ordination.\n\nIn the early Church higher clerics lived in marriages. Augustine of Hippo was one of the first to develop a theory that sexual feelings were sinful and negative. Augustine taught that the original sin of Adam and Eve was either an act of \"foolishness\" (\"insipientia\") followed by \"pride\" and \"disobedience\" to God, or else inspired by pride. The first couple disobeyed God, who had told them not to eat of the Tree of the knowledge of good and evil (Gen 2:17). The tree was a symbol of the order of creation. Self-centeredness made Adam and Eve eat of it, thus failing to acknowledge and respect the world as it was created by God, with its hierarchy of beings and values. They would not have fallen into pride and lack of wisdom, if Satan hadn't sown into their senses \"the root of evil\" (\"radix Mali\"). Their nature was wounded by concupiscence or libido, which affected human intelligence and will, as well as affections and desires, including sexual desire.\nThe sin of Adam is inherited by all human beings. Already in his pre-Pelagian writings, Augustine taught that Original Sin was transmitted by concupiscence, which he regarded as the passion of both, soul and body, making humanity a \"massa damnata\" (mass of perdition, condemned crowd) and much enfeebling, though not destroying, the freedom of the will.\n\nIn the early 3rd century, the Canons of the Apostolic Constitutions decreed that only lower clerics might still marry after their ordination, but marriage of bishops, priests, and deacons were not allowed. Augustine's view of sexual feelings as sinful affected his view of women. For example, he considered a man’s erection to be sinful, though involuntary, because it did not take place under his conscious control. His solution was to place controls on women to limit their ability to influence men. He equated flesh with woman and spirit with man.\n\nHe believed that the serpent approached Eve because she was less rational and lacked self-control, while Adam's choice to eat was viewed as an act of kindness so that Eve would not be left alone. Augustine believed sin entered the world because man (the spirit) did not exercise control over woman (the flesh). Augustine's views on women were not all negative, however. In his \"Tractates on the Gospel of John\", Augustine, commenting on the Samaritan woman from John 4:1–42, uses the woman as a figure of the church.\n\nAccording to Raming, the authority of the \"Decretum Gratiani\", a collection of Roman Catholic canon law which prohibits women from leading, teaching, or being a witness, rests largely on the views of the early church fathers, especially St. Augustine. The laws and traditions founded upon St. Augustine's views of sexuality and women continue to exercise considerable influence over church doctrinal positions regarding the role of women in the church.\n\nOne explanation for the origin of obligatory celibacy is that it is based on the writings of Saint Paul, who wrote of the advantages celibacy allowed a man in serving the Lord. Celibacy was popularised by the early Christian theologians like Saint Augustine of Hippo and Origen. Another possible explanation for the origins of obligatory celibacy revolves around more practical reason, \"the need to avoid claims on church property by priests' offspring\". It remains a matter of Canon Law (and often a criterion for certain religious orders, especially Franciscans) that priests may not own land and therefore cannot pass it on to legitimate or illegitimate children. The land belongs to the Church through the local diocese as administered by the Local Ordinary (usually a bishop), who is often an \"ex officio\" corporation sole. Celibacy is viewed differently by the Catholic Church and the various Protestant communities. It includes clerical celibacy, celibacy of the consecrated life, voluntary lay celibacy, and celibacy outside of marriage.\n\nThe Protestant Reformation rejected celibate life and sexual continence for preachers. Protestant celibate communities have emerged, especially from Anglican and Lutheran backgrounds. A few minor Christian sects advocate celibacy as a better way of life. These groups included the Shakers, the Harmony Society and the Ephrata Cloister. Celibacy not only for religious and monastics (brothers/monks and sisters/nuns) but also for bishops is upheld by the Catholic Church traditions.\n\nMany evangelicals prefer the term \"abstinence\" to \"celibacy.\" Assuming everyone will marry, they focus their discussion on refraining from premarital sex and focusing on the joys of a future marriage. But some evangelicals, particularly older singles, desire a positive message of celibacy that moves beyond the \"wait until marriage\" message of abstinence campaigns. They seek a new understanding of celibacy that is focused on God rather than a future marriage or a lifelong vow to the Church.\n\nThere are also many Pentecostal churches which practice celibate ministry. For instance, The Pentecostal Mission is a church spread worldwide which strictly forbids its ministers to marry.\n\nDuring the first three or four centuries, no law was promulgated prohibiting clerical marriage. Celibacy was a matter of choice for bishops, priests, and deacons. The early Church resisted some forms of asceticism. Scripture reflects the fact that early Christians embraced marriage and yet felt that an ascetic bias against marriage was seeping into their culture: 1 Timothy 4:1 \"In the last times, some will turn away from the faith by paying attention to deceitful spirits and demonic instructions through the hypocrisy of liars with branded consciences. They forbid marriage and require abstinence from foods that God created to be received with thanksgiving for those who believe and know the truth. For everything created by God is good and nothing is to be rejected when received with thanksgiving. For it is made holy by the invocation of God in prayer\".\n\nStatutes forbidding clergy from having wives were written beginning with the Council of Elvira (306) but these early statutes were not universal and were often defied by clerics and then retracted by hierarchy. The Synod of Gangra (345) condemned a false asceticism whereby worshipers boycotted celebrations presided over by married clergy.\" The Apostolic Constitutions () excommunicated a priest or bishop who left his wife ‘under the pretense of piety”’ (Mansi, 1:51).\n\n\"A famous letter of Synesius of Cyrene () is evidence both for the respecting of personal decision in the matter and for contemporary appreciation of celibacy. For priests and deacons clerical marriage continued to be in vogue\".\n\n\"The Second Lateran Council (1139) seems to have enacted the first written law making sacred orders a diriment impediment to marriage for the universal Church.\" Celibacy was first required of some clerics in 1123 at the First Lateran Council. Because clerics resisted it, the celibacy mandate was restated at the Second Lateran Council (1139) and the Council of Trent (1545–64). In places, coercion and enslavement of clerical wives and children was apparently involved in the enforcement of the law. “The earliest decree in which the children [of clerics] were declared to be slaves and never to be enfranchised [freed] seems to have been a canon of the Synod of Pavia in 1018. Similar penalties were promulgated against wives and concubines (see the Synod of Melfi, 1189 can. Xii), who by the very fact of their unlawful connexion with a subdeacon or clerk of higher rank became liable to be seized by the over-lord”. Mandatory celibacy for priests continues to be a contested issue even today.\n\nIn the Roman Catholic Church, the Twelve Apostles are considered to have been the first priests and bishops of the Church. Some say the call to be eunuchs for the sake of Heaven in Matthew 19 was a call to be sexually continent and that this developed into mandatory celibacy for priests as the successors of the apostles. Others see the call to be sexually continent in Matthew 19 to be a caution for men who were too readily divorcing and remarrying.\n\nThe view of the Church is that celibacy is a reflection of life in Heaven, a source of detachment from the material world which aids in one's relationship with God. Celibacy is designed to \"consecrate themselves with undivided heart to the Lord and to \"the affairs of the Lord, they give themselves entirely to God and to men. It is a sign of this new life to the service of which the Church's minister is consecrated; accepted with a joyous heart celibacy radiantly proclaims the Reign of God.\" In contrast, Saint Peter, whom the Church considers its first Pope, was married given that he had a mother-in-law whom Christ healed (Matthew 8).\n\nUsually, only celibate men are ordained as priests in the Latin Rite. More recently, married clergy who have converted from other Christian denominations have been ordained Roman Catholic priests without becoming celibate. Mandatory priestly celibacy is not \"doctrine\" of the Church (such as the belief in the Assumption of Mary) but a matter of discipline, like the use of the vernacular (local) language in Mass or Lenten fasting and abstinence. As such, it can theoretically change at any time though it still must be obeyed by Catholics until the change were to take place. The Eastern Catholic Churches ordain both celibate and married men. However, in both the East and the West, bishops are chosen from among those who are celibate. In Ireland, several priests have fathered children, the two most prominent being Bishop Eamonn Casey and Father Michael Cleary.\n\nThe classical heritage flourished throughout the Middle Ages in both the Byzantine Greek East and the Latin West. Will Durant has made a case that certain prominent features of Plato's ideal community were discernible in the organization, dogma and effectiveness of \"the\" Medieval Church in Europe:\n\n\"The clergy, like Plato's guardians, were placed in authority... by their talent as shown in ecclesiastical studies and administration, by their disposition to a life of meditation and simplicity, and ... by the influence of their relatives with the powers of state and church. In the latter half of the period in which they ruled [AD 800 onwards], the clergy were as free from family cares as even Plato could desire [for such guardians]... [Clerical] Celibacy was part of the psychological structure of the power of the clergy; for on the one hand they were unimpeded by the narrowing egoism of the family, and on the other their apparent superiority to the call of the flesh added to the awe in which lay sinners held them...\" \"In the latter half of the period in which they ruled, the clergy were as free from family cares as even Plato could desire\".\n\n\"Greater understanding of human psychology has led to questions regarding the impact of celibacy on the human development of the clergy. The realization that many non-European countries view celibacy negatively has prompted questions concerning the value of retaining celibacy as an absolute and universal requirement for ordained ministry in the Roman Catholic Church\"\n\n\"The declining number of priests in active ministry, the exemption from the requirement of celibacy for married clergy who enter the Catholic Church after having been ordained in the Episcopal Church, and reported incidences of de facto nonobservance of the requirement by clergy in various parts of the world, especially in Africa and Latin America, suggests that the discussion [of celibacy] will continue\"\nCatholic Churches developed the less well-known institution of chaste marriage.\n\nThe reintroduction of a permanent diaconate has permitted the Church to allow married men to become deacons but they may not go on to become priests.\n\nSome homosexual Christians choose to be celibate following their denomination's teachings on homosexuality.\n\nIn 2014, the American Association of Christian Counselors amended its code of ethics to eliminate the promotion of conversion therapy for homosexuals and encouraged them to be celibate instead.\n\nIn Hinduism, celibacy is usually associated with the \"sadhus\" (\"holy men\"), ascetics who withdraw from society and renounce all worldly ties. Celibacy, termed \"brahmacharya\" in Vedic scripture, is the fourth of the \"yamas\" and the word literally translated means \"dedicated to the Divinity of Life\". The word is often used in yogic practice to refer to celibacy or denying pleasure, but this is only a small part of what \"brahmacharya\" represents. The purpose of practicing \"brahmacharya\" is to keep a person focused on the purpose in life, the things that instill a feeling of peace and contentment.\n\nIslamic attitudes toward celibacy have been complex, Muhammad denounced it, however some Sufi orders embrace it. Islam does not promote celibacy; rather it condemns premarital sex and extramarital sex. In fact, according to Islam, marriage enables one to attain the highest form of righteousness within this sacred spiritual bond and is as such to be sought after and desired. It disagrees with the concept that marriage acts as a form of distraction in attaining nearness to God. The Qur'an (57:27) states, \"But the Monasticism which they invented for themselves, We did not prescribe for them but only to please God therewith, but that they did not observe it with the right observance.\"\n\nThe following sayings about the Prophet also address celibacy:\n\n\"There have been people who have come to the prophet and explained how they love to be engaged in prayer and fasting for the sake of God. The Prophet Mohammed told them that, despite this being good, it is also a blessing to raise a family, to remain moderate and not to concentrate too much on one aspect as not only can this be unhealthy for an individual as well as upon society, it may also take one away from God.\"\n\nCelibacy appears as a peculiarity among some Sufis.\n\nCelibacy was practiced by women saints in Sufism. Celibacy was debated along with women's roles in Sufism in medieval times.\n\nCelibacy, poverty, meditation, and mysticism within an ascetic context along with worship centered around Saint's tombs were promoted by the Qadiri Sufi order among Hui Muslims in China. In China, unlike other Muslim sects, the leaders (Shaikhs) of the Qadiriyya Sufi order are celibate. Unlike other Sufi orders in China, the leadership within the order is not a hereditary position, rather, one of the disciples of the celibate Shaikh is chosen by the Shaikh to succeed him . The 92-year-old celibate Shaikh Yang Shijun was the leader of the Qadiriya order in China as of 1998.\n\nCelibacy is practiced by Haydariya Sufi dervishes.\n\nThe spiritual teacher Meher Baba stated that \"[F]or the [spiritual] aspirant a life of strict celibacy is preferable to married life, if restraint comes to him easily without undue sense of self-repression. Such restraint is difficult for most persons and sometimes impossible, and for them married life is decidedly more helpful than a life of celibacy. For ordinary persons, married life is undoubtedly advisable unless they have a special aptitude for celibacy\". Baba also asserted that \"The value of celibacy lies in the habit of restraint and the sense of detachment and independence which it gives\" and that \"The aspirant must choose one of the two courses which are open to him. He must take to the life of celibacy or to the married life, and he must avoid at all costs a cheap compromise between the two. Promiscuity in sex gratification is bound to land the aspirant in a most pitiful and dangerous chaos of ungovernable lust.\"\n\nIn Sparta and many other Greek cities, failure to marry was grounds for loss of citizenship, and could be prosecuted as a crime. Both Cicero and Dionysius of Halicarnassus stated that Roman law forbade celibacy. There are no records of such a prosecution, nor is the Roman punishment for refusing to marry known.\n\nPythagoreanism was the system of esoteric and metaphysical beliefs held by Pythagoras and his followers. Pythagorean thinking was dominated by a profoundly mystical view of the world. The Pythagorean code further restricted his members from eating meat, fish, and beans which they practised for religious, ethical and ascetic reasons, in particular the idea of metempsychosis – the transmigration of souls into the bodies of other animals.\n\"Pythagoras himself established a small community that set a premium on study, vegetarianism, and sexual restraint or abstinence. Later philosophers believed that celibacy would be conducive to the detachment and equilibrium required by the philosopher's calling.\"\n\nThe tradition of sworn virgins developed out of the \"Kanuni i Lekë Dukagjinit\" (, or simply the \"Kanun\"). The \"Kanun\" is not a religious document – many groups follow it, including Roman Catholics, the Albanian Orthodox, and Muslims.\n\nWomen who become sworn virgins make a vow of celibacy, and are allowed to take on the social role of men: inheriting land, wearing male clothing, etc.\n\n\n\n"}
{"id": "6036", "url": "https://en.wikipedia.org/wiki?curid=6036", "title": "Coalition government", "text": "Coalition government\n\nA coalition government is a cabinet of a parliamentary government in which multiple political parties cooperate, reducing the dominance of any one party within that coalition. The usual reason for this arrangement is that no party on its own can achieve a majority in the parliament. A coalition government might also be created in a time of national difficulty or crisis (for example, during wartime or economic crisis) to give a government the high degree of perceived political legitimacy or collective identity it desires while also playing a role in diminishing internal political strife. In such times, parties have formed all-party coalitions (national unity governments, grand coalitions). If a coalition collapses, a confidence vote is held or a motion of no confidence is taken.\n\nWhen a general election does not produce a clear majority for a single party, parties either form coalition cabinets, supported by a parliamentary majority, or minority cabinets which may consist of one or more parties. Cabinets based on a group of parties that command a majority in parliament tend to be more stable and long-lived than minority cabinets. While the former are prone to internal struggles, they have less reason to fear votes of no confidence. Majority governments based on a single party are typically even more stable, as long as their majority can be maintained.\n\nCountries which often operate with coalition cabinets include: the Nordic countries, the Benelux countries, Australia, Austria, Cyprus, France, Germany, Greece, India, Indonesia, Ireland, Israel, Italy, Japan, Kenya, Kosovo, Latvia, Lebanon, Nepal, New Zealand, Pakistan, Thailand, Trinidad and Tobago, Turkey and Ukraine. Switzerland has been ruled by a coalition of the four strongest parties in parliament from 1959 to 2008, called the \"Magic Formula\". Between 2010 and 2015, the United Kingdom also operated a formal coalition between the Conservative and the Liberal Democrat parties, but this was unusual: the UK usually has a single-party majority government.\n\nIn the United Kingdom, coalition governments (sometimes known as \"national governments\") usually have only been formed at times of national crisis. All six coalition governments in the last 120 years have involved the Liberal and Conservative parties. The most prominent was the National Government of 1931 to 1940. There were multi-party coalitions during both world wars. Apart from this, when no party has had a majority, minority governments normally have been formed with one or more opposition parties agreeing to vote in favour of the legislation which governments need to function: for instance the Labour government of James Callaghan came to an agreement with the Liberals in 1977 when it lost the narrow majority it had gained in the October 1974 election. However, in the run-up to the 1997 general election, Labour opposition leader Tony Blair was in talks with Liberal Democrat leader Paddy Ashdown about forming a coalition government if Labour failed to win a majority at the election; but there proved to be no need for a coalition as Labour won the election by a landslide. The 2010 general election resulted in a hung parliament (Britain's first for 36 years), and the Conservatives, led by David Cameron, which had won the largest number of seats, formed a coalition with the Liberal Democrats in order to gain a parliamentary majority, ending 13 years of Labour government. This was the first time that the Conservatives and Lib Dems had made a power-sharing deal at Westminster. It was also the first full coalition in Britain since 1945, having been formed 70 years virtually to the day after the establishment of Winston Churchill's wartime coalition, although there had been a \"Lib-Lab pact\", an agreement stopping well short of a coalition, between the Labour and Liberal parties, from March 1977 until July 1978, after a series of by-election defeats had eroded Labour's majority of three seats which had been gained at the October 1974 election.\n\nIn Germany, for instance, coalition government is the norm, as it is rare for either the Christian Democratic Union of Germany together with their partners the Christian Social Union in Bavaria (CDU/CSU), or the Social Democratic Party of Germany (SPD), to win an unqualified majority in a national election. Thus, at the federal level, governments are formed with at least two parties. For example, Helmut Kohl's CDU governed for years in coalition with the Free Democratic Party (FDP); from 1998 to 2005 Gerhard Schröder's SPD was in power with the Greens; and from 2009 Angela Merkel, CDU/CSU was in power with the FDP.\n\n\"Grand coalitions\" of the two large parties also occur, but these are relatively rare, as large parties usually prefer to associate with small ones. However, if none of the larger parties can receive enough votes to form their preferred coalition, a grand coalition might be their only choice for forming a government. This was the situation in Germany in 2005 when Angela Merkel became Chancellor: in early elections, the CDU/CSU did not garner enough votes to form a majority coalition with the FDP; similarly the SPD and Greens did not have enough votes to continue with their formerly ruling coalition. A grand coalition government was subsequently forged between the CDU/CSU and the SPD. Partnerships like these typically involve carefully structured cabinets. The CDU/CSU ended up holding the Chancellory while the SPD took the majority of cabinet posts.\n\nIn Germany, coalitions rarely consist of more than two parties (CDU and CSU, two allies which always form a single caucus, are in this regard considered a single party).\n\nIn federal Australian politics, the conservative Liberal, National, Country Liberal and Liberal National parties are united in a coalition, known simply as the Coalition. The Coalition has become so stable, at least at the federal level, that in practice the lower house of Parliament has become a two-party house, with the Coalition and the Labor Party being the major parties. This coalition is also found in the states of New South Wales and Victoria. In South Australia and Western Australia the Liberal and National parties compete separately, while in the Northern Territory and Queensland the two parties have merged, forming the Country Liberal Party, in 1978, and the Liberal National Party, in 2008, respectively.\n\nThe other federal coalition has been:\n\nIn Belgium, where there are separate Dutch-speaking and French-speaking parties for each political grouping, coalition cabinets of up to six parties are common.\n\nIn Canada, the Great Coalition was formed in 1864 by the Clear Grits, Parti bleu, and Liberal-Conservative Party. During the First World War, Prime Minister Robert Borden attempted to form a coalition with the opposition Liberals to broaden support for controversial conscription legislation. The Liberal Party refused the offer but some of their members did cross the floor and join the government. Although sometimes referred to as a coalition government, according to the definition above, it was not. It was disbanded after the end of the war.\n\nIn British Columbia, the governing Liberals formed a coalition with the opposition Conservatives in order to prevent the surging, left-wing Cooperative Commonwealth Federation from taking power in the British Columbia general election, 1941. Liberal premier Duff Pattullo refused to form a coalition with the third-place Conservatives, so his party removed him. The Liberal–Conservative coalition introduced a winner-take-all preferential voting system (the \"Alternative Vote\") in the hopes that their supporters would rank the other party as their second preference; however, this strategy did not take CCF second preferences into account. In the British Columbia general election, 1952, to the surprise of many, the right-wing populist BC Social Credit Party won a minority. They were able to win a majority in the subsequent election as Liberal and Conservative supporters shifted their anti-CCF vote to Social Credit.\n\nManitoba has had more formal coalition governments than any other province. Following gains by the United Farmer's/Progressive movement elsewhere in the country, the United Farmers of Manitoba unexpectedly won the 1921 election. Like their counterparts in Ontario, they had not expected to win and did not have a leader. They asked John Bracken, a professor in animal husbandry, to become leader and premier. Bracken changed the party's name to the Progressive Party of Manitoba. During the Great Depression, Bracken survived at a time when other premiers were being defeated by forming a coalition government with the Manitoba Liberals (eventually, the two parties would merge into the , and decades later, the party would change its name to the Manitoba Liberal Party). In 1940, Bracken formed a wartime coalition government with almost every party in the Manitoba Legislature (the Conservatives, CCF, and Social Credit; however, the CCF broke with the coalition after a few years over policy differences). The only party not included was the small, communist Labor-Progressive Party, which had a handful of seats.\n\nIn Saskatchewan, NDP premier Roy Romanow formed a formal coalition with the Saskatchewan Liberals in 1999 after being reduced to a minority. After two years, the newly elected Liberal leader Jim Melanchuk chose to withdraw from the coalition; however, 2 out of 3 members of his caucus disagreed with him and left the Liberals to run as New Democrats in the upcoming election. The Saskatchewan NDP was re-elected with a majority under its new leader Lorne Calvert, while the Saskatchewan Liberals lost their remaining seats and have not been competitive in the province since.\n\nAccording to historian Christopher Moore, coalition governments in Canada became much less possible in 1919, when the leaders of parties were no longer chosen by elected MPs but instead began to be chosen by party members. That kind of leadership selection process had never been tried in any parliament system before and remains uncommon in the parliaments of the world today. According to Moore, as long as that kind of leadership selection process remains in place and concentrates power in the hands of the leader, as opposed to backbenchers, then coalition governments will be very difficult to form. Moore shows that the diffusion of power within a party tends to also lead to a diffusion of power in the parliament in which that party operates, thereby making coalitions more likely.\n\nDuring the 2008 Canadian parliamentary dispute, two of Canada's opposition parties signed an agreement to form what would become the country's second coalition government since Confederation if the minority Conservative government was defeated on a vote of non-confidence, unseating Stephen Harper as Prime Minister. The agreement outlined a formal coalition consisting of two opposition parties, the Liberal Party and the New Democratic Party. The Bloc Québécois agreed to support the proposed coalition on confidence matters for 18 months. In the end, parliament was prorogued by the Governor General, and the coalition dispersed following the election.\n\nIn Denmark, all governments from 1982 until the June 2015 elections have been coalitions. The first coalition in Danish political history was formed in 1929 by Thorvald Stauning and consisted of the Social Democrats (Staunings own party) and the Social Liberals. Since then, a number of parties have participated in coalitions.\n\nExcluding the post-WW2 Liberation Cabinet's member parties, the following parties have done so: The Centre Democrats, the Christian People's Party, the Conservative People's Party, the Retsforbund, the Social Democrats, the Socialist People's Party, the Social Liberal Party, and Venstre.\n\nIn Finland, no party has had an absolute majority in the parliament since independence, and multi-party coalitions have been the norm. Finland experienced its most stable government (Lipponen I and II) since independence with a five-party governing coalition, a so-called \"rainbow government\". The Lipponen cabinets set the stability record and were unusual in the respect that both moderate (SDP) and radical left wing (Left Alliance) parties sat in the government with the major right-wing party (National Coalition). The Katainen cabinet was also a rainbow coalition of a total of five parties.\n\nSince India's Independence on 15 August 1947, Indian National Congress, the major political party instrumental in Indian Independence Movement, ruled the nation. The first Prime Minister Jawaharlal Nehru, second PM Lal Bahadur Shastri and the third PM Indira Gandhi, all were from the Congress party. However, Raj Narain, who had unsuccessfully contested election against Indira from the constituency of Rae Bareilly in 1971, lodged a case, alleging electoral malpractices. In June 1975, Indira was found guilty and barred by High Court from holding public office for six years. In response, an ungracious Emergency was declared under the pretext of national security. The next election's result was that India's first-ever coalition government was formed at the national level under the Prime Ministership of Morarji Desai, which was also the first non-Congress national government, which existed from 24 March 1977 to 15 July 1979, headed by the Janata Party, an amalgam of political parties opposed to Emergency imposed between 1975 and 1977. As the popularity of Janata Party dwindled, Morarji Desai had to resign and Charan Singh, a rival of Desai became the fifth PM. However, due to lack of support, this coalition government did not complete its five-year term.\n\nCongress returned to the power in 1980 under Indira Gandhi, and later under Rajiv Gandhi as the 6th PM. However, the next general election of 1989 once again brought a coalition government under National Front, which lasted till 1991, with two Prime Ministers, the second one being supported by Congress. The 1991 election resulted in a Congress led stable minority government for five years. The next 11th parliament produced three Prime Ministers in two years and forced the country back to the polls in 1998. The first successful coalition government in India which completed the whole 5-year term was the Bharatiya Janata Party (BJP) led National Democratic Alliance with Atal Bihari Vajpayee as PM from 1999 to 2004. Then another coalition, Congress led United Progressive Alliance, consisting of 13 separate parties ruled India for two terms from 2004 to 2014 with Manmohan Singh as PM. However, in the 16th general election in May 2014, BJP secured majority on its own (first party to do so since 1984 election) and National Democratic Alliance again came into power, with Narendra Modi as Prime Minister and more.\n\nAs a result of the toppling of Suharto, political freedom is significantly increased. Compared to only three parties allowed to exist in the New Order era, a total of 48 political parties participated in the 1999 election, a total of 24 parties in the 2004 election, 38 parties in the 2009 election, and 15 parties in the 2014 election. There are no majority winner of those elections and coalition governments are inevitable. The current government is a coalition of seven parties led by the PDIP and Golkar.\n\nIn Republic of Ireland, coalition governments are quite common; not since 1977 has a single party been able to form a majority government. Coalitions are the typically formed of two or more parties always consisting of one of the two biggest parties, Fianna Fáil and Fine Gael, and one or more smaller parties or independent members of parliament. The current government consists of a minority Fine Gael government, supported by a confidence and supply arrangement with Fianna Fáil.\n\nIreland's first coalition government was formed in 1948. Ireland has had consecutive coalition governments since the 1989 general election, excluding two brief Fianna Fáil minority administrations in 1994 and 2011 that followed the withdrawal of their coalition partners from government. Before 1989, Fianna Fáil had opposed participation in coalition governments, preferring single-party minority government instead.\n\nIrish coalition governments have traditionally been based on one of two large blocs in Dáil Éireann: either Fianna Fáil in coalition with smaller parties or independents, or Fine Gael and the Labour Party in coalition, sometimes with smaller parties. The only exception to these traditional alliances was the 23rd Government of Ireland, comprising Fianna Fáil and the Labour Party, which ruled between 1993 and 1994. The Government of the 31st Dáil, though a traditional Fine Gael–Labour coalition, resembles a grand coalition, due to the collapse of the Fianna Fáil to third place among parties in Dáil Éireann.\n\nA similar situation exists in Israel, which typically has at least 10 parties holding representation in the Knesset. The only faction to ever gain the majority of Knesset seats was Alignment, an alliance of the Labor Party and Mapam that held an absolute majority for a brief period from 1968 to 1969. Historically, control of the Israeli government has alternated between periods of rule by the right-wing Likud in coalition with several right-wing and religious parties and periods of rule by the center-left Labor in coalition with several left-wing parties. Ariel Sharon's formation of the centrist Kadima party in 2006 drew support from former Labor and Likud members, and Kadima ruled in coalition with several other parties.\n\nIsrael also formed a national unity government from 1984–1988. The premiership and foreign ministry portfolio were held by the head of each party for two years, and they switched roles in 1986.\n\nPost-World War II Japan has historically been dominated by the Liberal Democratic Party, but there was a brief coalition government formed after the 1993 election following LDP's first loss of its overall House of Representatives majority since 1955, winning only 223 out of 511 seats. The LDP government was replaced by an eight-party coalition government, which consisted of all of the previous opposition parties excluding the Japanese Communist Party, who together controlled 243 seats. Every Japanese government since then has been a coalition government in one way or another.\n\nAdvocates of proportional representation suggest that a coalition government leads to more consensus-based politics, as a government comprising differing parties (often based on different ideologies) need to compromise about governmental policy. Another stated advantage is that a coalition government better reflects the popular opinion of the electorate within a country.\n\nThose who disapprove of coalition governments believe that such governments have a tendency to be fractious and prone to disharmony, as their component parties hold differing beliefs and thus may not always agree on policy. Sometimes the results of an election mean that the coalitions which are mathematically most probable are ideologically infeasible, for example in Flanders or Northern Ireland. A second difficulty might be the ability of minor parties to play \"kingmaker\" and, particularly in close elections, gain far more power in exchange for their support than the size of their vote would otherwise justify.\n\nCoalition governments have also been criticized for sustaining a consensus on issues when disagreement and the consequent discussion would be more fruitful. To forge a consensus, the leaders of ruling coalition parties can agree to silence their disagreements on an issue to unify the coalition against the opposition. The coalition partners, if they control the parliamentary majority, can collude to make the parliamentary discussion on the issue irrelevant by consistently disregarding the arguments of the opposition and voting against the opposition's proposals — even if there is disagreement within the ruling parties about the issue.\n\nPowerful parties can also act in an oligocratic way to form an alliance to stifle the growth of emerging parties. Of course, such an event is rare in coalition governments when compared to two-party systems, which typically exist because of stifling of the growth of emerging parties, often through discriminatory nomination rules regulations and plurality voting systems, and so on.\n\nA single, more powerful party can shape the policies of the coalition disproportionately. Smaller or less powerful parties can be intimidated to not openly disagree. In order to maintain the coalition, they would have to vote against their own party's platform in the parliament. If they do not, the party has to leave the government and loses executive power.\n"}
{"id": "6038", "url": "https://en.wikipedia.org/wiki?curid=6038", "title": "Chemical engineering", "text": "Chemical engineering\n\nChemical engineering is a branch of engineering that applies physical sciences (physics and chemistry), life sciences (microbiology and biochemistry), together with applied mathematics and economics to produce, transform, transport, and properly use chemicals, materials and energy. A chemical engineer designs large-scale processes that convert chemicals, raw materials, living cells, microorganisms and energy into useful forms and products.\n\nA 1996 \"British Journal for the History of Science\" article cites James F. Donnelly for mentioning an 1839 reference to chemical engineering in relation to the production of sulfuric acid. In the same paper however, George E. Davis, an English consultant, was credited for having coined the term. Davis also tried to found a \"Society of Chemical Engineering\", but instead it was named the Society of Chemical Industry (1881), with Davis as its first Secretary. The \"History of Science in United States: An Encyclopedia\" puts the use of the term around 1890. \"Chemical engineering\", describing the use of mechanical equipment in the chemical industry, became common vocabulary in England after 1850. By 1910, the profession, \"chemical engineer,\" was already in common use in Britain and the United States.\n\nChemical engineering emerged upon the development of unit operations, a fundamental concept of the discipline of chemical engineering. Most authors agree that Davis invented the concept of unit operations if not substantially developed it. He gave a series of lectures on unit operations at the Manchester Technical School (later part of the University of Manchester) in 1887, considered to be one of the earliest such about chemical engineering. Three years before Davis' lectures, Henry Edward Armstrong taught a degree course in chemical engineering at the City and Guilds of London Institute. Armstrong's course \"failed simply because its graduates ... were not especially attractive to employers.\" Employers of the time would have rather hired chemists and mechanical engineers. Courses in chemical engineering offered by Massachusetts Institute of Technology (MIT) in the United States, Owens College in Manchester, England, and University College London suffered under similar circumstances.\n\nStarting from 1888, Lewis M. Norton taught at MIT the first chemical engineering course in the United States. Norton's course was contemporaneous and essentially similar with Armstrong's course. Both courses, however, simply merged chemistry and engineering subjects. \"Its practitioners had difficulty convincing engineers that they were engineers and chemists that they were not simply chemists.\" Unit operations was introduced into the course by William Hultz Walker in 1905. By the early 1920s, unit operations became an important aspect of chemical engineering at MIT and other US universities, as well as at Imperial College London. The American Institute of Chemical Engineers (AIChE), established in 1908, played a key role in making chemical engineering considered an independent science, and unit operations central to chemical engineering. For instance, it defined chemical engineering to be a \"science of itself, the basis of which is ... unit operations\" in a 1922 report; and with which principle, it had published a list of academic institutions which offered \"satisfactory\" chemical engineering courses. Meanwhile, promoting chemical engineering as a distinct science in Britain lead to the establishment of the Institution of Chemical Engineers (IChemE) in 1922. IChemE likewise helped make unit operations considered essential to the discipline.\n\nBy the 1940s, it became clear that unit operations alone was insufficient in developing chemical reactors. While the predominance of unit operations in chemical engineering courses in Britain and the United States continued until the 1960s, transport phenomena started to experience greater focus. Along with other novel concepts, such process systems engineering (PSE), a \"second paradigm\" was defined. Transport phenomena gave an analytical approach to chemical engineering while PSE focused on its synthetic elements, such as control system and process design. Developments in chemical engineering before and after World War II were mainly incited by the petrochemical industry, however, advances in other fields were made as well. Advancements in biochemical engineering in the 1940s, for example, found application in the pharmaceutical industry, and allowed for the mass production of various antibiotics, including penicillin and streptomycin. Meanwhile, progress in polymer science in the 1950s paved way for the \"age of plastics\".\n\nConcerns regarding the safety and environmental impact of large-scale chemical manufacturing facilities were also raised during this period. \"Silent Spring\", published in 1962, alerted its readers to the harmful effects of DDT, a potent insecticide. The 1974 Flixborough disaster in the United Kingdom resulted in 28 deaths, as well as damage to a chemical plant and three nearby villages. The 1984 Bhopal disaster in India resulted in almost 4,000 deaths . These incidents, along with other incidents, affected the reputation of the trade as industrial safety and environmental protection were given more focus. In response, the IChemE required safety to be part of every degree course that it accredited after 1982. By the 1970s, legislation and monitoring agencies were instituted in various countries, such as France, Germany, and the United States.\n\nAdvancements in computer science found applications designing and managing plants, simplifying calculations and drawings that previously had to be done manually. The completion of the Human Genome Project is also seen as a major development, not only advancing chemical engineering but genetic engineering and genomics as well. Chemical engineering principles were used to produce DNA sequences in large quantities.\n\nChemical engineering involves the application of several principles. Key concepts are presented below.\n\nChemical engineering involves managing plant processes and conditions to ensure optimal plant operation. Chemical reaction engineers construct models for reactor analysis and design using laboratory data and physical parameters, such as chemical thermodynamics, to solve problems and predict reactor performance.\n\nChemical engineering design concerns the creation of plans, specification, and economic analyses for pilot plants, new plants or plant modifications. Design engineers often work in a consulting role, designing plants to meet clients' needs. Design is limited by a number of factors, including funding, government regulations and safety standards. These constraints dictate a plant's choice of process, materials and equipment.\n\nPlant construction is coordinated by project engineers and project managers depending on the size of the investment. A chemical engineer may do the job of project engineer full-time or part of the time, which requires additional training and job skills, or act as a consultant to the project group. In USA the education of chemical engineering graduates from the Baccalaureate programs accredited by ABET do not usually stress project engineering education, which can be obtained by specialized training, as electives, or from graduate programs. Project engineering jobs are some of the largest employers for chemical engineers.\n\nA unit operation is a physical step in an individual chemical engineering process. Unit operations (such as crystallization, filtration, drying and evaporation) are used to prepare reactants, purifying and separating its products, recycling unspent reactants, and controlling energy transfer in reactors. On the other hand, a unit process is the chemical equivalent of a unit operation. Along with unit operations, unit processes constitute a process operation. Unit processes (such as nitration and oxidation) involve the conversion of material by biochemical, thermochemical and other means. Chemical engineers responsible for these are called process engineers.\n\nProcess design requires the definition of equipment types and sizes as well as how they are connected together and the materials of construction. Details are often printed on a Process Flow Diagram which is used to control the capacity and reliability of a new or modified chemical factory.\n\nEducation for chemical engineers in the first college degree 3 or 4 years of study stresses the principles and practices of process design. The same skills are used in existing chemical plants to evaluate the efficiency and make recommendations for improvements.\n\nModeling and analysis of transport phenomena is essential for many industrial applications. Transport phenomena involve fluid dynamics, heat transfer and mass transfer, which are governed mainly by momentum transfer, energy transfer and transport of chemical species respectively. Models often involve separate considerations for macroscopic, microscopic and molecular level phenomena. Modeling of transport phenomena therefore requires an understanding of applied mathematics.\n\nChemical engineers \"develop economic ways of using materials and energy\". Chemical engineers use chemistry and engineering to turn raw materials into usable products, such as medicine, petrochemicals and plastics on a large-scale, industrial setting. They are also involved in waste management and research. Both applied and research facets could make extensive use of computers.\n\nChemical engineers may be involved in industry or university research where they are tasked with designing and performing experiments to create better and safer methods for production, pollution control, and resource conservation. They may be involved in designing and constructing plants as a project engineer. Chemical engineers serving as project engineers use their knowledge in selecting optimal production methods and plant equipment to minimize costs and maximize safety and profitability. After plant construction, chemical engineering project managers may be involved in equipment upgrades, process changes, troubleshooting, and daily operations in either full-time or consulting roles. \n\nToday, the field of chemical engineering is a diverse one, covering areas from biotechnology and nanotechnology to mineral processing.\n\n\n\n\n"}
{"id": "6041", "url": "https://en.wikipedia.org/wiki?curid=6041", "title": "List of comedians", "text": "List of comedians\n\nA comedian is one who entertains through comedy, such as jokes and other forms of humour. Following is a list of comedians, comedy groups, and comedy writers.\n\n\"(sorted alphabetically by surname)\"\n\n\n\n\"(sorted alphabetically by surname)\"\n\nLists of comedians by nationality\n\n\nOther related lists\n"}
{"id": "6042", "url": "https://en.wikipedia.org/wiki?curid=6042", "title": "Compact space", "text": "Compact space\n\nIn mathematics, and more specifically in general topology, compactness is a property that generalizes the notion of a subset of Euclidean space being closed (that is, containing all its limit points) and bounded (that is, having all its points lie within some fixed distance of each other). Examples include a closed interval, a rectangle, or a finite set of points. This notion is defined for more general topological spaces than Euclidean space in various ways.\n\nOne such generalization is that a space is \"sequentially\" compact if any infinite sequence of points sampled from the space must frequently (infinitely often) get arbitrarily close to some point of the space. An equivalent definition is that every sequence of points must have an infinite subsequence that converges to some point of the space. The Heine–Borel theorem states that a subset of Euclidean space is compact in this sequential sense if and only if it is closed and bounded. Thus, if one chooses an infinite number of points in the \"closed\" unit interval some of those points must get arbitrarily close to some real number in that space. For instance, some of the numbers accumulate to 0 (others accumulate to 1). The same set of points would not accumulate to any point of the \"open\" unit interval ; so the open unit interval is not compact. Euclidean space itself is not compact since it is not bounded. In particular, the sequence of points has no subsequence that converges to any given real number.\n\nApart from closed and bounded subsets of Euclidean space, typical examples of compact spaces include spaces consisting not of geometrical points but of functions. The term \"compact\" was introduced into mathematics by Maurice Fréchet in 1904 as a distillation of this concept. Compactness in this more general situation plays an extremely important role in mathematical analysis, because many classical and important theorems of 19th-century analysis, such as the extreme value theorem, are easily generalized to this situation. A typical application is furnished by the Arzelà–Ascoli theorem or the Peano existence theorem, in which one is able to conclude the existence of a function with some required properties as a limiting case of some more elementary construction.\n\nVarious equivalent notions of compactness, including sequential compactness and limit point compactness, can be developed in general metric spaces. In general topological spaces, however, different notions of compactness are not necessarily equivalent. The most useful notion, which is the standard definition of the unqualified term \"compactness\", is phrased in terms of the existence of finite families of open sets that \"cover\" the space in the sense that each point of the space must lie in some set contained in the family. This more subtle notion, introduced by Pavel Alexandrov and Pavel Urysohn in 1929, exhibits compact spaces as generalizations of finite sets. In spaces that are compact in this sense, it is often possible to patch together information that holds locally—that is, in a neighborhood of each point—into corresponding statements that hold throughout the space, and many theorems are of this character.\n\nThe term compact set is sometimes a synonym for compact space, but usually refers to a compact subspace of a topological space.\n\nIn the 19th century, several disparate mathematical properties were understood that would later be seen as consequences of compactness. On the one hand, Bernard Bolzano (1817) had been aware that any bounded sequence of points (in the line or plane, for instance) has a subsequence that must eventually get arbitrarily close to some other point, called a limit point. Bolzano's proof relied on the method of bisection: the sequence was placed into an interval that was then divided into two equal parts, and a part containing infinitely many terms of the sequence was selected. The process could then be repeated by dividing the resulting smaller interval into smaller and smaller parts until it closes down on the desired limit point. The full significance of Bolzano's theorem, and its method of proof, would not emerge until almost 50 years later when it was rediscovered by Karl Weierstrass.\n\nIn the 1880s, it became clear that results similar to the Bolzano–Weierstrass theorem could be formulated for spaces of functions rather than just numbers or geometrical points. The idea of regarding functions as themselves points of a generalized space dates back to the investigations of Giulio Ascoli and Cesare Arzelà. The culmination of their investigations, the Arzelà–Ascoli theorem, was a generalization of the Bolzano–Weierstrass theorem to families of continuous functions, the precise conclusion of which was that it was possible to extract a uniformly convergent sequence of functions from a suitable family of functions. The uniform limit of this sequence then played precisely the same role as Bolzano's \"limit point\". Towards the beginning of the twentieth century, results similar to that of Arzelà and Ascoli began to accumulate in the area of integral equations, as investigated by David Hilbert and Erhard Schmidt. For a certain class of Green functions coming from solutions of integral equations, Schmidt had shown that a property analogous to the Arzelà–Ascoli theorem held in the sense of mean convergence—or convergence in what would later be dubbed a Hilbert space. This ultimately led to the notion of a compact operator as an offshoot of the general notion of a compact space. It was Maurice Fréchet who, in 1906, had distilled the essence of the Bolzano–Weierstrass property and coined the term \"compactness\" to refer to this general phenomenon (he used the term already in his 1904 paper which led to the famous 1906 thesis).\n\nHowever, a different notion of compactness altogether had also slowly emerged at the end of the 19th century from the study of the continuum, which was seen as fundamental for the rigorous formulation of analysis. In 1870, Eduard Heine showed that a continuous function defined on a closed and bounded interval was in fact uniformly continuous. In the course of the proof, he made use of a lemma that from any countable cover of the interval by smaller open intervals, it was possible to select a finite number of these that also covered it. The significance of this lemma was recognized by Émile Borel (1895), and it was generalized to arbitrary collections of intervals by Pierre Cousin (1895) and Henri Lebesgue (1904). The Heine–Borel theorem, as the result is now known, is another special property possessed by closed and bounded sets of real numbers.\n\nThis property was significant because it allowed for the passage from local information about a set (such as the continuity of a function) to global information about the set (such as the uniform continuity of a function). This sentiment was expressed by , who also exploited it in the development of the integral now bearing his name. Ultimately the Russian school of point-set topology, under the direction of Pavel Alexandrov and Pavel Urysohn, formulated Heine–Borel compactness in a way that could be applied to the modern notion of a topological space. showed that the earlier version of compactness due to Fréchet, now called (relative) sequential compactness, under appropriate conditions followed from the version of compactness that was formulated in terms of the existence of finite subcovers. It was this notion of compactness that became the dominant one, because it was not only a stronger property, but it could be formulated in a more general setting with a minimum of additional technical machinery, as it relied only on the structure of the open sets in a space.\nAn example of a compact space is the (closed) unit interval of real numbers. If one chooses an infinite number of distinct points in the unit interval, then there must be some accumulation point in that interval. For instance, the odd-numbered terms of the sequence get arbitrarily close to 0, while the even-numbered ones get arbitrarily close to 1. The given example sequence shows the importance of including the boundary points of the interval, since the limit points must be in the space itself — an open (or half-open) interval of the real numbers is not compact. It is also crucial that the interval be bounded, since in the interval one could choose the sequence of points , of which no sub-sequence ultimately gets arbitrarily close to any given real number.\n\nIn two dimensions, closed disks are compact since for any infinite number of points sampled from a disk, some subset of those points must get arbitrarily close either to a point within the disc, or to a point on the boundary. However, an open disk is not compact, because a sequence of points can tend to the boundary without getting arbitrarily close to any point in the interior. Likewise, spheres are compact, but a sphere missing a point is not since a sequence of points can tend to the missing point, thereby not getting arbitrarily close to any point \"within\" the space. Lines and planes are not compact, since one can take a set of equally-spaced points in any given direction without approaching any point.\n\nVarious definitions of compactness may apply, depending on the level of generality. A subset of Euclidean space in particular is called compact if it is closed and bounded. This implies, by the Bolzano–Weierstrass theorem, that any infinite sequence from the set has a subsequence that converges to a point in the set. Various equivalent notions of compactness, such as sequential compactness and limit point compactness, can be developed in general metric spaces.\n\nIn general topological spaces, however, the different notions of compactness are not equivalent, and the most useful notion of compactness—originally called \"bicompactness\"—is defined using covers consisting of open sets (see \"Open cover definition\" below). That this form of compactness holds for closed and bounded subsets of Euclidean space is known as the Heine–Borel theorem. Compactness, when defined in this manner, often allows one to take information that is known locally—in a neighbourhood of each point of the space—and to extend it to information that holds globally throughout the space. An example of this phenomenon is Dirichlet's theorem, to which it was originally applied by Heine, that a continuous function on a compact interval is uniformly continuous; here, continuity is a local property of the function, and uniform continuity the corresponding global property.\n\nFormally, a topological space is called \"compact\" if each of its open covers has a finite subcover. That is, is compact if for every collection of open subsets of such that\n\nthere is a finite subset of such that\n\nSome branches of mathematics such as algebraic geometry, typically influenced by the French school of Bourbaki, use the term \"quasi-compact\" for the general notion, and reserve the term \"compact\" for topological spaces that are both Hausdorff and \"quasi-compact\". A compact set is sometimes referred to as a \"compactum\", plural \"compacta\".\n\nA subset of a topological space is said to be compact if it is compact as a subspace (in the subspace topology). That is, is compact if for every arbitrary collection of open subsets of such that\n\nthere is a finite subset of such that\n\nCompactness is a \"topological\" property. That is, if formula_5, with subset equipped with the subspace topology, then is compact in if and only if is compact in .\n\nAssuming the axiom of choice, the following are equivalent:\n\nFor any subset \"A\" of Euclidean space R, \"A\" is compact if and only if it is closed and bounded; this is the Heine–Borel theorem.\n\nAs a Euclidean space is a metric space, the conditions in the next subsection also apply to all of its subsets. Of all of the equivalent conditions, it is in practice easiest to verify that a subset is closed and bounded, for example, for a closed interval or closed \"n\"-ball.\n\nFor any metric space (\"X,d\"), the following are equivalent:\n\nA compact metric space (X,d) also satisfies the following properties:\n\nLet \"X\" be a topological space and C(\"X\") the ring of real continuous functions on \"X\". For each \"p\"∈\"X\", the evaluation map\ngiven by ev(\"f\")=\"f\"(\"p\") is a ring homomorphism. The kernel of ev is a maximal ideal, since the residue field is the field of real numbers, by the first isomorphism theorem. A topological space \"X\" is pseudocompact if and only if every maximal ideal in C(\"X\") has residue field the real numbers. For completely regular spaces, this is equivalent to every maximal ideal being the kernel of an evaluation homomorphism. There are pseudocompact spaces that are not compact, though.\n\nIn general, for non-pseudocompact spaces there are always maximal ideals \"m\" in C(\"X\") such that the residue field C(\"X\")/\"m\" is a (non-archimedean) hyperreal field. The framework of non-standard analysis allows for the following alternative characterization of compactness: a topological space \"X\" is compact if and only if every point \"x\" of the natural extension \"*X\" is infinitely close to a point \"x\" of \"X\" (more precisely, \"x\" is contained in the monad of \"x\").\n\nA space \"X\" is compact if its natural extension \"*X\" (for example, an ultrapower) has the property that every point of \"*X\" is infinitely close to a suitable point of formula_7. For example, an open real interval \"X\"=(0,1) is not compact because its hyperreal extension *(0,1) contains infinitesimals, which are infinitely close to 0, which is not a point of \"X\".\n\nA continuous image of a compact space is compact.\nThis implies the extreme value theorem: a continuous real-valued function on a nonempty compact space is bounded above and attains its supremum. (Slightly more generally, this is true for an upper semicontinuous function.) As a sort of converse to the above statements, the pre-image of a compact space under a proper map is compact.\n\nA closed subset of a compact space is compact., and a finite union of compact sets is compact.\n\nThe product of any collection of compact spaces is compact. (This is Tychonoff's theorem, which is equivalent to the axiom of choice.)\n\nEvery topological space \"X\" is an open dense subspace of a compact space having at most one point more than \"X\", by the Alexandroff one-point compactification. By the same construction, every locally compact Hausdorff space \"X\" is an open dense subspace of a compact Hausdorff space having at most one point more than \"X\".\n\nA nonempty compact subset of the real numbers has a greatest element and a least element.\n\nLet \"X\" be a simply ordered set endowed with the order topology. Then \"X\" is compact if and only if \"X\" is a complete lattice (i.e. all subsets have suprema and infima).\n\n\n\n\n\n"}
{"id": "6045", "url": "https://en.wikipedia.org/wiki?curid=6045", "title": "Clodius", "text": "Clodius\n\nClodius is an alternate form of the Roman \"nomen\" Claudius, a patrician \"gens\" that was traditionally regarded as Sabine in origin. The alternation of \"o\" and \"au\" is characteristic of the Sabine dialect. The feminine form is Clodia.\n\nthe Late Republic, the spelling \"Clodius\" is most prominently associated with Publius Clodius Pulcher, a popularist politician who gave up his patrician status through a order in order to qualify for the office of tribune of the \"plebs\". Clodius positioned himself as a champion of the urban \"plebs\", supporting free grain for the poor and the right of association in guilds (\"collegia\"); because of this individual's ideology, \"Clodius\" has often been taken as a more \"plebeian\" spelling and a gesture of political solidarity. Clodius's two elder brothers, the Appius Claudius Pulcher who was consul in 54 BC and the C. Claudius Pulcher who was praetor in 56 BC, conducted more conventional political careers and are referred to in contemporary sources with the traditional spelling.\n\nThe view that \"Clodius\" represents a plebeian or politicized form has been questioned by Clodius's chief modern-era biographer. In \"The Patrician Tribune\", W. Jeffrey Tatum points out that the spelling is also associated with Clodius's sisters and that \"the political explanation … is almost certainly wrong.\" A plebeian branch of the \"gens\", the Claudii Marcelli, retained the supposedly patrician spelling, while there is some inscriptional evidence that the \"-o-\" form may also have been used on occasion by close male relatives of the \"patrician tribune\" Clodius. Tatum argues that the use of \"-o-\" by the \"chic\" Clodia Metelli was a fashionable affectation, and that Clodius, whose perhaps inordinately loving relationship with his sister was the subject of much gossip and insinuation, was imitating his stylish sibling. The linguistic variation of \"o\" for \"au\" was characteristic of the Umbrian language, of which Sabine was a branch. Forms using \"o\" were considered archaic or rustic in the 50s BC, and the use of \"Clodius\" would have been either a whimsical gesture of pastoral fantasy, or a trendy assertion of antiquarian authenticity.\n\nIn addition to Clodius, Clodii from the Republican era include:\n\n\nPeople using the name \"Clodius\" during the period of the Roman Empire include:\n\nThe Clodii Celsini continued to practice the traditional religions of antiquity in the face of Christian hegemony through at least the 4th century, when Clodius Celsinus Adelphius (see below) converted. Members of this branch include:\n\n\n"}
{"id": "6046", "url": "https://en.wikipedia.org/wiki?curid=6046", "title": "Cicero", "text": "Cicero\n\nMarcus Tullius Cicero (; ; 3 January 106 BC – 7 December 43 BC) was a Roman politician and lawyer, who served as consul in the year 63 BC. He came from a wealthy municipal family of the Roman equestrian order, and is considered one of Rome's greatest orators and prose stylists.\n\nHis influence on the Latin language was so immense that the subsequent history of prose, not only in Latin but in European languages up to the 19th century, was said to be either a reaction against or a return to his style. According to Michael Grant, \"the influence of Cicero upon the history of European literature and ideas greatly exceeds that of any other prose writer in any language\". Cicero introduced the Romans to the chief schools of Greek philosophy and created a Latin philosophical vocabulary (with neologisms such as \"evidentia\", \"humanitas\", \"qualitas\", \"quantitas\", and \"essentia\") distinguishing himself as a translator and philosopher.\n\nThough he was an accomplished orator and successful lawyer, Cicero believed his political career was his most important achievement. It was during his consulship that the second Catilinarian conspiracy attempted to overthrow the government through an attack on the city by outside forces, and Cicero suppressed the revolt by executing five conspirators without due process. During the chaotic latter half of the 1st century BC marked by civil wars and the dictatorship of Gaius Julius Caesar, Cicero championed a return to the traditional republican government. Following Julius Caesar's death, Cicero became an enemy of Mark Antony in the ensuing power struggle, attacking him in a series of speeches. He was proscripted as an enemy of the state by the Second Triumvirate and consequently executed by soldiers operating on their behalf in 43 BC after having been intercepted during attempted flight from the Italian peninsula. His severed hands and head were then, as a final revenge of Mark Antony, displayed in the Roman Forum.\n\nPetrarch's rediscovery of Cicero's letters is often credited for initiating the 14th-century Renaissance in public affairs, humanism, and classical Roman culture. According to Polish historian Tadeusz Zieliński, \"the Renaissance was above all things a revival of Cicero, and only after him and through him of the rest of Classical antiquity.\" The peak of Cicero's authority and prestige came during the 18th-century Enlightenment, and his impact on leading Enlightenment thinkers and political theorists such as John Locke, David Hume, Montesquieu and Edmund Burke was substantial.\nHis works rank among the most influential in European culture, and today still constitute one of the most important bodies of primary material for the writing and revision of Roman history, especially the last days of the Roman Republic.\n\nCicero was born in 106 BC in Arpinum, a hill town southeast of Rome. His father was a well-to-do member of the equestrian order and possessed good connections in Rome. However, being a semi-invalid, he could not enter public life and studied extensively to compensate. Although little is known about Cicero's mother, Helvia, it was common for the wives of important Roman citizens to be responsible for the management of the household. Cicero's brother Quintus wrote in a letter that she was a thrifty housewife.\n\nCicero's cognomen, or personal surname, comes from the Latin for chickpea, \"cicer\". Plutarch explains that the name was originally given to one of Cicero's ancestors who had a cleft in the tip of his nose resembling a chickpea. However, it is more likely that Cicero's ancestors prospered through the cultivation and sale of chickpeas. Romans often chose down-to-earth personal surnames. The famous family names of Fabius, Lentulus, and Piso come from the Latin names of beans, lentils, and peas, respectively. Plutarch writes that Cicero was urged to change this deprecatory name when he entered politics, but refused, saying that he would make \"Cicero\" more glorious than \"Scaurus\" (\"Swollen-ankled\") and \"Catulus\" (\"Puppy\").\n\nDuring this period in Roman history, \"cultured\" meant being able to speak both Latin and Greek. Cicero was therefore educated in the teachings of the ancient Greek philosophers, poets and historians; he obtained much of his understanding of the theory and practice of rhetoric from the Greek poet Archias and from the Greek rhetorician Apollonius. Cicero used his knowledge of Greek to translate many of the theoretical concepts of Greek philosophy into Latin, thus translating Greek philosophical works for a larger audience. It was precisely his broad education that tied him to the traditional Roman elite.\n\nAccording to Plutarch, Cicero was an extremely talented student, whose learning attracted attention from all over Rome, affording him the opportunity to study Roman law under Quintus Mucius Scaevola. Cicero's fellow students were Gaius Marius Minor, Servius Sulpicius Rufus (who became a famous lawyer, one of the few whom Cicero considered superior to himself in legal matters), and Titus Pomponius. The latter two became Cicero's friends for life, and Pomponius (who later received the nickname \"Atticus\", and whose sister married Cicero's brother) would become, in Cicero's own words, \"as a second brother\", with both maintaining a lifelong correspondence.\n\nCicero wanted to pursue a public career in politics along the steps of the Cursus honorum. In 90 BC–88 BC, he served both Gnaeus Pompeius Strabo and Lucius Cornelius Sulla as they campaigned in the Social War, though he had no taste for military life, being an intellectual first and foremost. Cicero started his career as a lawyer around 83–81 BC. His first major case, of which a written record is still extant, was his 80 BC defense of Sextus Roscius on the charge of patricide. Taking this case was a courageous move for Cicero; patricide was considered an appalling crime, and the people whom Cicero accused of the murder, the most notorious being Chrysogonus, were favorites of Sulla. At this time it would have been easy for Sulla to have the unknown Cicero murdered. Cicero's defense was an indirect challenge to the dictator Sulla, and on the strength of his case, Roscius was acquitted.\n\nCicero’s case was divided into three parts. The first part detailed exactly the charge brought by Ericius. Cicero explained how a rustic son of a farmer, who lives off the pleasures of his own land, would not have gained anything from committing patricide because he would have eventually inherited his father's land anyway. The second part concerned the boldness and greed of two of the accusers, Magnus and Capito. Cicero told the jury that they were the more likely perpetrators of murder because the two were greedy, both for conspiring together against a fellow kinsman and, in particular, Magnus, for his boldness and for being unashamed to appear in court to support the false charges. The third part explained that Chrysogonus had immense political power, and the accusation was successfully made due to that power. Even though Chrysogonus may not have been what Cicero said he was, through rhetoric Cicero successfully made him appear to be a foreign freed man who prospered by devious means in the aftermath of the civil war. Cicero surmised that it showed what kind of a person he was and that something like murder was not beneath him.\n\nCicero's interest in philosophy figured heavily in his later career and led to him providing a comprehensive account of Greek philosophy for a Roman audience, including creating a philosophical vocabulary in Latin. In 87 BC, Philo of Larissa, the head of the Academy that was founded by Plato in Athens about 300 years earlier, arrived in Rome. Cicero, \"inspired by an extraordinary zeal for philosophy\", sat enthusiastically at his feet and absorbed Plato's philosophy.\nCicero said of Plato's Dialogues, that if Zeus were to speak, he would use their language.\n\nIn 79 BC, Cicero left for Greece, Asia Minor and Rhodes. This was perhaps to avoid the potential wrath of Sulla, though Cicero himself says it was to hone his skills and improve his physical fitness. In Athens he studied philosophy with Antiochus of Ascalon, the 'Old Academic' and initiator of Middle Platonism. In Asia Minor, he met the leading orators of the region and continued to study with them. Cicero then journeyed to Rhodes to meet his former teacher, Apollonius Molon, who had previously taught him in Rome. Molon helped Cicero hone the excesses in his style, as well as train his body and lungs for the demands of public speaking. Charting a middle path between the competing Attic and Asiatic styles, Cicero would ultimately become considered second only to Demosthenes among history's orators.\n\nCicero married Terentia probably at the age of 27, in 79 BC. According to the upper class mores of the day it was a marriage of convenience, but lasted harmoniously for nearly 30 years. Terentia's family was wealthy, probably the plebeian noble house of Terenti Varrones, thus meeting the needs of Cicero's political ambitions in both economic and social terms. She had a half-sister named Fabia, who as a child had become a Vestal Virgin, a very great honour. Terentia was a strong willed woman and (citing Plutarch) \"she took more interest in her husband's political career than she allowed him to take in household affairs.\"\n\nIn the 50s BC, Cicero's letters to Terentia became shorter and colder. He complained to his friends that Terentia had betrayed him but did not specify in which sense. Perhaps the marriage simply could not outlast the strain of the political upheaval in Rome, Cicero's involvement in it, and various other disputes between the two. The divorce appears to have taken place in 51 BC or shortly before. In 46 or 45 BC, Cicero married a young girl, Publilia, who had been his ward. It is thought that Cicero needed her money, particularly after having to repay the dowry of Terentia, who came from a wealthy family. This marriage did not last long.\n\nAlthough his marriage to Terentia was one of convenience, it is commonly known that Cicero held great love for his daughter Tullia. When she suddenly became ill in February 45 BC and died after having seemingly recovered from giving birth to a son in January, Cicero was stunned. \"I have lost the one thing that bound me to life\" he wrote to Atticus. Atticus told him to come for a visit during the first weeks of his bereavement, so that he could comfort him when his pain was at its greatest. In Atticus's large library, Cicero read everything that the Greek philosophers had written about overcoming grief, \"but my sorrow defeats all consolation.\" Caesar and Brutus as well as Servius Sulpicius Rufus sent him letters of condolence.\n\nCicero hoped that his son Marcus would become a philosopher like him, but Marcus himself wished for a military career. He joined the army of Pompey in 49 BC and after Pompey's defeat at Pharsalus 48 BC, he was pardoned by Caesar. Cicero sent him to Athens to study as a disciple of the peripatetic philosopher Kratippos in 48 BC, but he used this absence from \"his father's vigilant eye\" to \"eat, drink and be merry.\" After Cicero's murder he joined the army of the \"Liberatores\" but was later pardoned by Augustus. Augustus' bad conscience for not having objected to Cicero's being put on the proscription list during the Second Triumvirate led him to aid considerably Marcus Minor's career. He became an augur, and was nominated consul in 30 BC together with Augustus. As such, he was responsible for revoking the honors of Mark Antony, who was responsible for the proscription, and could in this way take revenge. Later he was appointed proconsul of Syria and the province of Asia.\n\nHis first office was as one of the twenty annual quaestors, a training post for serious public administration in a diversity of areas, but with a traditional emphasis on administration and rigorous accounting of public monies under the guidance of a senior magistrate or provincial commander. Cicero served as quaestor in western Sicily in 75 BC and demonstrated honesty and integrity in his dealings with the inhabitants. As a result, the grateful Sicilians asked Cicero to prosecute Gaius Verres, a governor of Sicily, who had badly plundered the province. His prosecution of Gaius Verres was a great forensic success for Cicero. Governor Gaius Verres hired the prominent lawyer of a noble family Quintus Hortensius Hortalus. After a lengthy period in Sicily collecting testimonials and evidence and persuading witnesses to come forward, Cicero returned to Rome and won the case in a series of dramatic court battles. His unique style of oratory set him apart from the flamboyant Hortensius. On the conclusion of this case, Cicero came to be considered the greatest orator in Rome. The view that Cicero may have taken the case for reasons of his own is viable. Hortensius was, at this point, known as the best lawyer in Rome; to beat him would guarantee much success and the prestige that Cicero needed to start his career. Cicero's oratorical skill is shown in his character assassination of Verres and various other techniques of persuasion used on the jury. One such example is found in the speech \"Against Verres I\", where he states \"with you on this bench, gentlemen, with Marcus Acilius Glabrio as your president, I do not understand what Verres can hope to achieve\". Oratory was considered a great art in ancient Rome and an important tool for disseminating knowledge and promoting oneself in elections, in part because there were no regular newspapers or mass media. Cicero was neither a patrician nor a plebeian noble; his rise to political office despite his relatively humble origins has traditionally been attributed to his brilliance as an orator.\n\nCicero grew up in a time of civil unrest and war. Sulla's victory in the first of a series of civil wars led to a new constitutional framework that undermined \"libertas\" (liberty), the fundamental value of the Roman Republic. Nonetheless, Sulla's reforms strengthened the position of the equestrian class, contributing to that class's growing political power. Cicero was both an Italian \"eques\" and a \"novus homo\", but more importantly he was a Roman constitutionalist. His social class and loyalty to the Republic ensured that he would \"command the support and confidence of the people as well as the Italian middle classes\". The \"optimates\" faction never truly accepted Cicero; and this undermined his efforts to reform the Republic while preserving the constitution. Nevertheless, he successfully ascended the \"cursus honorum\", holding each magistracy at or near the youngest possible age: quaestor in 75 BC (age 31), aedile in 69 BC (age 37), and praetor in 66 BC (age 40), when he served as president of the \"Reclamation\" (or extortion) Court. He was then elected consul at age 43.\n\nCicero was elected consul for the year 63 BC. His co-consul for the year, Gaius Antonius Hybrida, played a minor role. During his year in office, he thwarted a conspiracy centered on assassinating him and overthrowing the Roman Republic with the help of foreign armed forces, led by Lucius Sergius Catilina. Cicero procured a \"senatus consultum ultimum\" (a declaration of martial law) and drove Catiline from the city with four vehement speeches (the Catiline Orations), which to this day remain outstanding examples of his rhetorical style. The Orations listed Catiline and his followers' debaucheries, and denounced Catiline's senatorial sympathizers as roguish and dissolute debtors clinging to Catiline as a final and desperate hope. Cicero demanded that Catiline and his followers leave the city. At the conclusion of his first speech, Catiline hurriedly left the Senate, (which was being held in the Temple of Jupiter Stator). In his following speeches, Cicero did not directly address Catiline. He delivered the second and third orations before the people, and the last one again before the Senate. By these speeches, Cicero wanted to prepare the Senate for the worst possible case; he also delivered more evidence against Catiline.\n\nCatiline fled and left behind his followers to start the revolution from within while Catiline assaulted the city with an army of \"moral bankrupts and honest fanatics\". Catiline had attempted to involve the Allobroges, a tribe of Transalpine Gaul, in their plot, but Cicero, working with the Gauls, was able to seize letters that incriminated the five conspirators and forced them to confess in front of the Senate.\n\nThe Senate then deliberated upon the conspirators' punishment. As it was the dominant advisory body to the various legislative assemblies rather than a judicial body, there were limits to its power; however, martial law was in effect, and it was feared that simple house arrest or exile – the standard options – would not remove the threat to the state. At first Decimus Silanus spoke for the \"extreme penalty\"; many were swayed by Julius Caesar, who decried the precedent it would set and argued in favor of life imprisonment in various Italian towns. Cato the Younger rose in defence of the death penalty and the entire Senate finally agreed on the matter. Cicero had the conspirators taken to the Tullianum, the notorious Roman prison, where they were strangled. Cicero himself accompanied the former consul Publius Cornelius Lentulus Sura, one of the conspirators, to the Tullianum. Cicero received the honorific \"\"Pater Patriae\"\" for his efforts to suppress the conspiracy, but lived thereafter in fear of trial or exile for having put Roman citizens to death without trial.\n\nAfter the conspirators were put to death, Cicero was proud of his accomplishment. Some of his political enemies argued that though the act gained Cicero popularity, he exaggerated the extent of his success. He overestimated his popularity again several years later after being exiled from Italy and then allowed back from exile. At this time, he claimed that the Republic would be restored along with him.\n\nIn 60 BC, Julius Caesar invited Cicero to be the fourth member of his existing partnership with Pompey and Marcus Licinius Crassus, an assembly that would eventually be called the First Triumvirate. Cicero refused the invitation because he suspected it would undermine the Republic.\n\nIn 58 BC, Publius Clodius Pulcher, the tribune of the plebs, introduced a law (the \"Leges Clodiae\") threatening exile to anyone who executed a Roman citizen without a trial. Cicero, having executed members of the Catiline Conspiracy four years previously without formal trial, and having had a public falling out with Clodius, was clearly the intended target of the law. Cicero argued that the \"senatus consultum ultimum\" indemnified him from punishment, and he attempted to gain the support of the senators and consuls, especially of Pompey. When help was not forthcoming, he went into exile. He arrived at Thessalonica, on May 23, 58 BC. Cicero's exile caused him to fall into depression. He wrote to Atticus: \"Your pleas have prevented me from committing suicide. But what is there to live for? Don't blame me for complaining. My afflictions surpass any you ever heard of earlier\". After the intervention of recently elected tribune Titus Annius Milo, the senate voted in favor of recalling Cicero from exile. Clodius cast the single vote against the decree. Cicero returned to Italy on August 5, 57 BC, landing at Brundisium. He was greeted by a cheering crowd, and, to his delight, his beloved daughter Tullia.\n\nCicero tried to re-enter politics, but his attack on a bill of Caesar's proved unsuccessful. The conference at Luca in 56 BC forced Cicero to recant and support the triumvirate. After this, a cowed Cicero concentrated on his literary works. It is uncertain whether he was directly involved in politics for the following few years. He reluctantly accepted a promagistracy in Cilicia for 51 BC, because there were few other eligible governors available as a result of a legislative requirement enacted by Pompey in 52 BC, specifying an interval of five years between a consulship or praetorship and a provincial command. He served as proconsul of Cilicia from May 51 to November 50 BC. He was given instructions to keep nearby Cappadocia loyal to the King, Ariobarzanes III, which he achieved ‘satisfactorily without war.’ Rome’s defeat by the Parthians and an uprising in Syria caused disquiet in Cilicia. Cicero maintained calm though his mild government. He discovered that much of public property had been embezzled and restored it. This made the cities better off. He retained the civil rights of, and did not impose penalties on, the men who gave the property back. Cicero defeated some robbers who were based on Mount Amanus and his soldiers hailed him as imperator. On his way back to Rome he stopped in Rhodes. He then spent some time in Athens, where he caught up with an old friend from his previous stay there and met men of great learning.\n\nThe struggle between Pompey and Julius Caesar grew more intense in 50 BC. Cicero favoured Pompey, seeing him as a defender of the senate and Republican tradition, but at that time avoided openly alienating Caesar. When Caesar invaded Italy in 49 BC, Cicero fled Rome. Caesar, seeking the legitimacy of an endorsement by a senior senator, courted Cicero's favour, but even so Cicero slipped out of Italy and traveled to Dyrrachium (Epidamnos), Illyria, where Pompey's staff was situated. Cicero traveled with the Pompeian forces to Pharsalus in 48 BC, though he was quickly losing faith in the competence and righteousness of the Pompeian side. Eventually, he provoked the hostility of his fellow senator Cato, who told him that he would have been of more use to the cause of the \"optimates\" if he had stayed in Rome. After Caesar's victory at the Battle of Pharsalus on August 9, Cicero returned to Rome only very cautiously. Caesar pardoned him and Cicero tried to adjust to the situation and maintain his political work, hoping that Caesar might revive the Republic and its institutions.\n\nIn a letter to Varro on c. April 20, 46 BC, Cicero outlined his strategy under Caesar's dictatorship. Cicero, however, was taken completely by surprise when the \"Liberatores\" assassinated Caesar on the ides of March, 44 BC. Cicero was not included in the conspiracy, even though the conspirators were sure of his sympathy. Marcus Junius Brutus called out Cicero's name, asking him to restore the republic when he lifted his bloodstained dagger after the assassination. A letter Cicero wrote in February 43 BC to Trebonius, one of the conspirators, began, \"How I could wish that you had invited me to that most glorious banquet on the Ides of March\"! Cicero became a popular leader during the period of instability following the assassination. He had no respect for Mark Antony, who was scheming to take revenge upon Caesar's murderers. In exchange for amnesty for the assassins, he arranged for the Senate to agree not to declare Caesar to have been a tyrant, which allowed the Caesarians to have lawful support and kept Caesar's reforms and policies intact.\n\nCicero and Antony now became the two leading men in Rome– Cicero as spokesman for the Senate; Antony as consul, leader of the Caesarian faction, and unofficial executor of Caesar's public will. Relations between the two, never friendly, worsened after Cicero claimed that Antony was taking liberties in interpreting Caesar's wishes and intentions. Octavian was Caesar's adopted son and heir; after he returned to Italy, Cicero began to play him against Antony. He praised Octavian, declaring he would not make the same mistakes as his father. He attacked Antony in a series of speeches he called the Philippics, after Demosthenes's denunciations of Philip II of Macedon. At the time Cicero's popularity as a public figure was unrivalled.\n\nCicero supported Decimus Junius Brutus Albinus as governor of Cisalpine Gaul (\"Gallia Cisalpina\") and urged the Senate to name Antony an enemy of the state. The speech of Lucius Piso, Caesar's father-in-law, delayed proceedings against Antony. Antony was later declared an enemy of the state when he refused to lift the siege of Mutina, which was in the hands of Decimus Brutus. Cicero’s plan to drive out Antony failed. Antony and Octavian reconciled and allied with Lepidus to form the Second Triumvirate after the successive battles of Forum Gallorum and Mutina. The Triumvirate began proscribing their enemies and potential rivals immediately after legislating the alliance into official existence for a term of five years with consular \"imperium\". Cicero and all of his contacts and supporters were numbered among the enemies of the state, and reportedly, Octavian argued for two days against Cicero being added to the list.\n\nCicero was one of the most viciously and doggedly hunted among the proscribed. He was viewed with sympathy by a large segment of the public and many people refused to report that they had seen him. He was caught December 7, 43 BC leaving his villa in Formiae in a litter going to the seaside where he hoped to embark on a ship destined for Macedonia. When his killers – Herennius (a centurion) and Popilius (a tribune) – arrived, Cicero's own slaves said they had not seen him, but he was given away by Philologus, a freed slave of his brother Quintus Cicero.\n\nCicero's last words are said to have been, \"There is nothing proper about what you are doing, soldier, but do try to kill me properly.\" He bowed to his captors, leaning his head out of the litter in a gladiatorial gesture to ease the task. By baring his neck and throat to the soldiers, he was indicating that he wouldn't resist. According to Plutarch, Herennius first slew him, then cut off his head. On Antony's instructions his hands, which had penned the Philippics against Antony, were cut off as well; these were nailed along with his head on the Rostra in the Forum Romanum according to the tradition of Marius and Sulla, both of whom had displayed the heads of their enemies in the Forum. Cicero was the only victim of the proscriptions who was displayed in that manner. According to Cassius Dio (in a story often mistakenly attributed to Plutarch), Antony's wife Fulvia took Cicero's head, pulled out his tongue, and jabbed it repeatedly with her hairpin in final revenge against Cicero's power of speech.\n\nCicero's son, Marcus Tullius Cicero Minor, during his year as a consul in 30 BC, avenged his father's death, to a certain extent, when he announced to the Senate Mark Antony's naval defeat at Actium in 31 BC by Octavian and his capable commander-in-chief, Agrippa.\n\nOctavian is reported to have praised Cicero as a patriot and a scholar of meaning in later times, within the circle of his family. However, it was Octavian's acquiescence that had allowed Cicero to be killed, as Cicero was proscribed by the new triumvirate.\n\nCicero's career as a statesman was marked by inconsistencies and a tendency to shift his position in response to changes in the political climate. His indecision may be attributed to his sensitive and impressionable personality; he was prone to overreaction in the face of political and private change.\n\"Would that he had been able to endure prosperity with greater self-control, and adversity with more fortitude!\" wrote C. Asinius Pollio, a contemporary Roman statesman and historian.\n\nCicero has been traditionally considered the master of Latin prose, with Quintilian declaring that Cicero was \"not the name of a man, but of eloquence itself.\" The English words \"Ciceronian\" (meaning \"eloquent\") and \"cicerone\" (meaning \"local guide\") derive from his name. He is credited with transforming Latin from a modest utilitarian language into a versatile literary medium capable of expressing abstract and complicated thoughts with clarity. Julius Caesar praised Cicero's achievement by saying \"it is more important to have greatly extended the frontiers of the Roman spirit (\"ingenium\") than the frontiers of the Roman empire\". According to John William Mackail, \"Cicero's unique and imperishable glory is that he created the language of the civilized world, and used that language to create a style which nineteen centuries have not replaced, and in some respects have hardly altered.\"\n\nCicero was also an energetic writer with an interest in a wide variety of subjects, in keeping with the Hellenistic philosophical and rhetorical traditions in which he was trained. The quality and ready accessibility of Ciceronian texts favored very wide distribution and inclusion in teaching curricula, as suggested by a graffito at Pompeii, admonishing: \"You will like Cicero, or you will be whipped\".\nCicero was greatly admired by influential Church Fathers such as Augustine of Hippo, who credited Cicero's lost \"Hortensius\" for his eventual conversion to Christianity, and St. Jerome, who had a feverish vision in which he was accused of being \"follower of Cicero and not of Christ\" before the judgment seat.\nThis influence further increased after the Early Middle Ages in Europe, which more of his writings survived than any other Latin author. Medieval philosophers were influenced by Cicero's writings on natural law and innate rights.\n\nPetrarch's rediscovery of Cicero's letters provided the impetus for searches for ancient Greek and Latin writings scattered throughout European monasteries, and the subsequent rediscovery of classical antiquity led to the Renaissance. Subsequently, Cicero became synonymous with classical Latin to such an extent that a number of humanist scholars began to assert that no Latin word or phrase should be used unless it appeared in Cicero's works, a stance criticized by Erasmus.\n\nHis voluminous correspondence, much of it addressed to his friend Atticus, has been especially influential, introducing the art of refined letter writing to European culture. Cornelius Nepos, the 1st century BC biographer of Atticus, remarked that Cicero's letters contained such a wealth of detail \"concerning the inclinations of leading men, the faults of the generals, and the revolutions in the government\" that their reader had little need for a history of the period.\n\nAmong Cicero's admirers were Desiderius Erasmus, Martin Luther, and John Locke. Following the invention of Johannes Gutenberg's printing press, \"De Officiis\" was the second book printed in Europe, after the Gutenberg Bible. Scholars note Cicero's influence on the rebirth of religious toleration in the 17th century.\n\nWhile Cicero the humanist deeply influenced the culture of the Renaissance, Cicero the republican inspired the Founding Fathers of the United States and the revolutionaries of the French Revolution. John Adams said, \"As all the ages of the world have not produced a greater statesman and philosopher united than Cicero, his authority should have great weight.\" Jefferson names Cicero as one of a handful of major figures who contributed to a tradition “of public right” that informed his draft of the Declaration of Independence and shaped American understandings of \"the common sense\" basis for the right of revolution. Camille Desmoulins said of the French republicans in 1789 that they were \"mostly young people who, nourished by the reading of Cicero at school, had become passionate enthusiasts for liberty\".\n\nJim Powell starts his book on the history of liberty with the sentence: \"Marcus Tullius Cicero expressed principles that became the bedrock of liberty in the modern world.\"\n\nLikewise, no other ancient personality has inspired as much venomous dislike as Cicero, especially in more modern times. His commitment to the values of the Republic accommodated a hatred of the poor and persistent opposition to the advocates and mechanisms of popular representation. Friedrich Engels referred to him as \"the most contemptible scoundrel in history\" for upholding republican \"democracy\" while at the same time denouncing land and class reforms. Cicero has faced criticism for exaggerating the democratic qualities of republican Rome, and for defending the Roman oligarchy against the popular reforms of Caesar. Michael Parenti admits Cicero's abilities as an orator, but finds him a vain, pompous and hypocritical personality who, when it suited him, could show public support for popular causes that he privately despised. Parenti presents Cicero's prosecution of the Catiline conspiracy as legally flawed at least, and possibly unlawful.\n\nCicero also had an influence on modern astronomy. Nicolaus Copernicus, searching for ancient views on earth motion, said that he \"first ... found in Cicero that Hicetas supposed the earth to move.\"\n\nCicero was declared a righteous pagan by the Early Church, and therefore many of his works were deemed worthy of preservation. The Bogomils considered him a rare exception of a pagan saint. Subsequent Roman and medieval Christian writers quoted liberally from his works \"De Re Publica\" (\"On the Commonwealth\") and \"De Legibus\" (\"On the Laws\"), and much of his work has been recreated from these surviving fragments. Cicero also articulated an early, abstract conceptualization of rights, based on ancient law and custom. Of Cicero's books, six on rhetoric have survived, as well as parts of eight on philosophy. Of his speeches, 88 were recorded, but only 58 survive.\n\n\n\nCicero's letters to and from various public and private figures are considered some of the most reliable sources of information for the people and events surrounding the fall of the Roman Republic. While 37 books of his letters have survived into modern times, 35 more books were known to antiquity that have since been lost. These included letters to Caesar, to Pompey, to Octavian, and to his son Marcus.\n\nBen Jonson dramatised the conspiracy of Catiline in his play \"Catiline His Conspiracy\", featuring Cicero as a character. Cicero also appears as a minor character in William Shakespeare's play \"Julius Caesar\".\n\nCicero was portrayed on the motion picture screen by British actor Alan Napier in the 1953 film \"Julius Caesar\", based on Shakespeare's play. He has also been played by such noted actors as Michael Hordern (in \"Cleopatra\"), and André Morell (in the 1970 \"Julius Caesar\"). Most recently, Cicero was portrayed by David Bamber in the HBO series \"Rome\" (2005–2007) and appeared in both seasons.\n\nIn the historical novel series \"Masters of Rome\", Colleen McCullough presents an unflattering depiction of Cicero's career, showing him struggling with an inferiority complex and vanity, morally flexible and fatally indiscreet, while his rival Julius Caesar is shown in a more approving light. Cicero is portrayed as a hero in the novel \"A Pillar of Iron\" by Taylor Caldwell (1965). Robert Harris' novels \"Imperium\", \"Lustrum\" (published under the name \"Conspirata\" in the United States) and \"Dictator\" is the three-part novel series based upon the life of Cicero. In these novels Cicero's character is depicted in a more balanced way than in those of McCullough, with his positive traits equaling or outweighing his weaknesses (while conversely Caesar is depicted as more sinister than in McCullough). Cicero is a major recurring character in the \"Roma Sub Rosa\" series of mystery novels by Steven Saylor. He also appears several times as a peripheral character in John Maddox Roberts' \"SPQR\" series. The protagonist, Decius Metellus, admires Cicero for his erudition, but is disappointed by his lack of real opposition to Caesar, as well as puzzled by his relentless fawning on the \"Optimates\", who secretly despise Cicero as a parvenu.\n\n\n\n\n"}
{"id": "6047", "url": "https://en.wikipedia.org/wiki?curid=6047", "title": "Consul", "text": "Consul\n\nConsul (abbrev. \"cos.\"; Latin plural \"consules\") was the title of one of the chief magistrates of the Roman Republic, and subsequently a somewhat significant title under the Roman Empire. The title was also used in other city states and also revived in modern states, notably in the First French Republic. The relating adjective is consular, from the \"consularis\".\n\nIn modern terminology, a consul is a type of diplomat. The \"American Heritage Dictionary\" defines consul as \"an official appointed by a government to reside in a foreign country and represent its interests there.\"\n\nIn most governments, the consul is the head of the consular section of an embassy, and is responsible for all consular services such as immigrant and non-immigrant visas, passports, and citizen services for expatriates living or traveling in the host country.\n\nThroughout most of southern France, a consul ( or \"\") was an office equivalent to the of the north and roughly similar with English aldermen. The most prominent were those of Bordeaux and Toulouse, which came to be known as jurats and capitouls, respectively. The capitouls of Toulouse were granted transmittable nobility. In many other smaller towns the first consul, was the equivalent of a mayor today, assisted by a variable number of secondary consuls and jurats. His main task was to levy and collect tax.\n\nThe Dukes of Gaeta often used also the title of \"consul\" in its Greek form \"Hypatos\" (see List of Hypati and Dukes of Gaeta).\n\nThe city-state of Genoa, unlike ancient Rome, bestowed the title of \"consul\" on various state officials, not necessarily restricted to the highest. Among these were Genoese officials stationed in various Mediterranean ports, whose role included helping Genoese merchants and sailors in difficulties with the local authorities. This institution, with its name, was later emulated by other powers and is reflected in the modern usage of the word (see Consul (representative)).\n\nAfter Napoleon Bonaparte staged a coup against the Directory government in November 1799, the French Republic adopted a constitution which conferred executive powers upon three consuls, elected for a period of ten years. In reality, the first consul, Bonaparte, dominated his two colleagues and held supreme power, soon making himself consul for life (1802) and eventually, in 1804, emperor.\n\nThe office was held by:\n\n\nThe French-sponsored Roman Republic (15 February 1798 – 23 June 1800) was headed by multiple consuls:\n\n\"Consular rule was interrupted by the Neapolitan occupation (27 November – 12 December 1798), which installed a Provisional Government:\n\n\"Rome was occupied by France (11 July – 28 September 1799) and again by Naples (30 September 1799 – 23 June 1800), bringing an end to the Roman Republic.\"\n\nThe short-lived Bolognese Republic, proclaimed in 1796 as a French client republic in the Central Italian city of Bologna, had a government consisting of nine consuls and its head of state was the \"Presidente del Magistrato\", i.e., chief magistrate, a presiding office held for four months by one of the consuls. As noted above, Bologna already had consuls at some parts of its Medieval history.\n\nIn between series of juntas (and various other short-lived regimes), the young republic was governed by \"consuls of the republic\" in power (2 consuls alternating in power every 4 months):\n\nAfter a few presidents of the Provisional Junta, there were again consuls of the republic, 14 March 1841 – 13 March 1844 (ruling jointly, but occasionally styled \"first consul\", \"second consul\"): Carlos Antonio López Ynsfrán (b. 1792 – d. 1862) + Mariano Roque Alonzo Romero (d. 1853) (the lasts of the aforementioned juntistas, Commandant-General of the Army)\nThereafter all republican rulers were styled \"president\".\n\nWhile many cities (as in Gaul) had a double-headed chief magistracy, often another title was used, such as \"Duumvir\" or native styles such as \"Meddix\", but \"consul\" was used in some.\n\nIt was not uncommon for an organization under Roman private law to copy the terminology of state and city institutions for its own statutory agents. The founding statute, or contract, of such an organisation was called \"lex\", 'law'. The people elected each year were patricians, members of the upper class.\n\nAmong the many petty local republics that were formed during the first year of the Greek Revolution, prior to the creation of a unified Provisional Government at the First National Assembly at Epidaurus, were:\n\"Note: in Greek, the term for \"consul\" is \"hypatos\" (ὕπατος), which translates as \"supreme one\", and hence does not necessarily imply a joint office.\"\n\nDifferently named, but same function\n\nModern UN System \n\nRoman Empire\n\n"}
{"id": "6050", "url": "https://en.wikipedia.org/wiki?curid=6050", "title": "List of equations in classical mechanics", "text": "List of equations in classical mechanics\n\nClassical mechanics is the branch of physics used to describe the motion of macroscopic objects. It is the most familiar of the theories of physics. The concepts it covers, such as mass, acceleration, and force, are commonly used and known. The subject is based upon a three-dimensional Euclidean space with fixed axes, called a frame of reference. The point of concurrency of the three axes is known as the origin of the particular space.\n\nClassical mechanics utilises many equations—as well as other mathematical concepts—which relate various physical quantities to one another. These include differential equations, manifolds, Lie groups, and ergodic theory. This page gives a summary of the most important of these.\n\nThis article lists equations from Newtonian mechanics, see analytical mechanics for the more general formulation of classical mechanics (which includes Lagrangian and Hamiltonian mechanics).\n\nEvery conservative force has a potential energy. By following two principles one can consistently assign a non-relative value to \"U\":\n\n\nIn the following rotational definitions, the angle can be any angle about the specified axis of rotation. It is customary to use \"θ\", but this does not have to be the polar angle used in polar coordinate systems. The unit axial vector\n\ndefines the axis of rotation, formula_2 = unit vector in direction of r, formula_3 = unit vector tangential to the angle.\n\n!\n! Translation\n! Rotation\n!Velocity\nInstantaneous:\n\nInstantaneous:\n\nInstantaneous:\n"}
{"id": "6051", "url": "https://en.wikipedia.org/wiki?curid=6051", "title": "Cursus honorum", "text": "Cursus honorum\n\nThe cursus honorum (Latin: \"course of offices\") was the sequential order of public offices held by aspiring politicians in both the Roman Republic and the early Roman Empire. It was designed for men of senatorial rank. The cursus honorum comprised a mixture of military and political administration posts. Each office had a minimum age for election. There were minimum intervals between holding successive offices and laws forbade repeating an office.\n\nThese rules were altered and flagrantly ignored in the course of the last century of the Republic. For example, Gaius Marius held consulships for five years in a row between 104 BC and 100 BC. Officially presented as opportunities for public service, the offices often became mere opportunities for self-aggrandizement. The reforms of Lucius Cornelius Sulla required a ten-year period between holding another term in the same office.\n\nTo have held each office at the youngest possible age (\"suo anno,\" \"in his year\") was considered a great political success, since to miss out on a praetorship at 39 meant that one could not become consul at 42. Cicero expressed extreme pride not only in being a \"novus homo\" (\"new man\"; comparable to a \"self-made man\") who became consul even though none of his ancestors had ever served as a consul, but also in having become consul \"in his year\".\n\nThe cursus honorum began with ten years of military duty in the Roman cavalry (the \"equites\") or in the staff of a general who was a relative or a friend of the family. The ten years of service were intended to be mandatory in order to qualify for political office, but in practice, the rule was not always rigidly applied.\n\nA more prestigious position was that of a military tribune. In the early Roman Republic, 24 men at the age of around 20 were elected by the Tribal Assembly to serve as a commander in the legions, with six tribunes to each and command rotating among them. Tribunes could also be appointed by the consuls or by military commanders in the field as necessary. After the reforms of Gaius Marius in 107 BC, the six tribunes acted as staff officers for the legionary Legatus and were appointed tasks and command of units of troops whenever the need arose.\n\nThe following steps of the \"cursus honorum\" were achieved by direct election every year.\n\nThe first official post was that of \"quaestor\". Candidates had to be at least 30 years old. However, men of patrician rank could subtract two years from this and other minimum age requirements.\n\nTwenty quaestors served in the financial administration at Rome or as second-in-command to a governor in the provinces. They could also serve as the paymaster for a legion. A young man who obtained this job was expected to become a very important official. An additional task of all quaestors was the supervision of public games. As a quaestor, an official was allowed to wear the toga praetexta, but was not escorted by lictors, nor did he possess imperium.\n\nAt 36 years of age, former quaestors could stand for election to one of the \"aedile\" positions. Of these aediles, two were plebeian and two were patrician, with the patrician aediles called Curule Aediles. The plebeian aediles were elected by the Plebeian Council and the curule aediles were either elected by the Tribal Assembly or appointed by the reigning consul. The aediles had administrative responsibilities in Rome. They had to take care of the temples (whence their title, from the Latin \"aedes\", \"temple\"), organize games, and be responsible for the maintenance of the public buildings in Rome. Moreover, they took charge of Rome's water and food supplies; in their capacity as market superintendents, they served sometimes as judges in mercantile affairs.\n\nThe Aedile was the supervisor of public works; the words \"edifice\" and \"edification\" stem from the same root. He oversaw the public works, temples and markets. Therefore, the Aediles would have been in some cooperation with the current Censors, who had similar or related duties. Also they oversaw the organization of festivals and games (\"ludi\"), which made this a very sought-after office for a career minded politician of the late republic, as it was a good means of gaining popularity by staging spectacles.\n\nCurule Aediles were added at a later date in the 4th century BC, and their duties do not differ substantially from plebeian aediles. However, unlike plebeian aediles, curule aediles were allowed certain symbols of rank—the \"sella curulis\" or 'curule chair,' for example—and only patricians could stand for election to curule aedile. This later changed, and both Plebeians and Patricians could stand for Curule Aedileship.\n\nThe elections for Curule Aedile were at first alternated between Patricians and Plebeians, until late in the 2nd century BC, when the practice was abandoned and both classes became free to run during all years.\n\nWhile part of the \"cursus honorum\", this step was optional and not required to hold future offices. Though the office was usually held after the quaestorship and before the praetorship, there are some cases with former praetors serving as aediles.\n\nAfter holding either the office of quaestor or aedile, a man of 39 years could run for \"praetor\". The number of Praetors elected varied through history, generally increasing with time. During the republic, six or eight were generally elected each year to serve judicial functions throughout Rome and other governmental responsibilities. In the absence of the Consuls, a Praetor would be given command of the garrison in Rome or in Italy. Also, a Praetor could exercise the functions of the Consuls throughout Rome, but their main function was that of a judge. They would preside over trials involving criminal acts as well as grant court orders or validate \"illegal\" acts as acts of administering justice. As a Praetor, a magistrate was escorted by six lictors, and wielded imperium. After a term as Praetor, the magistrate would serve as a provincial governor in the office of Propraetor, wielding Propraetor imperium, commanding the province’s legions, and possessing ultimate authority within his province(s).\n\nOf all the Praetors, two were more prestigious than the others. The first was the Praetor Peregrinus, who was the chief judge in trials involving one or more foreigners. The other was the Praetor Urbanus, the chief judicial office in Rome. He had the power to overturn any verdict by any other courts, and served as judge in cases involving criminal charges against provincial governors. The Praetor Urbanus was not allowed to leave the city for more than ten days. If one of these two Praetors was absent from Rome, the other would perform the duties of both.\n\nThe office of \"consul\" was the most prestigious of all, and represented the summit of a successful career. The minimum age was 42 for plebeians and 40 for patricians. Years were identified by the names of the two consuls elected for a particular year; for instance, \"M. Messalla et M. Pisone consulibus\", \"in the consulship of Messalla and Piso,\" dates an event to 61 BC. Consuls were responsible for the city's political agenda, commanded large-scale armies and controlled important provinces. The consuls served for only a year (a restriction intended to limit the amassing of power by individuals) and could only rule when they agreed, because each consul could veto the other's decision.\n\nThe consuls would alternate monthly as the chairman of the Senate. They also were the supreme commanders in the Roman army, with each being granted two legions during their consular year. Consuls also exercised the highest juridical power in the Republic, being the only office with the power to override the decisions of the Praetor Urbanus. Only laws and the decrees of the Senate or the People's assembly limited their powers, and only the veto of a fellow consul or a tribune of the plebs could supersede their decisions.\n\nA consul was escorted by twelve lictors, owned imperium and wore the toga praetexta. Because the consul was the highest executive office within the Republic, they had the power to veto any action or proposal by any other magistrate, save that of the Tribune of the Plebs. After a consulship, a consul was assigned one of the more important provinces and acted as the governor in the same way that a Propraetor did, only owning Proconsular imperium. A second consulship could only be attempted after an interval of 10 years to prevent one man holding too much power.\n\nAlthough not part of the Cursus Honorum, upon completing a term as either Praetor or Consul, an officer was required to serve a term as Propraetor and Proconsul, respectively, in one of Rome's many provinces. These Propraetors and Proconsuls held near autocratic authority within their selected province or provinces. Because each governor held equal imperium to the equivalent magistrate, they were escorted by the same number of lictors (12) and could only be vetoed by a reigning Consul or Praetor. Their abilities to govern were only limited by the decrees of the Senate or the people's assemblies, and the Tribune of the Plebs was unable to veto their acts as long as the governor remained at least a mile outside of Rome.\n\nAfter a term as consul, the final step in the Cursus Honorum was the office of \"censor\". This was the only office in the Roman Republic whose term was a period of eighteen months instead of the usual twelve. Censors were elected every five years and although the office held no military imperium, it was considered a great honour. The censors took a regular census of the people and then apportioned the citizens into voting classes on the basis of income and tribal affiliation. The censors enrolled new citizens in tribes and voting classes as well. The censors were also in charge of the membership roll of the Senate, every five years adding new senators who had been elected to the requisite offices. Censors could also remove unworthy members from the Senate. This ability was lost during the dictatorship of Sulla. Censors were also responsible for construction of public buildings and the moral status of the city.\n\nCensors also had financial duties, in that they had to put out to tender projects that were to be financed by the state. Also, the censors were in charge of the leasing out of conquered land for public use and auction. Though this office owned no imperium, meaning no lictors for protection, they were allowed to wear the \"toga praetexta\".\n\nThe office of Tribune of the Plebs was an important step in the political career of plebeians. Patricians could not hold the office. The Tribune was an office first created to protect the right of the common man in Roman politics and served as the head of the Plebeian Council. In the mid-to-late Republic, however, plebeians were often just as, and sometimes more, wealthy and powerful than patricians. Those who held the office were granted sacrosanctity (the right to be legally protected from any physical harm), the power to rescue any plebeian from the hands of a patrician magistrate, and the right to veto any act or proposal of any magistrate, including another tribune of the people and the consuls. The tribune also had the power to exercise capital punishment against any person who interfered in the performance of his duties. The tribunes could even convene a Senate meeting and lay legislation before it and arrest magistrates. Their houses had to remain open for visitors even during the night, and they were not allowed to be more than a day's journey from Rome. Due to their unique power of sacrosanctity, the Tribune had no need for lictors for protection and owned no imperium, nor could they wear the toga praetexta. After Sulla's reforms, a person who had held the office of Tribune of the Plebs could no longer qualify for any other office, and the powers of the tribunes were more limited.\n\nAnother office not officially a step in the \"cursus honorum\" was the \"princeps senatus\", an extremely prestigious office for a patrician. The \"princeps senatus\" served as the leader of the Senate and was chosen to serve a five-year term by each pair of Censors every five years. Censors could, however, confirm a \"princeps senatus\" for a period of another five years. The \"princeps senatus\" was chosen from all Patricians who had served as a Consul, with former Censors usually holding the office. The office originally granted the holder the ability to speak first at session on the topic presented by the presiding magistrate, but eventually gained the power to open and close the senate sessions, decide the agenda, decide where the session should take place, impose order and other rules of the session, meet in the name of the senate with embassies of foreign countries, and write in the name of the senate letters and dispatches. This office, like the Tribune, did not own \"imperium\", was not escorted by lictors, and could not wear the \"toga praetexta\".\n\nOf all the offices within the Roman Republic, none granted as much power and authority as the position of \"dictator\", known as the Master of the People. In times of emergency, the Senate would declare that a dictator was required, and the current consuls would appoint a dictator. This was the only decision that could not be vetoed by the Tribune of the Plebs. The dictator was the sole exception to the Roman legal principles of having multiple magistrates in the same office and being legally able to be held to answer for actions in office. Essentially by definition, only one dictator could serve at a time, and no dictator could ever be held legally responsible for any action during his time in office for any reason.\n\nThe dictator was the highest magistrate in degree of imperium and was attended by twenty-four lictors (as were the former Kings of Rome). Although his term lasted only six months instead of twelve (except for the Dictatorships of Sulla and Caesar), all other magistrates reported to the dictator (except for the tribunes of the plebs - although they could not veto any of the dictator's acts), granting the dictator absolute authority in both civil and military matters throughout the Republic. The Dictator was free from the control of the Senate in all that he did, could execute anyone without a trial for any reason, and could ignore any law in the performance of his duties. The Dictator was the sole magistrate under the Republic that was truly independent in discharging his duties. All of the other offices were extensions of the Senate's executive authority and thus answerable to the Senate. Since the Dictator exercised his own authority, he did not suffer this limitation, which was the cornerstone of the office's power.\n\nWhen a Dictator entered office, he appointed to serve as his second-in-command a \"magister equitum\", the Master of the Horse, whose office ceased to exist once the Dictator left office. The magister equitum held Praetorian imperium, was attended by six lictors, and was charged with assisting the Dictator in managing the State. When the Dictator was away from Rome, the magister equitum usually remained behind to administer the city. The magister equitum, like the Dictator, had unchallengeable authority in all civil and military affairs, with his decisions only being overturned by the Dictator himself.\n\nThe Dictatorship was definitively abolished in 44 BC after the assassination of Gaius Julius Caesar (Lex Antonia).\n\n\n"}
{"id": "6056", "url": "https://en.wikipedia.org/wiki?curid=6056", "title": "Continental drift", "text": "Continental drift\n\nContinental drift is the movement of the Earth's continents relative to each other, thus appearing to \"drift\" across the ocean bed. The speculation that continents might have 'drifted' was first put forward by Abraham Ortelius in 1596. The concept was independently and more fully developed by Alfred Wegener in 1912, but his theory was rejected by some for lack of a mechanism (though this was supplied later by Arthur Holmes). The idea of continental drift has been subsumed by the theory of plate tectonics, which explains how the continents move.\n\nAbraham Ortelius , Theodor Christoph Lilienthal (1756), Alexander von Humboldt (1801 and 1845), Antonio Snider-Pellegrini , and others had noted earlier that the shapes of continents on opposite sides of the Atlantic Ocean (most notably, Africa and South America) seem to fit together. W. J. Kious described Ortelius' thoughts in this way:\n\nWriting in 1889, Alfred Russel Wallace remarks, \"It was formerly a very general belief, even amongst geologists, that the great features of the earth's surface, no less than the smaller ones, were subject to continual mutations, and that during the course of known geological time the continents and great oceans had again and again changed places with each other.\" He quotes Charles Lyell as saying, \"Continents, therefore, although permanent for whole geological epochs, shift their positions entirely in the course of ages.\" and claims that the first to throw doubt on this was James Dwight Dana in 1849.\n\nIn his \"Manual of Geology\", 1863, Dana says, \"The continents and oceans had their general outline or form defined in earliest time. This has been proved with respect to North America from the position and distribution of the first beds of the Silurian – those of the Potsdam epoch. … and this will probably prove to the case in Primordial time with the other continents also\". Dana was enormously influential in America – his \"Manual of Mineralogy\" is still in print in revised form – and the theory became known as \"Permanence theory\".\n\nThis appeared to be confirmed by the exploration of the deep sea beds conducted by the Challenger expedition, 1872-6, which showed that contrary to expectation, land debris brought down by rivers to the ocean is deposited comparatively close to the shore on what is now known as the continental shelf. This suggested that the oceans were a permanent feature of the Earth's surface, and did not change places with the continents.\n\nThe idea that the American continents had once formed a single landmass together with Europe and Asia before assuming their present shapes and positions was speculated by several scientists before Alfred Wegener's 1912 paper.\nAlthough Wegener's theory was formed independently and was more complete than those of his predecessors, Wegener later credited a number of past authors with similar ideas:\nFranklin Coxworthy (between 1848 and 1890),\nRoberto Mantovani (between 1889 and 1909), William Henry Pickering (1907)\nand Frank Bursley Taylor (1908). In addition, Eduard Suess had proposed a supercontinent Gondwana in 1885 and the Tethys Ocean in 1893, assuming a land-bridge between the present continents submerged in the form of a geosyncline, and John Perry had written an 1895 paper proposing that the earth's interior was fluid, and disagreeing with Lord Kelvin on the age of the earth.\n\nFor example: the similarity of southern continent geological formations had led Roberto Mantovani to conjecture in 1889 and 1909 that all the continents had once been joined into a supercontinent; Wegener noted the similarity of Mantovani's and his own maps of the former positions of the southern continents. In Mantovani's conjecture, this continent broke due to volcanic activity caused by thermal expansion, and the new continents drifted away from each other because of further expansion of the rip-zones, where the oceans now lie. This led Mantovani to propose an Expanding Earth theory which has since been shown to be incorrect.\n\nContinental drift without expansion was proposed by Frank Bursley Taylor, who suggested in 1908 (published in 1910) that the continents were moved into their present positions by a process of \"continental creep\". In a later paper he proposed that this occurred by their being dragged towards the equator by tidal forces during the hypothesized capture of the moon in the Cretaceous, resulting in \"general crustal creep\" toward the equator. Although his proposed mechanism was wrong, he was the first to realize the insight that one of the effects of continental motion would be the formation of mountains, and attributed the formation of the Himalayas to the collision between the Indian subcontinent with Asia. Wegener said that of all those theories, Taylor's, although not fully developed, had the most similarities to his own. In the mid-20th century, the theory of continental drift was referred to as the \"Taylor-Wegener hypothesis\", although this terminology eventually fell out of common use.\n\nAlfred Wegener first presented his hypothesis to the German Geological Society on January 6, 1912. His hypothesis was that the continents had once formed a single landmass, called Pangea, before breaking apart and drifting to their present locations.\n\nWegener was the first to use the phrase \"continental drift\" (1912, 1915) (in German \"die Verschiebung der Kontinente\" – translated into English in 1922) and formally publish the hypothesis that the continents had somehow \"drifted\" apart. Although he presented much evidence for continental drift, he was unable to provide a convincing explanation for the physical processes which might have caused this drift. His suggestion that the continents had been pulled apart by the centrifugal pseudoforce (\"Polflucht\") of the Earth's rotation or by a small component of astronomical precession was rejected, as calculations showed that the force was not sufficient. The Polflucht hypothesis was also studied by Paul Sophus Epstein in 1920 and found to be implausible.\n\nThe theory of continental drift was not accepted for many years. One problem was that a plausible driving force was missing. A second problem was that Wegener's estimate of the velocity of continental motion, 250 cm/year, was implausibly high. (The currently accepted rate for the separation of the Americas from Europe and Africa is about 2.5 cm/year). And it did not help that Wegener was not a geologist. Other geologists also believed that the evidence that Wegener had provided was not sufficient. It is now accepted that the plates carrying the continents do move across the Earth's surface, although not as fast as Wegener believed; ironically one of the chief outstanding questions is the one Wegener failed to resolve: what is the nature of the forces propelling the plates?\n\nThe British geologist Arthur Holmes championed the theory of continental drift at a time when it was deeply unfashionable. He proposed in 1931 that the Earth's mantle contained convection cells that dissipated radioactive heat and moved the crust at the surface. His \"Principles of Physical Geology\", ending with a chapter on continental drift, was published in 1944.\n\nDavid Attenborough, who attended university in the second half of the 1940s, recounted an incident illustrating its lack of acceptance then: \"I once asked one of my lecturers why he was not talking to us about continental drift and I was told, sneeringly, that if I could prove there was a force that could move continents, then he might think about it. The idea was moonshine, I was informed.\"\n\nGeological maps of the time showed huge land bridges spanning the Atlantic and Indian oceans to account for the similarities of fauna and flora and the divisions of the Asian continent in the Permian era but failing to account for glaciation in India, Australia and South Africa.\n\nAs late as 1953 – just five years before Carey introduced the theory of plate tectonics – the theory of continental drift was rejected by the physicist Scheidegger on the following grounds.\n\n\nGeophysicist Jack Oliver is credited with providing seismologic evidence supporting plate tectonics which encompassed and superseded continental drift with the article \"Seismology and the New Global Tectonics\", published in 1968, using data collected from seismologic stations, including those he set up in the South Pacific.\n\nIt is now known that there are two kinds of crust: continental crust and oceanic crust. Continental crust is inherently lighter and its composition is different from oceanic crust, but both kinds reside above a much deeper \"plastic\" mantle. Oceanic crust is created at spreading centers, and this, along with subduction, drives the system of plates in a chaotic manner, resulting in continuous orogeny and areas of isostatic imbalance. The theory of plate tectonics explains all this, including the movement of the continents, better than Wegener's theory.\n\nEvidence for the movement of continents on tectonic plates is now extensive. Similar plant and animal fossils are found around the shores of different continents, suggesting that they were once joined. The fossils of \"Mesosaurus\", a freshwater reptile rather like a small crocodile, found both in Brazil and South Africa, are one example; another is the discovery of fossils of the land reptile \"Lystrosaurus\" in rocks of the same age at locations in Africa, India, and Antarctica. There is also living evidence—the same animals being found on two continents. Some earthworm families (e.g. Ocnerodrilidae, Acanthodrilidae, Octochaetidae) are found in South America and Africa, for instance.\n\nThe complementary arrangement of the facing sides of South America and Africa is obvious, but is a temporary coincidence. In millions of years, slab pull and ridge-push, and other forces of tectonophysics, will further separate and rotate those two continents. It was this temporary feature which inspired Wegener to study what he defined as continental drift, although he did not live to see his hypothesis generally accepted.\n\nWidespread distribution of Permo-Carboniferous glacial sediments in South America, Africa, Madagascar, Arabia, India, Antarctica and Australia was one of the major pieces of evidence for the theory of continental drift. The continuity of glaciers, inferred from oriented glacial striations and deposits called tillites, suggested the existence of the supercontinent of Gondwana, which became a central element of the concept of continental drift. Striations indicated glacial flow away from the equator and toward the poles, based on continents' current positions and orientations, and supported the idea that the southern continents had previously been in dramatically different locations, as well as being contiguous with each other.\n\n\n\n"}
{"id": "6057", "url": "https://en.wikipedia.org/wiki?curid=6057", "title": "Commodores", "text": "Commodores\n\nThe Commodores are an American funk/soul band, which was at its peak in the late 1970s through the mid 1980s. The members of the group met as mostly freshmen at Tuskegee Institute (now Tuskegee University) in 1968, and signed with Motown in November 1972, having first caught the public eye opening for The Jackson 5 while on tour.\n\nThe group's most successful period was in the late 1970s and early 1980s when Lionel Richie was the co-lead singer. The band's biggest hit singles are ballads such as \"Easy\", \"Three Times a Lady\", and \"Nightshift\"; and funky dance hits which include \"Brick House\", \"Fancy Dancer\", \"Lady (You Bring Me Up)\", and \"Too Hot ta Trot\". In 1986 the Commodores won their first Grammy for the song \"Nightshift\".\n\nThe Commodores originally came together from two former student groups, the Mystics and the Jays. Richie described some members of the Mystics as \"jazz buffs\".\nTogether, a six-man band was created from which the notable individuals were Lionel Richie, Thomas McClary, and William King from the Mystics; Andre Callahan, Michael Gilbert, and Milan Williams were from the Jays. They wanted to change the name. To choose a new name, William \"WAK\" King opened a dictionary and randomly picked a word. \"We lucked out,\" he remarked with a laugh when telling this story to \"People\" magazine. \"We almost became 'The Commodes!'\"\n\nThe band originated while its members attended Tuskegee University in Alabama. After winning the university's annual freshman talent contest, they played at fraternity parties as well as a weekend gig at the Black Forest Inn, one of a few clubs in Tuskegee that catered to college students. They performed mostly cover tunes and some original songs with their first singer, James Ingram (not the famous solo artist). Ingram, older than the rest of the band, left to serve active duty in Vietnam, and was later replaced by Walter \"Clyde\" Orange, who would write or co-write many of their hit tunes. Lionel Richie and Orange alternated as lead singers. (Orange was the lead singer on the Top 10 hits \"Brick House\" and \"Nightshift\".)\n\nThe Commodores made a brief appearance in the 1978 film \"Thank God It's Friday\". They performed the song \"Too Hot ta Trot\" during the dance contest; the songs \"Brick House\" and \"Easy\" were also played during the movie.\n\n\"Machine Gun\", the instrumental title track from the band's debut album, became a staple at American sporting events, and is similarly featured in many films, including \"Boogie Nights\" and \"Looking for Mr. Goodbar\". It reached No. 22 on the \"Billboard\" Hot 100 in 1975. Another instrumental, \"Cebu\" (named after an island in the Philippines), later became a staple in the Quiet storm format. Three albums released in 1975 and 1976 (\"Caught in the Act\", \"Movin' On\", and \"Hot On The Tracks\") are considered the peak of their harder funk period. After those recordings the group started to move towards a softer sound. That move was hinted at in their 1976 Top Ten hits \"Sweet Love\" and \"Just to Be Close to You\". In 1977 the Commodores released \"Easy\", which became the group's biggest hit yet, reaching No. 4 in the U.S., followed by \"Brick House\", also top 5, both from their album \"The Commodores\", as was \"Zoom\". The group reached No. 1 in 1978 with \"Three Times a Lady\". In 1979 the Commodores scored another top-five ballad, \"Sail On\", before reaching the top of the charts once again with another ballad, \"Still\". In 1981 they released two top-ten hits with \"Oh No\" (No. 4) and their first upbeat single in almost five years, \"Lady (You Bring Me Up)\" (No. 8).\n\nIn 1982, Lionel Richie left to pursue a solo career. Skyler Jett replaced Richie as co-lead singer. Also in 1982, their manager Benjamin Ashburn who also managed another band Platinum Hook died of a heart attack aged 54.\n\nOver time, several founding members left - McClary left in 1983 (shortly after Richie) to pursue a solo career and to develop a gospel music company. McClary was replaced by guitarist-vocalist Sheldon Reynolds while LaPread left in 1986 and moved to Auckland, New Zealand and Reynolds departed for Earth, Wind & Fire in 1987, which prompted trumpeter William \"WAK\" King to take over primary guitar duties for live performances. Keyboardist Milan Williams exited the band in 1989 after allegedly refusing to tour South Africa.\n\nThe group also gradually abandoned its funk roots and moved into the more commercial pop arena. In 1984 former Heatwave singer James Dean \"J.D.\" Nicholas assumed co-lead vocal duties with drummer Walter Orange. The band remained hitless until 1985 when their final Motown album, \"Nightshift\", produced by Dennis Lambert—all prior albums were produced by James Anthony Carmichael—delivered the Grammy Award-winning title track \"Nightshift\" (No. 3 in the U.S.), a loving tribute to Marvin Gaye and Jackie Wilson who had both died the previous year. In 2010 a new version was recorded, dedicated to Michael Jackson. The Commodores were on a European tour performing at Wembley Arena, London, on June 25, 2009, when they walked off the stage after they were told that Michael Jackson had died. Initially the band thought it was a hoax. However, back in their dressing rooms they received confirmation and broke down in tears. The next night at Birmingham's NIA Arena, J.D. Nicholas added Jackson's name into the lyrics of the song, and thenceforth the Commodores have mentioned Jackson and other deceased R&B singers. Thus came the inspiration upon the one-year anniversary of Jackson's death, to re-record, with new lyrics, the hit song \"Nightshift\" as a tribute. \"Nightshift\" won The Commodores their first Grammy for Best R&B Performance by a Duo or Group With Vocals in 1985.\n\nIn 1990 the Commodores formed Commodores Records and re-recorded their 20 greatest hits as \"Commodores Hits Vol. I & II\". They have recorded a live album \"Commodores Live!\" along with a DVD of the same name, and a Christmas album titled \"Commodores Christmas\". In 2012, the band was working on new material, some contributions written by current and former members.\n\nThe Commodores now consist of Walter \"Clyde\" Orange, James Dean \"J.D.\" Nicholas, and William \"WAK\" King, along with their five-piece band, known as the \"Mean Machine\". The group continues to perform, playing at arenas, theaters, and festivals around the world.\n\n\n\n<nowiki>*</nowiki> Original member\n\nAmong multiple Grammy Award nominations, they won a Grammy for \"Nightshift\" in 1986. In 2003, they were inducted into The Vocal Group Hall of Fame.\n\n"}
{"id": "6058", "url": "https://en.wikipedia.org/wiki?curid=6058", "title": "Collagen", "text": "Collagen\n\nCollagen is the main structural protein in the extracellular space in the various connective tissues in animal bodies. As the main component of connective tissue, it is the most abundant protein in mammals, making up from 25% to 35% of the whole-body protein content. Depending upon the degree of mineralization, collagen tissues may be rigid (bone), compliant (tendon), or have a gradient from rigid to compliant (cartilage). Collagen, in the form of elongated fibrils, is mostly found in fibrous tissues such as tendons, ligaments and skin. It is also abundant in corneas, cartilage, bones, blood vessels, the gut, intervertebral discs, and the dentin in teeth. In muscle tissue, it serves as a major component of the endomysium. Collagen constitutes one to two percent of muscle tissue, and accounts for 6% of the weight of strong, tendinous muscles. The fibroblast is the most common cell that creates collagen.\n\nGelatin, which is used in food and industry, is collagen that has been irreversibly hydrolyzed. Collagen also has many medical uses in treating complications of the bones and skin.\n\nThe name \"collagen\" comes from the Greek κόλλα (\"kólla\"), meaning \"glue\", and suffix -γέν, \"-gen\", denoting \"producing\". This refers to the compound's early use in the process of boiling the skin and tendons of horses and other animals to obtain glue.\n\nCollagen occurs in many places throughout the body. Over 90% of the collagen in the human body, however, is type I.\n\nSo far, 28 types of collagen have been identified and described. They can be divided into several groups according to the structure they form:\n\n\nThe five most common types are:\n\n\nThe collagenous cardiac skeleton which includes the four heart valve rings, is histologically, elastically and uniquely bound to cardiac muscle. The cardiac skeleton also includes the separating septa of the heart chambers – the interventricular septum and the atrioventricular septum. Collagen contribution to the measure of cardiac performance summarily represents a continuous torsional force opposed to the fluid mechanics of blood pressure emitted from the heart. The collagenous structure that divides the upper chambers of the heart from the lower chambers is an impermeable membrane that excludes both blood and electrical impulses through typical physiological means. With support from collagen, atrial fibrillation should never deteriorate to ventricular fibrillation. Collagen is layered in variable densities with cardiac muscle mass. The mass, distribution, age and density of collagen all contribute to the compliance required to move blood back and forth. Individual cardiac valvular leaflets are folded into shape by specialized collagen under variable pressure. Gradual calcium deposition within collagen occurs as a natural function of aging. Calcified points within collagen matrices show contrast in a moving display of blood and muscle, enabling methods of cardiac imaging technology to arrive at ratios essentially stating blood in (cardiac input) and blood out (cardiac output). Pathology of the collagen underpinning of the heart is understood within the category of connective tissue disease.\n\nCollagen has been widely used in cosmetic surgery, as a healing aid for burn patients for reconstruction of bone and a wide variety of dental, orthopedic, and surgical purposes. Both human and bovine collagen is widely used as dermal fillers for treatment of wrinkles and skin aging. Some points of interest are:\n\nAs the skeleton forms the structure of the body, it is vital that it maintains its strength, even after breaks and injuries. Collagen is used in bone grafting as it has a triple helical structure, making it a very strong molecule. It is ideal for use in bones, as it does not compromise the structural integrity of the skeleton. The triple helical structure of collagen prevents it from being broken down by enzymes, it enables adhesiveness of cells and it is important for the proper assembly of the extracellular matrix.\n\nCollagen scaffolds are used in tissue regeneration, whether in sponges, thin sheets, or gels. Collagen has the correct properties for tissue regeneration such as pore structure, permeability, hydrophilicity and it is stable in vivo. Collagen scaffolds are also ideal for the deposition of cells, such as osteoblasts and fibroblasts and once inserted, growth is able to continue as normal in the tissue.\n\nCollagens are widely employed in the construction of the artificial skin substitutes used in the management of severe burns and wounds. These collagens may be derived from bovine, equine, porcine, or even human sources; and are sometimes used in combination with silicones, glycosaminoglycans, fibroblasts, growth factors and other substances.\n\nCollagen is one of the body’s key natural resources and a component of skin tissue that can benefit all stages of the wound healing process. When collagen is made available to the wound bed, closure can occur. Wound deterioration, followed sometimes by procedures such as amputation, can thus be avoided.\n\nCollagen is a natural product, therefore it is used as a natural wound dressing and has properties that artificial wound dressings do not have. It is resistant against bacteria, which is of vital importance in a wound dressing. It helps to keep the wound sterile, because of its natural ability to fight infection. When collagen is used as a burn dressing, healthy granulation tissue is able to form very quickly over the burn, helping it to heal rapidly.\n\nThroughout the 4 phases of wound healing, collagen performs the following functions in wound healing:\n\nWhen hydrolyzed, collagen is reduced to small peptides which can be ingested in the form of dietary supplement or functional foods and beverages with intent to aid joint and bone health and enhance skin health. Hydrolyzed collagen has a much smaller molecular weight in comparison to native collagen or gelatin, study suggests that more than 90% of hydrolyzed collagen is digested and available as small peptides in the blood stream within one hour. From the blood the peptides (containing hydroxyproline) are transported into the target tissues, e.g. skin, bones and cartilage, where the peptides act as building blocks for local cells and help boost the production of new collagen fibers. \n\nCollagen is used in laboratory studies for cell culture, studying cell behavior and cellular interactions with the extracellular environment.\n\nSome studies have shown efficacy of collagen supplementation for dogs with osteoarthritis pain, alone or in combination with other nutraceuticals like glucosamine and chondroitin.\n\nThe collagen protein is composed of a triple helix, which generally consists of two identical chains (α1) and an additional chain that differs slightly in its chemical composition (α2). The amino acid composition of collagen is atypical for proteins, particularly with respect to its high hydroxyproline content. The most common motifs in the amino acid sequence of collagen are glycine-proline-X and glycine-X-hydroxyproline, where X is any amino acid other than glycine, proline or hydroxyproline. The average amino acid composition for fish and mammal skin is given.\n\nFirst, a three-dimensional stranded structure is assembled, with the amino acids glycine and proline as its principal components. This is not yet collagen but its precursor, procollagen. Procollagen is then modified by the addition of hydroxyl groups to the amino acids proline and lysine. This step is important for later glycosylation and the formation of the triple helix structure of collagen. Because the hydroxylase enzymes that perform these reactions require vitamin C as a cofactor, a long-term deficiency in this vitamin results in impaired collagen synthesis and scurvy. These hydroxylation reactions are catalyzed by two different enzymes: prolyl-4-hydroxylase and lysyl-hydroxylase. Vitamin C also serves with them in inducing these reactions. In this service, one molecule of vitamin C is destroyed for each H replaced by OH.\n\nThe synthesis of collagen occurs inside and outside of the cell. The formation of collagen which results in fibrillary collagen (most common form) is discussed here. Meshwork collagen, which is often involved in the formation of filtration systems, is the other form of collagen. All types of collagens are triple helices, and the differences lie in the make-up of the alpha peptides created in step 2.\n\nCollagen has an unusual amino acid composition and sequence:\n\nCortisol stimulates degradation of (skin) collagen into amino acids.\n\nMost collagen forms in a similar manner, but the following process is typical for type I:\n\n\nVitamin C deficiency causes scurvy, a serious and painful disease in which defective collagen prevents the formation of strong connective tissue. Gums deteriorate and bleed, with loss of teeth; skin discolors, and wounds do not heal. Prior to the 18th century, this condition was notorious among long-duration military, particularly naval, expeditions during which participants were deprived of foods containing vitamin C.\n\nAn autoimmune disease such as lupus erythematosus or rheumatoid arthritis may attack healthy collagen fibers.\n\nMany bacteria and viruses secrete virulence factors, such as the enzyme collagenase, which destroys collagen or interferes with its production.\n\nA single collagen molecule, tropocollagen, is used to make up larger collagen aggregates, such as fibrils. It is approximately 300 nm long and 1.5 nm in diameter, and it is made up of three polypeptide strands (called alpha peptides, see step 2), each of which has the conformation of a left-handed helix – this should not be confused with the right-handed alpha helix. These three left-handed helices are twisted together into a right-handed triple helix or \"super helix\", a cooperative quaternary structure stabilized by many hydrogen bonds. With type I collagen and possibly all fibrillar collagens, if not all collagens, each triple-helix associates into a right-handed super-super-coil referred to as the collagen microfibril. Each microfibril is interdigitated with its neighboring microfibrils to a degree that might suggest they are individually unstable, although within collagen fibrils, they are so well ordered as to be crystalline.\n\nA distinctive feature of collagen is the regular arrangement of amino acids in each of the three chains of these collagen subunits. The sequence often follows the pattern Gly-Pro-X or Gly-X-Hyp, where X may be any of various other amino acid residues. Proline or hydroxyproline constitute about 1/6 of the total sequence. With glycine accounting for the 1/3 of the sequence, this means approximately half of the collagen sequence is not glycine, proline or hydroxyproline, a fact often missed due to the distraction of the unusual GXX character of collagen alpha-peptides. The high glycine content of collagen is important with respect to stabilization of the collagen helix as this allows the very close association of the collagen fibers within the molecule, facilitating hydrogen bonding and the formation of intermolecular cross-links. This kind of regular repetition and high glycine content is found in only a few other fibrous proteins, such as silk fibroin.\n\nCollagen is not only a structural protein. Due to its key role in the determination of cell phenotype, cell adhesion, tissue regulation and infrastructure, many sections of its non-proline-rich regions have cell or matrix association / regulation roles. The relatively high content of proline and hydroxyproline rings, with their geometrically constrained carboxyl and (secondary) amino groups, along with the rich abundance of glycine, accounts for the tendency of the individual polypeptide strands to form left-handed helices spontaneously, without any intrachain hydrogen bonding.\n\nBecause glycine is the smallest amino acid with no side chain, it plays a unique role in fibrous structural proteins. In collagen, Gly is required at every third position because the assembly of the triple helix puts this residue at the interior (axis) of the helix, where there is no space for a larger side group than glycine’s single hydrogen atom. For the same reason, the rings of the Pro and Hyp must point outward. These two amino acids help stabilize the triple helix—Hyp even more so than Pro; a lower concentration of them is required in animals such as fish, whose body temperatures are lower than most warm-blooded animals. Lower proline and hydroxyproline contents are characteristic of cold-water, but not warm-water fish; the latter tend to have similar proline and hydroxyproline contents to mammals. The lower proline and hydroxproline contents of cold-water fish and other poikilotherm animals leads to their collagen having a lower thermal stability than mammalian collagen. This lower thermal stability means that gelatin derived from fish collagen is not suitable for many food and industrial applications.\n\nThe tropocollagen subunits spontaneously self-assemble, with regularly staggered ends, into even larger arrays in the extracellular spaces of tissues. Additional assembly of fibrils is guided by fibroblasts, which deposit fully formed fibrils from fibripositors. In the fibrillar collagens, molecules are staggered to adjacent molecules by about 67 nm (a unit that is referred to as ‘D’ and changes depending upon the hydration state of the aggregate). In each D-period repeat of the microfibril, there is a part containing five molecules in cross-section, called the “overlap”, and a part containing only four molecules, called the \"gap\". These overlap and gap regions are retained as microfibrils assemble into fibrils, and are thus viewable using electron microscopy. The triple helical tropocollagens in the microfibrils are arranged in a quasihexagonal packing pattern.\n\nThere is some covalent crosslinking within the triple helices, and a variable amount of covalent crosslinking between tropocollagen helices forming well organized aggregates (such as fibrils). Larger fibrillar bundles are formed with the aid of several different classes of proteins (including different collagen types), glycoproteins and proteoglycans to form the different types of mature tissues from alternate combinations of the same key players. Collagen's insolubility was a barrier to the study of monomeric collagen until it was found that tropocollagen from young animals can be extracted because it is not yet fully crosslinked. However, advances in microscopy techniques (i.e. electron microscopy (EM) and atomic force microscopy (AFM)) and X-ray diffraction have enabled researchers to obtain increasingly detailed images of collagen structure \"in situ\". These later advances are particularly important to better understanding the way in which collagen structure affects cell–cell and cell–matrix communication, and how tissues are constructed in growth and repair, and changed in development and disease. For example, using AFM–based nanoindentation it has been shown that a single collagen fibril is a heterogeneous material along its axial direction with significantly different mechanical properties in its gap and overlap regions, correlating with its different molecular organizations in these two regions.\n\nCollagen fibrils/aggregates are arranged in different combinations and concentrations in various tissues to provide varying tissue properties. In bone, entire collagen triple helices lie in a parallel, staggered array. 40 nm gaps between the ends of the tropocollagen subunits (approximately equal to the gap region) probably serve as nucleation sites for the deposition of long, hard, fine crystals of the mineral component, which is (approximately) Ca(OH)(PO). Type I collagen gives bone its tensile strength.\n\nCollagen-related diseases most commonly arise from genetic defects or nutritional deficiencies that affect the biosynthesis, assembly, postranslational modification, secretion, or other processes involved in normal collagen production.\n\nIn addition to the above-mentioned disorders, excessive deposition of collagen occurs in scleroderma.\n\nOne thousand mutations have been identified in 12 out of more than 20 types of collagen. These mutations can lead to various diseases at the tissue level.\n\nOsteogenesis imperfecta – Caused by a mutation in type 1 collagen, dominant autosomal disorder, results in weak bones and irregular connective tissue, some cases can be mild while others can be lethal. Mild cases have lowered levels of collagen type 1 while severe cases have structural defects in collagen.\n\nChondrodysplasias – Skeletal disorder believed to be caused by a mutation in type 2 collagen, further research is being conducted to confirm this.\n\nEhlers-Danlos syndrome – Six different types of this disorder, which lead to deformities in connective tissue, are known. Some types can be lethal, leading to the rupture of arteries. Each syndrome is caused by a different mutation, for example type four of this disorder is caused by a mutation in collagen type 3.\n\nAlport syndrome – Can be passed on genetically, usually as X-linked dominant, but also as both an autosomal dominant and autosomal recessive disorder, sufferers have problems with their kidneys and eyes, loss of hearing can also develop in during the childhood or adolescent years.\n\nOsteoporosis – Not inherited genetically, brought on with age, associated with reduced levels of collagen in the skin and bones, growth hormone injections are being researched as a possible treatment to counteract any loss of collagen.\n\nKnobloch syndrome – Caused by a mutation in the COL18A1 gene that codes for the production of collagen XVIII. Patients present with protrusion of the brain tissue and degeneration of the retina, an individual who has family members suffering from the disorder is at an increased risk of developing it themselves as there is a hereditary link.\n\nCollagen is one of the long, fibrous structural proteins whose functions are quite different from those of globular proteins, such as enzymes. Tough bundles of collagen called \"collagen fibers\" are a major component of the extracellular matrix that supports most tissues and gives cells structure from the outside, but collagen is also found inside certain cells. Collagen has great tensile strength, and is the main component of fascia, cartilage, ligaments, tendons, bone and skin. Along with elastin and soft keratin, it is responsible for skin strength and elasticity, and its degradation leads to wrinkles that accompany aging. It strengthens blood vessels and plays a role in tissue development. It is present in the cornea and lens of the eye in crystalline form. It may be one of the most abundant proteins in the fossil record, given that it appears to fossilize frequently, even in bones from the Mesozoic and Paleozoic.\n\nCollagen has a wide variety of applications, from food to medical. For instance, it is used in cosmetic surgery and burn surgery. It is widely used in the form of collagen casings for sausages, which are also used in the manufacture of musical strings.\n\nIf collagen is subject to sufficient denaturation, e.g. by heating, the three tropocollagen strands separate partially or completely into globular domains, containing a different secondary structure to the normal collagen polyproline II (PPII), e.g. random coils. This process describes the formation of gelatin, which is used in many foods, including flavored gelatin desserts. Besides food, gelatin has been used in pharmaceutical, cosmetic, and photography industries.\n\nFrom the Greek for glue, \"kolla\", the word collagen means \"glue producer\" and refers to the early process of boiling the skin and sinews of horses and other animals to obtain glue. Collagen adhesive was used by Egyptians about 4,000 years ago, and Native Americans used it in bows about 1,500 years ago. The oldest glue in the world, carbon-dated as more than 8,000 years old, was found to be collagen—used as a protective lining on rope baskets and embroidered fabrics, and to hold utensils together; also in crisscross decorations on human skulls. Collagen normally converts to gelatin, but survived due to dry conditions. Animal glues are thermoplastic, softening again upon reheating, so they are still used in making musical instruments such as fine violins and guitars, which may have to be reopened for repairs—an application incompatible with tough, synthetic plastic adhesives, which are permanent. Animal sinews and skins, including leather, have been used to make useful articles for millennia.\n\nGelatin-resorcinol-formaldehyde glue (and with formaldehyde replaced by less-toxic pentanedial and ethanedial) has been used to repair experimental incisions in rabbit lungs.\n\nThe molecular and packing structures of collagen have eluded scientists over decades of research. The first evidence that it possesses a regular structure at the molecular level was presented in the mid-1930s. Since that time, many prominent scholars, including Nobel laureates Crick, Pauling, Rich and Yonath, and others, including Brodsky, Berman, and Ramachandran, concentrated on the conformation of the collagen monomer. Several competing models, although correctly dealing with the conformation of each individual peptide chain, gave way to the triple-helical \"Madras\" model of Ramachandran, which provided an essentially correct model of the molecule's quaternary structure although this model still required some refinement. The packing structure of collagen has not been defined to the same degree outside of the fibrillar collagen types, although it has been long known to be hexagonal or quasi-hexagonal. As with its monomeric structure, several conflicting models alleged that either the packing arrangement of collagen molecules is 'sheet-like' or microfibrillar. The microfibrillar structure of collagen fibrils in tendon, cornea and cartilage has been directly imaged by electron microscopy. The microfibrillar structure of tail tendon, as described by Fraser, Miller, and Wess (amongst others), was modeled as being closest to the observed structure, although it oversimplified the topological progression of neighboring collagen molecules, and hence did not predict the correct conformation of the discontinuous D-periodic pentameric arrangement termed simply: the microfibril. Various cross linking agents like L-Dopaquinone, embeline, potassium embelate and 5-O-methyl embelin could be developed as potential\ncross-linking/stabilization agents of collagen preparation and its application as wound dressing sheet in clinical applications is enhanced.\n\nThe evolution of collagens was a fundamental step in the early evolution of life, supporting the coalescence of multicellular life forms.\n\n\n"}
{"id": "6059", "url": "https://en.wikipedia.org/wiki?curid=6059", "title": "Calvin and Hobbes", "text": "Calvin and Hobbes\n\nCalvin and Hobbes is a daily comic strip by American cartoonist Bill Watterson that was syndicated from November 18, 1985 to December 31, 1995. Commonly cited as \"the last great newspaper comic\", \"Calvin and Hobbes\" has evinced broad and enduring popularity, influence and academic interest.\n\n\"Calvin and Hobbes\" follows the humorous antics of Calvin, a precocious, mischievous and adventurous six-year-old boy, and Hobbes, his sardonic stuffed tiger. Set in the contemporary suburban United States, the strip depicts Calvin's frequent flights of fancy and his friendship with Hobbes. It also examines Calvin's relationships with family and classmates, especially the love/hate relationship between him and his classmate, Susie Derkins. Hobbes' dual nature is a defining motif for the strip: to Calvin, Hobbes is a living anthropomorphic tiger; all the other characters see Hobbes as an inanimate stuffed toy. Though the series does not mention specific political figures or current events, it does explore broad issues like environmentalism, public education, philosophical quandaries, and the flaws of opinion polls.\n\nAt the height of its popularity, \"Calvin and Hobbes\" was featured in over 2,400 newspapers worldwide. In 2010, reruns of the strip appeared in more than 50 countries, and nearly 45 million copies of the \"Calvin and Hobbes\" books had been sold.\n\n\"Calvin and Hobbes\" was conceived when Bill Watterson, who was working in an advertising job he detested, began devoting his spare time to cartooning. He explored various strip ideas but all were rejected by the syndicates. United Feature Syndicate finally responded positively to one strip called \"Critturs\", which featured a side character (the main character's little brother) who had a stuffed tiger. Watterson began a new strip centered on them when he was told that these characters were the strongest. Though United Feature rejected the new strip, Universal Press Syndicate eventually took it.\n\nThe first strip was published on November 18, 1985 and the series quickly became a hit. Within a year of syndication, the strip was published in roughly 250 newspapers. Before long the strip was in wide circulation outside the United States. By April 5, 1987, Watterson was featured in an article in \"The Los Angeles Times\". \"Calvin and Hobbes\" earned Watterson the Reuben Award from the National Cartoonists Society in the Outstanding Cartoonist of the Year category, first in 1986 and again in 1988. He was nominated another time in 1992. The Society awarded him the Humor Comic Strip Award for 1988. Calvin and Hobbes has also won several more awards.\n\nWatterson took two extended breaks from writing new strips, from May 5, 1991, to February 1, 1992, and from April 3 through December 31, 1994.\nIn 1995, Watterson sent a letter via his syndicate to all editors whose newspapers carried his strip:\n\nThe final strip ran on Sunday, December 31, 1995. It depicted Calvin and Hobbes outside in freshly fallen snow, reveling in the wonder and excitement of the winter scene. \"It's a magical world, Hobbes, ol' buddy... Let's go exploring!\" Calvin exclaims as they zoom off over the snowy hills on their sled, leaving, according to one critic ten years later, \"a hole in the comics page that no strip has been able to fill.\"\n\nFrom the outset, Watterson found himself at odds with the syndicate, which urged him to begin merchandising the characters and touring the country to promote the first collections of comic strips. Watterson refused. To him, the integrity of the strip and its artist would be undermined by commercialization, which he saw as a major negative influence in the world of cartoon art.\n\nWatterson also grew increasingly frustrated by the gradual shrinking of available space for comics in the newspapers. He lamented that without space for anything more than simple dialogue or sparse artwork, comics as an art form were becoming dilute, bland, and unoriginal. Watterson strove for a full-page version of his strip, in contrast to the few cells allocated for most strips. He longed for the artistic freedom allotted to classic strips such as \"Little Nemo\" and \"Krazy Kat\", and he gave a sample of what could be accomplished with such liberty in the opening pages of the Sunday strip compilation, \"The Calvin and Hobbes Lazy Sunday Book\".\n\nDuring Watterson's first sabbatical from the strip, Universal Press Syndicate continued to charge newspapers full price to re-run old \"Calvin and Hobbes\" strips. Few editors approved of the move, but the strip was so popular that they had no choice but to continue to run it for fear that competing newspapers might pick it up and draw its fans away.\nUpon Watterson's return, Universal Press announced that Watterson had decided to sell his Sunday strip as an unbreakable half of a newspaper or tabloid page. Many editors and even a few cartoonists criticized him for what they perceived as arrogance and an unwillingness to abide by the normal practices of the cartoon business. Watterson had negotiated the deal to allow himself more creative freedom in the Sunday comics:\n\nI took a sabbatical after resolving a long and emotionally draining fight to prevent \"Calvin and Hobbes\" from being merchandised. Looking for a way to rekindle my enthusiasm for the duration of a new contract term, I proposed a redesigned Sunday format that would permit more panel flexibility. To my surprise and delight, Universal responded with an offer to market the strip as an unbreakable half page (more space than I'd dared to ask for), despite the expected resistance of editors.\n\nTo this day, my syndicate assures me that some editors liked the new format, appreciated the difference, and were happy to run the larger strip, but I think it's fair to say that this was not the most common reaction. The syndicate had warned me to prepare for numerous cancellations of the Sunday feature, but after a few weeks of dealing with howling, purple-faced editors, the syndicate suggested that papers could reduce the strip to the size tabloid newspapers used for their smaller sheets of paper. ... I focused on the bright side: I had complete freedom of design and there were virtually no cancellations.\n\nFor all the yelling and screaming by outraged editors, I remain convinced that the larger Sunday strip gave newspapers a better product and made the comics section more fun for readers. Comics are a visual medium. A strip with a lot of drawing can be exciting and add some variety. Proud as I am that I was able to draw a larger strip, I don't expect to see it happen again any time soon. In the newspaper business, space is money, and I suspect most editors would still say that the difference is not worth the cost. Sadly, the situation is a vicious circle: because there's no room for better artwork, the comics are simply drawn; because they're simply drawn, why should they have more room?\n\nWatterson did consider allowing Calvin and Hobbes to be animated, and has expressed admiration for the art form of animation. In a 1989 interview in \"The Comics Journal\" he said:\n\nAfter this he was asked if it was \"a bit scary to think of hearing Calvin's voice.\" He responded that it was \"very scary,\" and that although he loved the visual possibilities of animation, the thought of casting voice actors to play his characters was uncomfortable. He was also unsure whether he wanted to work with an animation team as he had done all previous work by himself. Ultimately, \"Calvin and Hobbes\" was never made into an animated series. Watterson later stated in \"The Calvin and Hobbes Tenth Anniversary Book\" that he liked the fact that his strip was a \"low-tech, one-man operation\", and took great pride in the fact that he drew every line and wrote every word on his own.\n\nBill Watterson insists that cartoon strips should stand on their own as an art form and has resisted the use of \"Calvin and Hobbes\" in merchandising of any sort. Watterson explained in a 2005 press release:\n\nAlmost no legitimate \"Calvin and Hobbes\" merchandise exists outside the book collections. Exceptions produced during the strip's original run include two 16-month calendars (1988–1989 and 1989–1990), and the textbook \"Teaching with Calvin and Hobbes\", which has been described as \"perhaps the most difficult piece of official \"Calvin and Hobbes\" memorabilia to find.\"\n\nOn July 16, 2010 the United States Postal Service released a set of postage stamps honoring five comic strips, one of them \"Calvin and Hobbes\".\n\nUclick, the digital division of Andrews McMeel Universal, offers licensed prints of \"Calvin and Hobbes\" strips through its website.\n\nThe strip's immense popularity has led to the appearance of various counterfeit items such as window decals and T-shirts that often feature crude humor, binge drinking and other themes that are not found in Watterson's work. Images from one strip in which Calvin and Hobbes dance to loud music at night were commonly used for copyright violations. After threat of a lawsuit alleging infringement of copyright and trademark, some sticker makers replaced Calvin with a different boy, while other makers made no changes. Watterson wryly commented, \"I clearly miscalculated how popular it would be to show Calvin urinating on a Ford logo.\"\n\nThe strip borrows several elements and themes from three major influences: Walt Kelly's \"Pogo\", George Herriman's \"Krazy Kat\", and Charles M. Schulz's \"Peanuts\". Schulz and Kelly particularly influenced Watterson's outlook on comics during his formative years.\n\nNotable elements of Watterson's artistic style are his characters' diverse and often exaggerated expressions (particularly those of Calvin), elaborate and bizarre backgrounds for Calvin's flights of imagination, expressions of motion, and frequent visual jokes and metaphors. In the later years of the strip, with more panel space available for his use, Watterson experimented more freely with different panel layouts, art styles, stories without dialogue, and greater use of whitespace. He also makes a point of not showing certain things explicitly: the \"Noodle Incident\" and the children's book \"Hamster Huey and the Gooey Kablooie\" are left to the reader's imagination, where Watterson was sure they would be \"more outrageous\" than he could portray.\n\nWatterson's technique started with minimalist pencil sketches drawn with a light pencil (though the larger Sunday strips often required more elaborate work) on a piece of Bristol board, with his brand of choice being Strathmore because he felt it held the drawings better on the page as opposed to the cheaper brands (Watterson said he would use any cheap pad of Bristol board his local supply store had, but switched to Strathmore after he found himself growing more and more displeased with the results). He would then use a small sable brush and India ink to fill in the rest of the drawing, saying that he did not want to simply trace over his penciling and thus make the inking more spontaneous. He lettered dialogue with a Rapidograph fountain pen, and he used a crowquill pen for odds and ends. Mistakes were covered with various forms of correction fluid, including the type used on typewriters. Watterson was careful in his use of color, often spending a great deal of time in choosing the right colors to employ for the weekly Sunday strip; his technique was to cut the color tabs the syndicate sent him into individual squares, lay out the colors, and then paint a watercolor approximation of the strip on tracing paper over the Bristol board and then mark the strip accordingly before sending it on. When \"Calvin and Hobbes\" began there were 64 colors available for the Sunday strips. For the later Sunday strips Watterson had 125 colors as well as the ability to fade the colors into each other.\n\nWatterson used the strip to poke fun at the art world, principally through Calvin's unconventional creations of snowmen but also through other expressions of childhood art. When Miss Wormwood complains that he is wasting class time drawing impossible things (a \"Stegosaurus\" in a rocket ship, for example), Calvin proclaims himself \"on the cutting edge of the \"avant-garde\".\" He begins exploring the medium of snow when a warm day melts his snowman. His next sculpture \"speaks to the horror of our own mortality, inviting the viewer to contemplate the evanescence of life.\" In later strips, Calvin's creative instincts diversify to include sidewalk drawings (or, as he terms them, examples of \"suburban postmodernism\").\n\nWatterson also lampooned the academic world. In one example, Calvin carefully crafts an \"artist's statement\", claiming that such essays convey more messages than artworks themselves ever do (Hobbes blandly notes, \"You misspelled \"Weltanschauung\"\"). He indulges in what Watterson calls \"pop psychobabble\" to justify his destructive rampages and shift blame to his parents, citing \"toxic codependency.\" In one instance, he pens a book report based on the theory that the purpose of academic writing is to \"inflate weak ideas, obscure poor reasoning, and inhibit clarity,\" entitled \"The Dynamics of Interbeing and Monological Imperatives in \"Dick and Jane:\" A Study in Psychic Transrelational Gender Modes\". Displaying his creation to Hobbes, he remarks, \"Academia, here I come!\" Watterson explains that he adapted this jargon (and similar examples from several other strips) from an actual book of art criticism.\n\nOverall, Watterson's satirical essays serve to attack both sides, criticizing both the commercial mainstream and the artists who are supposed to be \"outside\" it. Not long after he began drawing his \"Dinosaurs in Rocket Ships\" series, Calvin tells Hobbes:\n\nThe strip for Sunday, June 21, 1992, criticized the naming of The Big Bang theory as not evocative of the wonders behind it, and coined the term \"Horrendous Space Kablooie\", an alternative that achieved some informal popularity among scientists and was often shortened to \"the HSK.\" The term has also been referred to in newspapers, books, and university courses.\n\nCalvin, named after the 16th-century theologian John Calvin, is a six-year-old, whose last name is never mentioned in the strip. Despite his poor grades in school, Calvin demonstrates his intelligence through his sophisticated vocabulary and a philosophical mind:\n\nHe commonly wears his distinctive red-and-black striped shirt, black pants, and white-and-magenta sneakers. One of his most noticeable features is his spiky yellow hair (in early predecessors to the strip, Calvin is drawn with 'bangs', or hair fringe, that cover his eyes). He also wears a light blue jacket when going to school or when playing in the snow. He is an enthusiastic reader of comic books and has a tendency to order items marketed in comic books or on boxes of his favorite cereal, Chocolate Frosted Sugar Bombs (which is hinted as having far too many calories, made worse by the fact that Calvin frequently pours large amounts of additional sugar onto the cereal). Watterson described Calvin:\n\nCalvin also has a sensitive side as well. This is displayed, for example, when he finds a dying raccoon and tries to save it but fails. The scene is made even more poignant by Calvin asking Hobbes not to \"go anywhere\" while Hobbes hugs him and promises him he will not. In another story, Calvin's house is broken into and he frantically looks for Hobbes, even crying in joy as he is reunited with his friend.\n\nFrom Calvin's point of view, Hobbes is an anthropomorphic tiger, much larger than Calvin and full of independent attitudes and ideas. When the perspective shifts to any other character, readers again see merely a stuffed animal, usually seated at an off-kilter angle and blankly staring into space. Watterson explains:\n\nHobbes is named after the 17th-century philosopher Thomas Hobbes, who held what Watterson describes as \"a dim view of human nature.\" He feels that animals (and tigers in particular) are superior to humans, showing disdain for human nature, mannerisms, and how mankind destroys their environment. Hobbes is much more rational and aware of consequences than Calvin, but seldom interferes with Calvin's troublemaking beyond a few oblique warnings. Hobbes is sarcastic when Calvin is being hypocritical about things he dislikes.\n\nAlthough the debut strip shows Calvin capturing Hobbes by means of a snare (with a tuna sandwich as the bait), a later comic (August 1, 1989) indicates that Hobbes has been with Calvin since Calvin was a baby:\n\nAnother later strip featured Hobbes humorously claiming that Calvin's mother \"wanted another tiger\" instead of Calvin, indicating that Hobbes was around before Calvin was born. Watterson eventually decided that it was not important to establish how Calvin and Hobbes met.\n\nCalvin's unnamed mother and father are typical middle-class parents. Calvin's father is a patent attorney (like Watterson's own father) and his mother is a stay-at-home mom. Both remain unnamed except as \"Mom\" and \"Dad,\" or pet names such as \"honey\" and \"dear\" between themselves. Watterson says, \"As far as the strip is concerned, they are important only as Calvin's mom and dad.\" Like many other characters in the strip, they are relatively down to earth and their sensible attitudes serve as a foil for Calvin's outlandish behavior.\n\nWatterson says some fans were angered by the way Calvin's parents thought of Calvin. This is shown when Calvin's father claimed that he wanted a dachshund instead of Calvin, and often tries to \"deny\" that Calvin is his biological son:\n\nCalvin's parents are not above some outrageous behavior of their own. For example, Calvin asks for a cigarette and his mother gives him one to teach him a lesson. Calvin's father tells Calvin sarcastic lies when asked a straight question, and Calvin often believes them:\nWatterson defends what Calvin's parents do, remarking that in the case of parenting a kid like Calvin, \"I think they do a better job than I would.\" Calvin's father is overly concerned with \"character building\" activities in a number of strips, either in the things he makes Calvin do or in the masochistic eccentricities of his own lifestyle. For example, Calvin's father is shown coming home from an early morning run in the snow, which he follows with a bowl of plain oatmeal.\n\nCalvin's father has a brother named Max, who lives out of state. Watterson introduced him in a week of strips but said he later regretted the idea. Although Watterson said the idea of having Max in the strip was to set up potential future storylines, he realized that his intent to have the parents remain nameless worked against him because Max could not refer to either one by name. This was cited as one of the main reasons Max never reappeared.\n\nSusie Derkins, the only important character with both a first and last name, is a classmate of Calvin's who lives on his street. Getting her last name from the pet beagle of Watterson's wife's family, she appeared early in the strip as a new student in Calvin's class. She is polite and studious, and likes to play house or host tea parties with her stuffed animals. However, she is also depicted playing imaginary games with Calvin in which she is a high-powered lawyer or politician and he is her househusband. Though both of them hate to admit it, Calvin and Susie have quite a bit in common. For example, Susie is shown on occasion with a stuffed bunny rabbit named \"Mr. Bun.\" Susie also has a mischievous (and sometimes aggressive) streak, which can be seen when she subverts Calvin's attempts to cheat on school tests by feeding him incorrect answers, or clobbers Calvin when he attacks her with snowballs. Susie also regularly bests Calvin in confrontations such as their water balloon and snowball fights, employing guile or force. Hobbes often openly expresses romantic feelings for Susie, much to Calvin's disgust. Calvin starts a \"club\" (of which he and Hobbes are the only members) that he calls G.R.O.S.S. (Get Rid Of Slimy GirlS), and while holding \"meetings\" in Calvin's treehouse or in the \"box of secrecy\" in Calvin's room, they usually come up with some way to annoy or socially maim Susie, most of which backfire on them completely. In one instance, Calvin steals one of Susie's dolls for ransom, only to have Susie retaliate by nabbing Hobbes. Watterson admits that Calvin and Susie have a nascent crush on each other, and that Susie is inspired by the type of woman whom Watterson himself found attractive and eventually married.\n\nCalvin also interacts with a handful of secondary characters. These include his babysitter, the school bully, his school teacher and the school principal.\n\nRosalyn is Calvin's babysitter. She takes advantage of his parents' desperation to leave the house and the fact that no one else will babysit for Calvin by demanding advances and raises. She is also probably the only character in the strip whom Calvin really fears, as she does not mince words or actions to get Calvin to behave or go to bed on time. Watterson put her in a Sunday strip early on, never thinking of her as a regular character. But Rosalyn's intimidation of Calvin surprised Watterson, so she came back several times. At one point she was Calvin's swimming instructor, though he was shown to attend only one lesson. In Rosalyn's last story, she is revealed to have in common with Calvin a sense of imagination, and the two of them play Calvinball with Hobbes. Rosalyn is the only one without a role in Spaceman Spiff.\n\nAccording to Calvin, Moe is a \"six-year-old who shaves\" and a stereotypical bully who picks on Calvin (both physically and emotionally) who calls him names. Moe is the only regular character whose speech is shown in an unusual font as his frequently monosyllabic dialogue is shown in crude, lower-case letters. Watterson describes Moe as \"every jerk I've ever known.\"\n\nMiss Wormwood is Calvin's world-weary teacher and is named after the junior devil in C. S. Lewis's \"The Screwtape Letters\". She usually wears either a polka-dotted dress or a brown dress, and is another character who serves as a foil to Calvin's mischief. Calvin, when in his Spaceman Spiff persona, sees Miss Wormwood as a slimy, often dictatorial alien. Calvin refers to Miss Wormwood's indigestion (\"It's really gross how she drinks Maalox straight from the bottle\"), her medication (\"I wonder if her doctor knows she mixes all those prescriptions\") and her smoking habit (\"Rumor has it she's up to two packs a day, unfiltered\"). Miss Wormwood reacts to Calvin's behavior by tightly shutting her eyes and thinking \"Five years until retirement\" repeatedly. Watterson describes her as \"an unhappy person\" due to her belief in the value of education.\n\nMr. Spittle is the school principal to whose office Miss Wormwood threatens to send Calvin for his pranks. Susie also occasionally accompanies Calvin to the principal's office. Though his name has been shown in only one story, he has appeared many times including the first story about Calvin's duplicator.\n\nUncle Max is Calvin's paternal uncle. Uncle Max was originally meant as a character who could increase the possibilities of the strip: as Watterson noted, \"Calvin could go visit Uncle Max...\". However, Watterson dropped the character after finding it was awkward that Max could not address Calvin's parents by name, and realizing that Max was redundant as far as the strip's personality goes.\n\nThere are many recurring gags in the strip, some in \"reality\" and others in Calvin's imagination. These are as follows:\n\nCalvin imagines himself as a great many things including dinosaurs, elephants, jungle-farers and superheroes. Three of his alter egos are well defined and recurrent:\n\n\nCalvin has had several adventures involving corrugated cardboard boxes, which he adapts for many different uses. In one strip, when Calvin shows off his Transmogrifier, a device that transforms its user into any desired shape, Hobbes remarks, \"It's amazing what they do with corrugated cardboard these days.\" Calvin is able to change the function of the boxes by rewriting the label and flipping the box onto another side. In this way, a cardboard box can be used not only for its conventional purposes (a storage container for water balloons, for example), but also as a flying time machine, a duplicator or, with the attachment of a few wires and a colander, a \"Cerebral Enhance-o-tron.\"\n\nIn the real world, Calvin's antics with his box have had varying effects. When he transmogrified into a tiger, he still appeared as a regular human child to his parents. However, in a story where he made several duplicates of himself, his parents are seen interacting with what does seem like multiple Calvins, including in a strip where two of him are seen in the same panel as his father. It is ultimately unknown what his parents do or do not see, as Calvin tries to hide most of his creations (or conceal their effects) as not to traumatize his parents.\n\nIn addition, Calvin uses a cardboard box as a desk when he is attempting to sell things. Often, Calvin's merchandise is something that no one would want, such as \"suicide drink\", \"a swift kick in the butt\" for one dollar, or a \"frank appraisal of your looks\" for fifty cents. In one strip, he sells \"happiness\" for ten cents; if one bought it, Calvin hit the person in the face with a water balloon, then revealed that he meant his own happiness. In another strip, he sold \"insurance\", firing a slingshot at those who refused to buy it. In some strips, he tried to sell \"great ideas\", and in one earlier strip, he attempted to sell the family car to obtain money for a grenade launcher. In yet another strip, he sells \"life\" for five cents, where the customer receives nothing in return, which, in Calvin's opinion, is life.\n\nThe box has also functioned as a secret meeting place for G.R.O.S.S., as the \"Box of Secrecy\".\n\nCalvinball is a game played by Calvin and Hobbes as a rebellion against organized team sports; according to Hobbes, \"No sport is less organized than Calvinball!\" Calvinball was first introduced to the readers at the end of a 1990 storyline involving Calvin reluctantly joining recess baseball. It quickly became a staple of the comic afterwards.\n\nThe only hint at the true creation of the game ironically comes from the last Calvinball strip, in which a game of football quickly devolves into a game of Calvinball. Calvin remarks that \"sooner or later, all our games turn into Calvinball\", suggesting a similar scenario that directly led to the creation of the sport. Calvin and Hobbes usually play by themselves, although in one storyline Rosalyn (Calvin's baby-sitter) plays in return for Calvin doing his homework, and plays very well once she realizes that the rules are made up on the spot.\n\nThe only consistent rule states that Calvinball may never be played with the same rules twice. Scoring is also arbitrary, with Hobbes at times reporting scores of \"Q to 12\" and \"oogy to boogy\". The only recognizable sports Calvinball resembles are the ones it emulates (i.e., a cross between croquet, polo, badminton, capture the flag, and volleyball). Equipment includes a volleyball (the eponymous \"Calvinball\"), a croquet set, a badminton set, assorted flags, bags, signs, a hobby horse, and enigmatic and never-pictured \"time-fracture wickets\". Other things appear as needed, such as a bucket of ice-cold water, a water balloon, and various songs and poetry. Players also wear masks resembling blindfolds with holes for the eyes. When Rosalyn asks Calvin the reason for the requirement, Calvin responds: \"Sorry, no one's allowed to question the masks.\" When asked how to play, Watterson states: \"It's pretty simple: you make up the rules as you go.\" Calvinball is a nomic or self-modifying game, a contest of wits, skill and creativity rather than stamina or athletic skill, in which Hobbes (and on one occasion, Rosalyn) usually outwits Calvin, who takes it in stride, in contrast to his otherwise poor sportsmanship.\n\nCalvin often creates horrendous/dark humor scenes with his snowmen. He uses the snowman for social commentary, revenge, or pure enjoyment. Examples include Snowman Calvin being yelled at by Snowman Dad to shovel the snow; one snowman eating snow cones scooped out of a second snowman, who is lying on the ground with an ice-cream scoop in his back; a snowman house of horror; and snowmen representing the people he hates. \"The ones I \"really\" hate are small, so they'll go faster,\" he says. There was even an occasion on which Calvin accidentally brought a snowman to life and it made itself and a small army into \"deranged mutant killer monster snow goons.\"\n\nCalvin's snow art is often used as a commentary on art in general. For example, Calvin has complained more than once about the lack of originality in other people's snow art and compared it with his own grotesque snow sculptures. In one of these instances, Calvin and Hobbes claim to be the sole guardians of high culture; in another, Hobbes admires Calvin's willingness to put artistic integrity above marketability, causing Calvin to reconsider and make an ordinary snowman.\n\nCalvin and Hobbes frequently ride downhill in a wagon, sled, or toboggan, depending on the season, as a device to add some physical comedy to the strip and because, according to Watterson, \"it's a lot more interesting ... than talking heads.\" While the ride is sometimes the focus of the strip, it also frequently serves as a counterpoint or visual metaphor while Calvin ponders the meaning of life, death, God, philosophy or a variety of other weighty subjects. Many of their rides end in spectacular crashes which leave them battered and broken, a fact which convinces Hobbes to sometimes hop off before a ride even begins. In the final strip, Calvin and Hobbes depart on their toboggan to go exploring. This theme is similar (perhaps even homage) to scenarios in Walt Kelly's \"Pogo\".\n\nG.R.O.S.S., which stands for Get Rid Of Slimy GirlS, is a club which consists of only two members: Calvin and Hobbes. The club was founded in the garage of their house. To clear space for its activities, Calvin and (purportedly) Hobbes push Calvin's parents' car, causing it to roll into a ditch (but not suffer damage); the incident necessitates changing the club's location to Calvin's treehouse. They hold meetings to attempt to annoy Susie Derkins. Notable actions include planting a fake secret tape near her in attempt to draw her in to a trap, trapping her in a closet at their house, and creating elaborate water balloon traps. Calvin gave himself and Hobbes important positions in the club, Calvin being \"Dictator-for-Life\" and Hobbes being \"President-and-First-Tiger\". They go into Calvin's treehouse for their club meetings and often get into fights during them. The password to get into the treehouse is intentionally long and difficult, which has on at least one occasion ruined Calvin's plans. (Because Hobbes can climb the tree without the rope, he got to think up the password, which heaps praise upon tigers.) An example of this can be seen in the comic strip where Calvin, rushing to get into the treehouse to throw things at a passing Susie Derkins, insults Hobbes, who is in the treehouse and thus has to let down the rope. Hobbes forces Calvin to say the password for insulting him. By the time Susie arrives, in time to hear Calvin saying some of the password, causing him to stumble, Calvin is on \"\"Verse Seven:\" Tigers are perfect!/The E-pit-o-me/of good looks and grace/and quiet..uh..um..dignity\". The opportunity to pelt Susie with something having passed, Calvin threatens to turn Hobbes into a rug. G.R.O.S.S. is one of the most common adventures that Calvin has. The club anthem begins: \"Ohhhh Gross, best club in the cosmos...\"\n\nThere are 18 \"Calvin and Hobbes\" books, published from 1987 to 1997. These include 11 collections, which form a complete archive of the newspaper strips, except for a single daily strip from November 28, 1985. (The collections \"do\" contain a strip for this date, but it is not the same strip that appeared in some newspapers. Treasuries usually combine the two preceding collections with bonus material and include color reprints of Sunday comics.)\n\nWatterson included some new material in the treasuries. In \"The Essential Calvin and Hobbes\", which includes cartoons from the collections \"Calvin and Hobbes\" and \"Something Under the Bed Is Drooling\", the back cover features a scene of a giant Calvin rampaging through a town. The scene is based on Watterson's home town of Chagrin Falls, Ohio, and Calvin is holding the Chagrin Falls Popcorn Shop, an iconic candy and ice cream shop overlooking the town's namesake falls. Several of the treasuries incorporate additional poetry; \"The Indispensable Calvin and Hobbes\" book features a set of poems, ranging from just a few lines to an entire page, that cover topics such as Calvin's mother's \"hindsight\" and exploring the woods. In \"The Essential Calvin and Hobbes\", Watterson presents a long poem explaining a night's battle against a monster from Calvin's perspective.\n\nA complete collection of \"Calvin and Hobbes\" strips, in three hardcover volumes totaling 1440 pages, was released on October 4, 2005, by Andrews McMeel Publishing. It includes color prints of the art used on paperback covers, the treasuries' extra illustrated stories and poems, and a new introduction by Bill Watterson in which he talks about his inspirations and his story leading up to the publication of the strip. The alternate 1985 strip is still omitted, and two other strips (January 7, 1987, and November 25, 1988) have altered dialogue. A four-volume paperback version was released November 13, 2012.\n\nTo celebrate the release (which coincided with the strip's 20th anniversary and the tenth anniversary of its absence from newspapers), Bill Watterson answered 15 questions submitted by readers.\n\nEarly books were printed in smaller format in black and white. These were later reproduced in twos in color in the \"Treasuries\" (\"Essential\", \"Authoritative\", and \"Indispensable\"), except for the contents of \"Attack of the Deranged Mutant Killer Monster Snow Goons\". Those Sunday strips were not reprinted in color until the \"Complete\" collection was finally published in 2005.\n\nWatterson claims he named the books the \"\"Essential\", \"Authoritative\", and \"Indispensable\"\" because, as he says in \"The Calvin and Hobbes Tenth Anniversary Book\", the books are \"obviously none of these things.\"\n\nAn officially licensed children's textbook entitled \"Teaching with Calvin and Hobbes\" was published in a single print run in Fargo, North Dakota, in 1993. The book, which has been \"highly recommend[ed]\" as a teaching resource, includes five complete \"Calvin and Hobbes\" multi-strip story arcs together with lessons and questions to follow, such as, \"What do you think the principal meant when he said they had \"quite a file\" on Calvin?\"\n\nThe book is rare, sought by collectors, and highly valued. Only eight libraries in the world hold a copy of the book.\n\nIn 1993, paleontologist and paleoartist Gregory S. Paul praised Bill Watterson for the scientific accuracy of the dinosaurs appearing in \"Calvin and Hobbes\".\n\nIn her 1994 book \"When Toys Come Alive\", Lois Rostow Kuznets says that Hobbes serves both as a figure of Calvin's childish fantasy life and as an outlet for the expression of libidinous desires more associated with adults. Kuznets also looks at Calvin's other fantasies, suggesting that they are a second tier of fantasies utilized in places like school where transitional objects such as Hobbes would not be socially acceptable.\n\nPolitical scientist James Q. Wilson, in a paean to \"Calvin and Hobbes\" upon Watterson's decision to end the strip in 1995, characterized it as \"our only popular explication of the moral philosophy of Aristotle.\"\n\nAlisa White Coleman analyzed the strip's underlying messages concerning ethics and values in \"'Calvin and Hobbes': A Critique of Society's Values,\" published in the \"Journal of Mass Media Ethics\" in 2000.\n\nA collection of original Sunday strips was exhibited at Ohio State University's Billy Ireland Cartoon Library & Museum in 2001. Watterson himself selected the strips and provided his own commentary for the exhibition catalog, which was later published by Andrews McMeel as \"Calvin and Hobbes: Sunday Pages 1985–1995\".\n\nSince the discontinuation of \"Calvin and Hobbes\", individual strips have been licensed for reprint in schoolbooks, including the Christian homeschooling book \"The Fallacy Detective\" in 2002, and the university-level philosophy reader \"Open Questions: Readings for Critical Thinking and Writing\" in 2005; in the latter, the ethical views of Watterson and his characters Calvin and Hobbes are discussed in relation to the views of professional philosophers. Since 2009, Twitter users have indicated that \"Calvin and Hobbes\" strips have appeared in textbooks for subjects in the sciences, social sciences, mathematics, philosophy, and foreign language.\n\nIn a 2009 evaluation of the entire body of \"Calvin and Hobbes\" strips using grounded theory methodology, Christijan D. Draper found that: \"Overall, \"Calvin and Hobbes\" suggests that meaningful time use is a key attribute of a life well lived,\" and that \"the strip suggests one way to assess the meaning associated with time use is through preemptive retrospection by which a person looks at current experiences through the lens of an anticipated future...\"\n\nJamey Heit's \"Imagination and Meaning in Calvin and Hobbes\", a critical, academic analysis of the strip, was published in 2012.\n\n\"Calvin and Hobbes\" strips were again exhibited at the Billy Ireland Cartoon Library & Museum at Ohio State University in 2014, in an exhibition entitled \"Exploring Calvin and Hobbes\". An exhibition catalog by the same title, which also contained an interview with Watterson conducted by Jenny Robb, the curator of the museum, was published by Andrews McMeel in 2015.\n\nYears after its original newspaper run, \"Calvin and Hobbes\" has continued to exert influence in entertainment, art, and fandom.\n\nIn television, Calvin and Hobbes are depicted in stop motion animation in the 2006 \"Robot Chicken\" episode \"Lust for Puppets,\" and in traditional animation in the 2009 \"Family Guy\" episode \"Not All Dogs Go to Heaven.\" In the 2013 \"Community\" episode \"Paranormal Parentage,\" the characters Abed Nadir (Danny Pudi) and Troy Barnes (Donald Glover) dress as Calvin and Hobbes, respectively, for Halloween.\n\nBritish artists, merchandisers, booksellers, and philosophers were interviewed for a 2009 BBC Radio 4 half-hour programme about the abiding popularity of the comic strip, narrated by Phill Jupitus.\n\nThe first book-length study of the strip, \"Looking for Calvin and Hobbes: The Unconventional Story of Bill Watterson and His Revolutionary Comic Strip\" by Nevin Martell, was first published in 2009; an expanded edition was published in 2010. The book chronicles Martell's quest to tell the story of \"Calvin and Hobbes\" and Watterson through research and interviews with people connected to the cartoonist and his work. The director of the later documentary \"Dear Mr. Watterson\" referenced \"Looking for Calvin and Hobbes\" in discussing the production of the movie, and Martell appears in the film.\n\nThe American documentary film \"Dear Mr. Watterson\", released in 2013, explores the impact and legacy of \"Calvin and Hobbes\" through interviews with authors, curators, historians, and numerous professional cartoonists.\n\nThe enduring significance of \"Calvin and Hobbes\" to international cartooning was recognized by the jury of the Angoulême International Comics Festival in 2014 by the awarding of its Grand Prix to Watterson, only the fourth American to ever receive the honor (after Will Eisner, Robert Crumb, and Art Spiegelman).\n\nIn 2016 and 2017, author Berkley Breathed has included \"Calvin and Hobbes\" in various \"Bloom County\" cartoons. He launched the first cartoon on April Fool's Day 2016 and jokingly issued a statement suggesting that he had acquired \"Calvin and Hobbes\" from Bill Watterson, who was \"out of the Arizona facility, continent and looking forward to some well-earned financial security.\" While bearing Watterson's signature and drawing style, as well as featuring characters from both \"Calvin and Hobbes\" and Breathed's \"Bloom County\", it is unclear whether Watterson had any input into these cartoons or not.\n\nA number of artists and cartoonists have created works portraying Calvin as a teenager or an adult; the concept has also inspired writers.\n\nIn 2011, a comic strip appeared by cartoonists Dan and Tom Heyerman called \"Hobbes and Bacon\". The strip depicts Calvin as an adult, married to Susie Derkins, with a young daughter named after philosopher Francis Bacon, to whom Calvin gives Hobbes. Though consisting of only four strips originally, \"Hobbes and Bacon\" received considerable attention when it appeared and was continued by other cartoonists and artists.\n\nA novel entitled \"Calvin\" by CLA Young Adult Book Award-winning author Martine Leavitt was published in 2015. The story tells of seventeen-year-old Calvin — who was born on the day that \"Calvin and Hobbes\" ended, and who has now been diagnosed with schizophrenia — and his hallucination of Hobbes, his childhood stuffed tiger. With his friend Susie, who might also be a hallucination, Calvin sets off to find Bill Watterson, in the hope that the cartoonist can provide aid for Calvin's condition.\n\n\n\n\n\n\n"}
{"id": "6060", "url": "https://en.wikipedia.org/wiki?curid=6060", "title": "Campaign for Real Ale", "text": "Campaign for Real Ale\n\nThe Campaign for Real Ale (CAMRA) is an independent voluntary consumer organisation headquartered in St Albans, England, which promotes real ale, real cider and the traditional British pub. With over 185,000 members, it is now the largest single-issue consumer group in the UK, and is a founding member of the European Beer Consumers Union (EBCU).\n\nThe organisation was founded in 1971 in Kruger's bar in Dunquin, Kerry, Ireland by Michael Hardman, Graham Lees, Jim Makin, and Bill Mellor, who were opposed to the growing mass production of beer and the homogenisation of the British brewing industry. The original name was the Campaign for the Revitalisation of Ale. Following the formation of the Campaign, the first annual general meeting took place in 1972, at the Rose Inn in Coton Road, Nuneaton. Early membership consisted of the four founders and their friends. Yet interest in CAMRA and its objectives spread rapidly, with 5,000 members signed up by 1973. Other early influential members included Christopher Hutt, author of \"Death of the English Pub\", who succeeded Hardman as chairman, Frank Baillie, author of \"The Beer Drinker's Companion\", and later the current \"Good Beer Guide\" editor, Roger Protz.\n\nOn 31 March 2016, founder Michael Hardman returned to chair a Revitalisation Project Steering Group. The aim of the Revitalisation Project is to review the organisation's purpose. Consultation meetings took place in the spring and summer of 2016, and further discussion took place at the Bournemouth AGM and Conference in spring 2017 leading to possible refinement of proposals then subject to a final vote of all members at spring 2018's AGM and Conference.\n\nCAMRA's stated aims are to:\n\nCAMRA's campaigns include promoting small brewing and pub businesses, reforming licensing laws, reducing tax on beer, and stopping continued consolidation among local British brewers. It also makes an effort to promote less common varieties of beer, including stout, porter, and mild, as well as traditional cider and perry.\n\nCAMRA does not support the promotion and sale of keg based craft beer in the UK. CAMRA's Internal Policy document states that real ale can only be served without the use of additional carbonation. This policy means that \"any beer brand which is produced in both cask and keg versions\" is not admitted to CAMRA festivals if the brewery's marketing is deemed to imply an equivalence of quality or character between the two versions.\n\nIn 2009, CAMRA announced that it had reached the 100,000 members mark and subsequently went on to pass the 150,000 members mark in 2013. Member benefits include a monthly newspaper, \"What's Brewing\" and a quarterly \"BEER\" magazine, and free or reduced price admission to CAMRA-organised beer festivals. In recent times CAMRA has obtained benefits for its members from some commercial organisations and increasingly some licensed premises offer members price reductions on real ale (and sometimes cider and perry).\n\nCAMRA is organised on a federal basis, with numerous independent local branches, each covering a particular geographical area of the UK, that contribute to the central body of the organisation based in St Albans. It is governed by a voluntary unpaid national executive, elected by the membership. The local branches are grouped into 16 regions across the UK, such as the West Midlands or Wessex.\n\nThe current National Chairman is Colin Valentine, who took over from Paula Waters in February 2010. Tim Page, who succeeded Mike Benner, has been Chief Executive since November 2014.\n\nCAMRA has established influence at national government level, including Historic England, and has been designated by the Secretary of State for Trade and Industry as a \"super-complainant\" to the Office of Fair Trading.\n\nCAMRA publishes the \"Good Beer Guide\", an annually compiled directory of its recommended pubs and brewers; the \"Good Cider Guide\", an occasionally compiled directory of pubs that sell real Cider; the \"Good Bottled Beer Guide\", an occasionally compiled review of real ale in a bottle.\nCAMRA members receive a monthly newspaper called \"What's Brewing\" and a quarterly colour magazine called \"Beer\".\nIt also runs the Great British Beer Festival, a yearly event held in London at which a large selection of cask ales and ciders are tasted. It also maintains a National Inventory of Historic Pub Interiors to help bring greater recognition and protection to Britain's most historic pubs. In 2013 CAMRA launched public access to a national pub database website called Whatpub.com which is maintained by CAMRA members through the branches structure.\n\nCAMRA supports and promotes many beer and cider festivals around the country each year, which are organised by local CAMRA branches around the UK. Generally, each festival charges an entry fee which varies depending upon the area, and either covers entry only or includes a commemorative ⅓, ½ or 1 pint glass sporting the details of the festival. A festival programme is usually also provided, listing the drinks available for tasting and providing a brief description of each beverage. Members often get discounted or free entrance to CAMRA festivals. \n\nThe Campaign also organises the annual Great British Beer Festival in August. It is now held in the Great, National & West Halls at the Olympia Exhibition Centre, in Kensington, London, having been held for a few years at Earl's Court as well as regionally in the past at venues such as Brighton and Leeds.\n\nCAMRA presents awards for beers and pubs, such as the National Pub of the Year, in which approximately 4,000 active CAMRA members from 200 local branches vote for their favourite pub of the year. The branch winners are entered into 16 regional competitions which are then visited by several individuals who select the ones they like best. There are also the Pub Design Awards, which are held in association with English Heritage and the Victorian Society. These comprise several categories, including new build, refurbished and converted pubs. The best known CAMRA award is the Champion Beer of Britain, which is selected at the Great British Beer Festival, other awards include the Champion Beer of Scotland and the Champion Beer of Wales.\n\nCAMRA developed the National Beer Scoring Scheme (NBSS) as an easy to use scheme for judging beer quality in pubs, to assist CAMRA branches in selecting pubs for the \"Good Beer Guide\". The person filling in the form records their name, date, the pub, the beer and the score.\nThe scores range from 0, meaning that no real ale is available; increasing in increments of 0.5 through 2, which signifies an average beer that is drunk without calling attention to itself in either a positive or negative manner; up to 5, which signifies a perfect beer. CAMRA members may also input their beer scores on line via the CAMRA website Whatpub.com.\n\nThe CAMRA Pub Heritage Group identifies, records and helps to protect pub interiors of historic and/or architectural importance, and seeks to get them listed.\n\nThe group maintains two inventories of Heritage pubs, the National Inventory (NI), which contains only those pubs that have been maintained in their original condition (or have been modified very little) for at least thirty years, but usually since at least World War II. The second, larger, inventory is the Regional Inventory (RI), which is broken down by county and contains both those pubs listed in the NI and other pubs that are not eligible for the NI, for reasons such as having been overly modified, but are still considered historically important, or have particular architectural value.\n\nThe NI contains 289 pubs .\n\nThe LocAle scheme launched in 2007 was developed by Steve Westby of the Nottingham branch to promote locally brewed beers. The scheme functions slightly differently in each area, and is managed by each branch, but the overall rule is that each participating pub is allowed to purchase beer from whatever brewery they wish, but if the beer is to be promoted as a LocAle it must come from a brewery within a predetermined number of miles set by each CAMRA branch, generally around 20 or 25, although the North London branch has set it at 30 miles from brewery to pub door, even if it comes from a distribution centre further away; in addition, each participating pub must keep at least one LocAle for sale at all times.\nCAMRA members may join the CAMRA Members' Investment Club, which since 1989 has invested in real ale breweries and pub chains. With a fund value of nearly £20.5 million owned by around 4,000 members, it has produced a healthy financial return as well as offering a programme of brewery visits.\n\n\n"}
{"id": "6061", "url": "https://en.wikipedia.org/wiki?curid=6061", "title": "CNO cycle", "text": "CNO cycle\n\nThe CNO cycle (for carbon–nitrogen–oxygen) is one of the two known sets of fusion reactions by which stars convert hydrogen to helium, the other being the proton–proton chain reaction. Unlike the latter, the CNO cycle is a catalytic cycle. It is dominant in stars that are more than 1.3 times as massive as the Sun.\n\nIn the CNO cycle, four protons fuse, using carbon, nitrogen and oxygen isotopes as catalysts, to produce one alpha particle, two positrons and two electron neutrinos. Although there are various paths and catalysts involved in the CNO cycles, all these cycles have the same net result:\n\nThe positrons will almost instantly annihilate with electrons, releasing energy in the form of gamma rays. The neutrinos escape from the star carrying away some energy. One nucleus goes to become carbon, nitrogen, and oxygen isotopes through a number of transformations in an endless loop.\n\nTheoretical models suggest that the CNO cycle is the dominant source of energy in stars whose mass is greater than about 1.3 times that of the Sun. The proton–proton chain is more prominent in stars the mass of the Sun or less. This difference stems from temperature dependency differences between the two reactions; pp-chain reaction starts at temperatures around (4 megakelvins), making it the dominant energy source in smaller stars. A self-maintaining CNO chain starts at approximately , but its energy output rises much more rapidly with increasing temperatures. At approximately , the CNO cycle starts becoming the dominant source of energy.\nThe Sun has a core temperature of around , and only of nuclei produced in the Sun is\nborn in the CNO cycle. The CNO-I process was independently proposed by Carl von Weizsäcker and Hans Bethe in 1938 and 1939, respectively.\n\nUnder typical conditions found in stars, catalytic hydrogen burning by the CNO cycles is limited by proton captures. Specifically, the timescale for beta decay of the radioactive nuclei produced is faster than the timescale for fusion. Because of the long timescales involved, the cold CNO cycles convert hydrogen to helium slowly, allowing them to power stars in quiescent equilibrium for many years.\n\nThe first proposed catalytic cycle for the conversion of hydrogen into helium was initially called the carbon–nitrogen cycle (CN cycle), also honorarily referred to as the Bethe–Weizsäcker cycle, because it does not involve a stable isotope of oxygen. Bethe's original calculations suggested the CN-cycle was the Sun's primary source of energy, owing to the belief at the time that the Sun's composition was 10% nitrogen; the solar abundance of nitrogen is now known to be less than half a percent. This cycle is now recognized as the first part of the larger CNO nuclear burning network. The main reactions of the CNO-I cycle are →→→→→→:\nwhere the carbon-12 nucleus used in the first reaction is regenerated in the last reaction. After the two positrons emitted annihilate with two ambient electrons producing an additional 2.04 MeV, the total energy released in one cycle is 26.73 MeV; it should be noted that in some texts, authors are erroneously including the positron annihilation energy in with the beta-decay Q-value and then neglecting the equal amount of energy released by annihilation, leading to possible confusion. All values are calculated with reference to the Atomic Mass Evaluation 2003.\n\nThe limiting (slowest) reaction in the CNO-I cycle is the proton capture on . In 2006 it was experimentally measured down to stellar energies, revising the calculated age of globular clusters by around 1 billion years.\n\nThe neutrinos emitted in beta decay will have a spectrum of energy ranges, because although momentum is conserved, the momentum can be shared in any way between the positron and neutrino, with either emitted at rest and the other taking away the full energy, or anything in between, so long as all the energy from the Q-value is used. All momentum which get the electron and the neutrino together is not great enough to cause a significant recoil of the much heavier daughter nucleus and hence, its contribution to kinetic energy of the products, for the precision of values given here, can be neglected. Thus the neutrino emitted during the decay of nitrogen-13 can have an energy from zero up to 1.20 MeV, and the neutrino emitted during the decay of oxygen-15 can have an energy from zero up to 1.73 MeV. On average, about 1.7 MeV of the total energy output is taken away by neutrinos for each loop of the cycle, leaving about 25 MeV available for producing luminosity.\n\nIn a minor branch of the above reaction, that occurs in the Sun's core 0.04% of the time, the final reaction involving shown above does not produce carbon-12 and an alpha particle, but instead produces oxygen-16 and a photon and continues →→→→→→:\n\nLike the carbon, nitrogen, and oxygen involved in the main branch, the fluorine produced in the minor branch is merely an intermediate product and at steady state, does not accumulate in the star.\n\nThis subdominant branch is significant only for massive stars. The reactions are started when one of the reactions in CNO-II results in fluorine-18 and gamma instead of nitrogen-14 and alpha, and continues →→→→→→:\n\nLike the CNO-III, this branch is also only significant in massive stars. The reactions are started when one of the reactions in CNO-III results in fluorine-19 and gamma instead of nitrogen-15 and alpha, and continues →→→→→→:\n\nUnder conditions of higher temperature and pressure, such as those found in novae and x-ray bursts, the rate of proton captures exceeds the rate of beta-decay, pushing the burning to the proton drip line. The essential idea is that a radioactive species will capture a proton before it can beta decay, opening new nuclear burning pathways that are otherwise inaccessible. Because of the higher temperatures involved, these catalytic cycles are typically referred to as the hot CNO cycles; because the timescales are limited by beta decays instead of proton captures, they are also called the beta-limited CNO cycles.\n\nThe difference between the CNO-I cycle and the HCNO-I cycle is that captures a proton instead of decaying, leading to the total sequence →→→→→→:\n\nThe notable difference between the CNO-II cycle and the HCNO-II cycle is that captures a proton instead of decaying, and neon is produced in a subsequent reaction on , leading to the total sequence →→→→→→:\n\nAn alternative to the HCNO-II cycle is that captures a proton moving towards higher mass and using the same helium production mechanism as the CNO-IV cycle as →→→→→→:\n\nWhile the total number of \"catalytic\" nuclei are conserved in the cycle, in stellar evolution the relative proportions of the nuclei are altered. When the cycle is run to equilibrium, the ratio of the carbon-12/carbon-13 nuclei is driven to 3.5, and nitrogen-14 becomes the most numerous nucleus, regardless of initial composition. During a star's evolution, convective mixing episodes moves material, within which the CNO cycle has operated, from the star's interior to the surface, altering the observed composition of the star. Red giant stars are observed to have lower carbon-12/carbon-13 and carbon-12/nitrogen-14 ratios than do main sequence stars, which is considered to be convincing evidence for the operation of the CNO cycle.\n\n\n"}
{"id": "6062", "url": "https://en.wikipedia.org/wiki?curid=6062", "title": "Craps", "text": "Craps\n\nCraps is a dice game in which the players make wagers on the outcome of the roll, or a series of rolls, of a pair of dice. Players may wager money against each other (playing \"street craps\", also known as \"shooting dice\" or \"rolling dice\") or a bank (playing \"casino craps\", also known as \"table craps\", or often just \"craps\"). Because it requires little equipment, \"street craps\" can be played in informal settings.\n\nCraps developed from a simplification of the early English game of \"hazard\". Its origins are complex and may date to the Crusades, later being influenced by French gamblers. What was to become the modern American version of the game was brought to New Orleans by Bernard Xavier Philippe de Marigny de Mandeville, a gambler and politician descended from wealthy colonial Louisiana landowners. There was a flaw in Bernard's version of the game in which players could exploit the casino using fixed dice and taking advantage of the way players can bet with or against the dice thrower. An American dice maker named John H. Winn introduced the \"don't pass\" betting option in order to fix this problem and it is this version of craps that still exists today.\nThe game, first known as \"crapaud\" (a French word meaning \"toad\" in reference to the original style of play by people crouched over a floor or sidewalk), reportedly owes its modern popularity to street craps. Street craps may be played by rolling the dice against a backstop, such as a curb or stair-stoop, or without a backstop, at the choice of the players.\n\nDuring World War II, street-style craps became popular among soldiers, who often played it using an Army blanket as a shooting surface. With no backboard or sidewalk curb to hit against, this gave rise to presumed methods of dice control, of which the best was known as the \"army blanket roll\".\n\nBank craps or casino craps is a game played by multiple or a single player betting on the outcome of the dice. The players and casino employees stand or sit around a large oval \"craps table\". In most houses sitting at a craps table is discouraged unless a player has medical reasons for requiring a seat.\n\nIn a casino, players make bets with chips on a specially made craps table with a \"layout\" – a table cloth made of felt that displays the various betting possibilities, which vary somewhat in bet presence, position, and payout among casinos. The tables have the shape of a bathtub, about long, wide, about above the floor at the bottom, where the layout is, and about from the layout up to the rim of the tub.\n\nWith the table oriented with its long sides running left to right, along one long side is the casino's bank – thousands of chips, stacked 20 high, standing on the layout. Along the opposite side of the tub is usually a long, angled mirror. The left and right U-shaped sections of the table each have the same bet areas marked on the layout, with space for usually up to eight players to stand (or occasionally sit, on barstools) and place their bets on each side. The walls of the tub around these sections are usually covered with a rubberized pyramid-shaped texture, used to randomly reflect the dice that are thrown towards them from the opposite side of the table.\n\nAn additional group of bets, referred to as proposition bets, is in the middle of the layout and used for bets by players from both sides. The top rim of the table has horizontal grooves for players to keep their chips (lying horizontally) while not in play.\n\nThe table is run by up to four casino employees: a boxman, seated (usually the only seated employee) behind the casino's bank, who manages the chips, supervises the dealers, and handles \"coloring up\" players (exchanging small chip denominations for larger denominations in order to preserve the chips at a table); two base dealers who stand to either side of the boxman and collect and pay bets to players around their half of the table; and a stickman who stands directly across the table from the boxman, takes and pays (or directs the base dealers to do so) the bets in the center of the table, announces the results of each roll (usually with a distinctive patter), and moves the dice across the layout with an elongated wooden stick.\n\nEach employee also watches for mistakes by the others because of the sometimes large number of bets and frantic pace of the game. In smaller casinos or at quiet times of day, one or more of these employees may be missing, and have their job covered by another, or cause player capacity to be reduced.\n\nSome smaller casinos have introduced \"mini-craps\" tables which are operated with only two dealers; rather than being two essentially identical sides and the center area, a single set of major bets is presented, split by the center bets. Responsibility of the dealers is adjusted: the stickman continuing to handle the center bets, and the base dealer handling the other bets as well as cash and chip exchanges.\n\nBy contrast, in \"street craps\", there is no marked table and often the game is played with no back-stop against which the dice are to hit. (Despite the name \"street craps\", this game is often played in houses, usually on an un-carpeted garage or kitchen floor.) The wagers are made in cash, never in chips, and are usually thrown down onto the ground by the players. There are no attendants, and so the progress of the game, fairness of the throws, and the way that the payouts are made for winning bets are self-policed by the players.\n\nEach casino may set which bets are offered and different payouts for them, though a core set of bets and payouts is typical. Players take turns rolling two dice and whoever is throwing the dice is called the \"shooter\". Players can bet on the various options by placing chips directly on the appropriately-marked sections of the layout, or asking the base dealer or stickman to do so, depending on which bet is being made.\n\nWhile acting as the shooter, a player must have a bet on the \"Pass\" line and/or the \"Don't Pass\" line. \"Pass\" and \"Don't Pass\" are sometimes called \"Win\" and \"Don't Win\" or \"Right\" and \"Wrong\" bets. The game is played in rounds and these \"Pass\" and \"Don't Pass\" bets are betting on the outcome of a round. The shooter is presented with multiple dice (typically five) by the \"stickman\", and must choose two for the round. The remaining dice are returned to the stickman's bowl and are not used.\nEach round has two phases: \"come-out\" and \"point\". Dice are passed to the left. To start a round, the shooter makes one or more \"come-out\" rolls. The shooter must shoot toward the farther back wall and is generally required to hit the farther back wall with both dice. Casinos may allow a few warnings before enforcing the dice to hit the back wall and are generally lenient if at least one die hits the back wall. Both dice must be tossed in one throw. If only one die is thrown the shot is invalid. A come-out roll of 2, 3 or 12 is called \"craps\" or \"crapping out\", and anyone betting the Pass line loses. On the other hand, anyone betting the Don't Pass line on come out wins with a roll of 2 or 3 and ties (pushes) if a 12 is rolled. Shooters may keep rolling after crapping out; the dice are only required to be passed if a shooter sevens out (rolls a seven after a point has been established). A come-out roll of 7 or 11 is a \"natural\"; the Pass line wins and Don't Pass loses. The other possible numbers are the point numbers: 4, 5, 6, 8, 9, and 10. If the shooter rolls one of these numbers on the come-out roll, this establishes the \"point\" - to \"pass\" or \"win\", the point number must be rolled again before a seven.\n\nThe dealer flips a button to the \"On\" side and moves it to the point number signifying the second phase of the round. If the shooter \"hits\" the point value again (any value of the dice that sum to the point will do; the shooter doesn't have to exactly repeat the exact combination of the come-out roll) before rolling a seven, the Pass line wins and a new round starts. If the shooter rolls any seven before repeating the point number (a \"seven-out\"), the Pass line loses, the Don't Pass line wins, and the dice pass clockwise to the next new shooter for the next round. Once a point has been established any multi-roll bet (including Pass and/or Don't Pass line bets and odds) are unaffected by the 2, 3, 11 or 12, the only numbers which affect the round are the established point, any specific bet on a number, or any 7. Any single roll bet is always affected (win or lose) by the outcome of any roll.\n\nWhile the come-out roll may specifically refer to the first roll of a new shooter, any roll where no point is established may be referred to as a come-out. By this definition the start of any new round regardless if it is the shooter's first toss can be referred to as a come-out roll.\n\nAny player may make any bet on any number at any time. Players can place bets on the 4, 5, 6, 8, 9, 10, pass, don't pass, any field number, hard or easy ways, come or don't come (as long as a point is established), increase odds behind pass and don't pass lines or make any other bet the table offers. All bets besides the pass line and come bet may be removed or reduced anytime before the bet loses, this is known as \"taking it down\" in craps.\n\nThe maximum bet for Place, Buy, Lay, Pass and Come bets are generally equal to table maximum. Lay bet maximum are equal to the table maximum win, so if a player wishes to lay the 4 or 10, he or she may bet twice at amount of the table maximum for the win to be table maximum. Odds behind Pass, Come, Don't Pass and Don't Come may be however larger than the odds offered allows and can be greater than the table maximum in some casinos. Don't odds are capped on the maximum allowed win some casino allow the odds bet itself to be larger than the maximum bet allowed as long as the win is capped at maximum odds. Single rolls bets can be lower than the table minimum, but the maximum bet allowed is also lower than the table maximum. The maximum allowed single roll bet is based on the maximum allowed win from a single roll.\n\nIn all the above scenarios, whenever the Pass line wins, the Don't Pass line loses, and vice versa, with one exception: on the come-out roll, a roll of 12 will cause Pass Line bets to lose, but Don't Pass bets are pushed (or \"barred\"), neither winning nor losing. (The same applies to \"Come\" and \"Don't Come\" bets, discussed below.)\n\nA player wishing to play craps without being the shooter should approach the craps table and first check to see if the dealer's \"On\" button is on any of the point numbers.\n\n\nIn either case, all single or multi roll proposition bets may be placed in either of the two rounds.\n\nBetween dice rolls there is a period for dealers to make payouts and collect losing bets, after which players can place new bets. The stickman monitors the action at a table and decides when to give the shooter the dice, after which no more betting is allowed.\n\nWhen joining the game, one should place money on the table rather than passing it directly to a dealer, the dealer's exaggerated movements during the process of \"making change\" or \"change only\" (converting currency to an equivalent in casino cheques) are required so that any disputes can be later reviewed against security camera footage.\n\nIf a new player feels that he or she needs assistance in learning the rules of craps, it is recommended to approach an empty craps table at a slow time of day (for example, between 9 a.m. and 11 a.m.). The dealers are likely to be approachable and friendly and will explain the betting process. Also, casinos often offer training sessions for new craps players.\n\nThe dealers will insist that the shooter roll with one hand and that the dice bounce off the far wall surrounding the table. These requirements are meant to keep the game fair (preventing switching the dice or making a \"controlled shot\"). If a die leaves the table, the shooter will usually be asked to select another die from the remaining three but can request using the same die if it passes the boxman's inspection. This requirement is used to keep the game fair (and reduce the chance of loaded dice).\n\nThere are many local variants of the calls made by the stickman for rolls during a craps game. These often incorporate a reminder to the dealers as to which bets to pay or collect.\n\n\nRolls of 4, 6, 8, and 10 are called \"hard\" or \"easy\" (e.g. \"six the hard way\", \"easy eight\", \"hard ten\") depending on whether they were rolled as a \"double\" or as any other combination of values, because of their significance in center table bets known as the \"hard ways\". Hard way rolls are so named because there is only one way to roll them (i.e., the value on each die is the same when the number is rolled). Consequently, it is more likely to roll the number in combinations (easy) rather than as a double (hard).\n\nThe shooter is required to make either a pass line bet or a Don't Pass bet if he wants to shoot. On the come out roll each player may only make one bet on the Pass or Don't Pass, but may bet both if desired. The Pass Line and Don't Pass bet is optional for any player not shooting. In rare cases, some casinos require all players to make a minimum Pass Line or Don't Pass bet (if they want to make any other bet), whether they are currently shooting or not.\n\nThe fundamental bet in craps is the pass line bet, which is a bet for the shooter to win. This bet must be at least the table minimum and at most the table maximum.\nThe pass line bet pays even money.\n\nOnce a pass line bet is made, it is always working and cannot be turned \"Off\", taken down or reduced unless it wins or loses. A player may increase any corresponding odds (up to the table limit) behind the Pass line at any time after a point is established. Players may only bet the pass line on the come out roll when no point has been established, unless the casino allows put betting where the player can bet Pass line or increase an existing Pass line bet whenever desired and may take odds immediately if the point is already on.\n\nA don't pass bet is a bet for the shooter to lose (\"seven out, line away\") and is almost the opposite of the pass line bet. Like the Pass bet, this bet must be at least the table minimum and at most the table maximum.\nThe don't pass bet pays even money.\n\nAfter a point is established, a player may take down or reduce a don't pass bet and any corresponding odds at any time because odds of rolling a 7 before the point is in the player's favor. Once taken down or reduced, however, the don't pass bet may not be restored or increased. Because the shooter must have a line bet the shooter generally may not reduce a don't pass bet below the table minimum. In Las Vegas, a majority of casinos will allow the shooter to move the bet to the pass line in lieu of taking it down, however in other areas such as Pennsylvania and Atlantic City, this is not allowed. Even though players are allowed to remove the don't pass line bet after a point has been established, the bet cannot be turned \"Off\" without being removed. If a player chooses to remove the don't pass line bet, he or she can no longer lay odds behind the don't pass line. The player can, however, still make standard lay bets on any of the point numbers (4, 5, 6, 8, 9, 10).\n\nThere are two different ways to calculate the odds and house edge of this bet. The table below gives the numbers considering that the game ends in a push when a 12 is rolled, rather than being undetermined. Betting on don't pass is often called \"playing the dark side\", and it is considered by some players to be in poor taste, or even taboo, because it goes directly against conventional play, winning when most of the players lose.\n\nIf a 4, 5, 6, 8, 9, or 10 is thrown on the come-out roll (i.e., if a point is established), most casinos allow pass line players to \"take odds\" by placing up to some predetermined multiple of the pass line bet, behind the pass line. This additional bet wins if the point is rolled again before a 7 is rolled (the point is made) and pays at the true odds of 2-to-1 if 4 or 10 is the point, 3-to-2 if 5 or 9 is the point, or 6-to-5 if 6 or 8 is the point. Unlike the pass line bet itself, the pass line odds bet can be turned \"Off\" (not working), removed or reduced anytime before it loses. In Las Vegas, generally odds bets are required to be the table minimum. In Atlantic City and Pennsylvania, the combine odds and pass bet must be table minimum so players can bet the minimum single unit on odds depending on the point. If the point is a 4 or 10 players can bet as little as $1 on odds if the table minimum is low such as is $5, $10 or $15. If the player requests the pass odds be not working (\"Off\") and the shooter sevens-out or hits the point, the pass line bet will be lost or doubled and the pass odds returned.\n\nIndividual casinos (and sometimes tables within a casino) vary greatly in the maximum odds they offer, from single or double odds (one or two times the pass line bet) up to 100x or even unlimited odds. A variation often seen is \"3-4-5X Odds\", where the maximum allowed odds bet depends on the point: three times if the point is 4 or 10; four times on points of 5 or 9; or five times on points of 6 or 8. This rule simplifies the calculation of winnings: a maximum pass odds bet on a 3-4-5× table will always be paid at six times the pass line bet regardless of the point.\n\nAs odds bets are paid at true odds, in contrast with the pass line which is always even money, taking odds on a minimum pass line bet lessens the house advantage compared with betting the same total amount on the pass line only. A maximum odds bet on a minimum pass line bet often gives the lowest house edge available in any game in the casino. However, the odds bet cannot be made independently, so the house retains an edge on the pass line bet itself.\n\nIf a player is playing don't pass instead of pass, they may also \"lay odds\" by placing chips behind the don't pass line. If a 7 comes before the point is rolled, the odds pay at true odds of 1-to-2 if 4 or 10 is the point, 2-to-3 if 5 or 9 is the point, or 5-to-6 if 6 or 8 is the point. Typically the maximum lay bet will be expressed such that a player may \"win\" up to an amount equal to the maximum odds multiple at the table. If a player lays maximum odds with a point of 4 or 10 on a table offering five-times odds, he would be able to lay a maximum of ten times the amount of his Don't Pass bet. At 5x odds table, the maximum amount the combined bet can win will always be 6x the amount of the Don't Pass bet. Players can bet table minimum odds if desired and win less than table minimum. Like the Don't Pass bet the odds can be removed or reduced. Unlike the don't pass bet itself, the don't pass odds can be turned \"Off\" (not working). In Las Vegas generally odds bets are required to be the table minimum. In Atlantic City and Pennsylvania, the combine lay odds and Don't Pass bet must be table minimum so players may bet as little as the minimum two units on odds depending on the point. If the point is a 4 or 10 players can bet as little as $2 if the table minimum is low such as $5, $10 or $15 tables. If the player requests the don't pass odds to be not working (\"Off\") and the shooter hits the point or sevens-out, the don't pass bet will be lost or doubled and the don't pass odds returned. Unlike a standard lay bet on a point, lay odds behind the Don't Pass line does not charge commission (vig).\n\nA Come bet can be visualized as starting an entirely new pass line bet, unique to that player. Like the Pass Line each player may only make one Come bet per roll, this does not exclude a player from betting odds on an already established Come point. This bet must be at least the table minimum and at most the table maximum. Players may bet both the Come and Don't Come on the same roll if desired. Come bets can only be made after a point has been established since, on the come-out roll, a Come bet would be the same thing as a pass line bet. A player making a Come bet will bet on the first point number that \"comes\" from the shooter's next roll, regardless of the table's round. If a 7 or 11 is rolled on the first round, it wins. If a 2, 3, or 12 is rolled, it loses. If instead the roll is 4, 5, 6, 8, 9, or 10, the Come bet will be moved by the base dealer onto a box representing the number the shooter threw. This number becomes the \"come-bet point\" and the player is allowed to take odds, just like a pass line bet. Also like a pass line bet, the come bet is always working and cannot be turned \"Off\", removed or reduced until it wins or loses. However, the odds taken behind a Come bet can be turned \"Off\" (not working), removed or reduced anytime before the bet loses. In Las Vegas generally odds bets are required to be the table minimum. In Atlantic City and Pennsylvania, the combine odds and pass bet must be table minimum so players can bet the minimum single unit depending on the point. If the point is a 4 or 10 players can bet as little as $1 if the table minimum is low such as $5, $10 or $15 minimums. If the player requests the Come odds to be not working (\"Off\") and the shooter sevens-out or hits the Come bet point, the Come bet will be lost or doubled and the Come odds returned. If the casino allows put betting a player may increase a Come bet after a point has been established and bet larger odds behind if desired. Put betting also allows a player to bet on a Come and take odds immediately on a point number without a Come bet point being established.\n\nThe dealer will place the odds on top of the come bet, but slightly off center in order to differentiate between the original bet and the odds. The second round wins if the shooter rolls the come bet point again before a seven. Winning come bets are paid the same as winning pass line bets: even money for the original bet and true odds for the odds bet. If, instead, the seven is rolled before the come-bet point, the come bet (and any odds bet) loses.\n\nBecause of the come bet, if the shooter makes their point, a player can find themselves in the situation where they still have a come bet (possibly with odds on it) and the next roll is a come-out roll. In this situation, odds bets on the come wagers are usually presumed to be not working for the come-out roll. That means that if the shooter rolls a 7 on the come-out roll, any players with active come bets waiting for a come-bet point lose their initial wager but will have their odds bets returned to them.\n\nIf the come-bet point is rolled on the come-out roll, the odds do not win but the come bet does and the odds bet is returned (along with the come bet and its payoff). The player can tell the dealer that they want their odds working, such that if the shooter rolls a number that matches the come point, the odds bet will win along with the come bet, and if a seven is rolled, both lose.\n\nMany players will use a come bet as \"insurance\" against \"sevening out\": if the shooter rolls a seven, the come bet pays 1:1, offsetting the loss of the pass line bet. The risk in this strategy is the situation where the shooter does not hit a seven for several rolls, leading to multiple come bets that will be lost if the shooter eventually sevens out.\n\nIn the same way that a come bet is similar to a pass line bet, a don't come bet is similar to a don't pass bet. Like the come, the don't come can \"only\" be bet after a point has already been established as it is the same as a don't pass line bet when no point is established. This bet must be at least the table minimum and at most the table maximum. A don't come bet is played in two rounds. If a 2 or 3 is rolled in the first round, it wins. If a 7 or 11 is rolled, it loses. If a 12 is rolled, it is a push (subject to the same 2/12 switch described above for the don't pass bet). If, instead, the roll is 4, 5, 6, 8, 9, or 10, the don't come bet will be moved by the base dealer onto a box representing the number the shooter threw. The second round wins if the shooter rolls a seven before the don't come point. Like the Don't Pass each player may only make one Don't Come bet per roll, this does not exclude a player from laying odds on an already established Don't Come points. Players may bet both the Don't Come and Come on the same roll if desired.\n\nThe player may lay odds on a don't come bet, just like a don't pass bet; in this case, the dealer (not the player) places the odds bet on top of the bet in the box, because of limited space, slightly offset to signify that it is an odds bet and not part of the original don't come bet. Lay odds behind a Don't Come are subject to the same rules as Don't Pass lay odds. Unlike a standard lay bet on a point, lay odds behind a don't come point does not charge commission (vig) and gives the player true odds. Like the don't pass line bet, don't come bets can be removed or reduced after a don't come point has been established, but cannot be turned off (\"not working\") without being removed. If removed, the player can no longer lay odds behind the don't come point and cannot restore or increase the same don't come bet. Players must wait until next roll as long as a pass line point has been established (players cannot bet don't come on come out rolls) before they can make a new don't come bet. Las Vegas casinos which allow put betting allows players to move the Don't Come directly to any Come point as a put, however this is not allowed in Atlantic City or Pennsylvania. Unlike the don't come bet itself, the don't come odds can be turned \"Off\" (not working), removed or reduced if desired. In Las Vegas, players generally must lay at least table minimum on odds if desired and win less than table minimum, in Atlantic City and Pennsylvania players combined bet must be at least table minimum, so depending on the point number players may lay as little as 2 minimum units (e.g. 4 and 10). If the player requests the don't come odds be not working (\"Off\") and the shooter hits the don't come point or sevens-out, the don't come bet will be lost or doubled and the don't come odds returned.\n\nWinning don't come bets are paid the same as winning don't pass bets: even money for the original bet and true odds for the odds lay. Unlike come bets, the odds laid behind points established by don't come bets are always working including come out rolls unless the player specifies otherwise.\n\nThese are bets that may not be settled on the first roll and may need any number of subsequent rolls before an outcome is determined.\nMost multi-roll bets may fall into the situation where a point is made by the shooter before the outcome of the multi-roll bet is decided. These bets are often considered \"not working\" on the new come-out roll until the next point is established, unless the player calls the bet as \"working.\"\n\nCasino rules vary on this; some of these bets may not be callable, while others may be considered \"working\" during the come-out. Dealers will usually announce if bets are working unless otherwise called off. If a non-working point number placed, bought or laid becomes the new point as the result of a come-out, the bet is usually refunded, or can be moved to another number for free.\n\nPlayers can place bet any point number (4, 5, 6, 8, 9, 10) by placing their wager in the come area and telling the dealer how much and on what number(s), \"30 on the 6\", \"5 on the 5\" or \"25 buy the 10\". Both place and buy bets are bets that the number bet on will be rolled before a 7 is rolled. These bets are considered working bets, and will continue to be paid out each time a shooter rolls the place or buy point number. By rules, place bets are NOT working on the come out roll but can be \"turned on\" by the player. Players may remove or reduce (bet must be at least table minimum) this bet anytime before it loses (seven out).\n\nPlace bet payouts are slightly worse than the true odds: 9-to-5 on points 4 or 10, 7-to-5 on points 5 or 9, and 7-to-6 on points 6 or 8. The place bets on the outside numbers (4,5,9,10) should be made in units of $5, (on a $5 minimum table), in order to receive the correct exact payout of $5 paying $7 or $5 paying $9. The place bets on the 6 & 8 should be made in units of $6, (on a $5 minimum table), in order to receive the correct exact payout of $6 paying $7. Rarely casinos offer the place bet to lose. This bet is the opposite of the place bet and wins if a 7 is rolled before the specific point number. The place bet to lose typically carries a lower house edge than a place bet.\n\nPlayers can also \"buy\" a bet which are paid at true odds, but a 5% commission is charged on the amount of the bet. Buy bets are placed with the shooter betting at a specific number will come out before a player sevens out. The buy bet must be at least table minimum excluding commission, however some casinos require the minimum buy bet amount to be at least $20 to match the $1 charged on the 5% commission. Traditionally, the buy bet commission is paid no matter what, but in recent years a number of casinos have changed their policy to charge the commission only when the buy bet wins. Some casinos charge the commission as a one-time fee to buy the number; payouts are then always at true odds. Most casinos usually charge only $1 for a $25 green-chip bet (4% commission), or $2 for $50 (two green chips), reducing the house advantage a bit more. Players may remove or reduce this bet (bet must be at least table minimum excluding vig) anytime before it loses. Buy bets like place bets are not working when no point has been established unless the player specifies otherwise.\n\nWhere commission is charged only on wins, the commission is often deducted from the winning payoff—a winning $25 buy bet on the 10 would pay $49, for instance. The house edges stated in the table assume the commission is charged on all bets. They are reduced by at least a factor of two if commission is charged on winning bets only.\n\nA lay bet is the opposite of a buy bet, where a player bets on a 7 to roll before the number that is laid. Players may only lay the 4, 5, 6, 8, 9 or 10 and may lay multiple numbers if desired. Just like the buy bet lay bets pay true odds, but because the lay bet is the opposite of the buy bet, the payout is reversed. Therefore, players get 1 to 2 for the numbers 4 and 10, 2 to 3 for the numbers 5 and 9, and 5 to 6 for the numbers 6 and 8. A 5% commission (vigorish, vig, juice) is charged up front on the possible winning amount. For example: A $40 Lay Bet on the 4 would pay $20 on a win. The 5% vig would be $1 based on the $20 win. (NOT $2 based on the $40 bet as the way buy bet commissions are figured.) Like the buy bet the commission is adjusted to suit the betting unit such that fraction of a dollar payouts are not needed. Casinos may charge the vig up front thereby requiring the player to pay a vig win or lose, other casinos may only take the vig if the bet wins. Taking vig only on wins lowers house edge. Players may removed or reduce this bet (bet must be at least table minimum) anytime before it loses. Some casinos in Las Vegas allow players to lay table minimum plus vig if desired and win less than table minimum. Lay bet maximums are equal to the table maximum win, so if a player wishes to lay the 4 or 10, he or she may bet twice at amount of the table maximum for the win to be table maximum. Other casinos require the minimum bet to win at $20 even at the lowest minimum tables in order to match the $1 vig, this requires a $40 bet. Similar to buy betting, some casinos only take commission on win reducing house edge. Unlike place and buy bets, lay bets are always working even when no point has been established. The player must specify otherwise if he or she wishes to have the bet not working.\n\nIf a player is unsure of whether a bet is a single or multi-roll bet, it can be noted that all single-roll bets will be displayed on the playing surface in one color (usually red), while all multi-roll bets will be displayed in a different color (usually yellow).\n\nA put bet is a bet which allows players to increase or make a Pass line bet after a point has been established (after come-out roll). Players may make a put bet on the Pass line and take odds immediately or increase odds behind if a player decides to add money to an already existing Pass line bet. Put betting also allows players to increase an existing come bet for additional odds after a come point has been established or make a new come bet and take odds immediately behind if desired without a come bet point being established. If increased or added put bets on the Pass line and Come cannot be turned \"Off\", removed or reduced, but odds bet behind can be turned \"Off\", removed or reduced. The odds bet is generally required to be the table minimum. Player cannot put bet the Don't Pass or Don't Come. Put betting may give a larger house edge over place betting unless the casino offers high odds.\n\nPut bets are generally allowed in Las Vegas, but not allowed in Atlantic City and Pennsylvania.\n\nThis bet can only be placed on the numbers 4, 6, 8, and 10. In order for this bet to win, the chosen number must be rolled the \"hard way\" (as doubles) before a 7 or any other non-double combination (\"easy way\") totaling that number is rolled. For example, a player who bets a hard 6 can only win by seeing a 3-3 roll come up before any 7 or any easy roll totaling 6 (4-2 or 5-1); otherwise, he/she loses.\n\nIn Las Vegas casinos, this bet is generally working, including when no point has been established, unless the player specifies otherwise. In other casinos such as those in Atlantic City, hard ways are not working when the point is off unless the player requests to have it working on the come out roll.\n\nLike single-roll bets, hard way bets can be lower than the table minimum; however, the maximum bet allowed is also lower than the table maximum. The minimum hard way bet can be a minimum one unit. For example, lower stake table minimums of $5 or $10, generally allow minimum hard ways bets of $1. The maximum bet is based on the maximum allowed win from a single roll.\n\nEasy way is not a specific bet offered in standard casinos, but a term used to define any number combination which has two ways to roll. For example, (6-4, 4-6) would be a \"10 easy\". The 4, 6, 8 or 10 can be made both hard and easy ways. Betting point numbers (which pays off on easy or hard rolls of that number) or single-roll (\"hop\") bets (e.g., \"hop the 2-4\" is a bet for the next roll to be an easy six rolled as a two and four) are methods of betting easy ways.\n\nA player can choose either the 6 or 8 being rolled before the shooter throws a seven. These wagers are usually avoided by experienced craps players since they pay even money (1:1) while a player can make place bets on the 6 or the 8, which pay more (7:6). Some casinos (especially all those in Atlantic City) do not even offer the Big 6 & 8. The bets are located in the corners behind the pass line, and bets may be placed directly by players.\n\nThe only real advantage offered by the Big 6 & 8 is that they can be bet for the table minimum, whereas a place bet minimum may sometimes be greater than the table minimum (e.g. $6 place bet on a $3 minimum game.) In addition place bets are usually not working, except by agreement, when the shooter is \"coming out\" i.e. shooting for a point, and Big 6 and 8 bets always work. Some modern layouts no longer show the Big 6/Big 8 bet.\n\nSingle-roll (proposition) bets are resolved in one dice roll by the shooter. Most of these are called \"Service Bets\", and they are located at the center of most craps tables. Only the stickman or a dealer can place a service bet. Single rolls bets can be lower than the table minimum, but the maximum bet allowed is also lower than the table maximum. The maximum bet is based on the maximum allowed win from a single roll. The lowest single roll bet can be a minimum one unit bet. For example, tables with minimums of $5 or $10 generally allow minimum single roll bets of $1. Single bets are always working by default unless the player specifies otherwise. The bets include:\n\n2 (snake eyes, or Aces): Wins if shooter rolls a 2.\n\n3 (ace-deuce): Wins if the shooter rolls a 3.\n\nYo: Wins if the shooter rolls 11.\n\n12 (boxcars, midnight, or cornrows): Wins if shooter rolls a 12.\n\n2 or 12 (hi-lo): Wins if shooter rolls a 2 or 12. The stickman places this bet on the line dividing the 2 and 12 bets.\n\nAny Craps (Three-Way): Wins if the shooter rolls 2, 3 or 12.\n\nC & E: A combined bet, a player is betting half their bet on \"craps\" (2,3,12) and the other half on 11 (\"yo\"). The combine payout is 3:1 on craps and 7:1 on 11 (\"yo\"). Another method of calculating the payout is to divide the total bet in half. The player would receive 7:1 minus half the total bet payout on half the total bet for craps and 15:1 minus half the total bet payout on half the total bet for 11 (\"yo\"). For example, using this method if a player were to bet $2 on C & E, $1 would receive 7:1 payout on craps minus $1 for the bet on 11 so the total profit would be $6. If an 11 was rolled the player would receive 15:1 minus $1 for the bet on craps so the player's total profit is $14. Both methods of calculation yield the same result so either method can be used. If a player wishes to take the bet down after a win the player would receive the whole bet not half even though only one of the two bets can win per roll. The minimum bet on C & E is double the lowest unit bet allowed at the table. So if the minimum single roll bet is $1 the lowest C & E bet allowed would be $2. Players are, however, able to make odd number bets larger than $2 if desired. One of the two bets will always lose, the other may win.\n\nAny seven: Wins if the shooter rolls a 7 with 4:1 payout. This bet is also nicknamed \"Big Red,\" since the 7 on its betting space on the layout is usually large and red, and it is considered bad luck and a breach of etiquette to speak the word \"seven\" at the table.\n\nHorn: This is a bet that involves betting on 1 unit each for 2, 3, 11 and 12 at the same time for the next roll. The bet is actually four separate bets, and pays off depending on which number is actually rolled. The combine payout is 27:4 for 2, 12 and 3:1 for 3, 11. Each individual bet has the same payout as a single bet on the specific numbers, 30:1 for 2 and 12 minus the other three bets, 15:1 for 3 and 11 minus the other three bets. If a player wins the bet he can take down all four bets instead of a single bet even though only one bet can win per roll. Many players, in order to eliminate the confusion of tossing four chips to the center of the table or having change made while bets are being placed, will make a five-unit \"Horn High\" bet, which is a four-way bet with the extra unit going to one specific number. For example, if one tosses a $5 chip into the center and says \"horn high yo\", you are placing four $1 bets on each of the horn numbers and the extra dollar will go on the yo (11). Horn bets are generally required to be in multiples of 4 or 5 with the minimum bet being 4 times the minimum unit allowed. For example, if the single roll minimum at the table is $1 the Horn bet must be $4 or more.\n\nWhirl or World: A five-unit bet that is a combination of a horn and any-seven bet, with the idea that if a seven is rolled the bet is a push, because the money won on the seven is lost on the horn portions of the bet. The combine odds are 26:5 on the 2, 12, 11:5 on the 3, 11, and a push on the 7. Like the C & E and Horn bet, if a player wishes to take down the bet after a win he or she would receive all five units back. The minimum bet is five of the minimum units. For example, if the minimum single roll bet is $1, the minimum World/Whirl bet is $5.\n\nOn the Hop, Hop or Hopping: A single roll bet on any particular combination of the two dice on the next roll including combinations whose sum is 7 (e.g. 4 and 3). For example, if you bet on \"5 and 1\" on the hop, you are betting that the next roll will have a 5 on one die and a 1 on the other die. The bet pays 15:1 on easy ways (same as a bet on 3 or 11). Hard ways hop pays 30:1 (e.g., 3 and 3 on the hop, same as a bet on 2 or 12). The true odds are 17:1 and 35:1, resulting in a house edge of 11.11% and 13.89% respectively. When presented, hop bets are located at the center of the craps layout with the other proposition bets. If hop bets are not on the craps layout, they still may be bet on by players but they become the responsibility of the boxman to book the bet. Sometimes players may request to hop a whole number. In this case the money on the bet different combinations. For example, if a player says \"hop the tens\" (6-4, 5-5, 4-6) the player must give the dealer an even number bet so it can be divided among the hard and easy ways. If the player gives $10, $5 would be placed on the easy ways 10 with 15:1 odds and $5 would be placed on the hard way with 30:1 odds. If a player wishes to \"hop the sevens\" there would be three different combinations and six possible ways to roll a 7 (6-1, 5-2, 4-3, 3-4, 2-5, 1-6) therefore the player should bet in multiples of 3 so the bet can be divided among each combination with a 15:1 payout minus the other two bets, otherwise if players does not bet in multiples of 3, they would specific which combination has additional units.\n\nField: This bet is a wager that one of the numbers 2, 3, 4, 9, 10, 11, or 12 will appear on the next roll of the dice. This bet typically pays more (2:1 or 3:1) if 2 or 12 is rolled, and 1:1 if 3, 4, 9, 10 or 11 is rolled.\nThe Field bet is a \"Self-Service\" Bet. Unlike the other proposition bets which are handled by the dealers or stickman, the field bet is placed directly by the player.\n\nPlayers identify their Field bets by placing them in the Field area directly in front of them or as close to their position as possible. The initial bet and/or any payouts can \"ride\" through several rolls until they lose, and are assumed to be \"riding\" by dealers. It is thus the player's responsibility to collect their bet and/or winnings immediately upon payout, before the next dice roll, if they do not wish to let it ride.\n\nA player may wish to make multiple different bets. For example, a player may be wish to bet $1 on all hard ways and the horn. If one of the bets win the dealer may automatically replenish the losing bet with profits from the winning bet. In this example, if the shooter rolls a hard 8 (pays 9:1), the horn loses. The dealer may return $5 to the player and place the other $4 on the horn bet which lost. If the player does not want the bet replenished, he or she should request any or all bets be taken down.\n\nA working bet is a live bet. Bets may also be on the board, but not in play and therefore not working. Pass line and come bets are always working meaning the chips are in play and the player is therefore wagering live money. Other bets may be working or not working depending whether a point has been established or player's choice. Place and buy bets are working by default when a point is established and not working when the point is off unless the player specifies otherwise. Lay bets are always working even if a point has not been established unless the player requests otherwise. At any time, a player may wish to take any bet or bets out of play. The dealer will put an \"Off\" button on the player's specific bet or bets, this allows the player to keep his chips on the board without a live wager. For example, if a player decides not to wager a place bet mid-roll but wish to keep the chips on the number, he or she may request the bet be \"not working\" or \"Off\". The chips remain on the table, but the player cannot win from or lose chips which are not working.\n\nThe opposite is also allowed. By default place and buy bets are not working without an established point, a player may wish to wager chips before a point as been established. In this case, the player would request the bet be working in which the dealer will place an \"On\" button on the specified chips.\n\nThe probability of dice combinations determine the odds of the payout. The following chart shows the dice combinations needed to roll each number. The two and twelve are the hardest to roll since only one combination of dice is possible. The game of craps is built around the dice roll of seven, since it is the most easily rolled dice combination.\n\nViewed another way:\n\nThe expected value of all bets is usually negative, such that the average player will always lose money. This is because the house always sets the paid odds to below the actual odds. The only exception is the \"odds\" bet that the player is allowed to make after a point is established on a pass/come don't pass/don't come bet (the odds portion of the bet has a long-term expected value of 0). However, this \"free odds\" bet cannot be made independently, so the expected value of the entire bet, including odds, is still negative. Since there is no correlation between die rolls, there is normally no possible long-term winning strategy in craps.\n\nThere are occasional promotional variants that provide either no house edge or even a player edge. One example is a field bet that pays 3:1 on 12 and 2:1 on either 3 or 11. Overall, given the 5:4 true odds of this bet, and the weighted average paid odds of approximately 7:5, the \"player\" has a 5% advantage on this bet. This is sometimes seen at casinos running limited-time incentives, in jurisdictions or gaming houses that require the game to be fair, or in layouts for use in informal settings using play money. No casino currently runs a craps table with a bet that yields a player edge full-time.\n\nMaximizing the size of the odds bet in relation to the line bet will reduce, but never eliminate the house edge, and will increase variance. Most casinos have a limit on how large the odds bet can be in relation to the line bet, with single, double, and five times odds common. Some casinos offer 3-4-5 odds, referring to the maximum multiple of the line bet a player can place in odds for the points of 4 and 10, 5 and 9, and 6 and 8, respectively. During promotional periods, a casino may even offer 100x odds bets, which reduces the house edge to almost nothing, but dramatically increases variance, as the player will be betting in large betting units.\n\nSince several of the multiple roll bets pay off in ratios of fractions on the dollar, it is important that the player bets in multiples that will allow a correct payoff in complete dollars. Normally, payoffs will be rounded down to the nearest dollar, resulting in a higher house advantage. These bets include all place bets, taking odds, and buying on numbers 6, 8, 5, and 9, as well as laying all numbers.\n\nThese variants depend on the casino and the table, and sometimes a casino will have different tables that use or omit these variants and others.\n\n\nWhen craps is played in a casino, all bets have a house advantage. That is, it can be shown mathematically that a player will (with probability 100%) lose all his or her money to the casino in the long run, while in the short run the player is more likely to lose money than make money. There may be players who are lucky and get ahead for a period of time, but in the long run these winning streaks are eroded away. One can slow, but not eliminate, one's average losses by only placing bets with the smallest house advantage.\n\nThe pass/don't pass line, come/don't come line, place 6, place 8, buy 4 and buy 10 (only under the casino rules where commission is charged only on wins) have the lowest house edge in the casino, and all other bets will, on average, lose money between three and twelve times faster because of the difference in house edges.\n\nThe place bets and buy bets differ from the pass line and come line, in that place bets and buy bets can be removed at any time, since, while they are multi-roll bets, their odds of winning do not change from roll to roll, whereas pass line bets and come line bets are a combination of different odds on their first roll and subsequent rolls. The first roll of a pass line bet is 2:1 advantage for the player (8 wins, 4 losses), but it's \"paid for\" by subsequent rolls that are at the same disadvantage to the player as the don't pass bets were at an advantage. As such, they cannot profitably let you take down the bet after the first roll. Players can bet or lay odds behind an established point depending on whether it was a Pass/Come or Don't Pass/Don't Come to lower house edge by receiving true odds on the point. Casinos which allow put betting allows players to increase or make new pass/come bets after the come-out roll. This bet generally has a higher house edge than place betting, unless the casino offers high odds.\n\nConversely, you \"can\" take back (pick up) a don't pass or don't come bet after the first roll, but this cannot be \"recommended\", because you already endured the disadvantaged part of the combination - the first roll. On that come-out roll, you win just 3 times (2 and 3), while losing 8 of them (7 and 11) and pushing once (12) out of the 36 possible rolls. On the other 24 rolls that become a point, your don't pass bet is now to your advantage by 6:3 (4 and 10), 6:4 (5 and 9) and 6:5 (6 and 8). If a player chooses to remove the initial don't come and/or don't pass line bet, he or she can no longer lay odds behind the bet and cannot re-bet the same don't pass and/or don't come number (players must make a new don't pass or come bets if desired). However, players can still make standard lay bets odds on any of the point numbers (4,5,6,8,9,10).\n\nAmong these, and the remaining numbers and possible bets, there are a myriad of systems and progressions that can be used with many combinations of numbers.\n\nAn important alternative metric is house advantage per roll (rather than per bet), which may be expressed in loss per hour. The typical pace of rolls varies depending on the number of players, but 102 rolls per hour is a cited rate for a nearly full table. This same reference states that only \"29.6% of total rolls are come out rolls, on average\", so for this alternative metric, needing extra rolls to resolve the pass line bet, for example, is factored. This number then permits calculation of rate of loss per hour, and per the 4 day/5 hour per day gambling trip:\n\n\nBesides the rules of the actual game, certain unwritten rules of etiquette exist while playing craps and are expected to be followed. Many consider these guidelines as important as the actual rules themselves. New players should familiarize themselves with them before approaching a craps table.\n\nPlayers are not supposed to handle the dice with more than one hand (such as shaking them in cupped hands before rolling) nor take the dice past the edge of the table. The only way to change hands when throwing dice, if permitted at all, is to set the dice on the table, let go, then take them with the other hand. This reduces or eliminates the possibility of the shooter switching dice by sleight-of-hand.\n\nWhen throwing the dice, the player is expected to hit the farthest wall at the opposite end of the table. Most casinos will allow a roll that does not hit the opposite wall as long as the dice are thrown past the middle of the table. Occasionally a short roll will be called a \"no roll\" due to the more controllable nature of such a roll. The dice may not be slid across the table and must be tossed. Typically, players are asked not to throw the dice higher than the eye level of the dealers.\n\nDice are considered \"in play\" if they land on players' bets on the table, the dealer's working stacks, on the marker puck or with one die resting on top of the other. The roll is invalid if either or both dice land in the boxman's bank, the stickman's bowl (where the extra three dice are kept between rolls), or in the rails around the top of the table where players chips are kept. If a die or both dice leave the table, it is also a \"no roll\" and the boxman will examine the dice before letting them come back into the game. However, the player may request the same die or dice.\n\nWhen either of the dice land on or come to rest leaning against chips, markers, or the side of the table, the number that would be on top if the object the die is leaning on were removed, is the number that is used to make the call.\n\nIf one or both dice hits a player or dealer and rolls back onto the table, the roll counts as long as the person being hit did not interfere with either of the dice, though some casinos will rule \"no roll\" for this situation.\n\nIn most casinos the shooter may \"set\" the dice to a particular starting configuration before throwing (such as showing a particular number or combination, stacking the dice, or spacing them to be picked up between different fingers), but if they do, they are often asked to be quick about it so as not to delay the game. Some casinos have \"no setting\" rules.\n\nDealers are not allowed to touch the players or hand chips directly to a player, and vice versa. If \"buying in\" (paying cash for chips) at the table, players are expected to lay the cash down on the layout, which the dealer will take and then place chips in front of the player.\n\nSome crap table layouts state \"No Call Bets.\" A call bet is made when a player is allowed to make a bet without first placing the necessary chips in the right spot on the table. This might occur while a player is waiting for a marker (casino credit) to arrive, or after the dice have left the center of the table (after which time the players must usually remove their hands from the playing surface).\n\nThe casino may ask a player to leave the table or the casino for any reason.\n\nIt is generally preferable to place chips on the board rather than tossing them. Tossed chips may roll on edge out of the dealer's reach or upset other stacks of chips. A center bet, controlled by the stickman (usually the hardest person to reach) can be made by passing chips to the nearest dealer, who will relay the bet to the stickman. When chips must be tossed it is polite to gain the dealer or stickman's attention and toss as few chips as necessary to cover the bet (a $25 chip is preferable to a stack of five $5 chips). Conversely, it is desirable to have the dealers make change from a bet, rather than make change and then pay correct change for a bet (e.g. pay for a $24 bet with a $25 chip rather than break a $25 chip into four $5 chips and five $1 chips, and pay exactly $24 for the bet).\n\nWhen offered the dice to shoot, a player may pass the dice to the next player without fear of offending anyone; however, at least one player must always be a \"shooter\" betting on either the pass line or don't pass line for the game to continue.\n\nWhen tipping, the most common way is simply to toss chips onto the table and say, \"For the dealers\" or \"For the boys\" (the second is considered acceptable even though dealers often are women; by the same token, female stickmen and boxmen are still referred to as such; not for example, boxwoman or stickperson). It's also common to place a bet for the dealers. If the bet is one handled by the dealers, such as a Place bet or one of the proposition bets handled by the stick-man, the chip(s) should be placed, or thrown, and announced as a dealer bet, such as \"Dealer's hard eight\", or \"Place the eight for the dealers\".\n\nA \"two-way\" bet is one that is part for the player and part for the dealers (for example, tossing two chips and stating \"Two Way Hard Eight\" will place a bet for the player and the same bet for the dealer). Usually, the dealers' bet is smaller than the player's bet, but it is appreciated. The part of the bet for the dealer is called a \"toke\" bet; this is from the $1 slot machine coins or tokens that are sometimes used to place bets for the dealers in a casino.\n\nMost casinos require the dealers to pick up their winning bets, including the original tip, rather than \"let it ride\" as the player may choose to do. If the player wants the original dealer bet to remain in place, the phrase \"\"I control the bet\"\" should be clearly stated by the tipper, and acknowledged by one of the crew, immediately upon announcing the dealer bet.\n\nThis indicates that any winnings for that bet will be picked up by the dealers, and the original amount will remain in play until cleared by a loss or retracted by the player after a win (such as a single-roll bet that would normally be returned to the player with their winnings). It should be noted that because the house has an advantage on all bets (and in the case of some bets, a considerable edge) the dealers will ultimately receive a smaller tip from placed bets than from a direct tip.\n\nThe $1 \"yo\" (eleven) bet, split with the dealers on come-out rolls by calling out \"two-way yo\", tends to be a favorite with many players as means of tipping the dealers without giving up too much per gambling trip. If eleven comes out on the come out roll, the pass line win bets and the more substantial \"yo\" bet splits.\n\nAfter the come-out roll, it is considered bad luck to say the word \"seven\". A common \"nickname\" for this number is \"Big Red\", or just \"Red\".\n\nIt is considered bad luck to change dice in the middle of a roll. If one or both dice leave the table during a roll, and the shooter does not want a new die (or dice) substituted into the game, the shooter should immediately and clearly call \"Same Dice!\" The retrieved die (or dice) will then be returned to play after close inspection by the boxman. To speed play, most casinos will immediately begin the process of introducing new dice unless the shooter has requested otherwise, though some casinos will inspect and return the dice by default.\n\nProposition bets, the bets in the center of the table, are made by tossing chips to the center of the table and calling out the intended bet; the stickman will then place the chips correctly for the player. As mentioned above, care should be taken when tossing chips. Players furthest from the stickman can often elect to place a center bet with a dealer who will relay the bet to the center. Chips will be less likely to roll on edge if they are tossed with a gentle frisbee-like spin.\n\nIt is considered rude to \"late bet\", or make wagers while the dice are no longer in the middle of the table. While entirely permissible, excessive late betting will generally garner a warning as it slows play. At the discretion of the boxman or a \"pit boss\", dealers can disallow a bet made after the dice have left the center.\n\nFood, drinks, cigarettes, and other items should remain off the chip rail and should not be held over the table.\n\nPlayers feel it is bad luck for the shooter to leave the table after a successful come-out roll. A shooter retains the right to roll and is expected to continue rolling until he or she sevens out. If the shooter leaves the game before a decision is reached on a point number, the dice will be passed to the next player to continue where the shooter left off. Once a decision is reached, the \"substitute\" shooter can, at the discretion of the boxman, continue to roll the dice for a new \"come out\" as would have been the case had the previous shooter completed their roll.\n\nWhen the shooter is ready to roll, players should remove their hands from the table area in order to avoid interfering with the dice. The stickman will often say \"hands high, let 'em fly\" or \"dice are out, hands high\". Many players will suggest that a die that hits another player's hand or a stack of chips will be more likely to seven out. This is likely a case of confirmation bias; however, for the sake of a harmonious table, care should be taken to keep hands free of the play area.\n\nWhen making bets in the field or on the Big 6 or Big 8, it is the player's responsibility to track his or her bet. Place bets and Come Line bets will be tracked by the dealer, who will pay the player directly. Hardway and other proposition bets are tracked by the stickman and will be paid after the regular bets by the dealer to the player directly based on instructions from the stickman.\n\nWhen betting the \"wrong way\" by making don't pass and don't come bets, it is bad etiquette to cheer or clap if one wins those bets. In general, most people bet the \"right way\" on pass and come and can lose quite a bit of money on seven outs.\n\nThe phrase “barber pole” is derisive jargon in craps, and refers to the commingling of “gaming cheques of different denominations”. Wagers that combine different denominations are “supposed to be stacked with the highest denomination at the bottom\".\n\nWhen leaving a table it is generally considered bad form for the player to take a large stack of small-denomination chips. The player should instead wait until a natural break in play (such as the shooter sevening out) and then place the stack of chips on the playing surface and ask the dealer to \"color up\". Small denomination chips will be exchanged for large denominations, a process which may be verified by the pit boss, and the large denominations are returned to the player.\n\nNo wagering system can consistently beat casino games of pure chance such as craps, but that does not stop hopeful gamblers believing in them. One of the best known systems is the Martingale System, in which the player starts by betting a given amount, for instance $1, and doubles his bet whenever he loses. Upon winning, he starts over at the initial amount. The idea is to realize a net win equal to the initial amount after every eventual win.\n\nThis system fails because the player will either run out of money after having to double his bet several times in a row after a streak of losing bets, or he will be unable to bet the amount dictated by the system because it would exceed the maximum bet allowed by the casino. The Martingale system also only yields a profit equal to the initial bet amount every time the player wins. If the initial amount is small, the payout from each Martingale sequence will be just as small.\n\nOther systems depend on the gambler's fallacy, which in craps terms is the belief that past dice rolls influence the probabilities of future dice rolls. For example, the gambler's fallacy indicates that a craps player should bet on eleven if an eleven has not appeared or has appeared too often in the last 20 rolls. In practice this can be observed as players respond to a roll such as a Hard Six with an immediate wager on the Hard Six.\n\nIn reality, each roll of the dice is an independent event, so the probability of rolling eleven is exactly 1/18 on every roll, regardless of the number of times eleven has come up in the last x rolls. Even if the dice are actually biased toward particular results (\"loaded\"), each roll is still independent of all the previous ones. The common term to describe this is \"dice have no memory\".\n\nThe parity hedge system is a hoax promulgated by Quatloos. Despite the fact that no such system exists (indeed, it is a mathematical impossibility), several gambling-related web sites have retold the 'parity hedge' story without attribution.\n\nAnother approach is to \"set\" the dice in a particular orientation, and then throw them in such a manner that they do not tumble randomly. The theory, based on , is that given exactly the same throw from exactly the same starting configuration, the dice will tumble in the same way and therefore show the same or similar values every time. Unlike other systems, this one is mathematically plausible, because if it were possible to alter the probabilities of each outcome, then winning systems could be devised.\n\nCasinos do take steps to prevent this. The dice are usually required to hit the back wall of the table, which is normally faced with an angular texture such as pyramids, making controlled spins more difficult. Whether it is possible for human beings to consistently exercise the precise physical control necessitated by theory is a source of controversy. A small but dedicated community of controlled shooters maintains records and claim proof of dice influencing in casino conditions. Frank Scoblete, Stanford Wong, and Jerry L. Patterson, authors of books that feature dice control techniques, believe that it is possible to alter the odds in the player's favor by dice control.\n\nChris Pawlicki, a mechanical engineer who (under the pseudonym \"Sharpshooter\") wrote a book on dice setting called \"Get The Edge At Craps: How to Control the Dice\" as a part of the Frank Scoblete \"Get the Edge Guides\", defined the math and science behind dice control.\n\nIn addition, some people offer to teach dice-setting skills for a substantial fee. Currently there has been no independent conclusive evidence that such methods can be successfully applied in a real casino.\n\nBank craps is a variation of the original craps game and is sometimes known as Las Vegas Craps. This variant is quite popular in Nevada gambling houses, and its availability online has now made it a globally played game. Bank craps uses a special table layout and all bets must be made against the house. In Bank Craps, the dice are thrown over a wire or a string that is normally stretched a few inches from the table’s surface. The lowest house edge (for the pass/don't pass) in this variation is around 1.4%. Generally, if the word \"craps\" is used without any modifier, it can be inferred to mean this version of the game, to which most of this article refers.\n\nCrapless craps, also known as Bastard Craps, is a simple version of the original craps game, and is normally played as an online private game. The biggest difference between crapless craps and original craps, is that the shooter (person throwing the dice) is at a far greater disadvantage and has a house edge of 5.38%. Another difference is that this is one of the craps games in which a player can bet on rolling a 2, 3, 11 or 12 before a 7 is thrown. In crapless craps, 2 and 12 have odds of 11:2 and have a house edge of 7.143% while 3 and 11 have odds of 11:4 with a house edge of 6.25%.\n\nDie Rich Craps, also known as Fading Craps, Open Craps, or Money Craps, is a more recent version of the craps game, and is played using a single die. These variants are usually considered to be games involving big money, and are most commonly played in private. Die Rich Craps involves specific bets made against the book. The book keeps a specific percentage of the total amount of money wagered (5%–7%) and this is called vigorish. In the online and offline gambling circuits, this variation of craps is considered an illegal game. The craps table in this variant will always consist of a Win Line, Lose Line, and box numbers of 4, 5, 6, 8, 9 and 10.\n\nHigh Point Craps is another version of the original Craps game. The initial roll of a 2 or a 3 in High Point Craps is ignored. If a player rolls a 2 then the player will roll again. If a player rolls 11 or 12, the player wins. Any other total rolled, is considered as 1 point and the player rolls again. This time needing to roll a total that is higher than 11 or 12 to win. The house edge in this craps game variation is 2.35%.\n\nNew York Craps is one of the variations of craps played mostly in the Eastern coast of the USA, true to its name. History states that this game was actually found and played in casinos in Yugoslavia, the UK and the Bahamas. In this craps variant, the house edge is greater than Las Vegas Craps or Bank craps. The table layout is also different, and is called a double-end-dealer table. This variation is different from the original craps game in several ways, but the primary difference is that New York craps doesn’t allow Come or Don’t Come bets. New York Craps Players bet on box numbers like 4, 5, 6, 8, 9 or 10. The overall house edge in New York craps is 5%.\n\nSimplified Craps is a variation that can be won by rolling 2, 3, 4, 10, 11 or 12, but if a 5, 6, 7, 8 or 9 is rolled, the player loses. Simplified Craps has an overall house edge of 2.8%.\n\nIn order to get around California laws barring the payout of a game being directly related to the roll of dice, Indian reservations have adapted the game to substitute cards for dice.\n\nIn one variation, there are no dice at all. Two shoes are used, each containing some number of regular card decks that have been stripped down to just the Aces and deuces through sixes. The boxman simply deals one card from each shoe and that is the roll on which bets are settled. Since a card-counting scheme is easily devised to make use of the information of cards that have already been dealt, a relatively small portion (less than 50%) of each shoe is usually dealt in order to protect the house.\n\nIn a similar variation, cards representing dice are dealt directly from a continuous shuffling machine (CSM). Typically, the CSM will hold approximately 264 cards, or 44 sets of 1 through 6 spot cards. Two cards are dealt from the CSM for each roll. The game is played exactly as regular craps, but the roll distribution of the remaining cards in the CSM is slightly skewed from the normal symmetric distribution of dice.\n\nEven if the dealer were to shuffle each roll back into the CSM, the effect of buffering a number of cards in the chute of the CSM provides information about the skew of the next roll. Analysis shows this type of game is biased towards the don't pass and don't come bets. A player betting don't pass and don't come every roll and laying 10x odds receives a 2% profit on the initial don't pass / don't come bet each roll. Using a counting system allows the player to attain a similar return at lower variance.\n\nTo replicate the original dice odds exactly without dice or possibility of card-counting, another scheme uses two shuffle machines with just one deck of Ace through 6 each. Each machine selects one of the 6 cards at random and this is the roll. The selected cards are replaced and the decks are reshuffled for the next roll.\n\nIn this game variation, one red deck and one blue deck of six cards each (A through 6), and a red die and a blue die are used. Each deck is shuffled separately, usually by machine. Each card is then dealt onto the layout, into the 6 red and 6 blue numbered boxes. The shooter then shoots the dice. The red card in the red-numbered box corresponding to the red die, and the blue card in the blue-numbered box corresponding to the blue die are then turned over to form the roll on which bets are settled.\n\nAnother variation uses a red and a blue deck of 36 custom playing cards each. Each card has a picture of a two-die roll on it - from 1-1 to 6-6. The shooter shoots what looks like a red and a blue die, called \"cubes\". They are numbered such that they can never throw a pair, and that the blue one will show a higher value than the red one exactly half the time. One such scheme could be 222555 on the red die and 333444 on the blue die.\n\nOne card is dealt from the red deck and one is dealt from the blue deck. The shooter throws the \"cubes\" and the color of the cube that is higher selects the color of the card to be used to settle bets. On one such table, an additional one-roll prop bet was offered: If the card that was turned over for the \"roll\" was either 1-1 or 6-6, the other card was also turned over. If the other card was the \"opposite\" (6-6 or 1-1, respectively) of the first card, the bet paid 500:1 for this 647:1 proposition.\n\nAnd additional variation uses a single set of 6 cards, and regular dice. The roll of the dice maps to the card in that position, and if a pair is rolled, then the mapped card is used twice, as a pair.\n\nRecreational or informal playing of craps outside of a casino is referred to as \"street craps\" or \"private craps\". The most notable difference between playing street craps and bank craps is that there is no bank or house to cover bets in street craps. Players must bet against each other by covering or \"fading\" each other's bets for the game to be played. If money is used instead of chips and depending on the laws of where it is being played, street craps can be an illegal form of gambling.\n\nThere are many variations of street craps. The simplest way is to either agree on or roll a number as the point, then roll the point again before you roll a seven. Unlike more complex proposition bets offered by casinos, street craps has more simplified betting options. The shooter is required to make either a \"Pass\" or a \"Don't Pass\" bet if he wants to roll the dice. Another player must choose to cover the shooter to create a stake for the game to continue.\n\nIf there are several players, the rotation of the player who must cover the shooter may change with the shooter (comparable to a blind in poker). The person covering the shooter will always bet against the shooter. For example, if the shooter made a \"Pass\" bet, the person covering the shooter would make a \"Don't Pass\" bet to win. Once the shooter is covered, other players may make Pass/Don't Pass bets, or any other proposition bets, as long as there is another player willing to cover.\n\nDue to the random nature of the game, in popular culture \"a crapshoot\" is often used to describe an action with an unpredictable outcome.\n\nA Golden Arm is a craps player who rolls the dice for longer than one hour without losing. The first Golden Arm was Stanley Fujitake, who rolled for three hours and six minutes at the California Hotel and Casino in 1989.\n\nThe current record for length of a \"hand\" (successive rounds won by the same shooter) is 154 rolls including 25 passes by Patricia DeMauro of New Jersey, lasting 4 hours and 18 minutes, at The Borgata in Atlantic City, New Jersey, on May 23–24, 2009. She bested by over an hour the record held for almost 20 years – that of Fujitake.\n\nThe prayer or invocation \"Baby needs a new pair of shoes!\" is associated with shooting craps.\n\nNotes\n"}
{"id": "6066", "url": "https://en.wikipedia.org/wiki?curid=6066", "title": "Carl von Clausewitz", "text": "Carl von Clausewitz\n\nCarl Philipp Gottfried (or Gottlieb) von Clausewitz (; 1 June 1780 – 16 November 1831) was a Prussian general and military theorist who stressed the \"moral\" (meaning, in modern terms, psychological) and political aspects of war. His most notable work, \"Vom Kriege\" (\"On War\"), was unfinished at his death.\nClausewitz was a realist in many different senses and, while in some respects a romantic, also drew heavily on the rationalist ideas of the European Enlightenment.\n\nClausewitz's thinking is often described as Hegelian because of his dialectical method; but, although he was probably personally acquainted with Hegel, there remains debate as to whether or not Clausewitz was in fact influenced by him. He stressed the dialectical interaction of diverse factors, noting how unexpected developments unfolding under the \"fog of war\" (i.e., in the face of incomplete, dubious, and often completely erroneous information and high levels of fear, doubt, and excitement) call for rapid decisions by alert commanders. He saw history as a vital check on erudite abstractions that did not accord with experience. In contrast to the early work of Antoine-Henri Jomini, he argued that war could not be quantified or reduced to mapwork, geometry, and graphs. Clausewitz had many aphorisms, of which the most famous is \"War is the continuation of politics by other means.\"\n\nClausewitz's Christian names are sometimes given in non-German sources as Karl, \"Carl Philipp Gottlieb,\" or \"Carl Maria.\" He spelled his own given name with a \"C\" in order to identify with the classical Western tradition; writers who use \"Karl\" are often seeking to emphasize his German (rather than European) identity. \"Carl Philipp Gottfried\" appears on Clausewitz's tombstone. Nonetheless, sources such as military historian Peter Paret and \"Encyclopædia Britannica\" use Gottlieb instead of Gottfried.\n\nClausewitz was born on 1 June 1780 in Burg bei Magdeburg in the Prussian Duchy of Magdeburg as the fourth and youngest son of a middle-class family, though it made claims to noble status that Carl accepted. Clausewitz's family claimed descent from the Barons of Clausewitz in Upper Silesia, though scholars question the connection. His grandfather, the son of a Lutheran pastor, had been a professor of theology. Clausewitz's father, once a lieutenant in the Prussian army of Frederick II of Prussia (Frederick the Great), held a minor post in the Prussian internal-revenue service. Clausewitz entered the Prussian military service at the age of twelve as a Lance-Corporal, eventually attaining the rank of Major-General.\n\nClausewitz served in the Rhine Campaigns (1793–1794) including the Siege of Mainz, when the Prussian army invaded France during the French Revolution, and fought in the Napoleonic Wars from 1806 to 1815. He entered the \"Kriegsakademie\" (also cited as \"The German War School\", the \"Military Academy in Berlin\", and the \"Prussian Military Academy\") in Berlin in 1801 (aged 21), probably studied the writings of the philosopher Immanuel Kant, and won the regard of General Gerhard von Scharnhorst, the future first chief-of-staff of the newly reformed Prussian Army (appointed 1809). Clausewitz, Hermann von Boyen (1771–1848) and Karl von Grolman (1777–1843) were among Scharnhorst's primary allies in his efforts to reform the Prussian army between 1807 and 1814.\n\nClausewitz served during the Jena Campaign as aide-de-camp to Prince August. At the Battle of Jena-Auerstedt on 14 October 1806 – when Napoleon invaded Prussia and defeated the massed Prussian-Saxon army commanded by Karl Wilhelm Ferdinand, Duke of Brunswick - he was captured, one of the 25,000 prisoners taken that day as the Prussian army disintegrated. He was 26. Clausewitz was held prisoner with his prince in France from 1807 to 1808. Returning to Prussia, he assisted in the reform of the Prussian army and state.\nOn 10 December 1810 he married the socially prominent Countess Marie von Brühl, whom he had first met in 1803. She was a member of the noble German von Brühl family originating in Thuringia. The couple moved in the highest circles, socializing with Berlin's political, literary and intellectual élite. Marie was well-educated and politically well-connected—she played an important role in her husband's career progress and intellectual evolution. She also edited, published, and introduced his collected works.\n\nOpposed to Prussia's enforced alliance with Napoleon I, Clausewitz left the Prussian army and served in the Imperial Russian Army from 1812 to 1813 during the Russian Campaign, taking part in the Battle of Borodino (1812). Like many Prussian officers serving in Russia, he joined the Russian-German Legion in 1813. In the service of the Russian Empire, Clausewitz helped negotiate the Convention of Tauroggen (1812), which prepared the way for the coalition of Prussia, Russia, and the United Kingdom that ultimately defeated Napoleon and his allies.\n\nIn 1815 the Russian-German Legion became integrated into the Prussian Army and Clausewitz re-entered Prussian service as a colonel. He was soon appointed chief-of-staff of Johann von Thielmann's III Corps. In that capacity he served at the Battle of Ligny and the Battle of Wavre during the Waterloo Campaign in 1815. An army led personally by Napoleon defeated the Prussians at Ligny (south of Mont-Saint-Jean and the village of Waterloo) on 16 June 1815, but Napoleon's failure to destroy the Prussian forces led to his defeat a few days later at the Battle of Waterloo (18 June 1815), when the Prussian forces unexpectedly arrived on his right flank late in the afternoon to support the Anglo-Dutch-Belgian forces pressing his front. Clausewitz's unit fought at Wavre (18–19 June 1815), preventing large reinforcements from reaching Napoleon at Waterloo. After the war Clausewitz served as the director of the \"Kriegsakademie\", where he served until 1830. In that year he returned to duty with the army. Soon afterwards, the outbreak of several revolutions around Europe and a crisis in Poland appeared to presage another major European war. Clausewitz was appointed chief of staff of the only army Prussia was able to mobilize in this emergency, which was sent to the Polish border. Its commander, Gneisenau, died of cholera (August 1831), and Clausewitz took command of the Prussian army's efforts to construct a \"cordon sanitaire\" to contain the great cholera outbreak (the first time cholera had appeared in modern heartland Europe, causing a continent-wide panic). Clausewitz himself died of the same disease shortly afterwards, on 17 November 1831.\n\nHis widow edited, published, and wrote the introduction to his \"magnum opus\" on the philosophy of war in 1832. (He had started working on the text in 1816, but had not completed it.) She wrote the preface for \"On War\" and by 1835 had published most of his collected works. She died in January 1835.\n\nClausewitz was a professional combat soldier who was involved in numerous military campaigns, but he is famous primarily as a military theorist interested in the examination of war, utilizing the campaigns of Frederick the Great and Napoleon as frames of reference for his work. He wrote a careful, systematic, philosophical examination of war in all its aspects. The result was his principal book, \"On War,\" a major work on the philosophy of war. It was unfinished when Clausewitz died and contains material written at different stages in his intellectual evolution, producing some significant contradictions between different sections. The sequence and precise character of that evolution is a source of much debate, as are exact meaning behind his seemingly contradictory claims (discussions pertinent to the tactical, operational and strategic levels of war are one example). Clausewitz constantly sought to revise the text, particularly between 1827 and his departure on his last field assignments, to include more material on \"people's war\" and forms of war other than high-intensity warfare between states, but relatively little of this material was included in the book. Soldiers before this time had written treatises on various military subjects, but none had undertaken a great philosophical examination of war on the scale of those written by Clausewitz and Leo Tolstoy, both of which were inspired by the events of the Napoleonic Era.\n\nClausewitz's work is still studied today, demonstrating its continued relevance. More than sixteen major English-language books that focused specifically on his work were published between 2005 and 2014, whereas his 19th-century rival Jomini faded from influence. Historian Lynn Montross said the outcome, \"may be explained by the fact that Jomini produced a system of war, Clausewitz a philosophy. The one has been outdated by new weapons, the other still influences the strategy behind those weapons.\" Jomini did not attempt to define war. Clausewitz did, providing (and dialectically comparing) a number of definitions. The first is his dialectical thesis: \"War is thus an act of force to compel our enemy to do our will.\" The second, often treated as Clausewitz's 'bottom line,' is in fact merely his dialectical antithesis: \"War is merely the continuation of policy by other means.\" The synthesis of his dialectical examination of the nature of war is his famous \"trinity,\" saying that war is \"a fascinating trinity—composed of primordial violence, hatred, and enmity, which are to be regarded as a blind natural force; the play of chance and probability, within which the creative spirit is free to roam; and its element of subordination, as an instrument of policy, which makes it subject to pure reason.\" Christopher Bassford says the best shorthand for Clausewitz's trinity should be something like \"violent emotion/chance/rational calculation.\" However, it is frequently presented as \"people/army/government,\" a misunderstanding based on a later paragraph in the same chapter. This misrepresentation was popularized by U.S. Army Colonel Harry Summers' Vietnam-era interpretation, facilitated by weaknesses in the 1976 Howard/Paret translation.\n\nThe degree to which Clausewitz managed to revise his manuscript to reflect that synthesis is the subject of much debate. His final reference to war and \"Politik\", however, goes beyond his widely quoted antithesis: \"War is simply the continuation of political intercourse with the addition of other means. We deliberately use the phrase 'with the addition of other means' because we also want to make it clear that war in itself does not suspend political intercourse or change it into something entirely different. In essentials that intercourse continues, irrespective of the means it employs. The main lines along which military events progress, and to which they are restricted, are political lines that continue throughout the war into the subsequent peace.\"\n\nClausewitz introduced systematic philosophical contemplation into Western military thinking, with powerful implications not only for historical and analytical writing but also for practical policy, military instruction, and operational planning. He relied on his own experiences, contemporary writings about Napoleon, and on deep historical research. His historiographical approach is evident in his first extended study, written when he was 25, of the Thirty Years War. He rejects the Enlightenment's view of the war as a chaotic muddle and instead explains its drawn-out operations by the economy and technology of the age, the social characteristics of the troops, and the commanders' politics and psychology. In \"On War\", Clausewitz sees all wars as the sum of decisions, actions, and reactions in an uncertain and dangerous context, and also as a socio-political phenomenon. He also stressed the complex nature of war, which encompasses both the socio-political and the operational and stresses the primacy of state policy.\n\nThe word \"strategy\" had only recently come into usage in modern Europe, and Clausewitz's definition is quite narrow: \"the use of engagements for the object of war.\" Clausewitz conceived of war as a political, social, and military phenomenon which might — depending on circumstances — involve the entire population of a nation at war. In any case, Clausewitz saw military force as an instrument that states and other political actors use to pursue the ends of policy, in a dialectic between opposing wills, each with the aim of imposing his policies and will upon his enemy.\n\nClausewitz's emphasis on the inherent superiority of the defense suggests that habitual aggressors are likely to end up as failures. The inherent superiority of the defense obviously does not mean that the defender will always win, however: there are other asymmetries to be considered. He was interested in cooperation between the regular army and militia or partisan forces, or citizen soldiers, as one possible — sometimes the only — method of defense. In the circumstances of the Wars of the French Revolution and with Napoleon, which were energized by a rising spirit of nationalism, he emphasized the need for states to involve their entire populations in the conduct of war. This point is especially important, as these wars demonstrated that such energies could be of decisive importance and for a time led to a democratization of the armed forces much as universal suffrage democratized politics.\n\nWhile Clausewitz was intensely aware of the value of intelligence at all levels, he was also very skeptical of the accuracy of much military intelligence: \"Many intelligence reports in war are contradictory; even more are false, and most are uncertain... In short, most intelligence is false.\" This circumstance is generally described as part of the fog of war. Such skeptical comments apply only to intelligence at the tactical and operational levels; at the strategic and political levels he constantly stressed the requirement for the best possible understanding of what today would be called strategic and political intelligence. His conclusions were influenced by his experiences in the Prussian Army, which was often in an intelligence fog due partly to the superior abilities of Napoleon's system but even more to the nature of war. Clausewitz acknowledges that friction creates enormous difficulties for the realization of any plan, and the \"fog of war\" hinders commanders from knowing what is happening. It is precisely in the context of this challenge that he develops the concept of military genius, whose capabilities are seen above all in the execution of operations. 'Military genius' is not simply a matter of intellect, but a combination of qualities of intellect, experience, personality, and temperament (and there are many possible such combinations) that create a very highly developed mental aptitude for the waging of war.\n\nKey ideas discussed in \"On War\" include:\n\nClausewitz used a dialectical method to construct his argument, leading to frequent misinterpretation of his ideas. British military theorist B. H. Liddell Hart contends that the enthusiastic acceptance by the Prussian military establishment – especially Moltke the Elder, a former student of his – of what they believed to be Clausewitz's ideas, and the subsequent widespread adoption of the Prussian military system worldwide, had a deleterious effect on military theory and practice, due to their egregious misinterpretation of his ideas:\n\nAs so often happens, Clausewitz's disciples carried his teaching to an extreme which their master had not intended... [Clausewitz's] theory of war was expounded in a way too abstract and involved for ordinary soldier-minds, essentially concrete, to follow the course of his argument – which often turned back from the direction in which it was apparently leading. Impressed yet befogged, they grasped at his vivid leading phrases, seeing only their surface meaning, and missing the deeper current of his thought.\n\nAs described by Christopher Bassford, then professor of strategy at the National War College of the United States:\n\nOne of the main sources of confusion about Clausewitz's approach lies in his dialectical method of presentation. For example, Clausewitz's famous line that \"War is a mere continuation of politics by other means,\" (\"Der Krieg ist eine bloße Fortsetzung der Politik mit anderen Mitteln\") while accurate as far as it goes, was not intended as a statement of fact. It is the antithesis in a dialectical argument whose thesis is the point – made earlier in the analysis – that \"war is nothing but a duel [or wrestling match, a better translation of the German \"Zweikampf\"] on a larger scale.\" His synthesis, which resolves the deficiencies of these two bold statements, says that war is neither \"nothing but\" an act of brute force nor \"merely\" a rational act of politics or policy. This synthesis lies in his \"fascinating trinity\" [wunderliche Dreifaltigkeit]: a dynamic, inherently unstable interaction of the forces of violent emotion, chance, and rational calculation.\n\nAnother example of this confusion is the idea that Clausewitz was a proponent of total war as used in the Third Reich's propaganda in the 1940s. In fact, he never used the term \"total war\": rather, he discussed \"absolute war\" or \"ideal war\" as the purely \"logical\" result of the forces underlying a \"pure,\" Platonic \"ideal\" of war. In what he called a \"logical fantasy,\" war cannot be waged in a limited way: the rules of competition will force participants to use all means at their disposal to achieve victory. But in the \"real world\", he said, such rigid logic is unrealistic and dangerous. As a practical matter, the military objectives in \"real\" war that support political objectives generally fall into two broad types: \"war to achieve limited aims\"; and war to \"disarm\" the enemy, \"to render [him] politically helpless or militarily impotent.\" Thus the complete defeat of the enemy may not be necessary, desirable, or even possible.\n\nIn modern times the reconstruction of Clausewitzian theory has been a matter of much dispute. One analysis was that of Panagiotis Kondylis, a Greek-German writer and philosopher, who opposed the interpretations of Raymond Aron in \"Penser la Guerre, Clausewitz\", and other liberal writers. According to Aron, Clausewitz was one of the first writers to condemn the militarism of the Prussian general staff and its war-proneness, based on Clausewitz's argument that \"war is a continuation of politics by other means.\" In \"Theory of War,\" Kondylis claims that this is inconsistent with Clausewitzian thought. He claims that Clausewitz was morally indifferent to war (though this probably reflects a lack of familiarity with personal letters from Clausewitz, which demonstrate an acute awareness of war's tragic aspects) and that his advice regarding politics' dominance over the conduct of war has nothing to do with pacifist ideas. For Clausewitz, war is simply one unique means that is sometimes applied to the eternal quest for power, of \"raison d'État\" in an anarchic and unsafe world.\n\nOther notable writers who have studied Clausewitz's texts and translated them into English are historians Peter Paret of the Institute for Advanced Study and Sir Michael Howard, and the philosopher, musician, and game theorist Anatol Rapoport. Howard and Paret edited the most widely used edition of \"On War\" (Princeton University Press, 1976/1984) and have produced comparative studies of Clausewitz and other theorists, such as Tolstoy. Bernard Brodie's \"A Guide to the Reading of \"On War\"\", in the 1976 Princeton translation, expressed his interpretations of the Prussian's theories and provided students with an influential synopsis of this vital work.\n\nThe British military historian John Keegan attacked Clausewitz's theory in his book \"A History of Warfare\". Keegan argued that Clausewitz assumed the existence of states, yet 'war antedates the state, diplomacy and strategy by many millennia.'\n\nClausewitz died without completing \"On War\", but despite this his ideas have been widely influential in military theory and have had a strong influence on German military thought specifically. Later Prussian and German generals, such as Helmuth Graf von Moltke, were clearly influenced by Clausewitz: Moltke's widely quoted statement that \"No campaign plan survives first contact with the enemy\" is a classic reflection of Clausewitz's insistence on the roles of chance, friction, \"fog,\" uncertainty, and interactivity in war.\n\nClausewitz's influence spread to British thinking as well, though at first more as a historian and analyst than as a theorist. See for example Wellington's extended essay discussing Clausewitz's study of the Campaign of 1815—Wellington's only serious written discussion of the battle, which was widely discussed in 19th-century Britain. Clausewitz's broader thinking came to the fore following Britain's military embarrassments in the Boer War (1899–1902). One example of a heavy Clausewitzian influence in that era is Spenser Wilkinson, journalist, the first Chichele Professor of Military History at Oxford University, and perhaps the most prominent military analyst in Britain from c. 1885 until well into the interwar period. Another is naval historian Julian Corbett (1854–1922), whose work reflected a deep if idiosyncratic adherence to Clausewitz's concepts and frequently an emphasis on Clausewitz's ideas about 'limited war' and the inherent strengths of the defensive form of war. Corbett's practical strategic views were often in prominent public conflict with Wilkinson's--see, for example, Wilkinson's article \"Strategy at Sea,\" \"The Morning Post\", 12 February 1912. Following the First World War, however, the influential British military commentator B. H. Liddell Hart in the 1920s erroneously attributed to him the doctrine of \"total war\" that during the First World War had been embraced by many European general staffs and emulated by the British. More recent scholars typically see that war as so confused in terms of political rationale that it in fact contradicts much of \"On War.\" One of the most influential British Clausewitzians today is Colin S. Gray; historian Hew Strachan (like Wilkinson also the Chichele Professor of Military History at Oxford University, since 2001) has been an energetic proponent of the \"study\" of Clausewitz, but his own views on Clausewitz's ideas are somewhat ambivalent.\n\nWith some interesting exceptions (e.g., John McAuley Palmer, Robert M. Johnston, Hoffman Nickerson), Clausewitz had little influence on American military thought before 1945 other than via British writers, though Generals Eisenhower and Patton were avid readers. He did influence Karl Marx, Friedrich Engels, Vladimir Lenin, Leon Trotsky and Mao Zedong, and thus the Communist Soviet and Chinese traditions, as Lenin emphasized the inevitability of wars among capitalist states in the age of imperialism and presented the armed struggle of the working class as the only path toward the eventual elimination of war. Because Lenin was an admirer of Clausewitz and called him \"one of the great military writers\", his influence on the Red Army was immense. The Russian historian A.N. Mertsalov commented that \"It was an irony of fate that the view in the USSR was that it was Lenin who shaped the attitude towards Clausewitz, and that Lenin's dictum that war is a continuation of politics is taken from the work of this [allegedly] anti-humanist anti-revolutionary.\" The American mathematician Anatol Rapoport wrote in 1968 that Clausewitz as interpreted by Lenin formed the basis of all Soviet military thinking since 1917, and quoted the remarks by Marshal V.D. Sokolovsky:\n\nIn describing the essence of war, Marxism-Leninism takes as its point of departure the premise that war is not an aim in itself, but rather a tool of politics. In his remarks on Clausewitz's \"On War\", Lenin stressed that \"Politics is the reason, and war is only the tool, not the other way around. Consequently, it remains only to subordinate the military point of view to the political\".\n\nHenry A. Kissinger, however, described Lenin's approach as being that politics is a continuation of war by other means, thus turning Clausewitz's argument \"on its head.\"\n\nRapoport argued that:\n\nAs for Lenin's approval of Clausewitz, it probably stems from his obsession with the struggle for power. The whole Marxist conception of history is that of successive struggles for power, primarily between social classes. This was constantly applied by Lenin in a variety of contexts. Thus the entire history of philosophy appears in Lenin's writings as a vast struggle between \"idealism\" and \"materialism\". The fate of the socialist movement was to be decided by a struggle between the revolutionists and the reformers. Clausewitz's acceptance of the struggle for power as the essence of international politics must had impressed Lenin as starkly realistic.\n\nClausewitz directly influenced Mao Zedong, who read \"On War\" in 1938 and organized a seminar on Clausewitz for the Party leadership in Yan'an. Thus the \"Clausewitzian\" content in many of Mao's writings is not merely a regurgitation of Lenin but reflects Mao's own in-depth study.\nThe idea that war involves inherent \"friction\" that distorts, to a greater or lesser degree, all prior arrangements, has become common currency in fields such as business strategy and sport. The phrase \"fog of war\" derives from Clausewitz's stress on how confused warfare can seem while immersed within it. The term center of gravity, used in a military context derives from Clausewitz's usage, which he took from Newtonian Mechanics. In U.S. military doctrine, \"center of gravity\" refers to the basis of an opponent's power at the operational, strategic, or political level, though this is only one aspect of Clausewitz's use of the term.\n\nThe deterrence strategy of the United States in the 1950s was closely inspired by President Dwight Eisenhower’s reading of Clausewitz as a young officer in the 1920s. Eisenhower was greatly impressed by Clausewitz’s example of a theoretical, idealized “absolute war” in \"Vom Krieg\" as a way of demonstrating how absurd it would be to attempt such a strategy in practice. For Eisenhower, the age of nuclear weapons had made what was for Clausewitz in the early 19th century only a theoretical vision an all too real possibility in the mid-20th century. From Eisenhower’s viewpoint, the best deterrent to war was to show the world just how appalling and horrific a nuclear “absolute war” would be if it should ever occur, hence a series of much publicized nuclear tests in the Pacific, giving first priority in the defense budget to nuclear weapons and delivery systems over conventional weapons, and making repeated statements in public that the United States was able and willing at all times to use nuclear weapons. In this way through the Massive retaliation doctrine and the closely related foreign policy concept of Brinkmanship, Eisenhower hoped to hold out a creditable vision of Clausewitzian nuclear “absolute war” in order to deter the Soviet Union and/or China from ever risking a war or even conditions that might lead to a war with the United States.\n\nAfter 1970, some theorists claimed that nuclear proliferation made Clausewitzian concepts obsolete after the 20th-century period in which they dominated the world. John E. Sheppard, Jr., argues that by developing nuclear weapons, state-based conventional armies simultaneously both perfected their original purpose, to destroy a mirror image of themselves, and made themselves obsolete. No two powers have used nuclear weapons against each other, instead using conventional means or proxy wars to settle disputes. If such a conflict did occur, presumably both combatants would be annihilated. Heavily influenced by the war in Vietnam and by antipathy to American strategist Henry Kissinger, the American biologist, musician, and game-theorist Anatol Rapoport argued in 1968 that a Clausewitzian view of war was not only obsolete in the age of nuclear weapons, but also highly dangerous as it promoted a \"zero-sum paradigm\" to international relations and a \"dissolution of rationality\" amongst decision-makers.\n\nThe end of the 20th century and the beginning of the 21st century have seen many instances of state armies attempting to suppress insurgencies, terrorism, and other forms of asymmetrical warfare. Clausewitz did not focus solely on wars between countries with well-defined armies. The era of the French Revolution and Napoleon was full of revolutions, rebellions, and violence by \"non-state actors\", such as the wars in the French Vendée and in Spain. Clausewitz wrote a series of “Lectures on Small War” and studied the rebellion in the Vendée (1793–1796) and the Tyrolean uprising of 1809. In his famous “Bekenntnisdenkschrift” of 1812, he called for a “Spanish war in Germany” and laid out a comprehensive guerrilla strategy to be waged against Napoleon. In \"On War\" he included a famous chapter on “The People in Arms.”\n\nOne prominent critic of Clausewitz is the Israeli military historian Martin van Creveld. In his book \"The Transformation of War\", Creveld argued that Clausewitz's famous \"Trinity\" of people, army, and government was an obsolete socio-political construct based on the state, which was rapidly passing from the scene as the key player in war, and that he (Creveld) had constructed a new \"non-trinitarian\" model for modern warfare. Creveld's work has had great influence. Daniel Moran replied, 'The most egregious misrepresentation of Clausewitz’s famous metaphor must be that of Martin van Creveld, who has declared Clausewitz to be an apostle of Trinitarian War, by which he means, incomprehensibly, a war of 'state against state and army against army,' from which the influence of the people is entirely excluded.\" Christopher Bassford went further, noting that one need only \"read\" the paragraph in which Clausewitz defined his Trinity to see \"that the words 'people,' 'army,' and 'government' appear nowhere at all in the list of the Trinity’s components... Creveld's and Keegan's assault on Clausewitz's Trinity is not only a classic 'blow into the air,' i.e., an assault on a position Clausewitz doesn't occupy. It is also a pointless attack on a concept that is quite useful in its own right. In any case, their failure to read the actual wording of the theory they so vociferously attack, and to grasp its deep relevance to the phenomena they describe, is hard to credit.\"\n\nSome have gone further and suggested that Clausewitz's best-known aphorism, that war is a continuation of policy by other means, is not only irrelevant today but also inapplicable historically. For an opposing view see \"Clausewitz in the Twenty-First Century\" edited by Hew Strachan, and Andreas Herberg-Rothe. \n\nIn military academies, schools, and universities worldwide, Clausewitz's literature is often mandatory reading.\n\n\n\n\nAugust Otto Rühle von Lilienstern - Prussian officer from whom Clausewitz allegedly took, without acknowledgment, several important ideas (including that about war as pursuing political aims) made famous in \"On War\". However, such ideas as Clausewitz and Lilienstern shared in common derived from a common influence, i.e., Scharnhorst, who was Clausewitz's \"second father\" and professional mentor.\n\n\n"}
{"id": "6068", "url": "https://en.wikipedia.org/wiki?curid=6068", "title": "Common Lisp", "text": "Common Lisp\n\nCommon Lisp (CL) is a dialect of the Lisp programming language, published in ANSI standard document \"ANSI INCITS 226-1994 (R2004)\" (formerly \"X3.226-1994 (R1999)\"). The Common Lisp HyperSpec, a hyperlinked HTML version, has been derived from the ANSI Common Lisp standard.\n\nThe Common Lisp language was developed as a standardized and improved successor of Maclisp. By the early 1980s several groups were already at work on diverse successors to MacLisp: Lisp Machine Lisp (aka ZetaLisp), Spice Lisp, NIL and S-1 Lisp. Common Lisp sought to unify, standardise, and extend the features of these MacLisp dialects. Common Lisp is not an implementation, but rather a language specification. Several implementations of the Common Lisp standard are available, including free and open-source software and proprietary products.\nCommon Lisp is a general-purpose, multi-paradigm programming language. It supports a combination of procedural, functional, and object-oriented programming paradigms. As a dynamic programming language, it facilitates evolutionary and incremental software development, with iterative compilation into efficient run-time programs. This incremental development is often done interactively without interrupting the running application.\n\nIt also supports optional type annotation and casting, which can be added as necessary at the later profiling and optimization stages, to permit the compiler to generate more efficient code. For instance, codice_1 can hold an unboxed integer in a range supported by the hardware and implementation, permitting more efficient arithmetic than on big integers or arbitrary precision types. Similarly, the compiler can be told on a per-module or per-function basis which type safety level is wanted, using \"optimize\" declarations.\n\nCommon Lisp includes CLOS, an object system that supports multimethods and method combinations. It is often implemented with a Metaobject Protocol.\n\nCommon Lisp is extensible through standard features such as \"Lisp macros\" (code transformations) and \"reader macros\" (input parsers for characters).\n\nCommon Lisp provides some backwards compatibility to Maclisp and to John McCarthy's original Lisp. This allows older Lisp software to be ported to Common Lisp.\n\nWork on Common Lisp started in 1981 after an initiative by ARPA manager Bob Engelmore to develop a single community standard Lisp dialect. Much of the initial language design was done via electronic mail. Guy Lewis Steele, Jr. gave at the 1982 ACM Symposium on LISP and functional programming the first overview of Common Lisp.\n\nThe first language documentation was published 1984 as Common Lisp the Language, first edition. A second edition, published in 1990, incorporated many changes to the language, made during the ANSI Common Lisp standardization process. The final ANSI Common Lisp standard then was published in 1994. Since then no update to the standard has been published. Various extensions and improvements to Common Lisp (examples are Unicode, Concurrency, CLOS-based IO) have been provided by implementations and libraries (many available via Quicklisp).\n\nCommon Lisp is a dialect of Lisp; it uses S-expressions to denote both code and data structure. Function calls, macro forms and special forms are written as lists, with the name of the function first, as in these examples:\n\nCommon Lisp has many data types.\n\n\"Number\" types include integers, ratios, floating-point numbers, and complex numbers. Common Lisp uses bignums to represent numerical values of arbitrary size and precision. The ratio type represents fractions exactly, a facility not available in many languages. Common Lisp automatically coerces numeric values among these types as appropriate.\n\nThe Common Lisp \"character\" type is not limited to ASCII characters. Most modern implementations allow Unicode characters.\n\nThe \"symbol\" type is common to Lisp languages, but largely unknown outside them. A symbol is a unique, named data object with several parts: name, value, function, property list and package. Of these, \"value cell\" and \"function cell\" are the most important. Symbols in Lisp are often used similarly to identifiers in other languages: to hold the value of a variable; however there are many other uses. Normally, when a symbol is evaluated, its value is returned. Some symbols evaluate to themselves, for example all symbols in the keyword package are self-evaluating. Boolean values in Common Lisp are represented by the self-evaluating symbols T and NIL. Common Lisp has namespaces for symbols, called 'packages'.\n\nA number of functions are available for rounding scalar numeric values in various ways. The function codice_2 rounds the argument to the nearest integer, with halfway cases rounded to the even integer. The functions codice_3, codice_4, and codice_5 round towards zero, down, or up respectively. All these functions return the discarded fractional part as a secondary value. For example, codice_6 yields -3, 0.5; codice_7 yields -2, -0.5; codice_8 yields 2, 0.5; and codice_9 yields 4, -0.5.\n\n\"Sequence\" types in Common Lisp include lists, vectors, bit-vectors, and strings. There are many operations that can work on any sequence type.\n\nAs in almost all other Lisp dialects, \"lists\" in Common Lisp are composed of \"conses\", sometimes called \"cons cells\" or \"pairs\". A cons is a data structure with two slots, called its \"car\" and \"cdr\". A list is a linked chain of conses or the empty list. Each cons's car refers to a member of the list (possibly another list). Each cons's cdr refers to the next cons—except for the last cons in a list, whose cdr refers to the codice_10 value. Conses can also easily be used to implement trees and other complex data structures; though it is usually advised to use structure or class instances instead. It is also possible to create circular data structures with conses.\n\nCommon Lisp supports multidimensional \"arrays\", and can dynamically resize \"adjustable\" arrays if required. Multidimensional arrays can be used for matrix mathematics. A \"vector\" is a one-dimensional array. Arrays can carry any type as members (even mixed types in the same array) or can be specialized to contain a specific type of members, as in a vector of bits. Usually only a few types are supported. Many implementations can optimize array functions when the array used is type-specialized. Two type-specialized array types are standard: a \"string\" is a vector of characters, while a \"bit-vector\" is a vector of bits.\n\n\"Hash tables\" store associations between data objects. Any object may be used as key or value. Hash tables are automatically resized as needed.\n\n\"Packages\" are collections of symbols, used chiefly to separate the parts of a program into namespaces. A package may \"export\" some symbols, marking them as part of a public interface. Packages can use other packages.\n\n\"Structures\", similar in use to C structs and Pascal records, represent arbitrary complex data structures with any number and type of fields (called \"slots\"). Structures allow single-inheritance.\n\n\"Classes\" are similar to structures, but offer more dynamic features and multiple-inheritance. (See CLOS). Classes have been added late to Common Lisp and there is some conceptual overlap with structures. Objects created of classes are called \"Instances\". A special case are Generic Functions. Generic Functions are both functions and instances.\n\nCommon Lisp supports first-class functions. For instance, it is possible to write functions that take other functions as arguments or return functions as well. This makes it possible to describe very general operations.\n\nThe Common Lisp library relies heavily on such higher-order functions. For example, the codice_11 function takes a relational operator as an argument and key function as an optional keyword argument. This can be used not only to sort any type of data, but also to sort data structures according to a key.\n\nThe evaluation model for functions is very simple. When the evaluator encounters a form codice_12 then it presumes that the symbol named f is one of the following:\n\n\nIf f is the name of a function, then the arguments a1, a2, ..., an are evaluated in left-to-right order, and the function is found and invoked with those values supplied as parameters.\n\nThe macro codice_14 defines functions where a function definition gives the name of the function, the names of any arguments, and a function body:\n\nFunction definitions may include compiler directives, known as \"declarations\", which provide hints to the compiler about optimization settings or the data types of arguments. They may also include \"documentation strings\" (docstrings), which the Lisp system may use to provide interactive documentation:\n\nAnonymous functions (function literals) are defined using codice_13 expressions, e.g. codice_16 for a function that squares its argument. Lisp programming style frequently uses higher-order functions for which it is useful to provide anonymous functions as arguments.\n\nLocal functions can be defined with codice_17 and codice_18.\n\nThere are a number of other operators related to the definition and manipulation of functions. For instance, a function may be compiled with the codice_19 operator. (Some Lisp systems run functions using an interpreter by default unless instructed to compile; others compile every function).\n\nThe macro codice_20 defines generic functions.\nThe macro codice_21 defines methods. Generic functions are a collection of methods.\n\nMethods can specialize their parameters over CLOS \"standard classes\", \"system classes\", \"structure classes\" or objects. For many types there are corresponding \"system classes\".\n\nWhen a generic function is called, multiple-dispatch will determine the effective method to use.\n\nGeneric Functions are also a first class data type. There are many more features to Generic Functions and Methods than described above.\n\nThe namespace for function names is separate from the namespace for data variables. This is a key difference between Common Lisp and Scheme. For Common Lisp, operators that define names in the function namespace include codice_14, codice_17, codice_18, codice_21 and codice_20.\n\nTo pass a function by name as an argument to another function, one must use the codice_27 special operator, commonly abbreviated as codice_28. The first codice_11 example above refers to the function named by the symbol codice_30 in the function namespace, with the code codice_31. Conversely, to call a function passed in such a way, one would use the codice_32 operator on the argument.\n\nScheme's evaluation model is simpler: there is only one namespace, and all positions in the form are evaluated (in any order) -- not just the arguments. Code written in one dialect is therefore sometimes confusing to programmers more experienced in the other. For instance, many Common Lisp programmers like to use descriptive variable names such as \"list\" or \"string\" which could cause problems in Scheme, as they would locally shadow function names.\n\nWhether a separate namespace for functions is an advantage is a source of contention in the Lisp community. It is usually referred to as the \"Lisp-1 vs. Lisp-2 debate\". Lisp-1 refers to Scheme's model and Lisp-2 refers to Common Lisp's model. These names were coined in a 1988 paper by Richard P. Gabriel and Kent Pitman, which extensively compares the two approaches.\n\nCommon Lisp supports the concept of \"multiple values\", where any expression always has a single \"primary value\", but it might also have any number of \"secondary values\", which might be received and inspected by interested callers. This concept is distinct from returning a list value, as the secondary values are fully optional, and passed via a dedicated side channel. This means that callers may remain entirely unaware of the secondary values being there if they have no need for them, and it makes it convenient to use the mechanism for communicating information that is sometimes useful, but not always necessary. For example,\n\n\n\n\nMultiple values are supported by a handful of standard forms, most common of which are the codice_35 special form for accessing secondary values and codice_36 for returning multiple values:\n\nOther data types in Common Lisp include:\n\n\nLike programs in many other programming languages, Common Lisp programs make use of names to refer to variables, functions, and many other kinds of entities. Named references are subject to scope.\n\nThe association between a name and the entity which the name refers to is called a binding.\n\nScope refers to the set of circumstances in which a name is determined to have a particular binding.\n\nThe circumstances which determine scope in Common Lisp include:\n\n\nTo understand what a symbol refers to, the Common Lisp programmer must know what kind of reference is being expressed, what kind of scope it uses if it is a variable reference (dynamic versus lexical scope), and also the run-time situation: in what environment is the reference resolved, where was the binding introduced into the environment, et cetera.\n\nSome environments in Lisp are globally pervasive. For instance, if a new type is defined, it is known everywhere thereafter. References to that type look it up in this global environment.\n\nOne type of environment in Common Lisp is the dynamic environment. Bindings established in this environment have dynamic extent, which means that a binding is established at the start of the execution of some construct, such as a codice_51 block, and disappears when that construct finishes executing: its lifetime is tied to the dynamic activation and deactivation of a block. However, a dynamic binding is not just visible within that block; it is also visible to all functions invoked from that block. This type of visibility is known as indefinite scope. Bindings which exhibit dynamic extent (lifetime tied to the activation and deactivation of a block) and indefinite scope (visible to all functions which are called from that block) are said to have dynamic scope.\n\nCommon Lisp has support for dynamically scoped variables, which are also called special variables. Certain other kinds of bindings are necessarily dynamically scoped also, such as restarts and catch tags. Function bindings cannot be dynamically scoped using codice_17 (which only provides lexically scoped function bindings), but function objects (a first-level object in Common Lisp) can be assigned to dynamically scoped variables, bound using codice_51 in dynamic scope, then called using codice_32 or codice_57.\n\nDynamic scope is extremely useful because it adds referential clarity and discipline to global variables. Global variables are frowned upon in computer science as potential sources of error, because they can give rise to ad-hoc, covert channels of communication among modules that lead to unwanted, surprising interactions.\n\nIn Common Lisp, a special variable which has only a top-level binding behaves just like a global variable in other programming languages. A new value can be stored into it, and that value simply replaces what is in the top-level binding. Careless replacement of the value of a global variable is at the heart of bugs caused by use of global variables. However, another way to work with a special variable is to give it a new, local binding within an expression. This is sometimes referred to as \"rebinding\" the variable. Binding a dynamically scoped variable temporarily creates a new memory location for that variable, and associates the name with that location. While that binding is in effect, all references to that variable refer to the new binding; the previous binding is hidden. When execution of the binding expression terminates, the temporary memory location is gone, and the old binding is revealed, with the original value intact. Of course, multiple dynamic bindings for the same variable can be nested.\n\nIn Common Lisp implementations which support multithreading, dynamic scopes are specific to each thread of execution. Thus special variables serve as an abstraction for thread local storage. If one thread rebinds a special variable, this rebinding has no effect on that variable in other threads. The value stored in a binding can only be retrieved by the thread which created that binding. If each thread binds some special variable codice_58, then codice_58 behaves like thread-local storage. Among threads which do not rebind codice_58, it behaves like an ordinary global: all of these threads refer to the same top-level binding of codice_58.\n\nDynamic variables can be used to extend the execution context with additional context information which is implicitly passed from function to function without having to appear as an extra function parameter. This is especially useful when the control transfer has to pass through layers of unrelated code, which simply cannot be extended with extra parameters to pass the additional data. A situation like this usually calls for a global variable. That global variable must be saved and restored, so that the scheme doesn't break under recursion: dynamic variable rebinding takes care of this. And that variable must be made thread-local (or else a big mutex must be used) so the scheme doesn't break under threads: dynamic scope implementations can take care of this also.\n\nIn the Common Lisp library, there are many standard special variables. For instance, all standard I/O streams are stored in the top-level bindings of well-known special variables. The standard output stream is stored in *standard-output*.\n\nSuppose a function foo writes to standard output:\n\nTo capture its output in a character string, *standard-output* can be bound to a string stream and called:\n\nCommon Lisp supports lexical environments. Formally, the bindings in a lexical environment have lexical scope and may have either indefinite extent or dynamic extent, depending on the type of namespace. Lexical scope means that visibility is physically restricted to the block in which the binding is established. References which are not textually (i.e. lexically) embedded in that block simply do not see that binding.\n\nThe tags in a TAGBODY have lexical scope. The expression (GO X) is erroneous if it is not actually embedded in a TAGBODY which contains a label X. However, the label bindings disappear when the TAGBODY terminates its execution, because they have dynamic extent. If that block of code is re-entered by the invocation of a lexical closure, it is invalid for the body of that closure to try to transfer control to a tag via GO:\n\nWhen the TAGBODY is executed, it first evaluates the setf form which stores a function in the special variable *stashed*. Then the (go end-label) transfers control to end-label, skipping the code (print \"Hello\"). Since end-label is at the end of the tagbody, the tagbody terminates, yielding NIL. Suppose that the previously remembered function is now called:\n\nThis situation is erroneous. One implementation's response is an error condition containing the message, \"GO: tagbody for tag SOME-LABEL has already been left\". The function tried to evaluate (go some-label), which is lexically embedded in the tagbody, and resolves to the label. However, the tagbody isn't executing (its extent has ended), and so the control transfer cannot take place.\n\nLocal function bindings in Lisp have lexical scope, and variable bindings also have lexical scope by default. By contrast with GO labels, both of these have indefinite extent. When a lexical function or variable binding is established, that binding continues to exist for as long as references to it are possible, even after the construct which established that binding has terminated. References to lexical variables and functions after the termination of their establishing construct are possible thanks to lexical closures.\n\nLexical binding is the default binding mode for Common Lisp variables. For an individual symbol, it can be switched to dynamic scope, either by a local declaration, by a global declaration. The latter may occur implicitly through the use of a construct like DEFVAR or DEFPARAMETER. It is an important convention in Common Lisp programming that special (i.e. dynamically scoped) variables have names which begin and end with an asterisk sigil codice_62 in what is called the “earmuff convention”. If adhered to, this convention effectively creates a separate namespace for special variables, so that variables intended to be lexical are not accidentally made special.\n\nLexical scope is useful for several reasons.\n\nFirstly, references to variables and functions can be compiled to efficient machine code, because the run-time environment structure is relatively simple. In many cases it can be optimized to stack storage, so opening and closing lexical scopes has minimal overhead. Even in cases where full closures must be generated, access to the closure's environment is still efficient; typically each variable becomes an offset into a vector of bindings, and so a variable reference becomes a simple load or store instruction with a base-plus-offset addressing mode.\n\nSecondly, lexical scope (combined with indefinite extent) gives rise to the lexical closure, which in turn creates a whole paradigm of programming centered around the use of functions being first-class objects, which is at the root of functional programming.\n\nThirdly, perhaps most importantly, even if lexical closures are not exploited, the use of lexical scope isolates program modules from unwanted interactions. Due to their restricted visibility, lexical variables are private. If one module A binds a lexical variable X, and calls another module B, references to X in B will not accidentally resolve to the X bound in A. B simply has no access to X. For situations in which disciplined interactions through a variable are desirable, Common Lisp provides special variables. Special variables allow for a module A to set up a binding for a variable X which is visible to another module B, called from A. Being able to do this is an advantage, and being able to prevent it from happening is also an advantage; consequently, Common Lisp supports both lexical and dynamic scope.\n\nA \"macro\" in Lisp superficially resembles a function in usage. However, rather than representing an expression which is evaluated, it represents a transformation of the program source code. The macro gets the source it surrounds as arguments, binds them to its parameters and computes a new source form. This new form can also use a macro. The macro expansion is repeated until the new source form does not use a macro. The final computed form is the source code executed at runtime.\n\nTypical uses of macros in Lisp:\n\n\nVarious standard Common Lisp features also need to be implemented as macros, such as:\n\n\nMacros are defined by the \"defmacro\" macro. The special operator \"macrolet\" allows the definition of local (lexically scoped) macros. It is also possible to define macros for symbols using \"define-symbol-macro\" and \"symbol-macrolet\".\n\nPaul Graham's book On Lisp describes the use of macros in Common Lisp in detail. Doug Hoyte's book Let Over Lambda extends the discussion on macros, claiming \"Macros are the single greatest advantage that lisp has as a programming language and the single greatest advantage of any programming language.\" Hoyte provides several examples of iterative development of macros.\n\nMacros allow Lisp programmers to create new syntactic forms in the language. One typical use is to create new control structures. The example macro provides an codice_73 looping construct. The syntax is:\n\nThe macro definition for \"until\":\n\"tagbody\" is a primitive Common Lisp special operator which provides the ability to name tags and use the \"go\" form to jump to those tags. The backquote \"`\" provides a notation that provides code templates, where the value of forms preceded with a comma are filled in. Forms preceded with comma and at-sign are \"spliced\" in. The tagbody form tests the end condition. If the condition is true, it jumps to the end tag. Otherwise the provided body code is executed and then it jumps to the start tag.\n\nAn example form using above \"until\" macro:\nThe code can be expanded using the function \"macroexpand-1\". The expansion for above example looks like this:\nDuring macro expansion the value of the variable \"test\" is \"(= (random 10) 0)\" and the value of the variable \"body\" is \"((write-line \"Hello\"))\". The body is a list of forms.\n\nSymbols are usually automatically upcased. The expansion uses the TAGBODY with two labels. The symbols for these labels are computed by GENSYM and are not interned in any package. Two \"go\" forms use these tags to jump to. Since \"tagbody\" is a primitive operator in Common Lisp (and not a macro), it will not be expanded into something else. The expanded form uses the \"when\" macro, which also will be expanded. Fully expanding a source form is called \"code walking\".\n\nIn the fully expanded (\"walked\") form, the \"when\" form is replaced by the primitive \"if\":\nAll macros must be expanded before the source code containing them can be evaluated or compiled normally. Macros can be considered functions that accept and return abstract syntax trees (Lisp S-expressions). These functions\nare invoked before the evaluator or compiler to produce the final source code.\nMacros are written in normal Common Lisp, and may use any Common Lisp (or third-party) operator available.\n\nCommon Lisp macros are capable of what is commonly called \"variable capture\", where symbols in the macro-expansion body coincide with those in the calling context, allowing the programmer to create macros wherein various symbols have special meaning. The term \"variable capture\" is somewhat misleading, because all namespaces are vulnerable to unwanted capture, including the operator and function namespace, the tagbody label namespace, catch tag, condition handler and restart namespaces.\n\n\"Variable capture\" can introduce software defects. This happens in one of the following two ways:\n\n\nThe Scheme dialect of Lisp provides a macro-writing system which provides the referential transparency that eliminates both types of capture problem. This type of macro system is sometimes called \"hygienic\", in particular by its proponents (who regard macro systems which do not automatically solve this problem as unhygienic). \n\nIn Common Lisp, macro hygiene is ensured one of two different ways.\n\nOne approach is to use gensyms: guaranteed-unique symbols which can be used in a macro-expansion without threat of capture. The use of gensyms in a macro definition is a manual chore, but macros can be written which simplify the instantiation and use of gensyms. Gensyms solve type 2 capture easily, but they are not applicable to type 1 capture in the same way, because the macro expansion cannot rename the interfering symbols in the surrounding code which capture its references. Gensyms could be used to provide stable aliases for the global symbols which the macro expansion needs. The macro expansion would use these secret aliases rather than the well-known names, so redefinition of the well-known names would have no ill effect on the macro.\n\nAnother approach is to use packages. A macro defined in its own package can simply use internal symbols in that package in its expansion. The use of packages deals with type 1 and type 2 capture.\n\nHowever, packages don't solve the type 1 capture of references to standard Common Lisp functions and operators. The reason is that the use of packages to solve capture problems revolves around the use of private symbols (symbols in one package, which are not imported into, or otherwise made visible in other packages). Whereas the Common Lisp library symbols are external, and frequently imported into or made visible in user-defined packages.\n\nThe following is an example of unwanted capture in the operator namespace, occurring in the expansion of a macro:\n\nThe codice_73 macro will expand into a form which calls codice_75 which is intended to refer to the standard Common Lisp macro codice_75. However, in this context, codice_75 may have a completely different meaning, so codice_73 may not work properly.\n\nCommon Lisp solves the problem of the shadowing of standard operators and functions by forbidding their redefinition. Because it redefines the standard operator codice_75, the preceding is actually a fragment of non-conforming Common Lisp, which allows implementations to diagnose and reject it.\n\nThe \"condition system\" is responsible for exception handling in Common Lisp. It provides \"conditions\", \"handler\"s and \"restart\"s. \"Condition\"s are objects describing an exceptional situation (for example an error). If a \"condition\" is signaled, the Common Lisp system searches for a \"handler\" for this condition type and calls the handler. The \"handler\" can now search for restarts and use one of these restarts to automatically repair the current problem, using information such as the condition type and any relevant information provided as part of the condition object, and call the appropriate restart function.\n\nThese restarts, if unhandled by code, can be presented to users (as part of a user interface, that of a debugger for example), so that the user can select and invoke one of the available restarts. Since the condition handler is called in the context of the error (without unwinding the stack), full error recovery is possible in many cases, where other exception handling systems would have already terminated the current routine. The debugger itself can also be customized or replaced using the codice_80 dynamic variable. Code found within \"unwind-protect\" forms such as finalizers will also be executed as appropriate despite the exception.\n\nIn the following example (using Symbolics Genera) the user tries to open a file in a Lisp function \"test\" called from the Read-Eval-Print-LOOP (REPL), when the file does not exist. The Lisp system presents four restarts. The user selects the \"Retry OPEN using a different pathname\" restart and enters a different pathname (lispm-init.lisp instead of lispm-int.lisp). The user code does not contain any error handling code. The whole error handling and restart code is provided by the Lisp system, which can handle and repair the error without terminating the user code.\n\nCommon Lisp includes a toolkit for object-oriented programming, the Common Lisp Object System or CLOS, which is one of the most powerful object systems available in any language. For example, Peter Norvig explains how many Design Patterns are simpler to implement in a dynamic language with the features of CLOS (Multiple Inheritance, Mixins, Multimethods, Metaclasses, Method combinations, etc.).\nSeveral extensions to Common Lisp for object-oriented programming have been proposed to be included into the ANSI Common Lisp standard, but eventually CLOS was adopted as the standard object-system for Common Lisp. CLOS is a dynamic object system with multiple dispatch and multiple inheritance, and differs radically from the OOP facilities found in static languages such as C++ or Java. As a dynamic object system, CLOS allows changes at runtime to generic functions and classes. Methods can be added and removed, classes can be added and redefined, objects can be updated for class changes and the class of objects can be changed.\n\nCLOS has been integrated into ANSI Common Lisp. Generic Functions can be used like normal functions and are a first-class data type. Every CLOS class is integrated into the Common Lisp type system. Many Common Lisp types have a corresponding class. There is more potential use of CLOS for Common Lisp. The specification does not say whether conditions are implemented with CLOS. Pathnames and streams could be implemented with CLOS. These further usage possibilities of CLOS for ANSI Common Lisp are not part of the standard. Actual Common Lisp implementations are using CLOS for pathnames, streams, input/output, conditions, the implementation of CLOS itself and more.\n\nSeveral implementations of earlier Lisp dialects provided both an interpreter and a compiler. Unfortunately often the semantics were different. These earlier Lisps implemented lexical scoping in the compiler and dynamic scoping in the interpreter. Common Lisp requires that both the interpreter and compiler use lexical scoping by default. The Common Lisp standard describes both the semantics of the interpreter and a compiler. The compiler can be called using the function \"compile\" for individual functions and using the function \"compile-file\" for files. Common Lisp allows type declarations and provides ways to influence the compiler code generation policy. For the latter various optimization qualities can be given values between 0 (not important) and 3 (most important): \"speed\", \"space\", \"safety\", \"debug\" and \"compilation-speed\".\n\nThere is also a function to evaluate Lisp code: codice_81. codice_81 takes code as pre-parsed s-expressions and not, like in some other languages, as text strings. This way code can be constructed with the usual Lisp functions for constructing lists and symbols and then this code can be evaluated with the function codice_81. Several Common Lisp implementations (like Clozure CL and SBCL) are implementing codice_81 using their compiler. This way code is compiled, even though it is evaluated using the function codice_81.\n\nThe file compiler is invoked using the function \"compile-file\". The generated file with compiled code is called a \"fasl\" (from \"fast load\") file. These \"fasl\" files and also source code files can be loaded with the function \"load\" into a running Common Lisp system. Depending on the implementation, the file compiler generates byte-code (for example for the Java Virtual Machine), C language code (which then is compiled with a C compiler) or, directly, native code.\n\nCommon Lisp implementations can be used interactively, even though the code gets fully compiled. The idea of an Interpreted language thus does not apply for interactive Common Lisp.\n\nThe language makes distinction between read-time, compile-time, load-time and run-time, and allows user code to also make this distinction to perform the wanted type of processing at the wanted step.\n\nSome special operators are provided to especially suit interactive development; for instance, codice_86 will only assign a value to its provided variable if it wasn't already bound, while codice_87 will always perform the assignment. This distinction is useful when interactively evaluating, compiling and loading code in a live image.\n\nSome features are also provided to help writing compilers and interpreters. Symbols consist of first-level objects and are directly manipulable by user code. The codice_88 special operator allows to create lexical bindings programmatically, while packages are also manipulable. The Lisp compiler itself is available at runtime to compile files or individual functions. These make it easy to use Lisp as an intermediate compiler or interpreter for another language.\n\nThe following program calculates the smallest number of people in a room for whom the probability of completely unique birthdays is less than 50% (the birthday paradox, where for 1 person the probability is obviously 100%, for 2 it is 364/365, etc.). The answer is 23.\n\nCalling the example function using the REPL (Read Eval Print Loop):\n\nWe define a class codice_89 and a method for displaying the name and age of a person.\nNext we define a group of persons as a list of codice_89 objects.\nThen we iterate over the sorted list.\n\nIt prints the three names with descending age.\n\nUse of the LOOP macro is demonstrated:\n\nExample use:\n\nCompare with the built in exponentiation:\n\nWITH-OPEN-FILE is a macro that opens a file and provides a stream. When the form is returning, the file is automatically closed. FUNCALL calls a function object. The LOOP collects all lines that match the predicate.\n\nThe function AVAILABLE-SHELLS calls above function LIST-MATCHING-LINES with a pathname and an anonymous function as the predicate. The predicate returns the pathname of a shell or NIL (if the string is not the filename of a shell).\n\nExample results (on Mac OS X 10.6):\n\nCommon Lisp is most frequently compared with, and contrasted to, Scheme—if only because they are the two most popular Lisp dialects. Scheme predates CL, and comes not only from the same Lisp tradition but from some of the same engineers—Guy L. Steele, with whom Gerald Jay Sussman designed Scheme, chaired the standards committee for Common Lisp.\n\nCommon Lisp is a general-purpose programming language, in contrast to Lisp variants such as Emacs Lisp and AutoLISP which are extension languages embedded in particular products. Unlike many earlier Lisps, Common Lisp (like Scheme) uses lexical variable scope by default for both interpreted and compiled code.\n\nMost of the Lisp systems whose designs contributed to Common Lisp—such as ZetaLisp and Franz Lisp—used dynamically scoped variables in their interpreters and lexically scoped variables in their compilers. Scheme introduced the sole use of lexically scoped variables to Lisp; an inspiration from ALGOL 68 which was widely recognized as a good idea. CL supports dynamically scoped variables as well, but they must be explicitly declared as \"special\". There are no differences in scoping between ANSI CL interpreters and compilers.\n\nCommon Lisp is sometimes termed a \"Lisp-2\" and Scheme a \"Lisp-1\", referring to CL's use of separate namespaces for functions and variables. (In fact, CL has \"many\" namespaces, such as those for go tags, block names, and codice_72 keywords). There is a long-standing controversy between CL and Scheme advocates over the tradeoffs involved in multiple namespaces. In Scheme, it is (broadly) necessary to avoid giving variables names which clash with functions; Scheme functions frequently have arguments named codice_92, codice_93, or codice_94 so as not to conflict with the system function codice_95. However, in CL it is necessary to explicitly refer to the function namespace when passing a function as an argument—which is also a common occurrence, as in the codice_11 example above.\n\nCL also differs from Scheme in its handling of boolean values. Scheme uses the special values #t and #f to represent truth and falsity. CL follows the older Lisp convention of using the symbols T and NIL, with NIL standing also for the empty list. In CL, \"any\" non-NIL value is treated as true by conditionals, such as codice_68, whereas in Scheme all non-#f values are treated as true. These conventions allow some operators in both languages to serve both as predicates (answering a boolean-valued question) and as returning a useful value for further computation, but in Scheme the value '() which is equivalent to NIL in Common Lisp evaluates to true in a boolean expression.\n\nLastly, the Scheme standards documents require tail-call optimization, which the CL standard does not. Most CL implementations do offer tail-call optimization, although often only when the programmer uses an optimization directive. Nonetheless, common CL coding style does not favor the ubiquitous use of recursion that Scheme style prefers—what a Scheme programmer would express with tail recursion, a CL user would usually express with an iterative expression in codice_75, codice_99, codice_72, or (more recently) with the codice_101 package.\n\nSee the Category .\n\nCommon Lisp is defined by a specification (like Ada and C) rather than by one implementation (like Perl before version 6). There are many implementations, and the standard details areas in which they may validly differ.\n\nIn addition, implementations tend to come with extensions, which provide functionality not covered in the standard:\n\n\nFree and open source software libraries have been created to support extensions to Common Lisp in a portable way, and are most notably found in the repositories of the Common-Lisp.net and Common Lisp Open Code Collection projects.\n\nCommon Lisp implementations may use any mix of native code compilation, byte code compilation or interpretation. Common Lisp has been designed to support incremental compilers, file compilers and block compilers. Standard declarations to optimize compilation (such as function inlining or type specialization) are proposed in the language specification. Most Common Lisp implementations compile source code to native machine code. Some implementations can create (optimized) stand-alone applications. Others compile to interpreted bytecode, which is less efficient than native code, but eases binary-code portability. There are also compilers that compile Common Lisp code to C code. The misconception that Lisp is a purely interpreted language is most likely because Lisp environments provide an interactive prompt and that code is compiled one-by-one, in an incremental way. With Common Lisp incremental compilation is widely used.\n\nSome Unix-based implementations (CLISP, SBCL) can be used as a scripting language; that is, invoked by the system transparently in the way that a Perl or Unix shell interpreter is.\n\n\n\n\nSee the Category .\n\nCommon Lisp is used to develop research applications (often in Artificial Intelligence), for rapid development of prototypes or for deployed applications.\n\nCommon Lisp is used in many commercial applications, including the Yahoo! Store web-commerce site, which originally involved Paul Graham and was later rewritten in C++ and Perl. Other notable examples include:\n\n\nThere also exist open-source applications written in Common Lisp, such as:\n\n\nSince 2011 Zach Beane, with support of the Common Lisp Foundation, maintains the Quicklisp library manager. It allows easy access to several hundred libraries written in Common Lisp.\n\nA chronological list of books published (or about to be published) about Common Lisp (the language) or about programming with Common Lisp (especially AI programming).\n\n"}
{"id": "6069", "url": "https://en.wikipedia.org/wiki?curid=6069", "title": "Color code", "text": "Color code\n\nA color code or colour code is a system for displaying information by using different colors.\n\nThe earliest examples of color codes in use are for long distance communication by use of flags, as in semaphore communication. The United Kingdom adopted a color code scheme for such communication wherein red signified danger and white signified safety, with other colors having similar assignments of meaning.\n\nAs chemistry and other technologies advanced, it became expedient to use coloration as a signal for telling apart things that would otherwise be confusingly similar, such as wiring in electrical and electronic devices, and pharmaceutical pills.\n\nThe use of color codes has been extended to abstractions, such as the Homeland Security Advisory System color code in the United States. Similarly, hospital emergency codes often incorporate colors (such as the widely used \"Code Blue\" indicating a cardiac arrest), although they may also include numbers, and may not conform to a uniform standard.\n\nColor codes do present some potential problems. On forms and signage, the use of color can distract from black and white text. They are often difficult for color blind and blind people to interpret, and even for those with normal color vision, use of a large number of colors to code a large number of variables can lead to use of confusingly similar colors. \n\nSystems incorporating color-coding include:\n\n\n"}
{"id": "6080", "url": "https://en.wikipedia.org/wiki?curid=6080", "title": "CGI", "text": "CGI\n\nCGI may refer to:\n\n\n\n"}
{"id": "6082", "url": "https://en.wikipedia.org/wiki?curid=6082", "title": "Cortex", "text": "Cortex\n\nCortex or cortical may refer to:\n\n\n\n\n\n\n"}
{"id": "6084", "url": "https://en.wikipedia.org/wiki?curid=6084", "title": "Collection", "text": "Collection\n\nCollection or Collections usually refers to:\n\nCollection may also refer to:\n\n\n"}
{"id": "6085", "url": "https://en.wikipedia.org/wiki?curid=6085", "title": "Cauchy sequence", "text": "Cauchy sequence\n\nIn mathematics, a Cauchy sequence (; ), named after Augustin-Louis Cauchy, is a sequence whose elements become \"arbitrarily close to each other\" as the sequence progresses. More precisely, given any small positive distance, all but a finite number of elements of the sequence are less than that given distance from each other.\n\nIt is not sufficient for each term to become arbitrarily close to the \"preceding\" term. For instance, in the harmonic series formula_1 a difference between consecutive terms decreases as formula_2, however the series does not converge. Rather, it is required that \"all\" terms get arbitrarily close to \"each other\", starting from some point. More formally, for any given formula_3 (which means: arbitrarily \"small\") there exists an \"N\" such that for any pair \"m\",\"n\" > \"N\", we have formula_4 (whereas formula_5 is not sufficient).\n\nThe utility of Cauchy sequences lies in the fact that in a complete metric space (one where all such sequences are known to converge to a limit), the criterion for convergence depends only on the terms of the sequence itself, as opposed to the definition of convergence, which uses the limit value as well as the terms. This is often exploited in algorithms, both theoretical and applied, where an iterative process can be shown relatively easily to produce a Cauchy sequence, consisting of the iterates, thus fulfilling a logical condition, such as termination.\n\nThe notions above are not as unfamiliar as they might at first appear. The customary acceptance of the fact that any real number \"x\" has a decimal expansion is an implicit acknowledgment that a particular Cauchy sequence of rational numbers (whose terms are the successive truncations of the decimal expansion of \"x\") has the real limit \"x\". In some cases it may be difficult to describe \"x\" independently of such a limiting process involving rational numbers.\n\nGeneralizations of Cauchy sequences in more abstract uniform spaces exist in the form of Cauchy filters and Cauchy nets.\n\nA sequence\n\nof real numbers is called a \"Cauchy\" sequence, if for every positive real number \"ε\", there is a positive integer \"N\" such that for all natural numbers \"m\", \"n\" > \"N\"\n\nwhere the vertical bars denote the absolute value. In a similar way one can define Cauchy sequences of rational or complex numbers. Cauchy formulated such a condition by requiring formula_8 to be infinitesimal for every pair of infinite \"m, n\".\n\nSince the definition of a Cauchy sequence only involves metric concepts, it is straightforward to generalize it to any metric space \"X\". To do so, the absolute value |\"x\" - \"x\"| is replaced by the \"distance\" \"d\"(\"x\", \"x\") (where \"d\" denotes a metric) between \"x\" and \"x\".\n\nFormally, given a metric space (\"X\", \"d\"), a sequence\n\nis Cauchy, if for every positive real number \"ε\" > 0 there is a positive integer \"N\" such that for all positive integers m, n > \"N\", the distance\n\nRoughly speaking, the terms of the sequence are getting closer and closer together in a way that suggests that the sequence ought to have a limit in \"X\". Nonetheless, such a limit does not always exist within \"X\".\n\nA metric space \"X\" in which every Cauchy sequence converges to an element of \"X\" is called complete.\n\nThe real numbers are complete under the metric induced by the usual absolute value, and one of the standard constructions of the real numbers involves Cauchy sequences of rational numbers. In this construction, each equivalence class of Cauchy sequences of rational numbers with a certain tail behavior—that is, each class of sequences that get arbitrarily close to one another— is a real number.\n\nA rather different type of example is afforded by a metric space \"X\" which has the discrete metric (where any two distinct points are at distance \"1\" from each other). Any Cauchy sequence of elements of \"X\" must be constant beyond some fixed point, and converges to the eventually repeating term.\n\nThe rational numbers Q are not complete (for the usual distance):\nThere are sequences of rationals that converge (in R) to irrational numbers; these are Cauchy sequences having no limit in Q. In fact, if a real number \"x\" is irrational, then the sequence (\"x\"), whose \"n\"-th term is the truncation to \"n\" decimal places of the decimal expansion of \"x\", gives a Cauchy sequence of rational numbers with irrational limit \"x\". Irrational numbers certainly exist in R, for example:\n\n\nThe open interval formula_14 in the set of real numbers with an ordinary distance in R is not a complete space: there is a sequence formula_15 in it, which is Cauchy (for arbitrarily small distance bound formula_16 all terms formula_17 of formula_18 fit in the formula_19 interval), however does not converge in formula_20 — its 'limit', number formula_21, does not belong to the space formula_20.\n\n\nThese last two properties, together with the Bolzano–Weierstrass theorem, yield one standard proof of the completeness of the real numbers, closely related to both the Bolzano–Weierstrass theorem and the Heine–Borel theorem. Every Cauchy sequence of real numbers is bounded, hence by Bolzano-Weierstrass has a convergent subsequence, hence is itself convergent. It should be noted, though, that this proof of the completeness of the real numbers implicitly makes use of the least upper bound axiom. The alternative approach, mentioned above, of \"constructing\" the real numbers as the completion of the rational numbers, makes the completeness of the real numbers tautological.\n\nOne of the standard illustrations of the advantage of being able to work with Cauchy sequences and make use of completeness is provided by consideration of the summation of an infinite series of real numbers\n(or, more generally, of elements of any complete normed linear space, or Banach space). Such a series \nformula_23 is considered to be convergent if and only if the sequence of partial sums formula_24 is convergent, where \nformula_25. It is a routine matter \nto determine whether the sequence of partial sums is Cauchy or not,\nsince for positive integers \"p\" > \"q\", \n\nIf formula_27 is a uniformly continuous map between the metric spaces \"M\" and \"N\" and (\"x\") is a Cauchy sequence in \"M\", then formula_28 is a Cauchy sequence in \"N\". If formula_29 and formula_30 are two Cauchy sequences in the rational, real or complex numbers, then the sum formula_31 and the product formula_32 are also Cauchy sequences.\n\nThere is also a concept of Cauchy sequence for a topological vector space formula_33: Pick a local base formula_34 for formula_33 about 0; then (formula_36) is a Cauchy sequence if for each member formula_37, there is some number formula_38 such that whenever \nformula_39 is an element of formula_40. If the topology of formula_33 is compatible with a translation-invariant metric formula_42, the two definitions agree.\n\nSince the topological vector space definition of Cauchy sequence requires only that there be a continuous \"subtraction\" operation, it can just as well be stated in the context of a topological group: A sequence formula_43 in a topological group formula_44 is a Cauchy sequence if for every open neighbourhood formula_45 of the identity in formula_44 there exists some number formula_38 such that whenever formula_48 it follows that formula_49. As above, it is sufficient to check this for the neighbourhoods in any local base of the identity in formula_44.\n\nAs in the construction of the completion of a metric space, one can furthermore define the binary relation on Cauchy sequences in formula_44 that formula_43 and formula_53 are \"equivalent\" if for every open neighbourhood formula_45 of the identity in formula_44 there exists some number formula_38 such that whenever formula_48 it follows that formula_58. This relation is an equivalence relation: It is reflexive since the sequences are Cauchy sequences. It is symmetric since formula_59 which by continuity of the inverse is another open neighbourhood of the identity. It is transitive since formula_60 where formula_61 and formula_62 are open neighbourhoods of the identity such that formula_63; such pairs exist by the continuity of the group operation.\n\nThere is also a concept of Cauchy sequence in a group formula_44:\nLet formula_65 be a decreasing sequence of normal subgroups of formula_44 of finite index.\nThen a sequence formula_29 in formula_44 is said to be Cauchy (w.r.t. formula_69) if and only if for any formula_70 there is formula_38 such that formula_72.\n\nTechnically, this is the same thing as a topological group Cauchy sequence for a particular choice of topology on formula_44, namely that for which formula_69 is a local base.\n\nThe set formula_75 of such Cauchy sequences forms a group (for the componentwise product), and the set formula_76 of null sequences (s.th. formula_77) is a normal subgroup of formula_75. The factor group formula_79 is called the completion of formula_44 with respect to formula_69.\n\nOne can then show that this completion is isomorphic to the inverse limit of the sequence formula_82.\n\nAn example of this construction, familiar in number theory\nand algebraic geometry is the construction of the \"p-adic completion\" of the integers with respect to a prime \"p.\" In this case, \"G\" is the integers under addition, and \"H\" is the additive subgroup consisting of integer multiples of \"p\".\n\nIf formula_69 is a cofinal sequence (i.e., any normal subgroup of finite index contains some formula_84), then this completion is canonical in the sense that it is isomorphic to the inverse limit of formula_85, where formula_69 varies over \"all\" normal subgroups of finite index.\nFor further details, see ch. I.10 in Lang's \"Algebra\".\n\nIn constructive mathematics, Cauchy sequences often must be given with a \"modulus of Cauchy convergence\" to be useful. If formula_87 is a Cauchy sequence in the set formula_33, then a modulus of Cauchy convergence for the sequence is a function formula_89 from the set of natural numbers to itself, such that formula_90.\n\nClearly, any sequence with a modulus of Cauchy convergence is a Cauchy sequence. The converse (that every Cauchy sequence has a modulus) follows from the well-ordering property of the natural numbers (let formula_91 be the smallest possible formula_38 in the definition of Cauchy sequence, taking formula_70 to be formula_94). However, this well-ordering property does not hold in constructive mathematics (it is equivalent to the principle of excluded middle). On the other hand, this converse also follows (directly) from the principle of dependent choice (in fact, it will follow from the weaker AC), which is generally accepted by constructive mathematicians. Thus, moduli of Cauchy convergence are needed directly only by constructive mathematicians who (like Fred Richman) do not wish to use any form of choice.\n\nThat said, using a modulus of Cauchy convergence can simplify both definitions and theorems in constructive analysis. Perhaps even more useful are \"regular Cauchy sequences\", sequences with a given modulus of Cauchy convergence (usually formula_95 or formula_96). Any Cauchy sequence with a modulus of Cauchy convergence is equivalent (in the sense used to form the completion of a metric space) to a regular Cauchy sequence; this can be proved without using any form of the axiom of choice. Regular Cauchy sequences were used by Errett Bishop in his Foundations of Constructive Analysis, but they have also been used by Douglas Bridges in a non-constructive textbook (). However, Bridges also works on mathematical constructivism; the concept has not spread far outside of that milieu.\n\nA real sequence formula_97 has a natural hyperreal extension, defined for hypernatural values \"H\" of the index \"n\" in addition to the usual natural \"n\". The sequence is Cauchy if and only if for every infinite \"H\" and \"K\", the values formula_98 and formula_99 are infinitely close, or adequal, i.e. \nwhere \"st\" is the standard part function.\n\n\n"}
{"id": "6088", "url": "https://en.wikipedia.org/wiki?curid=6088", "title": "Common Era", "text": "Common Era\n\nCommon Era or Current Era (CE) is a year-numbering system (calendar era) for the Julian and Gregorian calendars that refers to the years since the start of this era, that is, the years beginning with AD 1. The preceding era is referred to as before the Common or Current Era (BCE). The Current Era notation system can be used as an alternative to the Dionysian era system, which distinguishes eras as AD (\"\", \"[the] year of [the] Lord\") and BC (\"before Christ\"). The two notation systems are numerically equivalent; thus \" CE\" corresponds to \"AD \" and \"400 BCE\" corresponds to \"400 BC\". The year-numbering system for the Gregorian calendar is the most widespread civil calendar system used in the world today. For decades, it has been the global standard, recognized by international institutions such as the United Nations and the Universal Postal Union.\n\nThe expression has been traced back to Latin usage to 1615, as ', and to 1635 in English as \"Vulgar Era\". The term \"Common Era\" can be found in English as early as 1708, and became more widely used in the mid-19th century by Jewish academics. In the later 20th century, the use of CE and BCE was popularized in academic and scientific publications, and more generally by authors and publishers wishing to emphasize secularism or sensitivity to non-Christians, by not explicitly referencing Jesus as \"Christ\" and \"Dominus\" (\"Lord\"), shortened from ' (\"in the year of Our Lord Jesus Christ\").\n\nThe year numbering system used with Common Era notation was devised by the Christian monk Dionysius Exiguus in the year 525 to replace the Era of Martyrs system, because he did not wish to continue the memory of a tyrant who persecuted Christians. He attempted to number years from an initial reference date (\"epoch\"), an event he referred to as the Incarnation of Jesus. Dionysius labeled the column of the table in which he introduced the new era as \"\"Anni Domini Nostri Jesu Christi\"\".\n\nNumbering years in this manner became more widespread in Europe with its usage by Bede in England in 731. Bede also introduced the practice of dating years before what he supposed was the year of birth of Jesus, and the practice of not using a year zero. In 1422, Portugal became the last Western European country to switch to the system begun by Dionysius.\n\nThe term \"Common Era\" is traced back in English to its appearance as \"Vulgar Era\" to distinguish dates on the Ecclesiastic calendar from those of the regnal year, the year of reign of a sovereign, typically used in national law.\n\nThe first use of the Latin term \"vulgaris aerae\" discovered so far was in a 1615 book by Johannes Kepler. Kepler uses it again in a 1616 table of ephemerides, and again in 1617. A 1635 English edition of that book has the title page in English – so far, the earliest-found usage of \"Vulgar Era\" in English. A 1701 book edited by John LeClerc includes \"Before Christ according to the Vulgar Æra, 6\". A 1716 book in English by Dean Humphrey Prideaux says, \"before the beginning of the vulgar æra, by which we now compute the years from his incarnation.\" A 1796 book uses the term \"vulgar era of the nativity\".\n\nThe first so-far-discovered usage of \"Christian Era\" is as the Latin phrase \"aerae christianae\" on the title page of a 1584 theology book. In 1649, the Latin phrase \"æræ Christianæ\" appeared in the title of an English almanac. A 1652 ephemeris is the first instance so-far-found for English usage of \"Christian Era\".\n\nThe English phrase \"common Era\" appears at least as early as 1708, and in a 1715 book on astronomy is used interchangeably with \"Christian Era\" and \"Vulgar Era\". A 1759 history book uses \"common æra\" in a generic sense, to refer to the common era of the Jews. The first-so-far found usage of the phrase \"before the common era\" is in a 1770 work that also uses \"common era\" and \"vulgar era\" as synonyms, in a translation of a book originally written in German. The 1797 edition of the Encyclopædia Britannica uses the terms \"vulgar era\" and \"common era\" synonymously. In 1835, in his book \"Living Oracles\", Alexander Campbell, wrote: \"The vulgar Era, or Anno Domini; the fourth year of Jesus Christ, the first of which was but eight days\", and also refers to the \"common era\" as a synonym for \"vulgar era\" with \"the fact that our Lord was born on the 4th year before the vulgar era, called Anno Domini, thus making (for example) the 42d year from his birth to correspond with the 38th of the common era...\" The \"Catholic Encyclopedia\" (1909) in at least one article reports all three terms (Christian, Vulgar, Common Era) being commonly understood by the early 20th century.\n\nThe phrase \"common era\", in lower case, also appeared in the 19th century in a \"generic\" sense, not necessarily to refer to the Christian Era, but to any system of dates in common use throughout a civilization. Thus, \"the common era of the Jews\", \"the common era of the Mahometans\", \"common era of the world\", \"the common era of the foundation of Rome\". When it did refer to the Christian Era, it was sometimes qualified, e.g., \"common era of the Incarnation\", \"common era of the Nativity\", or \"common era of the birth of Christ\".\n\nAn adapted translation of \"Common Era\" into pseudo-Latin as \"Era Vulgaris\" (in Latin this means \"Common Mistress\") was adopted in the 20th century by some followers of Aleister Crowley, and thus the abbreviation \"e.v.\" or \"EV\" may sometimes be seen as a replacement for AD.\n\nAlthough Jews have their own Hebrew calendar, they often use the Gregorian calendar.\n\nAs early as 1825, the abbreviation VE (for Vulgar Era) was in use among Jews to denote years in the Western calendar.\n\nCommon Era notation has also been in use for Hebrew lessons for \"more than a century\". Some Jewish academics were already using the \"CE\" and \"BCE\" abbreviations by the mid-19th century, such as in 1856, when Rabbi and historian Morris Jacob Raphall used the abbreviation in his book \"Post-Biblical History of The Jews\".\n\nIn the 200 years between 1808 and 2008 the ratio of usage of BCE to BC has increased by about 20% and CE to AD by about 50%, primarily since 1980.\n\nSome academics in the fields of theology, education and history have adopted CE and BCE notation, although there is some disagreement.\n\nMore visible uses of Common Era notation have recently surfaced at major museums in the English-speaking world. Furthermore, several style guides now prefer or mandate its usage.\nEven some style guides for Christian churches prefer its use: for example, the Episcopal Diocese \"Maryland Church News\".\n\nIn the United States, the usage of the BCE/CE notation in textbooks is growing. Some publications have moved over to using it exclusively. For example, the 2007 World Almanac was the first edition to switch over to the BCE/CE usage, ending a 138-year usage of the traditional BC/AD dating notation. It is used by the College Board in its history tests, and by the Norton Anthology of English Literature. Others have taken a different approach. The US-based History Channel uses BCE/CE notation in articles on non-Christian religious topics such as Jerusalem and Judaism.\n\nIn 2002, England and Wales introduced the BCE/CE notation system into the official school curriculum.\n\nIn June 2006, in the United States, the Kentucky State School Board reversed its decision to use BCE and CE in the state's new Program of Studies, leaving education of students about these concepts a matter of discretion at the local level.\n\nAlso in 2011, media reports suggested that the BC/AD notation in Australian school textbooks would be replaced by BCE/CE notation. The story became national news and drew opposition from some politicians and church leaders. Weeks after the story broke, the Australian Curriculum, Assessment and Reporting Authority denied the rumour and stated that the BC/AD notation would remain, with CE and BCE as an optional suggested learning activity.\n\nThe use of CE in Jewish scholarship was historically motivated by the desire to avoid the implicit \"Our Lord\" in the abbreviation \"AD\". Although other aspects of dating systems are based in Christian origins, AD is a direct reference to Jesus as Lord.\n\nProponents of the Common Era notation assert that the use of BCE/CE shows sensitivity to those who use the same year numbering system as the one that originated with and is currently used by Christians, but who are not themselves Christian.\n\nFormer United Nations Secretary-General Kofi Annan, himself a Protestant, has argued:\n\n[T]he Christian calendar no longer belongs exclusively to Christians. People of all faiths have taken to using it simply as a matter of convenience. There is so much interaction between people of different faiths and cultures – different civilizations, if you like – that some shared way of reckoning time is a necessity. And so the Christian Era has become the Common Era.\n\nAdena K. Berkowitz, when arguing at the Supreme Court opted to use BCE and CE because \"Given the multicultural society that we live in, the traditional Jewish designations – B.C.E. and C.E. – cast a wider net of inclusion\" \n\nSome oppose the Common Era notation for explicitly religious reasons. Because the BC/AD notation is based on the traditional year of the conception or birth of Jesus, some Christians are offended by the removal of the reference to him in era notation. The Southern Baptist Convention supports retaining the BC/AD abbreviations.\n\nThere are also secular concerns. English language expert Kenneth G. Wilson speculated in his style guide that \"if we do end by casting aside the AD/BC convention, almost certainly some will argue that we ought to cast aside as well the conventional numbering system [that is, the method of numbering years] itself, given its Christian basis.\" The short lived French Republican Calendar, for example, began with the first year of the French First Republic and rejected the seven-day week (with its connections to the Book of Genesis) for a ten-day week.\nPriest and writer on interfaith issues Raimon Panikkar contends that using the designation BCE/CE is a \"return... to the most bigoted Christian colonialism\" towards non-Christians, who do not necessarily consider the time period following the beginning of the calendar to be a \"common era\".\n\nAccording to a \"Los Angeles Times\" report, it was a student's use of BCE/CE notation, inspired by its use within Wikipedia, which prompted the teacher and politician Andrew Schlafly to found Conservapedia, a cultural conservative wiki. One of its \"Conservapedia Commandments\" is that users must always apply BC/AD notation, since its sponsors perceive BCE/CE notation to \"deny the historical basis\" of the dating system.\n\nThe abbreviation BCE, just as with BC, always follows the year number. Unlike AD, which traditionally precedes the year number, CE always follows the year number (if context requires that it be written at all). Thus, the current year is written as in both notations (or, if further clarity is needed, as CE, or as AD ), and the year that Socrates died is represented as 399 BCE (the same year that is represented by 399 BC in the BC/AD notation). The abbreviations are sometimes written with small capital letters, or with periods (e.g., \"B.C.E.\" or \"C.E.\"). Style guides for academic texts on religion generally prefer BCE/CE to BC/AD.\n\nSeveral languages other than English also have both religious and non-religious ways of identifying the era used in dates. In some communist states during the Cold War period, usage of non-religious notation was mandated.\n\n\n"}
{"id": "6091", "url": "https://en.wikipedia.org/wiki?curid=6091", "title": "Charles Robert Malden", "text": "Charles Robert Malden\n\nCharles Robert Malden (9 August 1797 – 23 May 1855), was a nineteenth-century British naval officer, surveyor and educator. He is the discoverer of Malden Island in the central Pacific, which is named in his honour. He also founded Windlesham House School at Brighton, England.\n\nMalden was born in Putney, Surrey, son of Jonas Malden, a surgeon. He entered British naval service at the age of 11 on 22 June 1809. He served nine years as a volunteer 1st class, midshipman, and shipmate, including one year in the English Channel and Bay of Biscay (1809), four years at the Cape of Good Hope and in the East Indies (1809–14), two and a half years on the North American and West Indian stations (1814–16), and a year and a half in the Mediterranean (1817–18). He was present at the capture of Mauritius and Java, and at the battles of Baltimore and New Orleans.\n\nHe passed the examination in the elements of mathematics and the theory of navigation at the Royal Naval Academy on 2–4 September 1816, and became a 1st Lieutenant on 1 September 1818. In eight years of active service as an officer, he served two and a half years in a surveying ship in the Mediterranean (1818–21), one and a half years in a surveying sloop in the English Channel and off the coast of Ireland (1823–24), and one and a half years as Surveyor of the frigate during a voyage (1824–26) to and from the Hawaiian Islands (then known as the \"Sandwich islands\").\nIn Hawaii he surveyed harbours which, he noted, were \"said not to exist by Captains Cook and Vancouver.\" On the return voyage he discovered and explored uninhabited Malden Island in the central Pacific on 30 July 1825. After his return he left active service but remained at half pay. He served for several years as hydrographer to King William IV.\n\nHe married Frances Cole, daughter of Rev. William Hodgson Cole, rector of West Clandon and Vicar of Wonersh, near Guildford, Surrey, on 8 April 1828. Malden became the father of seven sons and a daughter.\n\nFrom 1830-36 he took pupils for the Royal Navy at Ryde, Isle of Wight. He purchased the school of Henry Worsley at Newport, Isle of Wight, in December 1836, reopened it as a preparatory school on 20 February 1837, and moved it to Montpelier Road in Brighton in December 1837. He built the Windlesham House School at Brighton in 1844, and conducted the school until his death there in 1855.\n"}
{"id": "6094", "url": "https://en.wikipedia.org/wiki?curid=6094", "title": "CPD", "text": "CPD\n\nCPD may refer to:\n\n\n\n\n"}
{"id": "6095", "url": "https://en.wikipedia.org/wiki?curid=6095", "title": "Chechnya", "text": "Chechnya\n\nThe Chechen Republic (; ; , \"Noxçiyn Respublika\"), commonly referred to as Chechnya (; ; , \"Noxçiyçö\"), is a federal subject (a republic) of Russia.\n\nIt is located in the North Caucasus, situated in the southernmost part of Eastern Europe, and within of the Caspian Sea. The capital of the republic is the city of Grozny. , the republic was reported to have a population of 1,268,989 people; however, that number has been questioned by multiple demographers, who think such population growth after two deadly wars is highly implausible.\n\nAfter the dissolution of the Soviet Union in 1991, the Chechen-Ingush ASSR was split into two parts: the Republic of Ingushetia and the Chechen Republic. The latter proclaimed the Chechen Republic of Ichkeria, which sought independence. Following the First Chechen War with Russia, Chechnya gained \"de facto\" independence as the Chechen Republic of Ichkeria. Russian federal control was restored during the Second Chechen War. Since then there has been a systematic reconstruction and rebuilding process, though sporadic fighting continues in the mountains and southern regions of the republic.\n\nAccording to Leonti Mroveli, the 11th-century Georgian chronicler, the word Caucasian is derived from the Vainakh ancestor Kavkas.\nAccording to Professor George Anchabadze of Ilia State University American linguist Dr. Johanna Nichols \"has used language to connect the modern people of the Caucasus region to the ancient farmers of the Fertile Crescent\" and her research suggests that \"farmers of the region were proto-Nakh-Daghestanians.\" Nichols stated: \"The Nakh–Dagestanian languages are the closest thing we have to a direct continuation of the cultural and linguistic community that gave rise to Western civilization.\" Dr. Henry Harpending, University of Utah, supports her claims.\n\nPeople living in prehistoric mountain cave settlements used tools, mastered fire, and used animal skin for warmth and other purposes. Traces of human settlement that date back to 40,000 BC were found near Lake Kezanoi. Cave paintings, artifacts, and other archaeological evidence indicates continuous habitation for some 8,000 years.\n\n10,000–8000 BC\n\n6000–4000 BC\n\n4000–3000 BC\n\n900–1200 AD\n\n1239 AD\n\n1300–1400 AD\n\n1500 AD\n\nAs Russia set off for the first time to increase its political influence in the Caucasus and the Caspian Sea at the expense of Safavid Persia, Peter I launched the Russo-Persian War (1722–1723), in which Russia succeeded in taking much of the Caucasian territories from Iran for several years. Notable in Chechen history, this particular Russo-Persian War marked the first military encounter between Imperial Russia and the Vainakh.\n\nAs the Russians took control of the Caspian corridor and moved into Persian-ruled Dagestan, Peter's forces ran into mountain tribes. Peter sent a cavalry force to subdue them, but the Chechens routed them. In 1732, after Russia already ceded back most of the Caucasus to Persia, now led by Nader Shah, following the Treaty of Resht, Russian troops clashed again with Chechens in a village called Chechen-aul along the Argun River. The Russians were defeated again and withdrew, but this battle is responsible for the apocryphal story about how the Nokchi came to be known as \"Chechens\"-the people ostensibly named for the place the battle had taken place. The name Chechen was however already used since as early as 1692.\n\nUnder intermittent Persian rule since 1555, in 1783 the eastern Georgians of Kartl-Kakheti led by Erekle II and Russia signed the Treaty of Georgievsk. According to this treaty, Kartl-Kakheti received protection from Russia, and Georgia abjured any dependence on Iran. In order to increase its influence in the Caucasus and to secure communications with Kartli and other minority Christian regions of the Transcaucasia which it considered useful in its wars against Persia and Turkey, the Russian Empire began conquering the Northern Caucasus mountains. The Russian Empire used Christianity to justify its conquests, allowing Islam to spread widely because it positioned itself as the religion of liberation from tsardom, which viewed Nakh tribes as \"bandits\". The rebellion was led by Mansur Ushurma, a Chechen Naqshbandi (Sufi) sheikh—with wavering military support from other North Caucasian tribes. Mansur hoped to establish a Transcaucasus Islamic state under shari'a law. He was unable to fully achieve this because in the course of the war he was betrayed by the Ottomans, handed over to Russians, and executed in 1794.\n\nFollowing the forced ceding of the current territories of Dagestan, most of Azerbaijan, and Georgia by Persia to Russia, following the Russo-Persian War (1804–1813) and its outcoming Treaty of Gulistan, Russia significantly widened its foothold in the Caucasus at the expense of Persia. Another successful Caucasus war against Persia several years later, starting in 1826 and ending in 1828 with the Treaty of Turkmenchay, and a successful war against Ottoman Turkey in 1828, enabled Russia to use a much larger portion of its army in subduing the natives of the North Caucasus.\n\nThe resistance of the Nakh tribes never ended and was a fertile ground for a new Muslim-Avar commander, Imam Shamil, who fought against the Russians from 1834 to 1859 (see Murid War). In 1859, Shamil was captured by Russians at aul Gunib. Shamil left Boysangur Benoiski, a Chechen with one arm, one eye, and one leg, in charge of command at Gunib. Benoiski broke through the siege and continued to fight Russia for another two years until he was captured and killed by Russians. The Russian tsar hoped that by sparing the life of Shamil, the resistance in the North Caucasus would stop, but it did not. Russia began to use a colonization tactic by destroying Nakh settlements and building Cossack defence lines in the lowlands. The Cossacks suffered defeat after defeat, and were constantly attacked by mountaineers, who were robbing them of food and weaponry.\n\nThe tsarists' regime used a different approach at the end of the 1860s. They offered Chechens and Ingush to leave the Caucasus for the Ottoman Empire (see Muhajir (Caucasus)). It is estimated that about 80% of Chechens and Ingush left the Caucasus during the deportation. It weakened the resistance which went from open warfare to insurgent warfare. One of the notable Chechen resistance fighters at the end of the 19th century was a Chechen abrek Zelimkhan Gushmazukaev and his comrade-in-arms Ingush abrek Sulom-Beck Sagopshinski. Together they built up small units which constantly harassed Russian military convoys, government mints, and government post-service, mainly in Ingushetia and Chechnya. Ingush aul Kek was completely burned when the Ingush refused to hand over Zelimkhan. Zelimkhan was killed in the beginning of the 20th century. The war between Nakh tribes and Russia resurfaced during the times of the Russian Revolution, which saw the Nakh struggle against Anton Denikin, and later against the Soviet Union.\n\nOn December 21, 1917, Ingushetia, Chechnya, and Dagestan declared independence from Russia and formed a single state: \"United Mountain Dwellers of the North Caucasus\" (also known as Mountainous Republic of the Northern Caucasus) which was recognized by major world powers. The capital of the new state was moved to Temir-Khan-Shura (Dagestan). Tapa Chermoyev, a prominent Chechen statesman, was elected the first prime minister of the state. The second prime minister elected was Vassan-Girey Dzhabagiev, an Ingush statesman, who also was the author of the constitution of the republic in 1917, and in 1920 he was re-elected for the third term. In 1921 the Russians attacked and occupied the country and forcefully absorbed it into the Soviet state. The Caucasian war for independence restarted, and the government went into exile.\n\nDuring Soviet rule, Chechnya and Ingushetia were combined to form Chechen-Ingush Autonomous Soviet Socialist Republic. In the 1930s Chechnya was flooded with many Ukrainians fleeing the Holodomor. As the result many of the Ukrainians settled in Chechen-Ingush ASSR permanently and survived the famine.\nAlthough over 50,000 Chechens and over 12,000 Ingush were fighting against Nazi Germany on the front line (including heroes of the USSR: Abukhadzhi Idrisov, Khanpasha Nuradilov, Movlid Visaitov), and although Nazi German troops were fought to a complete stop at two Chechen-Ingush ASSR cities Malgobek and Ordzhonikidze (renamed to Vladikavkaz) after capturing half of the Caucasus in less than a month; Chechens and Ingush were falsely accused as Nazi supporters and entire nations were deported during Operation Lentil to the Kazakh SSR (later Kazakhstan) in 1944 near the end of World War II where over 60% of Chechen and Ingush populations perished. American historian Norman Naimark writes: The deportation was supposedly justified by the materials prepared by notorious NKVD officer Bogdan Kobulov accusing Chechens and Ingush in a mass conspiracy preparing rebellion and providing assistance to the German forces. Many of the materials were later proved to be fabricated. Even distinguished Red Army officers who fought bravely against Germans (e.g. the commander of 255th Separate Chechen-Ingush regiment Movlid Visaitov, the first to contact American forces at Elbe river) were deported. There is a theory that the real reason why Chechens and Ingush were deported is the desire of Russia to attack Turkey, a non-communist country, as Chechens and Ingush could impede such plans. In 2004, the European Parliament recognized the deportation of Chechens and Ingush as an act of genocide.\n\nThe territory of the Chechen-Ingush Autonomous Soviet Socialist Republic was divided between Stavropol Krai (where Grozny Okrug was formed), the Dagestan ASSR, the North Ossetian ASSR, and the Georgian SSR.\n\nThe Chechens and Ingush were allowed to return to their land after 1956 during de-Stalinization under Nikita Khrushchev when Chechen-Ingush Autonomous Soviet Socialist Republic was restored but both boundaries and ethnic composition of the territory significantly changed. There were many (predominantly Russian) migrants from other parts of the Soviet Union, who often settled in the abandoned family homes of Chechens and Ingushes. The republic lost its Prigorodny District which transferred to North Ossetian ASSR, but gained predominantly Russian Naursky District and Shelkovskoy District that is considered the homeland for Terek Cossacks.\n\nThe Russification policies towards Chechens continued after 1956, with Russian language proficiency required in many aspects of life, and for advancement in the Soviet system.\n\nOn November 26, 1990, the Supreme Council of Chechen-Ingush ASSR adopted the \"Declaration of State Sovereignty of the Chechen-Ingush Republic\". This declaration was part of the reorganization of the Soviet Union. This new treaty would have been signed August 22, 1991, which would have transformed 15 republic states into more than 80. The August 19–21, 1991 Soviet coup d'état attempt led to the abandonment of this reorganization.\n\nWith the impending dissolution of the Soviet Union in 1991, an independence movement, the Chechen National Congress, was formed, led by ex-Soviet Air Force general and new Chechen President Dzhokhar Dudayev. It campaigned for the recognition of Chechnya as a separate nation. This movement was opposed by Boris Yeltsin's Russian Federation, which argued that Chechnya had not been an independent entity within the Soviet Union—as the Baltic, Central Asian, and other Caucasian States had—but was part of the Russian Soviet Federative Socialist Republic and hence did not have a right under the Soviet constitution to secede. It also argued that other republics of Russia, such as Tatarstan, would consider seceding from the Russian Federation if Chechnya were granted that right. Finally, it argued that Chechnya was a major hub in the oil infrastructure of the federation and hence its secession would hurt the country's economy and energy access.\n\nIn the ensuing decade, the territory was locked in an ongoing struggle between various factions, usually fighting unconventionally and forgoing the position held by the several successive Russian governments through the current administration.\n\nThe First Chechen War took place over a two-year period that lasted from 1994 to 1996, when Russian forces attempted to regain control over Chechnya, which had declared independence in November 1991. Despite overwhelming numerical superiority in men, weaponry, and air support, the Russian forces were unable to establish effective permanent control over the mountainous area due to numerous successful full-scale battles and insurgency raids. In three months, Russia lost more tanks (over 1,997 tanks) in Grozny than during the Battle of Berlin in 1945.\nThe Budyonnovsk hospital hostage crisis in 1995 shocked the Russian public and led to international condemnation of the Chechen rebels.\n\nIn April 1996 the first democratically elected president of Chechnya, Dzhokhar Dudayev, was killed by Russian forces using a booby trap bomb and a missile fired from a warplane, after he was located by triangulating the position of a satellite phone he was using.\n\nThe widespread demoralization of the Russian forces in the area and a successful offensive to re-take Grozny by Chechen resistance forces led by Aslan Maskhadov prompted Russian President Boris Yeltsin to declare a ceasefire in 1996, and sign a peace treaty a year later that saw a withdrawal of Russian forces.\n\nAfter the war, parliamentary and presidential elections took place in January 1997 in Chechnya and brought to power new President Aslan Maskhadov, chief of staff and prime minister in the Chechen coalition government, for a five-year term. Maskhadov sought to maintain Chechen sovereignty while pressing the Russian government to help rebuild the republic, whose formal economy and infrastructure were virtually destroyed. Russia continued to send money for the rehabilitation of the republic; it also provided pensions and funds for schools and hospitals. Most of these funds were taken by Chechen authorities and divided between favoured warlords. Nearly half a million people (40% of Chechnya's prewar population) had been internally displaced and lived in refugee camps or overcrowded villages. There was an economic downturn. Two Russian brigades were permanently stationed in Chechnya.\n\nIn lieu of the devastated economic structure, kidnapping emerged as the principal source of income countrywide, procuring over US$200 million during the three-year independence of the chaotic fledgling state, although victims were rarely killed. In 1998, 176 people were kidnapped, 90 of whom were released, according to official accounts. President Maskhadov started a major campaign against hostage-takers, and on October 25, 1998, Shadid Bargishev, Chechnya's top anti-kidnapping official, was killed in a remote-controlled car bombing. Bargishev's colleagues then insisted they would not be intimidated by the attack and would go ahead with their offensive. Political violence and religious extremism, blamed on \"Wahhabism\", was rife. In 1998, Grozny authorities declared a state of emergency. Tensions led to open clashes between the Chechen National Guard and Islamist militants, such as the July 1998 confrontation in Gudermes.\n\nThe War of Dagestan began on August 7, 1999, during which the Islamic International Brigade (IIPB) began an unsuccessful incursion into the neighbouring Russian republic of Dagestan in favor of the Shura of Dagestan which sought independence from Russia. In September, a series of apartment bombs that killed around 300 people in several Russian cities, including Moscow, were blamed on the Chechen separatists. Some journalists contested the official explanation, instead blaming the Russian Secret Service for blowing up the buildings to initiate a new military campaign against Chechnya. In response to the bombings, a prolonged air campaign of retaliatory strikes against the Ichkerian regime and a ground offensive that began in October 1999 marked the beginning of the Second Chechen War. Much better organized and planned than the first Chechen War, the Russian military took control over most regions. The Russian forces used brutal force, killing 60 Chechen civilians during a mop-up operation in Aldy, Chechnya on February 5, 2000. After the re-capture of Grozny in February 2000, the Ichkerian regime fell apart.\n\nChechen rebels continued to fight Russian troops and conduct terrorist attacks. In October 2002, 40–50 Chechen rebels seized a Moscow theater and took about 900 civilians hostage. The crisis ended with 117 hostages and up to 50 rebels dead, mostly due to an unknown aerosol pumped throughout the building by Russian special forces to incapacitate the people inside.\n\nIn September 2004, separatist rebels occupied a school in the town of Beslan, North Ossetia, demanding recognition of the independence of Chechnya and a Russian withdrawal. 1,100 people (including 777 children) were taken hostage. The attack lasted three days, resulting in the deaths of over 331 people, including 186 children.\n\nIn response to the increasing terrorism, Russia tightened its grip on Chechnya and expanded its anti-terrorist operations throughout the region. Russia installed a pro-Russian Chechen regime. In 2003, a referendum was held on a constitution that reintegrated Chechnya within Russia, but provided limited autonomy. According to the Chechen government, the referendum passed with 95.5% of the votes and almost 80% turnout. The Economist was skeptical of the results, arguing that \"few outside the Kremlin regard the referendum as fair\". After the 2004 school siege, Russian president Vladimir Putin announced sweeping security and political reforms, sealing borders in the Caucasus region and revealing plans to give the central government more power. He also vowed to take tougher action against domestic terrorism, including preemptive strikes against Chechen separatists. In 2005 and 2006, prominent separatist leaders Aslan Maskhadov and Shamil Basayev were killed.\n\nSince 2007, Chechnya has been run by Ramzan Kadyrov. Kadyrov's rule has been characterized by high level corruption, a terrible human rights record and a growing cult of personality. However, his rule has also seen Chechnya rebuild, with much of Grozny already reconstructed.\n\nIn April 2009, Russia ended its counter-terrorism operation and pulled out the bulk of its army. Insurgency in the North Caucasus continued even after this date. The Caucasus Emirate has fully adopted the tenets of being a Salafist-takfiri jihadist group through its strict adherence to upholding tawhid, its obedience to the literal interpretation of the Quran and the Sunnah, and its complete rejection of bid‘ah, taqlid, and ijtihad.\n\nSituated in the eastern part of the North Caucasus, partially in Eastern Europe, Chechnya is surrounded on nearly all sides by Russian Federal territory. In the west, it borders North Ossetia and Ingushetia, in the north, Stavropol Krai, in the east, Dagestan, and to the south, Georgia. Its capital is Grozny.\n\nRivers:\n\n\nThere are no true districts of Chechnya, but many believe that the different dialects of the Chechen language define different districts. The main dialects are:\nGrozny, also known as the Dzhokhar dialect, is the dialect of people who live in and in some towns around Grozny.\nNaskhish, a dialect spoken to the north east of Chechnya. The most notable difference in this dialect is the addition of the letters \"ȯ\", \"ј\" and \"є\"\nDay, pronounced like the word 'die' is spoken in a small section of the south, around and in the town of Day.\n\nThere are other dialects which are believed to define districts, but because these areas are so isolated, not much research has been done on these areas.\n\nAccording to the 2010 Census, the population of the republic is 1,268,989, up from 1,103,686 recorded in the 2002 Census. As of the 2010 Census, Chechens at 1,206,551 make up 95.3% of the republic's population. Other groups include Russians (24,382, or 1.9%), Kumyks (12,221, or 1%), Ingush (1,296 or 0.1%) and a host of smaller groups, each accounting for less than 0.5% of the total population. The Armenian community, which used to number around 15,000 in Grozny alone, has dwindled to a few families. The Armenian church of Grozny was demolished in 1930. Birth rate was 25.41 in 2004. (25.7 in Achkhoi Martan, 19.8 in Groznyy, 17.5 in Kurchaloi, 28.3 in Urus Martan and 11.1 in Vedeno). According to the Chechen State Statistical Committee, Chechnya's population had grown to 1.205 million in January 2006.\n\nAt the end of the Soviet era, ethnic Russians (including Cossacks) comprised about 23% of the population (269,000 in 1989).\n\nAccording to some Russian sources, from 1991 to 1994 tens of thousands of people of non-Chechen ethnicity (mostly Russians, Ukrainians and Armenians) left the republic amidst reports of violence and discrimination against the non-Chechen population, as well as widespread lawlessness and ethnic cleansing under the government of Dzhokhar Dudayev.\n\nHowever, regarding this exodus, there is an alternative view. According to the Russian economists Boris Lvin and Andrei Iliaronov,\nThe Chechen authorities are regularly accused of crimes against the population, especially the Russian-speaking people. However, before the current war the emigration of the Russian-speaking population from Chechnya was no more intense than that from Kalmykia, Tuva and Sakha-Yakutia. In Grozny itself there remained a 200,000 strong Russian-speaking population which did not hasten to leave it.\nThe languages used in the Republic are Chechen and Russian. Chechen belongs to the Vaynakh or North-central Caucasian language family, which also includes Ingush and Batsb. Some scholars place it in a wider Iberian-Caucasian super-family.\n\nChechnya has one of the youngest populations in the generally aging Russian Federation; in the early 1990s, it was among the few regions experiencing natural population growth. Since 2002, Chechnya has experienced a classic post-conflict baby-boom. Chechen demographers in 2008 termed highly implausible the reported overall population growth as infant mortality in Chechnya was said to be 60 percent higher than the Russian average in 2007 and to have risen by 3.9 percent compared with 2006. Many experts have expressed doubts about the increase from 1.1 million in the 1990 to an estimated nearly 1.3 million in 2010 following two devastating wars that displaced hundreds of thousands people and virtually eliminated the large ethnic Russian minority in the republic. According to Russian demographer Dmitry Bogoyavlensky, the 2002 census results were clearly manipulated in the North Caucasus: an estimated 800,000 to 1 million non-existent people were added to the actual population of the region. Another Russian demographer, Anatoly Vishnevsky, pointed out that according to the 2002 census, some age groups, like those born in 1950, appeared to be larger in 2002 than in 1989. With the 2002 census, Moscow wanted to show there were not too many casualties and that the refugees had returned to Chechnya, while the local authorities wanted to receive more funds and thus needed a higher population to justify their demands. Also, in the multiethnic republics of North Caucasus normally unlike in other parts of Russia, government positions are distributed among the ethnicities according to their ratio in the general population. So ethnicities are zealously guarding their numbers in order not to be outnumbered by others and thereby left with less representation in the government and the local economy. Some 40 percent of newborns had some kind of genetic defect.\n\nNote: TFR 2009-12 source.\n\nIslam is the predominant religion in Chechnya, practised by 95% of those polled in Grozny in 2010. Chechens are overwhelmingly adherents to the Shafi'i Madhhab of Sunni Islam, the republic having converted to Islam between the 16th and the 19th centuries. Due to historical importance, many Chechens are Sufis, of either the Qadiri or Naqshbandi orders. Most of the population follows either the Shafi'i or the Hanafi, schools of jurisprudence, fiqh. The Shafi'i school of jurisprudence has a long tradition among the Chechens, and thus it remains the most practiced.\n\nThe once-strong Russian minority in Chechnya, mostly Terek Cossacks and estimated as numbering approximately 25,000 in 2012, are predominantly Russian Orthodox, although presently only one church exists in Grozny. In August 2011, Archbishop Zosima of Vladikavkaz and Makhachkala performed the first mass baptism ceremony in the history of Chechen republic in the Terek River of Naursky District in which 35 citizens of Naursky and Shelkovsky districts were converted to Orthodoxy.\n\nOn 19 January 2015, 12 days after the Charlie Hebdo shooting, a march took place in Grozny against the publication of caricatures of the prophet Mohammed. The Chechen Ministry of Interior reported that more than a million people participated, while according to the sources of Caucasian Knot the number was between 350,000 and 500,000.\n\nSince 1990, the Chechen Republic has had many legal, military, and civil conflicts involving separatist movements and pro-Russian authorities. Today, Chechnya is a relatively stable federal republic, although there is still some separatist movement activity. Its regional constitution entered into effect on April 2, 2003 after an all-Chechen referendum was held on March 23, 2003. Some Chechens were controlled by regional teips, or clans, despite the existence of pro- and anti-Russian political structures.\n\nThe former separatist religious leader (mufti) Akhmad Kadyrov, looked upon as a traitor by many separatists, was elected president with 83% of the vote in an internationally monitored election on October 5, 2003. Incidents of ballot stuffing and voter intimidation by Russian soldiers and the exclusion of separatist parties from the polls were subsequently reported by the Organization for Security and Co-operation in Europe (OSCE) monitors. On May 9, 2004, Kadyrov was assassinated in Grozny football stadium by a landmine explosion that was planted beneath a VIP stage and detonated during a parade, and Sergey Abramov was appointed to the position of acting prime minister after the incident. However, since 2005 Ramzan Kadyrov (son of Akhmad Kadyrov) has been caretaker prime minister, and in 2007 was appointed a new president. Many allege he is the wealthiest and most powerful man in the republic, with control over a large private militia referred to as the \"Kadyrovtsy\". The militia, which began as his father's security force, has been accused of killings and kidnappings by human rights organizations such as Human Rights Watch.\n\nIn 2009, the American organization Freedom House included Chechnya in the \"Worst of the Worst\" list of most repressive societies in the world, together with Burma, North Korea, Tibet, and others.\n\nIn addition to the Russian regional government, there was a separatist Ichkeria government that was not recognized by any state (although members have been given political asylum in European and Arab countries, as well as the United States).\n\nIchkeria is/was a member of the Unrepresented Nations and Peoples Organization. Former president of Georgia Zviad Gamsakhurdia deposed in a military coup of 1991 and a participant of the Georgian Civil War, recognised the independence of Chechen Republic of Ichkeria in 1993. Diplomatic relations with Ichkeria were also established by the partially recognized Islamic Emirate of Afghanistan under the Taliban government on January 16, 2000. This recognition ceased with the fall of the Taliban in 2001. However, despite Taliban recognition, there were no friendly relations between the Taliban and Ichkeria—Maskhadov rejected their recognition, stating that the Taliban were illegitimate. Ichkeria also received vocal support from the Baltic countries, a group of Ukrainian nationalists and Poland; Estonia once voted to recognize, but the act never was followed through due to pressure applied by both Russia and the EU.\n\nThe president of this government was Aslan Maskhadov, the Foreign Minister was Ilyas Akhmadov, who was the spokesman for Maskhadov. Aslan Maskhadov had been elected in an internationally monitored election in 1997 for 4 years, which took place after signing a peace agreement with Russia. In 2001 he issued a decree prolonging his office for one additional year; he was unable to participate in the 2003 presidential election, since separatist parties were barred by the Russian government, and Maskhadov faced accusations of terrorist offences in Russia. Maskhadov left Grozny and moved to the separatist-controlled areas of the south at the onset of the Second Chechen War. Maskhadov was unable to influence a number of warlords who retain effective control over Chechen territory, and his power was diminished as a result. Russian forces killed Maskhadov on March 8, 2005, and the assassination of Maskhadov was widely criticized since it left no legitimate Chechen separatist leader with whom to conduct peace talks. Akhmed Zakayev, Deputy Prime Minister and a Foreign Minister under Maskhadov, was appointed shortly after the 1997 election and is currently living under asylum in England. He and others chose Abdul Khalim Saidullayev, a relatively unknown Islamic judge who was previously the host of an Islamic program on Chechen television, to replace Maskhadov following his death. On June 17, 2006, it was reported that Russian special forces killed Abdul Khalim Saidullayev in a raid in a Chechen town Argun.\n\nThe successor of Saidullayev became Doku Umarov. On October 31, 2007 Umarov abolished the Chechen Republic of Ichkeria and its presidency and in its place proclaimed the Caucasian Emirate with himself as its Emir. This change of status has been rejected by many Chechen politicians and military leaders who continue to support the existence of the republic.\n\nThe Internal Displacement Monitoring Centre reports that after hundreds of thousands of ethnic Russians and Chechens fled their homes following inter-ethnic and separatist conflicts in Chechnya in 1994 and 1999, more than 150,000 people still remain displaced in Russia today.\n\nOn September 1, 1997, Criminal Code reportedly being implemented in the Chechen Republic-Ichkeriya, Article 148 punishes \"anal sexual intercourse between a man and a woman or a man and a man\". For first- and second-time offenders, the punishment is caning. A third conviction leads to the death penalty, which can be carried out in a number of ways including stoning or beheading.\n\nHuman rights groups criticized the conduct of the 2005 parliamentary elections as unfairly influenced by the central Russian government and military.\n\nIn 2006 Human Rights Watch reported that pro-Moscow Chechen forces under the command, in effect, of chapter of republic Ramzan Kadyrov, as well as federal police personnel, used torture to get information about separatist forces. \"If you are detained in Chechnya, you face a real and immediate risk of torture. And there is little chance that your torturer will be held accountable\", said Holly Cartner, Director Europe and Central Asia division of HRW.\n\nOn February 1, 2009, the \"New York Times\" released extensive evidence to support allegations of consistent torture and executions under the Kadyrov government. The accusations were sparked by the assassination in Austria of a former Chechen rebel who had gained access to Kadyrov's inner circle, 27-year-old Umar Israilov.\n\nOn July 1, 2009, Amnesty International released a detailed report covering the human rights violations committed by the Russian Federation against Chechen citizens. Among the most prominent features was that those abused had no method of redress against assaults, ranging from kidnapping to torture, while those responsible were never held accountable. This led to the conclusion that Chechnya was being ruled without law, being run into further devastating destabilization.\n\nOn March 10, 2011, Human Rights Watch reported that since Chechenization, the government has pushed for enforced Islamic dress code and other traditions which violently repress women. The president Ramzan Kadyrov is quoted as saying \"I have the right to criticize my wife. She doesn’t. With us [in Chechen society], a wife is a housewife. A woman should know her place. A woman should give her love to us [men]... She would be [man’s] property. And the man is the owner. Here, if a woman does not behave properly, her husband, father, and brother are responsible. According to our tradition, if a woman fools around, her family members kill her... That’s how it happens, a brother kills his sister or a husband kills his wife... As a president, I cannot allow for them to kill. So, let women not wear shorts...\". He has also openly defended honor killings on several occasions.\n\nOn July 9, 2017, Russian newspaper Novaya Gazeta reported that a number of people were subject to an extra-judicial execution on the night of January 26, 2017. It published 27 names of the people known to be dead, but stressed that the list is \"not all [of those killed]\"; the newspaper asserted that 50 people may have been killed in the execution. Some of the dead were gay, but not all; the deaths appeared to have been triggered by the death of a policeman, and according to the author of the report, Elena Milashina, were executed for terrorism.\n\nIn 2017, it was reported by Novaya Gazeta and human rights groups that Chechen authorities had allegedly set up concentration camps, one of which is in Argun, where gay men are interrogated and subjected to physical violence.\n\nDuring the war, the Chechen economy fell apart. In 1994, the separatists planned to introduce a new currency, but that did not happen due to Russian troops re-taking Chechnya in the Second Chechen War. \n\nThe economic situation in Chechnya has improved considerably since 2000. According to the \"New York Times\", major efforts to rebuild Grozny have been made, and improvements in the political situation have led some officials to consider setting up a tourism industry, though there are claims that construction workers are being irregularly paid and that poor people have been displaced.\n\nChechnya's unemployment was 67% in 2006 and fell to 21.5% in 2014\n\nTotal revenues of the budget of Chechnya for 2017 are 59.2 billion rubles. Of these, 48.5 billion rubles are so-called \"gratuitous receipts\" from the federal budget of the Russian Federation.\n\n\n\n\n"}
{"id": "6097", "url": "https://en.wikipedia.org/wiki?curid=6097", "title": "Canonization", "text": "Canonization\n\nCanonization is the act by which a Christian church declares that a person who has died was a saint, upon which declaration the person is included in the \"canon\", or list, of recognized saints. Originally, persons were recognized as saints without any formal process. Later, different processes were developed, such as those used today in the Anglican Communion, Eastern Orthodox Church and Roman Catholic Church.\n\nThe first persons honored as saints were the martyrs. Pious legends of their deaths were considered affirmations of the truth of their faith in Christ.\n\nThe Roman Rite's Canon of the Mass contains only the names of martyrs, along with that of the Blessed Virgin Mary and, since 1962, that of St. Joseph her spouse.\n\nBy the fourth century, however, \"confessors\"—people who had confessed their faith not by dying but by word and life—began to be venerated publicly. Examples of such people are Saint Hilarion and Saint Ephrem the Syrian in the East, and Saint Martin of Tours and Saint Hilary of Poitiers in the West. Their names were inserted in the diptychs, the lists of saints explicitly venerated in the liturgy, and their tombs were honoured in like manner as those of the martyrs. Since the witness of their lives was not as unequivocal as that of the martyrs, they were venerated publicly only with the approval by the local bishop. This process is often referred to as \"local canonization\".\n\nThis approval was required even for veneration of a reputed martyr. In his history of the Donatist heresy, Saint Optatus recounts that at Carthage a Catholic matron, named Lucilla, incurred the censures of the Church for having kissed the relics of a reputed martyr whose claims to martyrdom had not been juridically proved. And Saint Cyprian (died 258) recommended that the utmost diligence be observed in investigating the claims of those who were said to have died for the faith. All the circumstances accompanying the martyrdom were to be inquired into; the faith of those who suffered, and the motives that animated them were to be rigorously examined, in order to prevent the recognition of undeserving persons. Evidence was sought from the court records of the trials or from people who had been present at the trials.\n\nSaint Augustine of Hippo (died 430) tells of the procedure which was followed in his day for the recognition of a martyr. The bishop of the diocese in which the martyrdom took place set up a canonical process for conducting the inquiry with the utmost severity. The acts of the process were sent either to the metropolitan or primate, who carefully examined the cause, and, after consultation with the suffragan bishops, declared whether the deceased was worthy of the name of 'martyr' and public veneration.\n\nActs of formal recognition, such as the erection of an altar over the saint's tomb or transferring the saint's relics to a church, were preceded by formal inquiries into the sanctity of the person's life and the miracles attributed to that person's intercession.\n\nSuch acts of recognition of a saint were authoritative, in the strict sense, only for the diocese or ecclesiastical province for which they were issued, but with the spread of the fame of a saint, were often accepted elsewhere also.\n\nIn the Catholic Church, both Latin and Eastern Churches, the act of canonization is reserved to the Apostolic See and occurs at the conclusion of a long process requiring extensive proof that the candidate for canonization lived and died in such an exemplary and holy way that he is worthy to be recognized as a saint. The Church's official recognition of sanctity implies that the person is now in Heaven and that he may be publicly invoked and mentioned officially in the liturgy of the Church, including in the \"Litany of the Saints\". Other churches still use the older practice below (see, e. g., the practice of the Orthodox Church).\n\nIn the Catholic Church, canonization is a decree that allows universal veneration of the saint in the liturgy of the Roman Rite. For permission to venerate merely locally, only beatification is needed.\n\nFor several centuries the Bishops, or in some places only the Primates and Patriarchs, could grant martyrs and confessors public ecclesiastical honor; such honor, however, was always decreed only for the local territory of which the grantors had jurisdiction. Only acceptance of the \"cultus\" by the Pope made the \"cultus\" universal, because he alone can rule the universal Catholic Church. Abuses, however, crept into this discipline, due as well to indiscretions of popular fervor as to the negligence of some bishops in inquiring into the lives of those whom they permitted to be honoured as saints.\n\nIn the Medieval West, the Apostolic See was asked to intervene in the question of canonizations so as to ensure more authoritative decisions. The canonization of Saint Udalric, Bishop of Augsburg by Pope John XV in 993 was the first undoubted example of Papal canonization of a saint from outside of Rome; some historians maintain further that the first Papal canonization was of St. Swibert by Pope Leo III in 804.\n\nThereafter, recourse to the judgment of the Pope was had more frequently. Toward the end of the eleventh century the Popes judged it necessary to restrict episcopal authority regarding canonization, and therefore decreed that the virtues and miracles of persons proposed for public veneration should be examined in councils, more specifically in general councils. Pope Urban II, Pope Calixtus II, and Pope Eugene III conformed to this discipline.\n\nHugh de Boves, Archbishop of Rouen, canonized Walter of Pontoise, or St. Gaultier, in 1153, the final saint in Western Europe to be canonized by an authority other than the Pope: \"The last case of canonization by a metropolitan is said to have been that of St. Gaultier, or Gaucher, [A]bbot of Pontoise, by the Archbishop of Rouen. A decree of Pope Alexander III [in] 1170 gave the prerogative to the [P]ope thenceforth, so far as the Western Church was concerned.\" In a decretal of 1173, Pope Alexander III reprimanded some bishops for permitting veneration of a man who was merely killed while intoxicated, prohibited veneration of the man, and most significantly decreed that \"you shall not therefore presume to honor him in the future; for, even if miracles were worked through him, it is not lawful for you to venerate him as a saint without the authority of the Catholic Church.\" Theologians disagree as to the full import of the decretal of Pope Alexander III: either a new law was instituted, in which case the Pope then for the first time reserved the right of beatification to himself, or an existing law was confirmed.\n\nHowever, the procedure initiated by the decretal of Pope Alexander III was confirmed by a bull of Pope Innocent III issued on the occasion of the canonization of St. Cunegunda in 1200. The bull of Pope Innocent III resulted in increasingly elaborate inquiries to the Apostolic See concerning canonizations. Because the decretal of Pope Alexander III did not end all controversy and some bishops did not obey it in so far as it regarded beatification, the right of which they had certainly possessed hitherto, Pope Urban VIII issued the Apostolic letter \"Caelestis Hierusalem cives\" of 5 July 1634 that exclusively reserved to the Apostolic See both its immemorial right of canonization and that of beatification. He further regulated both of these acts by issuing his \"Decreta servanda in beatificatione et canonizatione Sanctorum\" on 12 March 1642.\n\nIn his \"De Servorum Dei beatifιcatione et de Beatorum canonizatione\" of five volumes the eminent canonist Prospero Lambertini (1675-1758), who later became Pope Benedict XIV, elaborated on the procedural norms of Pope Urban VIII's Apostolic letter \"Caelestis Hierusalem cives\" of 1634 and \"Decreta servanda in beatificatione et canonizatione Sanctorum\" of 1642, and on the conventional practice of the time. His work published from 1734-8 governed the proceedings until 1917. The article \"Beatification and canonization process in 1914\" describes the procedures followed until the promulgation of the \"Codex\" of 1917. The substance of \"De Servorum Dei beatifιcatione et de Beatorum canonizatione\" was incorporated into the \"Codex Iuris Canonici\" (\"Code of Canon Law\") of 1917, which governed until the promulgation of the revised \"Codex Iuris Canonici\" in 1983 by Pope St. John Paul II. Prior to promulgation of the revised \"Codex\" in 1983, Bl. Pope Paul VI initiated a simplification of the procedures.\n\nThe Apostolic constitution \"Divinus Perfectionis Magister\" of Pope St. John Paul II of 25 January 1983 and the norms issued by the Congregation for the Causes of Saints on 7 February 1983 to implement the constitution in dioceses, continued the simplification of the process initiated by Bl. Pope Paul VI. Contrary to popular belief, the reforms did not eliminate the office of the Promoter of the Faith (Latin: \"Promotor Fidei\"), popularly known as the \"Devil's Advocate\", whose office is to question the material presented in favor of canonization. The reforms were intended to reduce the adversarial nature of the process. In November 2012 Pope Benedict XVI appointed Monsignor Carmello Pellegrino as Promoter of the Faith.\n\nCandidates for canonization undergo the following process:\n\n\nThe satisfaction of the applicable conditions permits beatification, which then bestows on the Venerable the title of \"Blessed\" (Latin: \"Beatus\" or \"Beata\"). A feast day will be designated, but its observance is ordinarily only permitted for the Blessed's home diocese, to specific locations associated with him, and/or to the churches or houses of the Blessed's religious order if he belonged to one. Parishes may not normally be named in honor of beati.\n\n\nCanonization is a statement of the Church that the person certainly enjoys the Beatific Vision of Heaven. The title of \"Saint\" (Latin: \"Sanctus\" or \"Sancta\") is then proper, reflecting that the Saint is a refulgence of the holiness (\"sanctitas\") of God Himself, which alone comes from God's gift. The Saint is assigned a feast day which \"may\" be celebrated anywhere in the universal Church, although it is not necessarily added to the General Roman Calendar or local calendars as an \"obligatory\" feast; parish churches may be erected in his honor; and the faithful may freely celebrate and honor the Saint.\n\nAlthough recognition of sainthood by the Pope does not directly concern a fact of Divine revelation, nonetheless it must be \"definitively held\" by the faithful as \"infallible\" pursuant to, at the least, the Universal Magisterium of the Church, because it is a truth related to revelation by historical necessity.\n\nRegarding the Eastern Catholic Churches, individual \"sui juris\" churches have the right to \"glorify\" saints for their own jurisdictions, though this has rarely happened.\n\nPopes have several times permitted to the universal Church, without executing the ordinary judicial process of canonization described above, the veneration as a saint, the \"\"cultus\"\" of one long venerated as such locally. This act of a pope is denominated \"equipollent\" or \"equivalent canonization\" and \"confirmation of \"cultus\"\". According to the rules Pope Benedict XIV (\"regnat\" 17 August 1740 - 3 May 1758) instituted, there are three conditions for an equipollent canonization: (1) existence of an ancient \"cultus\" of the person, (2) a general and constant attestation to the virtues or martyrdom of the person by credible historians, and (3) uninterrupted fame of the person as a worker of miracles.\n\nAs examples, prior to his pontificate, of this mode of canonization, Pope Benedict XIV himself enumerated the equipollent canonizations of Saints:\n\nLater equipollent canonizations include those of Saints:\n\nPope Francis added Saints:\n\nFor the process by which the Orthodox Church grants official recognition to someone as a saint, see glorification.\n\nThe Church of England, the Mother Church of the Anglican Communion, canonized Charles I as a saint, in the Convocations of Canterbury and York of 1660.\n\n\n\nCatholic\n\nOrthodox\n"}
{"id": "6099", "url": "https://en.wikipedia.org/wiki?curid=6099", "title": "Carboxylic acid", "text": "Carboxylic acid\n\nA carboxylic acid is an organic compound that contains a carboxyl group (C(=O)OH). The general formula of a carboxylic acid is R–COOH, with R referring to the rest of the (possibly quite large) molecule. Carboxylic acids occur widely and include the amino acids (which make up proteins) and acetic acid (which is part of vinegar and occurs in metabolism).\n\nSalts and esters of carboxylic acids are called carboxylates. When a carboxyl group is deprotonated, its conjugate base forms a carboxylate anion. Carboxylate ions are resonance-stabilized, and this increased stability makes carboxylic acids more acidic than alcohols. Carboxylic acids can be seen as reduced or alkylated forms of the Lewis acid carbon dioxide; under some circumstances they can be decarboxylated to yield carbon dioxide.\n\nCarboxylic acids are commonly identified using their trivial names, and usually have the suffix \"-ic acid\". IUPAC-recommended names also exist; in this system, carboxylic acids have an \"-oic acid\" suffix. For example, butyric acid (CHCOH) is butanoic acid by IUPAC guidelines. The \"-oic acid\" nomenclature detail is based on the name of the previously-known chemical benzoic acid. For nomenclature of complex molecules containining a carboxylic acid, the carboxyl can be considered position one of the parent chain even if there are other substituents, for example, 3-chloropropanoic acid. Alternately, it can be named as a \"carboxy\" or \"carboxylic acid\" substituent on another parent structure, for example, 2-carboxyfuran.\n\nThe carboxylate anion (R–COO) of a carboxylic acid is usually named with the suffix \"-ate\", in keeping with the general pattern of \"-ic acid\" and \"-ate\" for a conjugate acid and its conjugate base, respectively. For example, the conjugate base of acetic acid is acetate.\n\nThe radical COOH (CAS# 2564-86-5) has only a separate fleeting existence. The acid dissociation constant of COOH has been measured using electron paramagnetic resonance spectroscopy. The carboxyl group tends to dimerise to form oxalic acid.\n\nCarboxylic acids are polar. Because they are both hydrogen-bond acceptors (the carbonyl –C=O) and hydrogen-bond donors (the hydroxyl –OH), they also participate in hydrogen bonding. Together the hydroxyl and carbonyl group forms the functional group carboxyl. Carboxylic acids usually exist as dimeric pairs in nonpolar media due to their tendency to \"self-associate.\" Smaller carboxylic acids (1 to 5 carbons) are soluble in water, whereas higher carboxylic acids are less soluble due to the increasing hydrophobic nature of the alkyl chain. These longer chain acids tend to be rather soluble in less-polar solvents such as ethers and alcohols.\n\nCarboxylic acids tend to have higher boiling points than water, not only because of their increased surface area, but because of their tendency to form stabilised dimers. Carboxylic acids tend to evaporate or boil as these dimers. For boiling to occur, either the dimer bonds must be broken or the entire dimer arrangement must be vaporised, both of which increase the enthalpy of vaporization requirements significantly.\n\nCarboxylic acids are Brønsted–Lowry acids because they are proton (H) donors. They are the most common type of organic acid.\n\nCarboxylic acids are typically weak acids, meaning that they only partially dissociate into H cations and RCOO anions in neutral aqueous solution. For example, at room temperature, in a 1-molar solution of acetic acid, only 0.4% of the acid molecules are dissociated. Electronegative substituents give stronger acids.\n\nDeprotonation of carboxylic acids gives carboxylate anions; these are resonance stabilized, because the negative charge is delocalized over the two oxygen atoms, increasing the stability of the anion. Each of the carbon–oxygen bonds in the carboxylate anion has a partial double-bond character.\n\nCarboxylic acids often have strong odors, especially the volatile derivatives. Most common are acetic acid (vinegar) and butyric acid (human vomit). Conversely esters of carboxylic acids tend to have pleasant odors and many are used in perfume.\n\nCarboxylic acids are readily identified as such by infrared spectroscopy. They exhibit a sharp band associated with vibration of the C–O vibration bond (\"ν\") between 1680 and 1725 cm. A characteristic \"ν\" band appears as a broad peak in the 2500 to 3000 cm region. By H NMR spectrometry, the hydroxyl hydrogen appears in the 10–13 ppm region, although it is often either broadened or not observed owing to exchange with traces of water.\n\nMany carboxylic acids are produced industrially on a large scale. They are also pervasive in nature. Esters of fatty acids are the main components of lipids and polyamides of aminocarboxylic acids are the main components of proteins.\n\nCarboxylic acids are used in the production of polymers, pharmaceuticals, solvents, and food additives. Industrially important carboxylic acids include acetic acid (component of vinegar, precursor to solvents and coatings), acrylic and methacrylic acids (precursors to polymers, adhesives), adipic acid (polymers), citric acid (beverages), ethylenediaminetetraacetic acid (chelating agent), fatty acids (coatings), maleic acid (polymers), propionic acid (food preservative), terephthalic acid (polymers).\n\nIn general, industrial routes to carboxylic acids differ from those used on smaller scale because they require specialized equipment.\n\n\nPreparative methods for small scale reactions for research or for production of fine chemicals often employ expensive consumable reagents.\n\nMany reactions afford carboxylic acids but are used only in specific cases or are mainly of academic interest:\n\nThe most widely practiced reactions convert carboxylic acids into esters, amides, carboxylate salts, acid chlorides, and alcohols. Carboxylic acids react with bases to form carboxylate salts, in which the hydrogen of the hydroxyl (–OH) group is replaced with a metal cation. Thus, acetic acid found in vinegar reacts with sodium bicarbonate (baking soda) to form sodium acetate, carbon dioxide, and water:\n\nCarboxylic acids also react with alcohols to give esters. This process is widely used, e.g. in the production of polyesters. Likewise, carboxylic acids are converted into amides, but this conversion typically does not occur by direct reaction of the carboxylic acid and the amine. Instead esters are typical precursors to amides. The conversion of amino acids into peptides is a major biochemical process that requires ATP.\n\nThe hydroxyl group on carboxylic acids may be replaced with a chlorine atom using thionyl chloride to give acyl chlorides. In nature, carboxylic acids are converted to thioesters.\n\nCarboxylic acid can be reduced to the alcohol by hydrogenation or using stoichiometric hydride reducing agents such as lithium aluminium hydride. \n\n\"N\",\"N\"-Dimethyl(chloromethylene)ammonium chloride (ClHC=N(CH)Cl) is a highly chemoselective agent for carboxylic acid reduction. It selectively activates the carboxylic acid to give the carboxymethyleneammonium salt, which can be reduced by a mild reductant like lithium tris(\"t\"-butoxy)aluminum hydride to afford an aldehyde in a one pot procedure. This procedure is known to tolerate reactive carbonyl functionalities such as ketone as well as moderately reactive ester, olefin, nitrile, and halide moieties.\n\n\n\n"}
{"id": "6100", "url": "https://en.wikipedia.org/wiki?curid=6100", "title": "Chernobyl", "text": "Chernobyl\n\nChernobyl or Chornobyl (IPA ; , ; , , , , ) is a city in the restricted Chernobyl Exclusion Zone situated in the Ivankiv Raion of northern Kiev Oblast, near Ukraine's border with Belarus. Chernobyl is about northeast of Kiev, and approximately southwest of the Belarusian city of Gomel. The city was the administrative center of Chernobyl Raion (district) from 1923 until it was disestablished in 1988. Before its evacuation, the city had about 14,000 residents. , the city has a population of 690.\n\nThe city was evacuated on 27 April 1986, 30 hours after the Chernobyl disaster at the Chernobyl Nuclear Power Plant which was the most disastrous nuclear accident in history. The power plant was within the Chernobyl Raion district. Pripyat, a city larger and closer to the power plant than Chernobyl, had been built as home for the power plant workers. After the accident, administration of the Chernobyl Raion district was transferred to the neighboring Ivankiv Raion. The city of Slavutych, built for those evacuated from Pripyat, received the population relocated from Chernobyl.\n\nToday Chernobyl is mostly a ghost town, but a small number of people still reside in houses marked with signs stating: \"Owner of this house lives here\". Workers on watch and administrative personnel of the Zone of Alienation are stationed in the city on a long-term basis. There are two general stores and a hotel for tourists.\n\nThe city's name is the same as the Ukrainian name for \"Artemisia vulgaris\" (mugwort or common wormwood, which is чорнобиль or \"chornobyl\").. An alternative etymology holds that it is a combination of the words \"chorniy\" (чорний, black) and \"byllia\" (билля, grass blades or stalks), in other words, black grass or black stalks.\n\nChernobyl was originally part of the land of Kievan Rus′. The first known mention of Chernobyl is from a 1193 charter, which describes it as a hunting-lodge of Knyaz Rurik Rostislavich. In the 13th century, it was a crown village of the Grand Duchy of Lithuania. The village was granted to Filon Kmita, a captain of the royal cavalry, as a fiefdom in 1566. The province where Chernobyl is located was transferred to the Kingdom of Poland in 1569, and later annexed by the Russian Empire in 1793. Prior to the 20th century, Chernobyl was inhabited by Ukrainian, some Polish peasants and a relatively large number of Jewish people.\n\nJews were brought to Chernpobyl by Filon Kmita, during the Polish campaign of colonization. After 1596, the traditionally Eastern Orthodox Ukrainian peasantry of the district were forcibly converted, by Poland, to the Greek Catholic Uniate religion. Many of these converts returned to Eastern Orthodoxy after the Partitions of Poland.\n\nIn 1626, during the Counter-reformation, the Dominican church and monastery were founded by Lukasz Sapieha. A group of Old Catholics opposed the decrees of the Council of Trent. In 1832, following the failed Polish November Uprising, the Dominican monastery was sequestrated. The church of the Old Catholics was disbanded in 1852.\n\nIn the second half of the 18th century, Chernobyl became a major center of Hasidic Judaism. The Chernobyl Hasidic dynasty had been founded by Rabbi Menachem Nachum Twersky. The Jewish population suffered greatly from pogroms in October 1905 and in March–April 1919; many Jews were killed or robbed at the instigation of the Russian nationalist Black Hundreds. When the Twersky Dynasty left Chernobyl in 1920, it ceased to exist as a centre of Hasidism.\n\nChernobyl had a population of 10,800 in 1898, including 7,200 Jews. Chernobyl was occupied in World War I; Ukrainians and Bolsheviks fought over the city in the ensuing Civil War. In the Polish–Soviet War of 1919–20, Chernobyl was taken first by the Polish Army and then by cavalry of the Red Army. From 1921 onwards, it was incorporated into the Ukrainian SSR.\n\nBetween 1929 and 1933, Chernobyl suffered from killings during Stalin's collectivization campaign. It was also affected by the famine that resulted from Stalin's policies. The Polish community of Chernobyl was deported to Kazakhstan in 1936, during the Frontier Clearances.\n\nDuring World War II, Chernobyl was occupied by the German Army from 25 August 1941 to 17 November 1943. The Jewish community was murdered during the Nazi occupation of 1941–44.\n\nTwenty years later, the area was chosen as the site of the first nuclear power station to be built on Ukrainian soil. The Duga-3 over-the-horizon radar array, several miles outside of Chernobyl, was the origin of the infamous Russian Woodpecker; it was designed as part of an anti-ballistic missile early warning radar network. With the dissolution of the Soviet Union in 1991, Chernobyl remained part of Ukraine.\n\nOn 26 April 1986, Reactor No. 4 at the Chernobyl Nuclear Power Plant exploded.\n\nChernobyl city was evacuated soon after the disaster. The base of operations for the administration and monitoring of the Chernobyl Exclusion Zone was moved from Pripyat to Chernobyl. Chernobyl currently contains offices for the State Agency of Ukraine on the Exclusion Zone Management and accommodations for visitors. Apartment blocks have been repurposed as accommodations for employees of the State Agency. The length of time that workers may spend within the Chernobyl Exclusion Zone is restricted by regulations that have been implemented to limit exposure to radiation.\n\nThe city has become overgrown and many types of animals live there. In fact, according to census information that was collected over an extended period of time, it is estimated that more mammals live there now than before the disaster.\n\nIn 2003, the United Nations Development Programme launched a project, called the Chernobyl Recovery and Development Programme (CRDP), for the recovery of the affected areas. The program, initiated in February 2002, based its activities on the Human Consequences of the Chernobyl Nuclear Accident report recommendations. The main goal of the CRDP's activities is supporting the efforts of the Government of Ukraine to mitigate the long-term social, economic, and ecological consequences of the Chernobyl disaster. CRDP works in the four areas of Ukraine that have been most affected by the Chernobyl nuclear accident: Kiev Oblast, Zhytomyrska Oblast, partially Kiev, Chernihivska Oblast, and Rivne Oblast.\n\n\"Chernobylite\" is the name cited by two media sources for highly radioactive, unusual and potentially novel crystalline formations found at the Chernobyl power plant after the meltdown. These formations were found in the basement below Reactor No. 4 during an investigation into missing reactor fuel.\n\n\n"}
{"id": "6102", "url": "https://en.wikipedia.org/wiki?curid=6102", "title": "Cyan", "text": "Cyan\n\nCyan ( or ) is a greenish-blue color. It is evoked by light with a predominant wavelength of between 490520 nm, between the wavelengths of blue and green.\n\nIn the subtractive color system, or CMYK (subtractive), which can be overlaid to produce all colors in paint and color printing, cyan is one of the primary colors, along with magenta, yellow, and black. In the additive color system, or RGB (additive) color model, used to create all the colors on a computer or television display, cyan is made by mixing equal amounts of green and blue light. Cyan is the complement of red; it can be made by the removal of red from white light. Mixing red light and cyan light at the right intensity will make white light.\n\nThe web color cyan is synonymous with aqua. Other colors in the cyan color range are teal, turquoise, electric blue, aquamarine, and others described as blue-green.\n\nIts name is derived from the Ancient Greek κύανος, transliterated \"kyanos\", meaning \"dark blue, dark blue enamel, lapis lazuli\". It was formerly known as \"cyan blue\" or cyan-blue, and its first recorded use as a color name in English was in 1879. Further origins of the color name can be traced back to a dye produced from the cornflower (\"Centaurea cyanus\").\n\nIn most languages, 'cyan' is not a basic color term and it phenomenologically appears as a greenish vibrant hue of blue to most English speakers. Reasons for why cyan is not linguistically acknowledged as a basic color term can be found in the frequent lack of distinction between blue and green in many languages.\n\nThe web color cyan shown at right is a secondary color in the RGB color model, which uses combinations of red, green and blue light to create all the colors on computer and television displays. In X11 colors, this color is called both cyan and aqua. In the HTML color list, this same color is called aqua.\n\nThe web colors are more vivid than the cyan used in the CMYK color system, and the web colors cannot be accurately reproduced on a printed page. To reproduce the web color cyan in inks, it is necessary to add some white ink to the printer's cyan below, so when it is reproduced in printing, it is not a primary subtractive color. It is called \"aqua\" (a name in use since 1598) because it is a color commonly associated with water, such as the appearance of the water at a tropical beach.\nCyan is also one of the common inks used in four-color printing, along with magenta, yellow, and black; this set of colors is referred to as CMYK as in spectrum(s).\n\nWhile both the additive secondary and the subtractive primary are called \"cyan\", they can be substantially different from one another. Cyan printing ink can be more saturated or less saturated than the RGB secondary cyan, depending on what RGB color space and ink are considered.\n\nProcess cyan is not an RGB color, and there is no fixed conversion from CMYK primaries to RGB. Different formulations are used for printer's ink, so there can be variations in the printed color that is pure cyan ink. This is because real-world subtractive (unlike additive) color mixing does not consistently produce the same result when mixing apparently identical colors, since the specific frequencies filtered out to produce that color affect how it interacts with other colors. A typical formulation of \"process cyan\" is shown in the color box at right.\n\n\n\n\n\n\n\n\n\n"}
{"id": "6105", "url": "https://en.wikipedia.org/wiki?curid=6105", "title": "Conventional insulinotherapy", "text": "Conventional insulinotherapy\n\nConventional insulinotherapy is a therapeutic regimen for treatment of diabetes mellitus which contrasts with the newer intensive insulinotherapy.\n\nThis older method (prior to the development home blood glucose monitoring) is still in use in a proportion of cases.\n\nConventional insulin therapy has these characteristics:\n\nThe down side of this method is that it is difficult to achieve as good results of glycemic control as with intensive insulinotherapy. The advantage is that, for diabetics with a regular lifestyle, the regime is less intrusive than the intensive therapy.\n"}
{"id": "6109", "url": "https://en.wikipedia.org/wiki?curid=6109", "title": "Cream", "text": "Cream\n\nCream is a dairy product composed of the higher-butterfat layer skimmed from the top of milk before homogenization. In un-homogenized milk, the fat, which is less dense, will eventually rise to the top. In the industrial production of cream, this process is accelerated by using centrifuges called \"separators\". In many countries, cream is sold in several grades depending on the total butterfat content. Cream can be dried to a powder for shipment to distant markets. Cream has high levels of saturated fat.\n\nCream skimmed from milk may be called \"sweet cream\" to distinguish it from whey cream skimmed from whey, a by-product of cheese-making. Whey cream has a lower fat content and tastes more salty, tangy and \"cheesy\". In many countries, cream is usually sold partially fermented: sour cream, crème fraîche, and so on.\n\nCream has many culinary uses in sweet, bitter, salty and tangy dishes.\n\nCream produced by cattle (particularly Jersey cattle) grazing on natural pasture often contains some natural carotenoid pigments derived from the plants they eat; this gives the cream a slight yellow tone, hence the name of the yellowish-white color, cream. This is also the origin of butter's yellow color. Cream from goat's milk, or from cows fed indoors on grain or grain-based pellets, is white.\n\nCream is used as an ingredient in many foods, including ice cream, many sauces, soups, stews, puddings, and some custard bases, and is also used for cakes. Whipped cream is served as a topping on ice cream sundaes, milkshakes, lassi, eggnog and sweet pies. Irish cream is an alcoholic liqueur which blends cream with whiskey, and often honey, wine, or coffee. Cream is also used in Indian curries such as masala dishes.\n\nCream (usually light/single cream or half and half) is often added to coffee in the US and Canada.\n\nBoth single and double cream can be used in cooking. Double cream or full-fat crème fraîche are often used when cream is added to a hot sauce, to prevent any problem with it separating or \"splitting\". Double cream can be thinned with milk to make an approximation of single cream.\n\nThe French word \"crème\" denotes not only dairy cream, but also other thick liquids such as sweet and savory custards, which are normally made with milk, not cream.\n\nDifferent grades of cream are distinguished by their fat content, whether they have been heat-treated, whipped, and so on. In many jurisdictions, there are regulations for each type.\n\nThe Australia New Zealand Food Standards Code – Standard 2.5.2 – Defines cream as milk product comparatively rich in fat, in the form of an emulsion of fat-in-skim milk, which can be obtained by separation from milk. Cream must contain no less than 350 g/kg of milk fat.\n\nManufacturers labels may distinguish between different fat contents, a general guideline is as follows:\n\nCanadian cream definitions are similar to those used in the United States, except for that of \"light cream\". In Canada, \"light cream\" is very low-fat cream, usually with 5% or 6% butterfat. Specific product characteristics are generally uniform throughout Canada, but names vary by both geographic and linguistic area and by manufacturer. It can be quite confusing: \"coffee cream\" may be 10% or 18% and \"half-and-half\" (\"crème légère\") may be 3%, 5%, 6% or 10%, all depending on location and brand.\nCream in Canada is defined to be the liquid obtained from milk after separating the various components to increase the milk fat content. The Canadian Food and Drug Regulations state that cream can contain agents to both stabilize and adjust the pH of the cream. For cream that will be used for whipping and has been heat-treated, Canadian regulation also states that cream cannot contain no more than 0.25% skim milk powder, 0.1% glucose solids, 0.005% calcium sulphate, 0.2% microcrystalline cellulose and no more than 0.02% xanthan gum. The content of milk fat present in canned cream must be displayed in percentage terms with \"milk fat\", \"B.F\", or \"M.F\" displayed after the percentage in order to be sold in Canada.  Fat content may also be displayed on canned cream in Canada.\n\nIn Japan, cream sold in supermarkets is usually between 35% and 48% butterfat.\n\nRussia, as well as other EAC countries, legally separates cream into two classes: normal (10–34% butterfat) and heavy (35–58%), but the industry has pretty much standardized around the following types:\nIn Sweden, cream is usually sold as:\n\nMellangrädde (27%) is, nowadays, a less common variant. Gräddfil and Creme Fraiche are two common sour cream products.\n\nIn Switzerland, the types of cream are legally defined as follows:\n\nSour cream and crème fraîche (German: Sauerrahm, Crème fraîche; French: crème acidulée, crème fraîche; Italian: panna acidula, crème fraîche) are defined as cream soured by bacterial cultures.\n\nThick cream (German: verdickter Rahm; French: crème épaissie; Italian: panna addensata) is defined as cream thickened using thickening agents.\n\nIn the United Kingdom, the types of cream are legally defined as followed:\n\nIn the United States, cream is usually sold as:\n\nMost cream products sold in the United States at retail contain the minimum permissible fat content for their product type, e.g., \"Half and half\" almost always contains only 10.5% butterfat.<br> Not all grades are defined by all jurisdictions, and the exact fat content ranges vary. The above figures are based on the Code of Federal Regulations, Title 21, Part 131\n\nCream may have thickening agents and stabilizers added. Thickeners include sodium alginate, carrageenan, gelatine, sodium bicarbonate, tetrasodium pyrophosphate, and alginic acid.\n\nOther processing may be carried out. For example, cream has a tendency to produce oily globules (called \"feathering\") when added to coffee. The stability of the cream may be increased by increasing the non-fat solids content, which can be done by partial demineralisation and addition of sodium caseinate, although this is expensive.\n\n by churning cream to separate the butterfat and buttermilk. This can be done by hand or by machine.\n\nWhipped cream is made by whisking or mixing air into cream with more than 30% fat, to turn the liquid cream into a soft solid. Nitrous oxide, from whipped-cream chargers may also be used to make whipped cream.\n\nSour cream, common in many countries including the U.S., Canada and Australia, is cream (12 to 16% or more milk fat) that has been subjected to a bacterial culture that produces lactic acid (0.5%+), which sours and thickens it.\n\nCrème fraîche (28% milk fat) is slightly soured with bacterial culture, but not as sour or as thick as sour cream. Mexican crema (or cream espesa) is similar to crème fraîche.\n\nSmetana is a heavy cream product (15-40% milk fat) Central and Eastern European sweet or sour cream.\n\nRjome or rømme is Norwegian sour cream containing 35% milk fat, similar to Icelandic sýrður rjómi.\n\nClotted cream, common in the United Kingdom, is made through a process that starts by slowly heating whole milk to produce a very high-fat (55%) product. This is similar to Indian malai.\n\nReduced cream is a cream product used in New Zealand to make Kiwi dip.\n\nSome non-edible substances are called creams due to their consistency: shoe cream is runny, unlike regular waxy shoe polish; hand/body 'creme' or \"skin cream\" is meant for moisturizing the skin.\n\nRegulations in many jurisdictions restrict the use of the word \"cream\" for foods. Words such as \"creme\", \"kreme\", \"creame\", or \"whipped topping\" (e.g., Cool Whip) are often used for products which cannot legally be called cream. Oreo cookies are a type of sandwich cookie in which two biscuits have a soft, sweet filling between them which is called \"crème filling\". In some cases foods can be described as cream although they do not contain predominantly milk fats; for example in Britain \"ice cream\" does not have to be a dairy product (although it must be labelled \"contains non-milk fat\"), and salad cream is the customary name for a condiment that has been produced since the 1920s.\n\n"}
{"id": "6111", "url": "https://en.wikipedia.org/wiki?curid=6111", "title": "Chemical vapor deposition", "text": "Chemical vapor deposition\n\nChemical vapor deposition (CVD) is a chemical process used to produce high quality, high-performance, solid materials. The process is often used in the semiconductor industry to produce thin films. In typical CVD, the wafer (substrate) is exposed to one or more volatile precursors, which react and/or decompose on the substrate surface to produce the desired deposit. Frequently, volatile by-products are also produced, which are removed by gas flow through the reaction chamber.\n\nMicrofabrication processes widely use CVD to deposit materials in various forms, including: monocrystalline, polycrystalline, amorphous, and epitaxial. These materials include: silicon (SiO, germanium, carbide, nitride, oxynitride), carbon (fiber, nanofibers, nanotubes, diamond and graphene), fluorocarbons, filaments, tungsten, titanium nitride and various high-k dielectrics.\n\nCVD is practiced in a variety of formats. These processes generally differ in the means by which chemical reactions are initiated.\n\n\nMost modern CVD is either LPCVD or UHVCVD.\n\nCVD is commonly used to deposit conformal films and augment substrate surfaces in ways that more traditional surface modification techniques are not capable of. CVD is extremely useful in the process of atomic layer deposition at depositing extremely thin layers of material. A variety of applications for such films exist. Gallium arsenide is used in some integrated circuits (ICs) and photovoltaic devices. Amorphous polysilicon is used in photovoltaic devices. Certain carbides and nitrides confer wear-resistance. Polymerization by CVD, perhaps the most versatile of all applications, allows for super-thin coatings which possess some very desirable qualities, such as lubricity, hydrophobicity and weather-resistance to name a few. CVD of metal-organic frameworks, a class of crystalline nanoporous materials, has recently been demonstrated. Applications for these films are anticipated in gas sensing and low-k dielectrics\n\nPolycrystalline silicon is deposited from trichlorosilane (SiHCl) or silane (SiH), using the following reactions:\n\nThis reaction is usually performed in LPCVD systems, with either pure silane feedstock, or a solution of silane with 70–80% nitrogen. Temperatures between 600 and 650 °C and pressures between 25 and 150 Pa yield a growth rate between 10 and 20 nm per minute. An alternative process uses a hydrogen-based solution. The hydrogen reduces the growth rate, but the temperature is raised to 850 or even 1050 °C to compensate. Polysilicon may be grown directly with doping, if gases such as phosphine, arsine or diborane are added to the CVD chamber. Diborane increases the growth rate, but arsine and phosphine decrease it.\n\nSilicon dioxide (usually called simply \"oxide\" in the semiconductor industry) may be deposited by several different processes. Common source gases include silane and oxygen, dichlorosilane (SiClH) and nitrous oxide (NO), or tetraethylorthosilicate (TEOS; Si(OCH)). The reactions are as follows :\n\nThe choice of source gas depends on the thermal stability of the substrate; for instance, aluminium is sensitive to high temperature. Silane deposits between 300 and 500 °C, dichlorosilane at around 900 °C, and TEOS between 650 and 750 °C, resulting in a layer of \"low- temperature oxide\" (LTO). However, silane produces a lower-quality oxide than the other methods (lower dielectric strength, for instance), and it deposits nonconformally. Any of these reactions may be used in LPCVD, but the silane reaction is also done in APCVD. CVD oxide invariably has lower quality than thermal oxide, but thermal oxidation can only be used in the earliest stages of IC manufacturing.\n\nOxide may also be grown with impurities (alloying or \"doping\"). This may have two purposes. During further process steps that occur at high temperature, the impurities may diffuse from the oxide into adjacent layers (most notably silicon) and dope them. Oxides containing 5–15% impurities by mass are often used for this purpose. In addition, silicon dioxide alloyed with phosphorus pentoxide (\"P-glass\") can be used to smooth out uneven surfaces. P-glass softens and reflows at temperatures above 1000 °C. This process requires a phosphorus concentration of at least 6%, but concentrations above 8% can corrode aluminium. Phosphorus is deposited from phosphine gas and oxygen:\n\nGlasses containing both boron and phosphorus (borophosphosilicate glass, BPSG) undergo viscous flow at lower temperatures; around 850 °C is achievable with glasses containing around 5 weight % of both constituents, but stability in air can be difficult to achieve. Phosphorus oxide in high concentrations interacts with ambient moisture to produce phosphoric acid. Crystals of BPO can also precipitate from the flowing glass on cooling; these crystals are not readily etched in the standard reactive plasmas used to pattern oxides, and will result in circuit defects in integrated circuit manufacturing.\n\nBesides these intentional impurities, CVD oxide may contain byproducts of the deposition. TEOS produces a relatively pure oxide, whereas silane introduces hydrogen impurities, and dichlorosilane introduces chlorine.\n\nLower temperature deposition of silicon dioxide and doped glasses from TEOS using ozone rather than oxygen has also been explored (350 to 500 °C). Ozone glasses have excellent conformality but tend to be hygroscopic – that is, they absorb water from the air due to the incorporation of silanol (Si-OH) in the glass. Infrared spectroscopy and mechanical strain as a function of temperature are valuable diagnostic tools for diagnosing such problems.\n\nSilicon nitride is often used as an insulator and chemical barrier in manufacturing ICs. The following two reactions deposit silicon nitride from the gas phase:\n\nSilicon nitride deposited by LPCVD contains up to 8% hydrogen. It also experiences strong tensile stress, which may crack films thicker than 200 nm. However, it has higher resistivity and dielectric strength than most insulators commonly available in microfabrication (10 Ω·cm and 10 MV/cm, respectively).\n\nAnother two reactions may be used in plasma to deposit SiNH:\n\nThese films have much less tensile stress, but worse electrical properties (resistivity 10 to 10 Ω·cm, and dielectric strength 1 to 5 MV/cm).\n\nCVD for tungsten is achieved from tungsten hexafluoride (WF), which may be deposited in two ways:\n\nOther metals, notably aluminium and copper, can be deposited by CVD. , commercially cost-effective CVD for copper did not exist, although volatile sources exist, such as Cu(hfac). Copper is typically deposited by electroplating. Aluminum can be deposited from triisobutylaluminium (TIBAL) and related organoaluminium compounds.\n\nCVD for molybdenum, tantalum, titanium, nickel is widely used. These metals can form useful silicides when deposited onto silicon. Mo, Ta and Ti are deposited by LPCVD, from their pentachlorides. Nickel, molybdenum, and tungsten can be deposited at low temperatures from their carbonyl precursors. In general, for an arbitrary metal \"M\", the chloride deposition reaction is as follows:\n\nwhereas the carbonyl decomposition reaction can happen spontaneously under thermal treatment or acoustic cavitation and is as follows:\n\nthe decomposition of metal carbonyls is often violently precipitated by moisture or air, where oxygen reacts with the metal precursor to form metal or metal oxide along with carbon dioxide.\n\nNiobium(V) oxide layers can be produced by the thermal decomposition of niobium(V) ethoxide with the loss of diethyl ether according to the equation:\n\nMany variations of CVD can be utilized to synthesize graphene. Although many advancements have been made, the processes listed below are not commercially viable yet.\nThe most popular carbon source used to produce graphene is methane gas. Less popular choices include petroleum asphalt, notable for being inexpensive but more difficult to work with.\nThe use of catalyst is viable in changing the physical process of graphene production. Notable examples include iron nanoparticles, nickel foam, and gallium vapor. These catalysts can either be used in situ during graphene buildup, or situated at some distance away at the deposition area. Some catalysts require another step to remove them from the sample material.\n\nThe direct growth of high-quality, large single-crystalline domains of graphene on a dielectric substrate is of vital importance for applications in electronics and optoelectronics. Combining the advantages of both catalytic CVD and the ultra-flat dielectric substrate, gaseous catalyst-assisted CVD paves the way for synthesizing high-quality graphene for device applications while avoiding the transfer process.\n\nPhysical conditions such as surrounding pressure, temperature, carrier gas, and chamber material play a big role in production of graphene.\n\nMost systems use LPCVD with pressures ranging from 1 to 1500 Pa. However, some still use APCVD. Low pressures are used more commonly as they help prevent unwanted reactions and produce more uniform thickness of deposition on the substrate.\n\nOn the other hand, temperatures used range from 800-1050 °C. High temperatures translate to an increase of the rate of reaction. Caution has to be exercised as high temperatures do pose higher danger levels in addition to greater energy costs. \nHydrogen gas and inert gases such as argon are flowed into the system. These gases act as a carrier, enhancing surface reaction and improving reaction rate, thereby increasing deposition of graphene onto the substrate. \nStandard quartz tubing and chambers are used in CVD of graphene. Quartz is chosen because it has a very high melting point and is chemically inert. In other words, quartz does not interfere with any physical or chemical reactions regardless of the conditions. \nRaman spectroscopy, X-ray spectroscopy, transmission electron microscopy (TEM), and scanning electron microscopy (SEM) are used to examine and characterize the graphene samples.\n\nRaman spectroscopy is used to characterize and identify the graphene particles; X-ray spectroscopy is used to characterize chemical states; TEM is used to provide fine details regarding the internal composition of graphene; SEM is used to examine the surface and topography.\n\nSometimes, atomic force microscopy (AFM) is used to measure local properties such as friction and magnetism.\n\nCold wall CVD technique can be used to study the underlying surface science involved in graphene nucleation and growth as it allows unprecedented control of process parameters like gas flow rates, temperature and pressure as demonstrated in a recent study. The study was carried out in a home-built vertical cold wall system utilizing resistive heating by passing direct current through the substrate. It provided conclusive insight into a typical surface-mediated nucleation and growth mechanism involved in two-dimensional materials grown using catalytic CVD under conditions sought out in the semiconductor industry.\n\nIn spite of graphene's exciting electronic and thermal properties, it is unsuitable as a transistor for future digital devices, due to the absence of a bandgap between the conduction and valence bands. This makes it impossible to switch between on and off states with respect to electron flow. Scaling things down, graphene nanoribbons of less than 10 nm in width do exhibit electronic bandgaps and are therefore potential candidates for digital devices. Precise control over their dimensions, and hence electronic properties, however, represents a challenging goal, and the ribbons typically possess rough edges that are detrimental to their performance.\nNow, Chen et al. report on a strategy to grow graphene nanoribbons with controlled widths and smooth edges directly onto dielectric hexagonal boron nitride (h-BN) substrates by chemical vapor deposition. The team use nickel nanoparticles to etch monolayer-deep, nanometre-wide trenches into h-BN, and subsequently fill them with graphene using chemical vapour deposition. Modifying the etching parameters allows the width of the trench to be tuned to less than 10 nm, and the resulting sub-10-nm ribbons display bandgaps of almost 0.5 eV.\n\nChemical vapor deposition (CVD) can be used to produce a synthetic diamond by creating the circumstances necessary for carbon atoms in a gas to settle on a substrate in crystalline form.\n\nCVD production of diamonds has received a great deal of attention in the materials sciences because it allows many new applications of diamonds that had previously been considered too difficult to make economical. CVD diamond growth typically occurs under low pressure (1–27 kPa; 0.145–3.926 psi; 7.5-203 Torr) and involves feeding varying amounts of gases into a chamber, energizing them and providing conditions for diamond growth on the substrate. The gases always include a carbon source, and typically include hydrogen as well, though the amounts used vary greatly depending on the type of diamond being grown. Energy sources include hot filament, microwave power, and arc discharges, among others. The energy source is intended to generate a plasma in which the gases are broken down and more complex chemistries occur. The actual chemical process for diamond growth is still under study and is complicated by the very wide variety of diamond growth processes used.\n\nUsing CVD, films of diamond can be grown over large areas of substrate with control over the properties of the diamond produced. In the past, when high pressure high temperature (HPHT) techniques were used to produce a diamond, the result was typically very small free standing diamonds of varying sizes. With CVD diamond growth areas of greater than fifteen centimeters (six inches) diameter have been achieved and much larger areas are likely to be successfully coated with diamond in the future. Improving this process is key to enabling several important applications.\n\nThe growth of diamond directly on a substrate allows the addition of many of diamond's important qualities to other materials. Since diamond has the highest thermal conductivity of any bulk material, layering diamond onto high heat producing electronics (such as optics and transistors) allows the diamond to be used as a heat sink. Diamond films are being grown on valve rings, cutting tools, and other objects that benefit from diamond's hardness and exceedingly low wear rate. In each case the diamond growth must be carefully done to achieve the necessary adhesion onto the substrate. Diamond's very high scratch resistance and thermal conductivity, combined with a lower coefficient of thermal expansion than Pyrex glass, a coefficient of friction close to that of Teflon (Polytetrafluoroethylene) and strong lipophilicity would make it a nearly ideal non-stick coating for cookware if large substrate areas could be coated economically.\n\nCVD growth allows one to control the properties of the diamond produced. In the area of diamond growth, the word \"diamond\" is used as a description of any material primarily made up of sp3-bonded carbon, and there are many different types of diamond included in this. By regulating the processing parameters—especially the gases introduced, but also including the pressure the system is operated under, the temperature of the diamond, and the method of generating plasma—many different materials that can be considered diamond can be made. Single crystal diamond can be made containing various dopants. Polycrystalline diamond consisting of grain sizes from several nanometers to several micrometers can be grown. Some polycrystalline diamond grains are surrounded by thin, non-diamond carbon, while others are not. These different factors affect the diamond's hardness, smoothness, conductivity, optical properties and more.\n\nCommercially, mercury cadmium telluride is of continuing interest for detection of infrared radiation. Consisting of an alloy of CdTe and HgTe, this material can be prepared from the dimethyl derivatives of the respective elements.\n\n\n"}

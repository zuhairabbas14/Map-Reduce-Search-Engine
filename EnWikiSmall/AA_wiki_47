{"id": "6424", "url": "https://en.wikipedia.org/wiki?curid=6424", "title": "Corona Australis", "text": "Corona Australis\n\nCorona Australis or Corona Austrina is a constellation in the Southern Celestial Hemisphere. Its Latin name means \"southern crown\", and it is the southern counterpart of Corona Borealis, the northern crown. One of the 48 constellations listed by the 2nd-century astronomer Ptolemy, it remains one of the 88 modern constellations. The Ancient Greeks saw Corona Australis as a wreath rather than a crown and associated it with Sagittarius or Centaurus. Other cultures have likened the pattern to a turtle, ostrich nest, a tent, or even a hut belonging to a rock hyrax.\n\nAlthough fainter than its northern counterpart, the oval- or horseshoe-shaped pattern of its brighter stars renders it distinctive. Alpha and Beta Coronae Australis are the two brightest stars with an apparent magnitude of around 4.1. Epsilon Coronae Australis is the brightest example of a W Ursae Majoris variable in the southern sky. Lying alongside the Milky Way, Corona Australis contains one of the closest star-forming regions to the Solar System—a dusty dark nebula known as the Corona Australis Molecular Cloud, lying about 430 light years away. Within it are stars at the earliest stages of their lifespan. The variable stars R and TY Coronae Australis light up parts of the nebula, which varies in brightness accordingly.\n\nThe name of the constellation was entered as \"Corona Australis\" when the International Astronomical Union (IAU) established the 88 modern constellations in 1922.\nIn 1932, the name was instead recorded as \"Corona Austrina\" when the IAU's commission on notation approved a list of four-letter abbreviations for the constellations.\nThe four-letter abbreviations were repealed in 1955. The IAU presently uses \"Corona Australis\" exclusively.\n\nCorona Australis is a small constellation bordered by Sagittarius to the north, Scorpius to the west, Telescopium to the south, and Ara to the southwest. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'CrA'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of four segments (\"illustrated in infobox\"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between −36.77° and −45.52°. Covering 128 square degrees, Corona Australis culminates at midnight around the 30th of June and ranks 80th in area. Only visible at latitudes south of 53° north, Corona Australis cannot be seen from the British Isles as it lies too far south, but it can be seen from southern Europe and readily from the southern United States.\n\nWhile not a bright constellation, Corona Australis is nonetheless distinctive due to its easily identifiable pattern of stars, which has been described as horseshoe- or oval-shaped. Though it has no stars brighter than 4th magnitude, it still has 21 stars visible to the unaided eye (brighter than magnitude 5.5). Nicolas Louis de Lacaille used the Greek letters Alpha through to Lambda to label the most prominent eleven stars in the constellation, designating two stars as Eta and omitting Iota altogether. Mu Coronae Australis, a yellow star of spectral type G5.5III and apparent magnitude 5.21, was labelled by Johann Elert Bode and retained by Benjamin Gould, who deemed it bright enough to warrant naming.\n\nThe only star in the constellation to have received a name is Alfecca Meridiana or Alpha CrA. The name combines the Arabic name of the constellation with the Latin for \"southern\". In Arabic, \"Alfecca\" means \"break\", and refers to the shape of both Corona Australis and Corona Borealis. Also called simply \"Meridiana\", it is a white main sequence star located 130 light years away from Earth, with an apparent magnitude of 4.10 and spectral type A2Va. A rapidly rotating star, it spins at almost 200 km per second at its equator, making a complete revolution in around 14 hours. Like the star Vega, it has excess infrared radiation, which indicates it may be ringed by a disk of dust. It is currently a main-sequence star, but will eventually evolve into a white dwarf; currently, it has a luminosity 31 times greater, and a radius and mass of 2.3 times that of the Sun. Beta Coronae Australis is an orange giant 510 light years from Earth. Its spectral type is K0II, and it is of apparent magnitude 4.11. Since its formation, it has evolved from a B-type star to a K-type star. Its luminosity class places it as a bright giant; its luminosity is 730 times that of the Sun, designating it one of the highest-luminosity K0-type stars visible to the naked eye. 100 million years old, it has a radius of 43 solar radii () and a mass of between 4.5 and 5 solar masses (). Alpha and Beta are so similar as to be indistinguishable in brightness to the naked eye.\n\nSome of the more prominent double stars include Gamma Coronae Australis—a pair of yellowish white stars 58 light years away from Earth, which orbit each other every 122 years. Widening since 1990, the two stars can be seen as separate with a 100 mm aperture telescope; they are separated by 1.3 arcseconds at an angle of 61 degrees. They have a combined visual magnitude of 4.2; each component is an F8V dwarf star with a magnitude of 5.01. Epsilon Coronae Australis is an eclipsing binary belonging to a class of stars known as W Ursae Majoris variables. These star systems are known as contact binaries as the component stars are so close together they touch. Varying by a quarter of a magnitude around an average apparent magnitude of 4.83 every seven hours, the star system lies 98 light years away. Its spectral type is F4VFe-0.8+. At the southern end of the crown asterism are the stars Eta¹ and Eta² Coronae Australis, which form an optical double. Of magnitude 5.1 and 5.5, they are separable with the naked eye and are both white. Kappa Coronae Australis is an easily resolved optical double—the components are of apparent magnitudes 6.3 and 5.7 and are 1700 and 490 light years away respectively. They appear at an angle of 359 degrees, separated by 21.6 arcseconds. Kappa² is actually the brighter of the pair and is more bluish white, with a spectral type of B9V, while Kappa¹ is of spectral type A0III. Lying 202 light years away, Lambda Coronae Australis is a double splittable in small telescopes. The primary is a white star of spectral type A2Vn and magnitude of 5.1, while the companion star has a magnitude of 9.7. The two components are separated by 29.2 arcseconds at an angle of 214 degrees.\n\nZeta Coronae Australis is a rapidly rotating main sequence star with an apparent magnitude of 4.8, 221.7 light years from Earth. The star has blurred lines in its hydrogen spectrum due to its rotation. Its spectral type is B9V. Theta Coronae Australis lies further to the west, a yellow giant of spectral type G8III and apparent magnitude 4.62. Corona Australis harbours RX J1856.5-3754, an isolated neutron star that is thought to lie 140 (±40) parsecs, or 460 (±130) light years, away, with a diameter of 14 km. It was once suspected to be a strange star, but this has been discounted.\n\nIn the north of the constellation is the Corona Australis Molecular Cloud, a dark molecular cloud with many embedded reflection nebulae, including NGC 6729, NGC 6726–7, and IC 4812. A star-forming region of around , it contains Herbig–Haro objects (protostars) and some very young stars. About 430 light years (130 parsecs) away, it is one of the closest star-forming regions to the Solar System. The related NGC 6726 and 6727, along with unrelated NGC 6729, were first recorded by Johann Friedrich Julius Schmidt in 1865. The Coronet cluster, about 554 light years (170 parsecs) away at the edge of the Gould Belt, is also used in studying star and protoplanetary disk formation.\n\nR Coronae Australis is an irregular variable star ranging from magnitudes 9.7 to 13.9. Blue-white, it is of spectral type B5IIIpe. A very young star, it is still accumulating interstellar material. It is obscured by, and illuminates, the surrounding nebula, NGC 6729, which brightens and darkens with it. The nebula is often compared to a comet for its appearance in a telescope, as its length is five times its width. S Coronae Australis is a G-class dwarf in the same field as R and is a T Tauri star. Nearby, another young variable star, TY Coronae Australis, illuminates another nebula: reflection nebula NGC 6726–7. TY Coronae Australis ranges irregularly between magnitudes 8.7 and 12.4,and the brightness of the nebula varies with it. Blue-white, it is of spectral type B8e. The largest young stars in the region, R, S, T, TY and VV Coronae Australis, are all ejecting jets of material which cause surrounding dust and gas to coalesce and form Herbig–Haro objects, many of which have been identified nearby. Lying adjacent to the nebulosity is the globular cluster known as NGC 6723, which is actually in the neighbouring constellation of Sagittarius and is much much further away.\n\nNear Epsilon and Gamma Coronae Australis is Bernes 157, a dark nebula and star forming region. It is a large nebula, 55 by 18 arcminutes, that possesses several stars around magnitude 13. These stars have been dimmed by up to 8 magnitudes by its dust clouds.\n\nIC 1297 is a planetary nebula of apparent magnitude 10.7, which appears as a green-hued roundish object in higher-powered amateur instruments. The nebula surrounds the variable star RU Coronae Australis, which has an average apparent magnitude of 12.9 and is a WC class Wolf–Rayet star. IC 1297 is small, at only 7 arcseconds in diameter; it has been described as \"a square with rounded edges\" in the eyepiece, elongated in the north-south direction. Descriptions of its color encompass blue, blue-tinged green, and green-tinged blue.\n\nCorona Australis' location near the Milky Way means that galaxies are uncommonly seen. NGC 6768 is a magnitude 11.2 object 35′ south of IC 1297. It is made up of two galaxies merging, one of which is an elongated elliptical galaxy of classification E4 and the other a lenticular galaxy of classification S0. IC 4808 is a galaxy of apparent magnitude 12.9 located on the border of Corona Australis with the neighbouring constellation of Telescopium and 3.9 degrees west-southwest of Beta Sagittarii. However, amateur telescopes will only show a suggestion of its spiral structure. It is 1.9 arcminutes by 0.8 arcminutes. The central area of the galaxy does appear brighter in an amateur instrument, which shows it to be tilted northeast-southwest.\n\nSoutheast of Theta and southwest of Eta lies the open cluster ESO 281-SC24, which is composed of the yellow 9th magnitude star GSC 7914 178 1 and five 10th to 11th magnitude stars. Halfway between Theta Coronae Australis and Theta Scorpii is the dense globular cluster NGC 6541. Described as between magnitude 6.3 and magnitude 6.6, it is visible in binoculars and small telescopes. Around 22000 light years away, it is around 100 light years in diameter. It is estimated to be around 14 billion years old. NGC 6541 appears 13.1 arcminutes in diameter and is somewhat resolvable in large amateur instruments; a 12-inch telescope reveals approximately 100 stars but the core remains unresolved.\n\nThe Corona Australids are a meteor shower that takes place between 14 and 18 March each year, peaking around 16 March. This meteor shower does not have a high peak hourly rate. In 1953 and 1956, observers noted a maximum of 6 meteors per hour and 4 meteors per hour respectively; in 1955 the shower was \"barely resolved\". However, in 1992, astronomers detected a peak rate of 45 meteors per hour. The Corona Australids' rate varies from year to year. At only six days, the shower's duration is particularly short, and its meteoroids are small; the stream is devoid of large meteoroids. The Corona Australids were first seen with the unaided eye in 1935 and first observed with radar in 1955. Corona Australid meteors have an entry velocity of 45 kilometers per second. In 2006, a shower originating near Beta Coronae Australis was designated as the Beta Coronae Australids. They appear in May, the same month as a nearby shower known as the May Microscopids, but the two showers have different trajectories and are unlikely to be related.\n\nCorona Australis may have been recorded by ancient Mesopotamians in the MUL.APIN, as a constellation called MA.GUR (\"The Bark\"). However, this constellation, adjacent to SUHUR.MASH (\"The Goat-Fish\", modern Capricornus), may instead have been modern Epsilon Sagittarii. As a part of the southern sky, MA.GUR was one of the fifteen \"stars of Ea\".\n\nIn the 3rd century BC, the Greek didactic poet Aratus wrote of, but did not name the constellation, instead calling the two crowns Στεφάνοι (\"Stephanoi\"). The Greek astronomer Ptolemy described the constellation in the 2nd century AD, though with the inclusion of Alpha Telescopii, since transferred to Telescopium. Ascribing 13 stars to the constellation, he named it Στεφάνος νοτιος (\"Stephanos notios\"), \"Southern Wreath\", while other authors associated it with either Sagittarius (having fallen off his head) or Centaurus; with the former, it was called \"Corona Sagittarii\". Similarly, the Romans called Corona Australis the \"Golden Crown of Sagittarius\". It was known as \"Parvum Coelum\" (\"Canopy\", \"Little Sky\") in the 5th century. The 18th-century French astronomer Jérôme Lalande gave it the names \"Sertum Australe\" (\"Southern Garland\") and \"Orbiculus Capitis\", while German poet and author Philippus Caesius called it \"Corolla\" (\"Little Crown\") or \"Spira Australis\" (\"Southern Coil\"), and linked it with the Crown of Eternal Life from the New Testament. Seventeenth-century celestial cartographer Julius Schiller linked it to the Diadem of Solomon. Sometimes, Corona Australis was not the wreath of Sagittarius but arrows held in his hand.\n\nCorona Australis has been associated with the myth of Bacchus and Stimula. Jupiter had impregnated Stimula, causing Juno to become jealous. Juno convinced Stimula to ask Jupiter to appear in his full splendor, which the mortal woman could not handle, causing her to burn. After Bacchus, Stimula's unborn child, became an adult and the god of wine, he honored his deceased mother by placing a wreath in the sky.\n\nIn Chinese astronomy, the stars of Corona Australis are located within the Black Tortoise of the North (北方玄武, \"Běi Fāng Xuán Wǔ\"). The constellation itself was known as \"ti'en pieh\" (\"Heavenly Turtle\") and during the Western Zhou period, marked the beginning of winter. However, precession over time has meant that the \"Heavenly River\" (Milky Way) became the more accurate marker to the ancient Chinese and hence supplanted the turtle in this role. Arabic names for Corona Australis include \"Al Ķubbah\" \"the Tortoise\", \"Al Ĥibā\" \"the Tent\" or \"Al Udḥā al Na'ām\" \"the Ostrich Nest\". It was later given the name \"Al Iklīl al Janūbiyyah\", which the European authors Chilmead, Riccioli and Caesius transliterated as Alachil Elgenubi, Elkleil Elgenubi and Aladil Algenubi respectively.\n\nThe ǀXam speaking San people of South Africa knew the constellation as \"≠nabbe ta !nu\" \"house of branches\"—owned originally by the Dassie (rock hyrax), and the star pattern depicting people sitting in a semicircle around a fire.\n\nThe indigenous Boorong people of northwestern Victoria saw it as \"Won\", a boomerang thrown by \"Totyarguil\" (Altair). The Aranda people of Central Australia saw Corona Australis as a coolamon carrying a baby, which was accidentally dropped to earth by a group of sky-women dancing in the Milky Way. The impact of the coolamon created Gosses Bluff crater, 175 km west of Alice Springs. The Torres Strait Islanders saw Corona Australis as part of a larger constellation encompassing part of Sagittarius and the tip of Scorpius's tail; the Pleiades and Orion were also associated. This constellation was Tagai's canoe, crewed by the Pleiades, called the \"Usiam\", and Orion, called the \"Seg\". The myth of Tagai says that he was in charge of this canoe, but his crewmen consumed all of the supplies onboard without asking permission. Enraged, Tagai bound the Usiam with a rope and tied them to the side of the boat, then threw them overboard. Scorpius's tail represents a suckerfish, while Eta Sagittarii and Theta Coronae Australis mark the bottom of the canoe. On the island of Futuna, the figure of Corona Australis was called \"Tanuma\" and in the Tuamotus, it was called \"Na Kaua-ki-Tonga\".\n\nSources\nOnline sources\n\n\"SIMBAD\"\n\n"}
{"id": "6426", "url": "https://en.wikipedia.org/wiki?curid=6426", "title": "Corcovado", "text": "Corcovado\n\nCorcovado (), meaning \"hunchback\" in Portuguese, is a mountain in central Rio de Janeiro, Brazil. The 710-metre (2,329 ft) granite peak is located in the Tijuca Forest, a national park. It is sometimes confused with nearby Sugarloaf Mountain.\n\nCorcovado hill lies just west of the city center but is wholly within the city limits and visible from great distances. It is known worldwide for the 38-metre (125 ft) statue of Jesus atop its peak, entitled \"Cristo Redentor\" or \"Christ the Redeemer\".\nThe peak and statue can be accessed via a narrow road, by the 3.8 kilometre (2.4 mi) Corcovado Rack Railway, which was opened in 1884 and refurbished in 1980, or by the walking trail on the south side of the mountain that starts from Parque Lage. The railway uses three electrically powered trains, with a passenger capacity of 540 passengers per hour. The rail trip takes approximately 20 minutes and departs every 20 minutes. Due to its limited passenger capacity, the wait to board at the entry station can take several hours. The year-round schedule is 8:30 to 18:30.\n\nFrom the train terminus and road, the observation deck at the foot of the statue is reached by 223 steps, or by elevators and escalators. Among the most popular year-round tourist attractions in Rio, the Corcovado railway, access roads, and statue platform are commonly crowded.\n\nThe most popular attraction of Corcovado mountain is the statue and viewing platform at its peak, drawing over 300,000 visitors per year. From the peak's platform the panoramic view includes downtown Rio, Sugarloaf Mountain, the Lagoa Rodrigo de Freitas (lake), Copacabana and Ipanema beaches, Estádio do Maracanã (Maracanã Stadium), and several of Rio's favelas. Cloud cover is common in Rio and the view from the platform is often obscured. Sunny days are recommended for optimal viewing.\n\nNotable past visitors to the mountain peak include Pope Pius XII, Pope John Paul II, Alberto Santos-Dumont, German Sueiro Vazquez, Albert Einstein, Diana, Princess of Wales, and General Sherman, among others. An additional attraction of the mountain is rock climbing. The south face had 54 climbing routes in 1992. The easiest way starts from Park Lage.\n\nThe Corcovado is also a symbol of the Brazilian culture.\n\nThe peak of Corcovado is a big granite dome, which describes a generally vertical rocky formation. It is claimed to be the highest such formation in Brazil, the second highest being Pedra Agulha, situated near to the town of Pancas in Espírito Santo.\n\n"}
{"id": "6427", "url": "https://en.wikipedia.org/wiki?curid=6427", "title": "Cheddar, Somerset", "text": "Cheddar, Somerset\n\nCheddar is a large village and civil parish in the Sedgemoor district of the English county of Somerset. It is situated on the southern edge of the Mendip Hills, north-west of Wells. The civil parish includes the hamlets of Nyland and Bradley Cross. The village, which has its own parish council, has a population of 5,755 and the parish has an acreage of as of 1961.\n\nCheddar Gorge, on the northern edge of the village, is the largest gorge in the United Kingdom and includes several show caves, including Gough's Cave. The gorge has been a centre of human settlement since Neolithic times including a Saxon palace. It has a temperate climate and provides a unique geological and biological environment that has been recognised by the designation of several Sites of Special Scientific Interest. It is also the site of several limestone quarries. The village gave its name to Cheddar cheese and has been a centre for strawberry growing. The crop was formerly transported on the Cheddar Valley rail line, which closed in the late 1960s but is now a cycle path. The village is now a major tourist destination with several cultural and community facilities, including the Cheddar Show Caves Museum.\n\nThe village supports a variety of community groups including religious, sporting and cultural organisations. Several of these are based on the site of The Kings of Wessex Academy, which is the largest educational establishment.\n\nThe name Cheddar comes from the Old English word \"ceodor\", meaning deep dark cavity or pouch.\n\nThere is evidence of occupation from the Neolithic period in Cheddar. Britain's oldest complete human skeleton, Cheddar Man, estimated to be 9,000 years old, was found in Cheddar Gorge in 1903. Older remains from the Upper Late Palaeolithic era (12,000–13,000 years ago) have been found. There is some evidence of a Bronze Age field system at the Batts Combe quarry site. There is also evidence of Bronze Age barrows at the mound in the Longwood valley, which if man-made it is likely to be a field system. The remains of a Roman villa have been excavated in the grounds of the current vicarage.\nThe village of Cheddar had been important during the Roman and Saxon eras. There was a royal palace at Cheddar during the Saxon period, which was used on three occasions in the 10th century to host the Witenagemot. The ruins of the palace were excavated in the 1960s. They are located on the grounds of The Kings of Wessex Academy, together with a 14th century chapel dedicated to St. Columbanus. Roman remains have also been uncovered at the site. Cheddar was listed in the Domesday Book of 1086 as \"Ceder\", meaning \"Shear Water\", from the Old English \"scear\" and Celtic \"dwr\". An alternate spelling in earlier documents, common through the 1850s is \"Chedder\". As early as 1130 AD, the Cheddar Gorge was recognised as one of the \"Four wonders of England\". Historically, Cheddar's source of wealth was farming and cheese making for which it was famous as early as 1170 AD. The parish was part of the Winterstoke Hundred.\n\nThe manor of Cheddar was deforested in 1337 and Bishop Ralph was granted a licence by the King to create a hunting forest.\n\nAs early as 1527 there are records of watermills on the river. In the 17th and 18th centuries, there were several watermills which ground corn and made paper, with 13 mills on the Yeo at the peak, declining to seven by 1791 and just three by 1915.\nIn the Victorian era it also became a centre for the production of clothing. The last mill, used as a shirt factory, closed in the early 1950s. William Wilberforce saw the poor conditions of the locals when he visited Cheddar in 1789. He inspired Hannah More in her work to improve the conditions of the Mendip miners and agricultural workers. In 1801, of common land were enclosed under the Inclosure Acts.\n\nTourism of the Cheddar gorge and caves began with the opening of the Cheddar Valley Railway in 1869.\n\nCheddar, its surrounding villages and specifically the gorge has been subject to flooding. In the Chew Stoke flood of 1968 the flow of water washed large boulders down the gorge, washed away cars, and damaged the cafe and the entrance to Gough's Cave.\n\nCheddar is recognised as a village. The adjacent settlement of Axbridge, although only about a third the population of Cheddar, is a town. This apparently illogical situation is explained by the relative importance of the two places in historic times. While Axbridge grew in importance as a centre for cloth manufacturing in the Tudor period and gained a charter from King John, Cheddar remained a more dispersed mining and dairy-farming village. Its population grew with the arrival of the railways in the Victorian era and the advent of tourism.\n\nThe parish council, which has 15 members who are elected for four years, is responsible for local issues, including setting an annual precept (local rate) to cover the council's operating costs and producing annual accounts for public scrutiny. The parish council evaluates local planning applications and works with the police, district council officers, and neighbourhood watch groups on matters of crime, security, and traffic. The parish council's role also includes initiating projects for the maintenance and repair of parish facilities, as well as consulting with the district council on the maintenance, repair, and improvement of highways, drainage, footpaths, public transport, and street cleaning. Conservation matters (including trees and listed buildings) and environmental issues are also the responsibility of the council.\n\nThe village is in the 'Cheddar and Shipham' electoral ward. After including Shipham the total population of the ward taken at the 2011 census is 6,842.\nThe village falls within the non-metropolitan district of Sedgemoor, which was formed on 1 April 1974 under the Local Government Act 1972. It was previously part of Axbridge Rural District. Sedgemoor is responsible for local planning and building control, local roads, council housing, environmental health, markets and fairs, refuse collection and recycling, cemeteries and crematoria, leisure services, parks, and tourism. Somerset County Council is responsible for running the largest and most expensive local services such as education, social services, the library, roads, public transport, trading standards, waste disposal and strategic planning, although fire, police and ambulance services are provided jointly with other authorities through the Devon and Somerset Fire and Rescue Service, Avon and Somerset Constabulary and the South Western Ambulance Service.\n\nIt is also part of the Wells county constituency represented in the House of Commons of the Parliament of the United Kingdom. It elects one Member of Parliament (MP) by the first past the post system of election, and is part of the South West England constituency of the European Parliament which elects six MEPs using the d'Hondt method of party-list proportional representation.\n\nCheddar is twinned with Felsberg, Germany and Vernouillet, France, and it has an active programme of exchange visits. Initially, Cheddar twinned with Felsberg in 1984. In 2000, Cheddar twinned with Vernouillet, which had also been twinned with Felsberg. Cheddar also has a friendship link with Ocho Rios in Saint Ann Parish, Jamaica.\n\nThe area is underlain by Black Rock slate, Burrington Oolite and Clifton Down Limestone of the Carboniferous Limestone Series, which contain ooliths and fossil debris on top of Old Red Sandstone, and by Dolomitic Conglomerate of the Keuper. Evidence for Variscan orogeny is seen in the sheared rock and cleaved shales. In many places weathering of these strata has resulted in the formation of immature calcareous soils.\n\nCheddar Gorge, which is located on the edge of the village, is the largest gorge in the United Kingdom.\nThe gorge is the site of the Cheddar Caves, where Cheddar Man was found in 1903. Older remains from the Upper Late Palaeolithic era (12,000–13,000 years ago) have been found. The caves, produced by the activity of an underground river, contain stalactites and stalagmites. Gough's Cave, which was discovered in 1903, leads around into the rock-face, and contains a variety of large rock chambers and formations. Cox's Cave, discovered in 1837, is smaller but contains many intricate formations. A further cave houses a children's entertainment walk known as the \"Crystal Quest\".\n\nCheddar Gorge, including Cox's Cave, Gough's Cave and other attractions, has become a tourist destination, attracting about 500,000 visitors per year.\nIn a 2005 poll of \"Radio Times\" readers, following its appearance on the 2005 television programme \"Seven Natural Wonders\", Cheddar Gorge was named as the second greatest natural wonder in Britain, surpassed only by the Dan yr Ogof caves.\n\nThere are several large and unique Sites of Special Scientific Interest (SSSI) around the village.\n\nCheddar Reservoir is a near-circular artificial reservoir operated by Bristol Water. Dating from the 1930s, it has a capacity of 135 million gallons (614,000 cubic metres). The reservoir is supplied with water taken from the Cheddar Yeo, which rises in Gough's Cave in Cheddar Gorge and is a tributary of the River Axe. The inlet grate for the water pipe that is used to transport the water can be seen next to the sensory garden in Cheddar Gorge. It has been designated as a Site of Special Scientific Interest (SSSI) due to its wintering waterfowl populations.\n\nCheddar Wood and the smaller Macall's Wood form a biological Site of Special Scientific Interest from what remains of the wood of the Bishops of Bath and Wells in the 13th century and of King Edmund the Magnificent's wood in the 10th. During the 19th century, its lower fringes were grubbed out to make strawberry fields. Most of these have been allowed to revert to woodland. The wood was coppiced until 1917. This site compromises a wide range of habitats which include ancient and secondary semi-natural broadleaved woodland, unimproved neutral grassland, and a complex mosaic of calcareous grassland and acidic dry dwarf-shrub heath. Cheddar Wood is one of only a few English stations for starved wood-sedge (\"Carex depauperata\"). Purple gromwell (\"Lithospermum purpurocaeruleum\"), a nationally rare plant, also grows in the wood. Butterflies include silver-washed fritillary (\"Argynnis paphia\"), dark green fritillary (\"Argynnis aglaja\"), pearl-bordered fritillary (\"Boloria euphrosyne\"), holly blue (\"Celastrina argiolus\") and brown argus (\"Aricia agestis\"). The slug \"Arion fasciatus\", which has a restricted distribution in the south of England, and the soldier beetle \"Cantharis fusca\" also occur.\n\nBy far the largest of the SSSIs is called Cheddar Complex and covers of the gorge, caves and the surrounding area. It is important because of both biological and geological features. It includes four SSSIs, formerly known as Cheddar Gorge SSSI, August Hole/Longwood Swallet SSSI, GB Cavern Charterhouse SSSI and Charterhouse on-Mendip SSSI. It is partly owned by the National Trust who acquired it in 1910 and partly managed by the Somerset Wildlife Trust.\n\nClose to the village and gorge are Batts Combe quarry and Callow Rock quarry, two of the active Quarries of the Mendip Hills where limestone is still extracted. Operating since the early 20th century, Batts Combe is owned and operated by Hanson Aggregates. The output in 2005 was around 4,000 tonnes of limestone per day, one third of which was supplied to an on-site lime kiln, which closed in 2009; the remainder was sold as coated or dusted aggregates. The limestone at this site is close to 99 percent carbonate of calcium and magnesium (dolomite).\n\nThe Chelmscombe Quarry finished its work as a limestone quarry in the 1950s and was then used by the Central Electricity Generating Board as a tower testing station. During the 1970s and 1980s it was also used to test the ability of containers of radioactive material to withstand impacts and other accidents.\n\nAlong with the rest of South West England, Cheddar has a temperate climate which is generally wetter and milder than the rest of the country. The annual mean temperature is approximately . Seasonal temperature variation is less extreme than most of the United Kingdom because of the adjacent sea, which moderates temperature. The summer months of July and August are the warmest with mean daily maxima of approximately . In winter mean minimum temperatures of or are common. In the summer the Azores high-pressure system affects the south-west of England. Convective cloud sometimes forms inland, reducing the number of hours of sunshine; annual sunshine rates are slightly less than the regional average of 1,600 hours. In December 1998 there were 20 days without sun recorded at Yeovilton. Most the rainfall in the south-west is caused by Atlantic depressions or by convection. Most of the rainfall in autumn and winter is caused by the Atlantic depressions, which are most active during those seasons. In summer, a large proportion of the rainfall is caused by sun heating the ground leading to convection and to showers and thunderstorms. Average rainfall is around . About 8–15 days of snowfall per year is typical. November to March have the highest mean wind speeds, and June to August have the lightest winds. The predominant wind direction is from the south-west.\n\nThe parish has a population of 5,093, with a mean age of 43 years. Residents live in 2,209 households. The vast majority of households (2,183) give their ethnic status as white.\n\nThe village gave its name to Cheddar cheese, which is the most popular type of cheese in the United Kingdom. The cheese is now made and consumed worldwide, and only one producer remains in the village.\n\nSince the 1880s, Cheddar's other main produce has been the strawberry,\nwhich is grown on the south-facing lower slopes of the Mendip hills. As a consequence of its use for transporting strawberries to market, the since-closed Cheddar Valley line became known as \"The Strawberry Line\" after it opened in 1869.\nThe line ran from Yatton to Wells. When the rest of the line was closed and all passenger services ceased, the section of the line between Cheddar and Yatton remained open for goods traffic. It provided a fast link with the main markets for the strawberries in Birmingham and London, but finally closed in 1964, becoming part of the Cheddar Valley Railway Nature Reserve.\n\nCheddar Ales is a small brewery based in the village, producing beer for local public houses. Tourism is a significant source of employment. Around 15 percent of employment in Sedgemoor is provided by tourism, but within Cheddar it is estimated to employ as many as 1,000 people.\nThe village also has a youth hostel, and a number of camping and caravan sites.\n\nCheddar has a number of active service clubs including Cheddar Vale Lions Club, Mendip Rotary and Mendip Inner Wheel Club. The clubs raise money for projects in the local community and hold annual events such as a fireworks display, duck races in the Gorge, a dragon boat race on the reservoir and concerts on the grounds of the nearby St Michael's Cheshire Home.\n\nSeveral notable people have been born or lived in Cheddar. Musician Jack Bessant, the bass guitarist with the band Reef grew up on his parents' strawberry farm, and Matt Goss and Luke Goss, former members of Bros, lived in Cheddar for nine months as children. Trina Gulliver, eight-time World Professional Darts Champion, currently lives in Cheddar.\nThe comedian Richard Herring grew up in Cheddar. His 2008 Edinburgh Festival Fringe show, \"The Headmaster's Son\" is based on his time at The Kings of Wessex School, where his father Keith was the headmaster. The final performance of this show was held at the school in November 2009. He also visited the school in March 2010 to perform his show \"Hitler Moustache\". In May 2013, a community radio station called Pulse was launched.\n\nThe market cross in Bath Street dates from the 15th century, with the shelter having been rebuilt in 1834. It has a central octagonal pier, a socket raised on four steps, a hexagonal shelter with six arched four-centred openings, shallow two-stage buttresses at each angle, and an embattled parapet. The shaft is crowned by an abacus with figures in niches, probably from the late 19th century, although the cross is now missing. It was rebuilt by Thomas, Marquis of Bath. It is a Scheduled Ancient Monument (Somerset County No 21) and Grade II* listed building.\nIn January 2000, the cross was seriously damaged in a traffic accident. By 2002, the cross had been rebuilt and the area around it was redesigned to protect and enhance its appearance.\nThe cross was badly damaged again in March 2012, when a taxi crashed into it late at night demolishing two sides.\nRepair work, which included the addition of wooden-clad steel posts to protect against future crashes, was completed in November 2012 at a cost of £60,000.\n\nHannah More, a philanthropist and educator, founded a school in the village in the late 18th century for the children of miners. Her first school was located in a 17th-century house. Now named \"Hannah More's Cottage\", the Grade II-listed building is used by the local community as a meeting place.\n\nThe village is situated on the A371 road which runs from Wincanton, to Weston-super-Mare. It is approximately from the route of the M5 motorway with around a drive to junction 21 or 22.\n\nIt was on the Cheddar Valley line, a railway line that was opened in 1869 and closed in 1963. It became known as The Strawberry Line because of the large volume of locally-grown strawberries that it carried. It ran from Yatton railway station through to Wells (Tucker Street) railway station and joined the East Somerset Railway to make a through route via Shepton Mallet (High Street) railway station to Witham. Sections of the now-disused railway have been opened as the Strawberry Line Trail, which currently runs from Yatton to Cheddar. The Cheddar Valley line survived until the \"Beeching Axe\". Towards the end of its life there were so few passengers that diesel railcars were sometimes used. The Cheddar branch closed to passengers on 9 September 1963 and to goods in 1964. The line closed in the 1960s, when it became part of the Cheddar Valley Railway Nature Reserve, and part of the National Cycle Network route 26. The cycle route also intersects with the West Mendip Way and various other footpaths.\n\nThe first school in Cheddar was set up by Hannah More during the 18th Century, however now Cheddar has three schools belonging to the Cheddar Valley Group of Schools, twelve schools that provide Cheddar Valley's three-tier education system. Cheddar First School has ten classes for children between 4 and 9 years. Fairlands Middle School, a middle school categorised as a middle-deemed-secondary school, has 510 pupils between 9 and 13. Fairlands takes children moving up from Cheddar First School as well as other first schools in the Cheddar Valley. The Kings of Wessex Academy, a coeducational comprehensive school, has been rated as \"good\" by Ofsted. It has 1,176 students aged 13 to 18, including 333 in the sixth form. Kings is a faith school linked to the Church of England. It was awarded the specialist status of Technology College in 2001, enabling it to develop its Information Technology (IT) facilities and improve courses in science, mathematics and design technology. In 2007 it became a foundation school, giving it more control over its own finances. The academy owns and runs a sports centre and swimming pool, Kings Fitness & Leisure, with facilities that are used by students as well as residents. It has since November 2016 been a part of the Wessex Learning Trust which incorporates eight academies from the surrounding area.\n\nThe Church of St Andrew dates from the 14th century. It was restored in 1873 by William Butterfield. It is a Grade I listed building and contains some 15th century stained glass and an altar table of 1631. The chest tomb in the chancel is believed to contain the remains of Sir Thomas Cheddar and is dated 1442. The tower, which rises to , contains a bell dating from 1759 made by Thomas Bilbie of the Bilbie family.\n\nThere are also churches for Roman Catholic, Methodist and other denominations, including Cheddar Valley Community Church, who not only meet at The Kings of Wessex School on Sunday, but also have their own site on Tweentown for meeting during the week. The Baptist chapel was built in 1831.\n\nKings Fitness & Leisure, situated on the grounds of The Kings of Wessex School, provides a venue for various sports and includes a 20-metre swimming pool, racket sport courts, a sports hall, dance studios and a gym. A youth sports festival was held on Sharpham Road Playing Fields in 2009. In 2010 a skatepark was built in the village, funded by the Cheddar Local Action Team.\n\nCheddar Football Club, founded in 1892 and nicknamed \"The Cheesemen\", play in the Somerset County Football League Premier Division. In 2009 plans were revealed to move the club from its present home at Bowdens Park on Draycott Road to a new larger site.\n\nCheddar Cricket Club was formed in the late 19th century and moved to Sharpham Road Playing Fields in 1964. They now play in the West of England Premier League Somerset Division. Cheddar Rugby Club, who own part of the Sharpham playing fields, was formed in 1836. The club organises an annual Cheddar Rugby Tournament. Cheddar Lawn Tennis Club, was formed in 1924, and play in the North Somerset League and also has social tennis and coaching. Cheddar Running Club organised an annual half marathon until 2009.\n\nThe village is both on the route of the West Mendip Way and Samaritans Way South West.\n\n"}
{"id": "6429", "url": "https://en.wikipedia.org/wiki?curid=6429", "title": "Compact disc", "text": "Compact disc\n\nCompact disc (CD) is a digital optical disc data storage format released in 1982 and co-developed by Philips and Sony. The format was originally developed to store and play only sound recordings but was later adapted for storage of data (CD-ROM). Several other formats were further derived from these, including write-once audio and data storage (CD-R), rewritable media (CD-RW), Video Compact Disc (VCD), Super Video Compact Disc (SVCD), Photo CD, PictureCD, CD-i, and Enhanced Music CD. The first commercially available Audio CD player, the Sony CDP-101, was released October 1982 in Japan.\n\nStandard CDs have a diameter of and can hold up to about 80 minutes of uncompressed audio or about 700 MiB of data. The Mini CD has various diameters ranging from ; they are sometimes used for CD singles, storing up to 24 minutes of audio, or delivering device drivers.\n\nAt the time of the technology's introduction in 1982, a CD could store much more data than a personal computer hard drive, which would typically hold 10 MB. By 2010, hard drives commonly offered as much storage space as a thousand CDs, while their prices had plummeted to commodity level. In 2004, worldwide sales of audio CDs, CD-ROMs and CD-Rs reached about 30 billion discs. By 2007, 200 billion CDs had been sold worldwide.\n\nFrom the early 2010s CDs were increasingly being replaced by other forms of digital storage and distribution, with the result that audio CD sales rates in the U.S. have dropped about 50% from their peak; however, they remain one of the primary distribution methods for the music industry. In 2014, revenues from digital music services matched those from physical format sales for the first time.\n\nAmerican inventor James T. Russell has been credited with inventing the first system to record digital information on an optical transparent foil that is lit from behind by a high-power halogen lamp. Russell's patent application was first filed in 1966, and he was granted a patent in 1970. Following litigation, Sony and Philips licensed Russell's patents (then held by a Canadian company, Optical Recording Corp.) in the 1980s.\n\nThe compact disc is an evolution of LaserDisc technology, where a focused laser beam\nis used that enables the high information density required for high-quality digital audio signals.\nPrototypes were developed by Philips and Sony independently in the late 1970s. Although originally dismissed by Philips Research management as a trivial pursuit, the CD became the primary focus for Philips as the LaserDisc format struggled. In 1979, Sony and Philips set up a joint task force of engineers to design a new digital audio disc. After a year of experimentation and discussion, the \"Red Book\" CD-DA standard was published in 1980. After their commercial release in 1982, compact discs and their players were extremely popular. Despite costing up to $1,000, over 400,000 CD players were sold in the United States between 1983 and 1984. By 1988 CD sales in the United States surpassed those of vinyl LPs, and by 1992 CD sales surpassed those of prerecorded music cassette tapes. The success of the compact disc has been credited to the cooperation between Philips and Sony, who came together to agree upon and develop compatible hardware. The unified design of the compact disc allowed consumers to purchase any disc or player from any company, and allowed the CD to dominate the at-home music market unchallenged.\n\nIn 1974, L. Ottens, director of the audio division of Philips, started a small group with the aim to develop an analog optical audio disc with a diameter of 20 cm and a sound quality superior to that of the vinyl record. However, due to the unsatisfactory performance of the analog format, two Philips research engineers recommended a digital format in March 1974. In 1977, Philips then established a laboratory with the mission of creating a digital audio disc. The diameter of Philips's prototype compact disc was set at 11.5 cm, the diagonal of an audio cassette.\n\nHeitaro Nakajima, who developed an early digital audio recorder within Japan's national public broadcasting organization NHK in 1970, became general manager of Sony's audio department in 1971. His team developed a digital PCM adaptor audio tape recorder using a Betamax video recorder in 1973. After this, in 1974 the leap to storing digital audio on an optical disc was easily made. Sony first publicly demonstrated an optical digital audio disc in September 1976. A year later, in September 1977, Sony showed the press a 30 cm disc that could play 60 minutes of digital audio (44,100 Hz sampling rate and 16-bit resolution) using MFM modulation. In September 1978, the company demonstrated an optical digital audio disc with a 150-minute playing time, 44,056 Hz sampling rate, 16-bit linear resolution, and cross-interleaved error correction code—specifications similar to those later settled upon for the standard compact disc format in 1980. Technical details of Sony's digital audio disc were presented during the 62nd AES Convention, held on 13–16 March 1979, in Brussels. Sony's AES technical paper was published on 1 March 1979. A week later, on 8 March, Philips publicly demonstrated a prototype of an optical digital audio disc at a press conference called \"Philips Introduce Compact Disc\" in Eindhoven, Netherlands.\n\nSony executive Norio Ohga, later CEO and chairman of Sony, and Heitaro Nakajima were convinced of the format's commercial potential and pushed further development despite widespread skepticism.\n\nAs a result, in 1979, Sony and Philips set up a joint task force of engineers to design a new digital audio disc. Led by engineers Kees Schouhamer Immink and Toshitada Doi, the research pushed forward laser and optical disc technology. After a year of experimentation and discussion, the task force produced the \"Red Book\" CD-DA standard. First published in 1980, the standard was formally adopted by the IEC as an international standard in 1987, with various amendments becoming part of the standard in 1996.\n\nPhilips coined the term \"compact disc\" in line with another audio product, the Compact Cassette, and contributed the general manufacturing process, based on video LaserDisc technology. Philips also contributed eight-to-fourteen modulation (EFM), which offers a certain resilience to defects such as scratches and fingerprints, while Sony contributed the error-correction method, CIRC.\n\nThe \"Compact Disc Story\", told by a former member of the task force, gives background information on the many technical decisions made, including the choice of the sampling frequency, playing time, and disc diameter. The task force consisted of around four to eight persons, though according to Philips, the compact disc was \"invented collectively by a large group of people working as a team.\"\n\nPhilips established the Polydor Pressing Operations plant in Langenhagen near Hannover, Germany, and quickly passed a series of milestones.\n\nThe Japanese launch was followed in March 1983 by the introduction of CD players and discs to Europe and North America (where CBS Records released sixteen titles). This event is often seen as the \"Big Bang\" of the digital audio revolution. The new audio disc was enthusiastically received, especially in the early-adopting classical music and audiophile communities, and its handling quality received particular praise. As the price of players gradually came down, and with the introduction of the portable Discman the CD began to gain popularity in the larger popular and rock music markets. One of the first CD markets was devoted to reissuing popular music whose commercial potential was already proven. An advantage of the format was the ability to produce and market boxed sets and multi-volume collections. The first artist to sell a million copies on CD was Dire Straits, with their 1985 album \"Brothers in Arms\". The first major artist to have his entire catalogue converted to CD was David Bowie, whose 15 studio albums were made available by RCA Records in February 1985, along with four greatest hits albums. On February 26, 1987, the first four UK albums by The Beatles were released in mono on compact disc. In 1988, 400 million CDs were manufactured by 50 pressing plants around the world.\n\nThe CD was planned to be the successor of the vinyl record for playing music, rather than primarily as a data storage medium. From its origins as a musical format, CDs have grown to encompass other applications. In 1983, following the CD's introduction, Immink and Braat presented the first experiments with erasable compact discs during the 73rd AES Convention. In June 1985, the computer-readable CD-ROM (read-only memory) and, in 1990, CD-Recordable were introduced, also developed by both Sony and Philips. Recordable CDs were a new alternative to tape for recording music and copying music albums without defects introduced in compression used in other digital recording methods. Other newer video formats such as DVD and Blu-ray use the same physical geometry as CD, and most DVD and Blu-ray players are backward compatible with audio CD.\n\nBy the early 2000s, the CD player had largely replaced the audio cassette player as standard equipment in new automobiles, with 2010 being the final model year for any car in the United States to have a factory-equipped cassette player. With the increasing popularity of portable digital audio players, such as mobile phones, and solid state music storage, CD players are being phased out of automobiles in favor of minijack auxiliary inputs, wired connection to USB devices and wireless Bluetooth connection.\n\nMeanwhile, with the advent and popularity of Internet-based distribution of files in lossily-compressed audio formats such as MP3, sales of CDs began to decline in the 2000s. For example, between 2000 and 2008, despite overall growth in music sales and one anomalous year of increase, major-label CD sales declined overall by 20%, although independent and DIY music sales may be tracking better according to figures released 30 March 2009, and CDs still continue to sell greatly. As of 2012, CDs and DVDs made up only 34 percent of music sales in the United States. , only 24% of music in the United States was purchased on physical media, ⅔ of this consisting of CDs; however, in the same year in Japan, over 80% of music was bought on CDs and other physical formats.\n\nDespite the rapidly declining sales year-over-year, the pervasiveness of the technology remains: Companies are placing CDs in pharmacies, supermarkets, and filling station convenience stores targeting buyers least able to utilize Internet-based distribution.\n\nSony and Philips received praise for the development of the compact disc from professional organizations. These awards include\n\nA CD is made from thick, polycarbonate plastic and weighs 15–20 grams. From the center outward, components are: the center spindle hole (15 mm), the first-transition area (clamping ring), the clamping area (stacking ring), the second-transition area (mirror band), the program (data) area, and the rim. The inner program area occupies a radius from 25 to 58 mm.\n\nA thin layer of aluminium or, more rarely, gold is applied to the surface, making it reflective. The metal is protected by a film of lacquer normally spin coated directly on the reflective layer. The label is printed on the lacquer layer, usually by screen printing or offset printing.\n\nCD data is represented as tiny indentations known as \"pits\", encoded in a spiral track moulded into the top of the polycarbonate layer. The areas between pits are known as \"lands\". Each pit is approximately 100 nm deep by 500 nm wide, and varies from 850 nm to 3.5 µm in length. The distance between the tracks, the pitch, is 1.6 µm.\n\nA motor within the CD player spins the disc to a scanning velocity of 1.2–1.4 m/s (constant linear velocity) – equivalent to approximately 500 RPM at the inside of the disc, and approximately 200 RPM at the outside edge. (A disc played from beginning to end slows its rotation rate during playback.)\nThe program area is 86.05 cm and the length of the recordable spiral is (86.05 cm / 1.6 µm) = 5.38 km. With a scanning speed of 1.2 m/s, the playing time is 74 minutes, or 650 MiB of data on a CD-ROM. A disc with data packed slightly more densely is tolerated by most players (though some old ones fail). Using a linear velocity of 1.2 m/s and a narrower track pitch of 1.5 µm increases the playing time to 80 minutes, and data capacity to 700 MiB.\n\nA CD is read by focusing a 780 nm wavelength (near infrared) semiconductor laser housed within the CD player, through the bottom of the polycarbonate layer. The change in height between pits and lands results in a difference in the way the light is reflected. By measuring the intensity change with a photodiode, the data can be read from the disc. In order to accommodate the spiral pattern of data, the semiconductor laser is placed on a swing arm within the disc tray of any CD player. This swing arm allows the laser to read information from the centre to the edge of a disc, without having to interrupt the spinning of the disc itself.\n\nThe pits and lands do not directly represent the zeros and ones of binary data. Instead, non-return-to-zero, inverted encoding is used: a change from pit to land or land to pit indicates a one, while no change indicates a series of zeros. There must be at least two and no more than ten zeros between each one, which is defined by the length of the pit. This in turn is decoded by reversing the eight-to-fourteen modulation used in mastering the disc, and then reversing the cross-interleaved Reed–Solomon coding, finally revealing the raw data stored on the disc. These encoding techniques (defined in the \"Red Book\") were originally designed for CD Digital Audio, but they later became a standard for almost all CD formats (such as CD-ROM).\n\nCDs are susceptible to damage during handling and from environmental exposure. Pits are much closer to the label side of a disc, enabling defects and contaminants on the clear side to be out of focus during playback. Consequently, CDs are more likely to suffer damage on the label side of the disc. Scratches on the clear side can be repaired by refilling them with similar refractive plastic or by careful polishing. The edges of CDs are sometimes incompletely sealed, allowing gases and liquids to corrode the metal reflective layer and to interfere with the focus of the laser on the pits. The fungus \"Geotrichum candidum\", found in Belize, has been found to consume the polycarbonate plastic and aluminium found in CDs.\n\nThe digital data on a CD begins at the center of the disc and proceeds toward the edge, which allows adaptation to the different size formats available. Standard CDs are available in two sizes. By far, the most common is in diameter, with a 74- or 80-minute audio capacity and a 650 or 700 MiB (737,280,000-byte) data capacity. This capacity was reportedly specified by Sony executive Norio Ohga in May 1980 so as to be able to contain the entirety of Beethoven's Ninth Symphony on one disc. This is a myth according to Kees Immink, as the code format had not yet been decided in May 1980. The adoption of EFM one month later would have allowed a playing time of 97 minutes for 120 mm diameter or 74 minutes for a disc as small as 100 mm. The 120 mm diameter has been adopted by subsequent formats, including Super Audio CD, DVD, HD DVD, and Blu-ray Disc. The 80 mm diameter discs (\"Mini CDs\") can hold up to 24 minutes of music or 210 MiB.\n\nThe logical format of an audio CD (officially Compact Disc Digital Audio or CD-DA) is described in a document produced in 1980 by the format's joint creators, Sony and Philips. The document is known colloquially as the \"Red Book\" CD-DA after the colour of its cover. The format is a two-channel 16-bit PCM encoding at a 44.1 kHz sampling rate per channel. Four-channel sound was to be an allowable option within the \"Red Book\" format, but has never been implemented. Monaural audio has no existing standard on a \"Red Book\" CD; thus, mono source material is usually presented as two identical channels in a standard \"Red Book\" stereo track (i.e., mirrored mono); an MP3 CD, however, can have audio file formats with mono sound.\n\nCD-Text is an extension of the \"Red Book\" specification for audio CD that allows for storage of additional text information (e.g., album name, song name, artist) on a standards-compliant audio CD. The information is stored either in the lead-in area of the CD, where there is roughly five kilobytes of space available, or in the subcode channels R to W on the disc, which can store about 31 megabytes.\n\nCompact Disc + Graphics is a special audio compact disc that contains graphics data in addition to the audio data on the disc. The disc can be played on a regular audio CD player, but when played on a special CD+G player, it can output a graphics signal (typically, the CD+G player is hooked up to a television set or a computer monitor); these graphics are almost exclusively used to display lyrics on a television set for karaoke performers to sing along with. The CD+G format takes advantage of the channels R through W. These six bits store the graphics information.\n\nCD + Extended Graphics (CD+EG, also known as CD+XG) is an improved variant of the Compact Disc + Graphics (CD+G) format. Like CD+G, CD+EG utilizes basic CD-ROM features to display text and video information in addition to the music being played. This extra data is stored in subcode channels R-W. Very few, if any, CD+EG discs have been published.\n\nSuper Audio CD (SACD) is a high-resolution read-only optical audio disc format that was designed to provide higher fidelity digital audio reproduction than the \"Red Book\". Introduced in 1999, it was developed by Sony and Philips, the same companies that created the \"Red Book\". SACD was in a format war with DVD-Audio, but neither has replaced audio CDs. The SACD standard is referred to the \"Scarlet Book\" standard.\n\nTitles in the SACD format can be issued as hybrid discs; these discs contain the SACD audio stream as well as a standard audio CD layer which is playable in standard CD players, thus making them backward compatible.\n\nCD-MIDI is a format used to store music-performance data which upon playback is performed by electronic instruments that synthesize the audio. Hence, unlike the original \"Red Book\" CD-DA, these recordings are not digitally sampled audio recordings. The CD-MIDI format is defined as an extension to the original \"Red Book\".\n\nFor the first few years of its existence, the CD was a medium used purely for audio. However, in 1988, the \"Yellow Book\" CD-ROM standard was established by Sony and Philips, which defined a non-volatile optical data computer data storage medium using the same physical format as audio compact discs, readable by a computer with a CD-ROM drive.\n\nVideo CD (VCD, View CD, and Compact Disc digital video) is a standard digital format for storing video media on a CD. VCDs are playable in dedicated VCD players, most modern DVD-Video players, personal computers, and some video game consoles.\n\nThe VCD standard was created in 1993 by Sony, Philips, Matsushita, and JVC and is referred to as the \"White Book\" standard.\n\nOverall picture quality is intended to be comparable to VHS video. Poorly compressed VCD video can sometimes be lower quality than VHS video, but VCD exhibits block artifacts rather than analog noise and does not deteriorate further with each use.\n\n352x240 (or SIF) resolution was chosen because it is half the vertical and half the horizontal resolution of NTSC video. 352x288 is similarly one quarter PAL/SECAM resolution. This approximates the (overall) resolution of an analog VHS tape, which, although it has double the number of (vertical) scan lines, has a much lower horizontal resolution.\n\nSuper Video CD (Super Video Compact Disc or SVCD) is a format used for storing video media on standard compact discs. SVCD was intended as a successor to VCD and an alternative to DVD-Video and falls somewhere between both in terms of technical capability and picture quality.\n\nSVCD has two-thirds the resolution of DVD, and over 2.7 times the resolution of VCD. One CD-R disc can hold up to 60 minutes of standard quality SVCD-format video. While no specific limit on SVCD video length is mandated by the specification, one must lower the video bit rate, and therefore quality, to accommodate very long videos. It is usually difficult to fit much more than 100 minutes of video onto one SVCD without incurring significant quality loss, and many hardware players are unable to play video with an instantaneous bit rate lower than 300 to 600 kilobits per second.\n\nPhoto CD is a system designed by Kodak for digitizing and storing photos on a CD. Launched in 1992, the discs were designed to hold nearly 100 high-quality images, scanned prints and slides using special proprietary encoding. Photo CDs are defined in the \"Beige Book\" and conform to the CD-ROM XA and CD-i Bridge specifications as well. They are intended to play on CD-i players, Photo CD players and any computer with the suitable software irrespective of the operating system. The images can also be printed out on photographic paper with a special Kodak machine. This format is not to be confused with Kodak Picture CD, which is a consumer product in CD-ROM format.\n\nThe Philips \"Green Book\" specifies a standard for interactive multimedia compact discs designed for CD-i players (1993). CD-i discs can contain audio tracks which can be played on regular CD players, but CD-i discs are not compatible with most CD-ROM drives and software. The CD-i Ready specification was later created to improve compatibility with audio CD players, and the CD-i Bridge specification was added to create CD-i compatible discs that can be accessed by regular CD-ROM drives.\n\nPhilips defined a format similar to CD-i called CD-i Ready, which puts CD-i software and data into the pregap of track 1. This format was supposed to be more compatible with older audio CD players.\n\nEnhanced Music CD, also known as CD Extra or CD Plus, is a format which combines audio tracks and data tracks on the same disc by putting audio tracks in a first session and data in a second session. It was developed by Philips and Sony, and it is defined in the \"Blue Book\".\n\nVinyl Disc is the hybrid of a standard audio CD and the vinyl record. The vinyl layer on the disc's label side can hold approximately three minutes of music.\n\nReplicated CDs are mass-produced initially using a hydraulic press. Small granules of heated raw polycarbonate plastic are fed into the press. A screw forces the liquefied plastic into the mold cavity. The mold closes with a metal stamper in contact with the disc surface. The plastic is allowed to cool and harden. Once opened, the disc substrate is removed from the mold by a robotic arm, and a 15 mm diameter center hole (called a stacking ring) is created. The time it takes to \"stamp\" one CD is usually two to three seconds.\n\nThis method produces the clear plastic blank part of the disc. After a metallic reflecting layer (usually aluminium, but sometimes gold or other metal) is applied to the clear blank substrate, the disc goes under a UV light for curing and it is ready to go to press. To prepare to press a CD, a glass master is made, using a high-powered laser on a device similar to a CD writer. The glass master is a positive image of the desired CD surface (with the desired microscopic pits and lands). After testing, it is used to make a die by pressing it against a metal disc.\n\nThe die is a negative image of the glass master: typically, several are made, depending on the number of pressing mills that are to make the CD. The die then goes into a press, and the physical image is transferred to the blank CD, leaving a final positive image on the disc. A small amount of lacquer is applied as a ring around the center of the disc, and rapid spinning spreads it evenly over the surface. Edge protection lacquer is applied before the disc is finished. The disc can then be printed and packed.\n\nManufactured CDs that are sold in stores are sealed via a process called \"polywrapping\" or shrink wrapping.\n\nThe most expensive part of a CD is the jewel case. In 1995, material costs were 30 cents for the jewel case and 10 to 15 cents for the CD. Wholesale cost of CDs was $0.75 to $1.15, which retailed for $16.98. On average, the store received 35 percent of the retail price, the record company 27 percent, the artist 16 percent, the manufacturer 13 percent, and the distributor 9 percent. When 8-track tapes, cassette tapes, and CDs were introduced, each was marketed at a higher price than the format they succeeded, even though the cost to produce the media was reduced. This was done because the apparent value increased. This continued from vinyl to CDs but was broken when Apple marketed MP3s for $0.99, and albums for $9.99. The incremental cost, though, to produce an MP3 is very small.\n\nRecordable Compact Discs, CD-Rs, are injection-molded with a \"blank\" data spiral. A photosensitive dye is then applied, after which the discs are metalized and lacquer-coated. The write laser of the CD recorder changes the colour of the dye to allow the read laser of a standard CD player to see the data, just as it would with a standard stamped disc. The resulting discs can be read by most CD-ROM drives and played in most audio CD players. CD-Rs follow the \"Orange Book\" standard.\n\nCD-R recordings are designed to be permanent. Over time, the dye's physical characteristics may change causing read errors and data loss until the reading device cannot recover with error correction methods. The design life is from 20 to 100 years, depending on the quality of the discs, the quality of the writing drive, and storage conditions. However, testing has demonstrated such degradation of some discs in as little as 18 months under normal storage conditions. This failure is known as disc rot, for which there are several, mostly environmental, reasons.\n\nThe recordable audio CD is designed to be used in a consumer audio CD recorder. These consumer audio CD recorders use SCMS (Serial Copy Management System), an early form of digital rights management (DRM), to conform to the AHRA (Audio Home Recording Act). The Recordable Audio CD is typically somewhat more expensive than CD-R due to lower production volume and a 3% AHRA royalty used to compensate the music industry for the making of a copy.\n\nHigh-capacity recordable CD is a higher-density recording format that can hold 90 or 99 minutes of audio on a disc (compared to about 80 minutes for \"Red Book\" audio) or 30 minutes of audio on an disc (compared to about 24 minutes for \"Red Book\" audio). The higher capacity is incompatible with some recorders and recording software.\n\nCD-RW is a re-recordable medium that uses a metallic alloy instead of a dye. The write laser in this case is used to heat and alter the properties (amorphous vs. crystalline) of the alloy, and hence change its reflectivity. A CD-RW does not have as great a difference in reflectivity as a pressed CD or a CD-R, and so many earlier CD audio players \"cannot\" read CD-RW discs, although \"most\" later CD audio players and stand-alone DVD players can. CD-RWs follow the \"Orange Book\" standard.\n\nThe ReWritable Audio CD is designed to be used in a consumer audio CD recorder, which will not (without modification) accept standard CD-RW discs. These consumer audio CD recorders use the Serial Copy Management System (SCMS), an early form of digital rights management (DRM), to conform to the United States' Audio Home Recording Act (AHRA). The ReWritable Audio CD is typically somewhat more expensive than CD-RW due to (a) lower volume and (b) a 3% AHRA royalty used to compensate the music industry for the making of a copy.\n\nDue to technical limitations, the original ReWritable CD could be written no faster than 4x speed. High Speed ReWritable CD has a different design, which permits writing at speeds ranging from 4x to 12x. Original CD-RW drives can only write to original ReWritable CDs. High Speed CD-RW drives can typically write to both original ReWritable CDs and High Speed ReWritable CDs. Both types of CD-RW discs can be read in most CD drives. Higher speed CD-RW discs, Ultra Speed (16x to 24x write speed) and Ultra Speed+ (32x write speed) are now available.\n\nThe \"Red Book\" audio specification, except for a simple \"anti-copy\" statement in the subcode, does not include any copy protection mechanism. Known at least as early as 2001, attempts were made by record companies to market \"copy-protected\" non-standard compact discs, which cannot be ripped, or copied, to hard drives or easily converted to other formats (like Flac, MP3 or Vorbis). One major drawback to these copy-protected discs is that most will not play on either computer CD-ROM drives or some standalone CD players that use CD-ROM mechanisms. Philips has stated that such discs are not permitted to bear the trademarked \"Compact Disc Digital Audio\" logo because they violate the \"Red Book\" specifications. Numerous copy-protection systems have been countered by readily available, often free, software.\n\n\n"}
{"id": "6431", "url": "https://en.wikipedia.org/wiki?curid=6431", "title": "Charles Farrar Browne", "text": "Charles Farrar Browne\n\nCharles Farrar Browne (April 26, 1834 – March 6, 1867) was a United States humor writer, better known under his \"nom de plume\", Artemus Ward. He is considered to be America's first stand-up comedian. At birth, his surname was \"Brown\"; he added the \"e\" after he became famous.\n\nBrowne was born in Waterford, Maine. He began his career as a compositor and occasional contributor to the daily and weekly journals. In 1858, he published in \"The Plain Dealer\" (Cleveland, Ohio) the first of the \"Artemus Ward\" series, which, in a collected form, achieved great popularity in both America and England. Brownes' companion at the Plain Dealer George Hoyt wrote \"his desk was a rickety table which had been whittled and gashed until it looked as if it had been the victim of lightning. His chair was a fit companion thereto, a wabbling, unsteady affair, sometimes with four and sometimes with three legs. But Browne saw neither the table, nor the chair, nor any person who might be near, nothing, in fact, but the funny pictures which were tumbling out of his brain. When writing, his gaunt form looked ridiculous enough. One leg hung over the arm of his chair like a great hook, while he would write away, sometimes laughing to himself, and then slapping the table in the excess of his mirth.\"\n\nIn 1860, he became editor of \"Vanity Fair\", a humorous New York weekly, which proved a failure. About the same time, he began to appear as a lecturer and, by his droll and eccentric humor, attracted large audiences.\n\nIn 1863, Browne came as Artemus Ward to San Francisco to perform. Browne was an expert at publicity and by the time of his arrival, his manager had already been there for weeks advertising with notices in the local papers and talking with prominent citizens for endorsements. On November 13, 1863, he performed to a packed crowd at Platt's Music Hall. Ward played the part of Artemus as an illiterate rube but with \"Yankee common sense.\" Writer Brett Harte was in the audience that night and he described it in \"the Golden Era\" as capturing American speech, \"humor that belongs to the country of boundless prairies, limitless rivers, and stupendous cataracts--that fun which overlies the surface of our national life, which is met in the stage, rail-car, canal and flat-boat, which bursts out over camp-fires and around bar-room stoves.\"\n\n\"Artemus Ward\" was the favorite author of U.S. President Abraham Lincoln. Before presenting \"The Emancipation Proclamation\" to his Cabinet, Lincoln read to them the latest episode, \"Outrage in Utiky\", also known as \"High-Handed Outrage at Utica\".\n\nBrowne was also known as a member of the New York Bohemian set which included leader Henry Clapp Jr., Walt Whitman, Fitz Hugh Ludlow, and actress Adah Isaacs Menken.\n\nWard met Mark Twain when Ward performed in Virginia City, Nevada and the two became friends. In his correspondences with Twain, Browne called him \"My Dearest Love.\" Legend has it that, following Ward's stage performance, he, Mark Twain, and Dan De Quille were taking a drunken rooftop tour of Virginia City until a town constable threatened to blast all three of them with a shotgun loaded with rock salt. Browne recommended Twain to the editors of the New York Press and urged him to journey to New York.\n\nIn 1866, Ward visited England, where he became exceedingly popular both as a lecturer and as a contributor to \"Punch\". In the spring of the following year, Ward's health gave way and he died of tuberculosis at Southampton on March 6, 1867.\n\nAfter initially being buried at Kensal Green Cemetery, Ward's remains were removed to the United States on May 20, 1868. He is buried at Elm Vale Cemetery in Waterford, Maine.\n\n\n\n"}
{"id": "6432", "url": "https://en.wikipedia.org/wiki?curid=6432", "title": "Caelum", "text": "Caelum\n\nCaelum is a faint constellation in the southern sky, introduced in the 1750s by Nicolas Louis de Lacaille and counted among the 88 modern constellations. Its name means “\"chisel\"” in Latin, and it was formerly known as Caelum Scalptorium (“\"the engravers’ chisel\"”); It is a rare word, unrelated to the far more common Latin \"caelum\", meaning “sky, heaven, atmosphere”. It is the eighth-smallest constellation, and subtends a solid angle of around 0.038 steradians, just less than that of Corona Australis.\n\nDue to its small size and location away from the plane of the Milky Way, Caelum is a rather barren constellation, with few objects of interest. The constellation's brightest star, Alpha Caeli, is only of magnitude 4.45, and only one other star, (Gamma)\"γ\" Caeli, is brighter than magnitude 5. Other notable objects in Caelum are RR Caeli, a binary star with one known planet approximately away; X Caeli, a Delta Scuti variable that forms an optical double with \"γ\" Caeli; and HE0450-2958, a Seyfert galaxy that at first appeared as just a jet, with no host galaxy visible.\n\nCaelum was first introduced in the eighteenth century by Nicolas Louis de Lacaille, a French astronomer who introduced thirteen other southern constellations at the same time.\nLacaille gave the constellation the French name \"Burin\", which was originally Latinized to \"Caelum Scalptorium\" (“\"The Engravers’ Chisel\"”).\n\nFrancis Baily shortened this name to \"Caelum\", as suggested by John Herschel. In Lacaille's original chart, the constellation was shown both as a burin and an échoppe, although it has come to be recognized simply as a chisel. Johann Elert Bode stated the name as plural with a singular possessor, \"Caela Scalptoris\" – in German (\"die\") \"Grabstichel\" (“\"the Engraver’s Chisels\"”) – but this did not stick.\n\nCaelum is bordered by Dorado and Pictor to the south, Horologium and Eridanus to the east, Lepus to the north, and Columba to the west. Covering only 125 square degrees, it ranks 81st of the 88 modern constellations in size. It appears prominently in the southern sky during the Southern Hemisphere's summer, and the whole constellation is visible for at least part of the year to observers south of latitude 41°N.\nIts main asterism consists of four stars, and twenty stars in total are brighter than magnitude 6.5.\n\nThe constellation’s boundaries, as set by Eugène Delporte in 1930, are defined by a 12-sided polygon. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between and . The International Astronomical Union (IAU) adopted the three-letter abbreviation “Cae” for the constellation in 1922.\n\nCaelum is a faint constellation: It has no star brighter than magnitude 4 and only two stars brighter than magnitude 5.\n\nLacaille gave six stars Bayer designations, labeling them Alpha (\"α\") to Zeta (\"ζ\") in 1756, but omitted Epsilon (\"ε\") and designated two adjacent stars as Gamma (\"γ\"). Bode extended the designations to Rho (\"ρ\") for other stars, but most of these have fallen out of use. Caelum is too far south for any of its stars to bear Flamsteed designations.\nThe brightest star, (Alpha) \"α\" Caeli, is a double star, containing an F-type main-sequence star of magnitude 4.45 and a red dwarf of magnitude 12.5, from Earth. (Beta) \"β\" Caeli, another F-type star of magnitude 5.05, is further away, being located from Earth. Unlike \"α\", \"β\" Caeli is a subgiant star, slightly evolved from the main sequence. (Delta) \"δ\" Caeli, also of magnitude 5.05, is a B-type subgiant and is much farther from Earth, at .\n\n(Gamma) \"γ\" Caeli is a double-star with a red giant primary of magnitude 4.58 and a secondary of magnitude 8.1. The primary is from Earth. The two components are difficult to resolve with small amateur telescopes because of their difference in visual magnitude and their close separation. This star system forms an optical double with the unrelated X Caeli (previously named \"γ\" Caeli), a Delta Scuti variable located from Earth. These are a class of short-period (six hours at most) pulsating stars that have been used as standard candles and as subjects to study astroseismology. X Caeli itself is also a binary star, specifically a contact binary, meaning that the stars are so close that they share envelopes. The only other variable star in Caelum visible to the naked eye is RV Caeli, a pulsating red giant of spectral type M1III, which varies between magnitudes 6.44 and 6.56.\n\nThree other stars in Caelum are still occasionally referred to by their Bayer designations, although they are only on the edge of naked-eye visibility. (Nu) \"ν\" Caeli is another double star, containing a white giant of magnitude 6.07 and a star of magnitude 10.66, with unknown spectral type. The system is approximately away. (Lambda) \"λ\" Caeli, at magnitude 6.24, is much redder and farther away, being a red giant around from Earth. (Zeta) \"ζ\" Caeli is even fainter, being only of magnitude 6.36. This star, located away, is a K-type subgiant of spectral type K1. The other twelve naked-eye stars in Caelum are not referred to by Bode's Bayer designations anymore, including RV Caeli.\n\nOne of the nearest stars in Caelum is the eclipsing binary star RR Caeli, at a distance of . This star system consists of a dim red dwarf and a white dwarf. Despite its closeness to the Earth, the system's apparent magnitude is only 14.40 due to the faintness of its components, and thus it cannot be easily seen with amateur equipment. In 2012, the system was found to contain a giant planet, and there is evidence for a second substellar body. The system is a post-common-envelope binary and is losing angular momentum over time, which will eventually cause mass transfer from the red dwarf to the white dwarf. In approximately 9–20 billion years, this will cause the system to become a cataclysmic variable.\n\nDue to its small size and location away from the plane of the Milky Way, Caelum is rather devoid of deep-sky objects, and contains no Messier objects. The only deep-sky object in Caelum to receive much attention is HE0450-2958, an unusual Seyfert galaxy. Originally, the jet's host galaxy proved elusive to find, and this jet appeared to be emanating from nothing. Although it has been suggested that the object is an ejected supermassive black hole, the host is now agreed to be a small galaxy that is difficult to see due to light from the jet and a nearby starburst galaxy.\n\n"}
{"id": "6433", "url": "https://en.wikipedia.org/wiki?curid=6433", "title": "Clarinet", "text": "Clarinet\n\nThe clarinet is a musical-instrument family belonging to the group known as the woodwind instruments. It has a single-reed mouthpiece, a straight cylindrical tube with an almost cylindrical bore, and a flared bell. A person who plays a clarinet is called a \"clarinetist\" (sometimes spelled \"clarinettist\").\n\nThe word \"clarinet\" may have entered the English language via the French \"clarinette\" (the feminine diminutive of Old French \"clarin\" or \"clarion\"), or from Provençal \"\", \"oboe\". It would seem however that its real roots are to be found amongst some of the various names for trumpets used around the renaissance and baroque eras. \"Clarion\", \"clarin\" and the Italian \"clarino\" are all derived from the medieval term \"claro\" which referred to an early form of trumpet. This is probably the origin of the Italian \"clarinetto\", itself a diminutive of \"clarino\", and consequently of the European equivalents such as \"clarinette\" in French or the German \"Klarinette\". According to Johann Gottfried Walther, writing in 1732, the reason for the name is that \"it sounded from far off not unlike a trumpet\". The English form \"clarinet\" is found as early as 1733, and the now-archaic \"clarionet\" appears from 1784 until the early years of the 20th century.\n\nWhile the similarity in sound between the earliest clarinets and the trumpet may hold a clue to its name, other factors may have been involved. During the late baroque era, composers such as Bach and Handel were making new demands on the skills of their trumpeters, who were often required to play difficult melodic passages in the high, or as it came to be called, \"clarion\" register. Since the trumpets of this time had no valves or pistons, melodic passages would often require the use of the highest part of the trumpet's range, where the harmonics were close enough together to produce scales of adjacent notes as opposed to the gapped scales or arpeggios of the lower register. The trumpet parts that required this speciality were known by the term \"clarino\" and this in turn came to apply to the musicians themselves. It is probable that the term clarinet may stem from the diminutive version of the 'clarion' or 'clarino' and it has been suggested that clarino players may have helped themselves out by playing particularly difficult passages on these newly developed \"mock trumpets\".\n\nJohann Christoph Denner is generally believed to have invented the clarinet in Germany around the year 1700 by adding a register key to the earlier chalumeau. Over time, additional keywork and airtight pads were added to improve the tone and playability.\n\nThese days the most popular clarinet is the B clarinet. However, the clarinet in A, just a semitone lower, is commonly used in orchestral music. Since the middle of the 19th century the bass clarinet (nowadays invariably in B but with extra keys to extend the register down a few notes) has become an essential addition to the orchestra. The clarinet family ranges from the (extremely rare) BBB octo-contrabass to the A piccolo clarinet. The clarinet has proved to be an exceptionally flexible instrument, equally at home in the classical repertoire as in concert bands, military bands, marching bands, klezmer, and jazz.\n\nThe cylindrical bore is primarily responsible for the clarinet's distinctive timbre, which varies between its three main registers, known as the chalumeau, clarion, and altissimo. The tone quality can vary greatly with the musician, the music, the instrument, the mouthpiece, and the reed. The differences in instruments and geographical isolation of players in different countries led to the development, from the last part of the 18th century onwards, of several different schools of clarinet playing. The most prominent were the German/Viennese traditions and the French school. The latter was centered on the clarinetists of the Conservatoire de Paris. The proliferation of recorded music has made examples of different styles of clarinet playing available. The modern clarinetist has a diverse palette of \"acceptable\" tone qualities to choose from.\nThe A clarinet and B clarinet have nearly the same bore, and use the same mouthpiece. Orchestral players using the A and B instruments in the same concert could use the same mouthpiece (and often the same barrel) for both (see 'usage' below). The A and the B instruments have nearly identical tonal quality, although the A typically has a slightly warmer sound. The tone of the E clarinet is brighter than that of the lower clarinets and can be heard even through loud orchestral or concert band textures. The bass clarinet has a characteristically deep, mellow sound, while the alto clarinet is similar in tone to the bass (though not as dark).\n\nClarinets have the largest pitch range of common woodwinds. The intricate key organization that makes this range possible can make the playability of some passages awkward. The bottom of the clarinet's written range is defined by the keywork on each instrument, standard keywork schemes allowing a low E on the common B clarinet. The lowest concert pitch depends on the transposition of the instrument in question. The nominal highest note of the B clarinet is a semitone higher than the highest note of the oboe. Since the clarinet has a wider range of notes, the lowest note of the B clarinet is significantly deeper (a minor or major sixth) than the lowest note of the oboe.\n\nNearly all soprano and piccolo clarinets have keywork enabling them to play the E below middle C as their lowest written note (in scientific pitch notation that sounds D on a soprano clarinet or C, i.e. concert middle C, on a piccolo clarinet), though some B clarinets go down to E to enable them to match the range of the A clarinet. On the B soprano clarinet, the concert pitch of the lowest note is D, a whole tone lower than the written pitch.\n\nMost alto and bass clarinets have an extra key to allow a (written) E. Modern professional-quality bass clarinets generally have additional keywork to written C. Among the less commonly encountered members of the clarinet family, contra-alto and contrabass clarinets may have keywork to written E, D, or C; the basset clarinet and basset horn generally go to low C.\n\nDefining the top end of a clarinet's range is difficult, since many advanced players can produce notes well above the highest notes commonly found in method books. G is usually the highest note clarinetists encounter in classical repertoire. The C above that (C i.e. resting on the fifth ledger line above the treble staff) is attainable by advanced players and is shown on many fingering charts, and fingerings as high as A exist.\n\nThe range of a clarinet can be divided into three distinct registers. The lowest register, from low written E to the written B above middle C (B), is known as the \"chalumeau\" register (named after the instrument that was the clarinet's immediate predecessor). The middle register is known as the \"clarion\" register (sometimes in the U.S. as the \"clarino\" register from the Italian) and spans just over an octave (from written B above middle C (B) to the C two octaves above middle C (C)); it is the dominant range for most members of the clarinet family. The top or \"altissimo\" register consists of the notes above the written C two octaves above middle C (C). All three registers have characteristically different sounds. The chalumeau register is rich and dark. The clarion register is brighter and sweet, like a trumpet (\"clarion\") heard from afar. The altissimo register can be piercing and sometimes shrill.\n\nSound is a wave that propagates through the air as a result of a local variation in air pressure. The production of sound by a clarinet follows these steps:\n\n\nThe cycle repeats at a frequency relative to how long it takes a wave to travel to the first open hole and back twice (i.e. four times the length of the pipe). For example: when all the holes bar the very top one are open (i.e. the trill 'B' key is pressed), the note A4 (440 Hz) is produced. This represents a repeat of the cycle 440 times per second.\n\nIn addition to this primary compression wave, other waves, known as harmonics, are created. Harmonics are caused by factors including: the imperfect wobbling and shaking of the clarinet reed, the reed sealing the mouthpiece opening for part of the wave cycle (which creates a flattened section of the sound wave) and imperfections (bumps and holes) in the clarinet bore. A wide variety of compression waves are created, but only some (primarily the odd harmonics) are reinforced. These extra waves are what gives the clarinet its characteristic tone.\n\nThe bore of the clarinet is cylindrical for most of the tube with an inner bore diameter between , but there is a subtle hourglass shape, with the thinnest part below the junction between the upper and lower joint. The reduction is depending on the maker. This \"hourglass\" shape, although not visible to the naked eye, helps to correct the pitch/scale discrepancy between the chalumeau and clarion registers (perfect 12th). The diameter of the bore affects characteristics such as available harmonics, timbre, and pitch stability (how far the player can bend a note in the manner required in jazz and other music). The bell at the bottom of the instrument flares out to improve the tone and tuning of the lowest notes.\n\nMost modern clarinets have \"undercut\" tone holes that improve intonation and sound. Undercutting means chamfering the bottom edge of tone holes inside the bore. Acoustically, this makes the tone hole function as if it were larger, but its main function is to allow the air column to follow the curve up through the tone hole (surface tension) instead of \"blowing past\" it under the increasingly directional frequencies of the upper registers.\n\nThe fixed reed and fairly uniform diameter of the clarinet give the instrument an acoustical behavior approximating that of a cylindrical stopped pipe. Recorders use a tapered internal bore to overblow at the 8th (octave) when its thumb/register hole is pinched open while the clarinet, with its cylindrical bore, overblows on the 12th. Adjusting the angle of the bore taper controls the frequencies of the overblown notes (harmonics). Changing the mouthpiece's tip opening and the length of the reed changes aspects of the harmonic timbre or voice of the instrument because this changes the speed of reed vibrations. Generally, the goal of the clarinetist when producing a sound is to make as much of the reed vibrate as possible, making the sound fuller, warmer, and potentially louder.\n\nThe lip position and pressure, the shaping of the vocal tract, the choice of reed and mouthpiece, the amount of air pressure created, and the evenness of the air flow account for most of the player's ability to control the tone of a clarinet. A highly skilled musician will provide the ideal lip pressure and air pressure for each frequency (note) being produced. They will have an embouchure which places an even pressure across the reed by carefully controlling their lip muscles. The air flow will also be carefully controlled by using the strong stomach muscles (as opposed to the weaker and erratic chest muscles) and they will use the diaphragm to oppose the stomach muscles to achieve a tone softer than a forte, rather than weakening the stomach muscle tension to lower air pressure. Their vocal tract will be shaped to resonate at frequencies associated with the tone being produced.\n\nCovering or uncovering the tone holes varies the length of the pipe, changing the resonant frequencies of the enclosed air column and hence the pitch of the sound. A clarinetist moves between the chalumeau and clarion registers through use of the register key, or speaker key: clarinetists call the change from chalumeau register to clarion register \"the break\". The open register key stops the fundamental frequency from being reinforced and the reed is forced to vibrate at three times the speed it was originally vibrating at. This produces a note a twelfth above the original note. Most instruments overblow at two times the speed of the fundamental frequency (the octave) but as the clarinet acts as a closed pipe system, the reed cannot vibrate at twice the original speed because it would be creating a 'puff' of air at the time the previous 'puff' is returning as a rarefaction. This means that it cannot be reinforced and so would die away.\n\nThe chalumeau register plays fundamentals, whereas the clarion register, aided by the register key, plays third harmonics, a perfect twelfth higher than the fundamentals. The first several notes of the altissimo range, aided by the register key and venting with the first left-hand hole, play fifth harmonics, a major seventeenth (that is a perfect twelfth plus a major sixth) above the fundamental. The clarinet is therefore said to overblow at the twelfth, and when moving to the altissimo register, a seventeenth. By contrast, nearly all other woodwind instruments overblow at the octave, or like the ocarina and tonette, do not overblow at all. A clarinet must have holes and keys for nineteen notes (a chromatic octave and a half, from bottom E to B) in its lowest register to play the chromatic scale. This overblowing behavior explains the clarinet's great range and complex fingering system. The fifth and seventh harmonics are also available, sounding a further sixth and fourth (a flat, diminished fifth) higher respectively; these are the notes of the altissimo register. This is also why the inner \"waist\" measurement is so critical to these harmonic frequencies.\n\nThe highest notes on a clarinet can have a shrill piercing quality and can be difficult to tune accurately. Different instruments often play differently in this respect due to the sensitivity of the bore and reed measurements. Using alternate fingerings and adjusting the embouchure help correct the pitch of these higher notes.\n\nSince approximately 1850, clarinets have been nominally tuned according to twelve-tone equal temperament. Older clarinets were nominally tuned to meantone. A skilled performer can use his or her embouchure to considerably alter the tuning of individual notes or to produce vibrato, a pulsating change of pitch often employed in jazz. Vibrato is rare in classical or concert band literature; however, certain clarinetists, such as Richard Stoltzman, do use vibrato in classical music. Special fingerings may be used to play quarter tones and other microtonal intervals.\nAround 1900, Dr. Richard H. Stein, a Berlin musicologist, made a quarter-tone clarinet, which was soon abandoned. Years later, another German, Fritz Schüller of Markneukirchen, built a quarter tone clarinet, with two parallel bores of slightly different lengths whose tone holes are operated using the same keywork and a valve to switch from one bore to the other.\nClarinet bodies have been made from a variety of materials including wood, plastic, hard rubber, metal, resin, and ivory. The vast majority of clarinets used by professional musicians are made from African hardwood, mpingo (African Blackwood) or grenadilla, rarely (because of diminishing supplies) Honduran rosewood and sometimes even cocobolo. Historically other woods, notably boxwood, were used.\n\nMost modern, inexpensive instruments are made of plastic resin, such as ABS. These materials are sometimes called \"resonite,\" which is Selmer's trademark name for its type of plastic. Metal soprano clarinets were popular in the early 20th century, until plastic instruments supplanted them; metal construction is still used for the bodies of some contra-alto and contrabass clarinets, and for the necks and bells of nearly all alto and larger clarinets. Ivory was used for a few 18th-century clarinets, but it tends to crack and does not keep its shape well.\n\nBuffet Crampon's Greenline clarinets are made from a composite of grenadilla wood powder and carbon fiber. Such instruments are less affected by humidity and temperature changes than wooden instruments but are heavier. Hard rubber, such as ebonite, has been used for clarinets since the 1860s, although few modern clarinets are made of it. Clarinet designers Alastair Hanson and Tom Ridenour are strong advocates of hard rubber. Hanson Clarinets of England manufactures clarinets using a grenadilla compound reinforced with ebonite, known as 'BTR' (bithermal reinforced) grenadilla. This material is also not affected by humidity, and the weight is the same as that of a wooden clarinet.\n\nMouthpieces are generally made of hard rubber, although some inexpensive mouthpieces may be made of plastic. Other materials such as crystal/glass, wood, ivory, and metal have also been used. Ligatures are often made out of metal and plated in nickel, silver, or gold. Other ligature materials include wire, wire mesh, plastic, naugahyde, string, or leather.\n\nThe instrument uses a single reed made from the cane of \"Arundo donax\", a type of grass. Reeds may also be manufactured from synthetic materials. The ligature fastens the reed to the mouthpiece. When air is blown through the opening between the reed and the mouthpiece facing, the reed vibrates and produces the instrument's sound.\n\nBasic reed measurements are as follows: tip, wide; lay, long (distance from the place where the reed touches the mouthpiece to the tip); gap, (distance between the underside of the reed tip and the mouthpiece). Adjustment to these measurements is one method of affecting tone color.\n\nMost clarinetists buy manufactured reeds, although many make adjustments to these reeds and some make their own reeds from cane \"blanks\". Reeds come in varying degrees of hardness, generally indicated on a scale from one (soft) through five (hard). This numbering system is not standardized—reeds with the same hardness number often vary in hardness across manufacturers and models. Reed and mouthpiece characteristics work together to determine ease of playability, pitch stability, and tonal characteristics.\n\nNote: A Boehm system soprano clarinet is shown in the photos illustrating this section. However, all modern clarinets have similar components.\nThe \"reed\" is attached to the \"mouthpiece\" by the \"ligature\", and the top half-inch or so of this assembly is held in the player's mouth. In the past clarinetists used to wrap a string around the mouthpiece and reed instead of using a ligature. The formation of the mouth around the mouthpiece and reed is called the \"embouchure\".\nThe reed is on the underside of the mouthpiece, pressing against the player's lower lip, while the top teeth normally contact the top of the mouthpiece (some players roll the upper lip under the top teeth to form what is called a 'double-lip' embouchure). Adjustments in the strength and shape of the embouchure change the tone and intonation (tuning). It is not uncommon for clarinetists to employ methods to relieve the pressure on the upper teeth and inner lower lip by attaching pads to the top of the mouthpiece or putting (temporary) padding on the front lower teeth, commonly from folded paper.\nNext is the short \"barrel\"; this part of the instrument may be extended to fine-tune the clarinet. As the pitch of the clarinet is fairly temperature-sensitive, some instruments have interchangeable barrels whose lengths vary slightly. Additional compensation for pitch variation and tuning can be made by pulling out the barrel and thus increasing the instrument's length, particularly common in group playing in which clarinets are tuned to other instruments (such as in an orchestra or concert band). Some performers use a plastic barrel with a thumbwheel that adjusts the barrel length. On basset horns and lower clarinets, the barrel is normally replaced by a curved metal neck.\nThe main body of most clarinets is divided into the \"upper joint\", the holes and most keys of which are operated by the left hand, and the \"lower joint\" with holes and most keys operated by the right hand. Some clarinets have a single joint: on some basset horns and larger clarinets the two joints are held together with a screw clamp and are usually not disassembled for storage. The left thumb operates both a \"tone hole\" and the \"register key\". On some models of clarinet, such as many Albert system clarinets and increasingly some higher-end Boehm system clarinets, the register key is a 'wraparound' key, with the key on the back of the clarinet and the pad on the front. Advocates of the wraparound register key say it improves sound, and it is harder for moisture to accumulate in the tube beneath the pad. Nevertheless, there is a consensus among repair techs that this type of register key is harder to keep in adjustment, i.e., it is hard to have enough spring pressure to close the hole securely.\n\nThe body of a modern soprano clarinet is equipped with numerous \"tone holes\" of which seven (six front, one back) are covered with the fingertips, and the rest are opened or closed using a set of keys. These tone holes let the player produce every note of the chromatic scale. On alto and larger clarinets, and a few soprano clarinets, key-covered holes replace some or all finger holes. The most common system of keys was named the Boehm system by its designer Hyacinthe Klosé in honour of flute designer Theobald Boehm, but it is not the same as the Boehm system used on flutes. The other main system of keys is called the Oehler system and is used mostly in Germany and Austria (see History). The related Albert system is used by some jazz, klezmer, and eastern European folk musicians. The Albert and Oehler systems are both based on the early Mueller system.\nThe cluster of keys at the bottom of the upper joint (protruding slightly beyond the cork of the joint) are known as the \"trill keys\" and are operated by the right hand. These give the player alternative fingerings that make it easy to play ornaments and trills. The entire weight of the smaller clarinets is supported by the right thumb behind the lower joint on what is called the \"thumb-rest\". Basset horns and larger clarinets are supported with a neck strap or a floor peg.\nFinally, the flared end is known as the \"bell\". Contrary to popular belief, the bell does not amplify the sound; rather, it improves the uniformity of the instrument's tone for the lowest notes in each register. For the other notes the sound is produced almost entirely at the tone holes and the bell is irrelevant. On basset horns and larger clarinets, the bell curves up and forward and is usually made of metal.\n\nTheobald Boehm did not directly invent the key system of the clarinet. Boehm was a flautist who created the key system that is now used for the transverse flute. Klosé and Buffet applied Boehm's system to the clarinet. Although the credit goes to those people, Boehm's name was given to that key system because it was based on that used for flute.\n\nThe current Boehm key system consists of generally 6 rings, on the thumb, 1st, 2nd, 4th, 5th and 6th holes, a register key just above the thumb hole, easily accessible with the thumb. Above the 1st hole, there is a key that lifts two covers creating the note A in the throat register (high part of low register) of the clarinet. A key at the side of the instrument at the same height as the A key lifts only one of the two covers, producing G a semitone lower. The A key can be used in conjunction solely with the register key to produce A/B.\n\nThe clarinet has its roots in the early single-reed instruments or hornpipes used in Ancient Greece, old Egypt, Middle East, and Europe since the Middle Ages, such as the albogue, alboka, and double clarinet.\n\nThe modern clarinet developed from a Baroque instrument called the chalumeau. This instrument was similar to a recorder, but with a single-reed mouthpiece and a cylindrical bore. Lacking a register key, it was played mainly in its fundamental register, with a limited range of about one and a half octaves. It had eight finger holes, like a recorder, and two keys for its two highest notes. At this time, contrary to modern practice, the reed was placed in contact with the upper lip.\n\nAround the turn of the 18th century, the chalumeau was modified by converting one of its keys into a register key to produce the first clarinet. This development is usually attributed to German instrument maker Johann Christoph Denner, though some have suggested his son Jacob Denner was the inventor. This instrument played well in the middle register with a loud, shrill sound, so it was given the name \"clarinetto\" meaning \"little trumpet\" (from \"clarino\" + \"-etto\"). Early clarinets did not play well in the lower register, so players continued to play the chalumeaux for low notes. As clarinets improved, the chalumeau fell into disuse, and these notes became known as the \"chalumeau register\". Original Denner clarinets had two keys, and could play a chromatic scale, but various makers added more keys to get improved tuning, easier fingerings, and a slightly larger range. The classical clarinet of Mozart's day typically had eight finger holes and five keys.\n\nClarinets were soon accepted into orchestras. Later models had a mellower tone than the originals. Mozart (d. 1791) liked the sound of the clarinet (he considered its tone the closest in quality to the human voice) and wrote numerous pieces for the instrument., and by the time of Beethoven (c. 1800–1820), the clarinet was a standard fixture in the orchestra.\n\nThe next major development in the history of clarinet was the invention of the modern pad. Because early clarinets used felt pads to cover the tone holes, they leaked air. This required pad-covered holes to be kept to a minimum, restricting the number of notes the clarinet could play with good tone. In 1812, Iwan Müller, a Baltic German community-born clarinetist and inventor, developed a new type of pad that was covered in leather or fish bladder. It was airtight and let makers increase the number of pad-covered holes. Müller designed a new type of clarinet with seven finger holes and thirteen keys. This allowed the instrument to play in any key with near-equal ease. Over the course of the 19th-century makers made many enhancements to Mueller's clarinet, such as the Albert system and the Baermann system, all keeping the same basic design. Modern instruments may also have cork or synthetic pads.\n\nThe final development in the modern design of the clarinet used in most of the world today was introduced by Hyacinthe Klosé in 1839. He devised a different arrangement of keys and finger holes, which allow simpler fingering. It was inspired by the Boehm system developed for flutes by Theobald Boehm. Klosé was so impressed by Boehm's invention that he named his own system for clarinets the Boehm system, although it is different from the one used on flutes. This new system was slow to gain popularity but gradually became the standard, and today the Boehm system is used everywhere in the world except Germany and Austria. These countries still use a direct descendant of the Mueller clarinet known as the Oehler system clarinet. Also, some contemporary Dixieland players continue to use Albert system clarinets.\n\nOther key systems have been developed, many built around modifications to the basic Boehm system: Full Boehm; Mazzeo,; McIntyre; NX; and Reform-Boehm. systems, for example. Each of these addressed—and often improved—issues of particular \"weak\" tones, or simplified awkward fingerings, but none has caught on widely among players, and the Boehm system remains the standard, to date.\n\nThe modern orchestral standard of using soprano clarinets in both B and A has to do partly with the history of the instrument, and partly with acoustics, aesthetics, and economics. Before about 1800, due to the lack of airtight pads \"(see History)\", practical woodwinds could have only a few keys to control accidentals (notes outside their diatonic home scales). The low (chalumeau) register of the clarinet spans a twelfth (an octave plus a perfect fifth), so the clarinet needs keys/holes to produce all nineteen notes in that range. This involves more keywork than is necessary on instruments that \"overblow\" at the octave—oboes, flutes, bassoons, and saxophones, for example, which need only twelve notes before overblowing.\n\nClarinets with few keys cannot therefore easily play chromatically, limiting any such instrument to a few closely related key signatures. For example, an eighteenth-century clarinet in C could be played in F, C, and G (and their relative minors) with good intonation, but with progressive difficulty and poorer intonation as the key moved away from this range. In contrast, for octave-overblowing instruments, an instrument in C with few keys could much more readily be played in any key.\n\nThis problem was overcome by using three clarinets—in A, B, and C—so that early 19th-century music, which rarely strayed into the remote keys (five or six sharps or flats), could be played as follows: music in 5 to 2 sharps (B major to D major concert pitch) on A clarinet (D major to F major for the player), music in 1 sharp to 1 flat (G to F) on C clarinet, and music in 2 flats to 4 flats (B to A) on the B clarinet (C to B for the player). Difficult key signatures and numerous accidentals were thus largely avoided.\n\nWith the invention of the airtight pad, and as key technology improved and more keys were added to woodwinds, the need for clarinets in multiple musical keys was reduced. However, the use of multiple instruments in different keys persisted, with the three instruments in C, B, and A all used as specified by the composer.\n\nThe lower-pitched clarinets sound more \"mellow\" (less bright), and the C clarinet—being the highest and therefore brightest of the three—fell out of favour as the other two clarinets could cover its range and their sound was considered better. While the clarinet in C began to fall out of general use around 1850, some composers continued to write C parts after this date, e.g., Bizet's Symphony in C (1855), Tchaikovsky's Symphony No. 2 (1872), Smetana's overture to \"The Bartered Bride\" (1866) and \"Má Vlast\" (1874), Dvořák's \"Slavonic Dance\" Op. 46, No. 1 (1878), Brahms' Symphony No. 4 (1885), Mahler's Symphony No. 6 (1906), and Richard Strauss deliberately reintroduced it to take advantage of its brighter tone, as in \"Der Rosenkavalier\" (1911).\n\nWhile technical improvements and an equal-tempered scale reduced the need for two clarinets, the technical difficulty of playing in remote keys persisted, and the A has thus remained a standard orchestral instrument. In addition, by the late 19th century, the orchestral clarinet repertoire contained so much music for clarinet in A that the disuse of this instrument was not practical. Attempts were made to standardise to the B instrument between 1930 and 1950 (e.g., tutors recommended learning the routine transposition of orchestral A parts on the B clarinet, including solos written for A clarinet, and some manufacturers provided a low E on the B to match the range of the A), but this failed in the orchestral sphere.\n\nSimilarly there have been E and D instruments in the upper soprano range, B, A, and C instruments in the bass range, and so forth; but over time the E and B instruments have become predominant. The B instrument remains dominant in concert bands and in jazz. Both B and C instruments are used in some ethnic traditions, such as klezmer music.\n\nIn classical music, clarinets are part of standard orchestral and concert band instrumentation.\n\nThe orchestra frequently includes two clarinetists playing individual parts—each player is usually equipped with a pair of standard clarinets in B and A, and clarinet parts commonly alternate between B and A instruments several times over the course of a piece, or less commonly, a movement (e.g., 1st movement Brahms' 3rd symphony). Clarinet sections grew larger during the last few decades of the 19th century, often employing a third clarinetist, an E or a bass clarinet. In the 20th century, composers such as Igor Stravinsky, Richard Strauss, Gustav Mahler, and Olivier Messiaen enlarged the clarinet section on occasion to up to nine players, employing many different clarinets including the E or D soprano clarinets, basset horn, alto clarinet, bass clarinet, and/or contrabass clarinet.\n\nIn concert bands, clarinets are an important part of the instrumentation. The E clarinet, B clarinet, alto clarinet, bass clarinet, and contra-alto/contrabass clarinet are commonly used in concert bands. Concert bands generally have multiple B clarinets; there are commonly 3 B clarinet parts with 2–3 players per part. There is generally only one player per part on the other clarinets. There are not always E clarinet, alto clarinet, and contra-alto clarinets/contrabass clarinet parts in concert band music, but all three are quite common.\n\nThis practice of using a variety of clarinets to achieve coloristic variety was common in 20th-century classical music and continues today. However, many clarinetists and conductors prefer to play parts originally written for obscure instruments on B or E clarinets, which are often of better quality and more prevalent and accessible.\n\nThe clarinet is widely used as a solo instrument. The relatively late evolution of the clarinet (when compared to other orchestral woodwinds) has left solo repertoire from the Classical period and later, but few works from the Baroque era. Many clarinet concertos have been written to showcase the instrument, with the concerti by Mozart, Copland, and Weber being well known.\n\nMany works of chamber music have also been written for the clarinet. Common combinations are:\n\nThe clarinet was originally a central instrument in jazz, beginning with the New Orleans players in the 1910s. It remained a signature instrument of jazz music through much of the big band era into the 1940s. American players Alphonse Picou, Larry Shields, Jimmie Noone, Johnny Dodds, and Sidney Bechet were all pioneers of the instrument in jazz. The B soprano was the most common instrument, but a few early jazz musicians such as Louis Nelson Delisle and Alcide Nunez preferred the C soprano, and many New Orleans jazz brass bands have used E soprano.\n\nSwing clarinetists such as Benny Goodman, Artie Shaw, and Woody Herman led successful big bands and smaller groups from the 1930s onward. Duke Ellington, active from the 1920s to the 1970s, used the clarinet as lead instrument in his works, with several players of the instrument (Barney Bigard, Jimmy Hamilton, and Russell Procope) spending a significant portion of their careers in his orchestra. Harry Carney, primarily Ellington's baritone saxophonist, occasionally doubled on bass clarinet. Meanwhile, Pee Wee Russell had a long and successful career in small groups.\n\nWith the decline of the big bands' popularity in the late 1940s, the clarinet faded from its prominent position in jazz. By that time, an interest in Dixieland or traditional New Orleans jazz had revived; Pete Fountain was one of the best known performers in this genre. Bob Wilber, active since the 1950s, is a more eclectic jazz clarinetist, playing in several classic jazz styles. During the 1950s and 1960s, Britain underwent a surge in the popularity of what was termed 'Trad jazz'. In 1956 the British clarinetist Acker Bilk founded his own ensemble. Several singles recorded by Bilk reached the British pop charts, including the ballad \"Stranger on the Shore\".\n\nThe clarinet's place in the jazz ensemble was usurped by the saxophone, which projects a more powerful sound and uses a less complicated fingering system. The requirement for an increased speed of execution in modern jazz also did not favour the clarinet, but the clarinet did not entirely disappear. A few players such as Buddy DeFranco, Tony Scott, and Jimmy Giuffre emerged during the 1950s playing bebop or other styles. A little later, Eric Dolphy (on bass clarinet), Perry Robinson, John Carter, Theo Jörgensmann, and others used the clarinet in free jazz. The French composer and clarinetist Jean-Christian Michel initiated a jazz-classical cross-over on the clarinet with the drummer Kenny Clarke.\n\nIn the U.S., the prominent players on the instrument since the 1980s have included Eddie Daniels, Don Byron, Marty Ehrlich, and others playing the clarinet in more contemporary contexts.\n\nThe clarinet is uncommon, but not unheard of, in rock music. Jerry Martini played clarinet on Sly and the Family Stone's 1968 hit, \"Dance to the Music\"; Don Byron, a founder of the Black Rock Coalition who was a member of hard rock guitarist Vernon Reid's band, plays clarinet on the \"Mistaken Identity\" album (1996). The Beatles, Pink Floyd, Radiohead, Aerosmith, Billy Joel, and Tom Waits have also all used clarinet on occasion.\n\nClarinets feature prominently in klezmer music, which entails a distinctive style of playing. The use of quarter-tones requires a different embouchure. Some klezmer musicians prefer Albert system clarinets.\n\nThe popular Brazilian music styles of choro and samba use the clarinet. Prominent contemporary players include Paulo Moura, Naylor 'Proveta' Azevedo, Paulo Sérgio dos Santos and Cuban born Paquito D'Rivera.\n\nEven though it has been adopted recently in Albanian folklore (around the 18th century), the clarinet, or \"gërneta\" as it is called, is one of the most important instruments in Albania, especially in the central and southern areas. The clarinet plays a crucial role in \"saze\" (folk) ensembles that perform in weddings and other celebrations. It is worth mentioning that the \"kaba\" (an instrumental Albanian Isopolyphony included in UNESCO's intangible cultural heritage list) is characteristic of these ensembles. Prominent Albanian clarinet players include Selim Leskoviku, Gaqo Lena, Remzi Lela (Çobani), Laver Bariu (Ustai), and Nevruz Nure (Lulushi i Korçës).\n\nThe clarinet is prominent in Bulgarian wedding music also; it is an offshoot of Roma/Romani traditional music. Ivo Papazov is a well-known clarinetist in this genre. In Moravian dulcimer bands, the clarinet is usually the only wind instrument among string instruments.\n\nIn old-town folk music in the Republic of Macedonia (called čalgija (\"чалгија\")), the clarinet has the most important role in wedding music; clarinet solos mark the high point of dancing euphoria. One of the most renowned Macedonian clarinet players is Tale Ognenovski, who gained worldwide fame for his virtuosity.\n\nIn Greece, the clarinet (usually referred to as \"κλαρίνο\"—\"clarino\") is prominent in traditional music, especially in central, northwest and northern Greece (Thessaly, Epirus and Macedonia). The double-reed zurna was the dominant woodwind instrument before the clarinet arrived in the country, although many Greeks regard the clarinet as a native instrument. Traditional dance music, wedding music and laments include a clarinet soloist and quite often improvisations. Petroloukas Chalkias is a famous clarinetist in this genre.\n\nThe instrument is equally famous in Turkey, especially the lower-pitched clarinet in G. The western European clarinet crossed via Turkey to Arabic music, where it is widely used in Arabic pop, especially if the intention of the arranger is to imitate the Turkish style.\n\nAlso in Turkish folk music, a clarinet-like woodwind instrument, the sipsi, is used. However, it is far more rare than the soprano clarinet and is mainly limited to folk music of the Aegean Region.\n\nGroups of clarinets playing together have become increasingly popular among clarinet enthusiasts in recent years. Common forms are:\n\nClarinet choirs and quartets often play arrangements of both classical and popular music, in addition to a body of literature specially written for a combination of clarinets by composers such as Arnold Cooke, Alfred Uhl, Lucien Caillet and Václav Nelhýbel.\n\nThere is a family of many differently pitched clarinet types, some of which are very rare. The following are the most important sizes, from highest to lowest:\n\nEEE and BBB octocontra-alto and octocontrabass clarinets have also been built. There have also been soprano clarinets in C, A, and B with curved barrels and bells marketed under the names saxonette, claribel, and clariphon.\n\n\n\n\n"}
{"id": "6434", "url": "https://en.wikipedia.org/wiki?curid=6434", "title": "Chojnów", "text": "Chojnów\n\nChojnów () is a small town in Legnica County, Lower Silesian Voivodeship, in south-western Poland. It is located on the Skora river, a tributary of the Kaczawa at an average altitude of above sea level. Chojnów is the administrative seat of the rural gmina called Gmina Chojnów, although the town is not part of its territory and forms a separate urban gmina. it had 14,389 inhabitants.\n\nChojnów is located west of Legnica, east from Bolesławiec and north of Złotoryja, from the A4 motorway. It has railroad connections to Bolesławiec and Legnica.\n\nCoat of arms of the Chojnów has is a blue escutcheon. On the dial there is a tower with three bastions of white colour. The central tower has two Windows, and one side. On the towers is located on the right side of the Moon and Sun on the left. In the gate of the Silesian Eagle on a yellow background.\nThe Motto of Chojnów is \"Friendly City\".\n\nChojnów is located in the Central-Western part of the Lower Silesia region. The Skora (Leather) River flows through the town in a westerly direction. The city of Chojnów is in area, including 41% agricultural land.\n\nChojnów has a connection with the major cities of the country (road and rail) and located south of Chojnów has the A4 Autostrada. To the South of the town is the surrounding Chojnowska Plain.\n\nThe town is first mentioned in a Latin mediaeval document issued in Wrocław on February 26, 1253, stating, the Silesian Duke Henry III when the town is mentioned under the name Honowo. Possible the name of nearby Hainan Island. The name in German is still of Haynan.\n\nThe settlement of \"Haynow\" was mentioned in a 1272 deed. It was already called a \"civitas\" in a 1288 document issued by the Piast duke Henry V of Legnica, and officially received town privileges in 1333. In 1288, the city is known from documents of the Prince of Legnica Henryk V colon. \n\nThe town survived the Hussites, who burned almost the entire town Center and castle, but it quickly helped recover its former glory. The largest boom Chojnów experienced was in the 16th century, however by the end of that century began to decline due to fires and epidemic, which claimed many victims in 1613AD.\n\nFrom 1618 to 1648 (during the Thirty Years War), there was another outbreak in the city and in 1740 Chojnów was conqored by the Prussians.\n\nDuring the Napoleonic wars there were more epidemics. A railway line was connected in the 19th century. Sewer, Gas lighting a Newspaper and a hospital soon followed as the towns economy improved.\n\nThe city was not spared in World War II, with 60% of the town being destroyed on February 10, 1945 when Soviet Red Army troops took the town. After World War II and the implementation of the Oder-Neisse line in 1945, the town passed to the Republic of Poland. The German population was expelled from the region.\n\nIn March 31, 2011, the city had 14367 inhabitants.\n\n\nChojnów is an industrial and agricultural town. Among local products are: paper, agricultural machinery, chains, metal furniture for hospitals, equipment for the meat industry, beer, wine, leather clothing, and clothing for infants, children and adults. The local government-run weekly newspaper is Gazeta Chojnowska, which has been published since 1992.\n\nAmong the interesting monuments of Chojnów are the 13th-century castle of the Dukes of Legnica (currently used as a museum), two old churches, the \"Baszta Tkaczy\" (\"Weavers' Tower\") and preserved fragments of city walls.\n\nThe biggest green area in Chojnów is small forest \"Park Piastowski\" (\"Piast's Park\"), named after Piast dynasty. Wild animals that can be found in the Chojnów area are roe-deer (\"sarna\", Capreolus capraea ?), foxes, rabbits and wild domestic animals, especially cats.\n\nEvery year in the first days of June, the \"Days of Chojnów\" (\"Dni Chojnowa\") are celebrated. The Whole-Poland bike race \"Masters\" has been organized yearly in Chojnów for the past few years.\n\nChojnów has a Municipal sports and recreation center formed in 2008 holding various events, festivals, reviews, exhibitions, competitions, \nThe regional Museum housed in the old Piast era castle. The collections include tiles, relics, and the castle garden.\nNext to the Museum there is a municipal library.\nIn śródmiejskim Park, near the Town Hall is the amphitheatre.\n\n\nIn Chojnów, there are two kindergartens, two elementary schools and two middle schools.\n\nChojnów is in the Catholic deanery of Chojnów and has two parishes, Immaculate conception of the Blessed Virgin Mary and also the Holy Apostles Peter and Paul. Both parishes have active congregations.\nThere are also two Congregations of Jehovah's witnesses.\n\n\nChojnów is twinned with:\n\n"}
{"id": "6435", "url": "https://en.wikipedia.org/wiki?curid=6435", "title": "Canes Venatici", "text": "Canes Venatici\n\nCanes Venatici is one of the 88 official modern constellations. It is a small northern constellation that was created by Johannes Hevelius in the 17th century. Its name is Latin for \"hunting dogs\", and the constellation is often depicted in illustrations as representing the dogs of Boötes the Herdsman, a neighboring constellation. Cor Caroli is the constellation's brightest star, with an apparent magnitude of 2.9. La Superba is one of the reddest stars in the sky and one of the brightest carbon stars. The Whirlpool Galaxy is a spiral galaxy tilted face-on to observers on Earth, and was the first galaxy whose spiral nature was discerned.\n\nThe stars of Canes Venatici are not bright. In classical times, they were listed by Ptolemy as unfigured stars below the constellation Ursa Major in his star catalogue. \n\nIn medieval times, the identification of these stars with the dogs of Boötes arose through a mistranslation. Some of Boötes's stars were traditionally described as representing the club (Greek, Κολλοροβος) of Boötes. When the Greek astronomer Ptolemy's \"Almagest\" was translated from Greek to Arabic, the translator Hunayn ibn Ishaq did not know the Greek word and rendered it as the nearest-looking Arabic word, writing العصا ذات الكلاب in ordinary unvowelled Arabic text \"\"al-`aşā dhāt al-kullāb\"\", which means \"the spearshaft having a hook\". When the Arabic text was later translated into Latin, the translator Gerard of Cremona mistook the Arabic word كلاب for \"kilāb\" (the plural of كلب \"kalb\"), meaning \"dogs\", writing \"hastile habens canes\" (\"spearshaft with dogs\").\nIn 1533, the German astronomer Peter Apian depicted Boötes as having two dogs with him.\n\nThese spurious dogs floated about the astronomical literature until Hevelius decided to specify their presence in the sky by making them a separate constellation in 1687. Hevelius chose the name \"Asterion\" (from the Greek 'αστέριον, meaning the \"little star\", the diminutive of 'αστηρ the \"star\", or adjective meaning \"starry\") for the northern dog and \"Chara\" (from the Greek χαρά, meaning \"joy\") for the southern dog, as \"Canes Venatici\", the Hunting Dogs, in his star atlas.\nIn his star catalogue, the Czech astronomer Bečvář assigned the names \"Asterion\" to β CVn and \"Chara\" to α CVn.\n\nCanes Venatici is bordered by Ursa Major to the north and west, Coma Berenices to the south, and Boötes to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'CVn'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of 14 sides. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between +27.84° and +52.36°. Covering 465 square degrees, it ranks 38th of the 88 constellations in size.\n\nCanes Venatici contains no bright stars, Alpha and Beta Canum Venaticorum being only of 3rd and 4th magnitude respectively. Flamsteed catalogued 25 stars in the constellation, labelling them 1 to 25 Canum Venaticorum, however 1 turned out to be in Ursa Major, 13 was in Coma Berenices and 22 did not exist.\n\n\nR Canum Venaticorum is a Mira variable that ranges between magnitudes 6.5 and 12.9 over a period of approximately 329 days.\n\nThe Giant Void, an extremely large void (part of the universe containing very few galaxies) is within the vicinity of this constellation. It may be possibly the largest void ever discovered, slightly larger than the Eridanus Supervoid and 1,200 times the volume of expected typical voids. It was discovered in 1988 in a deep-sky survey.\n\nCanes Venatici contains five Messier objects, including four galaxies. One of the more significant galaxies in Canes Venatici is the Whirlpool Galaxy (M51, NGC 5194) and NGC 5195, a small barred spiral galaxy that is seen face on. This was the first galaxy recognised as having a spiral structure, this structure being first observed by Lord Rosse in 1845. It is a face-on spiral galaxy 37 million light-years from Earth. Widely considered to be one of the most beautiful galaxies visible, M51 has many star-forming regions and nebulae in its arms, coloring them pink and blue in contrast to the older yellow core. M51 has a smaller companion, NGC 5195, that has very few star-forming regions and thus appears yellow. It is passing behind M51 and may be the cause of the larger galaxy's prodigious star formation.\n\nOther notable spiral galaxies in Canes Venatici are the Sunflower Galaxy (M63, NGC 5055), M94 (NGC 4736), and M106 (NGC 4258). M63, the Sunflower Galaxy, was named for its appearance in large amateur telescopes. It is a spiral galaxy with an integrated magnitude of 9.0. M94 is a small face-on spiral galaxy with an approximate magnitude of 8.0, about 15 million light-years from Earth. NGC 4631 is a barred spiral galaxy, which is one of the largest and brightest edge-on galaxies in the sky.\n\nM3 (NGC 5272) is a globular cluster 32,000 light-years from Earth. It is 18' in diameter, and at magnitude 6.3 is bright enough to be seen with binoculars. It can even be seen with the naked eye under particularly dark skies.\n\nM94, also classified as NGC 4736, is a face-on spiral galaxy 15 million light-years from Earth. It has very tight spiral arms and a bright core. The outskirts of the galaxy are incredibly luminous in the ultraviolet because of a ring of new stars surrounding the core, 7,000 light-years in diameter. Though astronomers are not sure what has caused this ring of new stars, some hypothesize that it is from shock waves caused by a bar that is thus far invisible.\n\n\n"}
{"id": "6436", "url": "https://en.wikipedia.org/wiki?curid=6436", "title": "Chamaeleon", "text": "Chamaeleon\n\nChamaeleon () is a small constellation in the southern sky. It is named after the chameleon, a kind of lizard. It was first defined in the 16th century.\n\nChamaeleon was one of twelve constellations created by Petrus Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman. It first appeared on a 35-cm diameter celestial globe published in 1597 (or 1598) in Amsterdam by Plancius and Jodocus Hondius. Johann Bayer was the first uranographer to put Chamaeleon in a celestial atlas. It was one of many constellations created by European explorers in the 15th and 16th centuries out of unfamiliar Southern Hemisphere stars.\n\nThere are four bright stars in Chamaeleon. Alpha Chamaeleontis is a white-hued star of magnitude 4.1, 63 light-years from Earth. Beta Chamaeleontis is a blue-white hued star of magnitude 4.2, 27 light-years from Earth. Gamma Chamaeleontis is a red-hued giant star of magnitude 4.1, 413 light-years from Earth. The other bright star in Chamaeleon is Delta Chamaeleontis, a wide double star. The brighter star is Delta Chamaeleontis, a blue-hued star of magnitude 4.4, 364 light-years from Earth. Delta Chamaeleontis, the dimmer component, is an orange-hued giant star of magnitude 5.5, 354 light-years away.\n\nChamaeleon is also the location of Cha 110913, a unique dwarf star or proto solar system.\n\nIn 1999, a nearby open cluster was discovered centered on the star η Chamaeleontis. The cluster, known as either\nthe Eta Chamaeleontis cluster or Mamajek 1, is 8 million years old, and lies 316 light years from Earth.\n\nThe constellation contains a number of molecular clouds (the Chamaeleon dark clouds) that are forming low-mass T Tauri stars. The cloud complex lies some 400 to 600 light years from Earth, and contains tens of thousands of solar masses of gas and dust. The most prominent cluster of T Tauri stars and young B-type stars are in the Chamaeleon I cloud, and are associated with the reflection nebula IC 2631.\n\nChamaeleon contains one planetary nebula, NGC 3195, which is fairly faint. It appears in a telescope at about the same apparent size as Jupiter.\n\nIn Chinese astronomy, the stars that form Chamaeleon were classified as the Little Dipper (小斗, \"Xiǎodǒu\") among the Southern Asterisms (近南極星區, \"Jìnnánjíxīngōu\") by Xu Guangqi.\n\n\n\n"}
{"id": "6437", "url": "https://en.wikipedia.org/wiki?curid=6437", "title": "Cholesterol", "text": "Cholesterol\n\nCholesterol, from the Ancient Greek \"chole-\" (bile) and \"stereos\" (solid) followed by the chemical suffix \"-ol\" for an alcohol, is an organic molecule. It is a sterol (or modified steroid), a type of lipid molecule, and is biosynthesized by all animal cells, because it is an essential structural component of all animal cell membranes; essential to maintain both membrane structural integrity and fluidity. Cholesterol enables animal cells to dispense with a cell wall (to protect membrane integrity and cell viability), thereby allowing animal cells to change shape rapidly and animals to move (unlike bacteria and plant cells, which are restricted by their cell walls).\n\nIn addition to its importance for animal cell structure, cholesterol also serves as a precursor for the biosynthesis of steroid hormones, bile acid, and vitamin D. Cholesterol is the principal sterol synthesized by all animals. In vertebrates, hepatic cells typically produce the greatest amounts. It is absent among prokaryotes (bacteria and archaea), although there are some exceptions, such as \"Mycoplasma\", which require cholesterol for growth.\n\nFrançois Poulletier de la Salle first identified cholesterol in solid form in gallstones in 1769. However, it was not until 1815 that chemist Michel Eugène Chevreul named the compound \"cholesterine\".\n\nSince cholesterol is essential for all animal life, each cell is capable of synthesizing it by way of a complex 37-step process, beginning with the mevalonate pathway and ending with a 19-step conversion of lanosterol to cholesterol. Furthermore, it can be absorbed directly from animal-based foods.\n\nA human male weighing 68 kg (150 lb) normally synthesizes about 1 gram (1,000 mg) per day, and his body contains about 35 g, mostly contained within the cell membranes. Typical daily cholesterol dietary intake for a man in the United States is 307 mg (above the upper limit recommended by the Dietary Guidelines Advisory Committee).\n\nMost ingested cholesterol is esterified, and esterified cholesterol is poorly absorbed. The body also compensates for any absorption of additional cholesterol by reducing cholesterol synthesis. For these reasons, cholesterol in food, seven to ten hours after ingestion, has little, if any effect on concentrations of cholesterol in the blood. However, during the first seven hours after ingestion of cholesterol, as absorbed fats are being distributed around the body within extracellular water by the various lipoproteins (which transport all fats in the water outside cells), the concentrations increase. It is also important to recognize, however, that the concentrations measured in the samples of blood plasma vary with the measurement methods used. Traditional, cheaper methods do not reflect (a) which lipoproteins are transporting the various fat molecules, nor (b) which cells are ingesting, burning or exporting the fat molecules being measured as totals from samples of blood plasma.\n\nCholesterol is recycled in the body. The liver excretes it in a non-esterified form (via bile) into the digestive tract. Typically, about 50% of the excreted cholesterol is reabsorbed by the small intestine back into the bloodstream.\n\nPlants make cholesterol in very small amounts. Plants manufacture phytosterols (substances chemically similar to cholesterol), which can compete with cholesterol for reabsorption in the intestinal tract, thus potentially reducing cholesterol reabsorption. When intestinal lining cells absorb phytosterols, in place of cholesterol, they usually excrete the phytosterol molecules back into the GI tract, an important protective mechanism.\n\nCholesterol, given that it composes about 30% of all animal cell membranes, is required to build and maintain membranes and modulates membrane fluidity over the range of physiological temperatures. The hydroxyl group on cholesterol interacts with the polar heads of the membrane phospholipids and sphingolipids, while the bulky steroid and the hydrocarbon chain are embedded in the membrane, alongside the nonpolar fatty-acid chain of the other lipids. Through the interaction with the phospholipid fatty-acid chains, cholesterol increases membrane packing, which both alters membrane fluidity and maintains membrane integrity so that animal cells do not need to build cell walls (like plants and most bacteria). The membrane remains stable and durable without being rigid, allowing animal cells to change shape and animals to move.\n\nThe structure of the tetracyclic ring of cholesterol contributes to the fluidity of the cell membrane, as the molecule is in a \"trans\" conformation making all but the side chain of cholesterol rigid and planar. In this structural role, cholesterol also reduces the permeability of the plasma membrane to neutral solutes, hydrogen ions, and sodium ions.\n\nWithin the cell membrane, cholesterol also functions in intracellular transport, cell signaling and nerve conduction. Cholesterol is essential for the structure and function of invaginated caveolae and clathrin-coated pits, including caveola-dependent and clathrin-dependent endocytosis. The role of cholesterol in endocytosis of these types can be investigated by using methyl beta cyclodextrin (MβCD) to remove cholesterol from the plasma membrane. Recent studies show that cholesterol is also implicated in cell signaling processes, assisting in the formation of lipid rafts in the plasma membrane, which brings receptor proteins in close proximity with high concentrations of second messenger molecules. In multiple layers, cholesterol and phospholipids, both electrical insulators, can facilitate speed of transmission of electrical impulses along nerve tissue. For many neuron fibers, a myelin sheath, rich in cholesterol since it is derived from compacted layers of Schwann cell membrane, provides insulation for more efficient conduction of impulses. Demyelination (loss of some of these Schwann cells) is believed to be part of the basis for multiple sclerosis.\n\nWithin cells, cholesterol is also a precursor molecule for several biochemical pathways. For example, it is the precursor molecule for the synthesis of vitamin D and all steroid hormones, including the adrenal gland hormones cortisol and aldosterone, as well as the sex hormones progesterone, estrogens, and testosterone, and their derivatives.\n\nThe liver excretes cholesterol into biliary fluids, which is then stored in the gallbladder. Bile contains bile salts, which solubilize fats in the digestive tract and aid in the intestinal absorption of fat molecules as well as the fat-soluble vitamins, A, D, E, and K.\n\nAll animal cells manufacture cholesterol, for both membrane structure and other uses, with relative production rates varying by cell type and organ function. About 20% of total daily cholesterol production occurs in the liver; other sites of higher synthesis rates include the intestines, adrenal glands, and reproductive organs.\n\nSynthesis within the body starts with the mevalonate pathway where two molecules of acetyl CoA condense to form acetoacetyl-CoA. This is followed by a second condensation between acetyl CoA and acetoacetyl-CoA to form 3-hydroxy-3-methylglutaryl CoA (HMG-CoA). \nThis molecule is then reduced to mevalonate by the enzyme HMG-CoA reductase. Production of mevalonate is the rate-limiting and irreversible step in cholesterol synthesis and is the site of action for statins (a class of cholesterol lowering drugs).\nMevalonate is finally converted to isopentenyl pyrophosphate (IPP) through two phosphorylation steps and one decarboxylation step that requires ATP.\n\nThree molecules of isopentenyl pyrophosphate condense to form farnesyl pyrophosphate through the action of geranyl transferase.\n\nTwo molecules of farnesyl pyrophosphate then condense to form squalene by the action of squalene synthase in the endoplasmic reticulum. \n\nOxidosqualene cyclase then cyclizes squalene to form lanosterol. Finally, lanosterol is converted to cholesterol through a 19-step process.\n\nThe final 19 steps to cholesterol contain NADPH and Oxygen to help oxidize methyl groups for removal of carbons, mutases to move alkene groups, and NADH to help reduce ketones.\nKonrad Bloch and Feodor Lynen shared the Nobel Prize in Physiology or Medicine in 1964 for their discoveries concerning some of the mechanisms and methods of regulation of cholesterol and fatty acid metabolism, though lots of complexities remain to be sorted out.\n\nBiosynthesis of cholesterol is directly regulated by the cholesterol levels present, though the homeostatic mechanisms involved are only partly understood. A higher intake from food leads to a net decrease in endogenous production, whereas lower intake from food has the opposite effect. The main regulatory mechanism is the sensing of intracellular cholesterol in the endoplasmic reticulum by the protein SREBP (sterol regulatory element-binding protein 1 and 2). In the presence of cholesterol, SREBP is bound to two other proteins: SCAP (SREBP cleavage-activating protein) and INSIG-1. When cholesterol levels fall, INSIG-1 dissociates from the SREBP-SCAP complex, which allows the complex to migrate to the Golgi apparatus. Here SREBP is cleaved by S1P and S2P (site-1 protease and site-2 protease), two enzymes that are activated by SCAP when cholesterol levels are low.\n\nThe cleaved SREBP then migrates to the nucleus, and acts as a transcription factor to bind to the sterol regulatory element (SRE), which stimulates the transcription of many genes. Among these are the low-density lipoprotein (LDL) receptor and HMG-CoA reductase. The LDL receptor scavenges circulating LDL from the bloodstream, whereas HMG-CoA reductase leads to an increase of endogenous production of cholesterol. A large part of this signaling pathway was clarified by Dr. Michael S. Brown and Dr. Joseph L. Goldstein in the 1970s. In 1985, they received the Nobel Prize in Physiology or Medicine for their work. Their subsequent work shows how the SREBP pathway regulates expression of many genes that control lipid formation and metabolism and body fuel allocation.\n\nCholesterol synthesis can also be turned off when cholesterol levels are high. HMG-CoA reductase contains both a cytosolic domain (responsible for its catalytic function) and a membrane domain. The membrane domain senses signals for its degradation. Increasing concentrations of cholesterol (and other sterols) cause a change in this domain's oligomerization state, which makes it more susceptible to destruction by the proteosome. This enzyme's activity can also be reduced by phosphorylation by an AMP-activated protein kinase. Because this kinase is activated by AMP, which is produced when ATP is hydrolyzed, it follows that cholesterol synthesis is halted when ATP levels are low.\n\nAnimal fats are complex mixtures of triglycerides (stored energy, see: ), with lesser amounts of both the phospholipids and cholesterol molecules from which all animal (and human) cell membranes are constructed. Since all animal cells manufacture cholesterol, all animal-based foods contain cholesterol in varying amounts. Major dietary sources of cholesterol include cheese, egg yolks, beef, pork, poultry, fish, and shrimp. Human breast milk also contains significant quantities of cholesterol.\n\nFrom a dietary perspective, plant cells do not manufacture cholesterol, and it is not found in plant foods. Some plant foods, such as avocado, flax seeds and peanuts, contain phytosterols, which compete with cholesterol for absorption in the intestines, reducing the absorption of both dietary and bile cholesterol. However, a typical diet contributes on the order of 0.2 grams of phytosterols, which is not enough to have a significant impact on blocking cholesterol absorption. Phytosterols intake can be supplemented through the use of phytosterol-containing functional foods or dietary supplements that are recognized as having potential to reduce levels of LDL-cholesterol. Some supplemental guidelines have recommended doses of phytosterols in the 1.6-3.0 grams per day range (Health Canada, EFSA, ATP III, FDA). A recent meta-analysis demonstrating a 12% reduction in LDL-cholesterol at a mean dose of 2.1 grams per day. However, the benefits of a diet supplemented with phytosterols have been questioned.\n\nIn 2016, the United States Department of Agriculture Dietary Guidelines Advisory Committee recommended that Americans eat as little dietary cholesterol as possible. Increased dietary intake of industrial trans fats is associated with an increased risk in all-cause mortality and cardiovascular diseases. Trans fats have been shown to reduce levels of HDL while increasing levels of LDL. Based on such evidence and evidence implicating low HDL and high LDL levels in cardiovascular disease (see Hypercholesterolemia), many health authorities advocate reducing LDL-cholesterol through changes in diet in addition to other lifestyle modifications. Rats subjected to high-fat or fructose diets became dyslipidemic. However, well designed, adequately powered randomized controlled trials investigating patient-relevant outcomes of low-fat diets for otherwise healthy people with hypercholesterolaemia are lacking. Moreover, for familial hypercholesterolaemia, large, parallel, randomized controlled trials are still needed to investigate the effectiveness of a cholesterol-lowering diet and the addition of omega-3 fatty acids, soya protein, plant sterols or stanols.\n\nAs an isolated molecule, cholesterol is only minimally soluble in water; it dissolves into the (water-based) bloodstream only at exceedingly small concentrations. Instead, cholesterol is transported within lipoproteins, complex discoidal particles with exterior amphiphilic proteins and lipids, whose outward-facing surfaces are water-soluble and inward-facing surfaces are lipid-soluble; i.e. transport via emulsification. Triglycerides and cholesterol esters are carried internally. Phospholipids and cholesterol, being amphipathic, are transported in the monolayer surface of the lipoprotein particle.\n\nThere are several types of lipoproteins in the blood. In order of increasing density, they are chylomicrons, very-low-density lipoprotein (VLDL), intermediate-density lipoprotein (IDL), low-density lipoprotein (LDL), and high-density lipoprotein (HDL). Lower protein/lipid ratios make for less dense lipoproteins. Cholesterol within different lipoproteins is identical, although some is carried as its native \"free\" alcohol form (the cholesterol-OH group facing the water surrounding the particles), while others as fatty acyl esters, known also as cholesterol esters, within the particles.\n\nLipoprotein particles are organized by complex apolipoproteins, typically 80-100 different proteins per particle, which can be recognized and bound by specific receptors on cell membranes, directing their lipid payload into specific cells and tissues currently ingesting these fat transport particles. Lipoprotein particles thus include a molecular addresses which play key roles in distribution and delivery of fats around the body in the water outside cells.\n\nChylomicrons, the least dense cholesterol transport molecules, contain apolipoprotein B-48, apolipoprotein C, and apolipoprotein E (the principal cholesterol carrier in the brain) in their shells. Chylomicrons carry fats from the intestine to muscle and other tissues in need of fatty acids for energy or fat production. Unused cholesterol remains in more cholesterol-rich chylomicron remnants, and taken up from here to the bloodstream by the liver.\n\nVLDL molecules are produced by the liver from triacylglycerol and cholesterol which was not used in the synthesis of bile acids. These molecules contain apolipoprotein B100 and apolipoprotein E in their shells, and are degraded by lipoprotein lipase on the blood vessel wall to IDL.\n\nBlood vessels cleave and absorb triacylglycerol from IDL molecules, increasing the concentration of cholesterol. IDL molecules are then consumed in two processes: half is metabolized by HTGL and taken up by the LDL receptor on the liver cell surfaces, while the other half continues to lose triacylglycerols in the bloodstream until they become LDL molecules, with the highest concentration of cholesterol within them.\n\nLDL particles are the major blood cholesterol carriers. Each one contains approximately 1,500 molecules of cholesterol ester. LDL molecule shells contain just one molecule of apolipoprotein B100, recognized by LDL receptors in peripheral tissues. Upon binding of apolipoprotein B100, many LDL receptors concentrate in clathrin-coated pits. Both LDL and its receptor form vesicles within a cell via endocytosis. These vesicles then fuse with a lysosome, where the lysosomal acid lipase enzyme hydrolyzes the cholesterol esters. The cholesterol can then be used for membrane biosynthesis or esterified and stored within the cell, so as to not interfere with the cell membranes.\n\nLDL receptors are used up during cholesterol absorption, and its synthesis is regulated by SREBP, the same protein that controls the synthesis of cholesterol \"de novo\", according to its presence inside the cell. A cell with abundant cholesterol will have its LDL receptor synthesis blocked, to prevent new cholesterol in LDL molecules from being taken up. Conversely, LDL receptor synthesis proceeds when a cell is deficient in cholesterol.\n\nWhen this process becomes unregulated, LDL molecules without receptors begin to appear in the blood. These LDL molecules are oxidized and taken up by macrophages, which become engorged and form foam cells. These foam cells often become trapped in the walls of blood vessels and contribute to atherosclerotic plaque formation. Differences in cholesterol homeostasis affect the development of early atherosclerosis (carotid intima-media thickness). These plaques are the main causes of heart attacks, strokes, and other serious medical problems, leading to the association of so-called LDL cholesterol (actually a lipoprotein) with \"bad\" cholesterol.\n\nHDL particles are thought to transport cholesterol back to the liver, either for excretion or for other tissues that synthesize hormones, in a process known as reverse cholesterol transport (RCT). Large numbers of HDL particles correlates with better health outcomes., whereas low numbers of HDL particles is associated with atheromatous disease progression in the arteries.\n\nCholesterol is susceptible to oxidation and easily forms oxygenated derivatives known as oxysterols. Three different mechanisms can form these: autoxidation, secondary oxidation to lipid peroxidation, and cholesterol-metabolizing enzyme oxidation. A great interest in oxysterols arose when they were shown to exert inhibitory actions on cholesterol biosynthesis. This finding became known as the “oxysterol hypothesis”. Additional roles for oxysterols in human physiology include their participation in bile acid biosynthesis, function as transport forms of cholesterol, and regulation of gene transcription.\n\nIn biochemical experiments radiolabelled forms of cholesterol, such as tritiated-cholesterol are used. These derivatives undergo degradation upon storage and it is essential to purify cholesterol prior to use. Cholesterol can be purified using small Sephadex LH-20 columns.\n\nCholesterol is oxidized by the liver into a variety of bile acids. These, in turn, are conjugated with glycine, taurine, glucuronic acid, or sulfate. A mixture of conjugated and nonconjugated bile acids, along with cholesterol itself, is excreted from the liver into the bile. Approximately 95% of the bile acids are reabsorbed from the intestines, and the remainder are lost in the feces. The excretion and reabsorption of bile acids forms the basis of the enterohepatic circulation, which is essential for the digestion and absorption of dietary fats. Under certain circumstances, when more concentrated, as in the gallbladder, cholesterol crystallises and is the major constituent of most gallstones (lecithin and bilirubin gallstones also occur, but less frequently). Every day, up to 1 g of cholesterol enters the colon. This cholesterol originates from the diet, bile, and desquamated intestinal cells, and can be metabolized by the colonic bacteria. Cholesterol is converted mainly into coprostanol, a nonabsorbable sterol that is excreted in the feces. A cholesterol-reducing bacterium origin has been isolated from human feces.\n\nAlthough cholesterol is a steroid generally associated with mammals, the human pathogen \"Mycobacterium tuberculosis\" is able to completely degrade this molecule and contains a large number of genes that are regulated by its presence. Many of these cholesterol-regulated genes are homologues of fatty acid β-oxidation genes, but have evolved in such a way as to bind large steroid substrates like cholesterol.\n\nCholesterol binds to and affects the gating of a number of ion channels such as the nicotinic acetylcholine receptor, GABA receptor, and the inward-rectifier potassium ion channel. Cholesterol also activates the estrogen-related receptor alpha (ERRα), and may be the endogenous ligand for the receptor. The constitutively active nature of the receptor may be explained by the fact that cholesterol is ubiquitous in the body. Inhibition of ERRα signaling by reduction of cholesterol production has been identified as a key mediator of the effects of statins and bisphosphonates on bone, muscle, and macrophages. On the basis of these findings, it has been suggested that the ERRα should be de-orphanized and classified as a receptor for cholesterol.\n\nAccording to the lipid hypothesis, since cholesterol (like all fat molecules) is transported around the body (in the water outside cells) inside lipoprotein particles, elevated cholesterol concentrations (hypercholesterolemia) — potentially offers a lower cost way to estimate concentrations of LDL particles; possibly even low concentrations of functional HDL particles — both variations strongly associated with cardiovascular disease because LDL particles promote atheroma development in arteries (atherosclerosis).\n\nThis atherosclerotic disease process, over decades, leads to myocardial infarction (heart attack), stroke, and peripheral vascular disease. Since higher blood LDL, especially higher LDL particle concentrations and smaller LDL particle size, contribute to this process more than the cholesterol content of the HDL particles, LDL particles are often termed \"bad cholesterol\" because they have been linked to atheroma formation. On the other hand, high concentrations of functional HDL, which can remove cholesterol from cells and atheroma, offer protection and are sometimes referred to as \"good cholesterol\". These balances are mostly genetically determined, but can be changed by body build, medications, food choices, and other factors.\n\nConditions with elevated concentrations of oxidized LDL particles, especially \"small dense LDL\" (sdLDL) particles, are associated with atheroma formation in the walls of arteries, a condition known as atherosclerosis, which is the principal cause of coronary heart disease and other forms of cardiovascular disease. In contrast, HDL particles (especially large HDL) have been identified as a mechanism by which cholesterol and inflammatory mediators can be removed from atheroma. Increased concentrations of HDL correlate with lower rates of atheroma progressions and even regression. A 2007 study pooling data on almost 900,000 subjects in 61 cohorts demonstrated that blood total cholesterol levels have an exponential effect on cardiovascular and total mortality, with the association more pronounced in younger subjects. Still, because cardiovascular disease is relatively rare in the younger population, the impact of high cholesterol on health is still larger in older people.\n\nElevated levels of the lipoprotein fractions, LDL, IDL and VLDL are regarded as atherogenic (prone to cause atherosclerosis). Levels of these fractions, rather than the total cholesterol level, correlate with the extent and progress of atherosclerosis. Conversely, the total cholesterol can be within normal limits, yet be made up primarily of small LDL and small HDL particles, under which conditions atheroma growth rates would still be high. Recently, a \"post hoc\" analysis of the IDEAL and the EPIC prospective studies found an association between high levels of HDL cholesterol (adjusted for apolipoprotein A-I and apolipoprotein B) and increased risk of cardiovascular disease, casting doubt on the cardioprotective role of \"good cholesterol\".\n\nElevated cholesterol levels are treated with a strict diet consisting of low saturated fat, trans fat-free, low cholesterol foods, often followed by one of various hypolipidemic agents, such as statins, fibrates, cholesterol absorption inhibitors, nicotinic acid derivatives or bile acid sequestrants. Extreme cases have previously been treated with partial ileal bypass surgery, which has now been superseded by medication. Apheresis-based treatments are still used for very severe hyperlipidemias that are either unresponsive to treatment or require rapid lowering of blood lipids. There are several international guidelines on the treatment of hypercholesterolaemia.\n\nMultiple human trials using HMG-CoA reductase inhibitors, known as statins, have repeatedly confirmed that changing lipoprotein transport patterns from unhealthy to healthier patterns significantly lowers cardiovascular disease event rates, even for people with cholesterol values currently considered low for adults. Studies have also found that statins reduce atheroma progression. As a result, people with a history of cardiovascular disease may derive benefit from statins irrespective of their cholesterol levels (total cholesterol below 5.0 mmol/L [193 mg/dL]), and in men without cardiovascular disease, there is benefit from lowering abnormally high cholesterol levels (\"primary prevention\"). Primary prevention in women was originally practiced only by extension of the findings in studies on men, since, in women, none of the large statin trials conducted prior to 2007 demonstrated a statistically significant reduction in overall mortality or in cardiovascular endpoints. In 2008, a large clinical trial reported that, in apparently healthy adults with increased levels of the inflammatory biomarker high-sensitivity C-reactive protein but with low initial LDL, 20 mg/day of rosuvastatin for 1.9 years resulted in a 44% reduction in the incidence of cardiovascular events and a 20% reduction in all-cause mortality; the effect was statistically significant for both genders. Though this result was met with some skepticism, later studies and meta-analyses likewise demonstrated statistically significant (but smaller) reductions in all-cause and cardiovascular mortality, without significant heterogeneity by gender.\n\nThe 1987 report of National Cholesterol Education Program, Adult Treatment Panels suggests the total blood cholesterol level should be: < 200 mg/dL normal blood cholesterol, 200–239 mg/dL borderline-high, > 240 mg/dL high cholesterol. The American Heart Association provides a similar set of guidelines for total (fasting) blood cholesterol levels and risk for heart disease:\n\nHowever, as today's testing methods determine LDL (\"bad\") and HDL (\"good\") cholesterol separately, this simplistic view has become somewhat outdated. The desirable LDL level is considered to be less than 130 mg/dL (2.6 mmol/L), although a newer upper limit of 70 mg/dL (1.8 mmol/L) can be considered in higher-risk individuals based on some of the above-mentioned trials. A ratio of total cholesterol to HDL—another useful measure—of far less than 5:1 is thought to be healthier.\n\nTotal cholesterol is defined as the sum of HDL, LDL, and VLDL. Usually, only the total, HDL, and triglycerides are measured. For cost reasons, the VLDL is usually estimated as one-fifth of the triglycerides and the LDL is estimated using the Friedewald formula (or a variant): estimated LDL = [total cholesterol] − [total HDL] − [estimated VLDL]. VLDL can be calculated by dividing total triglycerides by five. Direct LDL measures are used when triglycerides exceed 400 mg/dL. The estimated VLDL and LDL have more error when triglycerides are above 400 mg/dL.\n\nGiven the well-recognized role of cholesterol in cardiovascular disease, some studies have shown an inverse correlation between cholesterol levels and mortality. A 2009 study of patients with acute coronary syndromes found an association of hypercholesterolemia with better mortality outcomes. In the Framingham Heart Study, in subjects over 50 years of age, they found an 11% increase overall and 14% increase in cardiovascular disease mortality per 1 mg/dL per year drop in total cholesterol levels. The researchers attributed this phenomenon to the fact that people with severe chronic diseases or cancer tend to have below-normal cholesterol levels. This explanation is not supported by the Vorarlberg Health Monitoring and Promotion Programme, in which men of all ages and women over 50 with very low cholesterol were likely to die of cancer, liver diseases, and mental diseases. This result indicates the low-cholesterol effect occurs even among younger respondents, contradicting the previous assessment among cohorts of older people that this is a proxy or marker for frailty occurring with age.\n\nAlthough the vast majority of doctors and medical scientists consider that there is a link between cholesterol and atherosclerosis as discussed above, a 2014 meta-analysis of over 500,000 patients concluded there is insufficient evidence to support the recommendation of high consumption of polyunsaturated fatty acids and low consumption of total saturated fats for cardiovascular health.\n\nAbnormally low levels of cholesterol are termed \"hypocholesterolemia\". Research into the causes of this state is relatively limited, but some studies suggest a link with depression, cancer, and cerebral hemorrhage. In general, the low cholesterol levels seem to be a consequence, rather than a cause, of an underlying illness. A genetic defect in cholesterol synthesis causes Smith-Lemli-Opitz syndrome, which is often associated with low plasma cholesterol levels. Hyperthyroidism, or any other endocrine disturbance which causes upregulation of the LDL receptor, may result in hypocholesterolemia.\n\nThe American Heart Association recommends testing cholesterol every 4–6 years for people aged 20 years or older. A separate set of American Heart Association guidelines issued in 2013 indicates that patients taking statin medications should have their cholesterol tested 4–12 weeks after their first dose and then every 3–12 months thereafter.\n\nA blood sample after 12-hour fasting is taken by a doctor, or a home cholesterol-monitoring device is used to determine a lipoprotein profile. This measures total cholesterol, LDL (bad) cholesterol, HDL (good) cholesterol, and triglycerides. It is recommended to test cholesterol at least every five years if a person has total cholesterol of 5.2 mmol/L or more (200+ mg/dL), or if a man over age 45 or a woman over age 50 has HDL (good) cholesterol less than 1 mmol/L (40 mg/dL), or there are other risk factors for heart disease and stroke. Other risk factors for heart disease include Diabetes, Hypertension (or use of anti-hypertensive medications), low HDL, family history of CAD and hypercholesterolemia, and cigarette smoking.\n\nSome cholesterol derivatives (among other simple cholesteric lipids) are known to generate the liquid crystalline \"cholesteric phase\". The cholesteric phase is, in fact, a chiral nematic phase, and it changes colour when its temperature changes. This makes cholesterol derivatives useful for indicating temperature in liquid crystal display thermometers and in temperature-sensitive paints.\n\nCholesterol has 256 stereoisomers that arise from its 8 stereocenters, although only two of the stereoisomers are of biochemical significance (\"nat\"-cholesterol and \"ent\"-cholesterol, for \"natural\" and \"enantiomer\", respectively), and only one occurs naturally (\"nat\"-cholesterol).\n\n\n"}
{"id": "6438", "url": "https://en.wikipedia.org/wiki?curid=6438", "title": "Chromosome", "text": "Chromosome\n\nA chromosome (from ancient Greek: χρωμόσωμα, \"chromosoma, chroma\" means color, \"soma\" means body) is a DNA molecule with part or all of the genetic material (genome) of an organism.\n\nChromosomes are normally visible under a light microscope only when the cell is undergoing the metaphase of cell division. Before this happens, every chromosome is copied once (S phase), and the copy is joined to the original by a centromere, resulting in an X-shaped structure. The original chromosome and the copy are now called sister chromatids. During metaphase, when a chromosome is in its most condensed state, the X-shape structure is called a metaphase chromosome. In this highly condensed form chromosomes are easiest to distinguish and study.\n\nChromosomes vary widely between different organisms. Some species such as certain bacteria, which lack histones, also contain plasmids or other extrachromosomal DNA. These are circular structures in the cytoplasm that contain cellular DNA and play a role in horizontal gene transfer. In prokaryotes (see nucleoids) and viruses, the DNA is often densely packed and organized; in the case of archaea, by homology to eukaryotic histones, and in the case of bacteria, by histone-like proteins.\n\nCompaction of the duplicated chromosomes during cell division (mitosis or meiosis) results either in a four-arm structure (pictured to the right) if the centromere is located in the middle of the chromosome or a two-arm structure if the centromere is located near one of the ends. Chromosomal recombination during meiosis and subsequent sexual reproduction play a significant role in genetic diversity. If these structures are manipulated incorrectly, through processes known as chromosomal instability and translocation, the cell may undergo mitotic catastrophe and die, or it may unexpectedly evade apoptosis, leading to the progression of cancer.\n\nSome use the term chromosome in a wider sense, to refer to the individualized portions of chromatin in cells, either visible or not under light microscopy. However, others use the concept in a narrower sense, to refer to the individualized portions of chromatin during cell division, visible under light microscopy due to high condensation.\n\nThe word \"chromosome\" () comes from the Greek (\"chroma\", \"colour\") and (\"soma\", \"body\"), describing their strong staining by particular dyes. \nSchleiden, Virchow and Bütschli were among the first scientists who recognized the structures now familiar as chromosomes. The term was coined by von Waldeyer-Hartz, referring to the term chromatin, which was introduced by Walther Flemming.\n\nIn a series of experiments beginning in the mid-1880s, Theodor Boveri gave the definitive demonstration that chromosomes are the vectors of heredity. His two principles were the \"continuity\" of chromosomes and the \"individuality\" of chromosomes. It is the second of these principles that was so original. Wilhelm Roux suggested that each chromosome carries a different genetic load. Boveri was able to test and confirm this hypothesis. Aided by the rediscovery at the start of the 1900s of Gregor Mendel's earlier work, Boveri was able to point out the connection between the rules of inheritance and the behaviour of the chromosomes. Boveri influenced two generations of American cytologists: Edmund Beecher Wilson, Nettie Stevens, Walter Sutton and Theophilus Painter were all influenced by Boveri (Wilson, Stevens, and Painter actually worked with him).\n\nIn his famous textbook \"The Cell in Development and Heredity\", Wilson linked together the independent work of Boveri and Sutton (both around 1902) by naming the chromosome theory of inheritance the Boveri–Sutton chromosome theory (the names are sometimes reversed). Ernst Mayr remarks that the theory was hotly contested by some famous geneticists: William Bateson, Wilhelm Johannsen, Richard Goldschmidt and T.H. Morgan, all of a rather dogmatic turn of mind. Eventually, complete proof came from chromosome maps in Morgan's own lab.\n\nThe number of human chromosomes was published in 1923 by Theophilus Painter. By inspection through the microscope, he counted 24 pairs, which would mean 48 chromosomes. His error was copied by others and it was not until 1956 that the true number, 46, was determined by Indonesia-born cytogeneticist Joe Hin Tjio.\n\nThe prokaryotes – bacteria and archaea – typically have a single circular chromosome, but many variations exist. The chromosomes of most bacteria, which some authors prefer to call genophores, can range in size from only 130,000 base pairs in the endosymbiotic bacteria \"Candidatus Hodgkinia cicadicola\" and \"Candidatus Tremblaya princeps\", to more than 14,000,000 base pairs in the soil-dwelling bacterium \"Sorangium cellulosum\". Spirochaetes of the genus \"Borrelia\" are a notable exception to this arrangement, with bacteria such as \"Borrelia burgdorferi\", the cause of Lyme disease, containing a single \"linear\" chromosome.\n\nProkaryotic chromosomes have less sequence-based structure than eukaryotes. Bacteria typically have a one-point (the origin of replication) from which replication starts, whereas some archaea contain multiple replication origins. The genes in prokaryotes are often organized in operons, and do not usually contain introns, unlike eukaryotes.\n\nStructure of chromosome:\n\nProkaryotes do not possess nuclei. Instead, their DNA is organized into a structure called the nucleoid. The nucleoid is a distinct structure and occupies a defined region of the bacterial cell. This structure is, however, dynamic and is maintained and remodeled by the actions of a range of histone-like proteins, which associate with the bacterial chromosome. In archaea, the DNA in chromosomes is even more organized, with the DNA packaged within structures similar to eukaryotic nucleosomes.\n\nBacterial chromosomes tend to be tethered to the plasma membrane of the bacteria. In molecular biology application, this allows for its isolation from plasmid DNA by centrifugation of lysed bacteria and pelleting of the membranes (and the attached DNA).\n\nProkaryotic chromosomes and plasmids are, like eukaryotic DNA, generally supercoiled. The DNA must first be released into its relaxed state for access for transcription, regulation, and replication.\n\nIn eukaryotes, nuclear chromosomes are packaged by proteins into a condensed structure called chromatin. This allows the very long DNA molecules to fit into the cell nucleus. The structure of chromosomes and chromatin varies through the cell cycle. Chromosomes are even more condensed than chromatin and are an essential unit for cellular division. Chromosomes must be replicated, divided, and passed successfully to their daughter cells so as to ensure the genetic diversity and survival of their progeny. Chromosomes may exist as either duplicated or unduplicated. Unduplicated chromosomes are single double helixes, whereas duplicated chromosomes contain two identical copies (called chromatids or sister chromatids) joined by a centromere.\nEukaryotes (cells with nuclei such as those found in plants, fungi, and animals) possess multiple large linear chromosomes contained in the cell's nucleus. Each chromosome has one centromere, with one or two arms projecting from the centromere, although, under most circumstances, these arms are not visible as such. In addition, most eukaryotes have a small circular mitochondrial genome, and some eukaryotes may have additional small circular or linear cytoplasmic chromosomes.\n\nIn the nuclear chromosomes of eukaryotes, the uncondensed DNA exists in a semi-ordered structure, where it is wrapped around histones (structural proteins), forming a composite material called chromatin.\n\nChromatin is the complex of DNA and protein found in the eukaryotic nucleus, which packages chromosomes. The structure of chromatin varies significantly between different stages of the cell cycle, according to the requirements of the DNA.\n\nDuring interphase (the period of the cell cycle where the cell is not dividing), two types of chromatin can be distinguished:\n\nIn the early stages of mitosis or meiosis (cell division), the chromatin double helix become more and more condensed. They cease to function as accessible genetic material (transcription stops) and become a compact transportable form. This compact form makes the individual chromosomes visible, and they form the classic four arm structure, a pair of sister chromatids attached to each other at the centromere. The shorter arms are called \"p arms\" (from the French \"petit\", small) and the longer arms are called \"q arms\" (\"q\" follows \"p\" in the Latin alphabet; q-g \"grande\"; alternatively it is sometimes said q is short for \"queue\" meaning tail in French). This is the only natural context in which individual chromosomes are visible with an optical microscope.\n\nMitotic metaphase chromosomes are best described by a linearly organized longitudinally compressed array of consecutive chromatin loops.\n\nDuring mitosis, microtubules grow from centrosomes located at opposite ends of the cell and also attach to the centromere at specialized structures called kinetochores, one of which is present on each sister chromatid. A special DNA base sequence in the region of the kinetochores provides, along with special proteins, longer-lasting attachment in this region. The microtubules then pull the chromatids apart toward the centrosomes, so that each daughter cell inherits one set of chromatids. Once the cells have divided, the chromatids are uncoiled and DNA can again be transcribed. In spite of their appearance, chromosomes are structurally highly condensed, which enables these giant DNA structures to be contained within a cell nucleus.\n\nChromosomes in humans can be divided into two types: autosomes (body chromosome(s)) and allosome (sex chromosome(s)). Certain genetic traits are linked to a person's sex and are passed on through the sex chromosomes. The autosomes contain the rest of the genetic hereditary information. All act in the same way during cell division. Human cells have 23 pairs of chromosomes (22 pairs of autosomes and one pair of sex chromosomes), giving a total of 46 per cell. In addition to these, human cells have many hundreds of copies of the mitochondrial genome. Sequencing of the human genome has provided a great deal of information about each of the chromosomes. Below is a table compiling statistics for the chromosomes, based on the Sanger Institute's human genome information in the Vertebrate Genome Annotation (VEGA) database. Number of genes is an estimate, as it is in part based on gene predictions. Total chromosome length is an estimate as well, based on the estimated size of unsequenced heterochromatin regions.\n\nThese tables give the total number of chromosomes (including sex chromosomes) in a cell nucleus. For example, human cells are diploid and have 22 different types of autosome, each present as two copies, and two sex chromosomes. This gives 46 chromosomes in total. Other organisms have more than two copies of their chromosome types, such as bread wheat, which is \"hexaploid\" and has six copies of seven different chromosome types – 42 chromosomes in total.\nNormal members of a particular eukaryotic species all have the same number of nuclear chromosomes (see the table). Other eukaryotic chromosomes, i.e., mitochondrial and plasmid-like small chromosomes, are much more variable in number, and there may be thousands of copies per cell.\n\nAsexually reproducing species have one set of chromosomes that are the same in all body cells. However, asexual species can be either haploid or diploid.\n\nSexually reproducing species have somatic cells (body cells), which are diploid [2n] having two sets of chromosomes (23 pairs in humans with one set of 23 chromosomes from each parent), one set from the mother and one from the father. Gametes, reproductive cells, are haploid [n]: They have one set of chromosomes. Gametes are produced by meiosis of a diploid germ line cell. During meiosis, the matching chromosomes of father and mother can exchange small parts of themselves (crossover), and thus create new chromosomes that are not inherited solely from either parent. When a male and a female gamete merge (fertilization), a new diploid organism is formed.\n\nSome animal and plant species are polyploid [Xn]: They have more than two sets of homologous chromosomes. Plants important in agriculture such as tobacco or wheat are often polyploid, compared to their ancestral species. Wheat has a haploid number of seven chromosomes, still seen in some cultivars as well as the wild progenitors. The more-common pasta and bread wheat types are polyploid, having 28 (tetraploid) and 42 (hexaploid) chromosomes, compared to the 14 (diploid) chromosomes in the wild wheat.\n\nProkaryote species generally have one copy of each major chromosome, but most cells can easily survive with multiple copies. For example, \"Buchnera\", a symbiont of aphids has multiple copies of its chromosome, ranging from 10–400 copies per cell. However, in some large bacteria, such as \"Epulopiscium fishelsoni\" up to 100,000 copies of the chromosome can be present. Plasmids and plasmid-like small chromosomes are, as in eukaryotes, highly variable in copy number. The number of plasmids in the cell is almost entirely determined by the rate of division of the plasmid – fast division causes high copy number.\n\nIn general, the karyotype is the characteristic chromosome complement of a eukaryote species. The preparation and study of karyotypes is part of cytogenetics.\n\nAlthough the replication and transcription of DNA is highly standardized in eukaryotes, \"the same cannot be said for their karyotypes\", which are often highly variable. There may be variation between species in chromosome number and in detailed organization.\nIn some cases, there is significant variation within species. Often there is:\nAlso, variation in karyotype may occur during development from the fertilized egg.\n\nThe technique of determining the karyotype is usually called \"karyotyping\". Cells can be locked part-way through division (in metaphase) in vitro (in a reaction vial) with colchicine. These cells are then stained, photographed, and arranged into a \"karyogram\", with the set of chromosomes arranged, autosomes in order of length, and sex chromosomes (here X/Y) at the end.\n\nLike many sexually reproducing species, humans have special gonosomes (sex chromosomes, in contrast to autosomes). These are XX in females and XY in males. \n\nInvestigation into the human karyotype took many years to settle the most basic question: \"How many chromosomes does a normal diploid human cell contain?\" In 1912, Hans von Winiwarter reported 47 chromosomes in spermatogonia and 48 in oogonia, concluding an XX/XO sex determination mechanism. Painter in 1922 was not certain whether the diploid number of man is 46 or 48, at first favouring 46. He revised his opinion later from 46 to 48, and he correctly insisted on humans having an XX/XY system.\n\nNew techniques were needed to definitively solve the problem:\n\nIt took until 1954 before the human diploid number was confirmed as 46. Considering the techniques of Winiwarter and Painter, their results were quite remarkable. Chimpanzees, the closest living relatives to modern humans, have 48 chromosomes as do the other great apes: in humans two chromosomes fused to form chromosome 2.\n\nChromosomal aberrations are disruptions in the normal chromosomal content of a cell and are a major cause of genetic conditions in humans, such as Down syndrome, although most aberrations have little to no effect. Some chromosome abnormalities do not cause disease in carriers, such as translocations, or chromosomal inversions, although they may lead to a higher chance of bearing a child with a chromosome disorder. Abnormal numbers of chromosomes or chromosome sets, called aneuploidy, may be lethal or may give rise to genetic disorders. Genetic counseling is offered for families that may carry a chromosome rearrangement.\n\nThe gain or loss of DNA from chromosomes can lead to a variety of genetic disorders. Human examples include:\n\nExposure of males to certain lifestyle, environmental and/or occupational hazards may increase the risk of aneuploid spermatozoa. In particular, risk of aneuploidy is increased by tobacco smoking, and occupational exposure to benzene, insecticides, and perfluorinated compounds. Increased aneuploidy is often associated with increased DNA damage in spermatozoa.\n\n\n"}
{"id": "6439", "url": "https://en.wikipedia.org/wiki?curid=6439", "title": "Charge", "text": "Charge\n\nCharge or charged may refer to:\n\n\n\n\n\n"}
{"id": "6440", "url": "https://en.wikipedia.org/wiki?curid=6440", "title": "Colonna family", "text": "Colonna family\n\nThe Colonna family, also known as Sciarrillo or Sciarra, is an Italian noble family. It was powerful in medieval and Renaissance Rome, supplying one Pope and many other Church and political leaders. The family is notable for its bitter feud with the Orsini family over influence in Rome, until it was stopped by Papal Bull in 1511. In 1571, the heads of both families married nieces of Pope Sixtus V. Thereafter, historians recorded that \"\"no peace had been concluded between the princes of Christendom, in which they had not been included by name\"\".\n\nAccording to tradition, the Colonna are a branch of the Counts of Tusculum — by Peter (1099–1151) son of Gregory III, called Peter \"de Columna\" from his property the Columna Castle in Colonna, Alban Hills. Even far further back, they trace their lineage past the counts of Tusculum via Lombard and Italo-Roman nobles, merchants, and clergy through the Early Middle Ages — ultimately claiming origins from the Julio-Claudian dynasty.\n\nThe first cardinal from the family was appointed in 1206 when Giovanni Colonna di Carbognano was made Cardinal Deacon of SS. Cosma e Damiano. For many years, cardinal Giovanni di San Paolo (elevated in 1193) was identified as member of the Colonna family and therefore its first representative in the College of Cardinals, but modern scholars have established that this was based on the false information from the beginning of 16th century.\n\nGiovanni Colonna (1206 c.- ), nephew of Cardinal Giovanni Colonna di Carbognano, made his solemn vows as a Dominican c. 1228 and received his theological and philosophical training at the Roman \"studium\" of Santa Sabina, the forerunner of the Pontifical University of Saint Thomas Aquinas, \"Angelicum. He served as the Provincial of the Roman province of the Dominican Order and led the provincial chapter of 1248 at Anagni. Colonna was appointed as Archbishop of Messina in 1255.\n\nIn 1248, after having dedicated her entire life to serving God and the poor, Margherita Colonna died. A member of the Franciscan Order, she was beatified by Pope Pius IX in 1848.\n\nAt this time a rivalry began with the pro-papal Orsini family, leaders of the Guelph faction. This reinforced the pro-Emperor Ghibelline course that the Colonna family followed throughout the period of conflict between the Papacy and the Holy Roman Empire.\n\nIn 1297, Cardinal Jacopo (Giacomo Colonna) disinherited his brothers Ottone, Matteo, and Landolfo of their lands. The latter three appealed to Pope Boniface VIII who ordered Jacopo to return the land, and furthermore hand over the family's strongholds of Colonna, Palestrina, and other towns to the Papacy. Jacopo refused; in May, Boniface removed him from the College of Cardinals and excommunicated him and his followers.\n\nThe Colonna family (aside from the three brothers allied with the Pope) declared that Boniface had been elected illegally following the unprecedented abdication of Pope Celestine V. The dispute led to open warfare, and in September Boniface appointed Landolfo to the command of his army, to put down the revolt of Landolfo's own Colonna relatives. By the end of 1298 Landolfo had captured Colonna, Palestrina and other towns and razed them to the ground. The family's lands were distributed among Landolfo and his loyal brothers; the rest of the family fled Italy.\n\nThe exiled Colonna allied with the Pope's other great enemy, Philip IV of France, who in his youth had been tutored by Cardinal Egidio Colonna. In September 1303, Sciarra and Philipp's advisor, Guillaume de Nogaret, led a small force into Anagni to arrest of Boniface VIII and bring him to France, where was to stand trial. While the two managed to apprehend the Pope and Sciarra reportedly slapped the pope in the face in the process, which was accordingly dubbed the \"Outrage of Anagni\". The attempt eventually failed after a few days, when locals freed the Pope. However, Boniface VIII died on the 11th October, allowing France to dominate his weaker successors during the Avignon papacy.\n\nThe family remained at the centre of civic and religious life throughout the late Middle Ages. Cardinal Egidio Colonna died at the papal court in Avignon in 1314. An Augustinian, he had studied theology in Paris under St. Thomas of Aquinas to become one of the most authoritative thinkers of his time.\n\nIn the 14th century, the family sponsored the decoration of the Church of San Giovanni, most notably the floor mosaics.\n\nIn 1328, Louis IV of Germany marched into Italy for his coronation as Holy Roman Emperor. As Pope John XXII was residing in Avignon and had publicly declared that he would not crown Louis, the King decided to be crowned by a member of the Roman aristocracy, who proposed Sciarra Colonna. In honor of this event, the Colonna family was granted the privilege of using the imperial pointed crown on top of their coat of arms.\n\nThe celebrated poet Petrarch, was a great friend of the family, in particular of Giovanni Colonna and often lived in Rome as a guest of the family. He composed a number of sonnets for special occasions within the Colonna family, including \"Colonna the Glorious, the great Latin name upon which all our hopes rest\". In this period, the Colonna started claiming they were descendants of the Julio-Claudian dynasty.\nAt the Council of Constance, the Colonna finally succeeded in their papal ambitions when Oddone Colonna was elected on 14 November 1417. As Martin V, he reigned until his death on 20 February 1431.\n\nVittoria Colonna became famous in the sixteenth century as a poet and a figure in literate circles.\n\nIn 1627 Anna Colonna, daughter of Filippo I Colonna, married Taddeo Barberini of the family Barberini; nephew of Pope Urban VIII.\n\nIn 1728, the Carbognano branch (Colonna di Sciarra) of the Colonna family added the name Barberini to its family name when Giulio Cesare Colonna di Sciarra married Cornelia Barberini, daughter of the last male Barberini to hold the name and granddaughter of Maffeo Barberini (son of Taddeo Barberini).\n\nThe Colonna family have been Prince Assistants to the Papal Throne since 1710, though their papal princely title only dates from 1854.\n\nThe family residence in Rome, the Palazzo Colonna, is open to the public every Saturday morning.\n\nThe main 'Colonna di Paliano' family is represented today by Prince Marcantonio Colonna di Paliano, Prince and Duke of Paliano (b. 1948), whose heir is Don Giovanni Andrea Colonna di Paliano (b. 1975), and by Don Prospero Colonna di Paliano, Prince of Avella (b. 1956), whose heir is Don Filippo Colonna di Paliano (b. 1995).\n\nThe 'Colonna di Stigliano' line is represented by Don Prospero Colonna di Stigliano, Prince of Stigliano (b. 1938), whose heir is his nephew Don Stefano Colonna di Stigliano (b. 1975).\n\n\n\n\n"}
{"id": "6443", "url": "https://en.wikipedia.org/wiki?curid=6443", "title": "Ceuta", "text": "Ceuta\n\nCeuta (assimilated pronunciation , also ; ; Arabic: سبتة, \"Sabtah\") is an Spanish autonomous city located on the north coast of Africa, separated by 14 kilometers from Cadiz province on the Spanish mainland by the Strait of Gibraltar and sharing a 6.4 kilometer land border with M'diq-Fnideq Prefecture in the Kingdom of Morocco. It lies along the boundary between the Mediterranean Sea and the Atlantic Ocean and is one of nine populated Spanish territories in Africa and, along with Melilla, one of two populated territories on mainland Africa. It was part of Cádiz province until 14 March 1995 when both Ceuta and Melilla's Statutes of Autonomy were passed, the latter having formerly been part of Almeria province.\n\nCeuta, like Melilla and the Canary Islands, was a free port before Spain joined the European Union. As of 2011, it has a population of 82,376. Its population consists of Christians, Muslims, and small minorities of Sephardic Jews and ethnic Sindhi Hindus.\n\nSpanish is the official language, while Darija Arabic is also spoken by between 40% and 50% of the population which is of Moroccan origin.\n\nCeuta's location has made it an important commercial trade and military way-point for many cultures, beginning with the Carthaginians in the 5th century BC, who called the city \"Abyla\"; initially, this was also its name in Greek and Latin. It was known variously in Ancient Greek as: Ἀβύλη, Ἀβύλα, Ἀβλύξ, or Ἀβίλη στήλη – \"Abyle\", \"Abila\", \"Ablyx\" or \"Abile Stele\" – \"Pillar of Abyle\") and in the Latin derivation from Greek as \"Abyla Mons Columna\" (\"Mount Abyla\" or \"Column of Abyla\"). Together with Gibraltar on the European side, it formed one of the famous \"Pillars of Hercules\". Later, it was renamed for a formation of seven surrounding smaller mountains, collectively referred to as \"Septem Fratres\" ('[The] Seven Brothers') by Pomponius Mela, which lent their name to a Roman fortification known as \"Castellum ad Septem Fratres\".\n\nIt changed hands again approximately 400 years later, when Vandal tribes ousted the Romans. After being controlled by the Visigoths, it then became an outpost of the Byzantine Empire. Ceuta was an important Christian center since the fourth century (as recent discovered ruins of a Roman basilica show).\n\nIn the 7th century the Umayyads tried to conquer the region but were unsuccessful. Byzantine governor, Julian (described as \"King of the Ghomara\") who was a vassal of the Visigothic kings of Iberia changed his allegiance after the king Roderic raped his daughter, and exhorted the Muslims to invade the Iberian Peninsula. Under the leadership of the Berber general Tariq ibn Ziyad, the Muslims used Ceuta as a staging ground for an assault on Visigothic Iberian Peninsula. After Julian's death, the Berbers took direct control of the city, which the indigenous Berber tribes resented. They destroyed Ceuta during the Kharijite rebellion led by Maysara al-Matghari in 740.\n\nCeuta lay in ruins until it was resettled in the 9th century by Mâjakas, chief of the Majkasa Berber tribe, who started the short-lived Banu Isam dynasty. His great-grandson briefly allied his tribe with the Idrisids, but the Banu Isam rule ended in 931 when he abdicated in favor of Abd ar-Rahman III, the Umayyad Caliph of Cordoba. Ceuta reverted to Moorish Andalusian rule in 927 along with Melilla, and later Tangier, in 951.\n\nChaos ensued with the fall of the Umayyad caliphate in 1031. Following this Ceuta and the rest of Muslim Iberia were controlled by successive North African dynasties. Starting in 1084, the Almoravid Berbers ruled the region until 1147, when the Almohads conquered the land. Apart from Ibn Hud's rebellion of 1232, they ruled until the Tunisian Hafsids established control. The Hafsids' influence in the west rapidly waned, and Ceuta's inhabitants eventually expelled them in 1249. After this, a period of political instability persisted, under competing interests from the Kingdom of Fez and the Kingdom of Granada. The Kingdom of Fez finally conquered the region in 1387, with assistance from the Crown of Aragon.\n\nOn the morning of 21 August 1415, king John I of Portugal led his sons and their assembled forces in a surprise assault that would come to be known as the Conquest of Ceuta. The battle itself was almost anti-climactic, because the 45,000 men who traveled on 200 Portuguese ships caught the defenders of Ceuta off guard and thus only suffered eight casualties. By nightfall the town was captured. On the morning of August 22, Ceuta was in Portuguese hands. Álvaro Vaz de Almada, 1st Count of Avranches was asked to hoist what was to become the flag of Ceuta, which is identical to the flag of Lisbon, but in which the coat of arms of the Kingdom of Portugal was added to the center, the original Portuguese flag and coat of arms of Ceuta remained unchanged, and the modern-day Ceuta flag features the configuration of the Portuguese shield.\n\nJohn's son Henry the Navigator distinguished himself in the battle, being wounded during the conquest. The looting of the city proved to be less profitable than expected for John I; he ultimately decided to keep the city, in order to pursue further enterprises in the area.\n\nFrom 1415 to 1437 Pedro de Meneses, 1st Count of Vila Real became the first governor of Ceuta.\n\nThe Benemerine sultan started the Siege of Ceuta (1418) but was defeated by the first governor of Ceuta before reinforcements arrived in the form of John, Constable of Portugal and his brother Henry the Navigator who were sent with troops to defend Ceuta.\n\nUnder King John I of Portugals son, Duarte, the colony at Ceuta rapidly became a drain on the Portuguese treasury. Trans-Saharan trade journeyed instead to Tangier. It was soon realised that without the city of Tangier, possession of Ceuta was worthless. In 1437, Duarte's brothers Henry the Navigator and Fernando, the Saint Prince persuaded him to launch an attack on the Marinid sultanate. The resulting Battle of Tangier (1437), led by Henry, was a debacle. In the resulting treaty, Henry promised to deliver Ceuta back to the Marinids in return for allowing the Portuguese army to depart unmolested, which he reneged on.\n\nPossession of Ceuta would indirectly lead to further Portuguese expansion. The main area of Portuguese expansion, at this time, was the coast of Magreb, where there was grain, cattle, sugar, and textiles, as well as fish, hides, wax, and honey.\n\nCeuta had to endure alone for 43 years, until the position of the city was consolidated with the taking of Ksar es-Seghir (1458), Arzila and Tangier (1471) by the Portuguese.\n\nThe city was recognized as a Portuguese possession by the Treaty of Alcáçovas (1479) and by the Treaty of Tordesilhas (1494).\n\nIn the 1540s the Portuguese began building the Royal Walls of Ceuta as they are today including bastions, a navigable moat and a drawbridge. Some of these bastions are still standing, like the bastions of Coraza Alta, Bandera and Mallorquines.\n\nLuís de Camões lived in Ceuta between 1549 and 1551, losing his right eye in battle, which influenced his work of poetry Os Lusíadas.\n\nIn 1578 king Sebastian of Portugal died at the Battle of Alcácer Quibir (known as the Battle of Three Kings) in what is today northern Morocco, without descendants, triggering the 1580 Portuguese succession crisis. His granduncle, the elderly Cardinal Henry, succeeded him as King, but Henry also had no descendants, having taken holy orders. When the Cardinal-King died two years after Sebastian's disappearance, three grandchildren of king Manuel I of Portugal claimed the throne: Infanta Catarina, Duchess of Braganza, António, Prior of Crato, and Philip II of Spain (Uncle of former King Sebastian of Portugal), who would go on to be crowned king Philip I of Portugal in 1581, uniting the two crowns and overseas empires known as the Iberian Union., which allowed the two kingdoms to continue without being merged into each other.\n\nDuring the Iberian Union 1580 to 1640, Ceuta attracted many residents of Spanish origin. Ceuta became the only city of the Portuguese Empire that sided with Spain when Portugal regained its independence in the Portuguese Restoration War of 1640.\nOn 1 January 1668 by the Treaty of Lisbon, King Afonso VI of Portugal recognized the formal allegiance of Ceuta to Spain and formally ceded Ceuta to King Carlos II of Spain. \n\nThe city was attacked by Moroccan forces under Moulay Ismail during the Siege of Ceuta (1694-1727). During the longest siege in the history, the city underwent changes leading to the loss of its Portuguese character. While most of the military operations took place around the Royal Walls of Ceuta, there were also small-scale penetrations by Spanish forces at various points on the Moroccan coast, and seizure of shipping in the Strait of Gibraltar.\n\nDisagreements regarding the border of Ceuta resulted in the Hispano-Moroccan War (1859–60), which ended at the Battle of Tetuán.\n\nIn July 1936, General Francisco Franco took command of the Spanish Army of Africa and rebelled against the Spanish republican government; his military uprising led to the Spanish Civil War of 1936–1939. Franco transported troops to mainland Spain in an airlift using transport aircraft supplied by Germany and Italy. Ceuta became one of the first casualties of the uprising: General Franco's rebel nationalist forces seized Ceuta, while at the same time the city came under fire from the air and sea forces of the official republican government.\n\nThe Llano Amarillo monument was erected to honor Francisco Franco, it was inaugurated on 13 July 1940. The tall obelisk has since been abandoned, but the shield symbols of the Falange and Imperial Eagle remain visible.\n\nWhen Spain recognized the independence of Spanish Morocco in 1956, Ceuta and the other \"plazas de soberanía\" remained under Spanish rule. Spain considered them integral parts of the Spanish state, but Morocco has disputed this point.\n\nCulturally, modern Ceuta is part of the Spanish region of Andalusia. It was attached to the province of Cádiz until 1925, the Spanish coast being only 20 km (12.5 miles) away. It is a cosmopolitan city, with a large ethnic Berber Muslim minority as well as Sephardic Jewish and Hindu minorities.\n\nOn 5 November 2007, King Juan Carlos I visited the city, sparking great enthusiasm from the local population and protests from the Moroccan government. It was the first time a Spanish head of state had visited Ceuta in 80 years.\n\nSince 2010, Ceuta (and Melilla) have declared the Muslim holiday of Eid al-Adha or Feast of the Sacrifice, as an official public holiday. It is the first time a non-Christian religious festival has been officially celebrated in Spain since the Reconquista.\n\nThe Catholic Diocese of Ceuta existed from 1417 to 1879. It was a suffragan of the Patriarchate of Lisbon until 1675 and the end of the Iberian Union, when Ceuta chose to remain linked to the king of Spain. Since then it has been a suffragan of the archbishopric of Seville. The Diocese of Tanger was suppressed and incorporated to that of Ceuta in 1570.\n\nIn 1851, upon the signature of the concordat between the Holy See and Spain, the diocese of Ceuta was agreed to be suppressed, being combined into the Diocese of Cádiz y Ceuta. Until then in the Diocese of Cádiz y Algeciras, the bishop was usually the apostolic administrator of Ceuta. The agreement was not implemented until 1879.\n\nCeuta is dominated by Monte Anyera, a hill along its western frontier with Morocco. The mountain is guarded by a military fort.\n\nMonte Hacho on the Peninsula of Almina overlooking the port is one of the possible locations for the southern pillar of the Pillars of Hercules of Greek legend (the other possibility being Jebel Musa).\n\nCeuta has a maritime-influenced Subtropical/Mediterranean climate, similar to nearby Spanish and Moroccan cities such as Tarifa, Algeciras or Tangiers. The average diurnal temperature variation is relatively low; the average annual temperature is with average yearly highs of and lows of though the Ceuta weather station has only been in operation since 2003. Ceuta has relatively mild winters for the latitude, while summers are warm yet milder than in the interior of Southern Spain, due to the moderating effect of the Straits of Gibraltar. Summers are very dry, but yearly precipitation is still at , which could be considered a humid climate if the summers weren't so arid.\n\nSince 1995, Ceuta is, along with Melilla, one of the two autonomous cities of Spain.\n\nCeuta is known officially in Spanish as \"Ciudad Autónoma de Ceuta\" (English: \"Autonomous City of Ceuta\"), with a rank between a standard Spanish city and an autonomous community. Ceuta is part of the territory of the European Union. The city was a free port before Spain joined the European Union in 1986. Now it has a low-tax system within the Economic and Monetary Union of the European Union. As of 2006, its population was 75,861.\n\nCeuta has held elections every four years since 1979, for its 25-seat assembly. The leader of its government was the Mayor until the Autonomy Statute had the title changed to the Mayor-President. In the most recent election in 2011, the People's Party (PP) won 18 seats, keeping Juan Jesús Vivas as Mayor-President, which he has been since 2001. The remaining seats are held by the regionalist Caballas Coalition (4) and the Socialist Workers' Party (PSOE, 3).\n\nDue to its small population, Ceuta elects only one member of the Congress of Deputies, the lower house of the Spanish legislature. Since the 2011 election, this post is held by Francisco Márquez de la Rubia of the PP.\n\nCeuta is subdivided into 63 \"barriadas\" (neighbourhoods), such as Barriada de Berizu, Barriada de P. Alfonso, Barriada del Sarchal, and El Hacho.\n\nThe government of Morocco has repeatedly called for Spain to transfer the sovereignty of Ceuta and Melilla, together with the rest of the Spanish \"plazas de soberanía\" on the North African coast, on the grounds of asserting its territorial integrity. Morocco has claimed the territories are colonies. A significant majority of the residents of Ceuta want the region to remain Spanish.\n\nThe official currency of Ceuta is the euro. It is part of a special low tax zone in Spain. Ceuta is one of two Spanish port cities on the northern shore of Africa, along with Melilla. They are historically military strongholds, free ports, oil ports, and also fishing ports. Today the economy of the city depends heavily on its port (now in expansion) and its industrial and retail centres. Ceuta Heliport is now used to connect the city to mainland Spain by air.\n\nThe city receives high numbers of ferries each day from Algeciras in Andalusia in the south of Spain, along with Melilla and the Canary Islands. The closest airport is Sania Ramel Airport in Morocco. There is a bus service throughout the city which does not pass into neighbouring Morocco.\n\nA single road border checkpoint allows for cars to travel between Morocco and Ceuta. The rest of the border is closed and inaccessible.\n\nDue to its location, Ceuta is home to a mixed ethnic and religious population. The two main religious groups are Christians and Muslims. As of 2006 approximately 50% of the population was Spanish/Christian and approximately 49% Arab-Berber/Muslims. As of 2012, the portion of Ceuta's population that identify as Roman Catholic was 68.0%, while the portion of Ceuta's population that identify as Muslim was 28.3%.\n\nSpanish is the primary and official language of the enclave. Moroccan Arabic, Berber, and French are also widely spoken.\n\nThe University of Granada offers undergraduate programs at their campus in Ceuta. Like all areas of Spain, Ceuta is also served by the National University of Distance Education (UNED).\n\nPrimary and secondary education is possible only in Spanish however a growing number of schools are entering the Bilingual Education Program.\n\nChristianity has been present in Ceuta (called in Roman times \"Septem\" or \"Septum\") continuously since the fall of the Western Roman Empire. The ruins of a basilica in downtown Ceuta confirm this reality.\n\nIn 1415, on conquering the city from the Muslims, the Portuguese started the construction of the Cathedral of St. Mary of the Assumption. The Roman Catholic Diocese of Ceuta was established two years later, and in 1851 was merged into the Roman Catholic Diocese of Cadiz y Ceuta. The present cathedral, dating from the late 17th century, combines baroque and neoclassical elements.\n\nAs in Melilla, Ceuta is attracted by migrants who try to use it as an entry to Europe. As a result the enclave is surrounded by double fences that are 6 meter high. Nevertheless hundreds of migrants congregate near the fences waiting for a chance to cross them. Regularly the fences are stormed by migrants trying to claim asylum once they enter Ceuta. \n\n\nCeuta is twinned with:\n\n\n\nAttribution:\n\n\n"}
{"id": "6444", "url": "https://en.wikipedia.org/wiki?curid=6444", "title": "Cleopatra (disambiguation)", "text": "Cleopatra (disambiguation)\n\nCleopatra ( \"Kleopatra\"; \"Glory of the father\") is a popular ancient Greek name used in the ancient world to refer to many Greco-Macedonian princesses, including those who reigned as queens of ancient Egypt. It may refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "6445", "url": "https://en.wikipedia.org/wiki?curid=6445", "title": "Carcinogen", "text": "Carcinogen\n\nA carcinogen is any substance, radionuclide, or radiation that promotes carcinogenesis, the formation of cancer. This may be due to the ability to damage the genome or to the disruption of cellular metabolic processes. Several radioactive substances are considered carcinogens, but their carcinogenic activity is attributed to the radiation, for example gamma rays and alpha particles, which they emit. Common examples of non-radioactive carcinogens are inhaled asbestos, certain dioxins, and tobacco smoke. Although the public generally associates carcinogenicity with synthetic chemicals, it is equally likely to arise in both natural and synthetic substances. Carcinogens are not necessarily immediately toxic; thus, their effect can be insidious.\n\nCancer is any disease in which normal cells are damaged and do not undergo programmed cell death as fast as they divide via mitosis. Carcinogens may increase the risk of cancer by altering cellular metabolism or damaging DNA directly in cells, which interferes with biological processes, and induces the uncontrolled, malignant division, ultimately leading to the formation of tumors. Usually, severe DNA damage leads to programmed cell death, but if the programmed cell death pathway is damaged, then the cell cannot prevent itself from becoming a cancer cell.\n\nThere are many natural carcinogens. Aflatoxin B, which is produced by the fungus \"Aspergillus flavus\" growing on stored grains, nuts and peanut butter, is an example of a potent, naturally occurring microbial carcinogen. Certain viruses such as hepatitis B and human papilloma virus have been found to cause cancer in humans. The first one shown to cause cancer in animals is Rous sarcoma virus, discovered in 1910 by Peyton Rous. Other infectious organisms which cause cancer in humans include some bacteria (e.g. \"Helicobacter pylori\" ) and helminths (e.g. \"Opisthorchis viverrini\" and \"Clonorchis sinensis\" .\n\nDioxins and dioxin-like compounds, benzene, kepone, EDB, and asbestos have all been classified as carcinogenic. As far back as the 1930s, Industrial smoke and tobacco smoke were identified as sources of dozens of carcinogens, including benzo[\"a\"]pyrene, tobacco-specific nitrosamines such as nitrosonornicotine, and reactive aldehydes such as formaldehyde, which is also a hazard in embalming and making plastics. Vinyl chloride, from which PVC is manufactured, is a carcinogen and thus a hazard in PVC production.\n\nCo-carcinogens are chemicals that do not necessarily cause cancer on their own, but promote the activity of other carcinogens in causing cancer.\n\nAfter the carcinogen enters the body, the body makes an attempt to eliminate it through a process called biotransformation. The purpose of these reactions is to make the carcinogen more water-soluble so that it can be removed from the body. However, in some cases, these reactions can also convert a less toxic carcinogen into a more toxic carcinogen.\n\nDNA is nucleophilic; therefore, soluble carbon electrophiles are carcinogenic, because DNA attacks them. For example, some alkenes are toxicated by human enzymes to produce an electrophilic epoxide. DNA attacks the epoxide, and is bound permanently to it. This is the mechanism behind the carcinogenicity of benzo[\"a\"]pyrene in tobacco smoke, other aromatics, aflatoxin and mustard gas.\n\nCERCLA identifies all radionuclides as carcinogens, although the nature of the emitted radiation (alpha, beta, gamma, or neutron and the radioactive strength), its consequent capacity to cause ionization in tissues, and the magnitude of radiation exposure, determine the potential hazard. Carcinogenicity of radiation depends on the type of radiation, type of exposure, and penetration. For example, alpha radiation has low penetration and is not a hazard outside the body, but emitters are carcinogenic when inhaled or ingested. For example, Thorotrast, a (incidentally radioactive) suspension previously used as a contrast medium in x-ray diagnostics, is a potent human carcinogen known because of its retention within various organs and persistent emission of alpha particles. Low-level ionizing radiation may induce irreparable DNA damage (leading to replicational and transcriptional errors needed for neoplasia or may trigger viral interactions) leading to pre-mature aging and cancer.\n\nNot all types of electromagnetic radiation are carcinogenic. Low-energy waves on the electromagnetic spectrum including radio waves, microwaves, infrared radiation and visible light are thought not to be, because they have insufficient energy to break chemical bonds. Evidence for carcinogenic effects of non-ionizing radiation is generally inconclusive, though there are some documented cases of radar technicians with prolonged high exposure experiencing significantly higher cancer incidence.\n\nHigher-energy radiation, including ultraviolet radiation (present in sunlight), x-rays, and gamma radiation, generally \"is\" carcinogenic, if received in sufficient doses. For most people, ultraviolet radiations from sunlight is the most common cause of skin cancer. In Australia, where people with pale skin are often exposed to strong sunlight, melanoma is the most common cancer diagnosed in people aged 15–44 years.\n\nSubstances or foods irradiated with electrons or electromagnetic radiation (such as microwave, X-ray or gamma) are not carcinogenic. In contrast, non-electromagnetic neutron radiation produced inside nuclear reactors can produce secondary radiation through nuclear transmutation.\n\nChemicals used in processed and cured meat such as some brands of bacon, sausages and ham may or may not produce carcinogens. For example, nitrites used as food preservatives in cured meat such as bacon have also been noted as being carcinogenic with demographic links, but not causation, to colon cancer. Cooking food at high temperatures, for example grilling or barbecuing meats, can, or can not, also lead to the formation of minute quantities of many potent carcinogens that are comparable to those found in cigarette smoke (i.e., benzo[\"a\"]pyrene). Charring of food looks like coking and tobacco pyrolysis, and produces carcinogens. There are several carcinogenic pyrolysis products, such as polynuclear aromatic hydrocarbons, which are converted by human enzymes into epoxides, which attach permanently to DNA. Pre-cooking meats in a microwave oven for 2–3 minutes before grilling shortens the time on the hot pan, and removes heterocyclic amine (HCA) precursors, which can help minimize the formation of these carcinogens.\n\nReports from the Food Standards Agency have found that the known animal carcinogen acrylamide is generated in fried or overheated carbohydrate foods (such as french fries and potato chips). Studies are underway at the FDA and European regulatory agencies to assess its potential risk to humans.\n\nThere is a strong association of smoking with lung cancer; the lifetime risk of developing lung cancer increases significantly in smokers. A large number of known carcinogens are found in cigarette smoke. Potent carcinogens found in cigarette smoke include polycyclic aromatic hydrocarbons (PAH, such as benzo[a]pyrene), Benzene, and Nitrosamine.\n\nCarcinogens can be classified as genotoxic or nongenotoxic. Genotoxins cause irreversible genetic damage or mutations by binding to DNA. Genotoxins include chemical agents like N-nitroso-N-methylurea (NMU) or non-chemical agents such as ultraviolet light and ionizing radiation. Certain viruses can also act as carcinogens by interacting with DNA.\n\nNongenotoxins do not directly affect DNA but act in other ways to promote growth. These include hormones and some organic compounds.\n\nThe International Agency for Research on Cancer (IARC) is an intergovernmental agency established in 1965, which forms part of the World Health Organization of the United Nations. It is based in Lyon, France. Since 1971 it has published a series of \"Monographs on the Evaluation of Carcinogenic Risks to Humans\" that have been highly influential in the classification of possible carcinogens.\n\nThe Globally Harmonized System of Classification and Labelling of Chemicals (GHS) is a United Nations initiative to attempt to harmonize the different systems of assessing chemical risk which currently exist (as of March 2009) around the world. It classifies carcinogens into two categories, of which the first may be divided again into subcategories if so desired by the competent regulatory authority:\n\nThe National Toxicology Program of the U.S. Department of Health and Human Services is mandated to produce a biennial \"Report on Carcinogens\". As of June 2011, the latest edition was the 12th report (2011). It classifies carcinogens into two groups:\n\nThe American Conference of Governmental Industrial Hygienists (ACGIH) is a private organization best known for its publication of threshold limit values (TLVs) for occupational exposure and monographs on workplace chemical hazards. It assesses carcinogenicity as part of a wider assessment of the occupational hazards of chemicals.\n\nThe European Union classification of carcinogens is contained in the Dangerous Substances Directive and the Dangerous Preparations Directive. It consists of three categories:\nThis assessment scheme is being phased out in favor of the GHS scheme (see above), to which it is very close in category definitions.\n\nUnder a previous name, the NOHSC, in 1999 Safe Work Australia published the Approved Criteria for Classifying Hazardous Substances [NOHSC:1008(1999)].\nSection 4.76 of this document outlines the criteria for classifying carcinogens as approved by the Australian government. This classification consists of three categories:\n\nA procarcinogen is a precursor to a carcinogen. One example is nitrites when taken in by the diet. They are not carcinogenic themselves, but turn into nitrosamines in the body, which can be carcinogenic.\n\nOccupational carcinogens are agents that pose a risk of cancer in several specific work-locations:\n\n\nIn this section, the carcinogens implicated as the main causative agents of the four most common cancers worldwide are briefly described. These four cancers are lung, breast, colon, and stomach cancers. Together they account for about 41% of worldwide cancer incidence and 42% of cancer deaths (for more detailed information on the carcinogens implicated in these and other cancers, see references).\n\nLung cancer (pulmonary carcinoma) is the most common cancer in the world, both in terms of cases (1.6 million cases; 12.7% of total cancer cases) and deaths (1.4 million deaths; 18.2% of total cancer deaths). Lung cancer is largely caused by tobacco smoke. Risk estimates for lung cancer in the United States indicate that tobacco smoke is responsible for 90% of lung cancers. Other factors are implicated in lung cancer, and these factors can interact synergistically with smoking so that total attributable risk adds up to more than 100%. These factors include occupational exposure to carcinogens (about 9-15%), radon (10%) and outdoor air pollution (1-2%). Tobacco smoke is a complex mixture of more than 5,300 identified chemicals. The most important carcinogens in tobacco smoke have been determined by a “Margin of Exposure” approach. Using this approach, the most important tumorigenic compounds in tobacco smoke were, in order of importance, acrolein, formaldehyde, acrylonitrile, 1,3-butadiene, cadmium, acetaldehyde, ethylene oxide, and isoprene. Most of these compounds cause DNA damage by forming DNA adducts or by inducing other alterations in DNA. DNA damages are subject to error-prone DNA repair or can cause replication errors. Such errors in repair or replication can result in mutations in tumor suppressor genes or oncogenes leading to cancer.\n\nBreast cancer is the second most common cancer [(1.4 million cases, 10.9%), but ranks 5th as cause of death (458,000, 6.1%)]. Increased risk of breast cancer is associated with persistently elevated blood levels of estrogen. Estrogen appears to contribute to breast carcinogenesis by three processes; (1) the metabolism of estrogen to genotoxic, mutagenic carcinogens, (2) the stimulation of tissue growth, and (3) the repression of phase II detoxification enzymes that metabolize ROS leading to increased oxidative DNA damage. The major estrogen in humans, estradiol, can be metabolized to quinone derivatives that form adducts with DNA. These derivatives can cause dupurination, the removal of bases from the phosphodiester backbone of DNA, followed by inaccurate repair or replication of the apurinic site leading to mutation and eventually cancer. This genotoxic mechanism may interact in synergy with estrogen receptor-mediated, persistent cell proliferation to ultimately cause breast cancer. Genetic background, dietary practices and environmental factors also likely contribute to the incidence of DNA damage and breast cancer risk.\n\nColorectal cancer is the third most common cancer [1.2 million cases (9.4%), 608,000 deaths (8.0%)]. Tobacco smoke may be responsible for up to 20% of colorectal cancers in the United States. In addition, substantial evidence implicates bile acids as an important factor in colon cancer. Twelve studies (summarized in Bernstein et al.) indicate that the bile acids deoxycholic acid (DCA) and/or lithocholic acid (LCA) induce production of DNA-damaging reactive oxygen species and/or reactive nitrogen species in human or animal colon cells. Furthermore, 14 studies showed that DCA and LCA induce DNA damage in colon cells. Also 27 studies reported that bile acids cause programmed cell death (apoptosis). Increased apoptosis can result in selective survival of cells that are resistant to induction of apoptosis. Colon cells with reduced ability to undergo apoptosis in response to DNA damage would tend to accumulate mutations, and such cells may give rise to colon cancer. Epidemiologic studies have found that fecal bile acid concentrations are increased in populations with a high incidence of colon cancer. Dietary increases in total fat or saturated fat result in elevated DCA and LCA in feces and elevated exposure of the colon epithelium to these bile acids. When the bile acid DCA was added to the standard diet of wild-type mice invasive colon cancer was induced in 56% of the mice after 8 to 10 months. Overall, the available evidence indicates that DCA and LCA are centrally important DNA-damaging carcinogens in colon cancer.\n\nStomach cancer is the fourth most common cancer [990,000 cases (7.8%), 738,000 deaths (9.7%)]. \"Helicobacter pylori\" infection is the main causative factor in stomach cancer. Chronic gastritis (inflammation) caused by \"H. pylori\" is often long-standing if not treated. Infection of gastric epithelial cells with \"H. pylori\" results in increased production of reactive oxygen species (ROS). ROS cause oxidative DNA damage including the major base alteration 8-hydroxydeoxyguanosine (8-OHdG). 8-OHdG resulting from ROS is increased in chronic gastritis. The altered DNA base can cause errors during DNA replication that have mutagenic and carcinogenic potential. Thus \"H. pylori\"-induced ROS appear to be the major carcinogens in stomach cancer because they cause oxidative DNA damage leading to carcinogenic mutations. Diet is thought to be a contributing factor in stomach cancer - in Japan where very salty pickled foods are popular, the incidence of stomach cancer is high. Preserved meat such as bacon, sausages, and ham increases the risk while a diet high in fresh fruit and vegetables may reduce the risk. The risk also increases with age.\n\n"}
{"id": "6446", "url": "https://en.wikipedia.org/wiki?curid=6446", "title": "Camouflage", "text": "Camouflage\n\nCamouflage is the use of any combination of materials, coloration, or illumination for concealment, either by making animals or objects hard to see (crypsis), or by disguising them as something else (mimesis). Examples include the leopard's spotted coat, the battledress of a modern soldier, and the leaf-mimic katydid's wings. A third approach, motion dazzle, confuses the observer with a conspicuous pattern, making the object visible but momentarily harder to locate. The majority of camouflage methods aim for crypsis, often through a general resemblance to the background, high contrast disruptive coloration, eliminating shadow, and countershading. In the open ocean, where there is no background, the principal methods of camouflage are transparency, silvering, and countershading, while the ability to produce light is among other things used for counter-illumination on the undersides of cephalopods such as squid. Some animals, such as chameleons and octopuses, are capable of actively changing their skin pattern and colours, whether for camouflage or for signalling.\n\nMilitary camouflage was spurred by the increasing range and accuracy of firearms in the 19th century. In particular the replacement of the inaccurate musket with the rifle made personal concealment in battle a survival skill. In the 20th century, military camouflage developed rapidly, especially during the First World War. On land, artists such as André Mare designed camouflage schemes and observation posts disguised as trees. At sea, merchant ships and troop carriers were painted in dazzle patterns that were highly visible, but designed to confuse enemy submarines as to the target's speed, range, and heading. During and after the Second World War, a variety of camouflage schemes were used for aircraft and for ground vehicles in different theatres of war. The use of radar since the mid-20th century has largely made camouflage for fixed-wing military aircraft obsolete.\n\nNon-military use of camouflage includes making cell telephone towers less obtrusive and helping hunters to approach wary game animals. Patterns derived from military camouflage are frequently used in fashion clothing, exploiting their strong designs and sometimes their symbolism. Camouflage themes recur in modern art, and both figuratively and literally in science fiction and works of literature.\n\nIn ancient Greece, Aristotle (384 BC – 322 BC) commented on the colour-changing abilities, both for camouflage and for signalling, of cephalopods including the octopus, in his \"Historia animalium\":\n\nCamouflage has been a topic of interest and research in zoology for well over a century. According to Charles Darwin's 1859 theory of natural selection, features such as camouflage evolved by providing individual animals with a reproductive advantage, enabling them to leave more offspring, on average, than other members of the same species. In his \"Origin of Species\", Darwin wrote:\n\nThe English zoologist Edward Bagnall Poulton studied animal coloration, especially camouflage. In his 1890 book \"The Colours of Animals\", he classified different types such as \"special protective resemblance\" (where an animal looks like another object), or \"general aggressive resemblance\" (where a predator blends in with the background, enabling it to approach prey). His experiments showed that swallowtailed moth pupae were camouflaged to match the backgrounds on which they were reared as larvae. Poulton's \"general protective resemblance\" was at that time considered to be the main method of camouflage, as when Frank Evers Beddard wrote in 1892 that \"tree-frequenting animals are often green in colour. Among vertebrates numerous species of parrots, iguanas, tree-frogs, and the green tree-snake are examples\". Beddard did however briefly mention other methods, including the \"alluring coloration\" of the flower mantis and the possibility of a different mechanism in the orange tip butterfly. He wrote that \"the scattered green spots upon the under surface of the wings might have been intended for a rough sketch of the small flowerets of the plant [an umbellifer], so close is their mutual resemblance.\" He also explained the coloration of sea fish such as the mackerel: \"Among pelagic fish it is common to find the upper surface dark-coloured and the lower surface white, so that the animal is inconspicuous when seen either from above or below.\"\n\nThe artist Abbott Handerson Thayer formulated what is sometimes called Thayer's Law, the principle of countershading. However, he overstated the case in the 1909 book \"Concealing-Coloration in the Animal Kingdom\", arguing that \"All patterns and colors whatsoever of all animals that ever preyed or are preyed on are under certain normal circumstances obliterative\" (that is, cryptic camouflage), and that \"Not one 'mimicry' mark, not one 'warning color'... nor any 'sexually selected' color, exists anywhere in the world where there is not every reason to believe it the very best conceivable device for the concealment of its wearer\", and using paintings such as \"Peacock in the Woods\" (1907) to reinforce his argument. Thayer was roundly mocked for these views by critics including Teddy Roosevelt.\n\nThe English zoologist Hugh Cott's 1940 book \"Adaptive Coloration in Animals\" corrected Thayer's errors, sometimes sharply: \"Thus we find Thayer straining the theory to a fantastic extreme in an endeavour to make it cover almost every type of coloration in the animal kingdom.\" Cott built on Thayer's discoveries, developing a comprehensive view of camouflage based on \"maximum disruptive contrast\", countershading and hundreds of examples. The book explained how disruptive camouflage worked, using streaks of boldly contrasting colour, paradoxically making objects less visible by breaking up their outlines. While Cott was more systematic and balanced in his view than Thayer, and did include some experimental evidence on the effectiveness of camouflage, his 500-page textbook was, like Thayer's, mainly a natural history narrative which illustrated theories with examples.\n\nCamouflage is a soft-tissue feature that is rarely preserved in the fossil record, but rare fossilised skin samples from the Cretaceous period show that some marine reptiles were countershaded. The skins, pigmented with dark-coloured eumelanin, reveal that both leatherback turtles and mosasaurs had dark backs and light bellies.\n\nShip camouflage was occasionally used in ancient times. Philostratus (c. 172–250 AD) wrote in his \"Imagines\" that Mediterranean pirate ships could be painted blue-gray for concealment. Vegetius (c. 360–400 AD) says that \"Venetian blue\" (sea green) was used in the Gallic Wars, when Julius Caesar sent his \"speculatoria navigia\" (reconnaissance boats) to gather intelligence along the coast of Britain; the ships were painted entirely in bluish-green wax, with sails, ropes and crew the same colour. There is little evidence of military use of camouflage on land before 1800, but two unusual ceramics show men in Peru's Mochica culture from before 500 AD, hunting birds with blowpipes which are fitted with a kind of shield near the mouth, perhaps to conceal the hunters' hands and faces. Another early source is a 15th-century French manuscript, \"The Hunting Book of Gaston Phebus\", showing a horse pulling a cart which contains a hunter armed with a crossbow under a cover of branches, perhaps serving as a hide for shooting game. Jamaican Maroons are said to have used plant materials as camouflage in the First Maroon War (c. 1655–1740).\n\nThe development of military camouflage was driven by the increasing range and accuracy of infantry firearms in the 19th century. In particular the replacement of the inaccurate musket with weapons such as the Baker rifle made personal concealment in battle essential. Two Napoleonic War skirmishing units of the British Army, the 95th Rifle Regiment and the 60th Rifle Regiment, were the first to adopt camouflage in the form of a rifle green jacket, while the Line regiments continued to wear scarlet tunics. A contemporary study in 1800 by the English artist and soldier Charles Hamilton Smith provided evidence that grey uniforms were less visible than green ones at a range of 150 yards.\n\nIn the American Civil War, rifle units such as the 1st United States Sharp Shooters (in the Federal army) similarly wore green jackets while other units wore more conspicuous colours. The first British Army unit to adopt khaki uniforms was the Corps of Guides at Peshawar, when Sir Harry Lumsden and his second in command, William Hodson introduced a \"drab\" uniform in 1848. Hodson wrote that it would be more appropriate for the hot climate, and help make his troops \"invisible in a land of dust\". Later they improvised by dyeing cloth locally. Other regiments in India soon adopted the khaki uniform, and by 1896 khaki drill uniform was used everywhere outside Europe; by the Second Boer War six years later it was used throughout the British Army.\n\nIn the First World War, the French army formed a camouflage corps, led by Lucien-Victor Guirand de Scévola, employing artists known as \"camoufleurs\" to create schemes such as tree observation posts and covers for guns. Other armies soon followed them. The term \"camouflage\" probably comes from \"camoufler\", a Parisian slang term meaning \"to disguise\", and may have been influenced by \"camouflet\", a French term meaning \"smoke blown in someone's face\". The English zoologist John Graham Kerr, artist Solomon J. Solomon and the American artist Abbott Thayer led attempts to introduce scientific principles of countershading and disruptive patterning into military camouflage, with limited success.\n\nShip camouflage was introduced in the early 20th century as the range of naval guns increased, with ships painted grey all over. In April 1917, when German U-boats were sinking many British ships with torpedoes, the marine artist Norman Wilkinson devised dazzle camouflage, which paradoxically made ships more visible but harder to target. In Wilkinson's own words, dazzle was designed \"not for low visibility, but in such a way as to break up her form and thus confuse a submarine officer as to the course on which she was heading\".\n\nIn the Second World War, the zoologist Hugh Cott, a protégé of Kerr, worked to persuade the British army to use more effective camouflage techniques, including countershading, but, like Kerr and Thayer in the First World War, with limited success. For example, he painted two rail-mounted coastal guns, one in conventional style, one countershaded. In aerial photographs, the countershaded gun was essentially invisible. The power of aerial observation and attack led every warring nation to camouflage targets of all types. The Soviet Union's Red Army created the comprehensive doctrine of \"Maskirovka\" for military deception, including the use of camouflage. For example, during the Battle of Kursk, General Katukov, the commander of the Soviet 1st Tank Army, remarked that the enemy \"did not suspect that our well-camouflaged tanks were waiting for him. As we later learned from prisoners, we had managed to move our tanks forward unnoticed\". The tanks were concealed in previously prepared defensive emplacements, with only their turrets above ground level. In the air, Second World War fighters were often painted in ground colours above and sky colours below, attempting two different camouflage schemes for observers above and below. Bombers and night fighters were often black, while maritime reconnaissance planes were usually white, to avoid appearing as dark shapes against the sky. For ships, dazzle camouflage was mainly replaced with plain grey in the Second World War, though experimentation with colour schemes continued.\n\nAs in the First World War, artists were pressed into service; for example, the surrealist painter Roland Penrose became a lecturer at the newly founded Camouflage Development and Training Centre at Farnham Castle, writing the practical \"Home Guard Manual of Camouflage\". The film-maker Geoffrey Barkas ran the Middle East Command Camouflage Directorate during the 1941–1942 war in the Western Desert, including the successful deception of Operation Bertram. Hugh Cott was chief instructor; the artist camouflage officers, who called themselves \"camoufleurs\", included Steven Sykes and Tony Ayrton. In Australia, artists were also prominent in the Sydney Camouflage Group, formed under the chairmanship of Professor William John Dakin, a zoologist from Sydney University. Max Dupain, Sydney Ure Smith and William Dobell were among the members of the group, which worked at Bankstown Airport, RAAF Base Richmond and Garden Island Dockyard.\n\nCamouflage has been used to protect military equipment such as vehicles, guns, ships, aircraft and buildings as well as individual soldiers and their positions.\nVehicle camouflage techniques begin with paint, which offers at best only limited effectiveness. Other methods for stationary land vehicles include covering with improvised materials such as blankets and vegetation, and erecting nets, screens and soft covers which may suitably reflect, scatter or absorb near infrared and radar waves. Some military textiles and vehicle camouflage paints also reflect infrared to help provide concealment from night vision devices.\nAfter the Second World War, radar made camouflage generally less effective, though coastal boats are sometimes painted like land vehicles. Aircraft camouflage too came to be seen as less important because of radar, and aircraft of different air forces, such as the Royal Air Force's Lightning, were often uncamouflaged.\n\nMany camouflaged textile patterns have been developed to suit the need to match combat clothing to different kinds of terrain (such as woodland, snow, and desert). The design of a pattern effective in all terrains has proved elusive. The American Universal Camouflage Pattern of 2004 attempted to suit all environments, but was withdrawn after a few years of service. Terrain-specific patterns have sometimes been developed but are ineffective in other terrains. The problem of making a pattern that works at different ranges has been solved with pixellated shapes, often designed digitally, that provide a fractal-like range of patch sizes so they appear disruptively coloured both at close range and at a distance. The first genuinely digital camouflage pattern was the Canadian CADPAT, issued to the army in 2002, soon followed by the American MARPAT. A pixellated appearance is not essential for this effect, though it is simpler to design and to print.\n\nCamouflage can be achieved by different methods, described below. Most of the methods contribute to crypsis, helping to hide against a background; but mimesis and motion dazzle protect without hiding. Methods may be applied on their own or in combination.\n\nCrypsis means making the animal or military equipment hard to see (or to detect in other ways, such as by sound or scent). Visual crypsis can be achieved in many different ways, such as by living underground or by being active only at night, as well as by a variety of methods of camouflage.\n\nSome animals' colours and patterns resemble a particular natural background. This is an important component of camouflage in all environments. For instance, tree-dwelling parakeets are mainly green; woodcocks of the forest floor are brown and speckled; reedbed bitterns are streaked brown and buff; in each case the animal's coloration matches the hues of its habitat. Similarly, desert animals are almost all desert coloured in tones of sand, buff, ochre, and brownish grey, whether they are mammals like the gerbil or fennec fox, birds such as the desert lark or sandgrouse, or reptiles like the skink or horned viper. Military uniforms, too, generally resemble their backgrounds; for example khaki uniforms are a muddy or dusty colour, originally chosen for service in South Asia. Many moths show industrial melanism, including the peppered moth which has coloration that blends in with tree bark. The coloration of these insects evolved between 1860 and 1940 to match the changing colour of the tree trunks on which they rest, from pale and mottled to almost black in polluted areas. This is taken by zoologists as evidence that camouflage is influenced by natural selection, as well as demonstrating that it changes where necessary to resemble the local background.\n\nDisruptive patterns use strongly contrasting, non-repeating markings such as spots or stripes to break up the outlines of an animal or military vehicle, or to conceal telltale features, especially the eyes, as in the common frog. Disruptive patterns may use more than one method to defeat visual systems such as edge detection. Predators like the leopard use disruptive camouflage to help them approach prey, while potential prey like the Egyptian nightjar use it to avoid detection by predators. Disruptive patterning is common in military usage, both for uniforms and for military vehicles. Disruptive patterning, however, does not always achieve crypsis on its own, as an animal or a military target may be given away by factors like shape, shine, and shadow.\n\nThe presence of bold skin markings does not in itself prove that an animal relies on camouflage, as that depends on its behaviour. For example, although giraffes have a high contrast pattern that could be disruptive coloration, the adults are extremely conspicuous when in the open. Some authors have argued that adult giraffes are cryptic, since when standing among trees and bushes they are hard to see at even a few metres' distance. However, adult giraffes move about to gain the best view of an approaching predator, relying on their size and ability to defend themselves, even from lions, rather than on camouflage. A different explanation is implied by the fact that young giraffes are far more vulnerable to predation than adults: more than half of all giraffe calves die within a year, and giraffe mothers hide their calves, which spend much of the time lying down in cover while their mothers are away feeding. Since the presence of a mother nearby does not affect survival, it is argued that young giraffes must be extremely well camouflaged; this is supported by the fact that coat markings are strongly inherited.\n\nSome animals, such as the horned lizards of North America, have evolved elaborate measures to eliminate shadow. Their bodies are flattened, with the sides thinning to an edge; the animals habitually press their bodies to the ground; and their sides are fringed with white scales which effectively hide and disrupt any remaining areas of shadow there may be under the edge of the body. The theory that the body shape of the horned lizards which live in open desert is adapted to minimise shadow is supported by the one species which lacks fringe scales, the roundtail horned lizard, which lives in rocky areas and resembles a rock. When this species is threatened, it makes itself look as much like a rock as possible by curving its back, emphasizing its three-dimensional shape. Some species of butterflies, such as the speckled wood, \"Pararge aegeria\", minimise their shadows when perched by closing the wings over their backs, aligning their bodies with the sun, and tilting to one side towards the sun, so that the shadow becomes a thin inconspicuous line rather than a broad patch. Similarly, some ground-nesting birds including the European nightjar select a resting position facing the sun. The elimination of shadow was identified as a principle of military camouflage during the Second World War.\n\nSome animals actively seek to hide by decorating themselves with materials such as twigs, sand, or pieces of shell from their environment, to break up their outlines, to conceal the features of their bodies, and to match their backgrounds. For example, a caddis fly larva builds a decorated case and lives almost entirely inside it; a decorator crab covers its back with seaweed, sponges and stones. The nymph of the predatory masked bug uses its hind legs and a 'tarsal fan' to decorate its body with sand or dust. There are two layers of bristles (trichomes) over the body. On these, the nymph spreads an inner layer of fine particles and an outer layer of coarser particles. The camouflage may conceal the bug from both predators and prey.\n\nSimilar principles can be applied for military purposes, for instance when a sniper wears a ghillie suit designed to be further camouflaged by decoration with materials such as tufts of grass from the sniper's immediate environment. Such suits were used as early as 1916, the British army having adopted \"coats of motley hue and stripes of paint\" for snipers. Cott takes the example of the larva of the blotched emerald moth, which fixes a screen of fragments of leaves to its specially hooked bristles, to argue that military camouflage uses the same method, pointing out that the \"device is ... essentially the same as one widely practised during the Great War for the concealment, not of caterpillars, but of caterpillar-tractors, [gun] battery positions, observation posts and so forth.\"\n\nMovement catches the eye of prey animals on the lookout for predators, and of predators hunting for prey. Most methods of crypsis therefore also require suitable cryptic behaviour, such as lying down and keeping still to avoid being detected, or in the case of stalking predators such as the tiger, moving with extreme stealth, both slowly and quietly, watching its prey for any sign they are aware of its presence. As an example of the combination of behaviours and other methods of crypsis involved, young giraffes seek cover, lie down, and keep still, often for hours until their mothers return; their skin pattern blends with the pattern of the vegetation, while the chosen cover and lying position together hide the animals' shadows. The flat-tail horned lizard similarly relies on a combination of methods: it is adapted to lie flat in the open desert, relying on stillness, its cryptic coloration, and concealment of its shadow to avoid being noticed by predators. In the ocean, the leafy sea dragon sways mimetically, like the seaweeds amongst which it rests, as if rippled by wind or water currents. Swaying is seen also in some insects, like Macleay's Spectre stick insect, \"Extatosoma tiaratum\". The behaviour may be motion crypsis, preventing detection, or motion masquerade, promoting misclassification (as something other than prey), or a combination of the two.\n\nMost forms of camouflage are ineffective when the camouflaged animal or object moves, because the motion is easily seen by the observing predator, prey or enemy. However, insects such as hoverflies and dragonflies use motion camouflage: the hoverflies to approach possible mates, and the dragonflies to approach rivals when defending territories. Motion camouflage is achieved by moving so as to stay on a straight line between the target and a fixed point in the landscape; the pursuer thus appears not to move, but only to loom larger in the target's field of vision. The same technique can be used for military purposes, for example by missiles to minimise their risk of detection by the enemy. However, missile engineers, and animals such as bats, use the technique primarily for its efficiency rather than camouflage.\n\nAnimals such as chameleon, frog, flatfish such as the peacock flounder, squid and octopus actively change their skin patterns and colours using special chromatophore cells to resemble their current background, or, as in most chameleons, for signalling. However, Smith's dwarf chameleon does use active colour change for camouflage.\nEach chromatophore contains pigment of only one colour. In fish and frogs, colour change is mediated by the type of chromatophores known as melanophores that contain dark pigment. A melanophore is star-shaped; it contains many small pigmented organelles which can be dispersed throughout the cell, or aggregated near its centre. When the pigmented organelles are dispersed, the cell makes a patch of the animal's skin appear dark; when they are aggregated, most of the cell, and the animal's skin, appears light. In frogs, the change is controlled relatively slowly, mainly by hormones. In fish, the change is controlled by the brain, which sends signals directly to the chromatophores, as well as producing hormones.\n\nThe skins of cephalopods such as the octopus contain complex units, each consisting of a chromatophore with surrounding muscle and nerve cells. The cephalopod chromatophore has all its pigment grains in a small elastic sac, which can be stretched or allowed to relax under the control of the brain to vary its opacity. By controlling chromatophores of different colours, cephalopods can rapidly change their skin patterns and colours.\n\nOn a longer timescale, animals like the Arctic hare, Arctic fox, stoat, and rock ptarmigan have snow camouflage, changing their coat colour (by moulting and growing new fur or feathers) from brown or grey in the summer to white in the winter; the Arctic fox is the only species in the dog family to do so. However, Arctic hares which live in the far north of Canada, where summer is very short, remain white year-round.\n\nThe principle of varying coloration either rapidly or with the changing seasons has military applications. \"Active camouflage\" could in theory make use of both dynamic colour change and counterillumination. Simple techniques such as changing uniforms and repainting vehicles for winter have been in use since the Second World War. In 2011, BAE Systems announced their Adaptiv infrared camouflage technology. It uses about 1000 hexagonal panels to cover the sides of a tank. The Peltier plate panels are heated and cooled to match either the vehicle's surroundings (crypsis), or an object such as a car (mimesis), when viewed in infrared.\n\nCountershading uses graded colour to counteract the effect of self-shadowing, creating an illusion of flatness. Self-shadowing makes an animal appear darker below than on top, grading from light to dark; countershading 'paints in' tones which are darkest on top, lightest below, making the countershaded animal nearly invisible against a suitable background. Thayer observed that \"Animals are painted by Nature, darkest on those parts which tend to be most lighted by the sky's light, and \"vice versa\"\". Accordingly, the principle of countershading is sometimes called \"Thayer's Law\". Countershading is widely used by terrestrial animals, such as gazelles and grasshoppers; marine animals, such as sharks and dolphins; and birds, such as snipe and dunlin.\n\nCountershading is less often used for military camouflage, despite Second World War experiments that showed its effectiveness. English zoologist Hugh Cott encouraged the use of techniques including countershading, but despite his authority on the subject, failed to persuade the British authorities. Soldiers often wrongly viewed camouflage netting as a kind of invisibility cloak, and they had to be taught to look at camouflage practically, from the enemy observer's point of view. At the same time in Australia, zoologist William John Dakin advised soldiers to copy animals' methods, using their instincts for wartime camouflage.\n\nThe term countershading has a second meaning unrelated to \"Thayer's Law\". It is that the upper and undersides of animals such as sharks, and of some military aircraft, are different colours to match the different backgrounds when seen from above or from below. Here the camouflage consists of two surfaces, each with the simple function of providing concealment against a specific background, such as a bright water surface or the sky. The body of a shark or the fuselage of an aircraft is not gradated from light to dark to appear flat when seen from the side. The camouflage techniques used are the matching of background colour and pattern, and disruption of outlines.\n\nCounter-illumination means producing light to match a background that is brighter than an animal's body or military vehicle; it is a form of active camouflage. It is notably used by some species of squid, such as the firefly squid and the midwater squid. The latter has light-producing organs (photophores) scattered all over its underside; these create a sparkling glow that prevents the animal from appearing as a dark shape when seen from below. Counterillumination camouflage is the likely function of the bioluminescence of many marine organisms, though light is also produced to attract or to detect prey and for signalling.\n\nCounterillumination has rarely been used for military purposes. \"Diffused lighting camouflage\" was trialled by Canada's National Research Council during the Second World War. It involved projecting light on to the sides of ships to match the faint glow of the night sky, requiring awkward external platforms to support the lamps. The Canadian concept was refined in the American Yehudi lights project, and trialled in aircraft including B-24 Liberators and naval Avengers. The planes were fitted with forward-pointing lamps automatically adjusted to match the brightness of the night sky. This enabled them to approach much closer to a target – within 3,000 yards (2,700 metres) – before being seen. Counterillumination was made obsolete by radar, and neither diffused lighting camouflage nor Yehudi lights entered active service.\n\nMany marine animals that float near the surface are highly transparent, giving them almost perfect camouflage. However, transparency is difficult for bodies made of materials that have different refractive indices from seawater. Some marine animals such as jellyfish have gelatinous bodies, composed mainly of water; their thick mesogloea is acellular and highly transparent. This conveniently makes them buoyant, but it also makes them large for their muscle mass, so they cannot swim fast, making this form of camouflage a costly trade-off with mobility. Gelatinous planktonic animals are between 50 and 90 percent transparent. A transparency of 50 percent is enough to make an animal invisible to a predator such as cod at a depth of ; better transparency is required for invisibility in shallower water, where the light is brighter and predators can see better. For example, a cod can see prey that are 98 percent transparent in optimal lighting in shallow water. Therefore, sufficient transparency for camouflage is more easily achieved in deeper waters.\nSome tissues such as muscles can be made transparent, provided either they are very thin or organised as regular layers or fibrils that are small compared to the wavelength of visible light. A familiar example is the transparency of the lens of the vertebrate eye, which is made of the protein crystallin, and the vertebrate cornea which is made of the protein collagen. Other structures cannot be made transparent, notably the retinas or equivalent light-absorbing structures of eyes — they must absorb light to be able to function. The camera-type eye of vertebrates and cephalopods must be completely opaque. Finally, some structures are visible for a reason, such as to lure prey. For example, the nematocysts (stinging cells) of the transparent siphonophore \"Agalma okenii\" resemble small copepods. Examples of transparent marine animals include a wide variety of larvae, including coelenterates, siphonophores, salps (floating tunicates), gastropod molluscs, polychaete worms, many shrimplike crustaceans, and fish; whereas the adults of most of these are opaque and pigmented, resembling the seabed or shores where they live. Adult comb jellies and jellyfish obey the rule, often being mainly transparent. Cott suggests this follows the more general rule that animals resemble their background: in a transparent medium like seawater, that means actually being transparent. The small Amazon river fish \"Microphilypnus amazonicus\" and the shrimps it associates with, \"Pseudopalaemon gouldingi\", are so transparent as to be \"almost invisible\"; further, these species appear to select whether to be transparent or more conventionally mottled (disruptively patterned) according to the local background in the environment.\n\nWhere transparency cannot be achieved, it can be imitated effectively by silvering to make an animal's body highly reflective. At medium depths at sea, light comes from above, so a mirror oriented vertically makes animals such as fish invisible from the side. Most fish in the upper ocean such as sardine and herring are camouflaged by silvering.\n\nThe marine hatchetfish is extremely flattened laterally, leaving the body just millimetres thick, and the body is so silvery as to resemble aluminium foil. The mirrors consist of microscopic structures similar to those used to provide structural coloration: stacks of between 5 and 10 crystals of guanine spaced about ¼ of a wavelength apart to interfere constructively and achieve nearly 100 per cent reflection. In the deep waters that the hatchetfish lives in, only blue light with a wavelength of 500 nanometres percolates down and needs to be reflected, so mirrors 125 nanometres apart provide good camouflage.\n\nIn fish such as the herring which live in shallower water, the mirrors must reflect a mixture of wavelengths, and the fish accordingly has crystal stacks with a range of different spacings. A further complication for fish with bodies that are rounded in cross-section is that the mirrors would be ineffective if laid flat on the skin, as they would fail to reflect horizontally. The overall mirror effect is achieved with many small reflectors, all oriented vertically. Silvering is found in other marine animals as well as fish. The cephalopods, including squid, octopus and cuttlefish, have multi-layer mirrors made of protein rather than guanine.\n\nIn mimesis (also called \"masquerade\"), the camouflaged object looks like something else which is of no special interest to the observer. Mimesis is common in prey animals, for example when a peppered moth caterpillar mimics a twig, or a grasshopper mimics a dry leaf. It is also found in nest structures; some eusocial wasps, such as \"Leipomeles dorsata\", build a nest envelope in patterns that mimic the leaves surrounding the nest.\n\nMimesis is also employed by some predators and parasites to lure their prey. For example, a flower mantis mimics a particular kind of flower, such as an orchid. This tactic has occasionally been used in warfare, for example with heavily armed Q-ships disguised as merchant ships.\n\nThe common cuckoo, a brood parasite, provides examples of mimesis both in the adult and in the egg. The female lays her eggs in nests of other, smaller species of bird, one per nest. The female mimics a sparrowhawk. The resemblance is sufficient to make small birds take action to avoid the apparent predator. The female cuckoo then has time to lay her egg in their nest without being seen to do so. The cuckoo's egg itself mimics the eggs of the host species, reducing its chance of being rejected.\n\nMost forms of camouflage are made ineffective by movement: a deer or grasshopper may be highly cryptic when motionless, but instantly seen when it moves. But one method, motion dazzle, requires rapidly moving bold patterns of contrasting stripes. Motion dazzle may degrade predators' ability to estimate the prey's speed and direction accurately, giving the prey an improved chance of escape. Motion dazzle distorts speed perception and is most effective at high speeds; stripes can also distort perception of size (and so, perceived range to the target). As of 2011, motion dazzle had been proposed for military vehicles, but never applied. Since motion dazzle patterns would make animals more difficult to locate accurately when moving, but easier to see when stationary, there would be an evolutionary trade-off between motion dazzle and crypsis.\n\nAn animal that is commonly thought to be dazzle-patterned is the zebra. The bold stripes of the zebra have been claimed to be disruptive camouflage, background-blending and countershading. After many years in which the purpose of the coloration was disputed, an experimental study by Tim Caro suggested in 2012 that the pattern reduces the attractiveness of stationary models to biting flies such as horseflies and tsetse flies. However, a simulation study by Martin How and Johannes Zanker in 2014 suggests that when moving, the stripes may confuse observers, such as mammalian predators and biting insects, by two visual illusions: the wagon-wheel effect, where the perceived motion is inverted, and the barberpole illusion, where the perceived motion is in a wrong direction.\n\nCamouflage is occasionally used to make buildings less conspicuous: for example, in South Africa, towers carrying cell telephone antennae are sometimes camouflaged as tall trees with plastic branches, in response to \"resistance from the community\". Since this method is costly (a figure of three times the normal cost is mentioned), alternative forms of camouflage can include using neutral colours or familiar shapes such as cylinders and flagpoles. Conspicuousness can also be reduced by siting masts near or actually on other structures.\n\nHunters of game have long made use of camouflage in the form of materials such as animal skins, mud, foliage, and green or brown clothing to enable them to approach wary game animals. Field sports such as driven grouse shooting conceal hunters in hides (also called blinds or shooting butts). Modern hunting clothing makes use of fabrics that provide a disruptive camouflage pattern; for example, in 1986 the hunter Bill Jordan created cryptic clothing for hunters, printed with images of specific kinds of vegetation such as grass and branches.\n\nAutomotive manufacturers often use patterns to disguise upcoming products. This camouflage is designed to obfuscate the vehicle's visual lines, and is used along with padding, covers, and decals. The patterns' purpose is to prevent visual observation (and to a lesser degree photography), that would subsequently enable reproduction of the vehicle's form factors.\n\nMilitary camouflage patterns influenced fashion and art from the time of the First World War onwards. Gertrude Stein recalled the cubist artist Pablo Picasso's reaction in around 1915:\n\nIn 1919, the attendants of a \"dazzle ball\", hosted by the Chelsea Arts Club, wore dazzle-patterned black and white clothing. The ball influenced fashion and art via postcards and magazine articles. The \"Illustrated London News\" announced:\nMore recently, fashion designers have often used camouflage fabric for its striking designs, its \"patterned disorder\" and its symbolism. Camouflage clothing can be worn largely for its symbolic significance rather than for fashion, as when, during the late 1960s and early 1970s in the United States, anti-war protestors often ironically wore military clothing during demonstrations against the American involvement in the Vietnam War.\n\nModern artists such as Ian Hamilton Finlay have used camouflage to reflect on war. His 1973 screenprint of a tank camouflaged in a leaf pattern, \"Arcadia\", is described by the Tate as drawing \"an ironic parallel between this idea of a natural paradise and the camouflage patterns on a tank\". The title refers to the Utopian Arcadia of poetry and art, and the \"memento mori\" Latin phrase \"Et in Arcadia ego\" which recurs in Hamilton Finlay's work. In science fiction, \"Camouflage\" is a novel about shapeshifting alien beings by Joe Haldeman. The word is used more figuratively in works of literature such as Thaisa Frank's collection of stories of love and loss, \"A Brief History of Camouflage\".\n\n\n\n\n\n\n"}
{"id": "6449", "url": "https://en.wikipedia.org/wiki?curid=6449", "title": "Clock", "text": "Clock\n\nA clock is an instrument to measure, keep, and indicate time. The word \"clock\" is derived (via Dutch, Northern French, and Medieval Latin) from the Celtic words \"clagan\" and \"clocca\" meaning \"bell\". A silent instrument missing such a striking mechanism has traditionally been known as a timepiece. In general usage today a \"clock\" refers to any device for measuring and displaying the time. Watches and other timepieces that can be carried on one's person are often distinguished from clocks.\n\nThe clock is one of the oldest human inventions, meeting the need to consistently measure intervals of time shorter than the natural units: the day, the lunar month, and the year. Devices operating on several physical processes have been used over the millennia. A sundial shows the time by displaying the position of a shadow on a flat surface. There is a range of duration timers, a well-known example being the hourglass. Water clocks, along with the sundials, are possibly the oldest time-measuring instruments. A major advance occurred with the invention of the verge escapement, which made possible the first mechanical clocks around 1300 in Europe, which kept time with oscillating timekeepers like balance wheels. Spring-driven clocks appeared during the 15th century. During the 15th and 16th centuries, clockmaking flourished. The next development in accuracy occurred after 1656 with the invention of the pendulum clock. A major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The electric clock was patented in 1840. The development of electronics in the 20th century led to clocks with no clockwork parts at all.\n\nThe timekeeping element in every modern clock is a harmonic oscillator, a physical object (resonator) that vibrates or oscillates at a particular frequency.\nThis object can be a pendulum, a tuning fork, a quartz crystal, or the vibration of electrons in atoms as they emit microwaves. Analog clocks usually indicate time using angles. Digital clocks display a numeric representation of time. Two numeric display formats are commonly used on digital clocks: 24-hour notation and 12-hour notation. Most digital clocks use electronic mechanisms and LCD, LED, or VFD displays. For convenience, distance, telephony or blindness, auditory clocks present the time as sounds. There are also clocks for the blind that have displays that can be read by using the sense of touch. Some of these are similar to normal analog displays, but are constructed so the hands can be felt without damaging them. The evolution of the technology of clocks continues today. The study of timekeeping is known as horology.\n\nThe apparent position of the Sun in the sky moves over the course of a day, reflecting the rotation of the Earth. Shadows cast by stationary objects move correspondingly, so their positions can be used to indicate the time of day. A sundial shows the time by displaying the position of a shadow on a (usually) flat surface, which has markings that correspond to the hours. Sundials can be horizontal, vertical, or in other orientations. Sundials were widely used in ancient times. With the knowledge of latitude, a well-constructed sundial can measure local solar time with reasonable accuracy, within a minute or two. Sundials continued to be used to monitor the performance of clocks until the modern era. However, practical limitations, such as that sundials work only when the Sun shines, and never during the night, encouraged the use of other techniques for measuring and displaying time.The Jantar Mantar At Delhi and Jaipur are examples of sundials . The were built by Maharaja Jai Singh II .\n\nMany devices can be used to mark passage of time without respect to reference time (time of day, minutes, etc.) and can be useful for measuring duration and/or intervals. Examples of such duration timers are, candle clocks, incense clocks and the hourglass. Both the candle clock and the incense clock work on the same principle wherein the consumption of resources is more or less constant allowing reasonably precise, and repeatable, estimates of time passages. In the hourglass, fine sand pouring through a tiny hole at a constant rate indicates an arbitrary, predetermined, passage of time, the resource is not consumed but re-used.\n\nWater clocks, also known as clepsydrae (sg: \"clepsydra\"), along with the sundials, are possibly the oldest time-measuring instruments, with the only exceptions being the vertical gnomon and the day counting tally stick. Given their great antiquity, where and when they first existed is not known and perhaps unknowable. The bowl-shaped outflow is the simplest form of a water clock and is known to have existed in Babylon and in Egypt around the 16th century BC. Other regions of the world, including India and China, also have early evidence of water clocks, but the earliest dates are less certain. Some authors, however, write about water clocks appearing as early as 4000 BC in these regions of the world.\n\nGreek astronomer Andronicus of Cyrrhus supervised the construction of the Tower of the Winds in Athens in the 1st century B.C. The Greek and Roman civilizations are credited for initially advancing water clock design to include complex gearing, which was connected to fanciful automata and also resulted in improved accuracy. These advances were passed on through Byzantium and Islamic times, eventually making their way back to Europe. Independently, the Chinese developed their own advanced water clocks（水鐘）in 725 A.D., passing their ideas on to Korea and Japan.\n\nSome water clock designs were developed independently and some knowledge was transferred through the spread of trade. Pre-modern societies do not have the same precise timekeeping requirements that exist in modern industrial societies, where every hour of work or rest is monitored, and work may start or finish at any time regardless of external conditions. Instead, water clocks in ancient societies were used mainly for astrological reasons. These early water clocks were calibrated with a sundial. While never reaching the level of accuracy of a modern timepiece, the water clock was the most accurate and commonly used timekeeping device for millennia, until it was replaced by the more accurate pendulum clock in 17th-century Europe.\n\nIslamic civilization is credited with further advancing the accuracy of clocks with elaborate engineering. In 797 (or possibly 801), the Abbasid caliph of Baghdad, Harun al-Rashid, presented Charlemagne with an Asian Elephant named Abul-Abbas together with a \"particularly elaborate example\" of a water clock.\nPope Sylvester II introduced clocks to northern and western Europe around 1000AD\n\nIn the 13th century, Al-Jazari, an engineer from Mesopotamia (lived 1136–1206) who worked for Artuqid king of Diyar-Bakr, Nasir al-Din, made numerous clocks of all shapes and sizes. A book on his work described 50 mechanical devices in 6 categories, including water clocks. The most reputed clocks included the Elephant, Scribe and Castle clocks, all of which have been successfully reconstructed. As well as telling the time, these grand clocks were symbols of status, grandeur and wealth of the Urtuq State.\n\nThe word \"horologia\" (from the Greek ὡρα, hour, and λέγειν, to tell) was used to describe early mechanical clocks, but the use of this word (still used in several Romance languages) for all timekeepers conceals the true nature of the mechanisms. For example, there is a record that in 1176 Sens Cathedral installed a ‘horologe’ but the mechanism used is unknown. According to Jocelin of Brakelond, in 1198 during a fire at the abbey of St Edmundsbury (now Bury St Edmunds), the monks 'ran to the clock' to fetch water, indicating that their water clock had a reservoir large enough to help extinguish the occasional fire. The word \"clock\" (from the Celtic words \"clocca\" and \"clogan\", both meaning \"bell\"), which gradually supersedes \"horologe\", suggests that it was the sound of bells which also characterized the prototype mechanical clocks that appeared during the 13th century in Europe.\n\nA water-powered cogwheel clock was created in China in AD 725 by Yi Xing and Liang Lingzan. This is not considered an escapement mechanism clock as it was unidirectional, the Song dynasty polymath and genius Su Song (1020–1101) incorporated it into his monumental innovation of the astronomical clock-tower of Kaifeng in 1088. His astronomical clock and rotating armillary sphere still relied on the use of either flowing water during the spring, summer, autumn seasons and liquid mercury during the freezing temperature of winter (i.e. hydraulics). A mercury clock, described in the \"Libros del saber\", a Spanish work from 1277 consisting of translations and paraphrases of Arabic works, is sometimes quoted as evidence for Muslim knowledge of a mechanical clock. A mercury-powered cogwheel clock was created by Ibn Khalaf al-Muradi\n\nIn Europe, between 1280 and 1320, there is an increase in the number of references to clocks and horologes in church records, and this probably indicates that a new type of clock mechanism had been devised. Existing clock mechanisms that used water power were being adapted to take their driving power from falling weights. This power was controlled by some form of oscillating mechanism, probably derived from existing bell-ringing or alarm devices. This controlled release of power—the escapement—marks the beginning of the true mechanical clock, which differed from the previously mentioned cogwheel clocks. Verge escapement mechanism derived in the surge of true mechanical clocks, which didn't need any kind of fluid power, like water or mercury, to work.\n\nThese mechanical clocks were intended for two main purposes: for signalling and notification (e.g. the timing of services and public events), and for modeling the solar system. The former purpose is administrative, the latter arises naturally given the scholarly interests in astronomy, science, astrology, and how these subjects integrated with the religious philosophy of the time. The astrolabe was used both by astronomers and astrologers, and it was natural to apply a clockwork drive to the rotating plate to produce a working model of the solar system.\n\nSimple clocks intended mainly for notification were installed in towers, and did not always require faces or hands. They would have announced the canonical hours or intervals between set times of prayer. Canonical hours varied in length as the times of sunrise and sunset shifted. The more sophisticated astronomical clocks would have had moving dials or hands, and would have shown the time in various time systems, including Italian hours, canonical hours, and time as measured by astronomers at the time. Both styles of clock started acquiring extravagant features such as automata.\n\nIn 1283, a large clock was installed at Dunstable Priory; its location above the rood screen suggests that it was not a water clock. In 1292, Canterbury Cathedral installed a 'great horloge'. Over the next 30 years there are mentions of clocks at a number of ecclesiastical institutions in England, Italy, and France. In 1322, a new clock was installed in Norwich, an expensive replacement for an earlier clock installed in 1273. This had a large (2 metre) astronomical dial with automata and bells. The costs of the installation included the full-time employment of two clockkeepers for two years.\n\nBesides the Chinese astronomical clock of Su Song in 1088 mentioned above, in Europe there were the clocks constructed by Richard of Wallingford in St Albans by 1336, and by Giovanni de Dondi in Padua from 1348 to 1364. They no longer exist, but detailed descriptions of their design and construction survive, and modern reproductions have been made. They illustrate how quickly the theory of the mechanical clock had been translated into practical constructions, and also that one of the many impulses to their development had been the desire of astronomers to investigate celestial phenomena.\n\nWallingford's clock had a large astrolabe-type dial, showing the sun, the moon's age, phase, and node, a star map, and possibly the planets. In addition, it had a wheel of fortune and an indicator of the state of the tide at London Bridge. Bells rang every hour, the number of strokes indicating the time. Dondi's clock was a seven-sided construction, 1 metre high, with dials showing the time of day, including minutes, the motions of all the known planets, an automatic calendar of fixed and movable feasts, and an eclipse prediction hand rotating once every 18 years. It is not known how accurate or reliable these clocks would have been. They were probably adjusted manually every day to compensate for errors caused by wear and imprecise manufacture. Water clocks are sometimes still used today, and can be examined in places such as ancient castles and museums. The Salisbury Cathedral clock, built in 1386, is considered to be the world's oldest surviving mechanical clock that strikes the hours.\n\nClockmakers developed their art in various ways. Building smaller clocks was a technical challenge, as was improving accuracy and reliability. Clocks could be impressive showpieces to demonstrate skilled craftsmanship, or less expensive, mass-produced items for domestic use. The escapement in particular was an important factor affecting the clock's accuracy, so many different mechanisms were tried.\n\nSpring-driven clocks appeared during the 15th century, although they are often erroneously credited to Nuremberg watchmaker Peter Henlein (or Henle, or Hele) around 1511. The earliest existing spring driven clock is the chamber clock given to Phillip the Good, Duke of Burgundy, around 1430, now in the Germanisches Nationalmuseum. Spring power presented clockmakers with a new problem: how to keep the clock movement running at a constant rate as the spring ran down. This resulted in the invention of the \"stackfreed\" and the fusee in the 15th century, and many other innovations, down to the invention of the modern \"going barrel\" in 1760.\n\nEarly clock dials did not indicate minutes and seconds. A clock with a dial indicating minutes was illustrated in a 1475 manuscript by Paulus Almanus, and some 15th-century clocks in Germany indicated minutes and seconds.\nAn early record of a seconds hand on a clock dates back to about 1560 on a clock now in the Fremersdorf collection.\n\nDuring the 15th and 16th centuries, clockmaking flourished, particularly in the metalworking towns of Nuremberg and Augsburg, and in Blois, France. Some of the more basic table clocks have only one time-keeping hand, with the dial between the hour markers being divided into four equal parts making the clocks readable to the nearest 15 minutes. Other clocks were exhibitions of craftsmanship and skill, incorporating astronomical indicators and musical movements. The cross-beat escapement was invented in 1584 by Jost Bürgi, who also developed the remontoire. Bürgi's clocks were a great improvement in accuracy as they were correct to within a minute a day. These clocks helped the 16th-century astronomer Tycho Brahe to observe astronomical events with much greater precision than before.\n\nThe next development in accuracy occurred after 1656 with the invention of the pendulum clock. Galileo had the idea to use a swinging bob to regulate the motion of a time-telling device earlier in the 17th century. Christiaan Huygens, however, is usually credited as the inventor. He determined the mathematical formula that related pendulum length to time (99.38 cm or 39.13 inches for the one second movement) and had the first pendulum-driven clock made. The first model clock was built in 1657 in the Hague, but it was in England that the idea was taken up. The longcase clock (also known as the \"grandfather clock\") was created to house the pendulum and works by the English clockmaker William Clement in 1670 or 1671. It was also at this time that clock cases began to be made of wood and clock faces to utilize enamel as well as hand-painted ceramics.\n\nIn 1670, William Clement created the anchor escapement, an improvement over Huygens' crown escapement. Clement also introduced the pendulum suspension spring in 1671. The concentric minute hand was added to the clock by Daniel Quare, a London clockmaker and others, and the second hand was first introduced.\n\nIn 1675, Huygens and Robert Hooke invented the spiral balance, or the hairspring, designed to control the oscillating speed of the balance wheel. This crucial advance finally made accurate pocket watches possible. The great English clockmaker, Thomas Tompion, was one of the first to use this mechanism successfully in his pocket watches, and he adopted the minute hand which, after a variety of designs were trialled, eventually stabilised into the modern-day configuration. The Rev. Edward Barlow invented the rack and snail striking mechanism for striking clocks, which was a great improvement over the previous mechanism. The repeating clock, that chimes the number of hours (or even minutes) was invented by either Quare or Barlow in 1676. George Graham invented the deadbeat escapement for clocks in 1720.\n\nA major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The position of a ship at sea could be determined with reasonable accuracy if a navigator could refer to a clock that lost or gained less than about 10 seconds per day. This clock could not contain a pendulum, which would be virtually useless on a rocking ship. In 1714, the British government offered large financial rewards to the value of 20,000 pounds, for anyone who could determine longitude accurately. John Harrison, who dedicated his life to improving the accuracy of his clocks, later received considerable sums under the Longitude Act.\n\nIn 1735, Harrison built his first chronometer, which he steadily improved on over the next thirty years before submitting it for examination. The clock had many innovations, including the use of bearings to reduce friction, weighted balances to compensate for the ship's pitch and roll in the sea and the use of two different metals to reduce the problem of expansion from heat. The chronometer was tested in 1761 by Harrison's son and by the end of 10 weeks the clock was in error by less than 5 seconds.\n\n The British had predominated in watch manufacture for much of the 17th and 18th centuries, but maintained a system of production that was geared towards high quality products for the elite. Although there was an attempt to modernise clock manufacture with mass production techniques and the application of duplicating tools and machinery by the British Watch Company in 1843, it was in the United States that this system took off. In 1816, Eli Terry and some other Connecticut clockmakers developed a way of mass-producing clocks by using interchangeable parts. Aaron Lufkin Dennison started a factory in 1851 in Massachusetts that also used interchangeable parts, and by 1861 was running a successful enterprise incorporated as the Waltham Watch Company.\n\nIn 1815, Francis Ronalds published the first electric clock powered by dry pile batteries. Alexander Bain, Scottish clockmaker, patented the electric clock in 1840. The electric clock's mainspring is wound either with an electric motor or with an electromagnet and armature. In 1841, he first patented the electromagnetic pendulum. By the end of the nineteenth century, the advent of the dry cell battery made it feasible to use electric power in clocks. Spring or weight driven clocks that use electricity, either alternating current (AC) or direct current (DC), to rewind the spring or raise the weight of a mechanical clock would be classified as an electromechanical clock. This classification would also apply to clocks that employ an electrical impulse to propel the pendulum. In electromechanical clocks the electricity serves no time keeping function. These types of clocks were made as individual timepieces but more commonly used in synchronized time installations in schools, businesses, factories, railroads and government facilities as a master clock and slave clocks.\n\nElectric clocks that are powered from the AC supply often use synchronous motors. The supply current alternates with a frequency of 50 hertz in many countries, and 60 hertz in others. The rotor of the motor rotates at a speed that is related to the alternation frequency. Appropriate gearing converts this rotation speed to the correct ones for the hands of the analog clock. The development of electronics in the 20th century led to clocks with no clockwork parts at all. Time in these cases is measured in several ways, such as by the alternation of the AC supply, vibration of a tuning fork, the behaviour of quartz crystals, or the quantum vibrations of atoms. Electronic circuits divide these high-frequency oscillations to slower ones that drive the time display. Even mechanical clocks have since come to be largely powered by batteries, removing the need for winding.\n\nThe piezoelectric properties of crystalline quartz were discovered by Jacques and Pierre Curie in 1880. The first crystal oscillator was invented in 1917 by Alexander M. Nicholson after which, the first quartz crystal oscillator was built by Walter G. Cady in 1921. In 1927 the first quartz clock was built by Warren Marrison and J. W. Horton at Bell Telephone Laboratories in Canada. The following decades saw the development of quartz clocks as precision time measurement devices in laboratory settings—the bulky and delicate counting electronics, built with vacuum tubes, limited their practical use elsewhere. The National Bureau of Standards (now NIST) based the time standard of the United States on quartz clocks from late 1929 until the 1960s, when it changed to atomic clocks. In 1969, Seiko produced the world's first quartz wristwatch, the Astron. Their inherent accuracy and low cost of production resulted in the subsequent proliferation of quartz clocks and watches.\n\nAs of the 2010s, atomic clocks are the most accurate clocks in existence. They are considerably more accurate than quartz clocks as they can be accurate to within a few seconds over thousands of years. Atomic clocks were first theorized by Lord Kelvin in 1879. In the 1930s the development of Magnetic resonance created practical method for doing this. A prototype ammonia maser device was built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). Although it was less accurate than existing quartz clocks, it served to demonstrate the concept. The first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale \"ephemeris time\" (ET). As of 2013, the most stable atomic clocks are ytterbium clocks, which are stable to within less than two parts in 1 quintillion ().\n\nThe invention of the mechanical clock in the 13th century initiated a change in timekeeping methods from continuous processes, such as the motion of the gnomon's shadow on a sundial or the flow of liquid in a water clock, to periodic oscillatory processes, such as the swing of a pendulum or the vibration of a quartz crystal, which had the potential for more accuracy. All modern clocks use oscillation.\n\nAlthough the mechanisms they use vary, all oscillating clocks, mechanical, digital and atomic, work similarly and can be divided into analogous parts. They consist of an object that repeats the same motion over and over again, an \"oscillator\", with a precisely constant time interval between each repetition, or 'beat'. Attached to the oscillator is a \"controller\" device, which sustains the oscillator's motion by replacing the energy it loses to friction, and converts its oscillations into a series of pulses. The pulses are then counted by some type of \"counter\", and the number of counts is converted into convenient units, usually seconds, minutes, hours, etc. Finally some kind of \"indicator\" displays the result in human readable form.\n\nThis provides power to keep the clock going.\nThe timekeeping element in every modern clock is a harmonic oscillator, a physical object (resonator) that vibrates or oscillates repetitively at a precisely constant frequency.\nThe advantage of a harmonic oscillator over other forms of oscillator is that it employs resonance to vibrate at a precise natural resonant frequency or 'beat' dependent only on its physical characteristics, and resists vibrating at other rates. The possible precision achievable by a harmonic oscillator is measured by a parameter called its Q, or quality factor, which increases (other things being equal) with its resonant frequency. This is why there has been a long term trend toward higher frequency oscillators in clocks. Balance wheels and pendulums always include a means of adjusting the rate of the timepiece. Quartz timepieces sometimes include a rate screw that adjusts a capacitor for that purpose. Atomic clocks are primary standards, and their rate cannot be adjusted.\n\nSome clocks rely for their accuracy on an external oscillator; that is, they are automatically synchronized to a more accurate clock:\n\nThis has the dual function of keeping the oscillator running by giving it 'pushes' to replace the energy lost to friction, and converting its vibrations into a series of pulses that serve to measure the time.\nIn mechanical clocks, the low Q of the balance wheel or pendulum oscillator made them very sensitive to the disturbing effect of the impulses of the escapement, so the escapement had a great effect on the accuracy of the clock, and many escapement designs were tried. The higher Q of resonators in electronic clocks makes them relatively insensitive to the disturbing effects of the drive power, so the driving oscillator circuit is a much less critical component.\n\nThis counts the pulses and adds them up to get traditional time units of seconds, minutes, hours, etc. It usually has a provision for \"setting\" the clock by manually entering the correct time into the counter.\n\nThis displays the count of seconds, minutes, hours, etc. in a human readable form.\n\nClocks can be classified by the type of time display, as well as by the method of timekeeping.\n\nAnalog clocks usually use a clock face which indicates time using rotating pointers called \"hands\" on a fixed numbered dial or dials. The standard clock face, known universally throughout the world, has a short \"hour hand\" which indicates the hour on a circular dial of 12 hours, making two revolutions per day, and a longer \"minute hand\" which indicates the minutes in the current hour on the same dial, which is also divided into 60 minutes. It may also have a \"second hand\" which indicates the seconds in the current minute. The only other widely used clock face today is the 24 hour analog dial, because of the use of 24 hour time in military organizations and timetables. Before the modern clock face was standardized during the Industrial Revolution, many other face designs were used throughout the years, including dials divided into 6, 8, 10, and 24 hours. During the French Revolution the French government tried to introduce a 10-hour clock, as part of their the decimal-based metric system of measurement, but it didn't catch on. An Italian 6 hour clock was developed in the 18th century, presumably to save power (a clock or watch striking 24 times uses more power).\nAnother type of analog clock is the sundial, which tracks the sun continuously, registering the time by the shadow position of its gnomon. Because the sun does not adjust to daylight savings times, users must add an hour during that time. Corrections must also be made for the equation of time, and for the difference between the longitudes of the sundial and of the central meridian of the time zone that is being used (i.e. 15 degrees east of the prime meridian for each hour that the time zone is ahead of GMT). Sundials use some or part of the 24 hour analog dial. There also exist clocks which use a digital display despite having an analog mechanism—these are commonly referred to as flip clocks. Alternative systems have been proposed. For example, the \"Twelv\" clock indicates the current hour using one of twelve colors, and indicates the minute by showing a proportion of a circular disk, similar to a moon phase.\n\nDigital clocks display a numeric representation of time. Two numeric display formats are commonly used on digital clocks:\n\n\nMost digital clocks use electronic mechanisms and LCD, LED, or VFD displays; many other display technologies are used as well (cathode ray tubes, nixie tubes, etc.). After a reset, battery change or power failure, these clocks without a backup battery or capacitor either start counting from 12:00, or stay at 12:00, often with blinking digits indicating that the time needs to be set. Some newer clocks will reset themselves based on radio or Internet time servers that are tuned to national atomic clocks. Since the advent of digital clocks in the 1960s, the use of analog clocks has declined significantly.\n\nSome clocks, called 'flip clocks', have digital displays that work mechanically. The digits are painted on sheets of material which are mounted like the pages of a book. Once a minute, a page is turned over to reveal the next digit. These displays are usually easier to read in brightly lit conditions than LCDs or LEDs. Also, they do not go back to 12:00 after a power interruption. Flip clocks generally do not have electronic mechanisms. Usually, they are driven by AC-synchronous motors.\n\nFor convenience, distance, telephony or blindness, auditory clocks present the time as sounds. The sound is either spoken natural language, (e.g. \"The time is twelve thirty-five\"), or as auditory codes (e.g. number of sequential bell rings on the hour represents the number of the hour like the bell, Big Ben). Most telecommunication companies also provide a speaking clock service as well.\n\nWord clocks are clocks that display the time visually using sentences. E.g.: \"It’s about three o’clock.\" These clocks can be implemented in hardware or software.\n\nSome clocks, usually digital ones, include an optical projector that shines a magnified image of the time display onto a screen or onto a surface such as an indoor ceiling or wall. The digits are large enough to be easily read, without using glasses, by persons with moderately imperfect vision, so the clocks are convenient for use in their bedrooms. Usually, the timekeeping circuitry has a battery as a backup source for an uninterrupted power supply to keep the clock on time, while the projection light only works when the unit is connected to an A.C. supply. Completely battery-powered portable versions resembling flashlights are also available.\n\nAuditory and projection clocks can be used by people who are blind or have limited vision. There are also clocks for the blind that have displays that can be read by using the sense of touch. Some of these are similar to normal analog displays, but are constructed so the hands can be felt without damaging them. Another type is essentially digital, and uses devices that use a code such as Braille to show the digits so that they can be felt with the fingertips.\n\nSome clocks have several displays driven by a single mechanism, and some others have several completely separate mechanisms in a single case. Clocks in public places often have several faces visible from different directions, so that the clock can be read from anywhere in the vicinity. Of course, all the faces show the same time. Other clocks show the current time in several time-zones. Watches that are intended to be carried by travellers often have two displays, one for the local time and the other for the time at home, which is useful for making pre-arranged phone calls. Some equation clocks have two displays, one showing mean time and the other solar time, as would be shown by a sundial. Some clocks have both analog and digital displays. Clocks with Braille displays usually also have conventional digits so they can be read by sighted people.\n\nClocks are in homes, offices and many other places; smaller ones (watches) are carried on the wrist or in a pocket; larger ones are in public places, e.g. a railway station or church. A small clock is often shown in a corner of computer displays, mobile phones and many MP3 players.\n\nThe primary purpose of a clock is to \"display\" the time. Clocks may also have the facility to make a loud alert signal at a specified time, typically to waken a sleeper at a preset time; they are referred to as \"alarm clocks\". The alarm may start at a low volume and become louder, or have the facility to be switched off for a few minutes then resume. Alarm clocks with visible indicators are sometimes used to indicate to children too young to read the time that the time for sleep has finished; they are sometimes called \"training clocks\".\n\nA clock mechanism may be used to \"control\" a device according to time, e.g. a central heating system, a VCR, or a time bomb (see: digital counter). Such mechanisms are usually called timers. Clock mechanisms are also used to drive devices such as solar trackers and astronomical telescopes, which have to turn at accurately controlled speeds to counteract the rotation of the Earth.\n\nMost digital computers depend on an internal signal at constant frequency to synchronize processing; this is referred to as a clock signal. (A few research projects are developing CPUs based on asynchronous circuits.) Some equipment, including computers, also maintains time and date for use as required; this is referred to as time-of-day clock, and is distinct from the system clock signal, although possibly based on counting its cycles.\n\nFor some scientific work timing of the utmost accuracy is essential. It is also necessary to have a standard of the maximum accuracy against which working clocks can be calibrated. An ideal clock would give the time to unlimited accuracy, but this is of course not realisable. Many physical processes, in particular including some transitions between atomic energy levels, occur at exceedingly stable frequency; counting cycles of such a process can give a very accurate and consistent time—clocks which work this way are usually called atomic clocks. Such clocks are typically large, very expensive, require a controlled environment, and are far more accurate than required for most purposes; they are typically used in a standards laboratory.\n\nUntil advances in the late twentieth century, navigation depended on the ability to measure latitude and longitude. Latitude can be determined through celestial navigation; the measurement of longitude requires accurate knowledge of time. This need was a major motivation for the development of accurate mechanical clocks. John Harrison created the first highly accurate marine chronometer in the mid-18th century. The Noon gun in Cape Town still fires an accurate signal to allow ships to check their chronometers. Many buildings near major ports used to have (some still do) a large ball mounted on a tower or mast arranged to drop at a pre-determined time, for the same purpose. While satellite navigation systems such as the Global Positioning System (GPS) require unprecedentedly accurate knowledge of time, this is supplied by equipment on the satellites; vehicles no longer need timekeeping equipment.\n\n\n\n"}
{"id": "6451", "url": "https://en.wikipedia.org/wiki?curid=6451", "title": "Charles Proteus Steinmetz", "text": "Charles Proteus Steinmetz\n\nCharles Proteus Steinmetz (April 9, 1865 – October 26, 1923; birth-name: Karl August Rudolph Steinmetz) was a German-born American mathematician and electrical engineer and professor at Union College. He fostered the development of alternating current that made possible the expansion of the electric power industry in the United States, formulating mathematical theories for engineers. He made ground-breaking discoveries in the understanding of hysteresis that enabled engineers to design better electromagnetic apparatus equipment including especially electric motors for use in industry.\n\nSteinmetz was born Karl August Rudolph Steinmetz on April 9, 1865 in Breslau, Province of Silesia, the son of Caroline (Neubert) and Karl Heinrich Steinmetz. He was baptized a Lutheran, the religion his family \"nominally\" belonged to. Steinmetz suffered from dwarfism, hunchback, and hip dysplasia, as did his father and grandfather. Steinmetz attended Johannes Gymnasium and astonished his teachers with his proficiency in mathematics and physics.\n\nFollowing the Gymnasium, Steinmetz went on to the University of Breslau to begin work on his undergraduate degree in 1883. He was on the verge of finishing his doctorate in 1888 when he came under investigation by the German police for activities on behalf of a socialist university group and articles he had written for a local socialist newspaper.\n\nAs socialist meetings and press had been banned in Germany, Steinmetz fled to Zürich in 1888 to escape possible arrest. Faced with an expiring visa, he emigrated to the United States in 1889. He changed his first name to \"Charles\" in order to sound more American, and chose the middle name \"Proteus\" after a childhood epithet given to him by classmates. Proteus was a wise hunchbacked character from the \"Odyssey\" who knew many secrets, and Steinmetz felt the name suited him.\n\nCornell University Professor Ronald R. Kline, the author of \"Steinmetz: Engineer and Socialist\", contended that other factors were more directly involved in Steinmetz's decision to leave his homeland, such as the fact that he was in arrears with his tuition at the University of Breslau and that life at home with his father, stepmother, and their daughters was full of tension.\n\nDespite his earlier efforts and interest in socialism, by 1922 Steinmetz concluded that socialism would never work in the United States, because the country lacked a \"powerful, centralized government of competent men, remaining continuously in office\", and because \"only a small percentage of Americans accept this viewpoint today\".\n\nA member of the original Technical Alliance, which also included Thorstein Veblen and Leland Olds, Steinmetz had great faith in the ability of machines to eliminate human toil and create abundance for all. He put it this way: \"Some day we make the good things of life for everybody\".\n\nSteinmetz is known for his contribution in three major fields of alternating current (AC) systems theory: hysteresis, steady-state analysis, and transients.\n\nShortly after arriving in the United States, Steinmetz went to work for Rudolf Eickemeyer in Yonkers, New York, and published in the field of magnetic hysteresis, which gave him worldwide professional recognition. Eickemeyer's firm developed transformers for use in the transmission of electrical power among many other mechanical and electrical devices. In 1893 Eickemeyer's company, along with all of its patents and designs, was bought by the newly formed General Electric Company, where he quickly became known as the engineering wizard in GE's engineering community.\n\nSteinmetz's work revolutionized AC circuit theory and analysis, which had been carried out using complicated, time-consuming calculus-based methods. In the groundbreaking paper, \"Complex Quantities and Their Use in Electrical Engineering\", presented at a July 1893 meeting published in the American Institute of Electrical Engineers (AIEE), Steinmetz simplified these complicated methods to \"a simple problem of algebra\". He systematized the use of complex number phasor representation in electrical engineering education texts, whereby the lower-case letter \"j\" is used to designate the 90-degree rotation operator in AC system analysis. His seminal books and many other AIEE papers \"taught a whole generation of engineers how to deal with AC phenomena\".\n\nSteinmetz also made greater strides to the understanding of lightning phenomena. He undertook a systematic study of it, resulting in experiments with \"man-made lightning\" in the laboratory; this work was published. Steinmetz was called the \"forger of thunderbolts\", being the first to create artificial lightning in his football field-sized laboratory and high towers built at General Electric, using 120,000 volt generators. He erected a lightning tower to attract natural lightning, and studied the patterns and effects of lightning resulting in several theories and ideas.\n\nSteinmetz acted in the following professional capacities:\n\nHe was granted an honorary degree from Harvard University in 1901 and a doctorate from Union College in 1903.\n\nSteinmetz wrote 13 books and 60 articles, not all about engineering. He was a member and adviser to the fraternity Phi Gamma Delta at Union College, whose chapter house there was one of the first electrified houses ever.\n\nWhile serving as president of the Schenectady Board of Education, Steinmetz introduced reforms including extended school hours, school meals, school nurses, special classes for the children of immigrants, and the distribution of free textbooks.\n\nAlthough Steinmetz loved children and family life, he knew that both his father and grandfather suffered from the same spine deformity that afflicted him. Therefore, he did not marry, for fear of passing on his deformity to any descendants.\n\nWhen Joseph LeRoy Hayden, a loyal and hardworking lab assistant, announced that he would marry and look for his own living quarters, Steinmetz made an unusual proposal regarding living arrangements. He proposed that Hayden's new wife and prospective family move into a large house Steinmetz had already built with its own research lab, greenhouse, and office. He carefully courted Hayden and his wife Corinne, before broaching his proposal. Hayden favored the idea, but his wife was very wary of this unorthodox setup. She finally agreed after Steinmetz acceded to her conditions, notably that she could \"run the house as I see fit\".\n\nAfter an uneasy start, the arrangement worked well for all parties, especially after the Haydens had three children. Steinmetz legally adopted Joseph Hayden as his son, and became the \"grandfather\" of the children, entertaining them with fantastic stories and spectacular scientific demonstrations. The unusual but harmonious living arrangements lasted for the rest of Steinmetz's life.\n\nSteinmetz founded America's first glider club but none of the glider prototypes developed in this club \"could be dignified with the term 'flight'\".\n\nSteinmetz was a lifelong agnostic.\n\nSteinmetz died on October 26, 1923 and was buried in Vale Cemetery, Schenectady, New York.\n\nBased on Steinmetz experiments, \"Steinmetz's formula\" defines the approximate heat energy due to magnetic hysteresis released, per cycle per unit area of magnetic material. Steinmetz equivalent circuit theory is still widely used for the design and testing of induction motors.\n\nOne of the highest technical awards given by the Institute of Electrical and Electronics Engineers, for major contributions to standardization within the field of electrical and electronics engineering, is named in his honor as the IEEE Charles Proteus Steinmetz Award. Other awards include the Certificate of Merit of Franklin Institute, 1908; the Elliott Cresson Medal, 1913; and the Cedergren Medal, 1914.\n\nHis connection to Union College is celebrated with the annual Steinmetz Symposium, a day-long event in which Union undergraduates give presentations on research they have done. Steinmetz Hall, which houses the Union College computer center, is named after him.\n\nSteinmetz was portrayed in 1959 by the actor Rod Steiger in the CBS television anthology series, \"The Joseph Cotten Show\". The episode focused on his socialist activities in Germany.\n\nA Chicago public high school, Steinmetz College Prep, is named for him.\n\nA public park in north Schenectady, New York was named for him in 1931.\n\nSteinmetz is featured in John Dos Passos's \"U.S.A.\" trilogy in one of the biographies. He also serves as a major character in Starling Lawrence's \"The Lightning Keeper\".\n\nNovelist John Ball grew up in Steinmetz's house. His parents were graduate students paid by General Electric to live with and take care of the man Ball called \"Uncle Steinie\". Ball used to tell his Steinmetz stories at the Southern California Mystery Writers Association meetings.\n\nSteinmetz is a major character in the novel \"Electric City\" by Elizabeth Rosner. In this epic story of technology, Rosner connects Steinmetz's early ethos as a socialist with his humanitarian vision of a better society based on technological progress: \"The political arena that had summoned him in his youth, Socialist views that sent him into exile all those years earlier, further contributed to a seemingly endless hunger for change\".\n\nAt the time of his death, Steinmetz held over 200 patents:\n\n\n\n"}
{"id": "6452", "url": "https://en.wikipedia.org/wiki?curid=6452", "title": "Charles Martel", "text": "Charles Martel\n\nCharles Martel (c. 686 – 22 October 741) was a Frankish statesman and military leader who as Duke and Prince of the Franks and Mayor of the Palace, was \"de facto\" ruler of Francia from 718 until his death. The son of the Frankish statesman Pepin of Herstal and a noblewoman named Alpaida, Charles successfully asserted his claims to power as successor to his father as the power behind the throne in Frankish politics. Continuing and building on his father's work, he restored centralized government in Francia and began the series of military campaigns that re-established the Franks as the undisputed masters of all Gaul.\n\nAfter work to establish a unity in Gaul, Charles' attention was called to foreign conflicts, and dealing with the Islamic advance into Western Europe was a foremost concern. Arab and Berber Islamic forces had conquered Spain (711), crossed the Pyrenees (720), seized a major dependency of the Visigoths (721–725), and after intermittent challenges, under Abdul Rahman Al Ghafiqi, the Arab Governor of al-Andalus, advanced toward Gaul and on Tours, \"the holy town of Gaul\"; in October 732, the army of the Umayyad Caliphate led by Al Ghafiqi met Frankish and Burgundian forces under Charles in an area between the cities of Tours and Poitiers (modern north-central France), leading to a decisive, historically important Frankish victory known as the Battle of Tours (or \"ma'arakat Balâṭ ash-Shuhadâ\", Battle of the Palace of Martyrs), ending the \"last of the great Arab invasions of France,\" a military victory termed \"brilliant\" on the part of Charles.\nCharles further took the offensive after Tours, destroying fortresses at Agde, Béziers and Maguelonne, and engaging Islamic forces at Nimes, though ultimately failing to recover Narbonne (737) or to fully reclaim the Visigoth's Narbonensis. He thereafter made significant further external gains against fellow Christian realms, establishing Frankish control over Bavaria, Alemannia, and Frisia, and compelling some of the Saxon tribes to offer tribute (738).\n\nApart from the military endeavours, Charles is considered to be a founding figure of the European Middle Ages. Skilled as an administrator as well as a warrior, he is credited with a seminal role in the emerging responsibilities of the knights of courts, and so in the development of the Frankish system of feudalism. Moreover, Charles—a great patron of Saint Boniface—made the first attempt at reconciliation between the Franks and the Papacy. Pope Gregory III, whose realm was being menaced by the Lombards, wished Charles to become the defender of the Holy See and offered him the Roman consulship, though Charles declined.\n\nHe divided Francia between his sons Carloman and Pepin. The latter became the first of the Carolingians. Charles' grandson, Charlemagne, extended the Frankish realms to include much of the West, and became the first Emperor in the West since the fall of Rome.\n\nCharles \"The Hammer\" Martel was the son of Pepin of Herstal and his second wife Alpaida. He had a brother named Childebrand, who later became the Frankish \"dux\" (that is, \"duke\") of Burgundy.\n\nIn older historiography, it was common to describe Charles as \"illegitimate\". This is still widely repeated in popular culture today. But, polygamy was a legitimate Frankish practice at the time and it is unlikely that Charles was considered \"illegitimate\". It is likely that the interpretation of \"illegitimacy\" derives from the desire of Pepin's first wife Plectrude to see her progeny as heirs to Pepin's power.\n\nAfter the reign of Dagobert I (629–639) the Merovingians effectively ceded power to the Pippinids, who ruled the Frankish realm of Austrasia in all but name as Mayors of the Palace. They controlled the royal treasury, dispensed patronage, and granted land and privileges in the name of the figurehead king. Charles' father, Pepin, was the second member of the family to rule the Franks. Pepin was able to unite all the Frankish realms by conquering Neustria and Burgundy. He was the first to call himself Duke and Prince of the Franks, a title later taken up by Charles.\n\nIn December 714, Pepin of Herstal died. Prior to his death, he had, at his wife Plectrude's urging, designated Theudoald, his grandson by their late son Grimoald, his heir in the entire realm. This was immediately opposed by the nobles because Theudoald was a child of only eight years of age. To prevent Charles using this unrest to his own advantage, Plectrude had him imprisoned in Cologne, the city which was destined to be her capital. This prevented an uprising on his behalf in Austrasia, but not in Neustria.\n\nIn 715, the Neustrian nobles proclaimed Ragenfrid mayor of their palace on behalf of, and apparently with the support of, Dagobert III, who in theory had the legal authority to select a mayor, though by this time the Merovingian dynasty had lost most such powers.\n\nThe Austrasians were not to be left supporting a woman and a young child. Before the end of the year, Charles Martel had escaped from prison and been acclaimed mayor by the nobles of that kingdom. That year, Dagobert III, a Merovingian, died and the Neustrians proclaimed Chilperic II, the cloistered son of Childeric II, as king.\n\nIn 716, Chilperic and Ragenfrid together led an army into Austrasia. The Neustrians allied with another invading force under Radbod, King of the Frisians and met Charles in battle near Cologne, which was still held by Plectrude. Charles had little time to gather men, or prepare, and the result was the only defeat of his life. The king and his mayor besieged Plectrude at Cologne, where she bought them off with a substantial portion of Pepin's treasure. Then they withdrew.\n\nCharles retreated to the hills of the Eifel to gather men, and train them. Having made the proper preparations, he fell upon the triumphant army near Malmedy as it was returning to its own province, and, in the ensuing Battle of Amblève, routed it. The few troops who were not killed or captured fled.\n\nIn this battle, Charles set a pattern for the remainder of his military career. He appeared \"where\" his enemies least expected him, while they were marching triumphantly home and far outnumbered him. He also attacked \"when\" least expected, at midday, when armies of that era traditionally were resting. Finally, he attacked them \"how\" they least expected it, by feigning a retreat to draw his opponents into a trap. The feigned retreat, next to unknown in Western Europe at that time—it was a traditionally eastern tactic—required both extraordinary discipline on the part of the troops and exact timing on the part of their commander. Charles, in this battle, had begun demonstrating the military brilliance that would mark his rule. The result was an unbroken victory streak that lasted until his death.\n\nAfter the victory at Amblève, Charles took time to rally more men and prepare. By the following spring, Charles had attracted enough support to descend in full force on the Neustrians. He chose the time and location. Charles eventually followed them and dealt them a serious blow at Vincy on 21 March 717. He pursued the fleeing king and mayor to Paris, before turning back to deal with Plectrude and Cologne. He took her city and dispersed her adherents, but allowed both Plectrude and the young Theudoald to live and treated them with kindness—unusual for those times, when mercy to a former gaoler, or a potential rival, was rare.\n\nOn this success, he proclaimed Chlotar IV king of Austrasia in opposition to Chilperic and deposed the archbishop of Reims, Rigobert, replacing him with Milo, a lifelong supporter.\n\nAfter subjugating all Austrasia, he marched against Radbod and pushed him back into his territory, even forcing the concession of West Frisia (later part of the county of Holland). He also sent the Saxons back over the Weser and thus secured his borders—in the name of the new king Clotaire, of course.\nIn 718, Chilperic responded to Charles' new ascendancy by making an alliance with Odo the Great (or Eudes, as he is sometimes known), the duke of Aquitaine, who had made himself independent during the civil war in 715, but was again defeated, at the Battle of Soissons, by Charles.\n\nChilperic fled with his ducal ally to the land south of the Loire and Ragenfrid fled to Angers. Soon Clotaire IV died and Odo gave up on Chilperic and, in exchange for recognising his dukedom, surrendered the king to Charles, who recognised his kingship over all the Franks in return for legitimate royal affirmation of his mayoralty, likewise over all the kingdoms (718).\n\nThe ensuing years were full of strife. Between 718 and 723, Charles secured his power through a series of victories: he won the loyalty of several important bishops and abbots (by donating lands and money for the foundation of abbeys such as Echternach), he subjugated Bavaria and Alemannia, and he defeated the pagan Saxons.\n\nHaving unified the Franks under his banner, Charles was determined to punish the Saxons who had invaded Austrasia. Therefore, late in 718, he laid waste their country to the banks of the Weser, the Lippe, and the Ruhr. He defeated them in the Teutoburg Forest. In 719, Charles seized West Frisia without any great resistance on the part of the Frisians, who had been subjects of the Franks but had seized control upon the death of Pippin. Although Charles did not trust the pagans, their ruler, Aldegisel, accepted Christianity, and Charles sent Willibrord, bishop of Utrecht, the famous \"Apostle to the Frisians\" to convert the people. Charles also did much to support Winfrid, later Saint Boniface, the \"Apostle of the Germans.\"\n\nWhen Chilperic II died the following year (720), Charles appointed as his successor the son of Dagobert III, Theuderic IV, who was still a minor, and who occupied the throne from 720 to 737. Charles was now appointing the kings whom he supposedly served, \"rois fainéants\" who were mere puppets in his hands; by the end of his reign they were so useless that he didn't even bother appointing one. At this time, Charles again marched against the Saxons. Then the Neustrians rebelled under Ragenfrid, who had left the county of Anjou. They were easily defeated (724), but Ragenfrid gave up his sons as hostages in turn for keeping his county. This ended the civil wars of Charles' reign.\n\nThe next six years were devoted in their entirety to assuring Frankish authority over the dependent Germanic tribes. Between 720 and 723, Charles was fighting in Bavaria, where the Agilolfing dukes had gradually evolved into independent rulers, recently in alliance with Liutprand the Lombard. He forced the Alemanni to accompany him, and Duke Hugbert submitted to Frankish suzerainty. He brought back the Agilolfing princess Swanachild, who apparently became his concubine.\n\nIn 725 and 728, he again entered Bavaria. In 730, he marched against Lantfrid, duke of Alemannia, who had also become independent, and killed him in battle. He forced the Alemanni capitulation to Frankish suzerainty and did not appoint a successor to Lantfrid. Thus, southern Germany once more became part of the Frankish kingdom, as had northern Germany during the first years of the reign.\n\nBy 731, the realm secure, Charles began to prepare exclusively for the coming crises from the south and west.\n\nBy 721, the emir of Córdoba had built up a strong army from Morocco, Yemen, and Syria to conquer Aquitaine. The large duchy in southwest Gaul was nominally under Frankish sovereignty, but in fact was almost independent under Odo the Great, Duke of Aquitaine, since the Merovingian kings had lost power. The invading Muslims besieged Toulouse, then Aquitaine's most important city, and Odo (also called Eudes, or Eudo) immediately left to find help.\n\nReturning three months later, Odo was in time to prevent the city's surrender and defeated the Muslim invaders on June 9, 721, at the Battle of Toulouse (721). After Odo's escape the Muslims had become overconfident, failing to maintain defenses or scout patrols. In a near classic enveloping movement Odo's forces launched a surprise attack on the besiegers, scattering them at the first attack and slaughtering units at rest or fleeing without weapons or armour.\n\nBy 730 Abdul Rahman Al Ghafiqi, who had been at Toulouse, was the emir of Cordoba. The Arab Chronicles make clear he had strongly opposed his predecessor's decision not to secure outer defenses against a relief force, which allowed Odo's force to attack with impunity before the Islamic cavalry could assemble or mount.\n\nThis time the Umayyad horsemen were ready for battle, and the results were horrific for the Aquitanians.\n\nHistorian Paul K. Davis wrote, \"Having defeated Eudes, he turned to the Rhine to strengthen his northeastern borders—but in 725 was diverted south with the activity of the Muslims in Acquitane.\" Charles then concentrated his attention to the Umayyads, virtually for the remainder of his life.\nDue to the situation in Iberia, Charles believed he needed a virtually full-time army—one he could train intensely—as a core of veteran Franks who would be augmented with the usual conscripts called up in time of war. (During the Early Middle Ages, troops were only available after the crops had been planted and before harvesting time.) To train the kind of infantry that could withstand the Muslim heavy cavalry, Charles needed them year-round, and he needed to pay them so their families could buy the food they would have otherwise grown.\n\nTo obtain money he seized church lands and property, and used the funds to pay his soldiers. The same Charles who had secured the support of the \"ecclesia\" by donating land, seized some of it back between 724 and 732. Of course, Church officials were enraged, and, for a time, it looked as though Charles might even be excommunicated for his actions. But then came a significant invasion.\n\nThe Muslims were not aware, at that time, of the true strength of the Franks, or the fact that they were building a disciplined army instead of the typical barbarian hordes that had dominated Europe after Rome's fall. The Arab Chronicles, the history of that age, show that Arab awareness of the Franks as a growing military power came only after the Battle of Tours when the Caliph expressed shock at his army's catastrophic defeat.\n\nOdo, hero of Toulouse, was badly defeated in the Arab invasion of 732 at the battle prior to the Arab sacking of Bordeaux, and again at the Battle of the River Garonne after he had gathered a second army—Christian chroniclers state, \"God alone knows the number of the slain\"— and the city of Bordeaux was sacked and looted. Odo fled to Charles, seeking help. Charles agreed to come to Odo's rescue, provided Odo acknowledged Charles and his house as his overlords, which Odo did formally at once. Odo and his remaining Aquitanian nobles formed the right flank of Charles's forces at Tours. Charles defeated the Moors commanded by Abderame; while the former and the latter squared off in battle, Odo set fire to the encampment of the latter. \"The victory at the battle near Poitiers and Tours would later earn Charles the cognomen \"Martellus\" (L., and so \"Martel\", Fr.: \"the hammer\") from 9th century chroniclers who, in the view of Pierre Riche, \"seem to have been… recalling Judas Maccabaeus, 'the Hammerer,'\" of some bibles, \"whom God had similarly blessed with victory\" (except, in that earlier case, over attacking Syrian forces).\"\n\nMany historians, including Sir Edward Creasy, believe that had he failed at Poitiers, Islam would probably have overrun Gaul, and perhaps the remainder of Western Europe. Gibbon made clear his belief that the Umayyad armies would have conquered from Japan to the Rhine, and even England, having the English Channel for protection, with ease, had Charles not prevailed. Creasy said \"the great victory won by Charles Martel ... gave a decisive check to the career of Arab conquest in Western Europe, rescued Christendom from Islam, [and] preserved the relics of ancient and the germs of modern civilization.\"\n\nGibbon's belief that the fate of Christianity hinged on this battle is echoed by other historians including John B. Bury, and was very popular for most of modern historiography. It fell somewhat out of style in the 20th century, when historians such as Bernard Lewis contended that Arabs had little intention of occupying northern France. More recently, however, many historians have tended once again to view the Battle of Poitiers as a very significant event in the history of Europe and Christianity. Equally many, such as William E. Watson, still believe this battle was one of macrohistorical world-changing importance, if they do not go so far as Gibbon does rhetorically.\n\nIndeed, 12 years later, when Charles had thrice rescued Gaul from Umayyad invasions, Antonio Santosuosso noted when he destroyed an Umayyad army sent to reinforce the invasion forces of the 735 campaigns, \"Charles Martel again came to the rescue.\"\n\nIn the modern era, Matthew Bennett argues that \"few battles are remembered 1,000 years after they are fought ... but the Battle of Poitiers, (Tours) is an exception ... Charles Martel turned back a Muslim raid that, had it been allowed to continue, might have conquered Gaul.\" Michael Grant, author of \"History of Rome\", assigns the Battle of Tours such importance that he lists it in the macrohistorical dates of the Roman era.\"\n\nIt is important to note, however, that modern Western historians, military historians, and writers, essentially fall into three camps. The first, those who believe Gibbon was right in his assessment that Charles saved Christianity and Western civilization by this battle, as typified by Bennett, Paul Davis, Robert Martin, and educator Dexter B. Wakefield, who writes in \"An Islamic Europe?\":\n\nThe second camp of contemporary historians believe that a failure by Charles at Tours could have been a disaster, destroying what would become Western civilization after the Renaissance. Certainly all historians agree that no power would have remained in Europe able to halt Islamic expansion had the Franks failed: William E. Watson, one of the most respected historians of this era, strongly supports Tours as a macrohistorical event, but distances himself from the rhetoric of Gibbon and Drubeck, writing, for example, of the battle's importance in Frankish and world history in 1993:\n\nThe final camp of Western historians believe that the importance of the battle is dramatically overstated. This view is typified by Alessandro Barbero, who writes, \"Today, historians tend to play down the significance of the battle of Poitiers, pointing out that the purpose of the Arab force defeated by Charles Martel was not to conquer the Frankish kingdom, but simply to pillage the wealthy monastery of St-Martin of Tours\". Similarly, Tomaž Mastnak writes:\n\nHowever, it is vital to note, when assessing Charles Martel's life, that even those historians who dispute the significance of this one battle as the event that saved Christianity, do not dispute that Charles himself had a huge effect on Western European history. Modern military historian Victor Davis Hanson acknowledges the debate on this battle, citing historians both for and against its macrohistorical placement:\nIn the subsequent decade, Charles led the Frankish army against the eastern duchies, Bavaria and Alemannia, and the southern duchies, Aquitaine and Provence. He dealt with the ongoing conflict with the Frisians and Saxons to his northeast with some success, but full conquest of the Saxons and their incorporation into the Frankish empire would wait for his grandson Charlemagne, primarily because Charles concentrated the bulk of his efforts against Muslim expansion.\n\nSo instead of concentrating on conquest to his east, he continued expanding Frankish authority in the west, and denying the Emirate of Córdoba a foothold in Europe beyond Al-Andalus. After his victory at Tours, Charles continued on in campaigns in 736 and 737 to drive other Muslim armies from bases in Gaul after they again attempted to expand beyond Al-Andalus.\n\nBetween his victory of 732 and 735, Charles reorganized the kingdom of Burgundy, replacing the counts and dukes with his loyal supporters, thus strengthening his hold on power. He was forced, by the ventures of Radbod, duke of the Frisians (719–734), son of the Duke Aldegisel who had accepted the missionaries Willibrord and Boniface, to invade independence-minded Frisia again in 734. In that year, he slew the duke, who had expelled the Christian missionaries, in the battle of the Boarn and so wholly subjugated the populace (he destroyed every pagan shrine) that the people were peaceful for twenty years after.\n\nThe dynamic changed in 735 because of the death of Odo the Great, who had been forced to acknowledge, albeit reservedly, the suzerainty of Charles in 719. Though Charles wished to unite the duchy directly to himself and went there to elicit the proper homage of the Aquitainians, the nobility proclaimed Odo's son, Hunald of Aquitaine, whose dukedom Charles recognised when the Umayyads invaded Provence the next year, and who equally was forced to acknowledge Charles as overlord as he had no hope of holding off the Muslims alone.\n\nThis naval Arab invasion was headed by Abdul Rahman's son. It landed in Narbonne in 736 and moved at once to reinforce Arles and move inland. Charles temporarily put the conflict with Hunald on hold, and descended on the Provençal strongholds of the Umayyads. In 736, he retook Montfrin and Avignon, and Arles and Aix-en-Provence with the help of Liutprand, King of the Lombards. Nîmes, Agde, and Béziers, held by Islam since 725, fell to him and their fortresses were destroyed.\n\nHe crushed one Umayyad army at Arles, as that force sallied out of the city, and then took the city itself by a direct and brutal frontal attack, and burned it to the ground to prevent its use again as a stronghold for Umayyad expansion. He then moved swiftly and defeated a mighty host outside of Narbonne at the River Berre, but failed to take the city. Military historians believe he could have taken it, had he chosen to tie up all his resources to do so—but he believed his life was coming to a close, and he had much work to do to prepare for his sons to take control of the Frankish realm.\n\nA direct frontal assault, such as took Arles, using rope ladders and rams, plus a few catapults, simply was not sufficient to take Narbonne without horrific loss of life for the Franks, troops Charles felt he could not lose. Nor could he spare years to starve the city into submission, years he needed to set up the administration of an empire his heirs would reign over. In addition, he faced strong opposition from regional lords such as the patrician Maurentius, from Marseille, who revolted against the Frankish leader. Moreover, the Aquitanian duke Hunald threatened his lines of communication with the north, so deciding him to withdraw from Septimania and destroy several strongholds (Béziers, Agde, etc.). He left Narbonne therefore, isolated and surrounded, and his son would return to conquer it for the Franks.\n\nNotable about these campaigns was Charles' incorporation, for the first time, of heavy cavalry with stirrups to augment his phalanx. His ability to coordinate infantry and cavalry veterans was unequaled in that era and enabled him to face superior numbers of invaders, and to decisively defeat them again and again. Some historians believe the Battle against the main Muslim force at the River Berre, near Narbonne, in particular was as important a victory for Christian Europe as Tours.\n\nFurther, unlike his father at Tours, Rahman's son in 736–737 knew that the Franks were a real power, and that Charles personally was a force to be reckoned with. He had no intention of allowing Charles to catch him unaware and dictate the time and place of battle, as his father had. He concentrated instead on seizing a substantial portion of the coastal plains around Narbonne in 736 and heavily reinforced Arles as he advanced inland.\n\nAbdul Rahman's son planned from there to move from city to city, fortifying as they went, and if Charles wished to stop them from making a permanent enclave for expansion of the Caliphate, he would have to come to them, in the open, where, he, unlike his father, would dictate the place of battle. All worked as he had planned, until Charles arrived, albeit more swiftly than the Moors believed he could call up his entire army. Unfortunately for Rahman's son, however, he had overestimated the time it would take Charles to develop heavy cavalry equal to that of the Muslims.\n\nThe Caliphate believed it would take a generation, but Charles managed it in five years. Prepared to face the Frankish phalanx, the Muslims were totally unprepared to face a mixed force of heavy cavalry and infantry in a phalanx. Thus, Charles again championed Christianity and halted Muslim expansion into Europe. These defeats, plus those at the hands of Leo III of the Byzantine Empire in Anatolia, were the last great attempt at expansion by the Umayyad Caliphate before the destruction of the dynasty at the Battle of the Zab, and the rending of the Caliphate forever, especially the utter destruction of the Umayyad army at River Berre near Narbonne in 737.\n\nIn 737, at the tail end of his campaigning in Provence and Septimania, the king, Theuderic IV, died. Charles, titling himself \"maior domus\" and \"princeps et dux Francorum\", did not appoint a new king and nobody acclaimed one. The throne lay vacant until Charles' death. As the historian Charles Oman says \"he cared not for name or style so long as the real power was in his hands.\"\n\nGibbon has said Charles was \"content with the titles of Mayor or Duke of the Franks, but he deserved to become the father of a line of kings,\" which he did. Gibbon also says of him, \"in the public danger, he was summoned by the voice of his country.\"\n\nThe interregnum, the final four years of Charles' life, was more peaceful than most of it had been and much of his time was now spent on administrative and organisational plans to create a more efficient state. Though, in 738, he compelled the Saxons of Westphalia to do him homage and pay tribute, and in 739 checked an uprising in Provence, the rebels being under the leadership of Maurontus. Charles set about integrating the outlying realms of his empire into the Frankish church.\n\nHe erected four dioceses in Bavaria (Salzburg, Regensburg, Freising, and Passau) and gave them Boniface as archbishop and metropolitan over all Germany east of the Rhine, with his seat at Mainz. Boniface had been under his protection from 723 on; indeed the saint himself explained to his old friend, Daniel of Winchester, that without it he could neither administer his church, defend his clergy, nor prevent idolatry. It was Boniface who had defended Charles most stoutly for his deeds in seizing ecclesiastical lands to pay his army in the days leading to Tours, as one doing what he must to defend Christianity.\n\nIn 739, Pope Gregory III begged Charles for his aid against Liutprand, but Charles was loath to fight his onetime ally and ignored the Papal plea. Nonetheless, the Papal applications for Frankish protection showed how far Charles had come from the days he was tottering on excommunication, and set the stage for his son and grandson to rearrange Italian political boundaries to suit the Papacy, and protect it.\n\nCharles Martel died on October 22, 741, at Quierzy-sur-Oise in what is today the Aisne \"département\" in the Picardy region of France. He was buried at Saint Denis Basilica in Paris.\n\nHis territories had been divided among his adult sons a year earlier: to Carloman he gave Austrasia, Alemannia, and Thuringia, and to Pippin the Younger Neustria, Burgundy, Provence, and Metz and Trier in the \"Mosel duchy\"; Grifo was given several lands throughout the kingdom, but at a later date, just before Charles died.\n\nGibbon called him \"the hero of the age\" and declared \"Christendom ... delivered ... by the genius and good fortune of one man, Charles Martel.\"\n\nAt the beginning of Charles Martel's career, he had many internal opponents and felt the need to appoint his own kingly claimant, Clotaire IV. By his end, however, the dynamics of rulership in Francia had changed, no hallowed Meroving was needed, neither for defence nor legitimacy: Charles divided his realm between his sons without opposition (though he ignored his young son Bernard). In between, he strengthened the Frankish state by consistently defeating, through superior generalship, the host of hostile foreign nations which beset it on all sides, including the non-Christian Saxons, whom his grandson Charlemagne would fully subdue, and Moors, whom he halted on a path of continental domination.\nThough he never cared about titles, his son Pippin (Fr.: Pepin) did, and finally asked the Pope \"who should be King, he who has the title, or he who has the power?\" The Pope, highly dependent on Frankish armies for his independence from Lombard and Byzantine power (the Byzantine Emperor still considered himself to be the only legitimate \"Roman Emperor\", and thus, ruler of all of the provinces of the ancient empire, whether recognised or not), declared for \"he who had the power\" and immediately crowned Pippin.\n\nDecades later, in 800, Pippin's son Charlemagne was crowned emperor by the Pope, further extending the principle by delegitimising the nominal authority of the Byzantine Emperor in the Italian peninsula (which had, by then, shrunk to encompass little more than Apulia and Calabria at best) and ancient Roman Gaul, including the Iberian outposts Charlemagne had established in the \"Marca Hispanica\" across the Pyrenees, what today forms Catalonia. In short, though the Byzantine Emperor claimed authority over all the old Roman Empire, as the legitimate \"Roman\" Emperor, it was simply not reality.\n\nThe bulk of the Western Roman Empire had come under Carolingian rule, the Byzantine Emperor having had almost no authority in the West since the sixth century, though Charlemagne, a consummate politician, preferred to avoid an open breach with Constantinople. An institution unique in history was being born: the Holy Roman Empire. Though the sardonic Voltaire ridiculed its nomenclature, saying that the Holy Roman Empire was \"neither Holy, nor Roman, nor an Empire,\" it constituted an enormous political power for a time, especially under the Saxon, the Salian dynasties, the House of Hapsburg, and, to a lesser extent, the Hohenstaufen. It lasted until 1806, by which time it was a nonentity. Though his grandson became its first emperor, the \"empire\" such as it was, was largely born during the reign of Charles Martel.\n\nCharles was that rarest of commodities in the Middle Ages: a brilliant strategic general, who also was a tactical commander \"par excellence\", able in the heat of battle to adapt his plans to his foe's forces and movement — and amazingly, to defeat them repeatedly, especially when, as at Tours, they were far superior in men and weaponry, and at Berre and Narbonne, when they were superior in numbers of fighting men. Charles had the last quality which defines genuine greatness in a military commander: he foresaw the dangers of his foes, and prepared for them with care; he used ground, time, place, and fierce loyalty of his troops to offset his foe's superior weaponry and tactics; third, he adapted, again and again, to the enemy on the battlefield, shifting to compensate for the unforeseen and unforeseeable.\n\nGibbon, whose tribute to Charles has been noted, was not alone among the great mid era historians in fervently praising Charles; Thomas Arnold ranks the victory of Charles Martel even higher than the victory of Arminius in the Battle of the Teutoburg Forest in its impact on all of modern history:\n\nGerman historians are especially ardent in their praise of Charles and in their belief that he saved Europe and Christianity from then all-conquering Islam, praising him also for driving back the ferocious Saxon barbarians on his borders. Schlegel speaks of this \"mighty victory\" in terms of fervent gratitude, and tells how \"the arm of Charles Martel saved and delivered the Christian nations of the West from the deadly grasp of all-destroying Islam\", and Ranke points out,\nIn 1922 and 1923, Belgian historian Henri Pirenne published a series of papers, known collectively as the \"Pirenne Thesis\", which remain influential to this day. Pirenne held that the Roman Empire continued, in the Frankish realms, up until the time of the Arab conquests in the 7th century. These conquests disrupted Mediterranean trade routes leading to a decline in the European economy. Such continued disruption would have meant complete disaster except for Charles Martel's halting of Islamic expansion into Europe from 732 on. What he managed to preserve led to the Carolingian Renaissance, named after him.\n\nProfessor Santosuosso perhaps sums up Charles best when he talks about his coming to the rescue of his Christian allies in Provence, and driving the Muslims back into the Iberian Peninsula forever in the mid and late 730s:\nSkilled as an administrator and ruler, Charles organized what would become the medieval European government: a system of fiefdoms, loyal to barons, counts, dukes and ultimately the King, or in his case, simply \"maior domus\" and princeps et dux Francorum. (\"Mayor of the Palace, Duke of the Franks\") His close coordination of church with state began the medieval pattern for such government. He created what would become the first western standing army since the fall of Rome by his maintaining a core of loyal veterans around which he organized the normal feudal levies. In essence, he changed Europe from a horde of barbarians fighting with one another, to an organized state.\n\nAlthough it took another two decades for the Franks to drive all the Arab garrisons out of Septimania and across the Pyrenees, Charles Martel's halt of the invasion of French soil turned the tide of Islamic advances, and the unification of the Frankish kingdoms under Charles, his son Pippin the Younger, and his grandson Charlemagne created a western power which prevented the Emirate of Córdoba from expanding over the Pyrenees. Charles, who in 732 was on the verge of excommunication, instead was recognised by the Church as its paramount defender. Pope Gregory II wrote to him more than once, asking his protection and aid.\n\nCharles' son Pippin the Younger (Pepin II, The Short) kept his father's promise and returned and took Narbonne by siege in 759. His grandson, Charlemagne, actually established the \"Marca Hispanica\" across the Pyrenees in part of what today is Catalonia, reconquering Girona in 785 and Barcelona in 801. Carolingians called this region of modern-day Spain \"The Moorish Marches\", and saw it as more than a simple check on the Muslims in Hispania. It formed a permanent buffer zone against Islam and became the basis, along with the efforts of Pelayo (Latin: Pelagius) and his descendants, for the Reconquista.\n\nVictor Davis Hanson argues that Charles Martel launched \"the thousand year struggle\" between European heavy infantry and Muslim cavalry. Of course, Charles is also the father of heavy cavalry in Europe, as he integrated heavy armoured cavalry into his forces. This creation of a real army would continue all through his reign, and that of his son, Pepin the Short, until his Grandson, Charlemagne, would possess the world's largest and finest army since the peak of Rome. Equally, the Muslims used infantry—indeed, at the Battle of Toulouse most of their forces were light infantry. It was not till Abdul Rahman Al Ghafiqi brought a huge force of Arab and Berber cavalry with him when he assumed the emirate of Al-Andulus that the Muslim forces became primarily cavalry.\n\nCharles' army was the first standing permanent army since the fall of Rome in 476. At its core was a body of tough, seasoned heavy infantry who displayed exceptional resolution at Tours. The Frankish infantry wore as much as 70 pounds of armour, including their heavy wooden shields with an iron boss. Standing close together, and well disciplined, they were unbreakable at Tours. Charles had taken the money and property he had seized from the church and paid local nobles to supply trained ready infantry year round.\n\nThis was the core of veterans who served with him on a permanent basis, and as Hanson says, \"provided a steady supply of dependable troops year around.\" While other Germanic cultures, such as the Visigoths or Vandals, had a proud martial tradition, and the Franks themselves had an annual muster of military aged men, such tribes were only able to field armies around planting and harvest. It was Charles' creation of a system whereby he could call on troops year round that gave the Carolingians the first standing and permanent army since Rome's fall in the west.\n\nCharles Martel's most important military achievement was the victory at Tours. Creasy argues that the Charles victory \"preserved the relics of ancient and the gems of modern civilizations.\" Gibbon called those eight days in 732, the week leading up to Tours, and the battle itself, \"the events that rescued our ancestors of Britain, and our neighbours of Gaul [France], from the civil and religious yoke of the Koran.\"\n\nCharles analysed what would be necessary for him to withstand a larger force and superior technology (the Muslim horsemen had adopted the armour and accoutrements of heavy cavalry from the Sassanid warrior class, which made the armored mounted knight possible). Not daring to send his few horsemen against the Islamic cavalry, he had his army fight in a formation used by the ancient Greeks to withstand superior numbers and weapons by discipline, courage, and a willingness to die for their cause: a phalanx. He had trained a core of his men year round, using mostly Church funds, and some had been with him since his earliest days after his father's death. It was this hard core of disciplined veterans that won the day for him at Tours.\n\nHanson emphasizes that Charles' greatest accomplishment as a general may have been his ability to keep his troops under control. Iron discipline saved his infantry from the fate of so many infantrymen—such as the Saxons at Hastings—who broke formation and were slaughtered piecemeal. After using this infantry force by itself at Tours, he studied the foe's forces and further adapted to them, initially using stirrups and saddles recovered from the foe's dead horses, and armour from the dead horsemen.\n\nThe defeats Charles inflicted on the Muslims were vital in that the split in the Islamic world left the Caliphate unable to mount an all-out attack on Europe via its Iberian stronghold after 750. His ability to meet this challenge, until the fragmentation of authority within the Muslims, is considered by most historians to be of macrohistorical importance, and is why Dante writes of him in Heaven as one of the \"Defenders of the Faith.\"\n\nH. G. Wells says of Charles Martel's decisive defeat of the Muslims in his \"Short History of the World:\nHowever, when the Muslim first crossed the Pyrenees, Aquitaine was actually an independent realm under duke Odo's leadership and the Gothic Septimania remained out of Frankish rule. Odo, who was Charles's southern rival, had struck a peace treaty after the Frankish civil wars in Neustria and Austrasia, and garnered much popularity and the Pope's favour for his victory on the 721 Battle of Toulouse against the Moors. On the eve of the Muslim expedition north (731), Charles Martel crossed the Loire and captured the Aquitanian city of Bourges, while Odo re-captured it briefly afterwards.\n\nJohn H. Haaren says in \"Famous Men of the Middle Ages\":\nJust as his grandson, Charlemagne, would become famous for his swift and unexpected movements in his campaigns, Charles was renowned for never doing what his enemies forecast he would do, and for moving far faster than his opponents believed he could. It is notable that the Northmen did not begin their European raids until after the death of Charles' grandson, Charlemagne. They had the naval capacity to begin those raids at least three generations earlier, and constructed defenses against counterattacks by land, but chose not to challenge Charles, his son Pippin, or his grandson, Charlemagne.\n\nJ. M. Roberts says of Charles Martel in his note on the Carolingians in his \"History of the World\":\n\nGibbon perhaps summarized Charles Martel's legacy most eloquently: \"in a laborious administration of 24 years he had restored and supported the dignity of the throne... by the activity of a warrior who in the same campaign could display his banner on the Elbe, the Rhone, and shores of the ocean.\"\n\nCharles had an active family life, about which accounts have been written. Charles Martel married twice, his first wife being Rotrude of Treves, daughter either of Lambert II, Count of Hesbaye, or of Leudwinus, Count of Treves. They had the following children:\n\n\nMost of the children married, and had children in those marriages, and so Charles' line was carried on. For instance, Hiltrud married Odilo I (a Duke of Bavaria). Landrade had been believed to have married a Sigrand (Count of Hesbania) but Sigrand's wife is more likely the sister of Rotrude. Auda married Thierry IV (a Count of Autun and Toulouse). Charles also married a second time, to Swanhild, and they had a single child, Grifo.\n\nFinally, Charles Martel also had known a mistress, Ruodhaid, with whom he had the children Bernard, Hieronymus, and Remigius, the latter who became an archbishop of Rouen.\n\n"}
{"id": "6456", "url": "https://en.wikipedia.org/wiki?curid=6456", "title": "Charles Edward Jones", "text": "Charles Edward Jones\n\nColonel Charles Edward (\"Chuck\") Jones (November 4, 1952 – September 11, 2001) was a United States Air Force officer, a computer programmer, and an astronaut in the USAF Manned Spaceflight Engineer Program.\n\nJones was born in Clinton, Indiana. He graduated from Wichita High School East in 1970 and earned a bachelor of science in astronautical engineering from the United States Air Force Academy in 1974. He entered the USAF Manned Spaceflight Engineer program in 1982, and was scheduled to fly on mission STS-71-B in December 1986, but the mission was cancelled after the \"Challenger\" Disaster in January 1986. He left the Manned Spaceflight Engineer program in 1987.\n\nHe was killed at the age of 48 in the attacks of September 11, 2001, aboard American Airlines Flight 11. He had been living as a retired US Air Force Colonel in Bedford, Massachusetts, at the time of his death. He was survived by his wife Jeanette.\n\nAt the National 9/11 Memorial, Jones is memorialized at the North Pool, on Panel N-74.\n\n\n"}
{"id": "6458", "url": "https://en.wikipedia.org/wiki?curid=6458", "title": "Ceramic", "text": "Ceramic\n\nA ceramic is an inorganic, non-metallic, solid material comprising metal, non-metal or metalloid atoms primarily held in ionic and covalent bonds. This article gives an overview of ceramic materials from the point of view of materials science.\n\nThe crystallinity of ceramic materials ranges from highly oriented to semi-crystalline, vitrified, and often completely amorphous (e.g., glasses). Most often, fired ceramics are either vitrified or semi-vitrified as is the case with earthenware, stoneware, and porcelain. Varying crystallinity and electron consumption in the ionic and covalent bonds cause most ceramic materials to be good thermal and electrical insulators (extensively researched in ceramic engineering). With such a large range of possible options for the composition/structure of a ceramic (e.g. nearly all of the elements, nearly all types of bonding, and all levels of crystallinity), the breadth of the subject is vast, and identifiable attributes (e.g. hardness, toughness, electrical conductivity, etc.) are hard to specify for the group as a whole. General properties such as high melting temperature, high hardness, poor conductivity, high moduli of elasticity, chemical resistance and low ductility are the norm, with known exceptions to each of these rules (e.g. piezoelectric ceramics, glass transition temperature, superconductive ceramics, etc.). Many composites, such as fiberglass and carbon fiber, while containing ceramic materials, are not considered to be part of the ceramic family.\n\nThe earliest ceramics made by humans were pottery objects (i.e. \"pots\" or \"vessels\") or figurines made from clay, either by itself or mixed with other materials like silica, hardened, sintered, in fire. Later ceramics were glazed and fired to create smooth, colored surfaces, decreasing porosity through the use of glassy, amorphous ceramic coatings on top of the crystalline ceramic substrates. Ceramics now include domestic, industrial and building products, as well as a wide range of ceramic art. In the 20th century, new ceramic materials were developed for use in advanced ceramic engineering, such as in semiconductors.\n\nThe word \"\"ceramic\"\" comes from the Greek word κεραμικός (\"keramikos\"), \"of pottery\" or \"for pottery\", from κέραμος (\"keramos\"), \"potter's clay, tile, pottery\". The earliest known mention of the root \"ceram-\" is the Mycenaean Greek \"ke-ra-me-we\", \"workers of ceramics\", written in Linear B syllabic script. The word \"ceramic\" may be used as an adjective to describe a material, product or process, or it may be used as a noun, either singular, or, more commonly, as the plural noun \"ceramics\".\n\nA ceramic material is an inorganic, non-metallic, often crystalline oxide, nitride or carbide material. Some elements, such as carbon or silicon, may be considered ceramics. Ceramic materials are brittle, hard, strong in compression, weak in shearing and tension. They withstand chemical erosion that occurs in other materials subjected to acidic or caustic environments. Ceramics generally can withstand very high temperatures, such as temperatures that range from 1,000 °C to 1,600 °C (1,800 °F to 3,000 °F). Glass is often not considered a ceramic because of its amorphous (noncrystalline) character. However, glassmaking involves several steps of the ceramic process and its mechanical properties are similar to ceramic materials.\n\nTraditional ceramic raw materials include clay minerals such as kaolinite, whereas more recent materials include aluminium oxide, more commonly known as alumina. The modern ceramic materials, which are classified as advanced ceramics, include silicon carbide and tungsten carbide. Both are valued for their abrasion resistance, and hence find use in applications such as the wear plates of crushing equipment in mining operations. Advanced ceramics are also used in the medicine, electrical, electronics industries and body armor.\n\nCrystalline ceramic materials are not amenable to a great range of processing. Methods for dealing with them tend to fall into one of two categories – either make the ceramic in the desired shape, by reaction \"in situ\", or by \"forming\" powders into the desired shape, and then sintering to form a solid body. Ceramic forming techniques include shaping by hand (sometimes including a rotation process called \"throwing\"), slip casting, tape casting (used for making very thin ceramic capacitors, e.g.), injection molding, dry pressing, and other variations. Details of these processes are described in the two books listed below. A few methods use a hybrid between the two approaches.\n\nNoncrystalline ceramics, being glass, tend to be formed from melts. The glass is shaped when either fully molten, by casting, or when in a state of toffee-like viscosity, by methods such as blowing into a mold. If later heat treatments cause this glass to become partly crystalline, the resulting material is known as a glass-ceramic, widely used as cook-top and also as a glass composite material for nuclear waste disposal.\n\nThe physical properties of any ceramic substance are a direct result of its crystalline structure and chemical composition. Solid state chemistry reveals the fundamental connection between microstructure and properties such as localized density variations, grain size distribution, type of porosity and second-phase content, which can all be correlated with ceramic properties such as mechanical strength σ by the Hall-Petch equation, hardness, toughness, dielectric constant, and the optical properties exhibited by transparent materials.\n\nPhysical properties of chemical compounds which provide evidence of chemical composition include odor, colour, volume, density (mass / volume), melting point, boiling point, heat capacity, physical form at room temperature (solid, liquid or gas), hardness, porosity, and index of refraction.\n\nCeramography is the art and science of preparation, examination and evaluation of ceramic microstructures. Evaluation and characterization of ceramic microstructures is often implemented on similar spatial scales to that used commonly in the emerging field of nanotechnology: from tens of angstroms (A) to tens of micrometers (µm). This is typically somewhere between the minimum wavelength of visible light and the resolution limit of the naked eye.\n\nThe microstructure includes most grains, secondary phases, grain boundaries, pores, micro-cracks, structural defects and hardness microindentions. Most bulk mechanical, optical, thermal, electrical and magnetic properties are significantly affected by the observed microstructure. The fabrication method and process conditions are generally indicated by the microstructure. The root cause of many ceramic failures is evident in the cleaved and polished microstructure. Physical properties which constitute the field of materials science and engineering include the following:\n\nMechanical properties are important in structural and building materials as well as textile fabrics. They include the many properties used to describe the strength of materials such as: elasticity / plasticity, tensile strength, compressive strength, shear strength, fracture toughness & ductility (low in brittle materials), and indentation hardness.\n\nIn modern materials science, fracture mechanics is an important tool in improving the mechanical performance of materials and components. It applies the physics of stress and strain, in particular the theories of elasticity and plasticity, to the microscopic crystallographic defects found in real materials in order to predict the macroscopic mechanical failure of bodies. Fractography is widely used with fracture mechanics to understand the causes of failures and also verify the theoretical failure predictions with real life failures.\n\nCeramic materials are usually ionic or covalent bonded materials, and can be crystalline or amorphous. A material held together by either type of bond will tend to fracture before any plastic deformation takes place, which results in poor toughness in these materials. Additionally, because these materials tend to be porous, the pores and other microscopic imperfections act as stress concentrators, decreasing the toughness further, and reducing the tensile strength. These combine to give catastrophic failures, as opposed to the normally much more gentle failure modes of metals.\n\nThese materials do show plastic deformation. However, due to the rigid structure of the crystalline materials, there are very few available slip systems for dislocations to move, and so they deform very slowly. With the non-crystalline (glassy) materials, viscous flow is the dominant source of plastic deformation, and is also very slow. It is therefore neglected in many applications of ceramic materials.\n\nTo overcome the brittle behaviour, ceramic material development has introduced the class of ceramic matrix composite materials, in which ceramic fibers are embedded and with specific coatings are forming fiber bridges across any crack. This mechanism substantially increases the fracture toughness of such ceramics. The ceramic disc brakes are, for example using a ceramic matrix composite material manufactured with a specific process.\n\nSome ceramics are semiconductors. Most of these are transition metal oxides that are II-VI semiconductors, such as zinc oxide.\n\nWhile there are prospects of mass-producing blue LEDs from zinc oxide, ceramicists are most interested in the electrical properties that show grain boundary effects.\n\nOne of the most widely used of these is the varistor. These are devices that exhibit the property that resistance drops sharply at a certain threshold voltage. Once the voltage across the device reaches the threshold, there is a breakdown of the electrical structure in the vicinity of the grain boundaries, which results in its electrical resistance dropping from several megohms down to a few hundred ohms. The major advantage of these is that they can dissipate a lot of energy, and they self-reset – after the voltage across the device drops below the threshold, its resistance returns to being high.\n\nThis makes them ideal for surge-protection applications; as there is control over the threshold voltage and energy tolerance, they find use in all sorts of applications. The best demonstration of their ability can be found in electrical substations, where they are employed to protect the infrastructure from lightning strikes. They have rapid response, are low maintenance, and do not appreciably degrade from use, making them virtually ideal devices for this application.\n\nSemiconducting ceramics are also employed as gas sensors. When various gases are passed over a polycrystalline ceramic, its electrical resistance changes. With tuning to the possible gas mixtures, very inexpensive devices can be produced.\n\nUnder some conditions, such as extremely low temperature, some ceramics exhibit high-temperature superconductivity. The exact reason for this is not known, but there are two major families of superconducting ceramics.\n\nPiezoelectricity, a link between electrical and mechanical response, is exhibited by a large number of ceramic materials, including the quartz used to measure time in watches and other electronics. Such devices use both properties of piezoelectrics, using electricity to produce a mechanical motion (powering the device) and then using this mechanical motion to produce electricity (generating a signal). The unit of time measured is the natural interval required for electricity to be converted into mechanical energy and back again.\n\nThe piezoelectric effect is generally stronger in materials that also exhibit pyroelectricity, and all pyroelectric materials are also piezoelectric. These materials can be used to inter convert between thermal, mechanical, or electrical energy; for instance, after synthesis in a furnace, a pyroelectric crystal allowed to cool under no applied stress generally builds up a static charge of thousands of volts. Such materials are used in motion sensors, where the tiny rise in temperature from a warm body entering the room is enough to produce a measurable voltage in the crystal.\n\nIn turn, pyroelectricity is seen most strongly in materials which also display the ferroelectric effect, in which a stable electric dipole can be oriented or reversed by applying an electrostatic field. Pyroelectricity is also a necessary consequence of ferroelectricity. This can be used to store information in ferroelectric capacitors, elements of ferroelectric RAM.\n\nThe most common such materials are lead zirconate titanate and barium titanate. Aside from the uses mentioned above, their strong piezoelectric response is exploited in the design of high-frequency loudspeakers, transducers for sonar, and actuators for atomic force and scanning tunneling microscopes.\n\nIncreases in temperature can cause grain boundaries to suddenly become insulating in some semiconducting ceramic materials, mostly mixtures of heavy metal titanates. The critical transition temperature can be adjusted over a wide range by variations in chemistry. In such materials, current will pass through the material until joule heating brings it to the transition temperature, at which point the circuit will be broken and current flow will cease. Such ceramics are used as self-controlled heating elements in, for example, the rear-window defrost circuits of automobiles.\n\nAt the transition temperature, the material's dielectric response becomes theoretically infinite. While a lack of temperature control would rule out any practical use of the material near its critical temperature, the dielectric effect remains exceptionally strong even at much higher temperatures. Titanates with critical temperatures far below room temperature have become synonymous with \"ceramic\" in the context of ceramic capacitors for just this reason.\n\nOptically transparent materials focus on the response of a material to incoming lightwaves of a range of wavelengths. Frequency selective optical filters can be utilized to alter or enhance the brightness and contrast of a digital image. Guided lightwave transmission via frequency selective waveguides involves the emerging field of fiber optics and the ability of certain glassy compositions as a transmission medium for a range of frequencies simultaneously (multi-mode optical fiber) with little or no interference between competing wavelengths or frequencies. This resonant mode of energy and data transmission via electromagnetic (light) wave propagation, though low powered, is virtually lossless. Optical waveguides are used as components in Integrated optical circuits (e.g. light-emitting diodes, LEDs) or as the transmission medium in local and long haul optical communication systems. Also of value to the emerging materials scientist is the sensitivity of materials to radiation in the thermal infrared (IR) portion of the electromagnetic spectrum. This heat-seeking ability is responsible for such diverse optical phenomena as Night-vision and IR luminescence.\n\nThus, there is an increasing need in the military sector for high-strength, robust materials which have the capability to transmit light (electromagnetic waves) in the visible (0.4 – 0.7 micrometers) and mid-infrared (1 – 5 micrometers) regions of the spectrum. These materials are needed for applications requiring transparent armor, including next-generation high-speed missiles and pods, as well as protection against improvised explosive devices (IED).\n\nIn the 1960s, scientists at General Electric (GE) discovered that under the right manufacturing conditions, some ceramics, especially aluminium oxide (alumina), could be made translucent. These translucent materials were transparent enough to be used for containing the electrical plasma generated in high-pressure sodium street lamps. During the past two decades, additional types of transparent ceramics have been developed for applications such as nose cones for heat-seeking missiles, windows for fighter aircraft, and scintillation counters for computed tomography scanners.\n\nIn the early 1970s, Thomas Soules pioneered computer modeling of light transmission through translucent ceramic alumina. His model showed that microscopic pores in ceramic, mainly trapped at the junctions of microcrystalline grains, caused light to scatter and prevented true transparency. The volume fraction of these microscopic pores had to be less than 1% for high-quality optical transmission.\n\nThis is basically a particle size effect. Opacity results from the incoherent scattering of light at surfaces and interfaces. In addition to pores, most of the interfaces in a typical metal or ceramic object are in the form of grain boundaries which separate tiny regions of crystalline order. When the size of the scattering center (or grain boundary) is reduced below the size of the wavelength of the light being scattered, the scattering no longer occurs to any significant extent.\n\nIn the formation of polycrystalline materials (metals and ceramics) the size of the crystalline grains is determined largely by the size of the crystalline particles present in the raw material during formation (or pressing) of the object. Moreover, the size of the grain boundaries scales directly with particle size. Thus a reduction of the original particle size below the wavelength of visible light (~ 0.5 micrometers for shortwave violet) eliminates any light scattering, resulting in a transparent material.\n\nRecently, Japanese scientists have developed techniques to produce ceramic parts that rival the transparency of traditional crystals (grown from a single seed) and exceed the fracture toughness of a single crystal. In particular, scientists at the Japanese firm Konoshima Ltd., a producer of ceramic construction materials and industrial chemicals, have been looking for markets for their transparent ceramics.\n\nLivermore researchers realized that these ceramics might greatly benefit high-powered lasers used in the National Ignition Facility (NIF) Programs Directorate. In particular, a Livermore research team began to acquire advanced transparent ceramics from Konoshima to determine if they could meet the optical requirements needed for Livermore’s Solid-State Heat Capacity Laser (SSHCL). Livermore researchers have also been testing applications of these materials for applications such as advanced drivers for laser-driven fusion power plants.\n\nUntil the 1950s, the most important ceramic materials were (1) pottery, bricks and tiles, (2) cements and (3) glass. A composite material of ceramic and metal is known as cermet.\n\nOther ceramic materials, generally requiring greater purity in their make-up than those above, include forms of several chemical compounds, including:\n\nFor convenience, ceramic products are usually divided into four main types; these are shown below with some examples:\n\nFrequently, the raw materials of modern ceramics do not include clays.\nThose that do are classified as follows:\n\nCeramics can also be classified into three distinct material categories: \n\nEach one of these classes can develop unique material properties because ceramics tend to be crystalline.\n\n\nCeramic artifacts have an important role in archaeology for understanding the culture, technology and behavior of peoples of the past. They are among the most common artifacts to be found at an archaeological site, generally in the form of small fragments of broken pottery called sherds. Processing of collected sherds can be consistent with two main types of analysis: technical and traditional.\n\nTraditional analysis involves sorting ceramic artifacts, sherds and larger fragments into specific types based on style, composition, manufacturing and morphology. By creating these typologies it is possible to distinguish between different cultural styles, the purpose of the ceramic and technological state of the people among other conclusions. In addition, by looking at stylistic changes of ceramics over time is it possible to separate (seriate) the ceramics into distinct diagnostic groups (assemblages). A comparison of ceramic artifacts with known dated assemblages allows for a chronological assignment of these pieces.\n\nThe technical approach to ceramic analysis involves a finer examination of the composition of ceramic artifacts and sherds to determine the source of the material and through this the possible manufacturing site. Key criteria are the composition of the clay and the temper used in the manufacture of the article under study: temper is a material added to the clay during the initial production stage, and it is used to aid the subsequent drying process. Types of temper include shell pieces, granite fragments and ground sherd pieces called 'grog'. Temper is usually identified by microscopic examination of the temper material. Clay identification is determined by a process of refiring the ceramic, and assigning a color to it using Munsell Soil Color notation. By estimating both the clay and temper compositions, and locating a region where both are known to occur, an assignment of the material source can be made. From the source assignment of the artifact further investigations can be made into the site of manufacture.\n\n\n"}
{"id": "6459", "url": "https://en.wikipedia.org/wiki?curid=6459", "title": "Wu Xing", "text": "Wu Xing\n\nThe Wu Xing (), also known as the Five Elements, Five Phases, the Five Agents, the Five Movements, Five Processes, the Five Steps/Stages and the Five Planets is the short form of \"Wǔ zhǒng liúxíng zhī qì\" (五種流行之氣) or \"the five types of chi dominating at different times\". It is a fivefold conceptual scheme that many traditional Chinese fields used to explain a wide array of phenomena, from cosmic cycles to the interaction between internal organs, and from the succession of political regimes to the properties of medicinal drugs. The \"Five Phases\" are Wood (木 \"mù\"), Fire (火 \"huǒ\"), Earth (土 \"tǔ\"), Metal (金 \"jīn\"), and Water (水 \"shuǐ\"). This order of presentation is known as the \"mutual generation\" (相生 \"xiāngshēng\") sequence. In the order of \"mutual overcoming\" (相剋/相克 \"xiāngkè\"), they are Wood, Earth, Water, Fire, and Metal.\n\nThe system of five phases was used for describing interactions and relationships between phenomena. After it came to maturity in the second or first century BCE during the Han dynasty, this device was employed in many fields of early Chinese thought, including seemingly disparate fields such as geomancy or Feng shui, astrology, traditional Chinese medicine, music, military strategy, and martial arts. The system is still used as a reference in some forms of complementary and alternative medicine and martial arts.\n\nXing () of 'Wu Xing' means moving; a planet is called a 'moving star'() in Chinese. Wu Xing () originally refers to the five major planets (Jupiter, Saturn, Mercury, Venus, Mars) that create five dimensions of earth life. \"Wu Xing\" is also widely translated as Five Elements and this is used extensively by many including practitioners of Five Element acupuncture. This translation arose by false analogy with the Western system of the four elements. Whereas the classical Greek elements were concerned with substances or natural qualities, the Chinese \"xíng\" are \"primarily concerned with process and change,\" hence the common translation as \"phases\" or \"agents\". By the same token, \"Mù\" is thought of as \"Tree\" rather than \"Wood\". The word 'element' is thus used within the context of Chinese medicine with a different meaning to its usual meaning.\n\nIt should be recognized that the word \"phase\", although commonly preferred, is not perfect. \"Phase\" is a better translation for the five \"seasons\" (五運 Wǔ Yùn) mentioned below, and so \"agents\" or \"processes\" might be preferred for the primary term \"xíng\". Manfred Porkert attempts to resolve this by using \"Evolutive Phase\" for 五行 \"Wǔ Xíng\" and \"Circuit Phase\" for 五運 \"Wǔ Yùn\", but these terms are unwieldy.\n\nSome of the Mawangdui Silk Texts (no later than 168 BC) also present the Wu Xing as \"five virtues\" or types of activities. Within Chinese medicine texts the Wu Xing are also referred to as Wu Yun (五運 wǔ yùn) or a combination of the two characters (Wu Xing-Yun) these emphasise the correspondence of five elements to five 'seasons' (four seasons plus one). Another tradition refers to the \"Wǔ Xíng\" as \"Wǔ Dé\" (五德), the Five Virtues ().\n\nThe five phases are usually used to describe the state in nature:\n\nThe doctrine of five phases describes two cycles, a generating or creation (生, \"shēng\") cycle, also known as \"mother-son\", and an overcoming or destruction (剋/克, \"kè\") cycle, also known as \"grandfather-grandson\", of interactions between the phases. Within Chinese medicine the effects of these two main relations are further elaborated:\n\nThe common memory jogs, which help to remind in what order the phases are:\n\n\nOther common words for this cycle include \"begets\", \"engenders\" and \"mothers\".\n\n\nThis cycle might also be called \"controls\", \"restrains\" or \"fathers\".\n\nAccording to Wu Xing theory, the structure of the cosmos mirrors the five phases. Each phase has a complex series of associations with different aspects of nature, as can be seen in the following table. In the ancient Chinese form of geomancy known as Feng Shui practitioners all based their art and system on the five phases (Wu Xing). All of these phases are represented within the trigrams. Associated with these phases are colors, seasons and shapes; all of which are interacting with each other.\n\nBased on a particular directional energy flow from one phase to the next, the interaction can be expansive, destructive, or exhaustive. A proper knowledge of each aspect of energy flow will enable the Feng Shui practitioner to apply certain cures or rearrangement of energy in a way they believe to be beneficial for the receiver of the Feng Shui Treatment.\n\nAccording to the Warring States period political philosopher Zou Yan 鄒衍 (ca. 305-240 BCE), each of the five elements possesses a personified “virtue” (\"de\" 德), which indicates the foreordained destiny (\"yun\" 運) of a dynasty; accordingly, the cyclic succession of the elements also indicates dynastic transitions. Zou Yan claims that the Mandate of Heaven sanctions the legitimacy of a dynasty by sending self-manifesting auspicious signs in the ritual color (yellow, blue, white, red, and black) that matches the element of the new dynasty (Earth, Wood, Metal, Fire, and Water). From the Qin dynasty onward, most Chinese dynasties invoked the theory of the Five Elements to legitimate their reign.\n\nThe interdependence of zang-fu networks in the body was said to be a circle of five things, and so mapped by the Chinese doctors onto the five phases.\n\nIn Ziwei, \"neiyin\" (纳音) or the method of divination is the further classification of the Five Elements into 60 \"ming\" (命), or life orders, based on the ganzhi. Similar to the astrology zodiac, the ming is used by fortune-tellers to analyse a person's personality and future fate.\n\nThe \"Yuèlìng\" chapter (月令篇) of the \"Lǐjì\" (禮記) and the \"Huáinánzǐ\" (淮南子) make the following correlations:\n\n\nT'ai chi ch'uan uses the five elements to designate different directions, positions or footwork patterns. Either forward, backward, left, right and centre, or three steps forward (attack) and two steps back (retreat).\n\n\"The Five Steps (五步 wǔ bù):\"\n\nXingyiquan uses the five elements metaphorically to represent five different states of combat.\n\nThere are spring, summer, fall, and winter teas. The perennial tea ceremony (\"perennial\", literally means four steps or sequences that are linked together, each representing a season of the year) includes four tea settings (茶席) and a tea master (司茶). The tea settings are:\n\n\nEach tea setting is arranged and stands for the four directions (north, south, east, and west). A vase of the seasons' flowers is put on tea table. Sometimes if four tea masters are included then five chairs are arranged per tea setting, making a total of twenty plus the 4 tea masters equalling 24, which symbolizes the 24 solar terms of the Chinese calendar, and represents that nature continues or is perennial.\n\n\n\n"}
{"id": "6462", "url": "https://en.wikipedia.org/wiki?curid=6462", "title": "Church of Christ, Scientist", "text": "Church of Christ, Scientist\n\nThe Church of Christ, Scientist was founded in 1879 in Boston, Massachusetts, by Mary Baker Eddy, author of \"Science and Health with Key to the Scriptures,\" and founder of Christian Science. The church was founded \"to commemorate the word and works of [Christ Jesus]\" and \"reinstate primitive Christianity and its lost element of healing\". Sunday services are held throughout the year and weekly testimony meetings are held on Wednesday evenings, where following brief readings from the Bible and the Christian Science textbook, those in attendance are invited to give testimonies of healing brought about through Christian Science prayer.\n\nIn the early decades of the 20th century, Christian Science churches sprang up in communities around the world, though in the last several decades of that century, there was a marked decline in membership, except in Africa, where there has been growth. Headquartered in Boston, the church has a worldwide membership of about 85,000.\n\nThe church was incorporated by Mary Baker Eddy in 1879 following a claimed personal healing in 1866, which she said resulted from reading the Bible. The Bible and Eddy's textbook on Christian healing, \"Science and Health with Key to the Scriptures\", are together the church's key doctrinal sources and have been ordained as the church's \"dual impersonal pastor\".\n\nThe First Church of Christ, Scientist, is widely known for its publications, especially \"The Christian Science Monitor\", a weekly newspaper published internationally in print and online. The seal of Christian Science is a cross and crown with the words, \"Heal the sick, raise the dead, cleanse the lepers, cast out demons,\" and is a registered trademark of the church.\n\nThe Church has collected over 50,000 testimonies of incidents that it considers healing through Christian Science treatment alone. While most of these testimonies represent ailments neither diagnosed nor treated by medical professionals, the Church requires three other people to vouch for any testimony published in its official organ, the \"Christian Science Journal\"; verifiers say that they witnessed the healing or know the testifier well.\n\nChristian Scientists may take an intensive two-week \"Primary\" class from an authorized Christian Science teacher. Those who wish to become \"Journal-listed\" (accredited) practitioners, devoting themselves full-time to the practice of healing, must first have Primary class instruction. When they have what the church regards as a record of healing, they may submit their names for publication in the directory of practitioners and teachers in the \"Christian Science Journal.\" A practitioner who has been listed for at least three years' may apply for \"Normal\" class instruction, given once every three years. Those who receive a certificate are authorized to teach. Both Primary and Normal classes are based on the Bible and the writings of Mary Baker Eddy. The Primary class focuses on the chapter, \"Recapitulation\" in \"Science and Health with Key to the Scriptures\". This chapter uses the Socratic method of teaching and contains the \"Scientific Statement of Being\". The \"Normal\" class focuses on the platform of Christian Science, contained on pages 330-340 of \"Science and Health.\"\n\nThe First Church of Christ, Scientist is the legal title of the Mother Church and administrative headquarters of the Christian Science Church. The complex is located in a plaza alongside Huntington Avenue in the Back Bay neighborhood of Boston, Massachusetts.\n\nThe church itself was built in 1894, and an annex larger in footprint than the original structure was added in 1906. It boasts one of the world's largest pipe organs, built by the Aeolian-Skinner Company of Boston. The Mary Baker Eddy Library for the Betterment of Humanity is housed in an 11-story structure originally built for The Christian Science Publishing Society constructed between 1932 and 1934, and the present plaza was constructed in the late 1960s and early 1970s to include a 28 story administration building, a colonnade, and a reflecting pool with fountain, designed by Araldo Cossutta of I. M. Pei and Partners (now Pei Cobb Freed).\n\nBranch churches of The Mother Church may take the title of \"First Church of Christ, Scientist\"; Second; but the article \"The\" must not be used, presumably to concede the primacy of the Boston Mother Church.\n\nAn international newspaper, the \"Christian Science Monitor\", founded by Eddy in 1908 and winner of seven Pulitzer prizes, is published by the church through the Christian Science Publishing Society.\n\nBranch Christian Science churches and Christian Science societies are subordinate to the Mother Church, but are self-governed. They have their own by-laws, bank accounts, assets and officers, but in order to be recognised must abide by the by-laws in the \"Manual of The Mother Church\". Church services are regulated by the \"Manual,\" the set of by-laws written by Eddy, that establishes the church organization and explains the duties and responsibilities of members, officers, practitioners, teachers and nurses; and establishes rules for discipline and other aspects of church business.\n\nThe Christian Science Board of Directors is a five-person executive entity created by Mary Baker Eddy to conduct the business of the Christian Science Church under the terms defined in the by-laws of the \"Church Manual\". Its functions and restrictions are defined by the \"Manual.\"\n\nThe Board (occasionally CSBD or the BoD for short) also includes functions defined by a Deed of Trust written by Eddy (one of several, in fact) under which it consisted of four persons, though she later expanded the Board to five persons, thus in effect leaving one of its members out of Deed functions. This later bore on a dispute during the 1920s, known as the Great Litigation in CS circles, pivoting on whether the CSBD could remove trustees of the Christian Science Publishing Society or whether the CSPS trustees were established independently.\n\nWhile Eddy's Manual established limited executive functions under the rule of law in place of a traditional hierarchy, the controversial 1991 publication of a book by Bliss Knapp led the then Board of Directors to make the unusual affidavit during a suit over Knapp's estate that neither acts by it violating the \"Manual,\" nor acts refraining from required action, constituted violations of the \"Manual\". A traditionally-minded minority held that the Board's act in publishing Knapp's book constituted a fundamental violation of several by-laws and its legal trust, automatically mandating the offending Board members' resignations under Article I, Section 9.\n\nAnother minority believed that Eddy intended various requirements for her consent (in their view, \"estoppels\") to effect the church's dissolution on her death, since they could no longer be followed literally. Ironically, one of the stronger arguments against this position came from an individual highly respected by their theological quarter, Bliss Knapp, who claimed that Eddy understood through her lawyer that these consent clauses would not hinder normal operation after her decease.\n\nChurches worldwide hold a one-hour service each Sunday, consisting of hymns, prayer, and currently, readings from the \"King James Version\" (KJV) of the Bible (although there is no requirement that this version of the Bible be used) and \"Science and Health with Key to the Scriptures\". These readings are the weekly Lesson-Sermon, which is read aloud at all Sunday services in all Christian Science churches worldwide, and is studied by individuals at home throughout the preceding week. The Lesson, as it is informally called, is compiled by a committee at The Mother Church, and is usually made up of six sections, each of which consists of passages from the Bible (read by the Second Reader) and passages from \"Science and Health\" (read by the First Reader).\n\nEddy selected 26 subjects for the Lesson-Sermon. These Lessons run in continuous rotation in the order she established, hence each subject is studied twice a year. In years in which there are 53 Sundays, the topic \"Christ Jesus\" occurs a third time, in December. In addition, there is a special, shortened Lesson-Sermon for Thanksgiving Day. Branch churches outside the United States may schedule their Thanksgiving service when convenient for them, most choosing a day in October or November, and the Thanksgiving Day proclamation by the United States president, may be omitted.\n\nBecause there are no clergy in the church, branch church Sunday services are conducted by two Readers: the First Reader, who reads passages from Science and Health, and the Second Reader, who reads passages from the Bible. First Readers determine the beginning \"scriptural selection\", hymns to be sung on Sundays, and the benediction. The vast majority of the service is the reading of the weekly Bible lesson supplied by Boston, and the order of the service set out by the Manual. To be elected the First Reader in one's branch church is one of the highest and most important positions the lay Christian Scientist may aspire to.\n\nChurches also hold a one-hour Wednesday evening testimony meeting, with similar readings, after which, those in attendance are invited to share accounts of healing through prayer. At these services, the First Reader reads passages from the Bible and Science and Health. Departing from denominational practice for over 120 years, English language churches may now choose alternate Bible translations at these services (i.e. Phillips).\n\nBranch churches also sponsor annual public talks (called lectures) given by speakers selected annually by the Board of Lectureship in Boston.\n\nBeginning in the mid-1980s, church executives undertook a controversial and ambitious foray into electronic broadcast media. The first significant effort was to create a weekly half-hour syndicated television program, The Christian Science Monitor Reports. \"Monitor Reports\" was anchored in its first season by newspaper veteran Rob Nelson. He was replaced in the second by the \"Christian Science Monitor\"'s former Moscow correspondent, David Willis. The program was usually broadcast by independent stations — often at odd hours.\n\nIn 1988, Monitor Reports was supplanted by a nightly half-hour news show, World Monitor, which was broadcast by the Discovery Channel. The program was anchored by veteran journalist John Hart. The Church then purchased a Boston cable TV station for elaborate in-house programming production. In parallel, the church purchased a shortwave radio station and syndicated radio production to National Public Radio. However, revenues fell far short of optimistic predictions by church managers, who had ignored early warnings by members and media experts.\n\nIn October 1991, after a series of conflicts over the boundaries between Christian Science teachings and his journalistic independence, John Hart resigned. The Monitor Channel went off the air in June 1992. Most of the other operations closed in well under a decade. Public accounts in both the mainstream and trade media reported that the church lost approximately $250 million on these ventures.\n\nThe hundreds of millions lost on broadcasting brought the church to the brink of bankruptcy. However, with the 1991 publication of \"The Destiny of The Mother Church\" by the late Bliss Knapp, the church secured a $90 million bequest from the Knapp trust. The trust dictated that the book be published as \"Authorized Literature,\" with neither modification nor comment. Historically, the church had censured Knapp for deviating at several points from Eddy's teaching, and had refused to publish the work. The church's archivist, fired in anticipation of the book's publication, wrote to branch churches to inform them of the book's history. Many Christian Scientists thought the book violated the church's by-laws, and the editors of the church's religious periodicals and several other church employees resigned in protest. Alternate beneficiaries subsequently sued to contest the church's claim it had complied fully with the will's terms, and the church ultimately received only half of the original sum.\n\nThe fallout of the broadcasting debacle also sparked a minor revolt among some prominent church members. In late 1993, a group of Christian Scientists filed suit against the Board of Directors, alleging a willful disregard for the Manual of the Mother Church in its financial dealings. The suit was thrown out by the Supreme Judicial Court of Massachusetts in 1997, but a lingering discontent with the church's financial matters persists to this day.\n\nIn spite of its early meteoric rise, church membership has declined over the past eight decades, according to the church's former treasurer, J. Edward Odegaard. Though the Church is prohibited by the Manual from publishing membership figures, the number of branch churches in the United States has fallen steadily since World War II. In 2009, for the first time in church history, more new members came from Africa than the United States.\n\nIn 2005, the \"Boston Globe\" reported that the church was considering consolidating Boston operations into fewer buildings and leasing out space in buildings it owned. Church official Philip G. Davis noted that the administration and Colonnade buildings had not been fully used for many years and that vacancy increased after staff reductions in 2004. The church posted an $8 million financial loss in fiscal 2003, and in 2004 cut 125 jobs, a quarter of the staff, at the \"Christian Science Monitor\". Conversely, Davis noted that \"the financial situation right now is excellent\" and stated that the church was not facing financial problems.\n\n\n"}
{"id": "6466", "url": "https://en.wikipedia.org/wiki?curid=6466", "title": "Connecticut", "text": "Connecticut\n\nConnecticut () is the southernmost state in the New England region of the northeastern United States. As of the 2010 Census, Connecticut features the highest per-capita income, Human Development Index (0.962), and median household income in the United States. Connecticut is bordered by Rhode Island to the east, Massachusetts to the north, New York to the west, and Long Island Sound to the south. Its capital city is Hartford, and its most populous city is Bridgeport. Although Connecticut is technically part of New England, it is often grouped along with New York and New Jersey as the Tri-state area. The state is named for the Connecticut River, a major U.S. river that approximately bisects the state. The word \"Connecticut\" is derived from various anglicized spellings of an Algonquian word for \"long tidal river\". \n\nConnecticut is the third smallest state by area, the 29th most populous, and the fourth most densely populated of the 50 United States. It is known as the \"Constitution State\", the \"Nutmeg State\", the \"Provisions State\", and the \"Land of Steady Habits\". It was influential in the development of the federal government of the United States. Much of southern and western Connecticut (along with the majority of the state's population) is part of the New York metropolitan area; three of Connecticut's eight counties are statistically included in the New York City combined statistical area, which is widely referred to as the Tri-State area. Connecticut's center of population is in Cheshire, New Haven County, which is also located within the Tri-State area.\n\nConnecticut's first European settlers were Dutch. They established a small, short-lived settlement in present-day Hartford at the confluence of the Park and Connecticut rivers called Huys de Goede Hoop. Initially, half of Connecticut was a part of the Dutch colony New Netherland, which included much of the land between the Connecticut and Delaware rivers. The first major settlements were established in the 1630s by England. Thomas Hooker led a band of followers overland from the Massachusetts Bay Colony and founded what became the Connecticut Colony; other settlers from Massachusetts founded the Saybrook Colony and the New Haven Colony. The Connecticut and New Haven Colonies established documents of Fundamental Orders, considered the first constitutions in North America. In 1662, the three colonies were merged under a royal charter, making Connecticut a crown colony. This colony was one of the Thirteen Colonies that revolted against British rule in the American Revolution.\n\nThe Connecticut River, Thames River, and ports along the Long Island Sound have given Connecticut a strong maritime tradition which continues today. The state also has a long history of hosting the financial services industry, including insurance companies in Hartford and hedge funds in Fairfield County.\n\nConnecticut is bordered on the south by Long Island Sound, on the west by New York, on the north by Massachusetts, and on the east by Rhode Island. The state capital and third largest city is Hartford, and other major cities and towns (by population) include Bridgeport, New Haven, Stamford, Waterbury, Norwalk, Danbury, New Britain, Greenwich, and Bristol. Connecticut is slightly larger than the country of Montenegro. There are 169 incorporated towns in Connecticut.\nThe highest peak in Connecticut is Bear Mountain in Salisbury in the northwest corner of the state. The highest point is just east of where Connecticut, Massachusetts, and New York meet (42° 3' N; 73° 29' W), on the southern slope of Mount Frissell, whose peak lies nearby in Massachusetts. At the opposite extreme, many of the coastal towns have areas that are less than 20 feet above sea level.\n\nConnecticut has a long maritime history and a reputation based on that history—yet the state has no direct oceanfront (technically speaking). The coast of Connecticut sits on Long Island Sound, which is an estuary. The state's access to the open Atlantic Ocean is both to the west (toward New York City) and to the east (toward the \"race\" near Rhode Island). This situation provides many safe harbors from ocean storms, and many transatlantic ships seek anchor inside Long Island Sound when tropical cyclones pass off the upper East Coast.\n\nThe Connecticut River cuts through the center of the state, flowing into Long Island Sound. The most populous metropolitan region centered within the state lies in the Connecticut River Valley. Despite Connecticut's relatively small size, it features wide regional variations in its landscape; for example, in the northwestern Litchfield Hills, it features rolling mountains and horse farms, whereas in areas to the east of New Haven along the coast, the landscape features coastal marshes, beaches, and large scale maritime activities.\n\nConnecticut's rural areas and small towns in the northeast and northwest corners of the state contrast sharply with its industrial cities such as Stamford, Bridgeport, and New Haven, located along the coastal highways from the New York border to New London, then northward up the Connecticut River to Hartford. Many towns in northeastern and northwestern Connecticut center around a green, such as the Litchfield Green, Lebanon Green (the largest in the state), and Wethersfield Green (the oldest in the state). Near the green typically stand historical visual symbols of New England towns, such as a white church, a colonial meeting house, a colonial tavern or inn, several colonial houses, and so on, establishing a scenic historical appearance maintained for both historic preservation and tourism. Many of the areas in southern and coastal Connecticut have been built up and rebuilt over the years, and look less visually like traditional New England.\n\nThe northern boundary of the state with Massachusetts is marked by the Southwick Jog or Granby Notch, an approximately square detour into Connecticut. The origin of this anomaly is clearly established in a long line of disputes and temporary agreements which were finally concluded in 1804, when southern Southwick's residents sought to leave Massachusetts, and the town was split in half.\n\nThe southwestern border of Connecticut where it abuts New York State is marked by a panhandle in Fairfield County, containing the towns of Greenwich, Stamford, New Canaan, Darien, and parts of Norwalk and Wilton. This irregularity in the boundary is the result of territorial disputes in the late 17th century, culminating with New York giving up its claim to the area, whose residents considered themselves part of Connecticut, in exchange for an equivalent area extending northwards from Ridgefield to the Massachusetts border, as well as undisputed claim to Rye, New York.\nAreas maintained by the National Park Service include Appalachian National Scenic Trail, Quinebaug and Shetucket Rivers Valley National Heritage Corridor, and Weir Farm National Historic Site.\n\nMuch of Connecticut has a humid continental climate, with cold winters with moderate snowfall and warm, humid summers. Far southern and coastal Connecticut has a milder humid temperate climate (also called subtropical in some climate classifications), with hot, humid summers and warmer winters with a mix of rain and infrequent snow. Most of Connecticut sees a fairly even precipitation pattern with rainfall/snowfall spread throughout the 12 months. Connecticut averages 56% of possible sunshine, averaging 2,400 hours of sunshine annually.\n\nEarly spring (April) can range from slightly cool to warm, while mid and late spring (May/early June) is warm. By mid June, the building Bermuda High creates a southerly flow of warm and humid tropical air, bringing hot weather conditions throughout the state, with average highs in New London of and in Windsor Locks. Although summers are sunny in Connecticut, quick moving summer thunderstorms can bring brief downpours with thunder and lightning. Occasionally these thunderstorms can be severe, and the state usually averages one tornado per year. During hurricane season, the remains of tropical cyclones occasionally affect the region, though a direct hit is rare. \n\nFall type weather (cooler days and nights, fewer air masses thundershowers) starts in October and normally lasts to the first days of December. Daily high temperatures in October and November range from the 50's to 60's F with nights in the 40's and upper 30's F (November). Colorful foliage begins across northern parts of the state in early October and moves south and east reaching southeast Connecticut by early November. Far southern and coastal areas however have more oak and hickory trees (and fewer maples), and are often less colorful than areas to the north. By early December average overnight lows are below freezing across the entire state.\n\nWinters (December through mid March) are generally cold from south to north in Connecticut. The coldest month (January) has average high temperatures ranging from in the coastal lowlands to in the inland and northern portions on the state. The average yearly snowfall ranges from about in the higher elevations of the northern portion of the state to only along the southeast coast of Connecticut (Branford to Groton). Generally, any locale north or west of Interstate 84 receives the most snow, during a storm, and throughout the season. Most of Connecticut has less than 60 days of snow cover. Snow usually falls from late November to late March in the northern part of the state, and from early December to mid March in the southern and coastal parts of the state. \n\nConnecticut's warmest temperature is which occurred in Danbury on July 15, 1995; the coldest temperature is which occurred in the Northwest Hills Falls Village on February 16, 1943, and Coventry on January 22, 1961.\n\nForests consist of a mix of Northeastern coastal forests of Oak in southern areas of the state, to the upland New England-Acadian forests in the northwestern parts of the state. Mountain Laurel (Kalmia latifolia) is the state flower, and is native to low ridges in several parts of Connecticut. Rosebay Rhododendron (Rhododendron maximum) is also native to eastern uplands of Connecticut and Pachaug State Forest is home to the Rhododendron Sanctuary Trail. Atlantic white cedar (Chamaecyparis thyoides), is found in wetlands in the southern parts of the state. Connecticut has one native cactus (Opuntia humifusa), found in sandy coastal areas and low hillsides. Several types of beach grasses and wildflowers are also native to Connecticut.. Connecticut spans USDA Plant Hardiness Zones 5b to 7a. Coastal Connecticut is the broad transition zone where more southern and subtropical plants are cultivated. In some coastal communities, Magnolia grandiflora (southern magnolia), Crape Myrtles, scrub palms (Sabal minor), and other broadleaved evergreens are cultivated in small numbers.\n\nThe name Connecticut is derived from anglicized versions of the Algonquian word that has been translated as \"long tidal river\" and \"upon the long river\", referring to the Connecticut River. The Connecticut region was inhabited by multiple Indian tribes before European settlement and colonization, including the Mohegans, the Pequots, and the Paugusetts.\n\nThe first European explorer in Connecticut was Dutch explorer Adriaen Block. After he explored this region in 1614, Dutch fur traders sailed up the Connecticut River (then known by the Dutch as Versche Rivier, \"Fresh River\") and built a fort at Dutch Point in present-day Hartford, which they called \"House of Hope\" ().\n\nThe Connecticut Colony was originally a number of separate, smaller settlements at present-day Windsor, Wethersfield, Saybrook, Hartford, and New Haven. The first English settlers came in 1633 and settled at Windsor, and then at Wethersfield the following year. John Winthrop the Younger of Massachusetts received a commission to create a new colony at Saybrook at the mouth of the Connecticut River in 1635.\n\nThe main body of settlers came in one large group in 1636. They were Puritans from Massachusetts, led by Thomas Hooker, who established the Connecticut Colony at Hartford. The Quinnipiack Colony was established by John Davenport, Theophilus Eaton, and others at present-day New Haven in March 1638. The New Haven Colony had its own constitution, \"The Fundamental Agreement of the New Haven Colony\", which was signed on June 4, 1639.\n\nThe settlements were established without official sanction of the English Crown; each was an independent political entity. They naturally were presumptively English but, in a legal sense, they were only secessionist outposts of Massachusetts Bay or expansions from Plymouth Colony. In 1662, Winthrop traveled to England and obtained a charter from Charles II which united the settlements of Connecticut.\n\nHistorically important colonial settlements included Windsor (1633), Wethersfield (1634), Saybrook (1635), Hartford (1636), New Haven (1638), Fairfield (1639), Guilford (1639), Milford (1639), Stratford (1639), Farmington (1640), Stamford (1641), and New London (1646).\n\nThe Pequot War marked the first major clash between Colonial settlers and Indians in New England. The Pequots reacted with increasing aggression to Colonial settlements in their territory, while simultaneously taking lands from the Narragansett and Mohegan tribes. Settlers responded to a murder in 1636 with a raid on a Pequot village on Block Island; the Pequots laid siege to Saybrook Colony's garrison that autumn, then raided Wethersfield in the spring of 1637. Colonists declared war on the Pequots, organized a band of militia and allies from the Mohegan and Narragansett tribes, and attacked a Pequot village on the Mystic River, with death toll estimates ranging between 300 and 700 Pequots. After suffering another major loss at a battle in Fairfield, the Pequots asked for a truce and peace terms.\n\nThe western boundaries of Connecticut have been subject to change over time. The Hartford Treaty with the Dutch was signed on September 19, 1650, but it was never ratified by the British. According to it, the western boundary of Connecticut ran north from Greenwich Bay for a distance of , \"provided the said line come not within of Hudson River.\" This agreement was observed by both sides until war erupted between England and The Netherlands in 1652. Conflict continued concerning colonial limits until the Duke of York captured New Netherland in 1664.\n\nOn the other hand, Connecticut's original Charter in 1662 granted it all the land to the \"South Sea\"—that is, the Pacific Ocean. Most Colonial royal grants were for long east-west strips. Connecticut took its grant seriously and established a ninth county between the Susquehanna and Delaware Rivers named Westmoreland County. This resulted in the brief Pennamite Wars with Pennsylvania.\n\nYale College was established in 1701, providing Connecticut with an important institution to educate clergy and civil leaders. The Congregational church dominated religious life in the colony and, by extension, town affairs in many parts.\n\nConnecticut designated four delegates to the Second Continental Congress who signed the Declaration of Independence: Samuel Huntington, Roger Sherman, William Williams, and Oliver Wolcott.\n\nConnecticut's legislature authorized the outfitting of six new regiments in 1775, in the wake of the clashes between British regulars and Massachusetts militia at Lexington and Concord. There were some 1,200 Connecticut troops on hand at the Battle of Bunker Hill in June 1775.\n\nIn 1777, the British got word of Continental Army supplies in Danbury, and they landed an expeditionary force of some 2,000 troops in Westport. This force then marched to Danbury and destroyed homes and much of the depot. Continental Army troops and militia led by General David Wooster and General Benedict Arnold engaged them on their return march at Ridgefield in 1777.\n\nFor the winter of 1778–79, General George Washington decided to split the Continental Army into three divisions encircling New York City, where British General Sir Henry Clinton had taken up winter quarters. Major General Israel Putnam chose Redding as the winter encampment quarters for some 3,000 regulars and militia under his command. The Redding encampment allowed Putnam's soldiers to guard the replenished supply depot in Danbury and to support any operations along Long Island Sound and the Hudson River Valley. Some of the men were veterans of the winter encampment at Valley Forge, Pennsylvania the previous winter. Soldiers at the Redding camp endured supply shortages, cold temperatures, and significant snow, with some historians dubbing the encampment \"Connecticut's Valley Forge\".\n\nThe state was also the launching site for a number of raids against Long Island orchestrated by Samuel Holden Parsons and Benjamin Tallmadge, and provided men and material for the war effort, especially to Washington's army outside New York City. General William Tryon raided the Connecticut coast in July 1779, focusing on New Haven, Norwalk, and Fairfield. New London and Groton Heights were raided in September 1781 by Benedict Arnold, who had turned traitor to the British.\n\nConnecticut ratified the U.S. Constitution on January 9, 1788, becoming the fifth state. The state prospered during the era following the American Revolution, as mills and textile factories were built and seaports flourished from trade and fisheries.\n\nIn 1786, Connecticut ceded territory to the U.S. government that became part of the Northwest Territory. The state retained land extending across the northern part of present-day Ohio called the Connecticut Western Reserve. The Western Reserve section was settled largely by people from Connecticut, and they brought Connecticut place names to Ohio.\n\nConnecticut made agreements with Pennsylvania and New York which extinguished her land claims within those states' boundaries and created the Connecticut Panhandle. The state then ceded the Western Reserve in 1800 to the federal government, which brought it to its present boundaries (other than minor adjustments with Massachusetts).\n\nThe British blockade during the War of 1812 hurt exports and bolstered the influence of Federalists who opposed the war. The cessation of imports from Britain stimulated the construction of factories to manufacture textiles and machinery. Connecticut came to be recognized as a major center for manufacturing, due in part to the inventions of Eli Whitney and other early innovators of the Industrial Revolution.\n\nThe state was known for its political conservatism, typified by its Federalist party and the Yale College of Timothy Dwight. The foremost intellectuals were Dwight and Noah Webster, who compiled his great dictionary in New Haven. Religious tensions polarized the state, as the Congregational Church struggled to maintain traditional viewpoints, in alliance with the Federalists. The failure of the Hartford Convention in 1814 hurt the Federalist cause, with the Republican Party gaining control in 1817.\n\nConnecticut had been governed under the \"Fundamental Orders\" since 1639, but the state adopted a new constitution in 1818.\n\nConnecticut manufacturers played a major role in supplying the Union forces with weapons and supplies during the Civil War. The state furnished 55,000 men, formed into thirty full regiments of infantry, including two in the U.S. Colored Troops, with several Connecticut men becoming generals. The Navy attracted 250 officers and 2,100 men, and Glastonbury native Gideon Welles was Secretary of the Navy. James H. Ward of Hartford was the first U.S. Naval Officer killed in the Civil War. Connecticut casualties included 2,088 killed in combat, 2,801 dying from disease, and 689 dying in Confederate prison camps.\n\nA surge of national unity in 1861 brought thousands flocking to the colors from every town and city. However, as the war became a crusade to end slavery, many Democrats (especially Irish Catholics) pulled back. The Democrats took a pro-slavery position and included many Copperheads willing to let the South secede. The intensely fought 1863 election for governor was narrowly won by the Republicans.\n\nConnecticut's extensive industry, dense population, flat terrain, and wealth encouraged the construction of railroads starting in 1839. By 1840, of line were in operation, growing to in 1850 and in 1860.\n\nThe New York, New Haven and Hartford Railroad, called the \"New Haven\" or \"The Consolidated\", became the dominant Connecticut railroad company after 1872. J. P. Morgan began financing the major New England railroads in the 1890s, dividing territory so that they would not compete. The New Haven purchased 50 smaller companies, including steamship lines, and built a network of light rails (electrified trolleys) that provided inter-urban transportation for all of southern New England. By 1912, the New Haven operated over of track with 120,000 employees.\n\nIn 1875, the first telephone exchange in the world was established in New Haven.\n\nWhen World War I broke out in 1914, Connecticut became a major supplier of weaponry to the U.S. military; by 1918, 80% of the state's industries were producing goods for the war effort. Remington Arms in Bridgeport produced half the small-arms cartridges used by the U.S. Army, with other major suppliers including Winchester in New Haven and Colt in Hartford.\n\nConnecticut was also an important U.S. Navy supplier, with Electric Boat receiving orders for 85 submarines, Lake Torpedo Boat building more than 20 subs, and the Groton Iron Works building freighters. On June 21, 1916, the U.S. Navy made Groton the site for its East Coast submarine base and school.\n\nThe state enthusiastically supported the American war effort in 1917 and 1918, with large purchases of war bonds, a further expansion of industry, and an emphasis on increasing food production on the farms. Thousands of state, local, and volunteer groups mobilized for the war effort and were coordinated by the Connecticut State Council of Defense. Manufacturers wrestled with manpower shortages; Waterbury's American Brass and Manufacturing Company was running at half capacity, so the federal government agreed to furlough soldiers to work there.\n\nIn 1919, J. Henry Roraback started the Connecticut Light & Power Co. which became the state's dominant electric utility. In 1925, Frederick Rentschler spurred the creation of Pratt & Whitney in Hartford to develop engines for aircraft; the company became an important military supplier in World War II and one of the three major manufacturers of jet engines in the world.\n\nOn September 21, 1938, the most destructive storm in New England history struck eastern Connecticut, killing hundreds of people. The eye of the \"Long Island Express\" passed just west of New Haven and devastated the Connecticut shoreline between Old Saybrook and Stonington from the full force of wind and waves, even though they had partial protection by Long Island. The hurricane caused extensive damage to infrastructure, homes, and businesses. In New London, a 500-foot sailing ship was driven into a warehouse complex, causing a major fire. Heavy rainfall caused the Connecticut River to flood downtown Hartford and East Hartford. An estimated 50,000 trees fell onto roadways.\n\nThe advent of Lend-Lease in support of Britain helped lift Connecticut from the Great Depression, with the state a major production center for weaponry and supplies used in World War II. Connecticut manufactured 4.1 percent of total U.S. military armaments produced during World War II, ranking ninth among the 48 states, with major factories including Colt for firearms, Pratt & Whitney for aircraft engines, Chance Vought for fighter planes, Hamilton Standard for propellers, and Electric Boat for submarines and PT boats. In Bridgeport, General Electric produced a significant new weapon to combat tanks: the bazooka.\n\nOn May 13, 1940, Igor Sikorsky made an untethered flight of the first practical helicopter. The helicopter saw limited use in World War II, but future military production made Sikorsky Aircraft's Stratford plant Connecticut's largest single manufacturing site by the start of the 21st century.\n\nConnecticut lost some wartime factories following the end of hostilities, but the state shared in a general post-war expansion that included the construction of highways and resulting in middle-class growth in suburban areas.\n\nPrescott Bush represented Connecticut in the U.S. Senate from 1952 to 1963; his son George H.W. Bush and grandson George W. Bush both became Presidents of the United States. In 1965, Connecticut ratified its current constitution, replacing the document that had served since 1818.\n\nIn 1968, commercial operation began for the Connecticut Yankee Nuclear Power Plant in East Haddam; in 1970, the Millstone Nuclear Power Station began operations in Waterford. In 1974, Connecticut elected Democratic Governor Ella T. Grasso, who became the first woman in any state to be elected governor.\n\nConnecticut's dependence on the defense industry posed an economic challenge at the end of the Cold War. The resulting budget crisis helped elect Lowell Weicker as governor on a third-party ticket in 1990. Weicker's remedy was a state income tax which proved effective in balancing the budget, but only for the short-term. He did not run for a second term, in part because of this politically unpopular move.\n\nIn 1992, initial construction was completed on Foxwoods Casino at the Mashantucket Pequots reservation in eastern Connecticut, which became the largest casino in the Western Hemisphere. Mohegan Sun followed four years later.\n\nIn 2000, presidential candidate Al Gore chose Senator Joe Lieberman as his running mate, marking the first time that a major party presidential ticket included someone of the Jewish faith. Gore and Lieberman fell five votes short of George W. Bush and Dick Cheney in the Electoral College.\nIn the terrorist attacks of September 11, 2001, 65 state residents were killed, mostly Fairfield County residents who were working in the World Trade Center.\nIn 2004, Republican Governor John G. Rowland resigned during a corruption investigation, later pleading guilty to federal charges.\nConnecticut was hit by three major storms in just over 14 months in 2011 and 2012, with all three causing extensive property damage and electric outages. Hurricane Irene struck Connecticut August 28, and damage totaled $235 million. Two months later, the \"Halloween nor'easter\" dropped extensive snow onto trees, resulting in snapped branches and trunks that damaged power lines; some areas were without electricity for 11 days. Hurricane Sandy had tropical storm-force winds when it reached Connecticut October 29, 2012. Sandy's winds drove storm surges into streets and cut power to 98 percent of homes and businesses, with more than $360 million in damage.\n\nOn December 14, 2012, Adam Lanza shot and killed 26 people at Sandy Hook Elementary School in Newtown, and then killed himself. The massacre spurred renewed efforts by activists for tighter laws on gun ownership nationally.\n\nIn the summer and fall of 2016, Connecticut experienced a drought in many parts of the state, causing some water-use bans. As of , 45% of the state was listed at Severe Drought by the US Drought Monitor, including almost all of Hartford and Litchfield counties. All the rest of the state was in Moderate Drought or Severe Drought, including Middlesex, Fairfield, New London, New Haven, Windham, and Tolland counties. This affected the agricultural economy in the state.\n\nThe United States Census Bureau estimates that the population of Connecticut was 3,590,886 on July 1, 2015, a 0.47% increase since the 2010 United States Census.\n\nAs of 2015, Connecticut had an estimated population of 3,590,886, which is an decrease of 5,791, or -0.16%, from the prior year and an increase of 16,789, or 0.47%, since the year 2010. This includes a natural increase since the last census of 67,427 people (that is 222,222 births minus 154,795 deaths) and an increase due to net migration of 41,718 people into the state. Immigration from outside the United States resulted in a net increase of 75,991 people, and migration within the country produced a net loss of 34,273 people. Based on the 2005 estimates, Connecticut moved from the 29th most populous state to 30th. 2016 estimates put Connecticut's population at 3,576,452.\n\n6.6% of its population was reported as being under 5 years old, 24.7% under 18 years old, and 13.8% were 65 years of age or older. Females made up approximately 51.6% of the population, with 48.4% male.\n\nIn 1790, 97% of the population in Connecticut was classified as \"rural\". The first census in which less than half the population was classified as rural was 1890. In the 2000 census, only 12.3% was considered rural. Most of western and southern Connecticut (particularly the Gold Coast) is strongly associated with New York City; this area is the most affluent and populous region of the state and has high property costs and high incomes. The center of population of Connecticut is located in the town of Cheshire.\nAs of the 2010 U.S. Census, Connecticut's race and ethnic percentages were:\n\nIn the same year Hispanics and Latinos of any race made up 13.4% of the population.\n\nThe state's most populous ethnic group, Non-Hispanic White, has declined from 98% in 1940 to 71% in 2010.\n\nAs of 2004, 11.4% of the population (400,000) was foreign-born. In 1870, native-born Americans had accounted for 75% of the state's population, but that had dropped to 35% by 1918.\n\nAs of 2000, 81.69% of Connecticut residents age 5 and older spoke English at home and 8.42% spoke Spanish, followed by Italian at 1.59%, French at 1.31% and Polish at 1.20%.\n\nThe largest European ancestry groups are:\nConnecticut has large Italian American, Irish American and English American populations, as well as German American and Polish American populations, with the Italian American population having the second highest percentage of any state, behind Rhode Island (19.3%). Italian is the largest ancestry group in five of the state's counties, while the Irish are the largest group in Tolland county, French Canadians the largest group in Windham county. Connecticut has the highest percentage of Puerto Ricans of any state. African Americans and Hispanics (mostly Puerto Ricans) are numerous in the urban areas of the state. Connecticut is also known for its relatively large Hungarian American population, the majority of which live in and around Fairfield, Stamford, Naugatuck and Bridgeport. Connecticut also has a sizable Polish American population, with New Britain containing the largest Polish American population in the state.\n\nMore recent immigrant populations include those from Jamaica, Guatemala, Haiti, Dominican Republic, Mexico, India, Philippines, Laos, Vietnam, Thailand, Cambodia, Indonesia, Brazil, Panama, Cape Verde and former Soviet countries.\n\nAs of 2011, 46.1% of Connecticut's population younger than age 1 were minorities.\n\n\"Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number.\"\nThe religious affiliations of the people of Connecticut as of 2014:\n\nA Pew survey of Connecticut residents' religious self-identification showed the following distribution of affiliations: Protestant 27%, Mormonism 0.5%, Jewish 1%, Roman Catholic 43%, Orthodox 1%, Non-religious 23%, Jehovah's Witness 1%, Hinduism 0.5%, Buddhism 1% and Islam 0.5%. Jewish congregations had 108,280 (3.2%) members in 2000. The Jewish population is concentrated in the towns near Long Island Sound between Greenwich and New Haven, in Greater New Haven and in Greater Hartford, especially the suburb of West Hartford. According to the Association of Religion Data Archives, the largest Christian denominations, by number of adherents, in 2010 were: the Catholic Church, with 1,252,936; the United Church of Christ, with 96,506; and non-denominational Evangelical Protestants, with 72,863.\n\nRecent immigration has brought other non-Christian religions to the state, but the numbers of adherents of other religions are still low. Connecticut is also home to New England's largest Protestant Church: The First Cathedral in Bloomfield, Connecticut located in Hartford County. Hartford is seat to the Roman Catholic Archdiocese of Hartford, which is sovereign over the Diocese of Bridgeport and the Diocese of Norwich.\n\nThe total gross state product for 2012 was $229.3 billion, up from $225.4 billion in 2011.\n\nConnecticut's per capita personal income in 2013 was estimated at $60,847, the highest of any state. There is, however, a great disparity in incomes throughout the state; after New York, Connecticut had the second largest gap nationwide between the average incomes of the top 1 percent and the average incomes of the bottom 99 percent. According to a 2013 study by Phoenix Marketing International, Connecticut had the third-largest number of millionaires per capita in the United States, with a ratio of 7.32 percent. New Canaan is the wealthiest town in Connecticut, with a per capita income of $85,459. Darien, Greenwich, Weston, Westport and Wilton also have per capita incomes over $65,000. Hartford is the poorest municipality in Connecticut, with a per capita income of $13,428 in 2000.\n\nThe state's seasonally adjusted unemployment rate in May 2016 was 5.7 percent, the 41st highest in the nation.\n\nBefore 1991, Connecticut had an investment-only income tax system. Income from employment was untaxed, but income from investments was taxed at 13%, the highest rate in the U.S., with no deductions allowed for costs of producing the investment income, such as interest on borrowing.\n\nIn 1991, under Governor Lowell P. Weicker, Jr., an Independent, the system was changed to one in which the taxes on employment income and investment income were equalized at a maximum rate of 4%. The new tax policy drew investment firms to Connecticut; as of 2014, Fairfield County was home to the headquarters for 14 of the 200 largest hedge funds in the world.\n\nAs of 2014, the income tax rates on Connecticut individuals are divided into six tax brackets of 3% (on income up to $10,000); 5% ($10,000-$50,000); 5.5% ($50,000-$100,000); 6% ($100,000-$200,000); 6.5% ($200,000-$250,000); and 6.7% (more than $250,000), with additional amounts owed depending on the bracket.\n\nAll wages of Connecticut residents are subject to the state's income tax, even if earned outside the state. However, in those cases, Connecticut income tax must be withheld only to the extent the Connecticut tax exceeds the amount withheld by the other jurisdiction. Since New York and Massachusetts have higher tax rates than Connecticut, this effectively means that Connecticut residents that work in those states have no Connecticut income tax withheld. Connecticut permits a credit for taxes paid to other jurisdictions, but since residents who work in other states are still subject to Connecticut income taxation, they may owe taxes if the jurisdictional credit does not fully offset the Connecticut tax amount.\n\nConnecticut levies a 6.35% state sales tax on the retail sale, lease, or rental of most goods. Some items and services in general are not subject to sales and use taxes unless specifically enumerated as taxable by statute. A provision excluding clothing under $50 from sales tax was repealed as of July 1, 2011. There are no additional sales taxes imposed by local jurisdictions. In August 2013, Connecticut authorized a sales tax \"holiday\" for one week during which retailers did not have to remit sales tax on certain items and quantities of clothing.\n\nAll real and personal property located within the state of Connecticut is taxable unless specifically exempted by statute. All assessments are at 70% of fair market value. Another 20% of the value may be taxed by the local government though. The maximum property tax credit is $300 per return and any excess may not be refunded or carried forward. Connecticut does not levy an intangible personal property tax. According to the Tax Foundation, the 2010 Census data shows Connecticut residents paying the 2nd highest average property taxes in the nation with only New Jersey ahead of them.\n\nThe Tax Foundation determined Connecticut residents had the third highest burden in the nation for state and local taxes at 11.86%, or $7,150, compared to the national average of 9.8%.\n\nAs of 2014, the gasoline tax in Connecticut is 49.3 cents per gallon (the third highest in the nation) and the diesel tax is 54.9 cents per gallon (the highest in the nation).\n\nOf home-sale transactions that closed in March 2014, the median home in Connecticut sold for $225,000, up 3.2% from March 2013. Connecticut ranked ninth nationally in foreclosure activity as of April 2014, with one of every 887 residential units involved in a foreclosure proceeding, or 0.11% of the total housing stock., including City Place I and the Traveler's Tower, both housing the major insurance industry.\n\nFinance and insurance is Connecticut's largest industry, according to the U.S. Census Bureau, generating 16.4% of gross domestic product (GDP) in 2009. Major financial industry employers include The Hartford, Travelers, Cigna, Aetna, Mass Mutual, People's United Financial, Royal Bank of Scotland, UBS Bridgewater Associates, and GE Capital. Separately, the real estate industry accounted for an additional 15% of economic activity in 2009, with major employers including Realogy and William Raveis Real Estate.\n\nManufacturing is the third biggest industry at 11.9% of GDP and dominated by Hartford-based United Technologies Corporation (UTC), which employs more than 22,000 people in Connecticut. Lockheed Martin subsidiary Sikorsky Aircraft operates Connecticut's single largest manufacturing plant in Stratford, where it makes helicopters. Other UTC divisions include UTC Propulsion and Aerospace Systems, including jet engine manufacturer Pratt & Whitney and UTC Building and Industrial Systems.\n\nOther major manufacturers include the Electric Boat division of General Dynamics, which makes submarines in Groton, and Boehringer Ingelheim, a pharmaceuticals manufacturer with its U.S. headquarters in Ridgefield.\n\nConnecticut historically was a center of gun manufacturing, and four gun-manufacturing firms continued to operate in the state as of December 2012, employing 2,000 people: Colt, Stag, Ruger, and Mossberg. Marlin, owned by Remington, closed in April 2011.\n\nA report issued by the Connecticut Commission on Culture & Tourism on December 7, 2006 demonstrated that the areas of the arts, film, history, and tourism generated more than $14 billion in economic activity and 170,000 jobs annually. This provides $9 billion in personal income for Connecticut residents and $1.7 billion in state and local revenue. The Foxwoods Resort Casino and Mohegan Sun casino number among the state's largest employers; both are located on Indian reservations in the eastern part of Connecticut.\nConnecticut's agricultural sector employed about 12,000 people as of 2010, with more than a quarter of that number involved in nursery stock production. Other agricultural products include dairy products and eggs, tobacco, fish and shellfish, and fruit.\n\nOyster harvesting was historically an important source of income to towns along the Connecticut coastline. In the 19th century, oystering boomed in New Haven, Bridgeport, and Norwalk and achieved modest success in neighboring towns. In 1911, Connecticut's oyster production reached its peak at nearly 25 million pounds of oyster meats. This was, at the time, higher than production in New York, Rhode Island, or Massachusetts. During this time, the Connecticut coast was known in the shellfishing industry as the oyster capital of the world. From before World War 1 until 1969, Connecticut laws restricted the right to harvest oysters in state-owned beds to sailing vessels. These laws prompted the construction of the oyster sloop style vessel that lasted well into the 20th century. The sloop is believed to be the last oyster sloop built in Connecticut, completed in Greenwich in 1948.\n\nThe Interstate highways in the state are Interstate 95 (I-95; the Connecticut Turnpike) traveling southwest to northeast along the coast, I-84 traveling southwest to northeast in the center of the state, I-91 traveling north to south in the center of the state, and I-395 traveling north to south near the eastern border of the state. The other major highways in Connecticut are the Merritt Parkway and Wilbur Cross Parkway, which together form Connecticut Route 15 (Route 15), traveling from the Hutchinson River Parkway in New York parallel to I-95 before turning north of New Haven and traveling parallel to I-91, finally becoming a surface road in Berlin. I-95 and Route 15 were originally toll roads; they relied on a system of toll plazas at which all traffic stopped and paid fixed tolls. A series of terrible crashes at these plazas eventually contributed to the decision to remove the tolls in 1988. Other major arteries in the state include U.S. Route 7 (US 7) in the west traveling parallel to the New York state line, Route 8 farther east near the industrial city of Waterbury and traveling north–south along the Naugatuck River Valley nearly parallel with US 7, and Connecticut Route 9 in the east.\n\nBetween New Haven and New York City, I-95 is one of the most congested highways in the United States. Although I-95 has been widened in several spots, some areas are only 3 lanes and this strains traffic capacity, resulting in frequent and lengthy rush hour delays. Frequently, the congestion spills over to clog the parallel Merritt Parkway and even US 1. The state has encouraged traffic reduction schemes, including rail use and ride-sharing.\n\nConnecticut also has a very active bicycling community, with one of the highest rates of bicycle ownership and use in the United States. New Haven's cycling community, organized in a local advocacy group called ElmCityCycling, is particularly active. According to the US Census 2006 American Community Survey, New Haven has the highest percentage of commuters who bicycle to work of any major metropolitan center on the East Coast.\n\nRail is a popular travel mode between New Haven and New York City's Grand Central Terminal. Southwestern Connecticut is served by the Metro-North Railroad's New Haven Line, operated by the Metropolitan Transportation Authority and providing commuter service to New York City and New Haven, with branches servicing New Canaan, Danbury, and Waterbury. Connecticut lies along Amtrak's Northeast Corridor which features frequent Northeast Regional and Acela Express service from New Haven south to New York City, Philadelphia, Baltimore, Washington, DC, and Norfolk, VA. \n\nCoastal cities and towns between New Haven and New London are also served by the Shore Line East commuter line. Several new stations were completed along the Connecticut shoreline recently, and a commuter rail service called the Hartford Line between New Haven and Springfield on Amtrak's New Haven-Springfield Line is scheduled to begin operating in 2018. A proposed commuter rail service, the Central Corridor Rail Line, will connect New London with Norwich, Willimantic, Storrs, and Stafford Springs, with service continuing into Massachusetts and Brattleboro. Amtrak also operates a shuttle service between New Haven and Springfield, Massachusetts, serving Wallingford, Meriden, Berlin, Hartford, Windsor Locks, and Springfield, MA and the Vermonter runs from Washington to St. Albans, Vermont via the same line.\n\nStatewide bus service is supplied by Connecticut Transit, owned by the Connecticut Department of Transportation, with smaller municipal authorities providing local service. Bus networks are an important part of the transportation system in Connecticut, especially in urban areas like Hartford, Stamford, Norwalk, Bridgeport and New Haven. Connecticut Transit also operates CTfastrak, a bus rapid transit service between New Britain and Hartford. The bus route opened to the public on March 28, 2015.\nBradley International Airport, is located in Windsor Locks, north of Hartford. Many residents of central and southern Connecticut also make heavy use of JFK International Airport and Newark International Airports, especially for international travel. Smaller regional air service is provided at Tweed New Haven Regional Airport. Larger civil airports include Danbury Municipal Airport and Waterbury-Oxford Airport in western Connecticut, Hartford–Brainard Airport in central Connecticut, and Groton-New London Airport in eastern Connecticut. Sikorsky Memorial Airport is located in Stratford and mostly services cargo, helicopter and private aviation.\n\nThe Bridgeport & Port Jefferson Ferry travels between Bridgeport, Connecticut and Port Jefferson, New York by crossing Long Island Sound. Ferry service also operates out of New London to Orient, New York; Fishers Island, New York; and Block Island, Rhode Island, which are popular tourist destinations. Small local services operate the Rocky Hill – Glastonbury Ferry and the Chester–Hadlyme Ferry which cross the Connecticut River.\n\nHartford has been the sole capital of Connecticut since 1875. Before then, New Haven and Hartford alternated as capitals.\n\nConnecticut is known as the \"Constitution State\". The origin of this nickname is uncertain, but it likely comes from Connecticut's pivotal role in the federal constitutional convention of 1787, during which Roger Sherman and Oliver Ellsworth helped to orchestrate what became known as the Connecticut Compromise, or the Great Compromise. This plan combined the Virginia Plan and the New Jersey Plan to form a bicameral legislature, a form copied by almost every state constitution since the adoption of the federal constitution. Variations of the bicameral legislature had been proposed by Virginia and New Jersey, but Connecticut's plan was the one that was in effect until the early 20th century, when Senators ceased to be selected by their state legislatures and were instead directly elected. Otherwise, it is still the design of Congress.\n\nThe nickname also might refer to the Fundamental Orders of 1638–39. These Fundamental Orders represent the framework for the first formal Connecticut state government written by a representative body in Connecticut. The State of Connecticut government has operated under the direction of four separate documents in the course of the state's constitutional history. After the Fundamental Orders, Connecticut was granted governmental authority by King Charles II of England through the Connecticut Charter of 1662.\n\nSeparate branches of government did not exist during this period, and the General Assembly acted as the supreme authority. A constitution similar to the modern U.S. Constitution was not adopted in Connecticut until 1818. Finally, the current state constitution was implemented in 1965. The 1965 constitution absorbed a majority of its 1818 predecessor, but incorporated a handful of important modifications.\n\nThe governor heads the executive branch. , Dannel Malloy is the Governor and Nancy Wyman is the Lieutenant Governor, both are Democrats. Malloy, the former mayor of Stamford, won the 2010 general election for Governor, and was sworn in on January 5, 2011. From 1639 until the adoption of the 1818 constitution, the governor presided over the General Assembly. In 1974, Ella Grasso was elected as the governor of Connecticut. This was the first time in United States history when a woman was a governor without her husband being governor first.\n\nThere are several executive departments: Administrative Services, Agriculture, Banking, Children and Families, Consumer Protection, Correction, Economic and Community Development, Developmental Services, Construction Services, Education, Emergency Management and Public Protection, Energy & Environmental Protection, Higher Education, Insurance, Labor, Mental Health and Addiction Services, Military, Motor Vehicles, Public Health, Public Utility Regulatory Authority, Public Works, Revenue Services, Social Services, Transportation, and Veterans Affairs. In addition to these departments, there are other independent bureaus, offices and commissions.\n\nIn addition to the Governor and Lieutenant Governor, there are four other executive officers named in the state constitution that are elected directly by voters: Secretary of the State, Treasurer, Comptroller, and Attorney General. All executive officers are elected to four-year terms.\n\nThe legislature is the General Assembly. The General Assembly is a bicameral body consisting of an upper body, the State Senate (36 senators); and a lower body, the House of Representatives (151 representatives). Bills must pass each house in order to become law. The governor can veto the bill, but this veto can be overridden by a two-thirds majority in each house. Per Article XV of the state constitution, Senators and Representatives must be at least 18 years of age and are elected to two-year terms in November on even-numbered years. There also must always be between 30 and 50 senators and 125 to 225 representatives. The Lieutenant Governor presides over the Senate, except when absent from the chamber, when the President pro tempore presides. The Speaker of the House presides over the House. , Brendan Sharkey is the Speaker of the House of Connecticut.\n\n, Connecticut's United States Senators are Richard Blumenthal (Democrat) and Chris Murphy (Democrat). Connecticut has five representatives in the U.S. House, all of whom are Democrats.\n\nLocally elected representatives also develop Local ordinances to govern cities and towns. The town ordinances often include noise control and zoning guidelines. However, the State of Connecticut does also provide statewide ordinances for noise control as well.\n\nThe highest court of Connecticut's judicial branch is the Connecticut Supreme Court, headed by the Chief Justice of Connecticut. The Supreme Court is responsible for deciding on the constitutionality of the law or cases as they relate to the law. Its proceedings are similar to those of the United States Supreme Court, with no testimony given by witnesses, and the lawyers of the two sides each present oral arguments no longer than thirty minutes. Following a court proceeding, the court may take several months to arrive at a judgment. the Chief Justice is Chase T. Rogers.\n\nIn 1818, the court became a separate entity, independent of the legislative and executive branches. The Appellate Court is a lesser statewide court and the Superior Courts are lower courts that resemble county courts of other states.\n\nThe State of Connecticut also offers access to Arrest warrant enforcement statistics through the Office of Policy and Management.\n\nConnecticut does not have county government, unlike all other states except Rhode Island. Connecticut county governments were mostly eliminated in 1960, with the exception of sheriffs elected in each county. In 2000, the county sheriff was abolished and replaced with the state marshal system, which has districts that follow the old county territories. The judicial system is divided into judicial districts at the trial-court level which largely follow the old county lines. The eight counties are still widely used for purely geographical and statistical purposes, such as weather reports and census reporting.\n\nConnecticut shares with the rest of New England a governmental institution called the New England town. The state is divided into 169 towns which serve as the fundamental political jurisdictions. There are also 21 cities, most of which simply follow the boundaries of their namesake towns and have a merged city-town government. There are two exceptions: the City of Groton, which is a subsection of the Town of Groton, and the City of Winsted in the Town of Winchester. There are also nine incorporated boroughs which may provide additional services to a section of town. Naugatuck is a consolidated town and borough.\n\nThe state is also divided into 15 planning regions defined by the state Office of Planning and Management, with the exception of the Town of Stafford in Tolland County. The Intragovernmental Policy Division of this Office coordinates regional planning with the administrative bodies of these regions. Each region has an administrative body known as a regional council of governments, a regional council of elected officials, or a regional planning agency. The regions are established for the purpose of planning \"coordination of regional and state planning activities; redesignation of logical planning regions and promotion of the continuation of regional planning organizations within the state; and provision for technical aid and the administration of financial assistance to regional planning organizations\".\n\nConnecticut residents who register to vote have the option of declaring an affiliation to a political party, may become unaffiliated at will, and may change affiliations subject to certain waiting periods. about 60% of registered voters are enrolled (just over 1% total in 28 third parties minor parties), and ratios among unaffiliated voters and the two major parties are about 8 unaffiliated for every 7 in the Democratic Party of Connecticut and for every 4 in the Connecticut Republican Party.\n\nMany Connecticut towns and cities show a marked preference for moderate candidates of either party.\n\nElections in Connecticut take place mostly at the levels of town and/or city, state legislative districts for both houses, Congressional districts, and statewide. In almost all races, the two major parties have some practical advantages granted on the basis of their respective performances in the most recent election covering the same constituency. Several processes, to varying degrees internal to either a major or minor party, are in practice nearly prerequisites to being permitted mention on the provided ballots, and even more so to winning office.\n\nMore specifically, the status of \"major party\" is usually reconfirmed every four years, as belonging to the two parties that polled best, statewide, in the gubernatorial column; this status includes the benefit of appearing in one of the top two rows on the ballot provided the party has at least one candidate on the ballot. Minor parties appear below major parties, and their performance in recent elections determines whether a candidates who wins in their nomination process must also meet a petitioning threshold in order to appear.\n\nIn a major party, a party convention for the office's constituency must be held; in practice, at the town level, a major party convention of voters of the town who are enrolled in the party usually is attended almost exclusively by members of the town party committee. The convention may choose to endorse a candidate, who will appear on the ballot unless additional candidates meet a petition threshold for a primary election; if at least one candidate meets the petition threshold, the endorsed candidate and all who meet the threshold appear on the primary ballot, and the winner of the primary election appears on the party line for that office.\n\nA candidate wishing to run on the ballot line of a minor-party which has recently enough met a general-election vote threshold follows similar steps; candidates of other minor parties must meet petition thresholds, and if other candidates of the same party, for the same office, do so as well, only the winner of a resulting primary will appear on the ballot.\n\nCampaigns by candidates not on the ballot generally are entirely symbolic, and while any voter can cast a write-in ballot, write-in ballots are not even tallied by election officials, except for candidates who have submitted a formal request that the tally be made.\n\nIn short, most winning candidates have won the endorsement of the applicable \"major\"-party convention; nearly all of the rest have won with a \"professionally managed\" primary-election campaign; and successful minor-party candidates are almost without exception major-party figures like Lowell Weicker whose minor parties disappear after that success. A Connecticut Party, which Weicker founded, became nominally the leading major party, and state law was changed during his administration to provide that in a situation such as his win, the top \"three\" parties in the governor's race all became major parties.\n\nChris Murphy and Richard Blumenthal are Connecticut's U.S. senators; both are Democrats.\n\nThe suburban towns of New Canaan and Darien in Fairfield County are considered the most Republican areas in the state. Westport, a wealthy town a few miles to the east, is often considered one of the most loyally Democratic, liberal towns in Fairfield County. The historically Republican-leaning wealthy town of Wilton voted in the majority for Barack Obama in the 2008 Presidential Election. Fairfield, the namesake of the county, has historically favored moderate Republicans in municipal, congressional, senatorial, and gubernatorial campaigns, but in recent years has supported Democratic Presidential nominees. Norwalk and Stamford, two larger, mixed-income communities in Fairfield County, have in many elections favored moderate Republicans including former Governor John G. Rowland and former Congressman Chris Shays, however they have favored Democrats in recent US presidential election years, with Shays being defeated by Democrat Jim Himes in the 2008 election.\n\nThe state's Republican-leaning areas are the rural Litchfield County and adjoining exurbs in the western side of Hartford County, the industrial towns of the Naugatuck River Valley, and some of the affluent Fairfield County towns near the New York border.\n\nJoe Lieberman's predecessor, Lowell P. Weicker, Jr., was the last Connecticut Republican to serve as Senator. Weicker was known as a liberal Republican. He broke with President Richard Nixon during Watergate and successfully ran for governor in 1990 as an independent, creating A Connecticut Party as his election vehicle. Before Weicker, the last Republican to represent Connecticut in the Senate was Prescott Bush, the father of former President George H.W. Bush and the grandfather of former President George W. Bush. He served 1953–63.\n\nWaterbury has a Democratic registration edge, but usually favors conservative candidates of both traditional parties. In Danbury unaffiliated voters outnumber voters registered with either major party. Other smaller cities including Meriden, New Britain, Norwich and Middletown favor Democratic candidates. The state's major cities—Hartford, New Haven, Bridgeport and Stamford—are all strongly Democratic.\n\n, Democrats controlled all five federal congressional seats. The last Republican to be elected, Chris Shays, lost his seat to Democrat Jim Himes in 2008.\n\nIn April 2012 both houses of the Connecticut state legislature passed a bill (20 to 16 and 86 to 62) that abolished the capital punishment for all future crimes, while 11 inmates who were waiting on the death row at the time could still be executed.\n\nIn July 2009 the Connecticut legislature overrode a veto by Governor M. Jodi Rell to pass SustiNet, the first significant public-option health care reform legislation in the nation.\n\nThe Connecticut State Board of Education manages the public school system for children in grades K–12. Board of Education members are appointed by the Governor of Connecticut. Statistics for each school are made available to the public through an online database system called \"CEDAR\". The CEDAR database also provides statistics for \"ACES\" or \"RESC\" schools for children with behavioral disorders.\n\nConnecticut was home to the nation's first law school, Litchfield Law School, which operated from 1773 to 1833 in Litchfield. Hartford Public High School (1638) is the third-oldest secondary school in the nation after the Collegiate School (1628) in Manhattan and the Boston Latin School (1635).\n\n\n\n\nThe state also has many noted private day schools, and its boarding schools draw students from around the world.\n\nThe Connecticut Sun of the WNBA currently play at the Mohegan Sun Arena in Uncasville.\n\nThe Hartford Yard Goats of the Eastern League are a AA affiliate of the Colorado Rockies. Also, the Connecticut Tigers play in the New York-Penn League and are a A affiliate of the Detroit Tigers. The Bridgeport Bluefish and the New Britain Bees play in the Atlantic League.\n\nCurrently, there are two Connecticut teams in the American Hockey League: the Bridgeport Sound Tigers, a farm team for the New York Islanders, compete at the Webster Bank Arena in Bridgeport; the Hartford Wolf Pack, the affiliate of the New York Rangers, play in the XL Center in Hartford.\n\nThe state hosts several major sporting events. Since 1952, a PGA Tour golf tournament has been played in the Hartford area. Originally called the \"Insurance City Open\" and later the \"Greater Hartford Open\", the event is now known as the Travelers Championship. The Connecticut Open tennis tournament is held annually in the Cullman-Heyman Tennis Center at Yale University in New Haven.\n\nLime Rock Park in Salisbury is a road racing course, home to IMSA, SCCA, USAC, and K&N Pro Series East races. Thompson International Speedway, Stafford Motor Speedway and Waterford Speedbowl are oval tracks holding weekly races for NASCAR Modifieds and other classes, including the NASCAR Whelen Modified Tour.\nThe state also hosts several major mixed martial arts events for Bellator MMA and the Ultimate Fighting Championship.\n\nConnecticut has been the home of multiple teams in the big four sports leagues, though currently hosts none.\nConnecticut's longest-tenured and only modern full-time \"big four\" franchise were the Hartford Whalers of the National Hockey League, who played in Hartford from 1975 to 1997 at the Hartford Civic Center. Their departure to Raleigh, North Carolina, over disputes with the state over the construction of a new arena, caused great controversy and resentment. The former Whalers are now known as the Carolina Hurricanes.\n\nIn 1926, Hartford had a franchise in the National Football League known as the Hartford Blues. The NFL would return to Connecticut from 1973 to 1974 when New Haven hosted the New York Giants at Yale Bowl while Giants Stadium was under construction.\n\nThe Hartford Dark Blues joined the National League for one season in 1876, making them the state's only Major League Baseball franchise, before moving to Brooklyn, New York and then disbanding one season later.\nFrom 1975 to 1995, the Boston Celtics of the National Basketball Association played a number of home games at the Hartford Civic Center.\n\nFrom 1996 to 1998, Connecticut was home to a professional woman's basketball team, American Basketball League franchise the New England Blizzard, who played at the XL Center.\n\nHartford has hosted two Arena Football League franchises, in the Connecticut Coyotes from 1995 to 1996 and the New England Sea Wolves from 1999 to 2000, both playing at the Civic Center. Hartford was home to the Hartford Colonials of the United Football League for one season in 2010.\n\nThe Connecticut Interscholastic Athletic Conference (CIAC) is the state's sanctioning body for high school sports.\n\nThe Connecticut Huskies, often called \"UConn\", play NCAA Division I sports and are popular in the state. Both the men's basketball and women's basketball teams have won multiple national championships, including in 2004, when UConn became the first school in NCAA Division I history to have its men's and women's basketball programs win the national title in the same year. In 2014, UConn repeated its feat of being the only school in NCAA Division I to win men's and women's basketball tournaments in the same year. The UConn women's basketball team holds the record for the longest consecutive winning streak in NCAA college basketball at 111 games, a streak that ended in 2017. The UConn Huskies football team has played in the Football Bowl Subdivision since 2002, and has played in four bowl games since.\n\nNew Haven biennially hosts \"The Game\" between the Yale Bulldogs and the Harvard Crimson, the country's second-oldest college football rivalry. Yale alum Walter Camp, deemed the \"Father of American Football\", helped develop modern football while living in New Haven.\n\nOther Connecticut universities which feature Division I sports teams are Quinnipiac University, Fairfield University, Central Connecticut State University, Sacred Heart University, and the University of Hartford.\n\nThe name \"Connecticut\" originated with the Mohegan word \"quonehtacut\", meaning \"place of long tidal river\". Connecticut's official nickname is \"The Constitution State\", adopted in 1959 and based on its colonial constitution of 1638–39 which was the first in America and, arguably, the world. Connecticut is also unofficially known as \"The Nutmeg State,\" whose origin is unknown. It may have come from its sailors returning from voyages with nutmeg, which was a very valuable spice in the 18th and 19th centuries. It may have originated in the early machined sheet tin nutmeg grinders sold by early Connecticut peddlers. It is also facetiously said to come from Yankee peddlers from Connecticut who would sell small carved nobs of wood shaped to look like nutmeg to unsuspecting customers. George Washington gave Connecticut the title of \"The Provisions State\" because of the material aid that the state rendered to the American Revolutionary War effort. Connecticut is also known as \"The Land of Steady Habits\".\n\nAccording to Webster's New International Dictionary (1993), a person who is a native or resident of Connecticut is a \"Connecticuter\". There are numerous other terms coined in print but not in use, such as \"Connecticotian\" (Cotton Mather in 1702) and \"Connecticutensian\" (Samuel Peters in 1781). Linguist Allen Walker Read suggests the more playful term \"connecticutie.\" \"Nutmegger\" is sometimes used, as is \"Yankee.\" The official state song is \"Yankee Doodle\"), though this usually refers to someone from the wider New England region. (In the Southern United States, the term \"Yankee\" refers to anyone who lives north of the Mason–Dixon line.) The traditional abbreviation of the state's name is \"Conn.;\" the official postal abbreviation is CT.\n\nCommemorative stamps issued by the United States Postal Service with Connecticut themes include Nathan Hale, Eugene O'Neill, Josiah Willard Gibbs, Noah Webster, Eli Whitney, the whaling ship the Charles W. Morgan which is docked at Mystic Seaport, and a decoy of a broadbill duck.\n\n\n\n"}
{"id": "6468", "url": "https://en.wikipedia.org/wiki?curid=6468", "title": "Country Liberal Party", "text": "Country Liberal Party\n\nThe Country Liberals (Northern Territory) (abbreviated as Country Liberals (NT) and formerly known as the Northern Territory Country Liberal Party) is a conservative political party in Australia founded in 1974, which operates in the Northern Territory.\n\nThe CLP first fielded candidates at the 1975 federal election, winning one seat in the Senate and the non-voting seat in the House of Representatives. Since 1979, the CLP has been formally affiliated with both the federal Liberal Party of Australia and the National Party of Australia (previously the Country Party and National Country Party). The Liberal Party, National Party, Liberal National Party of Queensland, and CLP form the Coalition of Australian centre-right parties, with the CLP alone contesting seats for the Coalition in the Northern Territory. The CLP has full voting rights within the National Party, and observer status with the Liberal Party. Currently, the CLP has one representative in federal parliament, Senator Nigel Scullion, who also serves as the Senate leader of the National Party.\n\nThe CLP dominated the Northern Territory Legislative Assembly from its establishment in 1974 until the 2001 general election, when the CLP lost government winning only 10 of the 25 seats, and was reduced further to four parliamentary members at the 2005 election. At the 2008 election it increased its numbers, winning 11 seats.\n\nThe CLP returned to office following the 2012 election, winning 16 of 25 seats, and leader Terry Mills became Chief Minister of the Northern Territory. Less than a year later, Mills was replaced as Chief Minister and CLP leader by Adam Giles at the 2013 CLP leadership ballot on 13 March. Giles was the first indigenous Australian to lead a state or territory government in Australia. Giles was defeated at the 2015 CLP leadership ballot but managed to survive in the aftermath. Multiple defections saw the CLP reduced to minority government a few months later. At the 27 August 2016 Territory election, the CLP was resoundingly defeated, winning just two of 25 seats. Gary Higgins became opposition leader and CLP leader while Lia Finocchiaro became deputy CLP leader on 2 September.\n\nThe CLP stands for office in the Northern Territory Assembly and Federal Parliament of Australia and primarily concerns itself with representing Territory interests. It is a regionally based party, that has parliamentary representation in both the Federal Parliament and at the Territory level.\n\nThe CLP competes against the Australian Labor Party (Australia's social-democratic party). It is closely affiliated with, but is independent from the Liberal Party of Australia (a mainly urban, pro-private enterprise party comprising conservative and liberal membership) and the National Party of Australia (a conservative agrarian and rural regional party).\n\nThe Party promotes local issues like statehood for the Northern Territory as well as more broadly liberal values like support for individualism and private enterprise, as well as traditional conservative values and progressive political policy. In indigenous policy, the party has committed to improving education and job creation and to reducing a culture of welfare dependency.\n\nBranch delegates and members of the party's Central Council attend the Annual Conference of the Country Liberal Party to decide the party's platform. The Central Council is composed of the party's office bearers, its leaders from the Territory Assembly and the Federal Parliament and representatives of party branches.\n\nThe Annual Conference of the Country Liberal Party, attended by branch delegates and members of the party's Central Council, decides matters relating to the party's platform and philosophy. The Central Council administers the party and makes decisions on pre-selections. It is composed of the party's office bearers, its leaders in the Northern Territory Legislative Assembly, members in the Federal Parliament, and representation from each of the party's branches.\n\nThe CLP president has full voting rights with the National Party and observer status with the Liberal Party. Both the Liberals and Nationals receive Country Liberal delegations at their conventions. After federal elections, the CLP directs its federal members and senators as to which of the two other parties they should sit with in the parliamentary chamber.\n\nThe Territory Country Party members first contested the 1919 federal election, with a newly established federal Country Party contesting the 1922 federal election. The 1922 election saw the main opposition party to the Australian Labor Party, the Nationalist Party of Australia deprived of a majority, and were required to form a coalition in order to command a majority on the floor of parliament. The price for such support was the resignation of Nationalist (ex-Labor) Prime Minister, Billy Hughes, who was replaced by Stanley Bruce.\n\nIn 1922, the federal Division of Northern Territory was created, with one non-voting Member in the House of Representatives. Harold George Nelson was the inaugural member serving between 16 December 1922 and 15 September 1934. He was elected as an Independent but later joined the Australian Labor Party (ALP). Between 15 September 1934 and 10 December 1949 the Division of Northern Territory was held by Adair Blain, an independent member. Between 10 December 1949 and 31 October 1966 the Division was held by Jock Nelson, a member of the ALP. The Territory seat was won by the Country Party's Sam Calder at the 1966 federal election, who held the seat from 26 November 1966 to 19 September 1980.\n\nIn 1966, the Country Party was established in the Northern Territory, while the Liberal Party was a small party. In recognition of this, the local Liberals supported the Country Party's Calder for the sole NT seat from 1969 to 1972. An alliance had formed, primarily against the conservatives' main opponent, the ALP. After the gradual extension of limited voting rights, in 1968 the federal Coalition government gave the Member for Northern Territory full voting rights.\n\nAfter the 1974 federal election and the subsequent Joint Sitting of parliament, legislation was passed to give the Australian Capital Territory and the Northern Territory representation in the Australian Senate, with two senators being elected.\n\nThe Whitlam Government passed legislation in 1974 to establish a fully elected unicameral Northern Territory Legislative Assembly to replace the previous partly elected Northern Territory Legislative Council, which had been in existence since 1947. The term of the Legislative Assembly was four years. Initially, the Legislative Assembly consisted of 19 members, which was increased in 1982 to 25 members, the present number. The Northern Territory was granted self-government in 1978.\n\nFollowing the creation of the Legislative Assembly in 1974, the Territory's branches of the Country and Liberal parties merged to form the \"Country Liberal Party\" (CLP) to field candidates at the 1974 general election for the Legislative Assembly, going on to win 17 out of 19 seats. Calder was largely responsible for the push to unite the non-Labor forces in the Territory.\n\nThe CLP fielded candidates at the 1975 federal election, winning one seat each in the Senate and in the House of Representatives. Since 1979, the CLP has been formally affiliated with both the federal National (previously the Country Party and National Country Party) and Liberal parties. The CLP contests seats for the Coalition in the Northern Territory rather than the Liberal or National parties. The CLP has full voting rights within the National Party, and observer status with the Liberal Party.\n\nThe CLP formed the Northern Territory government from 1974 until the 2001 election. For much of that time, it ruled the Northern Territory without facing more than nine opposition members. Indeed, the CLP's dominance was so absolute that its internal politics were seen as a bigger threat than any opposition party. This was especially pronounced in the mid-1980s, when a series of party-room coups resulted in the Territory having three Chief Ministers in four years.\n\nAt the 2001 election the Australian Labor Party won government by one seat, ending 27 years of CLP government. The loss marked a major turning point in Northern Territory politics, a result which was exacerbated when, at the 2005 election, the ALP won the second-largest majority government in the history of the Territory, reducing the once-dominant party to just four members in the Legislative Assembly. This result was only outdone by the 1974 election, in which the CLP faced only two independents as opposition. The CLP even lost two seats in Palmerston, an area where the ALP had never come close to winning any seats before.\n\nIn the 2001 federal election, the CLP won the newly formed seat of Solomon, based on Darwin/Palmerston, in the House of Representatives.\nIn the 2004 federal election, the CLP held one seat in the House of Representatives, and one seat in the Senate. The CLP lost its federal lower house in the 2007 federal election, but regained it when Palmerston deputy mayor Natasha Griggs won back Solomon for the CLP. She sat with the Liberals in the House.\n\nThe 2008 election saw the CLP recover from the severe loss it suffered three years earlier, increasing its representation from four to 11 members. Following the 2011 decision of ALP-turned-independent member Alison Anderson to join the CLP, this increased to CLP's representation to 12 in the Assembly, leaving the incumbent Henderson Government to govern in minority with the support of Independent MP Gerry Wood.\n\nHistorically, the CLP has been particularly dominant in the Territory's two major cities, Darwin/Palmerston and Alice Springs. However, in recent years the ALP has pulled even with the CLP in the Darwin area; indeed, its 2001 victory was fueled by an unexpected swing in Darwin.\n\nThe CLP under the leadership of Terry Mills returned to power in the 2012 election with 16 of 25 seats, defeating the incumbent Labor Government led by Paul Henderson. In the lead up to the Territory election, CLP Senator Nigel Scullion sharply criticised the Federal Labor Government for its suspension of the live cattle trade to Indonesia - an economic mainstay of the territory.\n\nThe election victory ended 11 years of ALP rule in the Northern Territory. The victory was also notable for the support it achieved from indigenous people in pastoral and remote electorates. Large swings were achieved in remote Territory electorates (where the indigenous population comprised around two-thirds of voters) and a total of five Aboriginal CLP candidates won election to the Assembly. Among the indigenous candidates elected were high-profile Aboriginal activist Bess Price and former ALP member Alison Anderson. Anderson was appointed Minister for Indigenous Advancement. In a nationally reported speech in November 2012, Anderson condemned welfare dependency and a culture of entitlement in her first ministerial statement on the status of Aboriginal communities in the Territory and said the CLP would focus on improving education and on helping create real jobs for indigenous people.\n\nAdam Giles replaced Mills as Chief Minister of the Northern Territory and party leader at the 2013 CLP leadership ballot on 13 March while Mills was on a trade mission in Japan. Giles was sworn in as Chief Minister on 14 March, becoming the first indigenous head of government of an Australian state or territory.\n\nWhen the CLP introduced mandatory alcohol rehabilitation for recidivist problem drinkers to replace a banned drinker register, Giles dismissed critics of the policy as \"lefty welfare-orientated people\".\n\nWillem Westra van Holthe challenged Giles at the 2015 CLP leadership ballot on 2 February and was elected leader by the party room in a late night vote conducted by phone. However, Giles refused to resign as Chief Minister following the vote. On 3 February, \"ABC News\" reported that officials were preparing an instrument for Giles' removal by the Administrator. The swearing-in of Westra van Holthe, which had been scheduled for 11:00 local time (01:30 UTC), was delayed. After a meeting of the parliamentary wing of the CLP, Giles announced that he would remain as party leader and Chief Minister, and that Westra van Holthe would be his deputy.\n\nJust one opinion poll has been released since the 2012 election – conducted by ReachTEL and commissioned by The Australian which surveyed 1036 residents via robocall on the afternoon of Sunday 1 March 2015 across all 18 electorates in Darwin, Palmerston and Alice Springs – which indicated a landslide 17.6% two-party swing against the incumbent CLP government since the last election.\n\nAfter four defections during the parliamentary term, the CLP was reduced to minority government by July 2015. Giles raised the possibility of an early election on 20 July stating that he would \"love\" to call a snap poll, but that it was \"pretty much impossible to do\". Crossbenchers dismissed the notion of voting against a confidence motion to bring down the government.\n\nTerritory government legislation passed in February 2016 changed the voting method of single-member electorates from full-preferential voting to optional preferential voting ahead of the 2016 territory election held on 27 August.\n\nFederally, a MediaReach seat-level opinion poll of 513 voters in the seat of Solomon conducted 22−23 June ahead of the 2016 federal election held on 2 July surprisingly found Labor candidate Luke Gosling heavily leading two-term CLP incumbent Natasha Griggs 61–39 on the two-party vote from a large 12.4 percent swing. The CLP lost Solomon to Labor at the election, with Gosling defeating Griggs 56–44 on the two-party vote from a 7.4 percent swing.\n\nAt the 27 August Territory election, the CLP was swept from power in a massive Labor landslide, suffering easily the worst defeat of a sitting government in Territory history. The party not only lost all of the bush seats it picked up in 2012, but was all but shut out of Darwin/Palmerston, winning only one seat there. All told, the CLP only won two seats, easily its worst showing in an election. Giles himself lost his own seat, becoming the second Majority Leader/Chief Minister to lose his own seat. Even before Giles' defeat was confirmed, second-term MP Gary Higgins—the only surviving member of the Giles cabinet—was named the party's new leader, with Lia Finocchiaro as his deputy.\n\nBy the spring of 2016, the party Website had turned into a redirect to its Facebook page.\n\n\n"}
{"id": "6469", "url": "https://en.wikipedia.org/wiki?curid=6469", "title": "Canon law", "text": "Canon law\n\nCanon law is the body of laws and regulations made by ecclesiastical authority (Church leadership), for the government of a Christian organization or church and its members. It is the internal ecclesiastical law, or operational policy, governing the Catholic Church (both the Latin Church and the Eastern Catholic Churches), the Eastern Orthodox and Oriental Orthodox churches, and the individual national churches within the Anglican Communion. The way that such church law is legislated, interpreted and at times adjudicated varies widely among these three bodies of churches. In all three traditions, a canon was originally a rule adopted by a church council; these canons formed the foundation of canon law.\n\nGreek \"kanon\" / , Arabic Qanun / قانون, Hebrew kaneh / קנה, \"straight\"; a rule, code, standard, or measure; the root meaning in all these languages is \"reed\" (\"cf.\" the Romance-language ancestors of the English word \"cane\").\n\nThe \"Apostolic Canons\" or \"Ecclesiastical Canons of the Same Holy Apostles\" is a collection of ancient ecclesiastical decrees (eighty-five in the Eastern, fifty in the Western Church) concerning the government and discipline of the Early Christian Church, incorporated with the Apostolic Constitutions which are part of the Ante-Nicene Fathers.\nIn the Fourth century the First Council of Nicaea (325) calls canons the disciplinary measures of the Church: the term canon, κανὠν, means in Greek, a rule. There is a very early distinction between the rules enacted by the Church and the legislative measures taken by the State called \"leges\", Latin for laws.\n\nIn the Catholic Church, canon law is the system of laws and legal principles made and enforced by the Church's hierarchical authorities to regulate its external organization and government and to order and direct the activities of Catholics toward the mission of the Church.\n\nIn the Latin Church, positive ecclesiastical laws, based directly or indirectly upon immutable divine law or natural law, derive formal authority in the case of universal laws from the supreme legislator (i.e. the Supreme Pontiff), who possesses the totality of legislative, executive, and judicial power in his person, while particular laws derive formal authority from a legislator inferior to the supreme legislator. The actual subject material of the canons is not just doctrinal or moral in nature, but all-encompassing of the human condition.\n\nThe Catholic Church also includes the main five rites (groups) of churches which are in full union with the Holy See and the Latin Church:\nAll of these church groups are in full communion with the Supreme Pontiff and are subject to the \"Code of Canons of the Eastern Churches\".\n\nThe Catholic Church has what is claimed to be the oldest continuously functioning internal legal system in Western Europe, much later than Roman law but predating the evolution of modern European civil law traditions. What began with rules (\"canons\") adopted by the Apostles at the Council of Jerusalem in the first century has developed into a highly complex legal system encapsulating not just norms of the New Testament, but some elements of the Hebrew (Old Testament), Roman, Visigothic, Saxon, and Celtic legal traditions.\n\nThe history of Latin canon law can be divided into four periods: the \"jus antiquum\", the \"jus novum\", the \"jus novissimum\" and the \"Code of Canon Law\". In relation to the Code, history can be divided into the \"jus vetus\" (all law before the Code) and the \"jus novum\" (the law of the Code, or \"jus codicis\").\n\nThe canon law of the Eastern Catholic Churches, which had developed some different disciplines and practices, underwent its own process of codification, resulting in the Code of Canons of the Eastern Churches promulgated in 1990 by Pope John Paul II.\n\nCatholic canon law is a fully developed legal system, with all the necessary elements: courts, lawyers, judges, a fully articulated legal code principles of legal interpretation, and coercive penalties, though it lacks civilly-binding force in most secular jurisdictions. The academic degrees in canon law are the J.C.B. (\"Juris Canonici Baccalaureatus\", Bachelor of Canon Law, normally taken as a graduate degree), J.C.L. (\"Juris Canonici Licentiatus\", Licentiate of Canon Law) and the J.C.D. (\"Juris Canonici Doctor\", Doctor of Canon Law). Because of its specialized nature, advanced degrees in civil law or theology are normal prerequisites for the study of canon law.\n\nMuch of the legislative style was adapted from the Roman Law Code of Justinian. As a result, Roman ecclesiastical courts tend to follow the Roman Law style of continental Europe with some variation, featuring collegiate panels of judges and an investigative form of proceeding, called \"inquisitorial\", from the Latin \"inquirere\", to enquire. This is in contrast to the adversarial form of proceeding found in the common law system of English and U.S. law, which features such things as juries and single judges.\n\nThe institutions and practices of canon law paralleled the legal development of much of Europe, and consequently both modern civil law and common law (legal system) bear the influences of canon law. Edson Luiz Sampel, a Brazilian expert in canon law, says that canon law is contained in the genesis of various institutes of civil law, such as the law in continental Europe and Latin American countries. Sampel explains that canon law has significant influence in contemporary society.\n\nCanonical jurisprudential theory generally follows the principles of Aristotelian-Thomistic legal philosophy. While the term \"law\" is never explicitly defined in the Code, the Catechism of the Catholic Church cites Aquinas in defining law as \"...an ordinance of reason for the common good, promulgated by the one who is in charge of the community\" and reformulates it as \"...a rule of conduct enacted by competent authority for the sake of the common good.\"\n\nThe law of the Eastern Catholic Churches in full union with Rome was in much the same state as that of the Latin or Western Church before 1917; much more diversity in legislation existed in the various Eastern Catholic Churches. Each had its own special law, in which custom still played an important part. In 1929 Pius XI informed the Eastern Churches of his intention to work out a Code for the whole of the Eastern Church. The publication of these Codes for the Eastern Churches regarding the law of persons was made between 1949 through 1958 but finalized nearly 30 years later.\n\nThe first Code of Canon Law (1917) was almost exclusively for the Latin Church, with extremely limited application to the Eastern Churches. After the Second Vatican Council, (1962 - 1965), another edition was published specifically for the Roman Rite in 1983. Most recently, 1990, the Vatican produced the \"Code of Canons\" of the Eastern Churches which became the 1st code of \"Eastern Catholic Canon Law\".\n\nThe Greek-speaking Orthodox have collected canons and commentaries upon them in a work known as the \"Pēdálion\" (Greek: Πηδάλιον, \"Rudder\"), so named because it is meant to \"steer\" the Church. The Orthodox Christian tradition in general treats its canons more as guidelines than as laws, the bishops adjusting them to cultural and other local circumstances. Some Orthodox canon scholars point out that, had the Ecumenical Councils (which deliberated in Greek) meant for the canons to be used as laws, they would have called them \"nómoi/νόμοι\" (laws) rather than \"kanónes/κανόνες\" (rules), but almost all Orthodox conform to them. The dogmatic decisions of the Councils, though, are to be obeyed rather than to be treated as guidelines, since they are essential for the Church's unity.\n\nIn the Church of England, the ecclesiastical courts that formerly decided many matters such as disputes relating to marriage, divorce, wills, and defamation, still have jurisdiction of certain church-related matters (e.g. discipline of clergy, alteration of church property, and issues related to churchyards). Their separate status dates back to the 12th century when the Normans split them off from the mixed secular/religious county and local courts used by the Saxons. In contrast to the other courts of England the law used in ecclesiastical matters is at least partially a civil law system, not common law, although heavily governed by parliamentary statutes. Since the Reformation, ecclesiastical courts in England have been royal courts. The teaching of canon law at the Universities of Oxford and Cambridge was abrogated by Henry VIII; thereafter practitioners in the ecclesiastical courts were trained in civil law, receiving a Doctor of Civil Law (D.C.L.) degree from Oxford, or a Doctor of Laws (LL.D.) degree from Cambridge. Such lawyers (called \"doctors\" and \"civilians\") were centered at \"Doctors Commons\", a few streets south of St Paul's Cathedral in London, where they monopolized probate, matrimonial, and admiralty cases until their jurisdiction was removed to the common law courts in the mid-19th century.\n\nOther churches in the Anglican Communion around the world (e.g., the Episcopal Church in the United States, and the Anglican Church of Canada) still function under their own private systems of canon law.\n\nCurrently, (2004), there are principles of canon law common to the churches within the Anglican Communion; their existence can be factually established; each province or church contributes through its own legal system to the principles of canon law common within the Communion; these principles have a strong persuasive authority and are fundamental to the self-understanding of each of the churches of the Communion; these principles have a living force, and contain in themselves the possibility of further development; and the existence of these principles both demonstrates unity and promotes unity within the Anglican Communion.\n\nIn Presbyterian and Reformed churches, canon law is known as \"practice and procedure\" or \"church order\", and includes the church's laws respecting its government, discipline, legal practice and worship.\n\nRoman canon law had been criticized by the Presbyterians as early as 1572 in the Admonition to Parliament. The protest centered on the standard defense that canon law could be retained so long as it did not contradict the civil law. According to Polly Ha, the Reformed Church Government refuted this claiming that the bishops had been enforcing canon law for 1500 years.\n\nThe Book of Concord is the historic doctrinal statement of the Lutheran Church, consisting of ten credal documents recognized as authoritative in Lutheranism since the 16th century. However, the Book of Concord is a confessional document (stating orthodox belief) rather than a book of ecclesiastical rules or discipline, like canon law. Each Lutheran national church establishes its own system of church order and discipline, though these are referred to as \"canons.\"\n\nThe Book of Discipline contains the laws, rules, policies and guidelines for The United Methodist Church. Its last edition was published in 2016.\n\n\n\n\n\n"}
{"id": "6501", "url": "https://en.wikipedia.org/wiki?curid=6501", "title": "Columbanus", "text": "Columbanus\n\nColumbanus (, 543 – 21 November 615), also known as St. Columban, was an Irish missionary notable for founding a number of monasteries from around 590 in the Frankish and Lombard kingdoms, most notably Luxeuil Abbey in present-day France and Bobbio Abbey in present-day Italy. He is remembered as a key figure in the Hiberno-Scottish mission, or Irish missionary activity in early medieval Europe. In recent years, however, as Columbanus's deeds and legacy have come to be re-examined by historians, the traditional narrative of his career has been challenged and doubts have been raised regarding his actual involvement in missionary work and the extent to which he was driven by purely religious motives or also by a concern for playing an active part in politics and church politics in Francia.\n\nColumbanus taught an Irish monastic rule and penitential practices for those repenting of sins, which emphasised private confession to a priest, followed by penances levied by the priest in reparation for the sins. Columbanus is one of the earliest identifiable Hiberno-Latin writers.\n\nMost of what we know about Columbanus is based on Columbanus' own works (as far as they have been preserved) and Jonas of Bobbio's \"Vita Columbani\" (\"Life of Columbanus\"), which was written between 639 and 641. Jonas entered Bobbio after Columbanus' death but relied on reports of monks who still knew Columbanus. A description of miracles of Columbanus written by an anonymous monk of Bobbio is of much later date. In the second volume of his \"Acta Sanctorum O.S.B.\", Mabillon gives the life in full, together with an appendix on the miracles of the saint, written by an anonymous member of the Bobbio community.\n\nColumbanus (the Latinised form of \"Columbán\", meaning \"the white dove\") was born in the Kingdom of Meath, now part of Leinster, in Ireland in 543, the year Saint Benedict died at Monte Cassino. Prior to his birth, his mother was said to have had visions of bearing a child who, in the judgement of those interpreting the visions, would become a \"remarkable genius\". Columbanus was well-educated in the areas of grammar, rhetoric, geometry, and the Holy Scriptures.\n\nColumbanus left home to study under Sinell, Abbot of Cluaninis in Lough Erne. Under Sinell's instruction, Columbanus composed a commentary on the Psalms. He then moved to Bangor Abbey on the coast of Down, where Saint Comgall was serving as the abbot. He stayed at Bangor until his fortieth year, when he received Comgall's permission to travel to the continent.\n\nColumbanus gathered twelve companions for his journey—Saint Attala, Columbanus the Younger, Cummain, Domgal (Deicolus), Eogain, Eunan, Saint Gall, Gurgano, Libran, Lua, Sigisbert, and Waldoleno—and together they set sail for the continent. After a brief stop in Britain, most likely on the Scottish coast, they crossed the channel and landed in Brittany in 585. At Saint-Malo in Brittany, there is a granite cross bearing the saint's name to which people once came to pray for rain in times of drought. The nearby village of Saint-Coulomb commemorates him in name.\n\nColumbanus and his companions were received with favour by King Gontram of Burgundy, and soon they made their way to Annegray, where they founded a monastery in an abandoned Roman fortress. Despite its remote location in the Vosges Mountains, the community became a popular pilgrimage site that attracted so many monastic vocations that two new monasteries had to be formed to accommodate them. In 590, Columbanus obtained from King Gontram the Gallo-Roman castle called \"Luxovium\" in present-day Luxeuil-les-Bains, some eight miles from Annegray. The castle, soon transformed into a monastery, was located in a wild region, thickly covered with pine forests and brushwood. Columbanus erected a third monastery called \"Ad-fontanas\" at present-day Fontaine-lès-Luxeuil, named for its numerous springs. These monastic communities remained under Columbanus' authority, and their rules of life reflected the Irish tradition in which he had been formed. As these communities expanded and drew more pilgrims, Columbanus sought greater solitude, spending periods of time in a hermitage and communicating with the monks through an intermediary. Often he would withdraw to a cave seven miles away, with a single companion who acted as messenger between himself and his companions.\n\nDuring his twenty years in Gaul (in present-day France), Columbanus became involved in a dispute with the Frankish bishops who may have feared his growing influence. During the first half of the sixth century, the councils of Gaul had given to bishops absolute authority over religious communities. As heirs to the Irish monastic tradition, Columbanus and his monks used the Irish Easter calculation, a version of Bishop Augustalis's 84-year \"computus\" for determining the date of Easter (Quartodecimanism), whereas the Franks had adopted the Victorian cycle of 532 years. The bishops objected to the newcomers' continued observance of their own dating, which—among other issues—caused the end of Lent to differ. They also complained about the distinct Irish tonsure. In 602, the bishops assembled to judge Columbanus, but he did not appear before them as requested. Instead, he sent a letter to the prelates—a strange mixture of freedom, reverence, and charity—admonishing them to hold synods more frequently, and advising them to pay more attention to matters of equal importance to that of the date of Easter. In defence of his following his traditional paschal cycle, he wrote:\n\nWhen the bishops refused to abandon the matter, Columbanus, following Saint Patrick's canon, appealed directly to Pope Gregory I. In the third and only surviving letter, he asks \"the holy Pope, his Father\" to provide \"the strong support of his authority\" and to render a \"verdict of his favour\", apologising for \"presuming to argue as it were, with him who sits in the chair of Peter, Apostle and Bearer of the Keys\". None of the letters were answered, most likely due to the pope's death in 604. Columbanus then sent a letter to Gregory's successor, Pope Boniface IV, asking him to confirm the tradition of his elders—if it is not contrary to the Faith—so that he and his monks can follow the rites of their ancestors. Before Boniface responded, Columbanus moved outside the jurisdiction of the Frankish bishops. Since the Easter issue appears to end around that time, Columbanus may have stopped celebrating Irish date of Easter after moving to Italy.\n\nColumbanus was also involved in a dispute with members of the Frankish royal family. Upon the death of King Gontram of Burgundy, the succession passed to his nephew, Childebert II, the son of his brother Sigebert and Sigebert's wife Brunhilda of Austrasia. When Childebert II died, he left two sons, Theuderic II who inherited the Kingdom of Burgundy, and Theudebert II who inherited the Kingdom of Austrasia. Since both were minors, Brunhilda, their grandmother, declared herself their guardian and controlled the governments of the two kingdoms.\n\nTheuderic II venerated Columbanus and often visited him, but the saint admonished and rebuked him for his behaviour. When Theuderic began living with a mistress, the saint objected, earning the displeasure of Brunhilda, who thought a royal marriage would threaten her own power. The saint did not spare the demoralised court, and Brunhilda became his bitterest foe. Angered by the saint's moral stand, Brunhilda stirred up the bishops and nobles to find fault with his monastic rules. When Theuderic II finally confronted Columbanus at Luxeuil, ordering him to conform to the country's conventions, the saint refused and was then taken prisoner to Besançon. Columbanus managed to escape his captors and returned to his monastery at Luxeuil. When the king and his grandmother found out, they sent soldiers to drive him back to Ireland by force, separating him from his monks by insisting that only those from Ireland could accompany him into exile.\n\nColumbanus was taken to Nevers, then travelled by boat down the Loire river to the coast. At Tours he visited the tomb of Saint Martin, and sent a message to Theuderic II indicating that within three years he and his children would perish. When he arrived at Nantes, he wrote a letter before embarkation to his fellow monks at Luxeuil monastery. Filled with love and affection, the letter urges his brethren to obey Attala, who stayed behind as abbot of the monastic community. The letter concludes:\n\nSoon after the ship set sail from Nantes, a severe storm drove the vessel back ashore. Convinced that his holy passenger caused the tempest, the captain refused further attempts to transport the monk. Columbanus made his way across Gaul to visit King Chlothar II of Neustria at Soissons where he was gladly received. Despite the king's offers to stay in his kingdom, Columbanus left Neustria in 611 for the court of King Theudebert II of Austrasia in the northeastern part of the Kingdom of the Merovingian Franks.\n\nColumbanus travelled to Metz, where he received an honourable welcome, and then proceeding to Mainz, where he sailed upwards the Rhine river to the lands of the Suebi and Alemanni in the northern Alps, intending to preach the Gospel to these people. He followed the Rhine river and its tributaries, the Aar and the Limmat, and then on to Lake Zurich. Columbanus chose the village of Tuggen as his initial community, but the work was not successful. He continued north-east by way of Arbon to Bregenz on Lake Constance, where there were still some traces of Christianity. Here the saint found an oratory dedicated to Saint Aurelia containing three brass images of their tutelary deities. Columbanus commanded Gallus, who knew the local language, to preach to the inhabitants, and many were converted. The three brass images were destroyed, and Columbanus blessed the little church, placing the relics of Saint Aurelia beneath the altar. A monastery was erected, Mehrerau Abbey, and the brethren observed their regular life. Columbanus stayed in Bregenz for about one year. Following an uprising against the community, possibly related to that region being taken over by the saint's old enemy King Theudebert II, Columbanus resolved to cross the Alps into Italy. Gallus remained in this area and died there 646. About seventy years later at the place of Gallus' cell the Monastery of Saint Gall was founded, which in itself was the origin of the city of St. Gallen again about another three hundred years later.\n\nColumbanus arrived in Milan in 612 and was warmly greeted by King Agilulf and Queen Theodelinda of the Lombards. He immediately began refuting the teachings of Arianism, which had enjoyed a degree of acceptance in Italy. He wrote a treatise against Arianism, which has since been lost. Queen Theodelinda, the devout daughter of Duke Garibald I of Bavaria, played an important role in restoring Nicene Christianity to a position of primacy against Arianism, and was largely responsible for the king's conversion to Christianity.\n\nAt the king's request, Columbanus wrote a letter to Pope Boniface IV on the controversy over the \"Three Chapters\"—writings by Syrian bishops suspected of Nestorianism, which had been condemned in the fifth century as heresy. Pope Gregory I had tolerated in Lombardy those persons who defended the \"Three Letters\", among them King Agilulf. Columbanus agreed to take up the issue on behalf of the king. The letter begins with an apology that a \"foolish Scot (\"Scottus\", Irishman)\" would be writing for a Lombard king. After acquainting the pope with the imputations brought against him, he entreats the pontiff to prove his orthodoxy and assemble a council. He writes that his freedom of speech is consistent with the custom of his country. Some of the language used in the letter might now be regarded as disrespectful, but in that time, faith and austerity could be more indulgent. At the same time, the letter expresses the most affectionate and impassioned devotion to the Holy See.\n\nIf Columbanus' zeal for orthodoxy caused him to overstep the limits of discretion, his real attitude towards Rome is sufficiently clear, calling the pope \"his Lord and Father in Christ\", the \"Chosen Watchman\", and the \"First Pastor, set higher than all mortals\".\n\nKing Agilulf gave Columbanus a tract of land called Bobbio between Milan and Genoa near the Trebbia river, situated in a defile of the Apennine Mountains, to be used as a base for the conversion of the Lombard people. The area contained a ruined church and wastelands known as \"Ebovium\", which had formed part of the lands of the papacy prior to the Lombard invasion. Columbanus wanted this secluded place, for while enthusiastic in the instruction of the Lombards he preferred solitude for his monks and himself. Next to the little church, which was dedicated to Saint Peter, Columbanus erected a monastery in 614. Bobbio Abbey at its foundation followed the Rule of Saint Columbanus, based on the monastic practices of Celtic Christianity. For centuries it remained the stronghold of orthodoxy in northern Italy. \n\nDuring the last year of his life, Columbanus received messenges from King Chlothar II, inviting the saint to return to Burgundy, now that his enemies were dead. Columbanus did not return, but requested that the king always protect his monks at Luxeuil Abbey. He prepared for death by retiring to his cave on the mountainside overlooking the Trebbia river, where, according to a tradition, he had dedicated an oratory to Our Lady. Columbanus died at Bobbio on 21 November 615.\n\nThe Rule of Saint Columbanus embodied the customs of Bangor Abbey and other Irish monasteries. Much shorter than the Rule of Saint Benedict, the Rule of Saint Columbanus consists of ten chapters, on the subjects of obedience, silence, food, poverty, humility, chastity, choir offices, discretion, mortification, and perfection.\n\nIn the first chapter, Columbanus introduces the great principle of his Rule: obedience, absolute and unreserved. The words of seniors should always be obeyed, just as \"Christ obeyed the Father up to death for us.\" One manifestation of this obedience was constant hard labour designed to subdue the flesh, exercise the will in daily self-denial, and set an example of industry in cultivation of the soil. The least deviation from the Rule entailed corporal punishment, or a severe form of fasting. In the second chapter, Columbanus instructs that the rule of silence be \"carefully observed\", since it is written: \"But the nurture of righteousness is silence and peace\". He also warns, \"Justly will they be damned who would not say just things when they could, but preferred to say with garrulous loquacity what is evil ...\" In the third chapter, Columbanus instructs, \"Let the monks' food be poor and taken in the evening, such as to avoid repletion, and their drink such as to avoid intoxication, so that it may both maintain life and not harm ...\" Columbanus continues:\n\nIn the fourth chapter, Columbanus presents the virtue of poverty and of overcoming greed, and that monks should be satisfied with \"small possessions of utter need, knowing that greed is a leprosy for monks\". Columbanus also instructs that \"nakedness and disdain of riches are the first perfection of monks, but the second is the purging of vices, the third the most perfect and perpetual love of God and unceasing affection for things divine, which follows on the forgetfulness of earthly things. Since this is so, we have need of few things, according to the word of the Lord, or even of one.\" In the fifth chapter, Columbanus warns against vanity, reminding the monks of Jesus' warning in Luke 16:15: \"You are the ones who justify yourselves in the eyes of others, but God knows your hearts. What people value highly is detestable in God's sight.\" In the sixth chapter, Columbanus instructs that \"a monk's chastity is indeed judged in his thoughts\" and warns, \"What profit is it if he be virgin in body, if he be not virgin in mind? For God, being Spirit.\"\n\nIn the seventh chapter, Columbanus instituted a service of perpetual prayer, known as \"laus perennis\", by which choir succeeded choir, both day and night. In the eighth chapter, Columbanus stresses the importance of discretion in the lives of monks to avoid \"the downfall of some, who beginning without discretion and passing their time without a sobering knowledge, have been unable to complete a praiseworthy life.\" Monks are instructed to pray to God for to \"illumine this way, surrounded on every side by the world's thickest darkness\". Columbanus continues: \nIn the ninth chapter, Columbanus presents mortification as an essential element in the lives of monks, who are instructed, \"Do nothing without counsel.\" Monks are warned to \"beware of a proud independence, and learn true lowliness as they obey without murmuring and hesitation.\" According to the Rule, there are three components to mortification: \"not to disagree in mind, not to speak as one pleases with the tongue, not to go anywhere with complete freedom.\" This mirrors the words of Jesus, \"For I have come down from heaven not to do my will but to do the will of him who sent me.\" (John 6:38) In the tenth and final chapter, Columbanus regulates forms of penance (often corporal) for offences, and it is here that the Rule of Saint Columbanus differs significantly from that of Saint Benedict.\n\nThe habit of the monks consisted of a tunic of undyed wool, over which was worn the cuculla, or cowl, of the same material. A great deal of time was devoted to various kinds of manual labour, not unlike the life in monasteries of other rules. The Rule of Saint Columbanus was approved of by the Synod of Mâcon in 627, but it was superseded at the close of the century by the Rule of Saint Benedict. For several centuries in some of the greater monasteries the two rules were observed conjointly.\n\nColumbanus did not lead a perfect life. According to Jonas and other sources, he could be impetuous and even headstrong, for by nature he was eager, passionate, and dauntless. These qualities were both the source of his power and the cause of his mistakes. His virtues, however, were quite remarkable. Like many saints, he had a great love for God's creatures. Stories claim that as he walked in the woods, it was not uncommon for birds to land on his shoulders to be caressed, or for squirrels to run down from the trees and nestle in the folds of his cowl. Although a strong defender of Irish traditions, he never wavered in showing deep respect for the Holy See as the supreme authority. His influence in Europe was due to the conversions he effected and to the rule that he composed. It may be that the example and success of Saint Columba in Caledonia inspired him to similar exertions. The life of Columbanus stands as the prototype of missionary activity in Europe, followed by such men as Saint Kilian, Vergilius of Salzburg, Donatus of Fiesole, Wilfrid, Willibrord, Suitbert of Kaiserwerdt, Saint Boniface, and Ursicinus of Saint-Ursanne.\n\nThe following are the principal miracles attributed to his intercession:\n\nJonas relates the occurrence of a miracle during Columbanus' time in Bregenz, when that region was experiencing a period of severe famine.\nIn France, the ruins of Columbanus' first monastery at Annegray are legally protected through the efforts of the Association Internationale des Amis de St Columban, which purchased the site in 1959. The association also owns and protects the site containing the cave, which acted as Columbanus' cell, and the holy well, which he created nearby. At Luxeuil-les-Bains, the Basilica of Saint Peter stands on the site of Columbanus' first church. A statue near the entrance, unveiled in 1947, shows him denouncing the immoral life of King Theuderic II. Formally an abbey church, the basilica contains old monastic buildings, which have been used as a minor seminary since the nineteenth century. It is dedicated to Columbanus and houses a bronze statue of him in its courtyard.\n\nIn Lombardy, San Colombano al Lambro in Milan, San Colombano Belmonte in Turin, and San Colombano Certénoli in Genoa all take their names from the saint. The last monastery erected by Columbanus at Bobbio remained for centuries the stronghold of orthodoxy in northern Italy.\n\nIf Bobbio Abbey in Italy became a citadel of faith and learning, Luxeuil Abbey in France became the \"nursery of saints and apostles\". The monastery produced sixty-three apostles who carried his rule, together with the Gospel, into France, Germany, Switzerland, and Italy. These disciples of Columbanus are accredited with founding over one hundred different monasteries. The canton and town still bearing the name of St. Gallen testify to how well one of his disciples succeeded.\n\nThe Missionary Society of Saint Columban, founded in 1916, and the Missionary Sisters of St. Columban, founded in 1924, are both dedicated to Columbanus.\n\nThe remains of Columbanus are preserved in the crypt at Bobbio Abbey. Many miracles have been credited to his intercession. In 1482, the relics were placed in a new shrine and laid beneath the altar of the crypt. The sacristy at Bobbio possesses a portion of the skull of the saint, his knife, wooden cup, bell, and an ancient water vessel, formerly containing sacred relics and said to have been given to him by Pope Gregory I. According to some authorities, twelve teeth of the saint were taken from the tomb in the fifteenth century and kept in the treasury, but these have since disappeared.\n\nColumbanus is named in the Roman Martyrology on 23 November, which is his feast day in Ireland. His feast is observed by the Benedictines on 24 November. Columbanus is the patron saint of motorcyclists. In art, Columbanus is represented bearded bearing the monastic cowl, holding in his hand a book with an Irish satchel, and standing in the midst of wolves. Sometimes he is depicted in the attitude of taming a bear, or with sun-beams over his head.\n\n\n"}
{"id": "6503", "url": "https://en.wikipedia.org/wiki?curid=6503", "title": "Concord, New Hampshire", "text": "Concord, New Hampshire\n\nConcord is the capital city of the U.S. state of New Hampshire and the county seat of Merrimack County. As of the 2010 census, its population was 42,695.\n\nConcord includes the villages of Penacook, East Concord, and West Concord. The city is home to the University of New Hampshire School of Law, New Hampshire's only law school; St. Paul's School, a private preparatory school; NHTI, a two-year community college; and the Granite State Symphony Orchestra.\n\nThe area that would become Concord was originally settled thousands of years ago by Abenaki Native Americans called the Pennacook. The tribe fished for migrating salmon, sturgeon, and alewives with nets strung across the rapids of the Merrimack River. The stream was also the transportation route for their birch bark canoes, which could travel from Lake Winnipesaukee to the Atlantic Ocean. The broad sweep of the Merrimack River valley floodplain provided good soil for farming beans, gourds, pumpkins, melons and maize.\n\nOn January 17, 1725, the Province of Massachusetts Bay, which then claimed territories west of the Merrimack River, granted the Concord area as the Plantation of Penacook. It was settled between 1725 and 1727 by Captain Ebenezer Eastman and others from Haverhill, Massachusetts. On February 9, 1734, the town was incorporated as Rumford, from which Sir Benjamin Thompson, Count Rumford would take his title. It was renamed Concord in 1765 by Governor Benning Wentworth following a bitter boundary dispute between Rumford and the town of Bow; the city name was meant to reflect the new concord, or harmony, between the disputant towns. Citizens displaced by the resulting border adjustment were given land elsewhere as compensation. In 1779, New Pennacook Plantation was granted to Timothy Walker, Jr. and his associates at what would be incorporated in 1800 as Rumford, Maine, the site of Pennacook Falls.\n\nConcord grew in prominence throughout the 18th century, and some of its earliest houses survive at the northern end of Main Street. In the years following the Revolution, Concord's central geographical location made it a logical choice for the state capital, particularly after Samuel Blodget in 1807 opened a canal and lock system to allow vessels passage around the Amoskeag Falls downriver, connecting Concord with Boston by way of the Middlesex Canal. In 1808, Concord was named the official seat of state government. The 1819 State House is the oldest capitol in the nation in which the state's legislative branches meet in their original chambers. The city would become noted for furniture-making and granite quarrying. In 1828, Lewis Downing joined J. Stephens Abbot to form Abbot and Downing. Their most famous coach was the Concord Coach, modeled after the coronation coach of King George III. In the 19th century, Concord became a hub for the railroad industry, with Penacook a textile manufacturing center using water power from the Contoocook River. Today, the city is a center for health care and several insurance companies. It is also home to Concord Litho, one of the largest independently owned commercial printing companies in the country.\n\nConcord is located at (43.2070, −71.5371).\n\nAccording to the United States Census Bureau, the city has a total area of . of it is land and of it is water, comprising 4.79% of the city. Concord is drained by the Merrimack River. Penacook Lake is in the west. The highest point in Concord is above sea level on Oak Hill, just west of the hill's summit in neighboring Loudon.\n\nConcord lies fully within the Merrimack River watershed, and is centered on the river, which runs from northwest to southeast through the city. Downtown is located on a low terrace to the west of the river, with residential neighborhoods climbing hills to the west and extending southwards towards the town of Bow. To the east of the Merrimack, atop a bluff, is a flat, sandy plain known as Concord Heights, which has seen most of the city's commercial development since 1960. The eastern boundary of Concord (with the town of Pembroke) is formed by the Soucook River, a tributary of the Merrimack. The Turkey River winds through the southwestern quarter of the city, passing through the campus of St. Paul's School before entering the Merrimack River in Bow. In the northern part of the city, the Contoocook River enters the Merrimack at the village of Penacook. Other village centers in the city include West Concord (actually north of downtown, on the west side of the Merrimack) and East Concord (also north of downtown, but on the east side of the Merrimack).\nThe city's neighboring communities are Bow to the south, Pembroke to the southeast, Loudon to the northeast, Canterbury, Boscawen, and Webster to the north, and Hopkinton to the west.\n\nConcord, as with much of New England, is within the humid continental climate zone (Köppen \"Dfb\"), with long, cold, snowy winters, very warm (and at times humid) summers, and relatively brief autumns and springs. In winter, successive storms deliver light to moderate snowfall amounts, contributing to the relatively reliable snow cover. In addition, lows reach at least on an average 15 nights per year, and the city straddles the border between USDA Hardiness Zone 5b and 6a. However, thaws are frequent, with one to three days per month with + highs from December to February. Summer can bring stretches of humid conditions as well as thunderstorms, and there is an average of 12 days of + highs annually. The window for freezing temperatures on average begins on September 27 and expires on May 14.\n\nThe monthly daily average temperature range from in January to in July. Temperature extremes have ranged from in February 1943 to in July 1966.\n\nAs of the census of 2010, there were 42,695 people, 17,592 households, and 10,052 families residing in the city. The population density was 632.5 people per square mile (244.2/km²). There were 18,852 housing units at an average density of 293.2 per square mile (113.2/km²). The racial makeup of the city was 91.8% White, 2.2% Black or African American, 0.3% Native American, 3.4% Asian, 0.0% Pacific Islander, 0.4% from some other race, and 1.8% from two or more races. 2.1% of the population were Hispanic or Latino of any race.\n\nThere were 17,592 households out of which 28.7% had children under the age of 18 living with them, 41.3% were headed by married couples living together, 11.6% had a female householder with no husband present, and 42.9% were non-families. 33.6% of all households were made up of individuals, and 12.0% were someone living alone who was 65 years of age or older. The average household size was 2.26, and the average family size was 2.90.\n\nIn the city, the population was spread out with 20.7% under the age of 18, 9.3% from 18 to 24, 28.0% from 25 to 44, 28.2% from 45 to 64, and 13.8% who were 65 years of age or older. The median age was 39.4 years. For every 100 females there were 98.5 males. For every 100 females age 18 and over, there were 96.9 males.\n\nFor the period 2009-11, the estimated median annual income for a household in the city was $52,695, and the median income for a family was $73,457. Male full-time workers had a median income of $49,228 versus $38,782 for females. The per capita income for the city was $29,296. About 5.5% of families and 10.1% of the population were below the poverty line, including 8.4% of those under age 18 and 5.5% of those age 65 or over.\n\nAccording to Concord's 2011 Comprehensive Annual Financial Report, the top employers in the city are:\n\nInterstate 89 and Interstate 93 are the two main interstate highways serving Concord, and join just south of the city limits. Interstate 89 links Concord with Lebanon and the state of Vermont to the northwest, while Interstate 93 connects the city to Plymouth, Littleton, and the White Mountains to the north and Manchester to the south. Interstate 393 is a spur highway leading east from Concord and merging with U.S. Route 4 as a direct route to New Hampshire's seacoast. North-south U.S. Route 3 serves as Concord's Main Street, while U.S. Route 202 and New Hampshire Route 9 cross the city from east to west. Also, state routes 13 and 132 serve the city: Route 13 leads southwest out of Concord towards Goffstown and Milford, while Route 132 travels north parallel to Interstate 93. New Hampshire Route 106 passes through the easternmost part of Concord, crossing I-393 and NH 9 before crossing the Soucook River into the town of Pembroke. To the north, NH 106 leads to Loudon, Belmont, and Laconia.\n\nLocal bus service is provided by Concord Area Transit (CAT), with three routes through the city. Regional bus service provided by Concord Coach Lines and Greyhound Lines is available from the Concord Transportation Center at 30 Stickney Avenue next to Exit 14 on Interstate 93, with service south to Boston and points in between, as well as north to Littleton and northeast to Berlin.\n\nThere is no passenger rail service to Concord.\n\nGeneral aviation services are available through Concord Municipal Airport, located east of downtown. There is no commercial air service within the city limits; the nearest such airport is Manchester–Boston Regional Airport, located to the south.\n\nConcord is governed via the manager-council system. The city council consists of 14 members, ten of which are elected from single-member wards, while the other four are elected at large. The mayor is elected directly every two years. The current mayor is Jim Bouley.\n\nAccording to the Concord city charter, the mayor chairs the council (composed of 15 members, including the mayor). However, the mayor has very few formal powers over the day-to-day management of the city. The actual operations of the city are overseen by the city manager, currently Thomas J. Aspell, Jr. The current police chief is Bradley S. Osgood.\n\nIn the New Hampshire Senate, Concord is in the 15th District, represented by Democrat Dan Feltes. On the New Hampshire Executive Council, Concord is in the 2nd District, represented by Democrat Andru Volinsky. In the United States House of Representatives, Concord is in New Hampshire's 2nd congressional district, represented by Democrat Ann McLane Kuster.\n\nNew Hampshire Department of Corrections operates the New Hampshire State Prison for Men in Concord.\n\nNewspapers\n\nRadio\nNew Hampshire Public Radio is headquartered in Concord.\n\nTelevision\n\nConcord has many landmarks and other tourist attractions.\n\nThe New Hampshire State House, designed by architect Stuart Park and constructed between 1815 and 1818, is the oldest state house in which the legislature meets in its original chambers. The building was remodeled in 1866, and the third story and west wing were added in 1910.\n\nLocated directly across from the State House is the Eagle Hotel on Main Street, which has been a downtown landmark since its opening in 1827. U.S. Presidents Ulysses S. Grant, Rutherford Hayes, and Benjamin Harrison all dined there, and Franklin Pierce spent the night before departing for his inauguration. Other well-known guests included Jefferson Davis, Charles Lindbergh, Eleanor Roosevelt, Richard M. Nixon (who carried New Hampshire in all three of his presidential bids), and Thomas E. Dewey. The hotel closed in 1961.\n\nSouth from the Eagle Hotel on Main Street is Phenix Hall, which replaced \"Old\" Phenix Hall, which burned in 1893. Both the old and new buildings featured multi-purpose auditoriums used for political speeches, theater productions, and fairs. Abraham Lincoln spoke at the old hall in 1860; Theodore Roosevelt, at the new hall in 1912.\n\nNorth on Main Street is the Walker-Woodman House, also known as the Reverend Timothy Walker House, the oldest standing two-story house in Concord. It was built for the Reverend Timothy Walker between 1733 and 1735.\n\nOn the north end of Main Street is the Pierce Manse, in which President Franklin Pierce lived in Concord before and following his presidency. The mid-1830s Greek Revival house was moved from Montgomery Street to North Main Street in 1971 to prevent its demolition.\n\nBeaver Meadow Golf Course, located in the northern part of Concord, is one of the oldest golf courses in New England. Besides this golf course, other important sporting venues in Concord include Everett Arena and Memorial Field.\n\nThe SNOB (Somewhat North Of Boston) Film Festival, started in the fall of 2002, brings independent films and filmmakers to Concord and has provided an outlet for local filmmakers to display their films. SNOB Film Festival was a catalyst for the building of Red River Theatres, a locally owned, nonprofit, independent cinema in 2007. The SNOB Film Festival is one of the many arts organizations in the city.\n\nOther sites of interest include the Capitol Center for the Arts, the New Hampshire Historical Society, which has two facilities in Concord, the Steeplegate Mall on Loudon Road, and the McAuliffe-Shepard Discovery Center, a planetarium named after Christa McAuliffe, the Concord teacher who died during the Space Shuttle Challenger disaster in 1986.\n\nConcord's public schools are within the Concord School District, except for schools in the Penacook area of the city, which are within the Merrimack Valley School District, a district which also includes several towns north of Concord. The only public high school in the Concord School District is Concord High School, which has about 2,000 students. The only public middle school in the Concord School District is Rundlett Middle School, which has roughly 1,500 students. Concord School District's elementary schools underwent a major re-configuration in 2012, with three newly constructed schools opening and replacing six previous schools. Kimball School and Walker School were replaced by Christa McAuliffe School on the Kimball School site, Conant School (and Rumford School, which closed a year earlier) were replaced by Abbot-Downing School at the Conant site, and Eastman and Dame schools were replaced by Mill Brook School, serving kindergarten through grade two, located next to Broken Ground Elementary School, serving grades three to five. Beaver Meadow School, the remaining elementary school, was unaffected by the changes.\n\nConcord schools in the Merrimack Valley School District include Merrimack Valley High School and Merrimack Valley Middle School, which are adjacent to each other and to Rolfe Park in Penacook village, and Penacook Elementary School, just south of the village.\n\nConcord has two parochial schools, Bishop Brady High School and Saint John Regional School.\n\nOther area schools include Concord Christian Academy, Parker Academy, Trinity Christian School, Shaker Road School, and St. Paul's School.\n\nConcord is also home to NHTI, Concord's Community College, Granite State College, the University of New Hampshire School of Law, and the Franklin Pierce University Doctorate of Physical Therapy program.\n\n"}
{"id": "6505", "url": "https://en.wikipedia.org/wiki?curid=6505", "title": "Chlorophyceae", "text": "Chlorophyceae\n\nThe Chlorophyceae are one of the classes of green algae, distinguished mainly on the basis of ultrastructural morphology. For example, the chlorophycean CW clade, and chlorophycean DO clade, are defined by the arrangement of their flagella. Members of the CW clade have flagella that are displaced in a \"clockwise\" (CW, 1–7 o'clock) direction e.g. Chlamydomonadales. Members of the DO clade have flagella that are \"directly opposed\" (DO, 12–6 o'clock) e.g. Sphaeropleales. They are usually green due to the dominance of pigments chlorophyll a and chlorophyll b. The chloroplast may be discoid, plate-like, reticulate, cup-shaped, spiral or ribbon shaped in different species. Most of the members have one or more storage bodies called pyrenoids located in the chloroplast. Pyrenoids contain protein besides starch. Some algae may store food in the form of oil droplets. Green algae usually have a rigid cell wall made up of an inner layer of cellulose and outer layer of pectose.\n\n\n\nVegetative reproduction usually takes place by fragmentation. Asexual reproduction is by flagellated zoospores.\nSexual reproduction shows considerable variation in the type and formation of sex cells and it may be isogamous, anisogamous or oogamous.\nThey share many similarities with the higher plants, including the presence of asymmetrical flagellated cells, the breakdown of the nuclear envelope at mitosis, and the presence of phytochromes, flavonoids, and the chemical precursors to the cuticle.\n\nThe following orders are typically recognised:\n\n\nIn older classifications, the term Chlorophyceae is sometimes used to apply to all the green algae except the Charales, and the internal division is considerably different.\n\nThe Orders of the Chlorophyceae as listed by: in Hoek, Mann and Jahns (1995)\n\n\n"}
{"id": "6508", "url": "https://en.wikipedia.org/wiki?curid=6508", "title": "Cyril", "text": "Cyril\n\nCyril (also Cyrillus or Cyryl) is a masculine given name. It is derived from the Greek name Κύριλλος (\"Kýrillos\") meaning \"Lordly, Masterful\", which in turn derives from Greek κυριος (\"kýrios\") \"lord\". There are various variant forms of the Cyril name such as Cyrill, Cyrille, Kirill, Kiryl, Kirillos, Kyrylo, Kiril, Kiro and Kyrill.\n\nIt may also refer to:\n\n\n\n\n\n"}
{"id": "6511", "url": "https://en.wikipedia.org/wiki?curid=6511", "title": "Computational complexity", "text": "Computational complexity\n\nComputational complexity may refer to:\n\n"}
{"id": "6512", "url": "https://en.wikipedia.org/wiki?curid=6512", "title": "Coercion", "text": "Coercion\n\nCoercion is the practice of forcing another party to act in an involuntary manner by use of intimidation or threats or some other form of pressure or force. It involves a set of various types of forceful actions that violate the free will of an individual to induce a desired response, for example: a bully demanding lunch money from a student or the student gets beaten. These actions may include, but are not limited to: extortion, blackmail, torture, threats to induce favors, or even sexual assault. In law, coercion is codified as a duress crime. Such actions are used as leverage, to force the victim to act in a way contrary to their own interests. Coercion may involve the actual infliction of physical pain/injury or psychological harm in order to enhance the credibility of a threat. The threat of further harm may lead to the cooperation or obedience of the person being coerced.\n\nThe purpose of coercion is to substitute one's aims to those of the victim. For this reason, many social philosophers have considered coercion as the polar opposite to freedom.\n\nVarious forms of coercion are distinguished: first on the basis of the \"kind of injury\" threatened, second according to its \"aims\" and \"scope\", and finally according to its \"effects\", from which its legal, social, and ethical implications mostly depend.\n\nPhysical coercion is the most commonly considered form of coercion, where the content of the conditional threat is the use of force against a victim, their relatives or property. An often used example is \"putting a gun to someone's head\" (\"at gunpoint\") or putting a \"knife under the throat\" (\"at knifepoint\" or cut-throat) to compel action or the victim gets killed or injured. These are so common that they are also used as metaphors for other forms of coercion.\n\nArmed forces in many countries use firing squads to maintain discipline and intimidate the masses, or opposition, into submission or silent compliance. However, there also are nonphysical forms of coercion, where the threatened injury does not immediately imply the use of force. Byman and Waxman (2000) define coercion as \"the use of threatened force, including the limited use of actual force to back up the threat, to induce an adversary to behave differently than it otherwise would.\" Coercion does not in many cases amount to destruction of property or life since compliance is the goal.\n\nIn psychological coercion, the threatened injury regards the victim's relationships with other people. The most obvious example is \"blackmail\", where the threat consists of the dissemination of damaging information. However, many other types are possible e.g. so-called \"emotional blackmail\", which typically involves threats of rejection from or disapproval by a peer-group, or creating feelings of guilt/obligation via a display of anger or hurt by someone whom the victim loves or respects. Another example is coercive persuasion.\n\nPsychological coercion – along with the other varieties – was extensively and systematically used by the government of the People's Republic of China during the \"Thought Reform\" campaign of 1951–1952. The process – carried out partly at \"revolutionary universities\" and partly within prisons – was investigated and reported upon by Robert Jay Lifton, then Research Professor of Psychiatry at Yale University: see Lifton (1961). The techniques used by the Chinese authorities included a technique derived from standard group psychotherapy, which was aimed at forcing the victims (who were generally intellectuals) to produce detailed and sincere ideological \"confessions\". For instance, a professor of formal logic called Chin Yueh-lin – who was then regarded as China's leading authority on his subject – was induced to write: \"The new philosophy [of Marxism-Leninism], being scientific, is the supreme truth\" [Lifton (1961) p. 545].\n\n\n"}
{"id": "6513", "url": "https://en.wikipedia.org/wiki?curid=6513", "title": "Client–server model", "text": "Client–server model\n\nThe client–server model is a distributed application structure that partitions tasks or workloads between the providers of a resource or service, called servers, and service requesters, called clients. Often clients and servers communicate over a computer network on separate hardware, but both client and server may reside in the same system. A server host runs one or more server programs which share their resources with clients. A client does not share any of its resources, but requests a server's content or service function. Clients therefore initiate communication sessions with servers which await incoming requests.\nExamples of computer applications that use the client–server model are Email, network printing, and the World Wide Web.\n\nThe \"client-server\" characteristic describes the relationship of cooperating programs in an application. The server component provides a function or service to one or many clients, which initiate requests for such services.\n\nServers are classified by the services they provide. For example, a web server serves web pages and a file server serves computer files. A shared resource may be any of the server computer's software and electronic components, from programs and data to processors and storage devices. The sharing of resources of a server constitutes a \"service\".\n\nWhether a computer is a client, a server, or both, is determined by the nature of the application that requires the service functions. For example, a single computer can run web server and file server software at the same time to serve different data to clients making different kinds of requests. Client software can also communicate with server software within the same computer. Communication between servers, such as to synchronize data, is sometimes called \"inter-server\" or \"server-to-server\" communication.\n\nIn general, a service is an abstraction of computer resources and a client does not have to be concerned with how the server performs while fulfilling the request and delivering the response. The client only has to understand the response based on the well-known application protocol, i.e. the content and the formatting of the data for the requested service.\n\nClients and servers exchange messages in a request–response messaging pattern. The client sends a request, and the server returns a response. This exchange of messages is an example of inter-process communication. To communicate, the computers must have a common language, and they must follow rules so that both the client and the server know what to expect. The language and rules of communication are defined in a communications protocol. All client-server protocols operate in the application layer. The application layer protocol defines the basic patterns of the dialogue. To formalize the data exchange even further, the server may implement an application programming interface (API). The API is an abstraction layer for accessing a service. By restricting communication to a specific content format, it facilitates parsing. By abstracting access, it facilitates cross-platform data exchange.\n\nA server may receive requests from many distinct clients in a short period of time. A computer can only perform a limited number of tasks at any moment, and relies on a scheduling system to prioritize incoming requests from clients to accommodate them. To prevent abuse and maximize availability, server software may limit the availability to clients. Denial of service attacks are designed to exploit a server's obligation to process requests by overloading it with excessive request rates.\n\nWhen a bank customer accesses online banking services with a web browser (the client), the client initiates a request to the bank's web server. The customer's login credentials may be stored in a database, and the web server accesses the database server as a client. An application server interprets the returned data by applying the bank's business logic, and provides the output to the web server. Finally, the web server returns the result to the client web browser for display. \n\nIn each step of this sequence of client–server message exchanges, a computer processes a request and returns data. This is the request-response messaging pattern. When all the requests are met, the sequence is complete and the web browser presents the data to the customer.\n\nThis example illustrates a design pattern applicable to the client–server model: separation of concerns.\n\nAn early form of client–server architecture is remote job entry, dating at least to OS/360 (announced 1964), where the request was to run a job, and the response was the output.\n\nWhile formulating the client–server model in the 1960s and 1970s, computer scientists building ARPANET (at the Stanford Research Institute) used the terms \"server-host\" (or \"serving host\") and \"user-host\" (or \"using-host\"), and these appear in the early documents RFC 5 and RFC 4. This usage was continued at Xerox PARC in the mid-1970s.\n\nOne context in which researchers used these terms was in the design of a computer network programming language called Decode-Encode Language (DEL). The purpose of this language was to accept commands from one computer (the user-host), which would return status reports to the user as it encoded the commands in network packets. Another DEL-capable computer, the server-host, received the packets, decoded them, and returned formatted data to the user-host. A DEL program on the user-host received the results to present to the user. This is a client–server transaction. Development of DEL was just beginning in 1969, the year that the United States Department of Defense established ARPANET (predecessor of Internet).\n\n\"Client-host\" and \"server-host\" have subtly different meanings than \"client\" and \"server\". A host is any computer connected to a network. Whereas the words \"server\" and \"client\" may refer either to a computer or to a computer program, \"server-host\" and \"user-host\" always refer to computers. The host is a versatile, multifunction computer; \"clients\" and \"servers\" are just programs that run on a host. In the client–server model, a server is more likely to be devoted to the task of serving.\n\nAn early use of the word \"client\" occurs in \"Separating Data from Function in a Distributed File System\", a 1978 paper by Xerox PARC computer scientists Howard Sturgis, James Mitchell, and Jay Israel. The authors are careful to define the term for readers, and explain that they use it to distinguish between the user and the user's network node (the client). (By 1992, the word \"server\" had entered into general parlance.)\n\nThe client–server model does not dictate that server-hosts must have more resources than client-hosts. Rather, it enables any general-purpose computer to extend its capabilities by using the shared resources of other hosts. Centralized computing, however, specifically allocates a large amount of resources to a small number of computers. The more computation is offloaded from client-hosts to the central computers, the simpler the client-hosts can be. It relies heavily on network resources (servers and infrastructure) for computation and storage. A diskless node loads even its operating system from the network, and a computer terminal has no operating system at all; it is only an input/output interface to the server. In contrast, a fat client, such as a personal computer, has many resources, and does not rely on a server for essential functions.\n\nAs microcomputers decreased in price and increased in power from the 1980s to the late 1990s, many organizations transitioned computation from centralized servers, such as mainframes and minicomputers, to fat clients. This afforded greater, more individualized dominion over computer resources, but complicated information technology management. During the 2000s, web applications matured enough to rival application software developed for a specific microarchitecture. This maturation, more affordable mass storage, and the advent of service-oriented architecture were among the factors that gave rise to the cloud computing trend of the 2010s.\n\nIn addition to the client–server model, distributed computing applications often use the peer-to-peer (P2P) application architecture.\n\nIn the client–server model, the server is often designed to operate as a centralized system that serves many clients. The computing power, memory and storage requirements of a server must be scaled appropriately to the expected work-load (\"i.e.\", the number of clients connecting simultaneously). Load-balancing and failover systems are often employed to scale the server implementation.\n\nIn a peer-to-peer network, two or more computers (\"peers\") pool their resources and communicate in a decentralized system. Peers are coequal, or equipotent nodes in a non-hierarchical network. Unlike clients in a client–server or client–queue–client network, peers communicate with each other directly.\nIn peer-to-peer networking, an algorithm in the peer-to-peer communications protocol balances load, and even peers with modest resources can help to share the load. If a node becomes unavailable, its shared resources remain available as long as other peers offer it. Ideally, a peer does not need to achieve high availability because other, redundant peers make up for any resource downtime; as the availability and load capacity of peers change, the protocol reroutes requests.\n\nVasudeva Varma regards both client-server and master-slave as sub-categories of distributed peer-to-peer systems.\n"}
{"id": "6514", "url": "https://en.wikipedia.org/wiki?curid=6514", "title": "County Dublin", "text": "County Dublin\n\nCounty Dublin ( or \"Contae Átha Cliath\") is a county in Ireland. Since the abolition of Dublin County Council in 1994, for local government it has been divided into four administrative areas: Dublin city, Dún Laoghaire–Rathdown, Fingal and South Dublin. The population of the entire county was 1,345,402 according to the census of 2016. It is conterminous with the Dublin Region and is in the province of Leinster. It is named after the city of Dublin, which is the regional capital and the capital city of Ireland. County Dublin was one of the first parts of Ireland to be shired by John, King of England following the Norman invasion of Ireland.\n\nSince the abolition of the Dublin Regional Assembly by statutory instrument No. 573/2014, the Eurostat statistical region known as \"Dublin Region\" falls under the remit of the wider Eastern and Midland Regional Authority.\n\nThere are four local authorities whose remit collectively encompasses the geographic area of the county and city of Dublin. These are Dublin City Council, South Dublin County Council, Dún Laoghaire-Rathdown County Council and Fingal County Council.\n\nPrior to the enactment of the Local Government (Dublin) Act 1993, the county was a unified whole even though it was administered by two local authorities - Dublin County Council and Dublin Corporation. Since the enactment of the Local Government Act 2001 in particular, the geographic area of the county has been divided between three entities at the level of \"county\" and a further entity at the level of \"city\". They rank equally as first level local administrative units of the NUTS 3 Dublin Region for Eurostat purposes. There are 34 LAU 1 entities in the Republic of Ireland. Each local authority is responsible for certain local services such as sanitation, planning and development, libraries, the collection of motor taxation, local roads and social housing.\n\nDublin County Council (which did not include the county borough of Dublin) was abolished in 1994 and the area divided among the administrative counties of Dún Laoghaire–Rathdown, Fingal and South Dublin each with its county seat. To these areas may be added the area of Dublin city which collectively comprise the Dublin Region (\"\") and come under the remit of the Dublin Regional Authority.\n\nThe area lost its administrative county status in 1994, with Section 9 Part 1(a) of the \"Local Government (Dublin) Act, 1993\" stating that \"the county shall cease to exist.\" In discussing the legislation to dissolve Dublin County Council, Avril Doyle TD said, \"The Bill before us today effectively abolishes County Dublin, and as one born and bred in these parts of Ireland I find it rather strange that we in this House are abolishing County Dublin. I am not sure whether Dubliners realise that that is what we are about today, but in effect that is the case.\"\n\nThe county is part of the Dublin constituency for the purposes of European elections. For elections to Dáil Éireann, the area of the county is currently (2016) divided into eleven constituencies: Dublin Bay North, Dublin Bay South, Dublin Central, Dublin Fingal, Dublin Mid-West, Dublin North-West, Dublin Rathdown, Dublin South-Central, Dublin South-West, Dublin West, and Dún Laoghaire. Together they return 44 deputies (TDs) to the Dáil.\n\nDespite the legal status of the Dublin Region, the term \"County Dublin\" is still in common usage. Many organisations and sporting teams continue to organise on a \"County Dublin\" or \"Dublin Region\" basis. The area formerly known as \"County Dublin\" is now defined in legislation solely as the \"Dublin Region\" under the \"Local Government Act, 1991 (Regional Authorities) (Establishment) Order, 1993\", and this is the terminology officially used by the four Dublin administrative councils in press releases concerning the former county area. The term \"Greater Dublin Area\", which might consist of some or all of the Dublin Region along with counties of Kildare, Meath and Wicklow, has no legal standing.\n\nThe Dublin Region is a NUTS Level III region of Ireland. The region is one of eight regions of the Republic of Ireland for the purposes of Eurostat statistics. Its NUTS code is IE021. It is co-extensive with the old county. The regional capital is Dublin City which is also the national capital.\n\nThe latest Ordnance Survey Ireland \"Discovery Series\" (Third Edition 2005) 1:50,000 map of the Dublin Region, Sheet 50, shows the boundaries of the city and three surrounding counties of the region. Extremities of the Dublin Region, in the north and south of the region, appear in other sheets of the series, 43 and 56 respectively.\n\n\nMost of the area can receive the five main UK television channels on analogue television as well as the main Irish channels, along with Sky TV and Virgin Media Ireland cable television.\n\n\nThe economy of County Dublin was identified as being the powerhouse behind the Celtic Tiger, a period of strong economic growth of the state. This resulted in the economy of the county expanding by almost 100% between the early 1990s and 2007. This growth resulted from incoming high-value industries, such as financial services and software manufacturing, as well as low-skilled retail and domestic services, which caused a shift away from older manufacturing-industry. This change saw high unemployment in the 1980s and early 1990s which resulted in damage to the capitals social structure.\n\nAccording to CSO figures, the region had a GDP of €87.238 bn and a GDP per capita of €68,208 in 2014 (the second highest was Cork at €50,544 per capita). \n\nSeparately, Eurostat figures for 2012 suggested the region then had a GDP of €72.384 bn and a GDP per capita of €57,200 - the highest on the island of Ireland (the second highest being Cork with €48,500). \n\nAs of early 2017, the unemployment rate for the Dublin region was estimated at 6%.\n\nCounty Dublin is the main transport node of Ireland, and contains one international airport, Dublin Airport. It is also served by two main seaports, Dún Laoghaire port and Dublin Port, which is just located outside of the city center. The two main train stations are Dublin Heuston and Dublin Connolly, both of which serve intercity trains.\n\nAccording to the 2006 census, County Dublin had a population of 1,187,176, which constitutes 30% of the national population. This was an increase of 9.5% on 2002 figures. Its population density was 1,218/km². The population of Dublin City, was 506,211.\n\nThe median age of the population of the county in the 2006 census was 35.6 years, with 62% of people aged between 20–64 years old. Net migration to the county between 2002 and 2006 was 48,000, with a natural increase of 33,000 people.\n\nThere are 10,469 Irish speakers in County Dublin attending the 31 Gaelscoileanna (Irish language primary schools) and eight Gaelcholáistí (Irish language secondary schools). There may be up to another 10,000 Irish speakers from the Gaeltacht living and working in Dublin also.\n\nA list of the largest urban areas (those with over 1,000 inhabitants) in County Dublin. Administrative county seats are shown in bold.\n\n\n"}
{"id": "6516", "url": "https://en.wikipedia.org/wiki?curid=6516", "title": "Cosmological argument", "text": "Cosmological argument\n\nIn natural theology, a cosmological argument is an argument in which the existence of a unique being, generally seen as some kind of god or demiurge is deduced or inferred from facts or alleged facts concerning causation, change, motion, contingency, or finitude in respect of the universe as a whole or processes within it. It is traditionally known as an argument from universal causation, an argument from first cause, or the causal argument. Whichever term is employed, there are three basic variants of the argument, each with subtle yet important distinctions: the arguments from \"in causa\" (causality), \"in esse\" (essentiality), and \"in fieri\" (becoming).\n\nThe basic premises of all of these are the concept of causality and the Universe having a beginning. The conclusion of these arguments is first cause, subsequently deemed to be God. The history of this argument goes back to Aristotle or earlier, was developed in Neoplatonism and early Christianity and later in medieval Islamic theology during the 9th to 12th centuries, and re-introduced to medieval Christian theology in the 13th century by Thomas Aquinas. The cosmological argument is closely related to the principle of sufficient reason as addressed by Gottfried Leibniz and Samuel Clarke, itself a modern exposition of the claim that \"nothing comes from nothing\" attributed to Parmenides.\n\nContemporary defenders of cosmological arguments include William Lane Craig, Robert Koons, Alexander Pruss, and William L. Rowe.\n\nPlato (c. 427–347 BC) and Aristotle (c. 384–322 BC) both posited first cause arguments, though each had certain notable caveats. In \"The Laws\" (Book X), Plato posited that all movement in the world and the Cosmos was \"imparted motion\". This required a \"self-originated motion\" to set it in motion and to maintain it. In \"Timaeus\", Plato posited a \"demiurge\" of supreme wisdom and intelligence as the creator of the Cosmos.\n\nAristotle argued \"against\" the idea of a first cause, often confused with the idea of a \"prime mover\" or \"unmoved mover\" ( or \"primus motor\") in his \"Physics\" and \"Metaphysics\". Aristotle argued in \"favor\" of the idea of several unmoved movers, one powering each celestial sphere, which he believed lived beyond the sphere of the fixed stars, and explained why how motion in the universe (which he believed was eternal) had continued for an infinite period of time. Aristotle argued the atomist's assertion of a non-eternal universe would require an uncaused cause — in his terminology, an efficient first cause — an idea he considered a non-sensical flaw in the reasoning of the atomists.\n\nLike Plato, Aristotle believed in an eternal cosmos with no beginning and no end (which in turn follows Parmenides' famous statement that \"nothing comes from nothing\"). In what he called \"first philosophy\" or metaphysics, Aristotle \"did\" intend a theological correspondence between the prime mover and deity (presumably Zeus); functionally, however, he provided an explanation for the apparent motion of the \"fixed stars\" (now understood as the daily rotation of the Earth). According to his theses, immaterial unmoved movers are eternal unchangeable beings that constantly think about thinking, but being immaterial, they're incapable of interacting with the cosmos and have no knowledge of what transpires therein. From an \"aspiration or desire\", the celestial spheres, \"imitate\" that purely intellectual activity as best they can, by uniform circular motion. The unmoved movers \"inspiring\" the planetary spheres are no different in kind from the prime mover, they merely suffer a dependency of relation to the prime mover. Correspondingly, the motions of the planets are subordinate to the motion inspired by the prime mover in the sphere of fixed stars. Aristotle's natural theology admitted no creation or capriciousness from the immortal pantheon, but maintained a defense against dangerous charges of impiety.\n\nPlotinus, a third-century Platonist, taught that the One transcendent absolute caused the universe to exist simply as a consequence of its existence (\"creatio ex deo\"). His disciple Proclus stated \"The One is God\".\n\nCenturies later, the Islamic philosopher Avicenna (c. 980–1037) inquired into the question of being, in which he distinguished between essence (\"Mahiat\") and existence (\"Wujud\"). He argued that the fact of existence could not be inferred from or accounted for by the essence of existing things, and that form and matter by themselves could not originate and interact with the movement of the Universe or the progressive actualization of existing things. Thus, he reasoned that existence must be due to an agent cause that necessitates, imparts, gives, or adds existence to an essence. To do so, the cause must coexist with its effect and be an existing thing.\n\nSteven Duncan writes that \"it was first formulated by a Greek-speaking Syriac Christian neo-Platonist, John Philoponus,\" who claims to find a contradiction between the Greek pagan insistence on the eternity of the world and the Aristotelian rejection of the existence of any actual infinite.\" Referring to the argument as the \"'Kalam' cosmological argument\", Duncan asserts that it \"received its fullest articulation at the hands of [medieval] Muslim and Jewish exponents of \"Kalam\" (\"the use of reason by believers to justify the basic metaphysical presuppositions of the faith).\"\n\nThomas Aquinas (c. 1225–1274) adapted and enhanced the argument he found in his reading of Aristotle and Avicenna to form one of the most influential versions of the cosmological argument. His conception of First Cause was the idea that the Universe must have been caused by something that was itself uncaused, which he asserted was God.\n\nIn the scholastic era, Aquinas formulated the \"argument from contingency\", following Aristotle in claiming that there must be something to explain why the Universe exists. Since the Universe could, under different circumstances, conceivably \"not\" exist (contingency), its existence must have a cause – not merely another contingent thing, but something that exists by necessity (something that \"must\" exist in order for anything else to exist). In other words, even if the Universe has always existed, it still owes its existence to an Uncaused Cause, Aquinas further said: \"...and this we understand to be God.\"\n\nAquinas's argument from contingency allows for the possibility of a Universe that has no beginning in time. It is a form of argument from universal causation. Aquinas observed that, in nature, there were things with contingent existences. Since it is possible for such things not to exist, there must be some time at which these things did not in fact exist. Thus, according to Aquinas, there must have been a time when nothing existed. If this is so, there would exist nothing that could bring anything into existence. Contingent beings, therefore, are insufficient to account for the existence of contingent beings: there must exist a \"necessary\" being whose non-existence is an impossibility, and from which the existence of all contingent beings is derived.\n\nThe German philosopher Gottfried Leibniz made a similar argument with his principle of sufficient reason in 1714. \"There can be found no fact that is true or existent, or any true proposition,\" he wrote, \"without there being a sufficient reason for its being so and not otherwise, although we cannot know these reasons in most cases.\" He formulated the cosmological argument succinctly: \"Why is there something rather than nothing? The sufficient reason [...] is found in a substance which [...] is a necessary being bearing the reason for its existence within itself.\"\n\nThe difference between the arguments from causation \"in fieri\" and \"in esse\" is a fairly important one. \"In fieri\" is generally translated as \"becoming\", while \"in esse\" is generally translated as \"in essence\". \"In fieri\", the process of becoming, is similar to building a house. Once it is built, the builder walks away, and it stands on its own accord. (It may require occasional maintenance, but that is beyond the scope of the first cause argument.)\n\n\"In esse\" (essence) is more akin to the light from a candle or the liquid in a vessel. George Hayward Joyce, SJ, explained that \"...where the light of the candle is dependent on the candle's continued existence, not only does a candle produce light in a room in the first instance, but its continued presence is necessary if the illumination is to continue. If it is removed, the light ceases. Again, a liquid receives its shape from the vessel in which it is contained; but were the pressure of the containing sides withdrawn, it would not retain its form for an instant.\" This form of the argument is far more difficult to separate from a purely first cause argument than is the example of the house's maintenance above, because here the First Cause is insufficient without the candle's or vessel's continued existence.\n\nThus, Leibniz' argument is \"in fieri\", while Aquinas' argument is both \"in fieri\" and \"in esse\". This distinction is an excellent example of the difference between a deistic view (Leibniz) and a theistic view (Aquinas). As a general trend, the modern slants on the cosmological argument, including the Kalam argument, tend to lean very strongly towards an \"in fieri\" argument.\n\nWilliam Lane Craig gives this argument in the following general form:\n\nCraig explains, by nature of the event (the Universe coming into existence), attributes unique to (the concept of) God must also be attributed to the cause of this event, including but not limited to: omnipotence, Creator, being eternal and absolute self-sufficiency. Since these attributes are unique to God, anything with these attributes must be God. Something does have these attributes: the cause; hence, the cause is God, the cause exists; hence, God exists.\n\nCraig defends the second premise, that the Universe had a beginning starting with Al-Ghazali's proof that an actual infinite is impossible. However, If the universe never had a beginning then there indeed would be an actual infinite, an infinite amount of cause and effect events. Hence, the Universe had a beginning.\n\nOne objection to the argument is that it leaves open the question of why the First Cause is unique in that it does not require any causes. Proponents argue that the First Cause is exempt from having a cause, while opponents argue that this is special pleading or otherwise untrue. Critics often press that arguing for the First Cause's exemption raises the question of why the First Cause is indeed exempt, whereas defenders maintain that this question has been answered by the various arguments, emphasizing that none of its major forms rests on the premise that everything has a cause.\n\nSecondly, it is argued that the premise of causality has been arrived at via \"a posteriori\" (inductive) reasoning, which is dependent on experience. David Hume highlighted this problem of induction and argued that causal relations were not true \"a priori\". However, as to whether inductive or deductive reasoning is more valuable still remains a matter of debate, with the general conclusion being that neither is prominent. Opponents of the argument tend to argue that it is unwise to draw conclusions from an extrapolation of causality beyond experience.\n\nThe basic cosmological argument merely establishes that a First Cause exists, not that it has the attributes of a theistic god, such as omniscience, omnipotence, and omnibenevolence. This is why the argument is often expanded to show that at least some of these attributes are necessarily true, for instance in the modern Kalam argument given above.\n\nA causal loop is a form of predestination paradox arising where traveling backwards in time is deemed a possibility. A sufficiently powerful entity in such a world would have the capacity to travel backwards in time to a point before its own existence, and to then create itself, thereby initiating everything which follows from it.\n\nThe usual reason which is given to refute the possibility of a causal loop is it requires that the loop as a whole be its own cause. Richard Hanley argues that causal loops are not logically, physically, or epistemically impossible: \"[In timed systems,] the only possibly objectionable feature that all causal loops share is that coincidence is required to explain them.\"\n\nDavid Hume and later Paul Edwards have invoked a similar principle in their criticisms of the cosmological argument. Rowe has called the principle the Hume-Edwards principle:\nNevertheless, David White argues that the notion of an infinite causal regress providing a proper explanation is fallacious. Furthermore, Demea states that even if the succession of causes is infinite, the whole chain still requires a cause. To explain this, suppose there exists a causal chain of infinite contingent beings. If one asks the question, \"Why are there any contingent beings at all?\", it won’t help to be told that \"There are contingent beings because other contingent beings caused them.\" That answer would just presuppose additional contingent beings. An adequate explanation of why some contingent beings exist would invoke a different sort of being, a necessary being that is \"not\" contingent. A response might suppose each individual is contingent but the infinite chain as a whole is not; or the whole infinite causal chain to be its own cause.\n\nSeverinsen argues that there is an \"infinite\" and complex causal structure. White tried to introduce an argument “without appeal to the principle of sufficient reason and without denying the possibility of an infinite causal regress”.\n\nSome cosmologists and physicists argue that a challenge to the cosmological argument is the nature of time: \"One finds that time just disappears from the Wheeler–DeWitt equation\" (Carlo Rovelli). The Big Bang theory states that it is the point in which all dimensions came into existence, the start of both space and time. Then, the question \"What was there before the Universe?\" makes no sense; the concept of \"before\" becomes meaningless when considering a situation without time. This has been put forward by J. Richard Gott III, James E. Gunn, David N. Schramm, and Beatrice Tinsley, who said that asking what occurred before the Big Bang is like asking what is north of the North Pole. However, some cosmologists and physicists do attempt to investigate causes for the Big Bang, using such scenarios as the collision of membranes.\n\nPhilosopher Edward Feser states that classical philosophers' arguments for the existence of God do not care about the Big Bang or whether the universe had a beginning. The question is not about what got things started or how long they have been going, but rather what keeps them going.\n\nAlternatively, the above objections can be dispelled by separating the Cosmological Argument from the A-Theory of Time and subsequently discussing God as a \"timeless\" (rather than \"before\" in a linear sense) cause of the Big Bang. There is also a Big Bang Argument, which is a variation of the Cosmological Argument using the Big Bang Theory to validate the premise that the Universe had a beginning.\n\n"}
{"id": "6517", "url": "https://en.wikipedia.org/wiki?curid=6517", "title": "Clutch", "text": "Clutch\n\nA clutch is a mechanical device which engages and disengages power transmission especially from driving shaft to driven shaft.\n\nIn the simplest application, clutches connect and disconnect two rotating shafts (drive shafts or line shafts). In these devices, one shaft is typically attached to an engine or other power unit (the driving member) while the other shaft (the driven member) provides output power for work. While typically the motions involved are rotary, linear clutches are also possible.\n\nIn a torque-controlled drill, for instance, one shaft is driven by a motor and the other drives a drill chuck. The clutch connects the two shafts so they may be locked together and spin at the same speed (engaged), locked together but spinning at different speeds (slipping), or unlocked and spinning at different speeds (disengaged).\n\nThis type of clutch has protruding circular edge and a hole for them that engages and disengages during operation. This type is less effective since human foot or hand power on clutching reaches about 10 KN or 1,000 kg.\n\nThe vast majority of clutches ultimately rely on frictional forces for their operation. The purpose of friction clutches is to connect a moving member to another that is moving at a different speed or stationary, often to synchronize the speeds, and/or to transmit power. Usually, as little slippage (difference in speeds) as possible between the two members is desired.\n\nVarious materials have been used for the disc-friction facings, including asbestos in the past. Modern clutches typically use a compound organic resin with copper wire facing or a ceramic material. Ceramic materials are typically used in heavy applications such as racing or heavy-duty hauling, though the harder ceramic materials increase flywheel and pressure plate wear.\n\nIn the case of \"wet\" clutches, composite paper materials are very common. Since these \"wet\" clutches typically use an oil bath or flow-through cooling method for keeping the disc pack lubricated and cooled, very little wear is seen when using composite paper materials.\n\nFriction-disc clutches generally are classified as \"push type\" or \"pull type\" depending on the location of the pressure plate fulcrum points. In a pull-type clutch, the action of pressing the pedal pulls the release bearing, pulling on the diaphragm spring and disengaging the vehicle drive. The opposite is true with a push type, the release bearing is pushed into the clutch disengaging the vehicle drive. In this instance, the release bearing can be known as a thrust bearing (as per the image above).\n\nA clutch damper is a device that softens the response of the clutch engagement/disengagement. In automotive applications, this is often provided by a mechanism in the clutch disc centres. In addition to the damped disc centres, which reduce driveline vibration, pre-dampers may be used to reduce gear rattle at idle by changing the natural frequency of the disc. These weaker springs are compressed solely by the radial vibrations of an idling engine. They are fully compressed and no longer in use once the main damper springs take up drive.\n\nMercedes truck examples:\nA clamp load of 33 kN is normal for a single plate 430. The 400 Twin application offers a clamp load of a mere 23 kN. Bursts speeds are typically around 5,000 rpm with the weakest point being the facing rivet.\n\nModern clutch development focuses its attention on the simplification of the overall assembly and/or manufacturing method. For example, drive straps are now commonly employed to transfer torque as well as lift the pressure plate upon disengagement of vehicle drive. With regard to the manufacture of diaphragm springs, heat treatment is crucial. Laser welding is becoming more common as a method of attaching the drive plate to the disc ring with the laser typically being between 2-3KW and a feed rate 1m/minute.\n\nThis type of clutch has several driving members interleaved or \"stacked\" with several driven members. It is used in racing cars including Formula 1, IndyCar, World Rally and even most club racing. Multiplate clutches see much use in drag racing, which requires the best acceleration possible, and is notorious for the abuse the clutch is subjected to. Thus motorcycles, automatic transmissions and in some diesel locomotives with mechanical transmissions. It is also used in some electronically controlled all-wheel drive systems as well as in some transfer cases. They can also be found in some heavy machinery such as tanks and AFV's (T-54) and earthmoving equipment (front-end loaders, bulldozers), as well as components in certain types of limited slip differentials. The benefit in the case of motorsports is that you can achieve the same total friction force with a much smaller overall diameter (or conversely, a much greater friction force for the same diameter, important in cases where a vehicle is modified with greater power, yet the maximum physical size of the clutch unit is constrained by the clutch housing). In motorsports vehicles that run at high engine/drivetrain speeds, the smaller diameter reduces rotational inertia, making the drivetrain components accelerate more rapidly, as well as reducing the angular velocity of the outer areas of the clutch unit, which could become highly stressed and fail at the extremely high drivetrain rotational rates achieved in sports such as Formula 1 or drag racing. In the case of heavy equipment, which often deal with very high torque forces and drivetrain loads, a single plate clutch of the necessary strength would be too large to easily package as a component of the driveline.\n\nAnother, different theme on the multiplate clutch is the clutches used in the fastest classes of drag racing, highly specialized, purpose-built cars such as Top Fuel or Funny Cars. These cars are so powerful that to attempt a start with a simple clutch would result in complete loss of traction. To avoid this problem, Top Fuel cars actually use a single, fixed gear ratio, and a \"series\" of clutches that are engaged one at a time, rather than in unison, progressively allowing more power to the wheels. A single one of these clutch plates (as designed) can not hold more than a fraction of the power of the engine, so the driver starts with only the first clutch engaged. This clutch is overwhelmed by the power of the engine, allowing only a fraction of the power to the wheels, much like \"slipping the clutch\" in a slower car, but working not requiring concentration from the driver. As speed builds, the driver pulls a lever, which engages a second clutch, sending a bit more of the engine power to the wheels, and so on. This continues through several clutches until the car has reached a speed where the last clutch can be engaged. With all clutches engaged, the engine is now sending all of its power to the rear wheels. This is far more predictable and repeatable than the driver manually slipping the clutch himself and then shifting through the gears, given the extreme violence of the run and the speed at which is all unfolds. Another benefit is that there is no need to break the power flow in order to swap gears (a conventional manual cannot transmit power while between gears, which is important because 1/100ths of a second are important in Top Fuel races). A traditional multiplate clutch would be more prone to overheating and failure, as all the plates must be subjected to heat and friction together until the clutch is fully engaged, while a Top Fuel car keeps its last clutches in \"reserve\" until the cars speed allows full engagement. It is relatively easy to design the last stages to be much more powerful than the first, in order to ensure they can absorb the power of the engine even if the first clutches burn out or overheat from the extreme friction.\n\nA \"wet clutch\" is immersed in a cooling lubricating fluid that also keeps surfaces clean and provides smoother performance and longer life. Wet clutches, however, tend to lose some energy to the liquid. Since the surfaces of a wet clutch can be slippery (as with a motorcycle clutch bathed in engine oil), stacking multiple clutch discs can compensate for the lower coefficient of friction and so eliminate slippage under power when fully engaged.\nThe Hele-Shaw clutch was a wet clutch that relied entirely on viscous effects, rather than on friction.\n\nA \"dry clutch\", as the name implies, is not bathed in liquid and uses friction to engage.\n\nA centrifugal clutch is used in some vehicles (e.g., mopeds) and also in other applications where the speed of the engine defines the state of the clutch, for example, in a chainsaw. This clutch system employs centrifugal force to automatically engage the clutch when the engine rpm rises above a threshold and to automatically disengage the clutch when the engine rpm falls low enough. See Saxomat and Variomatic.\n\nAs the name implies, a cone clutch has conical friction surfaces. The cone's taper means that a given amount of movement of the actuator makes the surfaces approach (or recede) much more slowly than in a disc clutch. As well, a given amount of actuating force creates more pressure on the mating surfaces.\nThe best known example of a cone clutch is a synchronizer ring in a manual transmission. The synchronizer ring is responsible for \"synchronizing\" the speeds of the shift hub and the gear wheel to ensure a smooth gear change.\n\nAlso known as a slip clutch or \"safety clutch\", this device allows a rotating shaft to slip when higher than normal resistance is encountered on a machine. An example of a safety clutch is the one mounted on the driving shaft of a large grass mower. The clutch yields if the blades hit a rock, stump, or other immobile object, thus avoiding a potentially damaging torque transfer to the engine, possibly twisting or fracturing the crankshaft.\n\nMotor-driven mechanical calculators had these between the drive motor and gear train, to limit damage when the mechanism jammed, as motors used in such calculators had high stall torque and were capable of causing damage to the mechanism if torque wasn't limited.\n\nCarefully designed clutches operate, but continue to transmit maximum permitted torque, in such tools as controlled-torque screwdrivers.\n\nSome clutches are not designed to slip; torque may only be transmitted either fully engaged or disengaged to avoid catastrophic damage. An example of this is the dog clutch, most commonly used in non-synchromesh transmissions.\n\nThere are multiple designs of vehicle clutch, but most are based on one or more friction discs pressed tightly together or against a flywheel using springs. The friction material varies in composition depending on many considerations such as whether the clutch is \"dry\" or \"wet\". Friction discs once contained asbestos, but this has been largely discontinued. Clutches found in heavy duty applications such as trucks and competition cars use ceramic plates that have a greatly increased friction coefficient. However, these have a \"grabby\" action generally considered unsuitable for passenger cars. The spring pressure is released when the clutch pedal is depressed thus either pushing or pulling the diaphragm of the pressure plate, depending on type. Raising the engine speed too high while engaging the clutch causes excessive clutch plate wear. Engaging the clutch abruptly when the engine is turning at high speed causes a harsh, jerky start. This kind of start is necessary and desirable in drag racing and other competitions, where speed is more important than comfort.\n\nIn a modern car with a manual transmission the clutch is operated by the left-most pedal using a hydraulic or cable connection from the pedal to the clutch mechanism. On older cars the clutch might be operated by a mechanical linkage. Even though the clutch may physically be located very close to the pedal, such remote means of actuation are necessary to eliminate the effect of vibrations and slight engine movement, engine mountings being flexible by design. With a rigid mechanical linkage, smooth engagement would be near-impossible because engine movement inevitably occurs as the drive is \"taken up.\"\n\nThe default state of the clutch is \"engaged\" - that is the connection between engine and gearbox is always \"on\" unless the driver presses the pedal and disengages it. If the engine is running with the clutch engaged and the transmission in neutral, the engine spins the input shaft of the transmission but power is not transmitted to the wheels.\n\nThe clutch is located between the engine and the gearbox, as disengaging it is usually required to change gear. Although the gearbox does not stop rotating during a gear change, there is no torque transmitted through it, thus less friction between gears and their engagement dogs. The output shaft of the gearbox is permanently connected to the final drive, then the wheels, and so both always rotate together, at a fixed speed ratio. With the clutch disengaged, the gearbox input shaft is free to change its speed as the internal ratio is changed. Any resulting difference in speed between the engine and gearbox is evened out as the clutch slips slightly during re-engagement.\n\nClutches in typical cars are mounted directly to the face of the engine's flywheel, as this already provides a convenient large diameter steel disk that can act as one driving plate of the clutch. Some racing clutches use small multi-plate disk packs that are not part of the flywheel. Both clutch and flywheel are enclosed in a conical bellhousing, which (in a rear-wheel drive car) usually forms the main mounting for the gearbox.\n\nA few cars, notably the Alfa Romeo Alfetta, Porsche 924, and Chevrolet Corvette (since 1997), sought a more even weight distribution between front and back by placing the weight of the transmission at the rear of the car, combined with the rear axle to form a transaxle. The clutch was mounted with the transaxle and so the propeller shaft rotated continuously with the engine, even when in neutral gear or declutched.\n\nMotorcycles typically employ a wet clutch with the clutch riding in the same oil as the transmission. These clutches are usually made up of a stack of alternating plain steel and friction plates. Some plates have lugs on their inner diameters that lock them to the engine crankshaft. Other plates have lugs on their outer diameters that lock them to a basket that turns the transmission input shaft. A set of coil springs or a diaphragm spring plate force the plates together when the clutch is engaged.\n\nOn motorcycles the clutch is operated by a hand lever on the left handlebar. No pressure on the lever means that the clutch plates are engaged (driving), while pulling the lever back towards the rider disengages the clutch plates through cable or hydraulic actuation, allowing the rider to shift gears or coast. Racing motorcycles often use slipper clutches to eliminate the effects of engine braking, which, being applied only to the rear wheel, can cause instability.\n\nCars use clutches in places other than the drive train. For example, a belt-driven engine cooling fan may have a heat-activated clutch. The driving and driven members are separated by a silicone-based fluid and a valve controlled by a bimetallic spring. When the temperature is low, the spring winds and closes the valve, which lets the fan spin at about 20% to 30% of the shaft speed. As the temperature of the spring rises, it unwinds and opens the valve, allowing fluid past the valve, makes the fan spin at about 60% to 90% of shaft speed. Other clutches—such as for an air conditioning compressor—electronically engage clutches using magnetic force to couple the driving member to the driven member.\n\n\nSingle-revolution clutches were developed in the 19th century to power machinery such as shears or presses where a single pull of the operating lever or (later) press of a button would trip the mechanism, engaging the clutch between the power source and the machine's crankshaft for exactly one revolution before disengaging the clutch. When the clutch is disengaged and the driven member is stationary. Early designs were typically dog clutches with a cam on the driven member used to disengage the dogs at the appropriate point.\n\nGreatly simplified single-revolution clutches were developed in the 20th century, requiring much smaller operating forces and in some variations, allowing for a fixed fraction of a revolution per operation. Fast action friction clutches replaced dog clutches in some applications, eliminating the problem of impact loading on the dogs every time the clutch engaged.\n\nIn addition to their use in heavy manufacturing equipment, single-revolution clutches were applied to numerous small machines. In tabulating machines, for example, pressing the operate key would trip a single revolution clutch to process the most recently entered number. In typesetting machines, pressing any key selected a particular character and also engaged a single rotation clutch to cycle the mechanism to typeset that character. Similarly, in teleprinters, the receipt of each character tripped a single-revolution clutch to operate one cycle of the print mechanism.\n\nIn 1928, Frederick G. Creed developed a single-turn spring clutch (see above) that was particularly well suited to the repetitive start-stop action required in teleprinters. In 1942, two employees of Pitney Bowes Postage Meter Company developed an improved single turn spring clutch. In these clutches, a coil spring is wrapped around the driven shaft and held in an expanded configuration by the trip lever. When tripped, the spring rapidly contracts around the power shaft engaging the clutch. At the end of one revolution, if the trip lever has been reset, it catches the end of the spring (or a pawl attached to it) and the angular momentum of the driven member releases the tension on the spring. These clutches have long operating lives, many have cycled for tens and perhaps hundreds of millions of cycles without need of maintenance other than occasional lubrication.\n\nThese superseded wrap-spring single-revolution clutches in page printers, such as teleprinters, including the Teletype Model 28 and its successors, using the same design principles. IBM Selectric typewriters also used them. These are typically disc-shaped assemblies mounted on the driven shaft. Inside the hollow disc-shaped drive drum are two or three freely floating pawls arranged so that when the clutch is tripped, the pawls spring outward much like the shoes in a drum brake. When engaged, the load torque on each pawl transfers to the others to keep them engaged. These clutches do not slip once locked up, and they engage very quickly, on the order of milliseconds. A trip projection extends out from the assembly. If the trip lever engaged this projection, the clutch was disengaged. When the trip lever releases this projection, internal springs and friction engage the clutch. The clutch then rotates one or more turns, stopping when the trip lever again engages the trip projection.\n\nThese mechanisms were found in some types of synchronous-motor-driven electric clocks. Many different types of synchronous clock motors were used, including the pre-World War II Hammond manual-start clocks. Some types of self-starting synchronous motors always started when power was applied, but in detail, their behaviour was chaotic and they were equally likely to start rotating in the wrong direction. Coupled to the rotor by one (or possibly two) stages of reduction gearing was a wrap-spring clutch-brake. The spring did not rotate. One end was fixed; the other was free. It rode freely but closely on the rotating member, part of the clock's gear train. The clutch-brake locked up when rotated backwards, but also had some spring action. The inertia of the rotor going backwards engaged the clutch and wound the spring. As it unwound, it restarted the motor in the correct direction. Some designs had no explicit spring as such—but were simply compliant mechanisms. The mechanism was lubricated and wear did not present a problem.\n\nA Lock-up clutch is used in some automatic transmissions for motor vehicles. Above a certain speed (usually 60 km/h) it locks the torque converter to minimise power loss and improve fuel efficiency.\n\n\n"}
{"id": "6520", "url": "https://en.wikipedia.org/wiki?curid=6520", "title": "Cow tipping", "text": "Cow tipping\n\nCow tipping is the purported activity of sneaking up on any unsuspecting or sleeping upright cow and pushing it over for entertainment. The practice of cow tipping is generally considered an urban legend, and stories of such feats viewed as tall tales. The implication that rural citizens seek such entertainment due to lack of other alternatives is viewed as a stereotype. The concept of cow tipping apparently developed in the 1970s, though tales of animals that cannot rise if they fall has historical antecedents dating to the Roman Empire.\n\nCows routinely lie down and can easily regain their footing unless sick or injured. Scientific studies have been conducted to determine if cow tipping is theoretically possible, with varying conclusions. All agree that cows are large animals that are difficult to surprise and will generally resist attempts to be tipped. Estimates suggest a force of between is needed, and that at least four and possibly as many as fourteen people would be required to achieve this. In real-life situations where cattle have to be laid on the ground, or \"cast\", such as for branding, hoof care or veterinary treatment, either rope restraints are required or specialized mechanical equipment is used that confines the cow and then tips it over. On rare occasions, cattle can lie down or fall down in proximity to a ditch or hill that restricts their normal ability to rise without help. Cow tipping has many references in popular culture and is also used as a figure of speech.\n\nThe urban legend of cow tipping relies upon the presumption that cattle are slow-moving, dim-witted, and weak-legged, thus easily pushed over without much force. Some versions suggest that because cows sleep standing up, it is possible to approach them and push them over without the animals reacting. However, cows only sleep lightly while standing up, and they are easily awakened. They lie down to sleep deeply. Furthermore, numerous sources have questioned the practice's feasibility, since most cows weigh over half a ton and easily resist any lesser force. \n\nA 2005 study led by Margo Lillie, a zoologist at the University of British Columbia, and her student Tracy Boechler, concluded that tipping a cow would require a force of nearly and is therefore impossible to accomplish by a single person. Her calculations found that it would require more than four people to apply enough force to push over a cow, based on an estimate that a single person could exert of force. However, since a cow can brace itself, Lillie and Boechler suggested that five or six people would, most likely, be needed. Further, cattle are well aware of their surroundings and are very difficult to surprise, due to excellent senses of both smell and hearing. Lillie and Boechler's analysis found that if a cow did not move, the principles of static physics suggest that two people might be able to tip a cow if its centre of mass were pushed over its hooves before the cow could react. However, cows are not rigid or unresponsive, and the faster humans have to move, the less force they can exert. Thus Lillie and Boechler concluded that it is unlikely that cows can actually be tipped over in this way. Lillie stated, \"It just makes the physics of it all, in my opinion, impossible.\"\n\nAlthough he agrees that it would take a force of about 3,000 newtons to push over a standing cow, biologist Steven Vogel thinks that the study by Lillie and Boechler overestimates the pushing ability of an individual human. Using data from Cotterell and Kamminga, who estimated that humans exert a pushing force of 280 newtons, Vogel suggests that someone applying force at the requisite height to topple a cow might generate a maximum push of no more than 300 newtons. By this calculation, at least 10 people would be needed to tip over a non-reacting cow. However, this combined force requirement, he says, might not be the greatest impediment to such a prank. Standing cows are not asleep and like other animals have ever-vigilant reflexes. \"If the cow does no more than modestly widen its stance without an overall shift of its center of gravity\", he says, \"about 4,000 newtons or 14 pushers would be needed—quite a challenge to deploy without angering the cow.\"\n\nThe belief that certain animals cannot rise if pushed over has historical antecedents, though cattle were not been so classified. Julius Caesar recorded a belief that a European elk (moose) had no knee joints and could not get up if it fell. Pliny said the same about the hind legs of an animal he called the achlis, which Pliny's 19th-century translators Bostock and Riley said was merely another name for the elk. They also noted that Pliny's belief about the jointless back legs of the achlis (elk) was false.\n\nIn 1255, Louis IX of France gave an elephant to Henry III of England for his menagerie in the Tower of London. A drawing by the historian Matthew Paris for his \"Chronica Majora\" can be seen in his bestiary at Parker Library of Corpus Christi College, Cambridge. An accompanying text cites elephant lore suggesting that elephants did not have knees and were unable to get up if they fell.\n\nJournalist Jake Steelhammer believes the American urban myth of cow tipping originated in the 1970s. It \"stampeded into the '80s\", he says, \"when movies like \"Tommy Boy\" and \"Heathers\" featured cow tipping expeditions.\" Stories about cow tipping tend to be second-hand, he says, told by someone who does not claim to have tipped a cow but who knows someone else who says he or she did.\n\nCattle may need to be deliberately thrown or tipped over for certain types of husbandry practices and medical treatment. When done for medical purposes, this is often called \"casting\", and when performed without mechanical assistance requires the attachment of of rope around the body and legs of the animal. After the rope is secured by non-slip bowline knots, it is pulled to the rear until the animal is off-balance. Once the cow is forced to lie down in sternal recumbency (on its chest), it can be rolled onto its side and its legs tied to prevent kicking.\n\nA calf table or calf cradle, also called a \"tipping table\" or a \"throw down\", is a relatively modern invention designed to be used on calves that are being branded. A calf is run into a chute, confined, and then tipped by the equipment onto its side for easier branding and castration.\n\nHydraulic tilt tables for adult cattle have existed since the 1970s and are designed to lift and tip cattle onto their sides to enable veterinary care, particularly of the animals' genitalia, and for hoof maintenance. (Unlike horses, cows generally do not cooperate with a farrier when standing.) A Canadian veterinarian explained, \"Using the table is much safer and easier than trying to get underneath to examine the animal\", and noted that cows tipped over on a padded table usually stop struggling and become calm fairly quickly. One design, developed at the Western College of Veterinary Medicine in Saskatoon, Saskatchewan, included \"cow comfort\" as a unique aspect of care using this type of apparatus.\n\nCows may tip themselves inadvertently. Due to their bulk and relatively short legs, cattle cannot roll over. Those that lie down and roll to their sides with their feet pointing uphill may become stuck and unable to rise without assistance, with potentially fatal results. In such cases, two humans can roll or flip a cow onto its other side, so that its feet are aimed downhill, thus allowing it to rise on its own.\nIn one documented case of \"real-life cow tipping\", a pregnant cow rolled into a gully in New Hampshire and became trapped in an inverted state until rescued by volunteer fire fighters. The owner of the cow commented that he had seen this happen \"once or twice\" before.\n\nTrauma or illness may also result in a cow unable to rise to its feet. Such animals are sometimes called \"downers.\" Sometimes this occurs as a result of muscle and nerve damage from calving or a disease such as mastitis. Leg injuries, muscle tears, or a massive infection of some sort may also be causes. Downer cows are encouraged to get to their feet and have a much greater chance of recovery if they do. If unable to rise, some have survived—with medical care—as long as 14 days and were ultimately able to get back on their feet. Appropriate medical treatment for a downer cow to prevent further injury includes rolling from one side to the other every three hours, careful and frequent feeding of small amounts of fodder, and access to clean water.\n\nDead animals may appear to have been tipped over. But this is actually the process of rigor mortis, which stiffens the muscles of the carcass, beginning six to eight hours after death and lasting for one to two days. It is particularly noticeable in the limbs, which stick out straight. Post-mortem bloat also occurs because of gas formation inside the body. The process may result in cattle carcasses that wind up on their back with all four feet in the air.\n\nAssorted individuals have claimed to have performed cow tipping, often while under the influence of alcohol. These claims, to date, cannot be reliably verified, with Jake Swearingen of \"Modern Farmer\" noting in 2013 that YouTube, a popular source of videos of challenges and stunts, \"fails to deliver one single actual cow-tipping video\".\n\nPranksters have sometimes pushed over artificial cows. Along Chicago's Michigan Avenue in 1999, two \"apparently drunk\" men felled six fiberglass cows that were part of a Cows on Parade public art exhibit. Four other vandals removed a \"Wow cow\" sculpture from its lifeguard chair at Oak Street Beach and abandoned it in a pedestrian underpass. A year later, New York City anchored its CowParade art cows, including \"A Streetcow Named Desire\", to concrete bases \"to prevent the udder disrespect of cow-tippers and thieves.\"\n\nCow tipping has been featured in films from the 1980s and later, such as \"Heathers\" (1988), \"Tommy Boy\" (1995), \" Barnyard\" (2006), and \"I Love You Beth Cooper\" (2009). It was also used in the title of a 1992 documentary film by Randy Redroad, \"Cow Tipping–The Militant Indian Waiter\". The 2006 Pixar film \"Cars\" features a vehicular variant called tractor-tipping.\n\nIn The Little Willies song \"Lou Reed\" from their 2006 eponymous debut album, Norah Jones sings about a fictional event during which musician Lou Reed tips cows in Texas. In another medium, \"The Big Bang Theory\", a television show, uses cow tipping lore as an element to establish the nature of a rural character, Penny.\n\nIn the United States and Canada, a form of vandalism involving Smart cars being flipped on their sides has been described. In 2009, the \"Toronto Star\" featured \"reports that vandals may be targeting the tiny vehicles in a 21st-century take on tipping cows\" in Amsterdam, Edmonton, and elsewhere; and a 2014 article in \"USA Today\" used the phrase \"\" to describe the act. The \"Star\" noted that at , the small cars weigh about the same as a mature cow.\n\nThe massively multiplayer online role-playing game \"Dragon's Tale\" provides cow tipping as a player activity that can earn cryptocurrency.\n\nThe term \"cow tipping\" is sometimes used as a figure of speech for pushing over something big. In \"A Giant Cow-Tipping by Savages\", author John Weir Close uses the term to describe contemporary mergers and acquisitions. \"Tipping sacred cows\" has been used as a deliberate mixed metaphor in titles of books on Christian ministry and business management.\n\n\n\n"}
{"id": "6526", "url": "https://en.wikipedia.org/wiki?curid=6526", "title": "Cassandra", "text": "Cassandra\n\nCassandra or Kassandra (Ancient Greek: Κασσάνδρα, , also ), also known as Alexandra, was a daughter of King Priam and of Queen Hecuba of Troy in Greek mythology. In modern usage her name is employed as a rhetorical device to indicate someone whose accurate prophecies are not believed by those around them.\n\nA common version of her story relates how, in an effort to seduce her, Apollo gave her the power of prophecy: when she refused him, he spat into her mouth to inflict a curse that nobody would ever believe her prophecies. In an alternative version, she fell asleep in a temple, and snakes licked (or whispered in) her ears so that she could hear the future.\n\nCassandra became a figure of epic tradition and of tragedy.\n\nHjalmar Frisk (\"Griechisches Etymologisches Wörterbuch\", Heidelberg, 1960–1970) notes \"unexplained etymology\", citing \"various hypotheses\" found in Wilhelm Schulze, Edgar Howard Sturtevant, J. Davreux, and . R. S. P. Beekes cites García Ramón's derivation of the name from the Proto-Indo-European root *\"(s)kend-\" \"raise\".\n\nCassandra was the daughter of King Priam (Priamos) and Queen Hecuba (Hekabe) and the fraternal twin sister of Helenus and a princess of Troy. According to legend, Cassandra had dark brown curly hair and dark brown eyes and was both beautiful and clever, but considered insane.\n\nCassandra's perceived insanity was the result of being cursed by the god Apollo. Many versions of the myth relate that she incurred the god's wrath by refusing him sex, sometimes after first promising herself in exchange for the power of prophecy. Hyginus says:\n\nIn another version, Cassandra consented to have sex with Apollo in exchange for the gift of prophecy, and then broke her promise. Her punishment was the curse of never being believed. This version of the myth is told by Cassandra in Aeschylus's \"Agamemnon\": \"Oh, but he struggled to win me, breathing ardent love for me...I consented to Loxias (Apollo) but broke my word...Ever since that fault I could persuade no one of anything.\"\n\nIn some versions of the myth, Apollo curses her by spitting into her mouth during a kiss. In Aeschylus' \"Agamemnon\", she foretells the betrayal of Clytemnestra. She also bemoans her relationship with Apollo:\n\n<poem>Apollo, Apollo!\nGod of all ways, but only Death's to me,\nOnce and again, O thou, Destroyer named,\nThou hast destroyed me, thou, my love of old!</poem>\n\nCassandra had served as a priestess of Apollo and taken a sacred vow of chastity to remain a virgin for her entire life.\n\nHer cursed gift from Apollo became a source of endless pain and frustration to Cassandra. Cassandra was seen as a liar and a madwoman by her family and by the Trojan people. In some versions of the story, she was often locked up in a pyramidal building on the citadel on her father King Priam’s orders. She was accompanied there by the wardress who cared for her under orders to inform the King of all of his daughter's \"prophetic utterances\". She was driven truly insane by this in the versions where she was incarcerated; though in the versions where she was not, she was usually viewed as being simply misunderstood.\n\nAccording to legend, Cassandra had instructed her twin brother Helenus in the power of prophecy so he could be a prophet. Like her, Helenus was always correct whenever he had made his predictions, but unlike his sister, people believed him.\n\nCassandra made many predictions, with all of her prophecies being disbelieved except for one. She was believed when she foresaw who Paris was and proclaimed that he was her abandoned brother. This took place after he had sought refuge in the altar of Zeus from their brothers’ wrath, which resulted in his reunion with their family. Cassandra foresaw that Paris’ abduction of Helen for his wife would bring about the Trojan War and cause the destruction of Troy. She did warn Paris not to go to Sparta along with Helenus who echoed her prophecy, but their warnings ended up being ignored. Cassandra saw Helen coming into Troy at Paris' return home from Sparta. She furiously snatched away Helen's golden veil and tore at her hair, for she had foreseen that Helen's arrival would bring the calamities of the Trojan War and the destruction of Troy. The Trojan people, however, welcomed Helen into their city.\n\nCassandra foresaw the destruction of Troy. In various accounts of the war, she warned the Trojans about the Greeks hiding inside the Trojan Horse, Agamemnon's death and her own demise at the hands of Aegisthus and Clytemnestra, her mother Hecuba's fate, Odysseus's ten year wanderings before returning to his home, and the murder of Aegisthus and Clytemnestra by her children Electra and Orestes. Cassandra predicted that her cousin Aeneas would escape during the fall of Troy and found a new nation in Rome. However, she was unable to do anything to forestall these tragedies since no one believed her.\n\nCoroebus and Othronus came to the aid of Troy during the Trojan War out of love for Cassandra in exchange for her hand in marriage. Priam decided to betroth Cassandra to Telephus’s son Eurypylus after Telephus had reinforced the Trojans by sending them an army of Mysians to come to defend Troy for them. Cassandra was also the first to see the body of her brother Hector being brought back to the city.\n\nIn \"The Fall of Troy\" told by Quintus Smyrnaeus, Cassandra had attempted to warn the Trojan people that she had foreseen the Greek warriors hiding in the Trojan Horse while they were celebrating their victory over the Greeks with feasting. They disbelieved her, calling her names and degrading her with insults. She grabbed an axe in one hand and a burning torch in her other, and ran towards the Trojan Horse, intent on destroying it herself to stop the Greeks from destroying Troy. The Trojan people stopped her before she could do so. The Greeks hiding inside the Trojan Horse were relieved that the Trojans had stopped Cassandra from destroying it, but they were surprised by how well she had known of their plan to defeat Troy.\n\nAt the fall of Troy, Cassandra sought shelter in the temple of Athena and there she embraced the wooden statue of Athena in supplication for her protection, where she was abducted and brutally raped by Ajax the Lesser. Cassandra was clinging so tightly to the statue of the goddess that Ajax knocked it over from its stand as he dragged her away. One account claimed that even Athena, who had worked hard to help the Greeks destroy Troy, was not able to restrain her tears and her cheeks burned with anger. In one account, this caused her image to give forth a sound that shook the floor of the temple at the sight of Cassandra's rape before her image turned its eyes away as Cassandra was violated, although others found this account too bold. Ajax's actions were a sacrilege because Cassandra was a supplicant of Athena and supplicants were untouchable in the sanctuary of a god, under the protection of that god. Furthermore, he committed another sacrilege by raping her inside the temple of Athena, despite it being strictly forbidden for people to have sexual intercourse in the temple of a god.\n\nOdysseus insisted to the other Greek leaders that Ajax should be stoned to death for his crimes, which had enraged Athena and the other gods. Ajax avoided their wrath, as none of them dared to punish him after he clung, as a suppliant, to Athena's altar and swore an oath proclaiming his innocence. Athena was furious at the Greeks' failure to punish Ajax for raping Cassandra in her temple, and she gravely punished them with the help of Poseidon and Zeus. Poseidon sent storms and strong winds for her to destroy much of the Greek fleet on their way home from Troy. She punished Ajax herself, by causing him to have a terrible death though the sources of his death differ. The Locrians had to atone for Ajax's great sacrilege against Cassandra in Athena's temple by sending two maidens to Troy every year for a thousand years to serve as slaves in Athena's temple—but if they were caught by the inhabitants before they reached the temple they were executed.\n\nIn some versions, Cassandra intentionally left a chest behind in Troy, with a curse on whichever Greek opened it first. Inside the chest was an image of Dionysus, made by Hephaestus and presented to the Trojans by Zeus. It was given to the Greek leader Eurypylus as a part of his share of the victory spoils of Troy. When he opened the chest and saw the image of the god, he went mad.\n\nCassandra was then taken as a concubine by King Agamemnon of Mycenae. Unbeknownst to Agamemnon, while he was away at war, his wife, Clytemnestra, had begun an affair with Aegisthus. Clytemnestra and Aegisthus then murdered both Agamemnon and Cassandra. Some sources mention that Cassandra and Agamemnon had twin boys, Teledamus and Pelops, both of whom were killed by Aegisthus.\n\nCassandra was sent to the Elysian Fields after her death, as her soul was judged worthy because of her dedication to the gods and her religious nature during her life.\n\nCassandra was buried either at Amyclae or Mycenae for the two towns disputed the possession of it. She had been buried most likely in Mycenae. Heinrich Schliemann was certain that he had discovered Cassandra’s tomb when he had excavated Mycenae since he had found the remains of a woman and two infants in one of the circle graves at Mycenae.\n\nFrom Aeschylus's trilogy \"Oresteia\", the play titled \"Agamemnon\" depicts the king, treading the scarlet cloth laid down for him, walking offstage to his sure death. After the chorus's ode of foreboding, time is suspended in Cassandra's \"mad scene\". She has been onstage, silent and ignored. Her madness that is unleashed now is not the physical torment of other characters in Greek tragedy, such as in Euripides' \"Heracles\" or Sophocles' \"Ajax\".\n\nAccording to author Seth Schein, two further familiar descriptions of her madness are that of Heracles in \"The Women of Trachis\" or Io in \"Prometheus Bound\". She speaks, disconnectedly and transcendent, in the grip of her psychic possession by Apollo, witnessing past and future events. Schein says, \"She evokes the same awe, horror and pity as do schizophrenics\". Cassandra is someone \"who often combine deep, true insight with utter helplessness, and who retreat into madness.\"\n\nEduard Fraenkel remarked on the powerful contrasts between declaimed and sung dialogue in this scene. The frightened and respectful chorus are unable to comprehend her. She goes to her inevitable offstage murder by Clytemnestra with full knowledge of what is to befall her.\n\nCassandra is an enduring archetype. Modern invocations of Cassandra are most frequently an example of a Cassandra complex. To emphasize such a situation, Cassandra's name is frequently used in fiction when prophecy comes up, especially true prophecy that is not believed. This can include the names of people, objects, or places.\n\nCassandra has been used as metaphor and allegory in psychological and philosophical tracts. For example, Florence Nightingale's book \"Suggestions for Thought to Searchers after Religious Truth\" has a section named for Cassandra, using her as a metaphor for the helplessness of women that she attributes to over-feminization. (Further examples are located on the Cassandra complex page.)\n\nThe Cassandra myth itself has also been retold several times by modern authors of novels and dramatizations, including works by Eric Shanower, Lindsay Clarke, Christa Wolf, Lesya Ukrainka, Woody Allen, Marion Zimmer Bradley, David Gemmell, and Hector Berlioz. A number of songs have also referred to her, such as \"Cassandra\" by Swedish pop band ABBA.\n\n\n\n"}
{"id": "6530", "url": "https://en.wikipedia.org/wiki?curid=6530", "title": "Couplet", "text": "Couplet\n\nA couplet is a pair of lines of metre in poetry. A couplet usually consists of two lines that rhyme and have the same metre. A couplet may be formal (closed) or run-on (open). In a formal (or closed) couplet, each of the two lines is end-stopped, implying that there is a grammatical pause at the end of a line of verse. In a run-on (or open) couplet, the meaning of the first line continues to the second.\n\nThe word \"couplet\" comes from the French word meaning \"two pieces of iron riveted or hinged together.\" The term \"couplet\" was first used to describe successive lines of verse in Sir P. Sidney's \" Arcadia \" in 1590: \"In singing some short coplets, whereto the one halfe beginning, the other halfe should answere.\" \n\nWhile couplets traditionally rhyme, not all do. Poems may use white space to mark out couplets if they do not rhyme. Couplets in iambic pentameter are called \"heroic couplets\". John Dryden in the 16–17th century and Alexander Pope in the 18th century were both well known for their writing in heroic couplets. The Poetic epigram is also in the couplet form. Couplets can also appear as part of more complex rhyme schemes, such as sonnets.\n\nRhyming couplets are one of the simplest rhyme schemes in poetry. Because the rhyme comes so quickly, it tends to call attention to itself. Good rhyming couplets tend to \"explode\" as both the rhyme and the idea come to a quick close in two lines. Here are some examples of rhyming couplets where the sense as well as the sound \"rhymes\":\n\nOn the other hand, because rhyming couplets have such a predictable rhyme scheme, they can feel artificial and plodding. Here is a Pope parody of the predictable rhymes of his era:\n\nRhyming couplets are often used in Early Modern English poetry, as seen in Chaucer's \"The Canterbury Tales\". This work of literature is written almost entirely in rhyming couplets. Similarly, Shakespearean sonnets often employ rhyming couplets at the end to emphasize the theme. Take one of Shakespeare's most famous sonnets, Sonnet 18, for example (the rhyming couplet is shown in italics):\n\nChinese couplets or \"contrapuntal couplets\" may be seen on doorways in Chinese communities worldwide. Couplets displayed as part of the Chinese New Year festival, on the first morning of the New Year, are called \"chunlian\". These are usually purchased at a market a few days before and glued to the doorframe. The text of the couplets is often traditional and contains hopes for prosperity. Other chunlian reflect more recent concerns. For example, the CCTV New Year's Gala usually promotes couplets reflecting current political themes in mainland China.\n\nSome Chinese couplets may consist of two lines of four characters each. Couplets are read from top to bottom where the first pline starts from the right.\n\nJ.V.Cunningham the American poet was noted for many distich included in the various forms of epigrams included in his poetry collections as exampled here:-\n\n<br> Deep summer, and time passes.Sorrow wastes<br>To a new sorrow.While Time heals time hastes \n\n"}
{"id": "6532", "url": "https://en.wikipedia.org/wiki?curid=6532", "title": "Charlotte Brontë", "text": "Charlotte Brontë\n\nCharlotte Brontë (, \"commonly\" ; 21 April 1816 – 31 March 1855) was an English novelist and poet, the eldest of the three Brontë sisters who survived into adulthood and whose novels have become classics of English literature. She first published her works (including her best known novel, \"Jane Eyre\") under the pen name Currer Bell.\n\nCharlotte Brontë was born in Thornton, west of Bradford in the West Riding of Yorkshire, in 1816, the third of the six children of Maria (née Branwell) and Patrick Brontë (formerly surnamed Brunty or Prunty), an Irish Anglican clergyman. In 1820 her family moved a few miles to the village of Haworth, where her father had been appointed perpetual curate of St Michael and All Angels Church. Maria died of cancer on 15 September 1821, leaving five daughters, Maria, Elizabeth, Charlotte, Emily and Anne, and a son, Branwell, to be taken care of by her sister, Elizabeth Branwell.\n\nIn August 1824 Patrick sent Charlotte, Emily, Maria and Elizabeth to the Clergy Daughters' School at Cowan Bridge in Lancashire. Charlotte maintained that the school's poor conditions permanently affected her health and physical development, and hastened the deaths of Maria (born 1814) and Elizabeth (born 1815), who both died of tuberculosis in June 1825. After the deaths of his older daughters, Patrick removed Charlotte and Emily from the school. Charlotte used the school as the basis for Lowood School in \"Jane Eyre\".\n\nAt home in Haworth Parsonage, Brontë acted as \"the motherly friend and guardian of her younger sisters\". Brontë wrote her first known poem at the age of 13 in 1829, and was to go on to write more than 200 poems in the course of her life. Many of her poems were \"published\" in their homemade magazine \"Branwell's Blackwood's Magazine\", and concerned the fictional Glass Town Confederacy. She and her surviving siblings — Branwell, Emily and Anne – created their own fictional worlds, and began chronicling the lives and struggles of the inhabitants of their imaginary kingdoms. Charlotte and Branwell wrote Byronic stories about their jointly imagined country, Angria, and Emily and Anne wrote articles and poems about Gondal. The sagas they created were episodic and elaborate, and they exist in incomplete manuscripts, some of which have been published as juvenilia. They provided them with an obsessive interest during childhood and early adolescence, which prepared them for literary vocations in adulthood.\n\nBetween 1831 and 1832, Brontë continued her education at Roe Head in Mirfield, where she met her lifelong friends and correspondents Ellen Nussey and Mary Taylor. In 1833 she wrote a novella, \"The Green Dwarf\", using the name Wellesley. Around about 1833, her stories shifted from tales of the supernatural to more realistic stories. She returned to Roe Head as a teacher from 1835 to 1838. Unhappy and lonely as a teacher at Roe Head, Brontë took out her sorrows in poetry, writing a series of melancholic poems. In \"We wove a Web in Childhood\" written in December 1835, Brontë drew a sharp contrast between her miserable life as a teacher vs. the vivid imaginary worlds she and her siblings had created. In another poem \"Morning was its freshness still\" written at the same time, Brontë wrote \"Tis bitter sometimes to recall/Illusions once deemed fair\". Many of her poems concerned the imaginary world of Angria, often concerning Byronic heroes, and in December 1836 she wrote to the Poet Laureate Robert Southey asking him for encouragement of her career as a poet. Sothey wrote back to say she was a bad poet and to consider another career, a letter that greatly hurt her. One scholar Dawn Potter wrote that Brontë had a streak of sadism in her novels with her characters always suffering in some way, which she suggested was due to her own unhappy life.\n\nIn 1839 she took up the first of many positions as governess to families in Yorkshire, a career she pursued until 1841. In particular, from May to July 1839 she was employed by the Sidgwick family at their summer residence, Stone Gappe, in Lothersdale, where one of her charges was John Benson Sidgwick (1835–1927), an unruly child who on one occasion threw a Bible at Charlotte, an incident that may have been the inspiration for a part of the opening chapter of \"Jane Eyre\" in which John Reed throws a book at the young Jane. Brontë did not enjoy her work as a governess, noting her employers treated her almost as a slave, constantly humiliating her.\n\nBrontë was of slight build and was less than five feet tall.\n\nIn 1842 Charlotte and Emily travelled to Brussels to enrol at the boarding school run by Constantin Héger (1809–96) and his wife Claire Zoé Parent Héger (1804–87). During her time in Brussels, Brontë who favoured the Protestant ideal of an individual in direct contact with God, objected to the stern Catholicism of Madame Héger, which she considered to be a tyrannical religion that enforced conformity and submission to the Pope. In return for board and tuition Charlotte taught English and Emily taught music. Their time at the school was cut short when their aunt Elizabeth Branwell, who had joined the family in Haworth to look after the children after their mother's death, died of internal obstruction in October 1842. Charlotte returned alone to Brussels in January 1843 to take up a teaching post at the school. Her second stay was not happy: she was homesick and deeply attached to Constantin Héger. She returned to Haworth in January 1844 and used the time spent in Brussels as the inspiration for some of the events in \"The Professor\" and \"Villette\".\n\nIn May 1846 Charlotte, Emily and Anne self-financed the publication of a joint collection of poems under their assumed names Currer, Ellis and Acton Bell. The pseudonyms veiled the sisters' sex while preserving their initials; thus Charlotte was Currer Bell. \"Bell\" was the middle name of Haworth's curate, Arthur Bell Nicholls whom Charlotte later married, and \"Currer\" was the surname of Frances Mary Richardson Currer who had funded their school (and maybe their father). Of the decision to use \"noms de plume\", Charlotte wrote:\n\nAlthough only two copies of the collection of poems were sold, the sisters continued writing for publication and began their first novels, continuing to use their \"noms de plume\" when sending manuscripts to potential publishers.\n\nBrontë's first manuscript, \"The Professor\", did not secure a publisher, although she was heartened by an encouraging response from Smith, Elder & Co. of Cornhill, who expressed an interest in any longer works Currer Bell might wish to send. Brontë responded by finishing and sending a second manuscript in August 1847. Six weeks later \"Jane Eyre: An Autobiography\" was published. It tells the story of a plain governess, Jane, who, after difficulties in her early life, falls in love with her employer, Mr Rochester. They marry, but only after Rochester's insane first wife, of whom Jane initially has no knowledge, dies in a dramatic house fire. The book's style was innovative, combining naturalism with gothic melodrama, and broke new ground in being written from an intensely evoked first-person female perspective. Brontë believed art was most convincing when based on personal experience; in \"Jane Eyre\" she transformed the experience into a novel with universal appeal.\n\n\"Jane Eyre\" had immediate commercial success and initially received favourable reviews. G. H. Lewes wrote that it was \"an utterance from the depths of a struggling, suffering, much-enduring spirit,\" and declared that it consisted of \"\"suspiria de profundis\"!\" (sighs from the depths). Speculation about the identity and gender of the mysterious Currer Bell heightened with the publication of \"Wuthering Heights\" by Ellis Bell (Emily) and \"Agnes Grey\" by Acton Bell (Anne). Accompanying the speculation was a change in the critical reaction to Brontë's work, as accusations were made that the writing was \"coarse\", a judgement more readily made once it was suspected that Currer Bell was a woman. However, sales of \"Jane Eyre\" continued to be strong and may even have increased as a result of the novel developing a reputation as an \"improper\" book. A talented amateur artist, Brontë personally did the drawings for the second edition of \"Jane Eyre\" and in the summer of 1834 two of her paintings were shown at an exhibition by the Royal Northern Society for the Encouragement of the Fine Arts in Leeds.\n\nIn 1848 Brontë began work on the manuscript of her second novel, \"Shirley\". It was only partially completed when the Brontë family suffered the deaths of three of its members within eight months. In September 1848 Branwell died of chronic bronchitis and marasmus, exacerbated by heavy drinking, although Brontë believed that his death was due to tuberculosis. Branwell was also a suspected \"opium eater\"; a laudanum addict. Emily became seriously ill shortly after Branwell's funeral and died of pulmonary tuberculosis in December 1848. Anne died of the same disease in May 1849. Brontë was unable to write at this time.\n\nAfter Anne's death Brontë resumed writing as a way of dealing with her grief, and \"Shirley\", which deals with themes of industrial unrest and the role of women in society, was published in October 1849. Unlike \"Jane Eyre\", which is written in the first person, \"Shirley\" is written in the third person and lacks the emotional immediacy of her first novel, and reviewers found it less shocking. Brontë, as her late sister's heir, suppressed the republication of Anne's second novel, \"The Tenant of Wildfell Hall\", an action which had a deleterious effect on Anne's popularity as a novelist and has remained controversial among the sisters' biographers ever since.\nIn view of the success of her novels, particularly \"Jane Eyre\", Brontë was persuaded by her publisher to make occasional visits to London, where she revealed her true identity and began to move in more exalted social circles, becoming friends with Harriet Martineau and Elizabeth Gaskell, and acquainted with William Makepeace Thackeray and G.H. Lewes. She never left Haworth for more than a few weeks at a time, as she did not want to leave her ageing father. Thackeray’s daughter, writer Anne Isabella Thackeray Ritchie, recalled a visit to her father by Brontë:\n\nBrontë's friendship with Elizabeth Gaskell, while not particularly close, was significant in that Gaskell wrote the first biography of Brontë after her death in 1855.\n\nBrontë's third novel, the last published in her lifetime, was \"Villette\", which appeared in 1853. Its main themes include isolation, how such a condition can be borne, and the internal conflict brought about by social repression of individual desire. Its main character, Lucy Snowe, travels abroad to teach in a boarding school in the fictional town of Villette, where she encounters a culture and religion different from her own, and falls in love with a man (Paul Emanuel) whom she cannot marry. Her experiences result in a breakdown but eventually she achieves independence and fulfilment through running her own school. A substantial amount of the novel's dialogue is in the French language. \"Villette\" marked Brontë's return to writing from a first-person perspective (that of Lucy Snowe); the technique she had used in \"Jane Eyre\". Another similarity to \"Jane Eyre\" lies in the use of aspects of her own life as inspiration for fictional events; in particular her reworking of the time she spent at the \"pensionnat\" in Brussels. \"Villette\" was acknowledged by critics of the day as a potent and sophisticated piece of writing although it was criticised for \"coarseness\" and for not being suitably \"feminine\" in its portrayal of Lucy's desires.\n\nBefore the publication of \"Villette\" Brontë received a proposal of marriage from Arthur Bell Nicholls, her father's curate, who had long been in love with her. She initially turned down his proposal and her father objected to the union at least partly because of Nicholls's poor financial status. Elizabeth Gaskell, who believed that marriage provided \"clear and defined duties\" that were beneficial for a woman, encouraged Brontë to consider the positive aspects of such a union and tried to use her contacts to engineer an improvement in Nicholls's finances. Brontë meanwhile was increasingly attracted to Nicholls and by January 1854 she had accepted his proposal. They gained the approval of her father by April and married in June. Her father Patrick had intended to give Charlotte away, but at the last minute decided he could not, and Charlotte had to make her way to the church without him. The married couple took their honeymoon in Banagher, County Offaly, Ireland. By all accounts, her marriage was a success and Brontë found herself very happy in a way that was new to her.\n\nBrontë became pregnant soon after her wedding, but her health declined rapidly and, according to Gaskell, she was attacked by \"sensations of perpetual nausea and ever-recurring faintness.\" She died, with her unborn child, on 31 March 1855, aged 38, three weeks before her 39th birthday. Her death certificate gives the cause of death as tuberculosis, but biographers including Claire Harman suggest that she died from dehydration and malnourishment due to vomiting caused by severe morning sickness or hyperemesis gravidarum. There is also evidence that she died from typhus, which she may have caught from Tabitha Ackroyd, the Brontë household's oldest servant, who died shortly before her. Brontë was interred in the family vault in the Church of St Michael and All Angels at Haworth.\n\n\"The Professor\", the first novel Brontë had written, was published posthumously in 1857. The fragment of a new novel she had been writing in her last years has been twice completed by recent authors, the more famous version being \"Emma Brown: A Novel from the Unfinished Manuscript by Charlotte Brontë\" by Clare Boylan in 2003. Most of her writings about the imaginary country Angria have also been published since her death.\n\nElizabeth Gaskell's biography \"The Life of Charlotte Brontë\" was published in 1857. It was an important step for a leading female novelist to write a biography of another, and Gaskell's approach was unusual in that, rather than analysing her subject's achievements, she concentrated on private details of Brontë's life, emphasising those aspects that countered the accusations of \"coarseness\" that had been levelled at her writing. The biography is frank in places, but omits details of Brontë's love for Héger, a married man, as being too much of an affront to contemporary morals and a likely source of distress to Brontë's father, widower, and friends. Mrs Gaskell also provided doubtful and inaccurate information about Patrick Brontë, claiming that he did not allow his children to eat meat. This is refuted by one of Emily Brontë's diary papers, in which she describes preparing meat and potatoes for dinner at the parsonage. It has been argued that Gaskell's approach transferred the focus of attention away from the 'difficult' novels, not just Brontë's, but all the sisters', and began a process of sanctification of their private lives.\n\nOn 29 July 1913 \"The Times\" of London printed four letters Brontë had written to Constantin Héger after leaving Brussels in 1844. Written in French except for one postscript in English, the letters broke the prevailing image of Brontë as an angelic martyr to Christian and female duties that had been constructed by many biographers, beginning with Gaskell. The letters, which formed part of a larger and somewhat one-sided correspondence in which Héger frequently appears not to have replied, reveal that she had been in love with a married man, although they are complex and have been interpreted in numerous ways, including as an example of literary self-dramatisation and an expression of gratitude from a former pupil.\n\n\n\"The Green Dwarf, A Tale of the Perfect Tense\" was written in 1833 under the pseudonym Lord Charles Albert Florian Wellesley. It shows the influence of Walter Scott, and Brontë's modifications to her earlier gothic style have led Christine Alexander to comment that, in the work, \"it is clear that Brontë was becoming tired of the gothic mode \"per se\"\".\n\n\n\n\n\n"}
{"id": "6533", "url": "https://en.wikipedia.org/wiki?curid=6533", "title": "Charles Williams (British writer)", "text": "Charles Williams (British writer)\n\nCharles Walter Stansby Williams (20 September 1886 – 15 May 1945) was a British poet, novelist, playwright, theologian, literary critic, and member of the Inklings.\n\nWilliams was born in London in 1886, the only son of (Richard) Walter Stansby Williams (1848–1929), a journalist and foreign business correspondent for an importing firm, writing in French and German, who was a 'regular and valued' contributor of verse, stories and articles to many popular magazines, and his wife Mary (née Wall, the sister of the ecclesiologist and historian J. Charles Wall), a former milliner, of Islington. He had one sister, Edith, born in 1889. The Williams family lived in 'shabby-genteel' circumstances, owing to Walter's increasing blindness and the decline of the firm by which he was employed, in Holloway.\n\nEducated at St Albans School, Hertfordshire, Williams was awarded a scholarship to University College London, but he left school in 1904 without attempting to gain a degree due to an inability to pay tuition fees.\n\nNo longer in college, Williams began work in 1904 in a Methodist bookroom. He was hired by the Oxford University Press (OUP) as a proofreading assistant in 1908 and quickly climbed to the position of editor. He continued to work at the OUP in various positions of increasing responsibility until his death in 1945. One of his greatest editorial achievements was the publication of the first major English-language edition of the works of Søren Kierkegaard.\n\nAlthough chiefly remembered as a novelist, Williams also published poetry, works of literary criticism, theology, drama, history, biography, and a voluminous number of book reviews. Some of his best known novels are \"War in Heaven\" (1930), \"Descent into Hell\" (1937), and \"All Hallows' Eve\" (1945). T. S. Eliot, who wrote an introduction for the last of these, described Williams's novels as \"supernatural thrillers\" because they explore the sacramental intersection of the physical with the spiritual while also examining the ways in which power, even spiritual power, can corrupt as well as sanctify. All of Williams's fantasies, unlike those of J. R. R. Tolkien and most of those of C. S. Lewis, are set in the contemporary world. Williams has been described by Colin Manlove as one of the three main writers of \"Christian fantasy\" in the twentieth century (the other two being C.S. Lewis and T. F. Powys). More recent writers of fantasy novels with contemporary settings, notably Tim Powers, cite Williams as a model and inspiration. W. H. Auden, one of Williams's greatest admirers, reportedly re-read Williams's extraordinary and highly unconventional history of the church, \"The Descent of the Dove\" (1939), every year. Williams's study of Dante entitled \"The Figure of Beatrice\" (1944) was very highly regarded at its time of publication and continues to be consulted by Dante scholars today. His work inspired Dorothy L. Sayers to undertake her translation of \"The Divine Comedy\". Williams, however, regarded his most important work to be his extremely dense and complex Arthurian poetry, of which two books were published, \"Taliessin through Logres\" (1938) and \"The Region of the Summer Stars\" (1944), and more remained unfinished at his death. Some of Williams' essays were collected and published posthumously in \"Image of the City and Other Essays\" (1958), edited by Anne Ridler.\n\nWilliams gathered many followers and disciples during his lifetime. He was, for a period, a member of the Salvator Mundi Temple of the Fellowship of the Rosy Cross. He met fellow Anglican Evelyn Underhill (who was affiliated with a similar group, the Order of the Golden Dawn) in 1937 and was later to write the introduction to her published \"Letters\" in 1943.\n\nWhen World War II broke out in 1939, Oxford University Press moved its offices from London to Oxford. Williams was reluctant to leave his beloved city, and Florence refused to go. From the nearly 700 letters he wrote his wife during the war years a generous selection has been published; \"primarily… love letters,\" the editor calls them. But the move to Oxford did allow him to participate regularly in Lewis’s literary society known as the Inklings. In this setting Williams was able to read (and improve) his final published novel, \"All Hallows' Eve\", as well as to hear J. R. R. Tolkien read aloud to the group some of his early drafts of \"The Lord of the Rings\". In addition to meeting in Lewis's rooms at Oxford, they also regularly met at The Eagle and Child pub in Oxford (better known by its nickname \"The Bird and Baby\"). During this time Williams also gave lectures at Oxford on John Milton, William Wordsworth, and other authors, and received an honorary M.A. degree. Williams is buried in Holywell Cemetery in Oxford: his headstone bears the word \"poet\", followed by the words \"Under the Mercy\", a phrase often used by Williams himself.\n\nIn 1917 Williams married his first sweetheart, Florence Conway, following a long courtship during which he presented her with a sonnet sequence that would later become his first published book of poetry, \"The Silver Stair\". Their son Michael was born in 1922.\n\nWilliams was an unswerving and devoted member of the Church of England, reputedly with a tolerance of the scepticism of others and a firm belief in the necessity of a \"doubting Thomas\" in any apostolic body).\n\nAlthough Williams attracted the attention and admiration of some of the most notable writers of his day, including T. S. Eliot and W. H. Auden, his greatest admirer was probably C. S. Lewis, whose novel \"That Hideous Strength\" (1945) has been regarded as partially inspired by his acquaintance with both the man and his novels and poems. Williams came to know Lewis after reading Lewis's then-recently published study \"The Allegory of Love\"; he was so impressed he jotted down a letter of congratulation and dropped it in the mail. Coincidentally, Lewis had just finished reading Williams's novel \"The Place of the Lion\" and had written a similar note of congratulation. The letters crossed in the mail and led to an enduring and fruitful friendship.\n\nWilliams developed both the concept of co-inherence, and also gave rare consideration to the theology of romantic love. Falling in love for Williams was a form of mystical envisioning in which one saw the beloved as he or she was seen through the eyes of God. Co-inherence was a term used in Patristic theology to describe the relationship between the human and divine natures of Jesus Christ, and the relationship between the persons of the blessed Trinity. Williams extended the term to include the ideal relationship between the individual parts of God's creation, including human beings. It is our mutual indwelling: Christ in us and we in Christ, interdependent. It is also the web of interrelationships, social and economic and ecological, by which the social fabric and the natural world function. But especially for Williams, co-inherence is a way of talking about the Body of Christ and the communion of saints. For Williams, salvation was not a solitary affair: \"The thread of the love of God was strong enough to save you and all the others, but not strong enough to save you alone.\" He proposed an order, the Companions of the Co-inherence, who would practice substitution and exchange, living in love-in-God, truly bearing one another's burdens, being willing to sacrifice and to forgive, living from and for one another in Christ. According to Gunnar Urang, co-inherence is the focus of all Williams's novels.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "6535", "url": "https://en.wikipedia.org/wiki?curid=6535", "title": "Celery", "text": "Celery\n\nCelery (\"Apium graveolens\") is a marshland plant in the family Apiaceae that has been cultivated as a vegetable since antiquity. Celery has a long fibrous stalk tapering into leaves. Depending on location and cultivar, either its stalks, leaves, or hypocotyl are eaten and used in cooking.\n\nCelery seed is also used as a spice; its extracts are used in medicines.\n\nCelery leaves are pinnate to bipinnate with rhombic leaflets long and 2–4 cm broad. The flowers are creamy-white, 2–3 mm in diameter, and are produced in dense compound umbels. The seeds are broad ovoid to globose, 1.5–2 mm long and wide. Modern cultivars have been selected for solid petioles, leaf stalks. A celery stalk readily separates into \"strings\" which are bundles of angular collenchyma cells exterior to the vascular bundles.\n\nWild celery, \"Apium graveolens\" var. \"graveolens\", grows to tall.\nIt occurs around the globe. The first cultivation is thought to have happened in the Mediterranean region, where the natural habitats were salty and wet, or marshy soils near the coast where celery grew in agropyro-rumicion-plant communities.\n\nNorth of the alps wild celery is found only in the foothill zone on soils with some salt content. It prefers moist or wet, nutrient rich, muddy soils. It cannot be found in Austria and is increasingly rare in Germany.\n\nFirst attested in English in 1664, the word \"celery\" derives from the French \"céleri\", in turn from Italian \"seleri\", the plural of \"selero\", which comes from Late Latin \"selinon\", the latinisation of the Greek σέλινον (\"selinon\"), \"celery\". The earliest attested form of the word is the Mycenaean Greek \"se-ri-no\", written in Linear B syllabic script.\n\nCelery was described by Carl Linnaeus in Volume One of his \"Species Plantarum\" in 1753.\n\nThe plants are raised from seed, sown either in a hot bed or in the open garden according to the season of the year, and, after one or two thinnings and transplantings, they are, on attaining a height of , planted out in deep trenches for convenience of blanching, which is effected by earthing up to exclude light from the stems.\n\nIn the past, celery was grown as a vegetable for winter and early spring; it was perceived as a cleansing tonic, welcomed to counter the salt-sickness of a winter diet without greens based on salted meats. By the 19th century, the season for celery had been extended, to last from the beginning of September to late in April.\n\nIn North America, commercial production of celery is dominated by the cultivar called 'Pascal' celery. Gardeners can grow a range of cultivars, many of which differ from the wild species, mainly in having stouter leaf stems. They are ranged under two classes, white and red.\nThe stalks grow in tight, straight, parallel bunches, and are typically marketed fresh that way, without roots and just a little green leaf remaining.\n\nThe stalks are eaten raw, or as an ingredient in salads, or as a flavoring in soups, stews, and pot roasts.\n\nIn Europe, another popular variety is celeriac (also known as \"celery root\"), \"Apium graveolens\" var. \"rapaceum\", grown because its hypocotyl forms a large bulb, white on the inside. The bulb could be kept for months in winter and mostly serves as a main ingredient in soup. It can also be ground up and used in salads.\nThe leaves are used as seasoning; the small, fibrous stalks find only marginal use.\n\nLeaf celery or Chinese celery, \"Apium graveolens var. secalinum\", is a cultivar from East Asia that grows in marshlands. Leaf celery is most likely the oldest cultivated form of celery. Leaf celery has characteristically thin skin stalks and a stronger taste and smell compared to other cultivars. It is used as a flavoring in soups and sometimes pickled as a side dish.\n\nThe wild form of celery is known as \"smallage\". It has a furrowed stalk with wedge-shaped leaves, the whole plant having a coarse, earthy taste, and a distinctive smell. The stalks are not usually eaten (except in soups or stews in French cuisine), but the leaves may be used in salads, and its seeds are those sold as a spice. With cultivation and blanching, the stalks lose their acidic qualities and assume the mild, sweetish, aromatic taste particular to celery as a salad plant.\n\nBecause wild celery is rarely eaten, yet susceptible to the same diseases as more well-used cultivars, it is often removed from fields to help prevent transmission of viruses like celery mosaic virus.\n\nHarvesting occurs when the average size of celery in a field is marketable; due to extremely uniform crop growth, fields are harvested only once. The petioles and leaves are removed and harvested; celery is packed by size and quality (determined by colour, shape, straightness and thickness of petiole, stalk and midrib length and absence of disease, cracks, splits, insect damage and rot). During commercial harvesting, celery is packaged into cartons which contain between 36 and 48 stalks and weigh up to 60 pounds. Under optimal conditions, celery can be stored for up to seven weeks between . Inner stalks may continue growing if kept at temperatures above . Shelf life can be extended by packaging celery in anti-fogging, micro-perforated shrink wrap. Freshly cut petioles of celery are prone to decay, which can be prevented or reduced through the use of sharp blades during processing, gentle handling, and proper sanitation.\n\nIn the past, restaurants used to store celery in a container of water with powdered vegetable preservative, but it was found that the sulfites in the preservative caused allergic reactions in some people. In 1986, the U.S. Food and Drug Administration banned the use of sulfites on fruits and vegetables intended to be eaten raw.\n\nCelery is eaten around the world as a vegetable. In North America the crisp petiole (leaf stalk) is used. In Europe the hypocotyl is used as a root vegetable. The leaves are strongly flavoured and are used less often, either as a flavouring in soups and stews or as a dried herb. Celery, onions, and bell peppers are the \"holy trinity\" of Louisiana Creole and Cajun cuisine. Celery, onions, and carrots make up the French mirepoix, often used as a base for sauces and soups. Celery is a staple in many soups, such as chicken noodle soup.\n\nIn temperate countries, celery is also grown for its seeds. Actually very small fruit, these \"seeds\" yield a valuable volatile oil used in the perfume and pharmaceutical industries. They contain an organic compound called apiole. Celery seeds can be used as flavouring or spice, either as whole seeds or ground.\n\nThe seeds can be ground and mixed with salt, to produce celery salt. Celery salt can be made from an extract of the roots or using dried leaves. Celery salt is used as a seasoning, in cocktails (notably to enhance the flavour of Bloody Mary cocktails), on the Chicago-style hot dog, and in Old Bay Seasoning.\n\nAulus Cornelius Celsus wrote that celery seeds could relieve pain in around AD 30.\n\nCelery is used in weight-loss diets, where it provides low-calorie dietary fibre bulk. Celery is often incorrectly thought to be a \"negative-calorie food,\" the digestion of which burns more calories than the body can obtain. In fact, eating celery provides positive net calories, with digestion consuming only a small proportion of the calories taken in.\n\nCelery is among a small group of foods (headed by peanuts) that appear to provoke the most severe allergic reactions; for people with celery allergy, exposure can cause potentially fatal anaphylactic shock. The allergen does not appear to be destroyed at cooking temperatures. Celery root—commonly eaten as celeriac, or put into drinks—is known to contain more allergen than the stalk. Seeds contain the highest levels of allergen content. Exercise-induced anaphylaxis may be exacerbated. An allergic reaction also may be triggered by eating foods that have been processed with machines that have previously processed celery, making avoiding such foods difficult. In contrast with peanut allergy being most prevalent in the US, celery allergy is most prevalent in Central Europe. In the European Union, foods that contain or may contain celery, even in trace amounts, must be clearly marked as such.\n\nPolyynes can be found in Apiaceae vegetables like celery, and their extracts show cytotoxic activities.\nCelery contains phenolic acid, which is an antioxidant.\n\nApiin and apigenin can be extracted from celery and parsley. Lunularin is a dihydrostilbenoid found in common celery.\n\nThe main chemicals responsible for the aroma and taste of celery are butylphthalide and sedanolide.\n\nDaniel Zohary and Maria Hopf note that celery leaves and inflorescences were part of the garlands found in the tomb of pharaoh Tutankhamun (died 1323 BC), and celery mericarps dated to the seventh century BC were recovered in the Heraion of Samos. However, they note \"since \"A. graveolens\" grows wild in these areas, it is hard to decide whether these remains represent wild or cultivated forms.\" Only by classical times is it certain that celery was cultivated.\n\nM. Fragiska mentions an archeological find of celery dating to the 9th century BC, at Kastanas; however, the literary evidence for ancient Greece is far more abundant. In Homer's \"Iliad\", the horses of the Myrmidons graze on wild celery that grows in the marshes of Troy, and in \"Odyssey\", there is mention of the meadows of violet and wild celery surrounding the cave of Calypso.\n\nIn the \"Capitulary\" of Charlemagne, compiled ca. 800, \"apium\" appears, as does \"olisatum\", or alexanders, among medicinal herbs and vegetables the Frankish emperor desired to see grown. At some later point in medieval Europe celery displaced alexanders.\n\nCelery's late arrival in the English kitchen is an end-product of the long tradition of seed selection needed to reduce the sap's bitterness and increase its sugars. By 1699, John Evelyn could recommend it in his \"Acetaria. A Discourse of Sallets\": \"Sellery, apium Italicum, (and of the Petroseline Family) was formerly a stranger with us (nor very long since in Italy) is an hot and more generous sort of Macedonian Persley or Smallage...and for its high and grateful Taste is ever plac'd in the middle of the Grand Sallet, at our Great Men's tables, and Praetors feasts, as the Grace of the whole Board\".\n\nCelery makes a minor appearance in colonial American gardens; its culinary limitations are reflected in the observation by the author of \"A Treatise on Gardening, by a Citizen of Virginia\" that it is \"one of the species of parsley.\" Its first extended treatment in print was in Bernard M'Mahon's \"American Gardener's Calendar\" (1806). After the mid-19th century, continued selections for refined crisp texture and taste brought celery to American tables, where it was served in celery vases to be salted and eaten raw.\n\nA chthonian symbol among the ancient Greeks, celery was said to have sprouted from the blood of Kadmilos, father of the Cabeiri, chthonian divinities celebrated in Samothrace, Lemnos, and Thebes. The spicy odour and dark leaf colour encouraged this association with the cult of death. In classical Greece, celery leaves were used as garlands for the dead, and the wreaths of the winners at the Isthmian Games were first made of celery before being replaced by crowns made of pine. According to Pliny the Elder in Achaea, the garland worn by the winners of the sacred Nemean Games was also made of celery. The Ancient Greek colony of Selinous (, \"Selinous\"), on Sicily, was named after wild parsley that grew abundantly there; Selinountian coins depicted a parsley leaf as the symbol of the city.\n\nThe perennial BBC television series Doctor Who featured the Fifth Doctor (played by Peter Davison, from 1981–84), who wore a sprig of celery as a corsage.\n\nThe name \"celery\" retraces the plant's route of successive adoption in European cooking, as the English \"celery\" (1664) is derived from the French \"céleri\" coming from the Lombard term, \"seleri\", from the Latin \"selinon\", borrowed from Greek.\n\n"}
{"id": "6536", "url": "https://en.wikipedia.org/wiki?curid=6536", "title": "CPM", "text": "CPM\n\nCPM may refer to:\n\n\n\n\n\n\n"}
{"id": "6537", "url": "https://en.wikipedia.org/wiki?curid=6537", "title": "Celestines", "text": "Celestines\n\nThe Celestines were a Roman Catholic monastic order, a branch of the Benedictines, founded in 1244. At the foundation of the new rule, they were called Hermits of St Damiano, or Moronites (or Murronites), and did not assume the appellation of Celestines until after the election of their founder, Peter of Morone (Pietro Murrone), to the Papacy as Celestine V. They used the post-nominal initials O.S.B. Cel.\n\nThe fame of the holy life and the austerities practised by Pietro Morone in his solitude on the Mountain of Majella, near Sulmona, attracted many visitors, several of whom were moved to remain and share his mode of life. They built a small convent on the spot inhabited by the holy hermit, which became too small for the accommodation of those who came to share their life of privations. Peter of Morone (later Pope Celestine V), their founder, built a number of other small oratories in that neighborhood.\n\nAbout the year 1254, Peter of Morone gave the order a rule formulated in accordance with his own practices. In 1264 the new institution was approved as a branch of the Benedictines by Urban IV; however, the next pope Pope Gregory X had commanded that all orders founded since the prior Lateran Council should not be further multiplied. Hearing a rumor that the order was to be suppressed, the reclusive Peter traveled to Lyon, where the Pope was holding a council. There he persuaded Gregory to approve his new order, making it a branch of the Benedictines and following the rule of Saint Benedict, but adding to it additional severities and privations. Gregory took it under the Papal protection, assured to it the possession of all property it might acquire, and endowed it with exemption from the authority of the ordinary. Nothing more was needed to ensure the rapid spread of the new association and Peter the hermit of Morone lived to see himself \"Superior-General\" to thirty-six monasteries and more than six hundred monks. \n\nAs soon as he had seen his new order thus consolidated he gave up the government of it to a certain Robert, and retired once again to an even more remote site to devote himself to solitary penance and prayer. Shortly afterwards, in a chapter of the order held in 1293, the original monastery of Majella being judged to be too desolate and exposed to too rigorous a climate, it was decided that the Abbey of the Holy Spirit at Monte Morrone, located in Sulmona, should be the headquarters of the order and the residence of the General-Superior, where it continued for centuries. The next year Peter of Morrone, despite his reluctance, was elected Pope by the name of Celestine V. From there on, the order he had founded took the name of Celestines. During his short reign as Pope, the former hermit confirmed the rule of the order, which he had himself composed, and conferred on the society a variety of special graces and privileges. In the only creation of cardinals promoted by him, among the twelve raised to the purple, there were two monks of his order. He also visited personally the Benedictine monastery on Monte Cassino, where he persuaded the monks to accept his more rigorous rule. He sent fifty monks of his order to introduce it, who remained there, however, for only a few months.\n\nAfter the death of the founder the order was favoured and privileged by Benedict XI, and rapidly spread through Italy, Germany, Flanders, and France, where they were received by Philip the Fair in 1300. The administration of the order was carried on somewhat after the pattern of Cluny, that is all monasteries were subject to the Abbey of the Holy Ghost at Sulmona, and these dependent houses were divided into provinces. The Celestines had ninety-six houses in Italy, twenty-one in France, and a few in Germany.\n\nSubsequently the French Celestines, with the consent of the Italian superiors of the order, and of Pope Martin V in 1427, obtained the privilege of making new constitutions for themselves, which they did in the 17th century in a series of regulations accepted by the provincial chapter in 1667. At that time the French congregation of the order was composed of twenty-one monasteries, the head of which was that of Paris, and was governed by a Provincial with the authority of General. Paul V was a notable benefactor of the order. The order became extinct in the eighteenth century.\n\nAccording to their special constitutions the Celestines were bound to say matins in the choir at two o'clock in the morning, and always to abstain from eating meat, save in illness. The distinct rules of their order with regard to fasting are numerous, but not more severe than those of similar congregations, though much more so than is required by the old Benedictine rule. In reading their minute directions for divers degrees of abstinence on various days, it is impossible to avoid being struck by the conviction that the great object of the framers of these rules was the general purpose of ensuring an ascetic mode of life.\n\nThe Celestines wore a white woollen cassock bound with a linen band, and a leathern girdle of the same colour, with a scapular unattached to the body of the dress, and a black hood. It was not permitted to them to wear any shirt save of serge. Their dress in short was very like that of the Cistercians. But it is a tradition in the order that in the time of the founder they wore a coarse brown cloth. The church and monastery of San Pietro in Montorio originally belonged to the Celestines in Rome; but they were turned out of it by Sixtus IV to make way for Franciscans, receiving from the Pope in exchange the Church of St Eusebius of Vercelli with the adjacent mansion for a monastery.\n\n"}
{"id": "6539", "url": "https://en.wikipedia.org/wiki?curid=6539", "title": "Cessna", "text": "Cessna\n\nThe Cessna Aircraft Company is an American general aviation aircraft manufacturing corporation headquartered in Wichita, Kansas. Best known for small, piston-powered aircraft, Cessna also produces business jets. The company is a subsidiary of the U.S. conglomerate Textron.\n\nIn March 2014 Cessna became a brand of Textron Aviation.\n\nClyde Cessna, a farmer in Rago, Kansas, built his own aircraft and flew it in June 1911, the first person to do so between the Mississippi River and the Rocky Mountains. Cessna started his wood-and-fabric aircraft ventures in Enid, Oklahoma, testing many of his early planes on the salt flats. When bankers in Enid refused to lend him more money to build his planes, he moved to Wichita.\nCessna Aircraft was formed when Clyde Cessna and Victor Roos became partners in the Cessna-Roos Aircraft Company in 1927. Roos resigned just one month into the partnership selling back his interest to Cessna. In the same year, the Kansas Secretary of State approved dropping Roos's name from the company name.\n\nThe Cessna DC-6 earned certification on the same day as the stock market crash of 1929, 29 October 1929.\n\nIn 1932 Cessna Aircraft Company closed its doors due to the Great Depression.\n\nHowever the Cessna CR-3 custom racer took its first flight in 1933. The plane won the 1933 American Air Race in Chicago and later set a new world speed record for engines smaller than 500 cubic inches by averaging .\n\nCessna's nephews, Dwane Wallace and his brother Dwight, bought the company from Cessna in 1934. They reopened it and began the process of building it into what would become a global success.\nThe Cessna C-37 was introduced in 1937 as Cessna's first seaplane when equipped with Edo floats. In 1940, Cessna received their largest order to date, when they signed a contract with the U.S. Army for 33 specially equipped Cessna T-50s. Later in 1940, the Royal Canadian Air Force placed an order for 180 T-50s.\n\nCessna returned to commercial production in 1946, after the revocation of wartime production restrictions (L-48) with the release of the Model 120 and Model 140. The approach was to introduce a new line of all-metal aircraft that used production tools, dies and jigs rather than the hand-built process tube-and-fabric construction used before the war.\n\nThe Model 140 was named by the US Flight Instructors Association as the \"Outstanding Plane of the Year\", in 1948.\n\nCessna's first helicopter, the Cessna CH-1, received FAA type certification, in 1955.\n\nCessna introduced the Cessna 172, in 1956. It became the most produced airplane in history.\n\nIn 1960 Cessna affiliated itself with Reims Aviation of Reims, France. 1963 saw Cessna produce its 50,000th airplane, a Cessna 172.\n\nCessna's first business jet, the Cessna Citation I performed its maiden flight on 15 September 1969.\n\nCessna produced its 100,000th single engine airplane in 1975.\n\nIn 1985 Cessna ceased to be an independent company. It was purchased by General Dynamics Corporation and became a wholly owned subsidiary. Production of the Cessna Caravan began. General Dynamics in turn, sold Cessna to Textron Inc, in 1992.\nLate in 2007, Cessna purchased the bankrupt Columbia Aircraft company for US$26.4M and would continue production of the Columbia 350 and 400 as the Cessna 350 and Cessna 400 at the Columbia factory in Bend, Oregon.\n\nOn 27 November 2007, Cessna announced the then-new Cessna 162 would be built in the People's Republic of China by Shenyang Aircraft Corporation, which is a subsidiary of the China Aviation Industry Corporation I (AVIC I), a Chinese government-owned consortium of aircraft manufacturers. Cessna reported that the decision was made to save money and also that the company had no more plant capacity in the USA at the time. Cessna received much negative feedback for this decision, with complaints centering on the recent quality problems with Chinese production of other consumer products, China's human rights record, exporting of jobs, and China's less than friendly political relationship with the USA. The customer backlash surprised Cessna and resulted in a company public relations campaign. In early 2009, the company attracted further criticism for continuing plans to build the 162 in China while laying off large numbers of workers in the USA. In the end the Cessna 162 was not a commercial success and only a small number were delivered before production was cancelled.\n\nThe company's business suffered notably during the Late-2000s recession, laying off more than half its workforce between January 2009 and September 2010.\n\nOn 4 November 2008 Cessna's parent company, Textron, indicated that Citation production would be reduced from the original 2009 target of 535 \"due to continued softening in the global economic environment\" and that this would result in an undetermined number of lay-offs at Cessna.\n\nOn 8 November 2008, at the AOPA Expo, CEO Jack Pelton indicated that Cessna sales of aircraft to individual buyers had fallen but piston and turboprop sales to business had not. \"While the economic slowdown has created a difficult business environment, we are encouraged by brisk activity from new and existing propeller fleet operators placing almost 200 orders for 2009 production aircraft,\" Pelton stated.\n\nBeginning in January 2009, a total of 665 jobs were cut at Cessna's Wichita and Bend, Oregon plants. The Cessna factory at Independence, Kansas, which builds the Cessna piston-engined aircraft and the Cessna Mustang, did not see any layoffs, but one third of the workforce at the former Columbia Aircraft facility in Bend was laid off. This included 165 of the 460 employees who built the Cessna 350 and 400. The remaining 500 jobs were eliminated at the main Cessna Wichita plant.\n\nIn January 2009 the company laid off an additional 2,000 employees, bringing the total to 4,600. The job cuts included 120 at the Bend, Oregon facility reducing the plant that built the Cessna 350 and 400 to fewer than half the number of workers that it had when Cessna bought it. Other cuts included 200 at the Independence, Kansas plant that builds the single-engined Cessnas and the Mustang, reducing that facility to 1,300 workers.\n\nOn 29 April 2009 the company suspended the Citation Columbus program and closing the Bend, Oregon facility. The Columbus program was finally cancelled in early July 2009. The company said \"Upon additional analysis of the business jet market related to this product offering, we decided to formally cancel further development of the Citation Columbus\". With the 350 and 400 production moving to Kansas, the company indicated that it would lay off 1,600 more workers, including the remaining 150 employees at the Bend plant and up to 700 workers from the Columbus program.\n\nIn early June 2009 Cessna laid off an additional 700 salaried employees, bringing the total number of lay-offs to 7600, which was more than half the company's workers at the time.\n\nThe company closed its three Columbus, Georgia manufacturing facilities between June 2010 and December 2011. The closures included the new facility that was opened in August 2008 at a cost of US$25M, plus the McCauley Propeller Systems plant. These closures resulted in total job losses of 600 in Georgia. Some of the work was relocated to Cessna's Independence, Kansas or Mexican facilities.\n\nCessna's parent company Textron posted a loss of US$8M in the first quarter of 2010, largely driven by continuing low sales at Cessna, which were down 44%. Cessna's workforce remained 50% laid-off and CEO Jack Pelton stated that he expected the recovery to be long and slow.\n\nIn September 2010, a further 700 employees were laid off, bringing the total to 8,000 jobs lost. CEO Jack Pelton indicated this round of layoffs was due to a \"stalled [and] lackluster economy\" and noted that while the number of orders cancelled for jets has been decreasing new orders have not met expectations. Pelton added \"our strategy is to defend and protect our current markets while investing in products and services to secure our future, but we can do this only if we succeed in restructuring our processes and reducing our costs.\"\n\nOn 2 May 2011 CEO Jack Pelton retired. The new CEO, Scott A. Ernest, started on 31 May 2011. Ernest joined Textron after 29 years at GE, where he had most recently served as vice president and general manager, global supply chain for GE Aviation. Ernest previously worked for Textron CEO Scott Donnelly when both worked at GE.\n\nIn September 2011 the Federal Aviation Administration proposed a US$2.4M fine against the company for its failure to follow quality assurance requirements while producing fiberglass components at its plant in Chihuahua, Mexico. Excess humidity meant that the parts did not cure correctly and quality assurance did not detect the problems. The failure to follow procedures resulted in the delamination in flight of a section of one Cessna 400's wing skin from the spar while the aircraft was being flown by an FAA test pilot. The aircraft was landed safely. The FAA also discovered 82 other aircraft parts that had been incorrectly made and not detected by the company's quality assurance. The investigation resulted in an emergency airworthiness directive that affected 13 Cessna 400s.\n\nSince March 2012, Cessna has been pursuing building business jets in China as part of a joint venture with Aviation Industry Corporation of China (AVIC). The company stated that it intends to eventually build all aircraft models in China, saying \"The agreements together pave the way for a range of business jets, utility single-engine turboprops and single-engine piston aircraft to be manufactured and certified in China.\"\n\nIn late April 2012 the company recalled laid-off workers and started new hiring to fill 150 positions in Wichita as a result of anticipated increased demand for aircraft production.\n\nDuring the 1950s and 1960s Cessna's marketing department followed the lead of Detroit automakers and came up with many unique marketing terms in an effort to differentiate its product line from their competitions'.\n\nOther manufacturers and the aviation press widely ridiculed and spoofed many of the marketing terms, but Cessna built and sold more aircraft than any other manufacturer during the boom years of the 1960s and 1970s.\n\nCessna marketing terminology includes:\n\n\nCurrently Cessna has the following models in production:\n\n"}
{"id": "6542", "url": "https://en.wikipedia.org/wiki?curid=6542", "title": "Czesław Miłosz", "text": "Czesław Miłosz\n\nCzesław Miłosz (; 30 June 1911 – 14 August 2004) was a Polish poet, prose writer, translator and diplomat. His World War II-era sequence \"The World\" is a collection of twenty \"naïve\" poems. Following the war, he served as Polish cultural attaché in Paris and Washington, D.C., then in 1951 defected to the West. His nonfiction book \"The Captive Mind\" (1953) became a classic of anti-Stalinism. From 1961 to 1998 he was a professor of Slavic Languages and Literatures at the University of California, Berkeley. He became a U.S. citizen in 1970. In 1978 he was awarded the Neustadt International Prize for Literature, and in 1980 the Nobel Prize in Literature. In 1999 he was named a Puterbaugh Fellow. After the fall of the Iron Curtain, he divided his time between Berkeley, California, and Kraków, Poland.\n\nCzesław Miłosz was born on June 30, 1911, in the village of Szetejnie (), Kovno Governorate, Russian Empire (now Kėdainiai district, Kaunas County, Lithuania) on the border between two Lithuanian historical regions, Samogitia and Aukštaitija, in central Lithuania. As the son of Aleksander Miłosz (d.1959), a Polish civil engineer of Lithuanian origin, and Weronika, \"née\" Kunat (1887-1945), descendant of the Syruć noble family (she was a granddaughter of Szymon Syruć), Miłosz was fluent in Polish, Lithuanian, Russian, English, and French. His brother, Andrzej Miłosz (1917–2002), a Polish journalist, translator of literature and of film subtitles into Polish, was a documentary-film producer who created Polish documentaries about his brother.\n\nMiłosz was raised Catholic in rural Lithuania and emphasized his identity with the multi-ethnic Grand Duchy of Lithuania, a stance that led to ongoing controversies. He refused to categorically identify himself as either a Pole or a Lithuanian. He said of himself: \"I am a Lithuanian to whom it was not given to be a Lithuanian\", and \"My family in the sixteenth century already spoke Polish, just as many families in Finland spoke Swedish and in Ireland English, so I am a Polish not a Lithuanian poet. But the landscapes and perhaps the spirits of Lithuania have never abandoned me\". Miłosz memorialised his Lithuanian childhood in a 1955 novel \"The Issa Valley\" and in the 1959 memoir \"Native Realm\". He employed a Lithuanian-language tutor late in life to improve the skills acquired in his childhood. His explanation was that it might be the language spoken in heaven. He often is quoted as having said, \"Language is the only homeland.\"\n\nIn his youth, Miłosz came to adopt, as he put it, a \"scientific, atheistic position mostly\", although he was later to return to the Catholic faith. After graduation from Sigismund Augustus \"Gymnasium\" in Wilno (then in Poland, now Vilnius in Lithuania), he studied law at Stefan Batory University. In 1931 he traveled to Paris, where he was influenced by his distant cousin Oscar Milosz, a French poet of Lithuanian descent and a Swedenborgian. In 1931, he formed the poetic group Żagary with the young poets Jerzy Zagórski, Teodor Bujnicki, Aleksander Rymkiewicz, Jerzy Putrament and Józef Maśliński. Miłosz's first volume of poetry was published in 1934. After receiving his law degree that year, he again spent a year in Paris on a fellowship. Upon returning, he worked as a commentator at Radio Wilno, but was dismissed, an action described as stemming from either his leftist views or for views overly sympathetic to Lithuania. Miłosz wrote all his poetry, fiction and essays in Polish and translated the Old Testament \"Psalms\" into Polish.\n\nMiłosz spent World War II in Warsaw, under Nazi Germany's \"General Government\". Here he attended underground lectures by Polish philosopher and historian of philosophy and aesthetics Władysław Tatarkiewicz. He did not join the Polish Home Army's resistance or participate in the Warsaw Uprising, partly from an instinct for self-preservation and partly because he saw its leadership as right-wing and dictatorial. According to Irena Grudzińska-Gross, he saw the uprising as a \"doomed military effort\" and lacked \"patriotic elation\" — he called it \"a blameworthy, lightheaded enterprise.\"\nAfter World War II, Miłosz served as cultural attaché of the newly formed People's Republic of Poland in and Washington, D.C. and Paris. For this he was criticized in some emigre circles. Conversely, he was attacked and censored in Poland when, in 1951, he defected and obtained political asylum in France. He described his life in Paris as difficult — there was still considerable intellectual sympathy for Communism. Albert Camus was supportive, but Pablo Neruda denounced him as \"The Man Who Ran Away.\" His attempts to seek asylum in the US were denied for several years, due to the climate of McCarthyism.\n\nMiłosz's 1953 book, \"The Captive Mind\", is a study about how intellectuals behave under a repressive regime. Miłosz observed that those who became dissidents were not necessarily those with the strongest minds but rather those with the weakest stomachs; the mind can rationalize anything, he said, but the stomach can take only so much. Throughout the Cold War, the book was often cited by U.S. conservative commentators such as William F. Buckley, Jr., and it has been a staple in political science courses on totalitarianism.\n\nIn 1953 he received the Prix Littéraire Européen (European Literary Prize).\n\nDuring the Nazi occupation of Poland, Miłosz was active in the work of Organizacja Socjalistyczno-Niepodległościowa \"Wolność\" (\"The 'Freedom' Socialist Pro-Independence Organisation\"). In his activity with \"Wolność\", Miłosz gave aid to Warsaw Jews. His brother Andrzej Miłosz was also active in helping Jews in Nazi-occupied Poland, and in 1943 Andrzej transported the Polish Jew Seweryn Tross and his wife from Vilnius to Warsaw. Czesław Miłosz took in the Trosses, found them a hiding place, and supported them financially. The Trosses ultimately died during the Warsaw Uprising. Miłosz helped at least three other Jews — Felicja Wołkomińska and her brother and sister. For these efforts, Miłosz received the medal of the Righteous Among the Nations in Yad Vashem, Israel in 1989.\n\nIn 1960 Miłosz emigrated to the United States, and in 1970 he became a U.S. citizen. In 1961 he began a professorship in the Department of Slavic Languages and Literatures at the University of California, Berkeley.\n\nIn 1978 he received the Neustadt International Prize for Literature. He retired the same year but continued teaching at Berkeley. His attitude about living in Berkeley is sensitively portrayed in his poem, \"A Magic Mountain,\" contained in a collection of translated poems, \"Bells in Winter\" (Ecco Press, 1985).\n\nIn 1980 Miłosz received the Nobel Prize in Literature. Since his works had been banned in Poland by the communist government, this was the first time that many Poles became aware of him. After the Iron Curtain fell, he was able to return to Poland, at first to visit, later to live part-time in Kraków. He divided his time between his home in Berkeley and an apartment in Kraków.\n\nIn 1977 he had been given an honorary doctorate by the University of Michigan; two weeks after he received the 1980 Nobel Prize in Literature, he returned to Michigan to lecture, and in 1983 he became the Visiting Walgreen Professor of Human Understanding. In 1989 he received the U.S. National Medal of Arts and an honorary doctorate from Harvard University.\n\nIn a 1994 interview, Miłosz spoke of the difficulty of writing religious poetry in a largely post-religious world. He reported a recent conversation with his compatriot Pope John Paul II; the latter, commenting upon some of Miłosz's work, in particular \"Six Lectures in Verse\", said to him: \"You make one step forward, one step back.\" The poet answered: \"Holy Father, how in the twentieth century can one write religious poetry differently?\" The Pope smiled. A few years later, in 2000, Miłosz dedicated a rather straightforward ode to John Paul II, on the occasion of the pope's eightieth birthday.\n\nCzesław Miłosz died on 14 August 2004 at his Kraków home, aged 93. He was buried in Kraków's Skałka Roman Catholic Church, becoming one of the last to be commemorated there. Protesters threatened to disrupt the proceedings on the grounds that he was anti-Polish, anti-Catholic, and had signed a petition supporting gay and lesbian freedom of speech and assembly. Pope John Paul II, along with Miłosz's confessor, issued public messages to the effect that Miłosz had been receiving the sacraments, which quelled the protest.\n\nHis first wife, Janina (née Dłuska, b. 1909), whom he married in 1944, died in 1986; they had two sons, Anthony (b. 1947) and John Peter (b. 1951). His second wife, Carol Thigpen (b. 1944), an American-born historian, died in 2002.\n\nMiłosz is honoured at Israel's Yad Vashem memorial to the Holocaust, as one of the \"Righteous among the Nations\". A poem by Miłosz appears on a Gdańsk memorial to protesting shipyard workers who had been killed by government security forces in 1970. His books and poems have been translated by many hands, including Jane Zielonko, Peter Dale Scott, Robert Pinsky, and Robert Hass.\n\nIn November 2011, Yale University hosted a conference on Miłosz's relationship with America. The Beinecke Rare Book and Manuscript Library, which holds the Czesław Miłosz Papers, also hosted an exhibition celebrating Miłosz's life and work, entitled \"Exile as Destiny: Czeslaw Miłosz and America\". 2011 was deemed \"The Miłosz Year,\" which culminated in a large conference in Kraków (May 9–15, 2011).\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "6543", "url": "https://en.wikipedia.org/wiki?curid=6543", "title": "Carnivore", "text": "Carnivore\n\nA carnivore meaning 'Meat Eater' (Latin, \"carne\" meaning 'meat' or 'flesh' and \"vorare\" meaning 'to devour') is an organism that derives its energy and nutrient requirements from a diet consisting mainly or exclusively of animal tissue, whether through predation or scavenging. Animals that depend solely on animal flesh for their nutrient requirements are called obligate carnivores while those that also consume non-animal food are called facultative carnivores. Omnivores also consume both animal and non-animal food, and apart from the more general definition, there is no clearly defined ratio of plant to animal material that would distinguish a facultative carnivore from an omnivore. A carnivore that sits at the top of the food chain is termed an apex predator.\n\nPlants that capture and digest insects (and, at times, other small animals) are called carnivorous plants. Similarly, fungi that capture microscopic animals are often called carnivorous fungi.\n\nThe word \"carnivore\" sometimes refers to the mammalian order Carnivora, but this is somewhat misleading. While many Carnivora meet the definition of being meat eaters, not all do, and even fewer are true obligate carnivores (see below). For example, most species of bears are actually omnivorous, except for the giant panda, which is almost exclusively herbivorous, and the exclusively meat-eating polar bear, which lives in the Arctic, where few plants grow. In addition, there are plenty of carnivorous species that are not members of Carnivora.\n\nOutside the animal kingdom, there are several genera containing carnivorous plants and several phyla containing carnivorous fungi. The former are predominantly insectivores, while the latter prey mostly on microscopic invertebrates, such as nematodes, amoebae and springtails.\nCarnivores are sometimes characterized by the type of prey that they consume. For example, animals that eat insects and similar invertebrates primarily or exclusively are called insectivores, while those that eat fish primarily or exclusively are called piscivores. The first tetrapods, or land-dwelling vertebrates, were piscivorous amphibians known as labyrinthodonts. They gave rise to insectivorous vertebrates and, later, to predators of other tetrapods.\n\nCarnivores may alternatively be classified according to the percentage of meat in their diet. The diet of a hypercarnivore consists of more than 70% meat, that of a mesocarnivore 50–70%, and that of a hypocarnivore less than 30%, with the balance consisting of non-animal foods such as fruits, other plant material, or fungi.\n\nObligate carnivores, or \"true\" carnivores, are those carnivores whose survival depends on nutrients which are found only in animal flesh. While obligate carnivores might be able to ingest small amounts of plant material, because of their evolution they lack the necessary physiology required to digest that plant matter. In fact, some obligate carnivorous mammals will only ever ingest vegetation for its specific use as an emetic to self-induce vomiting to rid itself of food that has upset its stomach.\n\nFor instance, felids including the domestic cat are obligate carnivores requiring a diet of primarily animal flesh and organs. Specifically, cats have high protein requirements and their metabolisms appear unable to synthesize certain essential nutrients (including retinol, arginine, taurine, and arachidonic acid), and thus, in nature, they can rely only on animal flesh as their diet to supply these nutrients.\n\nCharacteristics commonly associated with carnivores include organs for capturing and disarticulating prey (teeth and claws serve these functions in many vertebrates) and status as a predator. In truth, these assumptions may be misleading, as some carnivores do not hunt and are scavengers (though most hunting carnivores will scavenge when the opportunity exists). Thus they do not have the characteristics associated with hunting carnivores. Carnivores have comparatively short digestive systems, as they are not required to break down tough cellulose found in plants. Many animals that hunt other animals have evolved eyes that face forward, thus making depth perception possible. This is almost universal among mammalian predators. Other predators, like crocodiles, as well as most reptiles and amphibians, have sideways facing eyes and hunt by ambush rather than pursuit.\n\nThe first vertebrate carnivores were fish, and then amphibians that moved on to land. Early tetrapods were large amphibious piscivores. Some scientists assert that \"Dimetrodon\" \"was the first terrestrial vertebrate to develop the curved, serrated teeth that enable a predator to eat prey much larger than itself.\" While amphibians continued to feed\non fish and later insects, reptiles began exploring two new food types, tetrapods (carnivory), and\nlater, plants (herbivory). Carnivory was a natural transition from insectivory for medium and large tetrapods, requiring minimal adaptation (in contrast, a complex set of adaptations was necessary for feeding on highly fibrous plant materials).\n\nCarnivoramorphs are currently the dominant carnivorous mammals, and have been so since the Miocene. In the early to mid-Cenozoic, however, hyaenodonts, oxyaenid, entelodonts, ptolemaiidans, \"arctocyonids\" and \"mesonychians\" were dominant instead, representing a very high diversity of eutherian carnivores in the northern continents and Africa. In South America, sparassodonts were dominant instead, while Australia saw the presence of several marsupial predators, such as the dasyuromorphs and thylacoleonids.\n\nIn the Mesozoic, while theropod dinosaurs were the larger carnivores, several carnivorous mammal groups were already present. Most notable are the gobiconodontids, the triconodontid \"Jugulator\", the deltatheroideans and \"Cimolestes\". Many of these, such as \"Repenomamus\", \"Jugulator\" and \"Cimolestes\", were among the largest mammals in their faunal assemblages, capable of attacking dinosaurs.\n\nMost carnivorous mammals, from dogs to \"Deltatheridium\", share several adaptations in common, such as carnassialiforme teeth, long canines and even similar tooth replacement patterns. Most aberrant are thylacoleonids, which bear a diprodontan dentition completely unlike that of any mammal, and \"eutriconodonts\" like gobioconodontids and \"Jugulator\", by virtue of their cusp anatomy, though they still worked in the same way as carnassials.\n\nSome theropod dinosaurs such as \"Tyrannosaurus rex\" that existed during the Mesozoic Era were probably obligate carnivores.\n\n\n"}
{"id": "6546", "url": "https://en.wikipedia.org/wiki?curid=6546", "title": "Celts", "text": "Celts\n\nThe Celts ( or , see pronunciation of \"Celt\" for different usages) were an Indo-European people in Iron Age and Medieval Europe who spoke Celtic languages and had cultural similarities, although the relationship between ethnic, linguistic and cultural factors in the Celtic world remains uncertain and controversial. The exact geographic spread of the ancient Celts is also disputed; in particular, the ways in which the Iron Age inhabitants of Great Britain and Ireland should be regarded as Celts has become a subject of controversy.\n\nThe history of pre-Celtic Europe remains very uncertain. According to one theory, the common root of the Celtic languages, the Proto-Celtic language, arose in the Late Bronze Age Urnfield culture of Central Europe, which flourished from around 1200 BC. In addition, according to a theory proposed in the 19th century, the first people to adopt cultural characteristics regarded as Celtic were the people of the Iron Age Hallstatt culture in central Europe (c. 800–450 BC), named for the rich grave finds in Hallstatt, Austria. Thus this area is sometimes called the \"Celtic homeland\". By or during the later La Tène period (c. 450 BC up to the Roman conquest), this Celtic culture was supposed to have expanded by trans-cultural diffusion or migration to the British Isles (Insular Celts), France and the Low Countries (Gauls), Bohemia, Poland and much of Central Europe, the Iberian Peninsula (Celtiberians, Celtici, Lusitanians and Gallaeci) and northern Italy (Golasecca culture and Cisalpine Gauls) and, following the Celtic settlement of Eastern Europe beginning in 279 BC, as far east as central Anatolia (Galatians) in modern-day Turkey.\n\nThe earliest undisputed direct examples of a Celtic language are the Lepontic inscriptions beginning in the 6th century BC. Continental Celtic languages are attested almost exclusively through inscriptions and place-names. Insular Celtic languages are attested beginning around the 4th century in Ogham inscriptions, although it was clearly being spoken much earlier. Celtic literary tradition begins with Old Irish texts around the 8th century. Coherent texts of Early Irish literature, such as the \"Táin Bó Cúailnge\" (\"Cattle Raid of Cooley\"), survive in 12th century recensions.\n\nBy the mid-1st millennium, with the expansion of the Roman Empire and the Migration Period of Germanic peoples, Celtic culture and Insular Celtic languages had become restricted to Ireland, the western and northern parts of Great Britain (Wales, Scotland, and Cornwall), the Isle of Man, and Brittany. Between the 5th and 8th centuries, the Celtic-speaking communities in these Atlantic regions emerged as a reasonably cohesive cultural entity. They had a common linguistic, religious and artistic heritage that distinguished them from the culture of the surrounding polities. By the 6th century, however, the Continental Celtic languages were no longer in wide use.\n\nInsular Celtic culture diversified into that of the Gaels (Irish, Scottish and Manx) and the Celtic Britons (Welsh, Cornish, and Bretons) of the medieval and modern periods. A modern \"Celtic identity\" was constructed as part of the Romanticist Celtic Revival in Great Britain, Ireland, and other European territories, such as Portugal and Spanish Galicia. Today, Irish, Scottish Gaelic, Welsh, and Breton are still spoken in parts of their historical territories, and Cornish and Manx are undergoing a revival.\n\nThe first recorded use of the name of Celts – as  – to refer to an ethnic group was by Hecataeus of Miletus, the Greek geographer, in 517 BC, when writing about a people living near Massilia (modern Marseille). In the fifth century BC, Herodotus referred to \"Keltoi\" living around the head of the Danube and also in the far west of Europe. The etymology of the term \"Keltoi\" is unclear. Possible roots include Indo-European *\"kʲel\" 'to hide' (present also in Old Irish \"ceilid\"), IE *\"kʲel\" 'to heat' or *\"kel\" 'to impel'. Several authors have supposed it to be Celtic in origin, while others view it as a name coined by Greeks. Linguist Patrizia De Bernardo Stempel falls in the latter group, and suggests the meaning \"the tall ones\".\n\nIn the 1st century BC, Julius Caesar reported that the people known to the Romans as Gauls (\"Galli\") called themselves Celts, which suggests that even if the name \"Keltoi\" was bestowed by the Greeks, it had been adopted to some extent as a collective name by the tribes of Gaul. The geographer Strabo, writing about Gaul towards the end of the first century BC, refers to the “race which is now called both Gallic and Galatic,” though he also uses the term Celtica as a synonym for Gaul, which is separated from Iberia by the Pyrenees. Yet he reports Celtic peoples in Iberia, and also uses the ethnic names Celtiberi and Celtici for peoples there, as distinct from Lusitani and Iberi. Pliny the Elder cited the use of Celtici in Lusitania as a tribal surname, which epigraphic findings have confirmed.\n\nLatin Gallus (pl. \"Galli\") might stem from a Celtic ethnic or tribal name originally, perhaps one borrowed into Latin during the Celtic expansions into Italy during the early fifth century BC. Its root may be the Proto-Celtic \"*galno\", meaning “power, strength”, hence Old Irish \"gal\" “boldness, ferocity” and Welsh \"gallu\" “to be able, power”. The tribal names of Gallaeci and the Greek Γαλάται (\"Galatai\", Latinized \"Galatae\"; see the region Galatia in Anatolia) most probably have the same origin. The suffix \"-atai\" might be an Ancient Greek inflection. Classical writers did not apply the terms or \"Celtae\" to the inhabitants of Britain or Ireland, which has led to some scholars preferring not to use the term for the Iron Age inhabitants of those islands.\n\nCelt is a modern English word, first attested in 1707, in the writing of Edward Lhuyd, whose work, along with that of other late 17th-century scholars, brought academic attention to the languages and history of the early Celtic inhabitants of Great Britain. The English form Gaul (first recorded in the 17th century) and Gaulish come from the French \"Gaule\" and \"Gaulois\", a borrowing from Frankish \"*Walholant\", “Land of foreigners or Romans” (see Gaul: Name), the root of which is Proto-Germanic \"*walha-\", “foreigner, Roman, Celt”, whence the English word Welsh (Old English \"wælisċ\" < *\"walhiska-\"), South German \"\", meaning “Celtic speaker”, “French speaker” or “Italian speaker” in different contexts, and Old Norse \"valskr\", pl. \"valir\", “Gaulish, French”). Proto-Germanic \"*walha\" is derived ultimately from the name of the Volcae, a Celtic tribe who lived first in the South of Germany and in Central Europe emigrated then to Gaul. This means that English Gaul, despite its superficial similarity, is not actually derived from Latin \"Gallia\" (which should have produced \"**Jaille\" in French), though it does refer to the same ancient region.\n\nCeltic refers to a family of languages and, more generally, means “of the Celts” or “in the style of the Celts”. Several archaeological cultures are considered Celtic in nature, based on unique sets of artefacts. The link between language and artefact is aided by the presence of inscriptions. The relatively modern idea of an identifiable Celtic cultural identity or \"Celticity\" generally focuses on similarities among languages, works of art, and classical texts, and sometimes also among material artefacts, social organisation, homeland and mythology. Earlier theories held that these similarities suggest a common racial origin for the various Celtic peoples, but more recent theories hold that they reflect a common cultural and language heritage more than a genetic one. Celtic cultures seem to have been widely diverse, with the use of a Celtic language being the main thing they have in common.\n\nToday, the term Celtic generally refers to the languages and respective cultures of Ireland, Scotland, Wales, Cornwall, the Isle of Man, and Brittany, also known as the Celtic nations. These are the regions where four Celtic languages are still spoken to some extent as mother tongues. The four are Irish Gaelic, Scottish Gaelic, Welsh, and Breton; plus two recent revivals, Cornish (one of the Brittonic languages) and Manx (one of the Goidelic languages). There are also attempts to reconstruct Cumbric, a Brittonic language from North West England and South West Scotland). Celtic regions of Continental Europe are those whose residents claim a Celtic heritage, but where no Celtic language has survived; these areas include the western Iberian Peninsula, i.e. Portugal and north-central Spain (Galicia, Asturias, Cantabria, Castile and León, Extremadura).\n\nContinental Celts are the Celtic-speaking people of mainland Europe and Insular Celts are the Celtic-speaking peoples of the British and Irish islands and their descendants. The Celts of Brittany derive their language from migrating insular Celts, mainly from Wales and Cornwall, and so are grouped accordingly.\n\nThe Celtic languages form a branch of the larger Indo-European family. By the time speakers of Celtic languages entered history around 400 BC, they were already split into several language groups, and spread over much of Western continental Europe, the Iberian Peninsula, Ireland and Britain.\n\nSome scholars think that the Urnfield culture of western Middle Europe represents an origin for the Celts as a distinct cultural branch of the Indo-European family. This culture was preeminent in central Europe during the late Bronze Age, from circa 1200 BC until 700 BC, itself following the Unetice and Tumulus cultures. The Urnfield period saw a dramatic increase in population in the region, probably due to innovations in technology and agriculture. The Greek historian Ephorus of Cyme in Asia Minor, writing in the 4th century BC, believed that the Celts came from the islands off the mouth of the Rhine and were \"driven from their homes by the frequency of wars and the violent rising of the sea\".\n\nThe spread of iron-working led to the development of the Hallstatt culture directly from the Urnfield (c. 700 to 500 BC). Proto-Celtic, the latest common ancestor of all known Celtic languages, is considered by this school of thought to have been spoken at the time of the late Urnfield or early Hallstatt cultures, in the early 1st millennium BC. The spread of the Celtic languages to Iberia, Ireland and Britain would have occurred during the first half of the 1st millennium BC, the earliest chariot burials in Britain dating to c. 500 BC. Other scholars see Celtic languages as covering Britain and Ireland, and parts of the Continent, long before any evidence of \"Celtic\" culture is found in archaeology. Over the centuries the language(s) developed into the separate Celtiberian, Goidelic and Brittonic languages.\n\nThe Hallstatt culture was succeeded by the La Tène culture of central Europe, which was overrun by the Roman Empire, though traces of La Tène style are still to be seen in Gallo-Roman artefacts. In Britain and Ireland La Tène style in art survived precariously to re-emerge in Insular art. Early Irish literature casts light on the flavour and tradition of the heroic warrior elites who dominated Celtic societies. Celtic river-names are found in great numbers around the upper reaches of the Danube and Rhine, which led many Celtic scholars to place the ethnogenesis of the Celts in this area.\n\nDiodorus Siculus and Strabo both suggest that the heartland of the people they called Celts was in southern France. The former says that the Gauls were to the north of the Celts, but that the Romans referred to both as Gauls (in linguistic terms the Gauls were certainly Celts). Before the discoveries at Hallstatt and La Tène, it was generally considered that the Celtic heartland was southern France, see Encyclopædia Britannica for 1813.\n\nMyles Dillon and Nora Kershaw Chadwick accepted that \"the Celtic settlement of the British Isles\" might have to be dated to the Bell Beaker culture concluding that \"There is no reason why so early a date for the coming of the Celts should be impossible\". Martín Almagro Gorbea proposed the origins of the Celts could be traced back to the 3rd millennium BC, also seeking the initial roots in the Beaker period, thus offering the wide dispersion of the Celts throughout western Europe, as well as the variability of the different Celtic peoples, and the existence of ancestral traditions an ancient perspective. Using a multidisciplinary approach, Alberto J. Lorrio and Gonzalo Ruiz Zapatero reviewed and built on Almagro Gorbea's work to present a model for the origin of the Celtic archaeological groups in the Iberian Peninsula (Celtiberian, Vetton, Vaccean, the Castro culture of the northwest, Asturian-Cantabrian and Celtic of the southwest) and proposing a rethinking of the meaning of \"Celtic\" from a European perspective. More recently, John Koch and Barry Cunliffe have suggested that Celtic origins lie with the Atlantic Bronze Age, roughly contemporaneous with the Hallstatt culture but positioned considerably to the West, extending along the Atlantic coast of Europe.\n\nStephen Oppenheimer points out that the only written evidence that locates the Keltoi near the source of the Danube (i.e. in the Hallstatt region) is in the \"Histories\" of Herodotus. However, Oppenheimer shows that Herodotus seemed to believe the Danube rose near the Pyrenees, which would place the Ancient Celts in a region which is more in agreement with later classical writers and historians (i.e. in Gaul and the Iberian peninsula).\n\nThe Proto-Celtic language is usually dated to the Late Bronze Age. The earliest records of a Celtic language are the Lepontic inscriptions of Cisalpine Gaul (Northern Italy), the oldest of which predate the La Tène period. Other early inscriptions, appearing from the early La Tène period in the area of Massilia, are in Gaulish, which was written in the Greek alphabet until the Roman conquest. Celtiberian inscriptions, using their own Iberian script, appear later, after about 200 BC. Evidence of Insular Celtic is available only from about 400 AD, in the form of Primitive Irish Ogham inscriptions.\n\nBesides epigraphical evidence, an important source of information on early Celtic is toponymy.\n\nBefore the 19th century, scholars assumed that the original land of the Celts was west of the Rhine, more precisely in Gaul, because it was where Greek and Roman ancient sources, namely Caesar, located the Celts. This view was challenged by the 19th-century historian Marie Henri d'Arbois de Jubainville who placed the land of origin of the Celts east of the Rhine. Jubainville based his arguments on a phrase of Herodotus' that placed the Celts at the source of the Danube, and argued that Herodotus had meant to place the Celtic homeland in southern Germany.\nThe finding of the prehistoric cemetery of Hallstat in 1846 by Johan Ramsauer and the finding of the archaeological site of La Tène by Hansli Kopp in 1857 drew attention to this area.\n\nThe concept that the Hallstatt and La Tène cultures could be seen not just as chronological periods but as \"Culture Groups\", entities composed of people of the same ethnicity and language, had started to grow by the end of the 19th century. At the beginning of the 20th century the belief that these \"Culture Groups\" could be thought of in racial or ethnic terms was strongly held by Gordon Childe whose theory was influenced by the writings of Gustaf Kossinna. As the 20th century progressed, the racial ethnic interpretation of La Tène culture became much more strongly rooted, and any findings of La Tène culture and flat inhumation cemeteries were directly associated with the Celts and the Celtic language.\nThe Iron Age Hallstatt (c. 800–475 BC) and La Tène (c. 500–50 BC) cultures are typically associated with Proto-Celtic and Celtic culture.\n\nIn various academic disciplines the Celts were considered a Central European Iron Age phenomenon, through the cultures of Hallstatt and La Tène. However, archaeological finds from the Halstatt and La Tène culture were rare in the Iberian Peninsula, in southwestern France, northern and western Britain, southern Ireland and Galatia and did not provide enough evidence for a cultural scenario comparable to that of Central Europe. It is considered equally difficult to maintain that the origin of the Peninsular Celts can be linked to the preceding Urnfield culture. This has resulted in a more recent approach that introduces a 'proto-Celtic' substratum and a process of Celticisation, having its initial roots in the Bronze Age Bell Beaker culture.\n\nThe La Tène culture developed and flourished during the late Iron Age (from 450 BC to the Roman conquest in the 1st century BC) in eastern France, Switzerland, Austria, southwest Germany, the Czech Republic, Slovakia and Hungary. It developed out of the Hallstatt culture without any definite cultural break, under the impetus of considerable Mediterranean influence from Greek, and later Etruscan civilisations. A shift of settlement centres took place in the 4th century.\n\nThe western La Tène culture corresponds to historical Celtic Gaul. Whether this means that the whole of La Tène culture can be attributed to a unified Celtic people is difficult to assess; archaeologists have repeatedly concluded that language, material culture, and political affiliation do not necessarily run parallel. Frey notes that in the 5th century, \"burial customs in the Celtic world were not uniform; rather, localised groups had their own beliefs, which, in consequence, also gave rise to distinct artistic expressions\". Thus, while the La Tène culture is certainly associated with the Gauls, the presence of La Tène artefacts may be due to cultural contact and does not imply the permanent presence of Celtic speakers.\n\nPolybius published a history of Rome about 150 BC in which he describes the Gauls of Italy and their conflict with Rome. Pausanias in the 2nd century AD says that the Gauls \"originally called Celts\", \"live on the remotest region of Europe on the coast of an enormous tidal sea\". Posidonius described the southern Gauls about 100 BC. Though his original work is lost it was used by later writers such as Strabo. The latter, writing in the early 1st century AD, deals with Britain and Gaul as well as Hispania, Italy and Galatia. Caesar wrote extensively about his Gallic Wars in 58–51 BC. Diodorus Siculus wrote about the Celts of Gaul and Britain in his 1st-century history.\n\nThe Romans knew the Celts then living in what became present-day France as Gauls. The territory of these peoples probably included the Low Countries, the Alps and present-day northern Italy. Julius Caesar in his \"Gallic Wars\" described the 1st-century BC descendants of those Gauls.\n\nEastern Gaul became the centre of the western La Tène culture. In later Iron Age Gaul, the social organisation resembled that of the Romans, with large towns. From the 3rd century BC the Gauls adopted coinage. Texts with Greek characters from southern Gaul have survived from the 2nd century BC.\n\nGreek traders founded Massalia about 600 BC, with some objects (mostly drinking ceramics) being traded up the Rhone valley. But trade became disrupted soon after 500 BC and re-oriented over the Alps to the Po valley in the Italian peninsula. The Romans arrived in the Rhone valley in the 2nd century BC and encountered a mostly Celtic-speaking Gaul. Rome wanted land communications with its Iberian provinces and fought a major battle with the Saluvii at Entremont in 124–123 BC. Gradually Roman control extended, and the Roman Province of Gallia Transalpina developed along the Mediterranean coast. The Romans knew the remainder of Gaul as Gallia Comata – \"Hairy Gaul\".\n\nIn 58 BC the Helvetii planned to migrate westward but Julius Caesar forced them back. He then became involved in fighting the various tribes in Gaul, and by 55 BC had overrun most of Gaul. In 52 BC Vercingetorix led a revolt against the Roman occupation but was defeated at the siege of Alesia and surrendered.\n\nFollowing the Gallic Wars of 58–51 BC, Caesar's \"Celtica\" formed the main part of Roman Gaul, becoming the province of Gallia Lugdunensis. This territory of the Celtic tribes was bounded on the south by the Garonne and on the north by the Seine and the Marne. The Romans attached large swathes of this region to neighboring provinces Belgica and Aquitania, particularly under Augustus.\n\nPlace- and personal-name analysis and inscriptions suggest that the Gaulish Celtic language was spoken over most of what is now France.\n\nUntil the end of the 19th century, traditional scholarship dealing with the Celts did acknowledge their presence in the Iberian Peninsula as a material culture relatable to the Hallstatt and La Tène cultures. However, since according to the definition of the Iron Age in the 19th century Celtic populations were supposedly rare in Iberia and did not provide a cultural scenario that could easily be linked to that of Central Europe, the presence of Celtic culture in that region was generally not fully recognised. Modern scholarship, however, has clearly proven that Celtic presence and influences were most substantial in what is today Spain and Portugal (with perhaps the highest settlement saturation in Western Europe), particularly in the central, western and northern regions.\n\nIn addition to Gauls infiltrating from the north of the Pyrenees, the Roman and Greek sources mention Celtic populations in three parts of the Iberian Peninsula: the eastern part of the \"Meseta\" (inhabited by the Celtiberians), the southwest (Celtici, in modern-day Alentejo) and the northwest (Gallaecia and Asturias). A modern scholarly review find several archaeological groups of Celts in Spain:\n\n\n\nThe origins of the Celtiberians might provide a key to understanding the Celticisation process in the rest of the Peninsula. The process of Celticisation of the southwestern area of the peninsula by the Keltoi and of the northwestern area is, however, not a simple Celtiberian question. Recent investigations about the Callaici and Bracari in northwestern Portugal are providing new approaches to understanding Celtic culture (language, art and religion) in western Iberia.\n\nJohn T. Koch of Aberystwyth University suggested that Tartessian inscriptions of the 8th century BC might be classified as Celtic. This would mean that Tartessian is the earliest attested trace of Celtic by a margin of more than a century.\n\nThe Canegrate culture represented the first migratory wave of the proto-Celtic population from the northwest part of the Alps that, through the Alpine passes, had already penetrated and settled in the western Po valley between Lake Maggiore and Lake Como (Scamozzina culture). It has also been proposed that a more ancient proto-Celtic presence can be traced back to the beginning of the Middle Bronze Age, when North Westwern Italy appears closely linked regarding the production of bronze artefacts, including ornaments, to the western groups of the Tumulus culture. La Tène cultural material appeared over a large area of mainland Italy, the southernmost example being the Celtic helmet from Canosa di Puglia.\n\nItaly is home to Lepontic, the oldest attested Celtic language (from the 6th century BC). Anciently spoken in Switzerland and in Northern-Central Italy, from the Alps to Umbria. According to the \"Recueil des Inscriptions Gauloises\", more than 760 Gaulish inscriptions have been found throughout present-day France – with the notable exception of Aquitaine – and in Italy, which testifies the importance of Celtic heritage in the peninsula.\n\nIn 391 BC, Celts \"who had their homes beyond the Alps streamed through the passes in great strength and seized the territory that lay between the Apennine mountains and the Alps\" according to Diodorus Siculus. The Po Valley and the rest of northern Italy (known to the Romans as Cisalpine Gaul) was inhabited by Celtic-speakers who founded cities such as Milan. Later the Roman army was routed at the battle of Allia and Rome was sacked in 390 BC by the Senones.\n\nAt the battle of Telamon in 225 BC, a large Celtic army was trapped between two Roman forces and crushed.\n\nThe defeat of the combined Samnite, Celtic and Etruscan alliance by the Romans in the Third Samnite War sounded the beginning of the end of the Celtic domination in mainland Europe, but it was not until 192 BC that the Roman armies conquered the last remaining independent Celtic kingdoms in Italy.\n\nThe Celts also expanded down the Danube river and its tributaries. One of the most influential tribes, the Scordisci, had established their capital at Singidunum in the 3rd century BC, which is present-day Belgrade, Serbia. The concentration of hill-forts and cemeteries shows a density of population in the Tisza valley of modern-day Vojvodina, Serbia, Hungary and into Ukraine. Expansion into Romania was however blocked by the Dacians.\n\nFurther south, Celts settled in Thrace (Bulgaria), which they ruled for over a century, and Anatolia, where they settled as the Galatians \"(see also: Gallic Invasion of Greece)\". Despite their geographical isolation from the rest of the Celtic world, the Galatians maintained their Celtic language for at least 700 years. St Jerome, who visited Ancyra (modern-day Ankara) in 373 AD, likened their language to that of the Treveri of northern Gaul.\n\nFor Venceslas Kruta, Galatia in central Turkey was an area of dense Celtic settlement.\n\nThe Boii tribe gave their name to Bohemia, Bologna and possibly Bavaria, and Celtic artefacts and cemeteries have been discovered further east in what is now Poland and Slovakia. A Celtic coin (Biatec) from Bratislava's mint was displayed on the old Slovak 5-crown coin.\n\nAs there is no archaeological evidence for large-scale invasions in some of the other areas, one current school of thought holds that Celtic language and culture spread to those areas by contact rather than invasion. However, the Celtic invasions of Italy and the expedition in Greece and western Anatolia, are well documented in Greek and Latin history.\n\nThere are records of Celtic mercenaries in Egypt serving the Ptolemies. Thousands were employed in 283–246 BC and they were also in service around 186 BC. They attempted to overthrow Ptolemy II.\n\nAll Celtic languages extant today belong to the Insular Celtic languages, derived from the Celtic languages spoken in Iron Age Britain and Ireland. They were separated into a Goidelic and a Brythonic branch from an early period.\n\nLinguists have been arguing for many years whether a Celtic language came to Britain and Ireland and then split or whether there were two separate \"invasions\". The older view of prehistorians was that the Celtic influence in the British Isles was the result of successive invasions from the European continent by diverse Celtic-speaking peoples over the course of several centuries, accounting for the P-Celtic vs. Q-Celtic isogloss. This view has been challenged by the hypothesis that the Celtic languages of the British Isles form a phylogenetic Insular Celtic dialect group.\n\nIn the 19th and 20th centuries, scholars commonly dated the \"arrival\" of Celtic culture in Britain (via an invasion model) to the 6th century BC, corresponding to archaeological evidence of Hallstatt influence and the appearance of chariot burials in what is now England. Some Iron Age migration does seem to have occurred but the nature of the interactions with the indigenous populations of the isles is unknown. In the late Iron Age. According to this model, by about the 6th century (Sub-Roman Britain), most of the inhabitants of the Isles were speaking Celtic languages of either the Goidelic or the Brythonic branch. Since the late 20th century, a new model has emerged (championed by archaeologists such as Barry Cunliffe and Celtic historians such as John T. Koch) which places the emergence of Celtic culture in Britain much earlier, in the Bronze Age, and credits its spread not to invasion, but due to a gradual emergence \"in situ\" out of Proto-Indo-European culture (perhaps introduced to the region by the Bell Beaker People, and enabled by an extensive network of contacts that existed between the peoples of Britain and Ireland and those of the Atlantic seaboard.\n\nIt should be noted, however, that classical writers did not apply the terms or “Celtae” to the inhabitants of Britain or Ireland, leading a number of scholars to question the use of the term Celt to describe the Iron Age inhabitants of those islands. The first historical account of the islands of Britain and Ireland was by Pytheas, a Greek from the city of Massalia, who around 310-306 BC, sailed around what he called the \"Pretannikai nesoi\", which can be translated as the \"Pretannic Isles\". In general, classical writers referred to the inhabitants of Britain as Pretannoi or Britanni.\nStrabo, writing in the Roman era, clearly distinguished between the Celts and Britons.\n\nUnder Caesar the Romans conquered Celtic Gaul, and from Claudius onward the Roman empire absorbed parts of Britain. Roman local government of these regions closely mirrored pre-Roman tribal boundaries, and archaeological finds suggest native involvement in local government.\n\nThe native peoples under Roman rule became Romanised and keen to adopt Roman ways. Celtic art had already incorporated classical influences, and surviving Gallo-Roman pieces interpret classical subjects or keep faith with old traditions despite a Roman overlay.\n\nThe Roman occupation of Gaul, and to a lesser extent of Britain, led to Roman-Celtic syncretism. In the case of the continental Celts, this eventually resulted in a language shift to Vulgar Latin, while the Insular Celts retained their language.\n\nThere was also considerable cultural influence exerted by Gaul on Rome, particularly in military matters and horsemanship, as the Gauls often served in the Roman cavalry. The Romans adopted the Celtic cavalry sword, the spatha, and Epona, the Celtic horse goddess.\n\nTo the extent that sources are available, they depict a pre-Christian Iron Age Celtic social structure based formally on class and kingship, although this may only have been a particular late phase of organization in Celtic societies. Patron-client relationships similar to those of Roman society are also described by Caesar and others in the Gaul of the 1st century BC.\n\nIn the main, the evidence is of tribes being led by kings, although some argue that there is also evidence of oligarchical republican forms of government eventually emerging in areas which had close contact with Rome. Most descriptions of Celtic societies portray them as being divided into three groups: a warrior aristocracy; an intellectual class including professions such as druid, poet, and jurist; and everyone else. In historical times, the offices of high and low kings in Ireland and Scotland were filled by election under the system of tanistry, which eventually came into conflict with the feudal principle of primogeniture in which succession goes to the first-born son.\n\nLittle is known of family structure among the Celts. Patterns of settlement varied from decentralised to urban. The popular stereotype of non-urbanised societies settled in hillforts and duns, drawn from Britain and Ireland (there are about 3,000 hill forts known in Britain) contrasts with the urban settlements present in the core Hallstatt and La Tène areas, with the many significant \"oppida\" of Gaul late in the first millennium BC, and with the towns of Gallia Cisalpina.\n\nSlavery, as practised by the Celts, was very likely similar to the better documented practice in ancient Greece and Rome. Slaves were acquired from war, raids, and penal and debt servitude. Slavery was hereditary, though manumission was possible. The Old Irish and Welsh words for ‘slave’, \"cacht\" and \"caeth\" respectively, are cognate with Latin \"captus\" ‘captive’ suggesting that the slave trade was an early means of contact between Latin and Celtic societies. In the Middle Ages, slavery was especially prevalent in the Celtic countries. Manumissions were discouraged by law and the word for \"female slave\", \"cumal\", was used as a general unit of value in Ireland.\n\nArchaeological evidence suggests that the pre-Roman Celtic societies were linked to the network of overland trade routes that spanned Eurasia. Archaeologists have discovered large prehistoric trackways crossing bogs in Ireland and Germany. Due to their substantial nature, these are believed to have been created for wheeled transport as part of an extensive roadway system that facilitated trade. The territory held by the Celts contained tin, lead, iron, silver and gold. Celtic smiths and metalworkers created weapons and jewellery for international trade, particularly with the Romans.\n\nThe myth that the Celtic monetary system consisted of wholly barter is a common one, but is in part false. The monetary system was complex and is still not understood (much like the late Roman coinages), and due to the absence of large numbers of coin items, it is assumed that \"proto-money\" was used. This included bronze items made from the early La Tène period and onwards, which were often in the shape of axeheads, rings, or bells. Due to the large number of these present in some burials, it is thought they had a relatively high monetary value, and could be used for \"day to day\" purchases. Low-value coinages of potin, a bronze alloy with high tin content, were minted in most Celtic areas of the continent and in South-East Britain prior to the Roman conquest of these lands. Higher-value coinages, suitable for use in trade, were minted in gold, silver, and high-quality bronze. Gold coinage was much more common than silver coinage, despite being worth substantially more, as while there were around 100 mines in Southern Britain and Central France, silver was more rarely mined. This was due partly to the relative sparsity of mines and the amount of effort needed for extraction compared to the profit gained. As the Roman civilisation grew in importance and expanded its trade with the Celtic world, silver and bronze coinage became more common. This coincided with a major increase in gold production in Celtic areas to meet the Roman demand, due to the high value Romans put on the metal. The large number of gold mines in France is thought to be a major reason why Caesar invaded.\n\nThere are only very limited records from pre-Christian times written in Celtic languages. These are mostly inscriptions in the Roman and sometimes Greek alphabets. The Ogham script, an Early Medieval alphabet, was mostly used in early Christian times in Ireland and Scotland (but also in Wales and England), and was only used for ceremonial purposes such as inscriptions on gravestones. The available evidence is of a strong oral tradition, such as that preserved by bards in Ireland, and eventually recorded by monasteries. Celtic art also produced a great deal of intricate and beautiful metalwork, examples of which have been preserved by their distinctive burial rites.\n\nIn some regards the Atlantic Celts were conservative: for example, they still used chariots in combat long after they had been reduced to ceremonial roles by the Greeks and Romans. However, despite being outdated, Celtic chariot tactics were able to repel the invasion of Britain attempted by Julius Caesar.\n\nAccording to Diodorus Siculus:\n\nDuring the later Iron Age the Gauls generally wore long-sleeved shirts or tunics and long trousers (called \"braccae\" by the Romans). Clothes were made of wool or linen, with some silk being used by the rich. Cloaks were worn in the winter. Brooches and armlets were used, but the most famous item of jewellery was the torc, a neck collar of metal, sometimes gold. The horned Waterloo Helmet in the British Museum, which long set the standard for modern images of Celtic warriors, is in fact a unique survival, and may have been a piece for ceremonial rather than military wear.\n\nAccording to Aristotle, most \"belligerent nations\" were strongly influenced by their women, but the Celts were unusual because their men openly preferred male lovers (\"Politics\" II 1269b). H. D. Rankin in \"Celts and the Classical World\" notes that \"Athenaeus echoes this comment (603a) and so does Ammianus (30.9). It seems to be the general opinion of antiquity.\" In book XIII of his \"Deipnosophists\", the Roman Greek rhetorician and grammarian Athenaeus, repeating assertions made by Diodorus Siculus in the 1st century BC (Bibliotheca historica 5:32), wrote that Celtic women were beautiful but that the men preferred to sleep together. Diodorus went further, stating that \"the young men will offer themselves to strangers and are insulted if the offer is refused\". Rankin argues that the ultimate source of these assertions is likely to be Posidonius and speculates that these authors may be recording male \"bonding rituals\".\n\nThe sexual freedom of women in Britain was noted by Cassius Dio:\n\nThere are instances recorded where women participated both in warfare and in kingship, although they were in the minority in these areas. Plutarch reports that Celtic women acted as ambassadors to avoid a war among Celts chiefdoms in the Po valley during the 4th century BC.\n\nVery few reliable sources exist regarding Celtic views on gender divisions and societal status, though some archaeological evidence does suggest that their views of gender roles may differ from contemporary and less egalitarian classical counterparts of the Roman era. There are some general indications from Iron Age burial sites in the Champagne and Bourgogne regions of Northeastern France suggesting that women may have had roles in combat during the earlier \"La Tène\" period. However, the evidence is far from conclusive. Examples of individuals buried with both female jewellery and weaponry have been identified, such as the Vix Grave, and there are questions about the gender of some skeletons that were buried with warrior assemblages. However, it has been suggested that \"the weapons may indicate rank instead of masculinity\".\n\nAmong the insular Celts, there is a greater amount of historic documentation to suggest warrior roles for women. In addition to commentary by Tacitus about Boudica, there are indications from later period histories that also suggest a more substantial role for \"women as warriors\", in symbolic if not actual roles.\nPosidonius and Strabo described an island of women where men could not venture for fear of death, and where the women ripped each other apart. Other writers, such as Ammianus Marcellinus and Tacitus, mentioned Celtic women inciting, participating in, and leading battles. Posidonius' anthropological comments on the Celts had common themes, primarily primitivism, extreme ferocity, cruel sacrificial practices, and the strength and courage of their women.\n\nUnder Brehon Law, which was written down in early Medieval Ireland after conversion to Christianity, a woman had the right to divorce her husband and gain his property if he was unable to perform his marital duties due to impotence, obesity, homosexual inclination or preference for other women.\n\nCeltic art is generally used by art historians to refer to art of the La Tène period across Europe, while the Early Medieval art of Britain and Ireland, that is what \"Celtic art\" evokes for much of the general public, is called Insular art in art history. Both styles absorbed considerable influences from non-Celtic sources, but retained a preference for geometrical decoration over figurative subjects, which are often extremely stylised when they do appear; narrative scenes only appear under outside influence. Energetic circular forms, triskeles and spirals are characteristic. Much of the surviving material is in precious metal, which no doubt gives a very unrepresentative picture, but apart from Pictish stones and the Insular high crosses, large monumental sculpture, even with decorative carving, is very rare; possibly it was originally common in wood. Celts were also able to create developed musical instruments such as the carnyces, these famous war trumpets used before the battle to frighten the enemy, as the best preserved found in Tintignac (Gaul) in 2004 and which were decorated with a boar head or a snake head.\n\nThe interlace patterns that are often regarded as typical of \"Celtic art\" were characteristic of the whole of the British Isles, a style referred to as Insular art, or Hiberno-Saxon art. This artistic style incorporated elements of La Tène, Late Roman, and, most importantly, animal Style II of Germanic Migration Period art. The style was taken up with great skill and enthusiasm by Celtic artists in metalwork and illuminated manuscripts. Equally, the forms used for the finest Insular art were all adopted from the Roman world: Gospel books like the Book of Kells and Book of Lindisfarne, chalices like the Ardagh Chalice and Derrynaflan Chalice, and penannular brooches like the Tara Brooch. These works are from the period of peak achievement of Insular art, which lasted from the 7th to the 9th centuries, before the Viking attacks sharply set back cultural life.\n\nIn contrast the less well known but often spectacular art of the richest earlier Continental Celts, before they were conquered by the Romans, often adopted elements of Roman, Greek and other \"foreign\" styles (and possibly used imported craftsmen) to decorate objects that were distinctively Celtic. After the Roman conquests, some Celtic elements remained in popular art, especially Ancient Roman pottery, of which Gaul was actually the largest producer, mostly in Italian styles, but also producing work in local taste, including figurines of deities and wares painted with animals and other subjects in highly formalised styles. Roman Britain also took more interest in enamel than most of the Empire, and its development of champlevé technique was probably important to the later Medieval art of the whole of Europe, of which the energy and freedom of Insular decoration was an important element. Rising nationalism brought Celtic revivals from the 19th century.\n\nTribal warfare appears to have been a regular feature of Celtic societies. While epic literature depicts this as more of a sport focused on raids and hunting rather than organised territorial conquest, the historical record is more of tribes using warfare to exert political control and harass rivals, for economic advantage, and in some instances to conquer territory.\n\nThe Celts were described by classical writers such as Strabo, Livy, Pausanias, and Florus as fighting like \"wild beasts\", and as hordes. Dionysius said that their Such descriptions have been challenged by contemporary historians.\n\nPolybius (2.33) indicates that the principal Celtic weapon was a long bladed sword which was used for hacking edgewise rather than stabbing. Celtic warriors are described by Polybius and Plutarch as frequently having to cease fighting in order to straighten their sword blades. This claim has been questioned by some archaeologists, who note that Noric steel, steel produced in Celtic Noricum, was famous in the Roman Empire period and was used to equip the Roman military. However, Radomir Pleiner, in \"The Celtic Sword\" (1993) argues that \"the metallographic evidence shows that Polybius was right up to a point\", as around one third of surviving swords from the period might well have behaved as he describes.\n\nPolybius also asserts that certain of the Celts fought naked, \"The appearance of these naked warriors was a terrifying spectacle, for they were all men of splendid physique and in the prime of life.\" According to Livy, this was also true of the Celts of Asia Minor.\n\nCelts had a reputation as head hunters. According to Paul Jacobsthal, \"Amongst the Celts the human head was venerated above all else, since the head was to the Celt the soul, centre of the emotions as well as of life itself, a symbol of divinity and of the powers of the other-world.\" Arguments for a Celtic cult of the severed head include the many sculptured representations of severed heads in La Tène carvings, and the surviving Celtic mythology, which is full of stories of the severed heads of heroes and the saints who carry their own severed heads, right down to \"Sir Gawain and the Green Knight\", where the Green Knight picks up his own severed head after Gawain has struck it off, just as St. Denis carried his head to the top of Montmartre.\n\nA further example of this regeneration after beheading lies in the tales of Connemara's St. Feichin, who after being beheaded by Viking pirates carried his head to the Holy Well on Omey Island and on dipping the head into the well placed it back upon his neck and was restored to full health.\n\nDiodorus Siculus, in his 1st-century \"History\" had this to say about Celtic head-hunting:\nIn \"Gods and Fighting Men\", Lady Gregory's Celtic Revival translation of Irish mythology, heads of men killed in battle are described in the beginning of the story \"The Fight with the Fir Bolgs\" as pleasing to Macha, one aspect of the war goddess Morrigu.\n\nLike other European Iron Age tribal societies, the Celts practised a polytheistic religion.\nMany Celtic gods are known from texts and inscriptions from the Roman period.\nRites and sacrifices were carried out by priests known as druids. The Celts did not see their gods as having human shapes until late in the Iron Age. Celtic shrines were situated in remote areas such as hilltops, groves, and lakes.\n\nCeltic religious patterns were regionally variable; however, some patterns of deity forms, and ways of worshipping these deities, appeared over a wide geographical and temporal range. The Celts worshipped both gods and goddesses. In general, Celtic gods were deities of particular skills, such as the many-skilled Lugh and Dagda, while goddesses were associated with natural features, particularly rivers (such as Boann, goddess of the River Boyne). This was not universal, however, as goddesses such as Brighid and The Morrígan were associated with both natural features (holy wells and the River Unius) and skills such as blacksmithing and healing.\n\nTriplicity is a common theme in Celtic cosmology, and a number of deities were seen as threefold. This trait is exhibited by The Three Mothers, a group of goddesses worshipped by many Celtic tribes (with regional variations).\n\nThe Celts had hundreds of deities, some of which were unknown outside a single family or tribe, while others were popular enough to have a following that crossed lingual and cultural barriers. For instance, the Irish god Lugh, associated with storms, lightning, and culture, is seen in similar forms as Lugos in Gaul and Lleu in Wales. Similar patterns are also seen with the continental Celtic horse goddess Epona and what may well be her Irish and Welsh counterparts, Macha and Rhiannon, respectively.\n\nRoman reports of the druids mention ceremonies being held in sacred groves. La Tène Celts built temples of varying size and shape, though they also maintained shrines at sacred trees and votive pools.\n\nDruids fulfilled a variety of roles in Celtic religion, serving as priests and religious officiants, but also as judges, sacrificers, teachers, and lore-keepers. Druids organised and ran religious ceremonies, and they memorised and taught the calendar. Other classes of druids performed ceremonial sacrifices of crops and animals for the perceived benefit of the community.\n\nThe Coligny calendar, which was found in 1897 in Coligny, Ain, was engraved on a bronze tablet, preserved in 73 fragments, that originally was wide and high (Lambert p. 111). Based on the style of lettering and the accompanying objects, it probably dates to the end of the 2nd century. It is written in Latin inscriptional capitals, and is in the Gallic language. The restored tablet contains 16 vertical columns, with 62 months distributed over 5 years.\n\nThe French archaeologist J. Monard speculated that it was recorded by druids wishing to preserve their tradition of timekeeping in a time when the Julian calendar was imposed throughout the Roman Empire. However, the general form of the calendar suggests the public peg calendars (or \"parapegmata\") found throughout the Greek and Roman world.\n\nThe Roman invasion of Gaul brought a great deal of Celtic peoples into the Roman Empire. Roman culture had a profound effect on the Celtic tribes which came under the empire's control. Roman influence led to many changes in Celtic religion, the most noticeable of which was the weakening of the druid class, especially religiously; the druids were to eventually disappear altogether. Romano-Celtic deities also began to appear: these deities often had both Roman and Celtic attributes, combined the names of Roman and Celtic deities, and/or included couples with one Roman and one Celtic deity. Other changes included the adaptation of the Jupiter Column, a sacred column set up in many Celtic regions of the empire, primarily in northern and eastern Gaul. Another major change in religious practice was the use of stone monuments to represent gods and goddesses. The Celts had only created wooden idols (including monuments carved into trees, which were known as sacred poles) previously to Roman conquest.\n\nWhile the regions under Roman rule adopted Christianity along with the rest of the Roman empire, unconquered areas of Ireland and Scotland began to move from Celtic polytheism to Christianity in the 5th century. Ireland was converted by missionaries from Britain, such as Saint Patrick. Later missionaries from Ireland were a major source of missionary work in Scotland, Anglo-Saxon parts of Britain, and central Europe (see Hiberno-Scottish mission). Celtic Christianity, the forms of Christianity that took hold in Britain and Ireland at this time, had for some centuries only limited and intermittent contact with Rome and continental Christianity, as well as some contacts with Coptic Christianity. Some elements of Celtic Christianity developed, or retained, features that made them distinct from the rest of Western Christianity, most famously their conservative method of calculating the date of Easter. In 664, the Synod of Whitby began to resolve these differences, mostly by adopting the current Roman practices, which the Gregorian Mission from Rome had introduced to Anglo-Saxon England.\n\n\n\nGeography\n\nOrganisations\n"}
{"id": "6547", "url": "https://en.wikipedia.org/wiki?curid=6547", "title": "Conductor", "text": "Conductor\n\nConductor or conduction may refer to:\n\n\n\n\n"}
{"id": "6548", "url": "https://en.wikipedia.org/wiki?curid=6548", "title": "Claude Monet", "text": "Claude Monet\n\nOscar-Claude Monet (; ; 14 November 1840 – 5 December 1926) was a founder of French Impressionist painting, and the most consistent and prolific practitioner of the movement's philosophy of expressing one's perceptions before nature, especially as applied to plein-air landscape painting. The term \"Impressionism\" is derived from the title of his painting \"Impression, soleil levant\" (\"Impression, Sunrise\"), which was exhibited in 1874 in the first of the independent exhibitions mounted by Monet and his associates as an alternative to the Salon de Paris.\n\nMonet's ambition of documenting the French countryside led him to adopt a method of painting the same scene many times in order to capture the changing of light and the passing of the seasons. From 1883 Monet lived in Giverny, where he purchased a house and property and began a vast landscaping project which included lily ponds that would become the subjects of his best-known works. In 1899 he began painting the water lilies, first in vertical views with a Japanese bridge as a central feature, and later in the series of large-scale paintings that was to occupy him continuously for the next 20 years of his life.\n\nClaude Monet was born on 14 November 1840 on the fifth floor of 45 rue Laffitte, in the 9th arrondissement of Paris. He was the second son of Claude Adolphe Monet and Louise Justine Aubrée Monet, both of them second-generation Parisians. On 20 May 1841, he was baptized in the local parish church, Notre-Dame-de-Lorette, as Oscar-Claude, but his parents called him simply Oscar. (He signed his juvenilia \"O. Monet\".) Despite being baptized Catholic, Monet later became an atheist.\n\nIn 1845, his family moved to Le Havre in Normandy. His father wanted him to go into the family's ship-chandling and grocery business, but Monet wanted to become an artist. His mother was a singer, and supported Monet's desire for a career in art.\n\nOn 1 April 1851, Monet entered Le Havre secondary school of the arts. Locals knew him well for his charcoal caricatures, which he would sell for ten to twenty francs. Monet also undertook his first drawing lessons from Jacques-François Ochard, a former student of Jacques-Louis David. On the beaches of Normandy around 1856 he met fellow artist Eugène Boudin, who became his mentor and taught him to use oil paints. Boudin taught Monet \"en plein air\" (outdoor) techniques for painting. Both received the influence of Johan Barthold Jongkind.\n\nOn 28 January 1857, his mother died. At the age of sixteen, he left school and went to live with his widowed, childless aunt, Marie-Jeanne Lecadre.\nWhen Monet traveled to Paris to visit the Louvre, he witnessed painters copying from the old masters. Having brought his paints and other tools with him, he would instead go and sit by a window and paint what he saw. Monet was in Paris for several years and met other young painters, including Édouard Manet and others who would become friends and fellow Impressionists.\n\nAfter drawing a low ballot number in March 1861, Monet was drafted into the First Regiment of African Light Cavalry (\"Chasseurs d'Afrique\") in Algeria for a seven-year period of military service. His prosperous father could have purchased Monet's exemption from conscription but declined to do so when his son refused to give up painting. While in Algeria Monet did only a few sketches of casbah scenes, a single landscape, and several portraits of officers, all of which have been lost. In a \"Le Temps\" interview of 1900 however he commented that the light and vivid colours of North Africa \"contained the germ of my future researches\". After about a year of garrison duty in Algiers, Monet contracted typhoid fever and briefly went absent without leave. Following convalescence, Monet's aunt intervened to get him out of the army if he agreed to complete a course at an art school. It is possible that the Dutch painter Johan Barthold Jongkind, whom Monet knew, may have prompted his aunt on this matter.\n\nDisillusioned with the traditional art taught at art schools, in 1862 Monet became a student of Charles Gleyre in Paris, where he met Pierre-Auguste Renoir, Frédéric Bazille and Alfred Sisley. Together they shared new approaches to art, painting the effects of light \"en plein air\" with broken colour and rapid brushstrokes, in what later came to be known as Impressionism.\n\nIn January 1865 Monet was working on a version of \"Le déjeuner sur l'herbe\", aiming to present it for hanging at the Salon, which had rejected Manet's \"Le déjeuner sur l'herbe\" two years earlier. Monet's painting was very large and could not be completed in time. (It was later cut up, with parts now in different galleries.) Monet submitted instead a painting of \"Camille\" or \"The Woman in the Green Dress\" (\"La femme à la robe verte\"), one of many works using his future wife, Camille Doncieux, as his model. Both this painting and a small landscape were hung. The following year Monet used Camille for his model in \"Women in the Garden\", and \"On the Bank of the Seine, Bennecourt\" in 1868. Camille became pregnant and gave birth to their first child, Jean, in 1867. Monet and Camille married on 28 June 1870, just before the outbreak of the Franco-Prussian War, and, after their excursion to London and Zaandam, they moved to Argenteuil, in December 1871. During this time Monet painted various works of modern life. He and Camille lived in poverty for most of this period. Following the successful exhibition of some maritime paintings, and the winning of a silver medal at Le Havre, Monet's paintings were seized by creditors, from whom they were bought back by a shipping merchant, Gaudibert, who was also a patron of Boudin.\n\nFrom the late 1860s, Monet and other like-minded artists met with rejection from the conservative Académie des Beaux-Arts, which held its annual exhibition at the Salon de Paris. During the latter part of 1873, Monet, Pierre-Auguste Renoir, Camille Pissarro, and Alfred Sisley organized the (Anonymous Society of Painters, Sculptors, and Engravers) to exhibit their artworks independently. At their first exhibition, held in April 1874, Monet exhibited the work that was to give the group its lasting name. He was inspired by the style and subject matter of previous modern painters Camille Pissarro and Edouard Manet.\n\n\"Impression, Sunrise\" was painted in 1872, depicting a Le Havre port landscape. From the painting's title the art critic Louis Leroy, in his review, \"L'Exposition des Impressionnistes,\" which appeared in \"Le Charivari\", coined the term \"Impressionism\". It was intended as disparagement but the Impressionists appropriated the term for themselves.\n\nAfter the outbreak of the Franco-Prussian War (19 July 1870), Monet and his family took refuge in England in September 1870, where he studied the works of John Constable and Joseph Mallord William Turner, both of whose landscapes would serve to inspire Monet's innovations in the study of colour. In the spring of 1871, Monet's works were refused authorisation for inclusion in the Royal Academy exhibition.\n\nIn May 1871, he left London to live in Zaandam, in the Netherlands, where he made twenty-five paintings (and the police suspected him of revolutionary activities). He also paid a first visit to nearby Amsterdam. In October or November 1871, he returned to France. From December 1871 to 1878 he lived at Argenteuil, a village on the right bank of the Seine river near Paris, and a popular Sunday-outing destination for Parisians, where he painted some of his best-known works. In 1873, Monet purchased a small boat equipped to be used as a floating studio. From the boat studio Monet painted landscapes and also portraits of Édouard Manet and his wife; Manet in turn depicted Monet painting aboard the boat, accompanied by Camille, in 1874. In 1874, he briefly returned to Holland.\n\nThe first Impressionist exhibition was held in 1874 at 35 boulevard des Capucines, Paris, from 15 April to 15 May. The primary purpose of the participants was not so much to promote a new style, but to free themselves from the constraints of the Salon de Paris. The exhibition, open to anyone prepared to pay 60 francs, gave artists the opportunity to show their work without the interference of a jury.\n\nRenoir chaired the hanging committee and did most of the work himself, as others members failed to present themselves.\n\nIn addition to \"\" (pictured above), Monet presented four oil paintings and seven pastels. Among the paintings he displayed was \"The Luncheon\" (1868), which features Camille Doncieux and Jean Monet, and which had been rejected by the Paris Salon of 1870. Also in this exhibition was a painting titled \"Boulevard des Capucines\", a painting of the boulevard done from the photographer Nadar's apartment at no. 35. Monet painted the subject twice, and it is uncertain which of the two pictures, that now in the Pushkin Museum in Moscow, or that in the Nelson-Atkins Museum of Art in Kansas City, was the painting that appeared in the groundbreaking 1874 exhibition, though more recently the Moscow picture has been favoured. Altogether, 165 works were exhibited in the exhibition, including 4 oils, 2 pastels and 3 watercolours by Morisot; 6 oils and 1 pastel by Renoir; 10 works by Degas; 5 by Pissarro; 3 by Cézanne; and 3 by Guillaumin. Several works were on loan, including Cézanne's \"Modern Olympia\", Morisot's \"Hide and Seek\" (owned by Manet) and 2 landscapes by Sisley that had been purchased by Durand-Ruel.\n\nThe total attendance is estimated at 3500, and some works did sell, though some exhibitors had placed their prices too high. Pissarro was asking 1000 francs for \"The Orchard\" and Monet the same for \"Impression: Sunrise\", neither of which sold. Renoir failed to obtain the 500 francs he was asking for \"La Loge\", but later sold it for 450 francs to Père Martin, dealer and supporter of the group.\n\nIn 1876, Camille Monet became ill with tuberculosis. Their second son, Michel, was born on 17 March 1878. This second child weakened her already fading health. In the summer of that year, the family moved to the village of Vétheuil where they shared a house with the family of Ernest Hoschedé, a wealthy department store owner and patron of the arts. In 1878, Camille Monet was diagnosed with uterine cancer, and she died on 5 September 1879 at the age of thirty-two.\n\nMonet made a study in oils of his dead wife. Many years later, Monet confessed to his friend Georges Clemenceau that his need to analyse colours was both the joy and torment of his life. He explained,\n\nI one day found myself looking at my beloved wife's dead face and just systematically noting the colours according to an automatic reflex!\n\nJohn Berger describes the work as \"a blizzard of white, grey, purplish paint ... a terrible blizzard of loss which will forever efface her features. In fact there can be very few death-bed paintings which have been so intensely felt or subjectively expressive.\"\n\nAfter several difficult months following the death of Camille, Monet began to create some of his best paintings of the 19th century. During the early 1880s, Monet painted several groups of landscapes and seascapes in what he considered to be campaigns to document the French countryside. These began to evolve into series of pictures in which he documented the same scene many times in order to capture the changing of light and the passing of the seasons.\n\nMonet's friend Ernest Hoschedé became bankrupt, and left in 1878 for Belgium. After the death of Camille Monet in September 1879, and while Monet continued to live in the house in Vétheuil, Alice Hoschedé helped Monet to raise his two sons, Jean and Michel. She took them to Paris to live alongside her own six children, Blanche (who married Jean Monet), Germaine, Suzanne, Marthe, Jean-Pierre, and Jacques. In the spring of 1880, Alice Hoschedé and all the children left Paris and rejoined Monet at Vétheuil. In 1881, all of them moved to Poissy, which Monet hated. In April 1883, looking out the window of the little train between Vernon and Gasny, he discovered Giverny in Normandy. Monet, Alice Hoschedé and the children moved to Vernon, then to the house in Giverny, where he planted a large garden and where he painted for much of the rest of his life. Following the death of her estranged husband, Monet married Alice Hoschedé in 1892.\n\nMonet rented and eventually purchased a house and gardens in Giverny. At the beginning of May 1883, Monet and his large family rented the home and from a local landowner. The house was situated near the main road between the towns of Vernon and Gasny at Giverny. There was a barn that doubled as a painting studio, orchards and a small garden. The house was close enough to the local schools for the children to attend, and the surrounding landscape offered many suitable motifs for Monet's work.\n\nThe family worked and built up the gardens, and Monet's fortunes began to change for the better as his dealer, Paul Durand-Ruel, had increasing success in selling his paintings. By November 1890, Monet was prosperous enough to buy the house, the surrounding buildings and the land for his gardens. During the 1890s, Monet built a greenhouse and a second studio, a spacious building well lit with skylights.\n\nMonet wrote daily instructions to his gardener, precise designs and layouts for plantings, and invoices for his floral purchases and his collection of botany books. As Monet's wealth grew, his garden evolved. He remained its architect, even after he hired seven gardeners.\n\nMonet purchased additional land with a water meadow. In 1893 he began a vast landscaping project which included lily ponds that would become the subjects of his best-known works. White water lilies local to France were planted along with imported cultivars from South America and Egypt, resulting in a range of colours including yellow, blue and white lilies that turned pink with age. In 1899 he began painting the water lilies, first in vertical views with a Japanese bridge as a central feature, and later in the series of large-scale paintings that was to occupy him continuously for the next 20 years of his life. This scenery, with its alternating light and mirror-like reflections, became an integral part of his work. By the mid-1910s Monet had achieved:\n\nMonet's second wife, Alice, died in 1911, and his oldest son Jean, who had married Alice's daughter Blanche, Monet's particular favourite, died in 1914. After Alice died, Blanche looked after and cared for Monet. It was during this time that Monet began to develop the first signs of cataracts.\n\nDuring World War I, in which his younger son Michel served and his friend and admirer Georges Clemenceau led the French nation, Monet painted a series of weeping willow trees as homage to the French fallen soldiers. In 1923, he underwent two operations to remove his cataracts. The paintings done while the cataracts affected his vision have a general reddish tone, which is characteristic of the vision of cataract victims. It may also be that after surgery he was able to see certain ultraviolet wavelengths of light that are normally excluded by the lens of the eye; this may have had an effect on the colours he perceived. After his operations he even repainted some of these paintings, with bluer water lilies than before.\n\nMonet died of lung cancer on 5 December 1926 at the age of 86 and is buried in the Giverny church cemetery. Monet had insisted that the occasion be simple; thus only about fifty people attended the ceremony.\n\nHis home, garden, and waterlily pond were bequeathed by his son Michel, his only heir, to the French Academy of Fine Arts (part of the Institut de France) in 1966. Through the \"Fondation Claude Monet\", the house and gardens were opened for visits in 1980, following restoration. In addition to souvenirs of Monet and other objects of his life, the house contains his collection of Japanese woodcut prints. The house and garden, along with the Museum of Impressionism, are major attractions in Giverny, which hosts tourists from all over the world.\n\nMonet has been described as \"the driving force behind Impressionism\". Crucial to the art of the Impressionist painters was the understanding of the effects of light on the local colour of objects, and the effects of the juxtaposition of colours with each other. Monet's long career as a painter was spent in the pursuit of this aim.\n\nIn 1856, his chance meeting with Eugene Boudin, a painter of small beach scenes, opened his eyes to the possibility of plein-air painting. From that time, with a short interruption for military service, he dedicated himself to searching for new and improved methods of painterly expression. To this end, as a young man, he visited the Paris Salon and familiarised himself with the works of older painters, and made friends with other young artists. The five years that he spent at Argenteuil, spending much time on the River Seine in a little floating studio, were formative in his study of the effects of light and reflections. He began to think in terms of colours and shapes rather than scenes and objects. He used bright colours in dabs and dashes and squiggles of paint. Having rejected the academic teachings of Gleyre's studio, he freed himself from theory, saying \"I like to paint as a bird sings.\"\n\nIn 1877 a series of paintings at St-Lazare Station had Monet looking at smoke and steam and the way that they affected colour and visibility, being sometimes opaque and sometimes translucent. He was to further use this study in the painting of the effects of mist and rain on the landscape. The study of the effects of atmosphere was to evolve into a number of series of paintings in which Monet repeatedly painted the same subject in different lights, at different hours of the day, and through the changes of weather and season. This process began in the 1880s and continued until the end of his life in 1926.\n\nHis first series exhibited as such was of Haystacks, painted from different points of view and at different times of the day. Fifteen of the paintings were exhibited at the Galerie Durand-Ruel in 1891. In 1892 he produced what is probably his best-known series, twenty-six views of \"Rouen Cathedral\". In these paintings Monet broke with painterly traditions by cropping the subject so that only a portion of the façade is seen on the canvas. The paintings do not focus on the grand Medieval building, but on the play of light and shade across its surface, transforming the solid masonry.\n\nOther series include \"Poplars\", \"Mornings on the Seine\", and the \"Water Lilies\" that were painted on his property at Giverny. Between 1883 and 1908, Monet traveled to the Mediterranean, where he painted landmarks, landscapes, and seascapes, including a series of paintings in Venice. In London he painted four series: \"the Houses of Parliament, London\", \"Charing Cross Bridge\", \"Waterloo Bridge\", and \"Views of Westminster Bridge\". Helen Gardner writes:\n\nIn 2004, \"London, the Parliament, Effects of Sun in the Fog (Londres, le Parlement, trouée de soleil dans le brouillard)\" (1904), sold for US$20.1 million. In 2006, the journal \"Proceedings of the Royal Society\" published a paper providing evidence that these were painted in situ at St Thomas' Hospital over the river Thames.\n\n\"Falaises près de Dieppe (Cliffs near Dieppe)\" has been stolen on two separate occasions: once in 1998 (in which the museum's curator was convicted of the theft and jailed for five years and two months along with two accomplices) and most recently in August 2007. It was recovered in June 2008.\n\nMonet's \"Le Pont du chemin de fer à Argenteuil\", an 1873 painting of a railway bridge spanning the Seine near Paris, was bought by an anonymous telephone bidder for a record $41.4 million at Christie's auction in New York on 6 May 2008. The previous record for his painting stood at $36.5 million. Just a few weeks later, \"Le bassin aux nymphéas\" (from the water lilies series) sold at Christie's 24 June 2008 auction in London, lot 19, for £36,500,000 ($71,892,376.34) (hammer price) or £40,921,250 ($80,451,178) with fees, nearly doubling the record for the artist and representing one of the top 20 highest prices paid for a painting at the time.\n\nIn October 2013, Monet's paintings, \"L'Eglise de Vetheuil\" and \"Le Bassin aux Nymphease\", became subjects of a legal case in New York against NY-based Vilma Bautista, one-time aide to Imelda Marcos, wife of dictator Ferdinand Marcos, after she sold \"Le Bassin aux Nymphease\" for $32 million to a Swiss buyer. The said Monet paintings, along with two others, were acquired by Imelda during her husband's presidency and allegedly bought using the nation's funds. Bautista's lawyer claimed that the aide sold the painting for Imelda but did not have a chance to give her the money. The Philippine government seeks the return of the painting. \"Le Bassin aux Nymphease\", also known as \"Japanese Footbridge over the Water-Lily Pond at Giverny\", is part of Monet's famed Water Lilies series.\n\n\n"}
{"id": "6552", "url": "https://en.wikipedia.org/wiki?curid=6552", "title": "Conectiva", "text": "Conectiva\n\nConectiva was a company founded on August 28, 1995, in Curitiba, Paraná, Brazil, by a group of friends, among them Arnaldo Carvalho de Melo, who was a pioneer in the distribution of Linux and open source software in Brazilian Portuguese, Spanish and English for all of Latin America. Besides a customized Linux distribution for the Latin American market, Conectiva developed a series of products and additional services directed to meet the market demand for open source tools, including books, manuals, additional software like Linux Tools and embedded systems, OEM programs, applications port, training kits and the \"Revista do Linux\" Linux magazine. In addition, the company provided consulting services, training and technical support in all of Latin America through its own service centers and certified partners.\n\nConectiva also provided development, customization and professional services on a worldwide basis through its team of open source software engineers. Conectiva's development team had expertise in, amongst others, the following areas: Linux kernel development, high availability, device drivers, XFree86, network protocols, firewalling, clustering, performance analysis and optimisation, filesystems and resource management.\n\nOn 24 January 2005 it was announced that Mandrakesoft had acquired Conectiva for 1.79 million euro (2.3 million U.S. dollars at the time). On 7 April 2005 Mandrakesoft announced the decision to change the name of the parent company to Mandriva and their distribution name to Mandriva Linux, although the Brazilian operation would not change its name from Conectiva immediately.\n\n\n\n"}
{"id": "6555", "url": "https://en.wikipedia.org/wiki?curid=6555", "title": "Carthage", "text": "Carthage\n\nCarthage (, from ; ) was the centre or capital city of the ancient Carthaginian civilization, on the eastern side of the Lake of Tunis in what is now the Tunis Governorate in Tunisia.\n\nThe city developed from a Phoenician colony into the capital of an empire dominating the Mediterranean during the first millennium BC. The apocryphal queen Dido is regarded as the founder of the city, though her historicity has been questioned. According to accounts by Timaeus of Tauromenium, she purchased from a local tribe the amount of land that could be covered by an oxhide. Cutting the skin into strips, she laid out her claim and founded an empire that would become, through the Punic Wars, the only existential threat to the Roman Empire until the coming of the Vandals several centuries later.\n\nThe ancient city was destroyed by the Roman Republic in the Third Punic War in 146 BC then re-developed as Roman Carthage, which became the major city of the Roman Empire in the province of Africa. The Roman city was again occupied by the Muslim conquest of the Maghreb, in 698. The site remained uninhabited, the regional power shifting to the Medina of Tunis in the medieval period, until the early 20th century, when it began to develop into a coastal suburb of Tunis, incorporated as Carthage municipality in 1919.\n\nThe archaeological site was first surveyed in 1830, by Danish consul Christian Tuxen Falbe. Excavations were performed in the second half of the 19th century by \nCharles Ernest Beulé and by Alfred Louis Delattre. The Carthage National Museum was founded in 1875 by Cardinal Charles Lavigerie.\nExcavations performed by French archaeologists in the 1920s attracted an extraordinary amount of attention because of the evidence they produced for child sacrifice, in Greco-Roman and Biblical tradition associated with the Canaanite god Baal Hammon. The open-air Carthage Paleo-Christian Museum has exhibits excavated under the auspices of UNESCO from 1975 to 1984.\n\nThe name \"Carthage\" /ˈkarθɪdʒ/ is the Early Modern anglicisation of French \"Carthage\" /kaʁ.taʒ/, \nfrom Latin ', derived via Greek \"Karkhēdōn\" () and Etruscan \"*Carθaza\", from the Punic ' \"new city\",\nimplying it was a \"new Tyre\". The Latin \"\" is an \"n\"-stem, as reflected in the English adjective \"Carthaginian\".\nThe Latin adjective \"pūnicus\", a variant of the word \"Phoenician\", is reflected in English in some borrowings from Latin—notably the Punic Wars and the Punic language.\n\nThe Modern Standard Arabic form (\"\") is an adoption of French \"Carthage\", replacing an older local toponym reported as \"Cartagenna\" that directly continued the Latin name.\n\nCarthage was built on a promontory with sea inlets to the north and the south. The city's location made it master of the Mediterranean's maritime trade. All ships crossing the sea had to pass between Sicily and the coast of Tunisia, where Carthage was built, affording it great power and influence. Two large, artificial harbors were built within the city, one for harboring the city's massive navy of 220 warships and the other for mercantile trade. A walled tower overlooked both harbors. The city had massive walls, in length, longer than the walls of comparable cities. Most of the walls were located on the shore, thus could be less impressive, as Carthaginian control of the sea made attack from that direction difficult. The of wall on the isthmus to the west were truly massive and were never penetrated. The city had a huge necropolis or burial ground, religious area, market places, council house, towers, and a theater, and was divided into four equally sized residential areas with the same layout. Roughly in the middle of the city stood a high citadel called the Byrsa.\n\nCarthage was one of the largest cities of the Hellenistic period and was among the largest cities in preindustrial history. Whereas by AD 14, Rome had at least 750,000 inhabitants and in the following century may have reached 1 million, the cities of Alexandria and Antioch numbered only a few hundred thousand or less. According to the not always reliable history of Herodian, Carthage rivaled Alexandria for second place in the Roman empire.\nOn top of Byrsa hill, the location of the Roman Forum, a residential area from the last century of existence (early second century BCE.) of the Punic city was excavated by the French archaeologist Serge Lancel. The neighborhood, with its houses, shops, and private spaces, is significant for what it reveals about daily life there over 2100 years ago.\n\nThe remains have been preserved under embankments, the substructures of the later Roman forum, whose foundation piles dot the district. The housing blocks are separated by a grid of straight streets about wide, with a roadway consisting of clay; \"in situ\" stairs compensate for the slope of the hill. Construction of this type presupposes organization and political will, and has inspired the name of the neighborhood, \"Hannibal district\", referring to the legendary Punic general or sufet (consul) at the beginning of the second century BCE.\n\nThe habitat is typical, even stereotypical. The street was often used as a storefront/shopfront; cisterns were installed in basements to collect water for domestic use, and a long corridor on the right side of each residence led to a courtyard containing a sump, around which various other elements may be found. In some places, the ground is covered with mosaics called punica pavement, sometimes using a characteristic red mortar.\n\nThe merchant harbor at Carthage was developed, after settlement of the nearby Punic town of Utica. Eventually the surrounding countryside was brought into the orbit of the Punic urban centres, first commercially, then politically. Direct management over cultivation of neighbouring lands by Punic owners followed. A 28-volume work on agriculture written in Punic by Mago, a retired army general (c. 300), was translated into Latin and later into Greek. The original and both translations have been lost; however, some of Mago's text has survived in other Latin works. Olive trees (e.g., grafting), fruit trees (pomegranate, almond, fig, date palm), viniculture, bees, cattle, sheep, poultry, implements, and farm management were among the ancient topics which Mago discussed. As well, Mago addresses the wine-maker's art (here a type of sherry).\n\nIn Punic farming society, according to Mago, the small estate owners were the chief producers. They were, two modern historians write, not absent landlords. Rather, the likely reader of Mago was \"the master of a relatively modest estate, from which, by great personal exertion, he extracted the maximum yield.\" Mago counselled the rural landowner, for the sake of their own 'utilitarian' interests, to treat carefully and well their managers and farm workers, or their overseers and slaves. Yet elsewhere these writers suggest that rural land ownership provided also a new power base among the city's nobility, for those resident in their country villas. By many, farming was viewed as an alternative endeavour to an urban business. Another modern historian opines that more often it was the urban merchant of Carthage who owned rural farming land to some profit, and also to retire there during the heat of summer. It may seem that Mago anticipated such an opinion, and instead issued this contrary advice (as quoted by the Roman writer Columella):\n\n\"The man who acquires an estate must sell his house, lest he prefer to live in the town rather than in the country. Anyone who prefers to live in a town has no need of an estate in the country.\" \"One who has bought land should sell his town house, so that he will have no desire to worship the household gods of the city rather than those of the country; the man who takes greater delight in his city residence will have no need of a country estate.\"\n\nThe issues involved in rural land management also reveal underlying features of Punic society, its structure and stratification. The hired workers might be considered 'rural proletariat', drawn from the local Berbers. Whether or not there remained Berber landowners next to Punic-run farms is unclear. Some Berbers became sharecroppers. Slaves acquired for farm work were often prisoners of war. In lands outside Punic political control, independent Berbers cultivated grain and raised horses on their lands. Yet within the Punic domain that surrounded the city-state of Carthage, there were ethnic divisions in addition to the usual quasi feudal distinctions between lord and peasant, or master and serf. This inherent instability in the countryside drew the unwanted attention of potential invaders. Yet for long periods Carthage was able to manage these social difficulties.\n\nThe many amphorae with Punic markings subsequently found about ancient Mediterranean coastal settlements testify to Carthaginian trade in locally made olive oil and wine. Carthage's agricultural production was held in high regard by the ancients, and rivaled that of Rome—they were once competitors, e.g., over their olive harvests. Under Roman rule, however, grain production ([wheat] and barley) for export increased dramatically in 'Africa'; yet these later fell with the rise in Roman Egypt's grain exports. Thereafter olive groves and vineyards were re-established around Carthage. Visitors to the several growing regions that surrounded the city wrote admiringly of the lush green gardens, orchards, fields, irrigation channels, hedgerows (as boundaries), as well as the many prosperous farming towns located across the rural landscape.\n\nAccordingly, the Greek author and compiler Diodorus Siculus (fl. 1st century BCE), who enjoyed access to ancient writings later lost, and on which he based most of his writings, described agricultural land near the city of Carthage circa 310 BC:\n\"It was divided into market gardens and orchards of all sorts of fruit trees, with many streams of water flowing in channels irrigating every part. There were country homes everywhere, lavishly built and covered with stucco. ... Part of the land was planted with vines, part with olives and other productive trees. Beyond these, cattle and sheep were pastured on the plains, and there were meadows with grazing horses.\"\n\nThe \"Chora\" (farm lands of Carthage) encompassed a limited area: the north coastal \"tell\", the lower Bagradas river valley (inland from Utica), Cape Bon, and the adjacent \"sahel\" on the east coast. Punic culture here achieved the introduction of agricultural sciences first developed for lands of the eastern Mediterranean, and their adaptation to local African conditions.\n\nThe \"urban landscape\" of Carthage is known in part from ancient authors, augmented by modern digs and surveys conducted by archeologists. The \"first urban nucleus\" dating to the seventh century, in area about ten hectares (or four acres), was apparently located on low-lying lands along the coast (north of the later harbors). As confirmed by archaeological excavations, Carthage was a \"creation \"ex nihilo\"\", built on 'virgin' land, and situated at the end of a peninsula (per the ancient coastline). Here among \"mud brick walls and beaten clay floors\" (recently uncovered) were also found extensive cemeteries, which yielded evocative grave goods like clay masks. \"Thanks to this burial archaeology we know more about archaic Carthage than about any other contemporary city in the western Mediterranean.\" Already in the eighth century, fabric dyeing operations had been established, evident from crushed shells of murex (from which the 'Phoenician purple' was derived). Nonetheless, only a \"meager picture\" of the cultural life of the earliest pioneers in the city can be conjectured, and not much about housing, monuments or defenses. The Roman poet Virgil (70–19 BC) imagined early Carthage, when his legendary character Aeneas had arrived there:\n\n\"Aeneas found, where lately huts had been,\nmarvelous buildings, gateways, cobbled ways,\nand din of wagons. There the Tyrians\nwere hard at work: laying courses for walls,\nrolling up stones to build the citadel,\nwhile others picked out building sites and plowed\na boundary furrow. Laws were being enacted,\nmagistrates and a sacred senate chosen.\nHere men were dredging harbors, there they laid\nthe deep foundations of a theatre,\nand quarried massive pillars... .\"\nThe two inner harbours [called in Punic \"cothon\"] were located in the southeast; one being commercial, and the other for war. Their definite functions are not entirely known, probably for the construction, outfitting, or repair of ships, perhaps also loading and unloading cargo. Larger anchorages existed to the north and south of the city. North and west of the \"cothon\" were located several industrial areas, e.g., metalworking and pottery (e.g., for amphora), which could serve both inner harbours, and ships anchored to the south of the city.\n\nAbout the Byrsa, the citadel area to the north, considering its importance our knowledge of it is patchy. Its prominent heights were the scene of fierce combat during the fiery destruction of the city in 146 BC. The Byrsa was the reported site of the Temple of Eshmun (the healing god), at the top of a stairway of sixty steps. A temple of Tanit (the city's queen goddess) was likely situated on the slope of the 'lesser Byrsa' immediately to the east, which runs down toward the sea. Also situated on the Byrsa were luxury homes.\n\nSouth of the citadel, near the \"cothon\" (the inner harbours) was the \"tophet\", a special and very old cemetery, which when begun lay outside the city's boundaries. Here the \"Salammbô\" was located, the \"Sanctuary of Tanit\", not a temple but an enclosure for placing stone stelae. These were mostly short and upright, carved for funeral purposes. Evidence from here may indicate the occurrence of child sacrifice. Probably the \"tophet\" burial fields were \"dedicated at an early date, perhaps by the first settlers.\"\n\nBetween the sea-filled \"cothon\" for shipping and the Byrsa heights lay the \"agora\" [Greek: \"market\"], the city-state's central marketplace for business and commerce. The \"agora\" was also an area of public squares and plazas, where the people might formally assemble, or gather for festivals. It was the site of religious shrines, and the location of whatever were the major municipal buildings of Carthage. Here beat the heart of civic life. In this district of the Carthage, more probably, the ruling suffets presided, the council of elders convened, the tribunal of the 104 met, and justice was dispensed at trials in the open air.\n\nEarly residential districts wrapped around the Byrsa from the south to the north east. Houses usually were whitewashed and blank to the street, but within were courtyards open to the sky. In these neighborhoods multistory construction later became common, some up to six stories tall according to an ancient Greek author. Several architecutural floorplans of homes have been revealed by recent excavations, as well as the general layout of several city blocks. Stone stairs were set in the streets, and drainage was planned, e.g., in the form of soakways leaching into the sandy soil. Along the Byrsa's southern slope were located not only fine old homes, but also many of the earliest grave-sites, juxtaposed in small areas, interspersed with daily life.\n\nArtisan workshops were located in the city at sites north and west of the harbours. The location of three metal workshops (implied from iron slag and other vestiges of such activity) were found adjacent to the naval and commercial harbours, and another two were further up the hill toward the Byrsa citadel. Sites of pottery kilns have been identified, between the \"agora\" and the harbours, and further north. Earthenware often used Greek models. A fuller's shop for preparing woolen cloth (shrink and thicken) was evidently situated further to the west and south, then by the edge of the city. Carthage also produced objects of rare refinement. During the 4th and 3rd centuries, the sculptures of the sarcophagi became works of art. \"Bronze engraving and stone-carving reached their zenith.\"\n\nThe elevation of the land at the promontory on the seashore to the north-east (now called Sidi Bou Saïd), was twice as high above sea level as that at the Byrsa (100 m and 50 m). In between runs a ridge, several times reaching 50 m; it continues northwestward along the seashore, and forms the edge of a plateau-like area between the Byrsa and the sea. Newer urban developments lay here in these northern districts.\n\nSurrounding Carthage were walls \"of great strength\" said in places to rise above 13 m, being nearly 10 m thick, according to ancient authors. To the west, three parallel walls were built. The walls altogether ran for about to encircle the city. The heights of the Byrsa were additionally fortified; this area being the last to succumb to the Romans in 146 BC. Originally the Romans had landed their army on the strip of land extending southward from the city.\n\nGreek cities contested with Carthage for the Western Mediterranean culminating in the Sicilian Wars and the Pyrrhic War over Sicily, while the Romans fought three wars against Carthage, known as the Punic Wars.\n\nThe Carthaginian republic was one of the longest-lived and largest states in the ancient Mediterranean. Reports relay several wars with Syracuse and finally, Rome, which eventually resulted in the defeat and destruction of Carthage in the Third Punic War. The Carthaginians were Phoenician settlers originating in the Mediterranean coast of the Near East. They spoke Canaanite, a Semitic language, and followed a local variety of the ancient Canaanite religion.\n\nThe fall of Carthage came at the end of the Third Punic War in 146 BC at the Battle of Carthage. Despite initial devastating Roman naval losses and Rome's recovery from the brink of defeat after the terror of a 15-year occupation of much of Italy by Hannibal, the end of the series of wars resulted in the end of Carthaginian power and the complete destruction of the city by Scipio Aemilianus. The Romans pulled the Phoenician warships out into the harbor and burned them before the city, and went from house to house, capturing and enslaving the people. About 50,000 Carthaginians were sold into slavery. The city was set ablaze and razed to the ground, leaving only ruins and rubble. After the fall of Carthage, Rome annexed the majority of the Carthaginian colonies, including other North African locations such as Volubilis, Lixus, Chellah, and Mogador. \n\nThe legend that the city was sown with salt remains widely accepted despite lacking evidence among ancient historical accounts; R.T. Ridley found that the earliest such claim is attributed to B.L. Hallward's chapter in \"Cambridge Ancient History\", published in 1930. Ridley contended that Hallward's claim may have gained traction due to historical evidence of other salted-earth instances such as Abimelech's salting of Shechem in Judges 9:45. Many historians have since issued retractions acknowledging Ridley. B.H. Warmington similarly admitted fault in repeating Hallward's error, but posited that the legend precedes 1930 and inspired repetitions of the practice. For this reason, Warmington suggested that the symbolic value of the legend is so great and enduring that it mitigates the deficiency of concrete evidence that it happened and is useful to understand how subsequent historical narratives have been framed.\n\nStarting in the 19th century, various texts claim that the Roman general Scipio Aemilianus Africanus plowed over and sowed the city of Carthage with salt after defeating it in the Third Punic War (146 BC), sacking it, and forcing the survivors into slavery. However, no ancient sources exist documenting the salting itself. The Carthage story is a later invention, probably modeled on the story of Shechem. The ritual of symbolically drawing a plow over the site of a city is, however, mentioned in ancient sources, though not in reference to Carthage specifically. When Pope Boniface VIII destroyed Palestrina in 1299, he issued a papal bull that it be plowed \"following the old example of Carthage in Africa\", and also salted. \"I have run the plough over it, like the ancient Carthage of Africa, and I have had salt sown upon it...\"\n\nWhen Carthage fell, its nearby rival Utica, a Roman ally, was made capital of the region and replaced Carthage as the leading center of Punic trade and leadership. It had the advantageous position of being situated on the outlet of the Medjerda River, Tunisia's only river that flowed all year long. However, grain cultivation in the Tunisian mountains caused large amounts of silt to erode into the river. This silt accumulated in the harbor until it became useless, and Rome was forced to rebuild Carthage.\n\nBy 122 BC, Gaius Gracchus founded a short-lived colony, called \"Colonia Iunonia\", after the Latin name for the Punic goddess Tanit, \"Iuno Caelestis\". The purpose was to obtain arable lands for impoverished farmers. The Senate abolished the colony some time later, to undermine Gracchus' power.\n\nAfter this ill-fated attempt, a new city of Carthage was built on the same land by Julius Caesar in the period from 49 to 44 BC, and by the first century, it had grown to be the second-largest city in the western half of the Roman Empire, with a peak population of 500,000. It was the center of the province of Africa, which was a major breadbasket of the Empire. Among its major monuments was an amphitheater.\n\nCarthage also became a center of early Christianity (see Carthage (episcopal see)). In the first of a string of rather poorly reported councils at Carthage a few years later, no fewer than 70 bishops attended. Tertullian later broke with the mainstream that was increasingly represented in the West by the primacy of the Bishop of Rome, but a more serious rift among Christians was the Donatist controversy, which Augustine of Hippo spent much time and parchment arguing against. At the Council of Carthage (397), the biblical canon for the western Church was confirmed.\nThe political fallout from the deep disaffection of African Christians is supposedly a crucial factor in the ease with which Carthage and the other centers were captured in the fifth century by Genseric, king of the Vandals, who defeated the Roman general Bonifacius and made the city the capital of the Vandal Kingdom. Genseric was considered a heretic, too, an Arian, and though Arians commonly despised Catholic Christians, a mere promise of toleration might have caused the city's population to accept him.\n\nAfter a failed attempt to recapture the city in the fifth century, the Eastern Roman Empire finally subdued the Vandals in the Vandalic War in 533–534. Thereafter, the city became the seat of the praetorian prefecture of Africa, which was made into an exarchate during the emperor Maurice's reign, as was Ravenna on the Italian Peninsula. These two exarchates were the western bulwarks of the Byzantine Empire, all that remained of its power in the West. In the early seventh century Heraclius the Elder, the exarch of Carthage, overthrew the Byzantine emperor Phocas, whereupon his son Heraclius succeeded to the imperial throne.\n\nThe Roman Exarchate of Africa was not able to withstand the seventh-century Muslim conquest of the Maghreb. The Umayyad Caliphate under Abd al-Malik ibn Marwan in 686 sent a force led by Zuhayr ibn Qais, who won a battle over the Romans and Berbers led by King Kusaila of the Kingdom of Altava on the plain of Kairouan, but he could not follow that up. In 695, Hasan ibn al-Nu'man captured Carthage and advanced into the Atlas Mountains. An imperial fleet arrived and retook Carthage, but in 698, Hasan ibn al-Nu'man returned and defeated Emperor Tiberios III at the 698 Battle of Carthage. Roman imperial forces withdrew from all of Africa except Ceuta. Roman Carthage was destroyed—its walls torn down, its water supply cut off, and its harbors made unusable. \nThe destruction of the Exarchate of Africa marked a permanent end to the Byzantine Empire's influence in the region.\n\nThe Medina of Tunis, originally a Berber settlement, was established as the new regional center under the Umayyad Caliphate in the early 8th century. Under the Aghlabids, the people of Tunis revolted numerous times, but the city profited from economic improvements and quickly became the second most important in the kingdom. It was briefly the national capital, from the end of the reign of Ibrahim II in 902, until 909, when the Shi'ite Berbers took over Ifriqiya and founded the Fatimid Caliphate.\n\nCarthage remained a residential see until the high medieval period, mentioned in\ntwo letters of Pope Leo IX dated 1053, written in reply to consultations regarding a conflict between the bishops of Carthage and Gummi.\nIn each of the two letters, Pope Leo declares that, after the Bishop of Rome, the first archbishop and chief metropolitan of the whole of Africa is the bishop of Carthage. \nLater, an archbishop of Carthage named Cyriacus was imprisoned by the Arab rulers because of an accusation by some Christians. Pope Gregory VII wrote him a letter of consolation, repeating the hopeful assurances of the primacy of the Church of Carthage, \"whether the Church of Carthage should still lie desolate or rise again in glory\". \nBy 1076, Cyriacus was set free, but there was only one other bishop in the province. These are the last of whom there is mention in that period of the history of the see.\n\nCarthage is some east-northeast of Tunis; the settlements nearest to Carthage were the town of Sidi Bou Said to the north and the village of Le Kram to the south. \nSidi Bou Saint was a village which had grown around the tomb of the eponymous sufi saint (d. 1231), which had been developed into a town under Ottoman rule in the 18th century. Le Kram was developed in the late 19th century under French administration as a settlement close to the port of La Goulette.\n\nIn 1881, Tunisia became a French protectorate, and in the same year Charles Lavigerie, who was archbishop of Algiers, became apostolic administrator of the vicariate of Tunis. In the following year, Lavigerie became a cardinal. He \"saw himself as the reviver of the ancient Christian Church of Africa, the Church of Cyprian of Carthage\", and, on 10 November 1884, was successful in his great ambition of having the metropolitan see of Carthage restored, with himself as its first archbishop. In line with the declaration of Pope Leo IX in 1053, Pope Leo XIII acknowledged the revived Archdiocese of Carthage as the primatial see of Africa and Lavigerie as primate.\n\nThe Acropolium of Carthage (Saint Louis Cathedral of Carthage) was erected on Byrsa hill in 1884.\n\nThe Danish consul Christian Tuxen Falbe conducted a first survey of the topography of the archaeological site (published in 1833).\nAntiquarian interest was intensified following the publication of Flaubert's \"Salammbô\" in 1858. Charles Ernest Beulé performed some preliminary excavations of Roman remains on Byrsa hill in 1860. A more systematic survey of both Punic and Roman-era remains is due to Alfred Louis Delattre, who was sent to Tunis by cardinal Charles Lavigerie in 1875 on both an apostolic and an archaeological mission.\nAudollent (1901, p. 203) cites Delattre and Lavigerie to the effect that in the 1880s, locals still knew the area of the ancient city under the name of \"Cartagenna\" (i.e. reflecting the Latin \"n\"-stem \"Carthāgine\").\n\nAuguste Audollent divides the area of Roman Carthage into four quarters, \"Cartagenna\", \"Dermèche\", \"Byrsa\" and \"La Malga\". Cartagenna and Dermèche correspond with the lower city, including the site of Punic Carthage; Byrsa is associated with the upper city, which in Punic times was a walled citadel above the harbour; and \"La Malga\" is linked with the more remote parts of the upper city in Roman times.\n\nFrench-led excavations at Carthage began in 1921, and from 1923 reported finds of a large quantity of urns containing a mixture of animal and children's bones. René Dussaud identified a 4th-century BC stela found in Carthage as depicting a child sacrifice.\n\nA temple at Amman (1400–1250 BC) excavated and reported upon by J.B. Hennessy in 1966, shows the possibility of bestial and human sacrifice by fire. While evidence of child sacrifice in Canaan was the object of academic disagreement, with some scholars arguing that merely children's cemeteries had been unearthed in Carthage, the mixture of children's with animal bones as well as associated epigraphic evidence involving mention of \"mlk\" led to a consensus that, at least in Carthage, child sacrifice was indeed common practice.\n\nIn 2016, an ancient Carthaginian individual, who was excavated from a Punic tomb in Byrsa Hill, was found to belong to the rare U5b2c1 maternal haplogroup. The Young Man of Byrsa specimen dates from the late 6th century BCE, and his lineage is believed to represent early gene flow from Iberia to the Maghreb.\n\nIn 1920, the first seaplane base was built on the Lake of Tunis for the seaplanes of Compagnie Aéronavale. The Tunis Airfield opened in 1938, serving around 5,800 passengers annually on the Paris-Tunis route. \nDuring World War II, the airport was used by the United States Army Air Force Twelfth Air Force as a headquarters and command control base for the Italian Campaign of 1943. \nConstruction on the Tunis-Carthage Airport, which was fully funded by France, began in 1944, and in 1948 the airport become the main hub for Tunisair.\n\nIn the 1950s the Lycée Français de Carthage was established to serve French families in Carthage. In 1961 it was given to the Tunisian government as part of the Independence of Tunisia, so the nearby Collège Maurice Cailloux in La Marsa, previously an annex of the Lycée Français de Carthage, was renamed to the Lycée Français de La Marsa and began serving the \"lycée\" level. It is currently the Lycée Gustave Flaubert.\n\nAfter Tunisian independence in 1956, the Tunis conurbation gradually extended around the airport, and Carthage (قرطاج \" Qarṭāj\") is now a suburb of Tunis, covering the area between Sidi Bou Said and Le Kram.\nIts population as of January 2013 was estimated at 21,276, \nmostly attracting the more wealthy residents. If Carthage is not the capital, it tends to be the political pole, a « place of emblematic power » according to Sophie Bessis, leaving to Tunis the economic and administrative roles. The Carthage Palace (the Tunisian presidential palace) is located in the coast.\n\nThe suburb has six train stations of the TGM line between Le Kram and Sidi Bou Said: \nCarthage Salammbo (named for Salambo, the fictional daughter of Hamilcar), Carthage Byrsa (named for Byrsa hill), Carthage Dermech (\"Dermèche\"), Carthage Hannibal (named for Hannibal), Carthage Présidence (named for the Presidential Palace) and Carthage Amilcar (named for Hamilcar).\n\nThe merchants of Carthage were in part heirs of the Mediterranean trade developed by Phoenicia, and so also heirs of the rivalry with Greek merchants. Business activity was accordingly both stimulated and challenged. Cyprus had been an early site of such commercial contests. The Phoenicians then had ventured into the western Mediterranean, founding trading posts, including Utica and Carthage. The Greeks followed, entering the western seas where the commercial rivalry continued. Eventually it would lead, especially in Sicily, to several centuries of intermittent war. Although Greek-made merchandise was generally considered superior in design, Carthage also produced trade goods in abundance. That Carthage came to function as a manufacturing colossus was shown during the Third Punic War with Rome. Carthage, which had previously disarmed, then was made to face the fatal Roman siege. The city \"suddenly organised the manufacture of arms\" with great skill and effectiveness. According to Strabo (63 BC – AD 21) in his \"Geographica\":\n\n\"[Carthage] each day produced one hundred and forty finished shields, three hundred swords, five hundred spears, and one thousand missiles for the catapults... . Furthermore, [Carthage although surrounded by the Romans] built one hundred and twenty decked ships in two months... for old timber had been stored away in readiness, and a large number of skilled workmen, maintained at public expense.\"\n\nThe textiles industry in Carthage probably started in private homes, but the existence of professional weavers indicates that a sort of factory system later developed. Products included embroidery, carpets, and use of the purple murex dye (for which the Carthaginian isle of Djerba was famous). Metalworkers developed specialized skills, i.e., making various weapons for the armed forces, as well as domestic articles, such as knives, forks, scissors, mirrors, and razors (all articles found in tombs). Artwork in metals included vases and lamps in bronze, also bowls, and plates. Other products came from such crafts as the potters, the glassmakers, and the goldsmiths. Inscriptions on votive stele indicate that many were not slaves but 'free citizens'.\n\nPhoenician and Punic merchant ventures were often run as a family enterprise, putting to work its members and its subordinate clients. Such family-run businesses might perform a variety of tasks: (a) own and maintain the ships, providing the captain and crew; (b) do the negotiations overseas, either by barter or buy and sell, of (i) their own manufactured commodities and trade goods, and (ii) native products (metals, foodstuffs, etc.) to carry and trade elsewhere; and (c) send their agents to stay at distant outposts in order to make lasting local contacts, and later to establish a warehouse of shipped goods for exchange, and eventually perhaps a settlement. Over generations, such activity might result in the creation of a wide-ranging network of trading operations. Ancillary would be the growth of reciprocity between different family firms, foreign and domestic.\n\nState protection was extended to its sea traders by the Phoenician city of Tyre and later likewise by the daughter city-state of Carthage. , the well-regarded French historian of ancient North Africa, summarized the major principles guiding the civic rulers of Carthage with regard to its policies for trade and commerce: \n\nBoth the Phoenicians and the Cathaginians were well known in antiquity for their secrecy in general, and especially pertaining to commercial contacts and trade routes. Both cultures excelled in commercial dealings. Strabo (63BC-AD21) the Greek geographer wrote that before its fall (in 146 BC) Carthage enjoyed a population of 700,000, and directed an alliance of 300 cities. The Greek historian Polybius (c.203–120) referred to Carthage as \"the wealthiest city in the world\".\n\nA \"suffet\" (possibly two) was elected by the citizens, and held office with no military power for a one-year term. Carthaginian generals marshalled mercenary armies and were separately elected. From about 550 to 450 the Magonid family monopolized the top military position; later the Barcid family acted similarly. Eventually it came to be that, after a war, the commanding general had to testify justifying his actions before a court of 104 judges.\n\nAristotle (384–322) discusses Carthage in his work, \"Politica\"; he begins: \"The Carthaginians are also considered to have an excellent form of government.\" He briefly describes the city as a \"mixed constitution\", a political arrangement with cohabiting elements of monarchy, aristocracy, and democracy, i.e., a king (Gk: basileus), a council of elders (Gk: gerusia), and the people (Gk: demos). Later Polybius of Megalopolis (c.204–122, Greek) in his \"Histories\" would describe the Roman Republic in more detail as a mixed constitution in which the Consuls were the monarchy, the Senate the aristocracy, and the Assemblies the democracy.\n\nEvidently Carthage also had an institution of elders who advised the Suffets, similar to a Greek \"gerusia\" or the Roman Senate. We do not have a Punic name for this body. At times its members would travel with an army general on campaign. Members also formed permanent committees. The institution had several hundred members drawn from the wealthiest class who held office for life. Vacancies were probably filled by recruitment from among the elite, i.e., by co-option. From among its members were selected the 104 Judges mentioned above. Later the 104 would come to evaluate not only army generals but other office holders as well. Aristotle regarded the 104 as most important; he compared it to the ephorate of Sparta with regard to control over security. In Hannibal's time, such a Judge held office for life. At some stage there also came to be independent self-perpetuating boards of five who filled vacancies and supervised (non-military) government administration.\n\nPopular assemblies also existed at Carthage. When deadlocked the Suffets and the quasi-senatorial institution of elders might request the assembly to vote; also, assembly votes were requested in very crucial matters in order to achieve political consensus and popular coherence. The assembly members had no \"legal\" wealth or birth qualification. How its members were selected is unknown, e.g., whether by festival group or urban ward or another method.\n\nThe Greeks were favourably impressed by the constitution of Carthage; Aristotle had a separate study of it made which unfortunately is lost. In his \"Politica\" he states: \"The government of Carthage is oligarchical, but they successfully escape the evils of oligarchy by enriching one portion of the people after another by sending them to their colonies.\" \"[T]heir policy is to send some [poorer citizens] to their dependent towns, where they grow rich.\" Yet Aristotle continues, \"[I]f any misfortune occurred, and the bulk of the subjects revolted, there would be no way of restoring peace by legal means.\" Aristotle remarked also:\n\n\"Many of the Carthaginian institutions are excellent. The superiority of their constitution is proved by the fact that the common people remain loyal to the constitution; the Carthaginians have never had any rebellion worth speaking of, and have never been under the rule of a tyrant.\"\n\nHere one may remember that the city-state of Carthage, who citizens were mainly \"Libyphoenicians\" (of Phoenician ancestry born in Africa), dominated and exploited an agricultural countryside composed mainly of native Berber sharecroppers and farmworkers, whose affiliations to Carthage were open to divergent possibilities. Beyond these more settled Berbers and the Punic farming towns and rural manors, lived the independent Berber tribes, who were mostly pastoralists.\n\nIn the brief, uneven review of government at Carthage found in his \"Politica\" Aristotle mentions several faults. Thus, \"that the same person should hold many offices, which is a favorite practice among the Carthaginians.\" Aristotle disapproves, mentioning the flute-player and the shoemaker. Also, that \"magistrates should be chosen not only for their merit but for their wealth.\" Aristotle's opinion is that focus on pursuit of wealth will lead to oligarchy and its evils.\n\n\"[S]urely it is a bad thing that the greatest offices... should be bought. The law which allows this abuse makes wealth of more account than virtue, and the whole state becomes avaricious. For, whenever the chiefs of the state deem anything honorable, the other citizens are sure to follow their example; and, where virtue has not the first place, their aristocracy cannot be firmly established.\"\n\nIn Carthage the people seemed politically satisfied and submissive, according to the historian Warmington. They in their assemblies only rarely exercised the few opportunities given them to assent to state decisions. Popular influence over government appears not to have been an issue at Carthage. Being a commercial republic fielding a mercenary army, the people were not conscripted for military service, an experience which can foster the feel for popular political action. But perhaps this misunderstands the society; perhaps the people, whose values were based on small-group loyalty, felt themselves sufficiently connected to their city's leadership by the very integrity of the person-to-person linkage within their social fabric. Carthage was very stable; there were few openings for tyrants. Only after defeat by Rome devastated Punic imperial ambitions did the people of Carthage seem to question their governance and to show interest in political reform.\n\nIn 196, following the Second Punic War (218–201), Hannibal Barca, still greatly admired as a Barcid military leader, was elected suffet. When his reforms were blocked by a financial official about to become a judge for life, Hannibal rallied the populace against the 104 judges. He proposed a one-year term for the 104, as part of a major civic overhaul. Additionally, the reform included a restructuring of the city's revenues, and the fostering of trade and agriculture. The changes rather quickly resulted in a noticeable increase in prosperity. Yet his incorrigible political opponents cravenly went to Rome, to charge Hannibal with conspiracy, namely, plotting war against Rome in league with Antiochus the Hellenic ruler of Syria. Although the Roman Scipio Africanus resisted such manoeuvre, eventually intervention by Rome forced Hannibal to leave Carthage. Thus, corrupt city officials efficiently blocked Hannibal Barca in his efforts to reform the government of Carthage.\n\nMago (6th century) was King of Carthage; the head of state, war leader, and religious figurehead. His family was considered to possess a sacred quality. Mago's office was somewhat similar to that of a pharaoh, but although kept in a family it was not hereditary, it was limited by legal consent. Picard, accordingly, believes that the council of elders and the popular assembly are late institutions. Carthage was founded by the king of Tyre who had a royal monopoly on this trading venture. Thus it was the royal authority stemming from this traditional source of power that the King of Carthage possessed. Later, as other Phoenician ship companies entered the trading region, and so associated with the city-state, the King of Carthage had to keep order among a rich variety of powerful merchants in their negotiations among themselves and over risky commerce across the Mediterranean. Under these circumstance, the office of king began to be transformed. Yet it was not until the aristocrats of Carthage became wealthy owners of agricultural lands in Africa that a council of elders was institutionalized at Carthage.\n\nMost ancient literature concerning Carthage comes from Greek and Roman sources as Carthage's own documents were destroyed by the Romans. Apart from inscriptions, hardly any Punic literature has survived, and none in its own language and script. A brief catalogue would include:\n\n\n\"[F]rom the Greek author Plutarch [(c. 46 – c. 120)] we learn of the 'sacred books' in Punic safeguarded by the city's temples. Few Punic texts survive, however.\" Once \"the City Archives, the Annals, and the scribal lists of \"suffets\"\" existed, but evidently these were destroyed in the horrific fires during the Roman capture of the city in 146 BC.\n\nYet some Punic books (Latin: \"libri punici\") from the libraries of Carthage reportedly did survive the fires. These works were apparently given by Roman authorities to the newly augmented Berber rulers. Over a century after the fall of Carthage, the Roman politician-turned-author Gaius Sallustius Crispus or Sallust (86–34) reported his having seen volumes written in Punic, which books were said to be once possessed by the Berber king, Hiempsal II (r. 88–81). By way of Berber informants and Punic translators, Sallust had used these surviving books to write his brief sketch of Berber affairs.\n\nProbably some of Hiempsal II's \"libri punici\", that had escaped the fires that consumed Carthage in 146 BC, wound up later in the large royal library of his grandson Juba II (r.25 BC-AD 24). Juba II not only was a Berber king, and husband of Cleopatra's daughter, but also a scholar and author in Greek of no less than nine works. He wrote for the Mediterranean-wide audience then enjoying classical literature. The \"libri punici\" inherited from his grandfather surely became useful to him when composing his \"Libyka\", a work on North Africa written in Greek. Unfortunately, only fragments of \"Libyka\" survive, mostly from quotations made by other ancient authors. It may have been Juba II who 'discovered' the five-centuries-old 'log book' of Hanno the Navigator, called the \"Periplus\", among library documents saved from fallen Carthage.\n\nIn the end, however, most Punic writings that survived the destruction of Carthage \"did not escape the immense wreckage in which so many of Antiquity's literary works perished.\" Accordingly, the long and continuous interactions between Punic citizens of Carthage and the Berber communities that surrounded the city have no local historian. Their political arrangements and periodic crises, their economic and work life, the cultural ties and social relations established and nourished (infrequently as kin), are not known to us directly from ancient Punic authors in written accounts. Neither side has left us their stories about life in Punic-era Carthage.\n\nRegarding \"Phoenician\" writings, few remain and these seldom refer to Carthage. The more ancient and most informative are cuneiform tablets, ca. 1600–1185, from ancient Ugarit, located to the north of Phoenicia on the Syrian coast; it was a Canaanite city politically affiliated with the Hittites. The clay tablets tell of myths, epics, rituals, medical and administrative matters, and also correspondence. The highly valued works of Sanchuniathon, an ancient priest of Beirut, who reportedly wrote on Phoenician religion and the origins of civilization, are themselves completely lost, but some little content endures twice removed. Sanchuniathon was said to have lived in the 11th century, which is considered doubtful. Much later a \"Phoenician History\" by Philo of Byblos (64–141) reportedly existed, written in Greek, but only fragments of this work survive. An explanation proffered for why so few Phoenician works endured: early on (11th century) archives and records began to be kept on papyrus, which does not long survive in a moist coastal climate. Also, both Phoenicians and Carthaginians were well known for their secrecy.\n\nThus, of their ancient writings we have little of major interest left to us by Carthage, or by Phoenicia the country of origin of the city founders. \"Of the various Phoenician and Punic compositions alluded to by the ancient classical authors, not a single work or even fragment has survived in its original idiom.\" \"Indeed, not a single Phoenician manuscript has survived in the original [language] or in translation.\" We cannot therefore access directly the line of thought or the contour of their worldview as expressed in their own words, in their own voice. Ironically, it was the Phoenicians who \"invented or at least perfected and transmitted a form of writing [the alphabet] that has influenced dozens of cultures including our own.\"\n\nAs noted, the celebrated ancient books on agriculture written by Mago of Carthage survives only via quotations in Latin from several later Roman works.\n\n\n"}
{"id": "6556", "url": "https://en.wikipedia.org/wiki?curid=6556", "title": "Coprime integers", "text": "Coprime integers\n\nIn number theory, two integers \"a\" and \"b\" are said to be relatively prime, mutually prime, or coprime (also spelled co-prime) if the only positive integer that divides both of them is 1. That is, the only common positive factor of the two numbers is 1. This is equivalent to their greatest common divisor being 1. The numerator and denominator of a reduced fraction are coprime. In addition to formula_1 and formula_2 the notation formula_3 is sometimes used to indicate that \"a\" and \"b\" are relatively prime.\n\nFor example, 14 and 15 are coprime, being commonly divisible by only 1, but 14 and 21 are not, because they are both divisible by 7. The numbers 1 and −1 are the only integers coprime to every integer, and they are the only integers to be coprime with 0.\n\nA fast way to determine whether two numbers are coprime is given by the Euclidean algorithm.\n\nThe number of integers coprime to a positive integer \"n\", between 1 and \"n\", is given by Euler's totient function (or Euler's phi function) \"φ\"(\"n\").\n\nA set of integers can also be called coprime if its elements share no common positive factor except 1. A set of integers is said to be pairwise coprime if \"a\" and \"b\" are coprime for every pair (\"a\", \"b\") of different integers in it.\n\nA number of conditions are individually equivalent to \"a\" and \"b\" being coprime:\n\nAs a consequence of the third point, if \"a\" and \"b\" are coprime and \"br\" ≡ \"bs\" (mod \"a\"), then \"r\" ≡ \"s\" (mod \"a\"). That is, we may \"divide by \"b\"\" when working modulo \"a\". Furthermore, if \"b\" and \"b\" are both coprime with \"a\", then so is their product \"b\"\"b\" (modulo \"a\" it is a product of invertible elements, and therefore invertible); this also follows from the first point by Euclid's lemma, which states that if a prime number \"p\" divides a product \"bc\", then \"p\" divides at least one of the factors \"b\", \"c\".\n\nAs a consequence of the first point, if \"a\" and \"b\" are coprime, then so are any powers \"a\" and \"b\".\n\nIf \"a\" and \"b\" are coprime and \"a\" divides the product \"bc\", then \"a\" divides \"c\". This can be viewed as a generalization of Euclid's lemma.\n\nThe two integers \"a\" and \"b\" are coprime if and only if the point with coordinates (\"a\", \"b\") in a Cartesian coordinate system is \"visible\" from the origin (0,0), in the sense that there is no point with integer coordinates on the line segment between the origin and (\"a\", \"b\"). (See figure 1.)\n\nIn a sense that can be made precise, the probability that two randomly chosen integers are coprime is 6/π (see pi), which is about 61%. See below.\n\nTwo natural numbers \"a\" and \"b\" are coprime if and only if the numbers 2 − 1 and 2 − 1 are coprime. As a generalization of this, following easily from the Euclidean algorithm in base \"n\" > 1:\n\nA set of integers \"S\" = {\"a\", \"a\", ... \"a\"} can also be called \"coprime\" or \"setwise coprime\" if the greatest common divisor of all the elements of the set is 1. For example, the integers 6, 10, 15 are coprime because 1 is the only positive integer that divides all of them.\n\nIf every pair in a set of integers is coprime, then the set is said to be \"pairwise coprime\" (or \"pairwise relatively prime\", \"mutually coprime\" or \"mutually relatively prime\"). Pairwise coprimality is a stronger condition than setwise coprimality; every pairwise coprime finite set is also setwise coprime, but the reverse is not true. For example, the integers 4, 5, 6 are (setwise) coprime (because the only positive integer dividing \"all\" of them is 1), but they are not \"pairwise\" coprime (because gcd(4, 6) = 2).\n\nThe concept of pairwise coprimality is important as a hypothesis in many results in number theory, such as the Chinese remainder theorem.\n\nIt is possible for an infinite set of integers to be pairwise coprime. Notable examples include the set of all prime numbers, the set of elements in Sylvester's sequence, and the set of all Fermat numbers.\n\nTwo ideals \"A\" and \"B\" in the commutative ring \"R\" are called coprime (or comaximal) if \"A\" + \"B\" = \"R\". This generalizes Bézout's identity: with this definition, two principal ideals (\"a\") and (\"b\") in the ring of integers Z are coprime if and only if \"a\" and \"b\" are coprime. If the ideals \"A\" and \"B\" of \"R\" are coprime, then \"AB\" = \"A\"∩\"B\"; furthermore, if \"C\" is a third ideal such that \"A\" contains \"BC\", then \"A\" contains \"C\". The Chinese remainder theorem can be generalized to any commutative ring, using coprime ideals.\n\nGiven two randomly chosen integers \"a\" and \"b\", it is reasonable to ask how likely it is that \"a\" and \"b\" are coprime. In this determination, it is convenient to use the characterization that \"a\" and \"b\" are coprime if and only if no prime number divides both of them (see Fundamental theorem of arithmetic).\n\nInformally, the probability that any number is divisible by a prime (or in fact any integer) formula_5 is formula_6; for example, every 7th integer is divisible by 7. Hence the probability that two numbers are both divisible by \"p\" is formula_7, and the probability that at least one of them is not is formula_8. Any finite collection of divisibility events associated to distinct primes is mutually independent. For example, in the case of two events, a number is divisible by primes \"p\" and \"q\" if and only if it is divisible by \"pq\"; the latter event has probability 1/\"pq\". If one makes the heuristic assumption that such reasoning can be extended to infinitely many divisibility events, one is led to guess that the probability that two numbers are coprime is given by a product over all primes,\n\nHere \"ζ\" refers to the Riemann zeta function, the identity relating the product over primes to \"ζ\"(2) is an example of an Euler product, and the evaluation of \"ζ\"(2) as \"π\"/6 is the Basel problem, solved by Leonhard Euler in 1735.\n\nThere is no way to choose a positive integer at random so that each positive integer occurs with equal probability, but statements about \"randomly chosen integers\" such as the ones above can be formalized by using the notion of \"natural density\". For each positive integer \"N\", let \"P\" be the probability that two randomly chosen numbers in formula_10 are coprime. Although \"P\" will never equal formula_11 exactly, with work one can show that in the limit as formula_12, the probability formula_13 approaches formula_11.\n\nMore generally, the probability of \"k\" randomly chosen integers being coprime is 1/\"ζ\"(\"k\").\n\nAll pairs of positive coprime numbers formula_15 (with formula_16) can be arranged in two disjoint complete ternary trees, one tree starting from formula_17 (for even-odd and odd-even pairs), and the other tree starting from formula_18 (for odd-odd pairs). The children of each vertex formula_19 are generated as follows:\n\nThis scheme is exhaustive and non-redundant with no invalid members.\n\n"}
{"id": "6557", "url": "https://en.wikipedia.org/wiki?curid=6557", "title": "Control unit", "text": "Control unit\n\nThe control unit (CU) is a component of a computer's central processing unit (CPU) that directs the operation of the processor. It tells the computer's memory, arithmetic/logic unit and input and output devices on how to respond to a program's instructions.\n\nIt directs the operation of the other units by providing timing and control signals.\nMost computer resources are managed by the CU. It directs the flow of data between the CPU and the other devices. John von Neumann included the control unit as part of the von Neumann architecture. In modern computer designs, the control unit is typically an internal part of the CPU with its overall role and operation unchanged since its introduction.\n\nThe Control Unit (CU) is digital circuitry contained within the processor that coordinates the sequence of data movements into, out of, and between a processor's many sub-units. The result of these routed data movements through various digital circuits (sub-units) within the processor produces the manipulated data expected by a software instruction (loaded earlier, likely from memory). It controls (conducts) data flow inside the processor and additionally provides several external control signals to the rest of the computer to further direct data and instructions to/from processor external destination's (i.e. memory).\n\nExamples of devices that require a CU are CPUs and graphics processing units (GPUs). The CU receives external instructions or commands which it converts into a sequence of control signals that the CU applies to the data path to implement a sequence of register-transfer level operations.\n\nMore precisely, the Control Unit (CU) is generally a sizable collection of complex digital circuitry interconnecting and controlling the many execution units (i.e. ALU, data buffers, registers) contained within a CPU. The CU is normally the first CPU unit to accept from an externally stored computer program, a single instruction (based on the CPU's instruction set). The CU then decodes this individual instruction into several sequential steps (fetching addresses/data from registers/ memory, managing execution [i.e. data sent to the ALU or I/O], and storing the resulting data back into registers/memory) that controls and coordinates the CPU's inner works to properly manipulate the data. The design of these sequential steps are based on the needs of each instruction and can range in number of steps, the order of execution, and which units are enabled. Thus by only using a program of set instructions in memory, the CU will configure all the CPU's data flows as needed to manipulate the data correctly between instructions. This results in a computer that could run a complete program and requiring no human intervention to make hardware changes between instructions (as had to be done when using only punch cards for computations before stored programmed computers with CUs where invented). These detailed steps from the CU dictate which of the CPU's interconnecting hardware control signals to enable/disable or which CPU units are selected/de-selected and the unit's proper order of execution as required by the instruction's operation to produce the desired manipulated data. Additionally, the CU's orderly hardware coordination properly sequences these control signals then configures the many hardware units comprising the CPU, directing how data should also be moved, changed, and stored outside the CPU (i.e. memory) according to the instruction's objective. Depending on the type of instruction entering the CU, the order and number of sequential steps produced by the CU could vary the selection and configuration of which parts of the CPU's hardware are utilized to achieve the instruction's objective (mainly moving, storing, and modifying data within the CPU). This one feature, that efficiently uses just software instructions to control/select/configure a computer's CPU hardware (via the CU) and eventually manipulates a program's data, is a significant reason most modern computers are flexible and universal when running various programs. As compared to some 1930s or 1940s computers without a proper CU, they often required rewiring their hardware when changing programs. This CU instruction decode process is then repeated when the Program Counter is incremented to the next stored program address and the new instruction enters the CU from that address, and so on till the programs end.\n\nOther more advanced forms of Control Units manage the translation of instructions (but not the data containing portion) into several micro-instructions and the CU manages the scheduling of the micro-instructions between the selected execution units to which the data is then channeled and changed according to the execution unit's function (i.e., ALU contains several functions). On some processors, the Control Unit may be further broken down into additional units, such as an instruction unit or scheduling unit to handle scheduling, or a retirement unit to deal with results coming from the instruction pipeline. Again, the Control Unit orchestrates the main functions of the CPU: carrying out stored instructions in the software program then directing the flow of data throughout the computer based upon these instructions (roughly likened to how traffic lights will systematically control the flow of cars [containing data] to different locations within the traffic grid [CPU] until it parks at the desired parking spot [memory address/register]. The car occupants [data] then go into the building [execution unit] and comes back changed in some way then get back into the car and returns to another location via the controlled traffic grid).\n\nHardwired control units are implemented through use of combinational logic units, featuring a finite number of gates that can generate specific results based on the instructions that were used to invoke those responses. Hardwired control units are generally faster than microprogrammed designs.\n\nTheir design uses a fixed architecture—it requires changes in the wiring if the instruction set is modified or changed.\nThis architecture is preferred in reduced instruction \nset computers (RISC) as they use a simpler instruction set.\n\nA controller that uses this approach can operate at high speed; however, it has little flexibility, and the complexity of the instruction set it can implement is limited.\n\nThe hardwired approach has become less popular as computers have evolved. Previously, control units for CPUs used ad-hoc logic, and they were difficult to design.\n\nThe idea of microprogramming was introduced by Maurice Wilkes in 1951 as an intermediate level to execute computer program instructions. Microprograms were organized as a sequence of \"microinstructions\" and stored in special control memory. The algorithm for the microprogram control unit is usually specified by flowchart description. The main advantage of the microprogram control unit is the simplicity of its structure. Outputs of the controller are organized in microinstructions and they can be easily replaced.\n\n"}
{"id": "6558", "url": "https://en.wikipedia.org/wiki?curid=6558", "title": "Cello", "text": "Cello\n\nThe cello ( ; plural cellos or celli) or violoncello ( ; ) is a bowed, and sometimes plucked, string instrument with four strings tuned in perfect fifths. The strings from low to high are generally tuned to C, G, D and A, an octave lower than the viola. It is the bass member of the violin family of musical instruments, which also includes the violin, viola and the double bass. The cello is used as a solo musical instrument, as well as in chamber music ensembles (e.g., string quartet), string orchestras, as a member of the string section of symphony orchestras, and some types of rock bands. It is the second-largest and second lowest (in pitch) bowed string instrument in the modern symphony orchestra, the double bass being the largest and having the lowest (deepest) pitch.\n\nMusic for the cello is generally written in the bass clef, but both tenor clef and treble clef are also used for higher-range parts, both in orchestral/chamber music parts and in solo cello works. A person who plays the cello is called a \"cellist\" or \"violoncellist\". In a small Classical ensemble, such as a string quartet, the cello typically plays the bass part, the lowest-pitched musical line of the piece. In orchestra, in Baroque era (ca. 1600-1750) and Classical period (ca. 1725-1800), the cello typically plays the bass part, generally an octave higher than the double basses. In Baroque era music, the cello is used to play the basso continuo bassline, typically along with a keyboard instrument (e.g., pipe organ or harpsichord) or a fretted, plucked stringed instrument (e.g., lute or theorbo). In a Baroque performance, the cello player might be joined by other bass instruments, playing double bass, viol or other low-register instruments.\n\nThe name \"cello\" is derived from the ending of the Italian \"violoncello\", which means \"little violone\". The violone (\"big viol\") was the lowest-pitched instrument of the viol family, the group of stringed instruments that went out of fashion around the end of the 17th century in most countries except France, where they survived another half-century before the louder violin family came into greater favour in that country as well. In modern symphony orchestras, it is the second largest stringed instrument (the double bass is the largest). Thus, the name \"violoncello\" contained both the augmentative \"\"-one\"\" (\"big\") and the diminutive \"\"-cello\"\" (\"little\"). By the turn of the 20th century, it had become common to shorten the name to 'cello, with the apostrophe indicating the missing stem. It is now customary to use \"cello\" without apostrophe as the full designation. \"Viol\" is derived from the root \"viola\", which was derived from Medieval Latin \"vitula\", meaning stringed instrument.\n\nCellos are tuned in fifths, starting with C (two octaves below middle C), followed by G, D, and then A. It is tuned in the same intervals as the viola, but an octave lower. Unlike the violin or viola but similar to the double bass, the cello has an endpin that rests on the floor to support the instrument's weight. The cello is most closely associated with European classical music, and has been described as the closest sounding instrument to the human voice. The instrument is a part of the standard orchestra, as part of the string section, and is the bass voice of the string quartet (although many composers give it a melodic role as well), as well as being part of many other chamber groups. A large number of concertos and sonatas have been written for the cello.\n\nAmong the most well-known Baroque works for the cello are Johann Sebastian Bach's six unaccompanied Suites. The \"Prelude\" from the \"First Suite\" is particularly famous. From the Classical era, the two concertos by Joseph Haydn in C major and D major stand out, as do the five sonatas for cello and pianoforte of Ludwig van Beethoven, which span the important three periods of his compositional evolution. Romantic era repertoire includes the Robert Schumann Concerto, the Antonín Dvořák Concerto as well as the two sonatas and the Double Concerto by Johannes Brahms. Compositions from the early 20th century include Edward Elgar's Cello Concerto in E minor, Claude Debussy's Sonata for Cello and Piano, and unaccompanied cello sonatas by Zoltán Kodály and Paul Hindemith. The cello's versatility made it popular with composers in the mid- to late-20th century such as Sergei Prokofiev, Dmitri Shostakovich, Benjamin Britten, György Ligeti, Witold Lutoslawski and Henri Dutilleux, encouraged by soloists who specialized in contemporary music (such as Siegfried Palm and Mstislav Rostropovich) commissioning from and collaborating with composers.\n\nThe cello is increasingly common in traditional fiddle music, especially Scottish fiddle music. Well known players include Natalie Haas, Mike Block and Liz Davis Maxfield. In the 2010s, the instrument is less common in popular music, but was commonly used in 1970s pop and disco music. Today it is still sometimes featured in pop and rock recordings, examples of which are noted later in this article. The cello has also recently appeared in major hip-hop and R & B performances, such as singers Rihanna and Ne-Yo's performance at the American Music Awards. The instrument has also been modified for Indian classical music by Nancy Lesh and Saskia Rao-de Haas.\n\nThe violin family, including cello-sized instruments, emerged c. 1500 as family of instruments distinct from the viola da gamba family. The earliest depictions of the violin family, from northern Italy c. 1530, show three sizes of instruments, roughly corresponding to what we now call violins, violas, and cellos. Contrary to a popular misconception, the cello did not evolve from the viola da gamba, but existed alongside it for about two and a half centuries. The violin family is also known as the viola da braccio (meaning viola of the arm) family, a reference to the primary way the members of the family are held. This is to distinguish it from the viola da gamba (meaning viola of the leg) family, in which all the members are all held with the legs. The likely predecessors of the violin family include the lira da braccio and the rebec. The earliest surviving cellos are made by Andrea Amati, the first known member of the celebrated Amati family of luthiers. \n\nThe direct ancestor to the violoncello was the bass violin. Monteverdi referred to the instrument as \"basso de viola da braccio\" in \"Orfeo\" (1607). Although the first bass violin, possibly invented as early as 1538, was most likely inspired by the viol, it was created to be used in consort with the violin. The bass violin was actually often referred to as a \"\"violone\"\", or \"large viola\", as were the viols of the same period. Instruments that share features with both the bass violin and the \"viola da gamba\" appear in Italian art of the early 16th century.\n\nThe invention of wire-wound strings (fine wire around a thin gut core), around 1660 in Bologna, allowed for a finer bass sound than was possible with purely gut strings on such a short body. Bolognese makers exploited this new technology to create the cello, a somewhat smaller instrument suitable for solo repertoire due to both the timbre of the instrument and the fact that the smaller size made it easier to play virtuosic passages . This instrument had disadvantages as well, however. The cello's light sound was not as suitable for church and ensemble playing, so it had to be doubled by organ, theorbo or violone.\n\nAround 1700, Italian players popularized the cello in northern Europe, although the bass violin (basse de violon) continued to be used for another two decades in France. Many existing bass violins were literally cut down in size to convert them into cellos according to the smaller pattern developed by Stradivarius, who also made a number of old pattern large cellos (the 'Servais'). The sizes, names, and tunings of the cello varied widely by geography and time. The size was not standardized until around 1750.\n\nDespite similarities to the viola da gamba, the cello is actually part of the viola da braccio family, meaning \"viol of the arm\", which includes, among others, the violin and viola. Though paintings like Bruegel's \"The Rustic Wedding\", and Jambe de Fer in his \"Epitome Musical\" suggest that the bass violin had alternate playing positions, these were short-lived and the more practical and ergonomic \"a gamba\" position eventually replaced them entirely.\nBaroque era cellos differed from the modern instrument in several ways. The neck has a different form and angle, which matches the baroque bass-bar and stringing. Modern cellos have an endpin at the bottom to support the instrument (and transmit some of the sound through the floor), while Baroque cellos are held only by the calves of the player. Modern bows curve in and are held at the frog; Baroque bows curve out and are held closer to the bow's point of balance. Modern strings normally have a metal core, although some use a synthetic core; Baroque strings are made of gut, with the G and C strings wire-wound. Modern cellos often have fine-tuners connecting the strings to the tailpiece, which make it much easier to tune the instrument, but such pins are rendered ineffective by the flexibility of the gut strings used on Baroque cellos. Overall, the modern instrument has much higher string tension than the Baroque cello, resulting in a louder, more projecting tone, with fewer overtones.\n\nFew educational works specifically devoted to the cello existed before the 18th century, and those that do exist contain little value to the performer beyond simple accounts of instrumental technique. One of the earliest cello manuals is Michel Corrette's \"Méthode, thèorique et pratique pour apprendre en peu de temps le violoncelle dans sa perfection\" (Paris, 1741).\n\nCellos are part of the standard symphony orchestra, which usually includes eight to twelve cello players. The cello section, in standard orchestral seating, is located on stage left (the audience's right) in the front, opposite the first violin section. However, some orchestras and conductors prefer switching the positioning of the viola and cello sections. The \"principal\" cellist is the section leader, determining bowings for the section in conjunction with other string principals, playing solos and leading entrances (when the section begins to play its part). Principal players always sit closest to the audience.\n\nThe cellos are a critical part of orchestral music; all symphonic works involve the cello section, and many pieces require cello soli or solos. Much of the time, cellos provide part of the low-register harmony for the orchestra. Often, the cello section plays the melody for a brief period, before returning to the harmony role. There are also cello concertos, which are orchestral pieces that feature a solo cellist accompanied by an entire orchestra.\n\nThere are numerous cello concertos – where a solo cello is accompanied by an orchestra – notably 25 by Vivaldi, 12 by Boccherini, at least 3 by Haydn, 3 by C. P. E. Bach, 2 by Saint-Saëns, 2 by Dvořák, and one each by Schumann, Lalo, and Elgar. There were also some cellists who, while not otherwise composers, did write cello-specific repertoire, such as Nikolaus Kraft who wrote six cello concertos. Beethoven's Triple Concerto for Cello, Violin and Piano and Brahms' Double Concerto for Cello and Violin are also part of the concertante repertoire although in both cases the cello shares solo duties with at least one other instrument. Moreover, several composers wrote large-scale pieces for cello and orchestra, which are concertos in all but name. Some familiar \"concertos\" are Richard Strauss' tone poem \"Don Quixote\", Tchaikovsky's \"Variations on a Rococo Theme\", Bloch's \"Schelomo\" and Bruch's \"Kol Nidrei\".\n\nIn the 20th century, the cello repertoire grew immensely. This was partly due to the influence of virtuoso cellist Mstislav Rostropovich who inspired, commissioned and premiered dozens of new works. Among these, Prokofiev's \"Symphony-Concerto\", Britten's \"Cello Symphony\", the concertos of Shostakovich and Lutosławski as well as Dutilleux's \"Tout un monde lointain...\" have already become part of the standard repertoire. Other major composers who wrote concertante works for him include Messiaen, Jolivet, Berio and Penderecki. In addition, Arnold, Barber, Glass, Hindemith, Honegger, Ligeti, Myaskovsky, Penderecki, Rodrigo, Villa-Lobos and Walton also wrote major concertos for other cellists, notably for Gaspar Cassadó, Gregor Piatigorsky, Siegfried Palm and Julian Lloyd Webber.\n\nThere are also many sonatas for cello and piano. Those written by Beethoven, Mendelssohn, Chopin, Brahms, Grieg, Rachmaninoff, Debussy, Fauré, Shostakovich, Prokofiev, Poulenc, Carter, and Britten are particularly well known.\n\nOther important pieces for cello and piano include Schumann's five \"Stücke im Volkston\" and transcriptions like Schubert's Arpeggione Sonata (originally for arpeggione and piano), César Franck's Cello Sonata (originally a violin sonata, transcribed by Jules Delsart with the composer's approval), Stravinsky's \"Suite italienne\" (transcribed by the composer – with Gregor Piatigorsky – from his ballet \"Pulcinella\") and Bartók's first rhapsody (also transcribed by the composer, originally for violin and piano).\n\nThere are pieces for cello solo, J. S. Bach's six Suites for Cello (which are among the best-known solo cello pieces), Kodály's Sonata for Solo Cello and Britten's three Cello Suites. Other notable examples include Hindemith's and Ysaÿe's Sonatas for Solo Cello, Dutilleux's \"Trois Strophes sur le Nom de Sacher\", Berio's \"Les Mots Sont Allés\", Cassadó's Suite for Solo Cello, Ligeti's Solo Sonata, Carter's two \"Figment\"s and Xenakis' \"Nomos Alpha\" and \"Kottos\".\n\nThe cello is a member of the traditional string quartet as well as string quintets, sextet or trios and other mixed ensembles.\nThere are also pieces written for two, three, four or more cellos; this type of ensemble is also called a \"cello choir\" and its sound is familiar from the introduction to Rossini's William Tell Overture as well as Zaccharia's prayer scene in Verdi's Nabucco. Tchaikovsky's 1812 Overture also starts with a cello ensemble, with four cellos playing the top lines and two violas playing the bass lines. As a self-sufficient ensemble, its most famous repertoire is Villa-Lobos' first of his Bachianas Brasileiras for cello ensemble (the fifth is for soprano and 8 cellos). Other examples are Offenbach's cello duets, quartet, and sextet, Pärt's Fratres for 8 cellos and Boulez' \"Messagesquisse\" for 7 cellos, or even Villa-Lobos' rarely played \"Fantasia Concertante\" (1958) for 32 cellos. The 12 cellists of the Berlin Philharmonic Orchestra (or \"the Twelve\" as they have since taken to being called) specialize in this repertoire and have commissioned many works, including arrangements of well-known popular songs.\n\nThe cello is less common in popular music than in classical music. Several bands feature a cello in their standard line-up, e.g. Joe Kwon of The Avett Brothers. The more common use in pop and rock is to bring the instrument in for a particular song. In the 1960s, artists such as the Beatles and Cher used the cello in popular music, in songs such as The Beatles' \"Yesterday\", \"Eleanor Rigby\" and \"Strawberry Fields Forever\", and Cher's \"Bang Bang (My Baby Shot Me Down)\". \"Good Vibrations\" by the Beach Boys includes the cello in its instrumental ensemble, which includes a number of instruments unusual for this sort of music. Bass guitarist Jack Bruce, who had originally studied music on a performance scholarship for cello, played a prominent cello part in \"As You Said\" on Cream's \"Wheels of Fire\" studio album (1968).\nIn the 1970s, the Electric Light Orchestra enjoyed great commercial success taking inspiration from so-called \"Beatlesque\" arrangements, adding the cello (and violin) to the standard rock combo line-up and in 1978 the UK based rock band, Colosseum II, collaborated with cellist Julian Lloyd Webber on the recording \"Variations\". Most notably, Pink Floyd included a cello solo in their 1970 epic instrumental \"Atom Heart Mother\". Bass guitarist Mike Rutherford of Genesis was originally a cellist and included some cello parts in their \"Foxtrot\" album.\n\nEstablished non-traditional cello groups include Apocalyptica, a group of Finnish cellists best known for their versions of Metallica songs, Rasputina, a group of cellists committed to an intricate cello style intermingled with Gothic music, the Massive Violins, an ensemble of seven singing cellists known for their arrangements of rock, pop and classical hits, Von Cello, a cello fronted rock power trio, Break of Reality who mix elements of classical music with the more modern rock and metal genre, Cello Fury, a cello rock band that performs original rock/classical crossover music, and Jelloslave, a Minneapolis-based Cello duo with two percussionists. These groups are examples of a style that has become known as cello rock. The crossover string quartet bond also includes a cellist. Silenzium and Cellissimo Quartet are Russian (Novosibirsk) groups playing rock and metal and having more and more popularity in Siberia. Cold Fairyland from Shanghai, China is using a cello along a Pipa as the main solo instrument to create East meets West progressive (folk) rock.\n\nMore recent bands using the cello are Aerosmith, The Auteurs, Nirvana, Oasis, Murder by Death, Cursive, Clean Bandit, A Genuine Freakshow, Ra Ra Riot, Smashing Pumpkins, James, Talk Talk, Phillip Phillips, OneRepublic, and the baroque rock band Arcade Fire. An Atlanta-based trio, King Richard's Sunday Best, also uses a cellist in their lineup. So-called \"chamber pop\" artists like Kronos Quartet, The Vitamin String Quartet and Margot and the Nuclear So and So's have also recently made cello common in modern alternative rock. Heavy metal band System of a Down has also made use of the cello's rich sound. The indie rock band The Stiletto Formal are known for using a cello as a major staple of their sound, similarly, the indie rock band Canada employs two cello players in their lineup. The orch-rock group, The Polyphonic Spree, which has pioneered the use of stringed and symphonic instruments, employs the cello in very creative ways for many of their \"psychedelic-esque\" melodies. The first wave screamo band \"I Would Set Myself On Fire For You\" featured a cello as well as a viola to create a more folk-oriented sound. The band, Panic! at the Disco uses a cello in their song, \"Build God, Then We'll Talk\". The lead vocalist of the band, Brendon Urie, also did the recording of the cello solo.\n\nIn jazz, bassists Oscar Pettiford and Harry Babasin were among the first to use the cello as a solo instrument; both tuned their instrument in fourths, an octave above the double bass. Fred Katz (who was not a bassist) was one of the first notable jazz cellists to use the instrument's standard tuning and arco technique. Contemporary jazz cellists include Abdul Wadud, Diedre Murray, Ron Carter, Dave Holland, David Darling, Lucio Amanti, Akua Dixon, Ernst Reijseger, Fred Lonberg-Holm, Tom Cora, Vincent Courtois, John O'Keefe, Stephan Braun, Jean-Charles Capon, Erik Friedlander, Enrico Guerzoni and James Hinkley of jazz combo \"Billet-Deux\". Modern musical theatre pieces like Jason Robert Brown's The Last Five Years, Duncan Sheik's Spring Awakening, Adam Guettel's Floyd Collins, and Ricky Ian Gordon's My Life with Albertine use small string ensembles (including solo cellos) to a prominent extent.\n\nIn Indian Classical music Saskia Rao-de Haas is a well established soloist as well as playing duets with her sitarist husband Pt. Shubhendra Rao. Other cellists performing Indian classical music are: Nancy Lesh (Dhrupad) and Anup Biswas. Both Rao and Lesh play the cello sitting cross-legged on the floor.\n\nThe cello can also be used in bluegrass and folk music, with notable players including Ben Sollee of the Sparrow Quartet and the \"Cajun cellist\" Sean Grissom, as well as Vyvienne Long who, in addition to her own projects, has played for those of Damien Rice. Cellists such as Natalie Haas, Abby Newton and Liz Davis Maxfield have contributed significantly to the use of cello playing in Celtic folk music, often with the cello featured as a primary melodic instrument and employing the skills and techniques of traditional fiddle playing. Lindsay Mac is becoming well known for playing the cello like a guitar, with her cover of The Beatles' \"Blackbird\" a big hit on \"The Bob & Tom Show\".\n\nThe cello has been further popularized by the cello duo of Luka Šulić & Stjepan Hauser, known by the stage name 2Cellos, who have been active since 2011. They are famous for playing various songs on the cello, such as \"Smooth Criminal\" (Michael Jackson), \"Thunderstruck\" (AC/DC) and more.\n\nThe cello is typically made from carved wood, although other materials such as carbon fiber or aluminum may be used. A traditional cello has a spruce top, with maple for the back, sides, and neck. Other woods, such as poplar or willow, are sometimes used for the back and sides. Less expensive cellos frequently have tops and backs made of laminated wood. Laminated cellos are widely used in elementary and secondary school orchestras and youth orchestras, because they are much more durable than carved wood cellos (i.e., they are less likely to crack if bumped or dropped) and they are much less expensive.\n\nThe top and back are traditionally hand-carved, though less expensive cellos are often machine-produced. The sides, or ribs, are made by heating the wood and bending it around forms. The cello body has a wide top bout, narrow middle formed by two C-bouts, and wide bottom bout, with the bridge and F holes just below the middle. The top and back of the cello has decorative border inlay known as purfling. While purfling is attractive, it is also functional: if the instrument is struck, the purfling can prevent cracking of the wood. A crack may form at the rim of the instrument, but spreads no further. Without purfling, cracks can spread up or down the top or back. Playing, traveling and the weather all affect the cello and can increase a crack if purfling is not in place. Less expensive instruments typically have painted purfling.\n\nCello manufacturer Luis & Clark constructs cellos from carbon fibre. Carbon fibre instruments are particularly suitable for outdoor playing because of the strength of the material and its resistance to humidity and temperature fluctuations. Luis & Clark has produced over 1000 cellos, some of which are owned by cellists such as Yo-Yo Ma and Josephine van Lier. In the late 1920s and early 1930s, the Aluminum Company of America (Alcoa) as well as German luthier G.A. Pfretzschner produced an unknown number of aluminum cellos (in addition to aluminum double basses and violins).\n\nAbove the main body is the carved neck. The neck has a curved cross-section on its underside, which is where the player's thumb runs along the neck during playing. The neck which leads to a pegbox and the scroll. The neck, pegbox, and scroll are normally carved out of a single piece of wood, usually maple. The fingerboard is glued to the neck and extends over the body of the instrument. The fingerboard is given a curved shape, matching the curve on the bridge. Both the fingerboard and bridge need to be curved so that the performer can bow individual strings. If the cello were to have a flat fingerboard and bridge, as with a typical guitar, the performer would only be able to bow the \"outer\" two strings or bow all the strings. The performer would not be able to play the \"inner\" two strings alone. \n\nThe nut is a raised piece of wood, fitted where the fingerboard meets the pegbox, in which the strings rest in shallow slots or grooves to keep them the correct distance apart. The pegbox houses four tapered tuning pegs, one for each string. The pegs are used to tune the cello by either tightening or loosening the string. The pegs are called \"friction pegs\", because they maintain their position by friction. The scroll is a traditional ornamental part of the cello and a feature of all other members of the violin family. Ebony is usually used for the tuning pegs, fingerboard, and nut, but other hardwoods, such as boxwood or rosewood, can be used. Black fittings on low-cost instruments are often made from inexpensive wood that has been blackened or \"ebonized\" to look like ebony, which is much harder and more expensive. Ebonised parts such as tuning pegs may crack or split, and the black surface of the fingerboard will eventually wear down to reveal the lighter wood underneath.\n\nHistorically, cello strings had cores made out of catgut, which, despite its name is made from sheep or goat intestines which are dried out. Most modern strings used in the 2010s are wound with metallic materials like aluminum, titanium and chromium. Cellists may mix different types of strings on their instruments. The pitches of the open strings are C, G, D, and A (black note heads in the playing range figure above), unless alternative tuning (scordatura) is specified by the composer. Some composers ask that the low C be tuned down to a Bb or B so that the performer can play a different low note on the lowest open string.\n\nThe tailpiece and endpin are found in the lower part of the cello. The tailpiece is the part of the cello to which the \"ball ends\" of the strings are attached by passing them through holes. The tailpiece is attached to the bottom of the cello. The tailpiece is traditionally made of ebony or another hard wood, but can also be made of plastic or steel on lower-cost instruments. It attaches the strings to the lower end of the cello, and can have one or more fine tuners. The fine tuners are used to make smaller adjustments to the pitch of the string. The fine tuners can increase the tension of each string (raising the pitch) or decrease the tension of the string (lowering the pitch). When the performer is putting on a new string, the fine tuner for that string is normally reset to a middle position, and then the peg is turned to bring the string up to pitch. The fine turners are used for subtle, minor adjustments to pitch, such as tuning a cello to the oboe's 440 Hz A note or to tune the cello to a piano.\n\nThe endpin or spike is made of wood, metal or rigid carbon fibre and supports the cello in playing position. The endpin can be retracted into the hollow body of the instrument when the cello is being transported in its case. This makes the cello easier to move about. When the performer wishes to play the cello, the endpin is pulled out to lengthen it. The endpin is locked into the player's preferred length with a screw mechanism. The adjustable nature of endpins enables performers of different ages and body sizes to adjust the endpin length to suit them. In the Baroque period the cello was held between the calves, as there was no endpin at that time. The endpin was \"introduced by Adrien Servais 1845 to give the instrument greater stability\". Modern endpins are retractable and adjustable; older ones were removed when not in use. (The word \"endpin\" sometimes also refers to the button of wood located at this place in all instruments in the violin family, but this is usually called \"tailpin\".) The sharp tip of the cello's endpin is sometimes capped with a rubber tip that protects the tip from dulling and prevents the cello from slipping on the floor. Many cellists use a rubber pad with a metal cup to keep the tip from slipping on the floor. A number of accessories to keep the endpin from slipping; these include ropes which attach to the chair leg and other devices.\n\nThe bridge holds the strings above the cello and transfers their vibrations to the top of the instrument and the soundpost inside (see below). The bridge is not glued, but rather held in place by the tension of the strings. The bridge is usually positioned by the cross point of the \"f-hole\" (i.e., where the horizontal line occurs in the \"f\"). The f-holes, named for their shape, are located on either side of the bridge, and allow air to move in and out of the instrument as part of the sound-production process. The f-holes also act as access points to the interior of the cello for repairs or maintenance. Sometimes a small length of rubber hose containing a water-soaked sponge, called a Dampit, is inserted through the f-holes, and serves as a humidifier. This keeps the wood components of the cello from drying out.\n\nInternally, the cello has two important features: a bass bar, which is glued to the underside of the top of the instrument, and a round wooden sound post, a solid wooden cylinder which is wedged between the top and bottom plates. The bass bar, found under the bass foot of the bridge, serves to support the cello's top and distribute the vibrations from the strings to the body of the instrument. The sound post, found under the treble side of the bridge, connects the back and front of the cello. Like the bridge, the sound post is not glued, but is kept in place by the tensions of the bridge and strings. Together, the bass bar and sound post transfer the strings' vibrations to the top (front) of the instrument (and to a lesser extent the back), acting as a diaphragm to produce the instrument's sound.\n\nCellos are constructed and repaired using hide glue, which is strong but reversible, allowing for disassembly when needed. Tops may be glued on with diluted glue, since some repairs call for the removal of the top. Theoretically, hide glue is weaker than the body's wood, so as the top or back shrinks side-to-side, the glue holding it lets go, so the plate does not crack. Cellists repairing cracks in their cello do not use regular wood glue, because it cannot be steamed open when a repair has to be made by a luthier.\n\nTraditionally, bows are made from pernambuco or brazilwood. Both come from the same species of tree (\"Caesalpinia echinata\"), but pernambuco, used for higher-quality bows, is the heartwood of the tree and is darker in color than brazilwood (which is sometimes stained to compensate). Pernambuco is a heavy, resinous wood with great elasticity, which makes it an ideal wood for instrument bows. Horsehair is stretched out between the two ends of the bow. The taut horsehair is drawn over the strings to produce the cello's characteristic tone. A small knob is twisted to increase or decrease the tension of the horsehair. The tension on the bow is released when the instrument is not being used. The amount of tension a cellist puts on the bow hair depends on the preferences of the player, the style of music being played, and for students, the preferences of their teacher.\n\nBows are also made from other materials, such as carbon-fibre—stronger than wood—and fiberglass (often used to make inexpensive, lower-quality student bows). An average cello bow is long (shorter than a violin or viola bow) high (from the frog to the stick) and wide. The frog of a cello bow typically has a rounded corner like that of a viola bow, but is wider. A cello bow is roughly heavier than a viola bow, which in turn is roughly heavier than a violin bow. \n\nBow hair is traditionally horsehair, though synthetic hair, in varying colors, is also used. Prior to playing, the musician tightens the bow by turning a screw to pull the frog (the part of the bow under the hand) back, and increase the tension of the hair. Rosin is applied by the player to make the hairs sticky. Bows need to be re-haired periodically. Baroque style (1600–1750) cello bows were much thicker and were formed with a larger outward arch when compared to modern cello bows. The inward arch of a modern cello bow produces greater tension, which in turn gives off a louder sound.\n\nThe cello bow has also been used to play electric guitars. Jimmy Page pioneered its application on tracks such as \"Dazed and Confused\". The post-rock Icelandic band Sigur Rós's lead singer often plays a guitar using a cello bow.\n\nIn 1989, the German cellist Michael Bach began developing a curved bow, encouraged by John Cage, Dieter Schnebel, Mstislav Rostropovich and Luigi Colani: and since then many pieces have been composed especially for it. This curved bow (\"BACH.Bow\") is a convex curved bow which, unlike the ordinary bow, renders possible polyphonic playing on the various strings of the instrument. The solo repertoire for violin and cello by J. S. Bach the BACH.Bow is particularly suited to it: and it was developed with this in mind, polyphonic playing being required, as well as monophonic.\n\nWhen a string is bowed or plucked, it vibrates and moves the air around it, producing sound waves. Because the string is quite thin, not much air is moved by the string itself, and consequently if the string was not mounted on a hollow body, the sound would be weak. In acoustic stringed instruments such as the cello, this lack of volume is solved by mounting the vibrating string on a larger hollow wooden body. The vibrations are transmitted to the larger body, which can move more air and produce a louder sound. Different designs of the instrument produces variations in the instrument’s vibrational patterns and thus changes the character of the sound produced. A string’s fundamental pitch can be adjusted by changing its stiffness, which depends on tension and length. Tightening a string stiffens it by increasing both the outward forces along its length and the net forces it experiences during a distortion. A cello can be tuned by adjusting the tension of its strings, by turning the tuning pegs mounted on its pegbox, and tension adjusters (fine tuners) on the tail piece.\n\nA string’s length also affects its fundamental pitch. Shortening a string stiffens it by increasing its curvature during a distortion and subjecting it to larger net forces. Shortening the string also reduces its mass, but does not alter the mass per unit length, and it is the latter ratio rather than the total mass which governs the frequency. The string vibrates in a standing wave whose speed of propagation is given by , where \"T\" is the tension and \"m\" is the mass per unit length; there is a node at either end of the vibrating length, and thus the vibrating length \"l\" is half a wavelength. Since the frequency of any wave is equal to the speed divided by the wavelength, we have frequency = . (Note that some writers, including Muncaster (cited below) use the Greek letter \"μ\" in place of \"m\".) Thus shortening a string increases the frequency, and thus the pitch. Because of this effect, you can raise and change the pitch of a string by pressing it against the fingerboard in the cello’s neck and effectively shortening it. Likewise strings with less mass per unit length, if under the same tension, will have a higher frequency and thus higher pitch than more massive strings. This is a prime reason why the different strings on all string instruments have different fundamental pitches, with the lightest strings having the highest pitches.\n\nA played note of E or F has a frequency which is often very close to the natural resonating frequency of the body of the instrument, and if the problem is not addressed this can set the body into near resonance. This may cause an unpleasant sudden amplification of this pitch, and additionally a loud beating sound results from the interference produced between these nearby frequencies; this is known as the “wolf tone” because it is an unpleasant growling sound. The wood resonance appears to be split into two frequencies by the driving force of the sounding string. These two periodic resonances beat with each other. This wolf tone must be eliminated or significantly reduced for the cello to play the nearby notes with a pleasant tone. This can be accomplished by modifying the cello front plate, attaching a wolf eliminator (a metal cylinder or a rubber cylinder encased in metal), or moving the sound post.\nWhen a string is bowed or plucked to produce a note, the fundamental note is accompanied by higher frequency overtones. Each sound has a particular recipe of frequencies that combine to make the total sound.\n\nPlaying the cello is done while seated with the instrument supported on the floor by the endpin. The left hand fingertips stop the strings on the fingerboard, determining the pitch of the fingered note. The right hand plucks or bows the strings to sound the notes. The left hand fingertips stop the strings along their length, determining the pitch of each fingered note. Stopping the string closer to the bridge results in higher-pitched sound, because the vibrating string length has been shortened. In the \"neck\" positions (which use just less than half of the fingerboard, nearest the top of the instrument), the thumb rests on the back of the neck; in \"thumb position\" (a general name for notes on the remainder of the fingerboard) the thumb usually rests alongside the fingers on the string and the side of the thumb is used to play notes. The fingers are normally held curved with each knuckle bent, with the fingertips in contact with the string. If a finger is required on two (or more) strings at once to play perfect fifths (in double stops or chords) it is used flat. In slower, or more expressive playing, the contact point can move slightly away from the nail to the pad of the finger, allowing a fuller vibrato.\n\nVibrato is a small oscillation in the pitch of a note, usually considered expressive. Harmonics played on the cello fall into two classes; natural and artificial. Natural harmonics are produced by lightly touching (but not depressing) the string with the finger at certain places, and then bowing (or, rarely, plucking) the string. For example, the halfway point of the string will produce a harmonic that is one octave above the unfingered (open) string. Natural harmonics only produce notes that are part of the harmonic series on a particular string. Artificial harmonics (also called false harmonics or stopped harmonics), in which the player depresses the string fully with one finger while touching the same string lightly with another finger, can produce any note above middle C. \nGlissando (Italian for \"sliding\") is an effect played by sliding the finger up or down the fingerboard without releasing the string. This causes the pitch to rise and fall smoothly, without separate, discernible steps.\n\nIn cello playing, the bow is much like the breath of a wind instrument player. Arguably, it is the major determinant in the expressiveness of the playing. The right hand holds the bow and controls the duration and character of the notes. The bow is drawn across the strings roughly halfway between the end of the fingerboard and the bridge, in a direction perpendicular to the strings. The bow is held with all five fingers of the right hand, the thumb opposite the fingers and closer to the cellist's body. Tone production and volume of sound depend on a combination of several factors. The three most important ones are: bow speed, weight applied to the string, and point of contact of the bow hair with the string. \n\nDouble stops involve the playing of two notes at the same time. Two strings are fingered simultaneously, and the bow is drawn so as to sound them both at once. In pizzicato playing, the string is plucked directly with the fingers or thumb. Pizzicato is often abbreviated as \"Pizz.\". Position of the hand is slightly over the finger board and away from the bridge. \n\nA player using the Col legno technique rubs the strings with the wood of the bow rather than the hair. In spiccato playing, the strings are not \"drawn\" by the bow hair but struck by it, while still retaining some horizontal motion, to generate a more percussive, crisp sound. In staccato, the player moves the bow a small distance and stops it on the string, making a short sound, the rest of the written duration being taken up by silence. \nLegato is a technique where the notes are smoothly connected without accents or breaks. It is noted by a slur (curved line) above or below - depending on their position on the staff - the notes of the passage that is to be played legato.\n\n\"Sul ponticello\" (\"on the bridge\") refers to bowing closer to the bridge, while \"sul tasto\" (\"on the fingerboard\") calls for bowing nearer the end of the fingerboard. Sul tasto produces a more flute-like sound, with more emphasis on the fundamental frequency of the note, and softer overtones.\n\nStandard-sized cellos are referred to as \"full-size\" or \"\" but are also made in smaller (fractional) sizes (e.g. , , , , , , ). The smaller cellos are identical to standard cellos in construction, range, and usage, but are simply scaled-down for the benefit of children and shorter adults.\n\nCellos in sizes larger than do exist, and cellists with unusually large hands may require such a non-standard instrument. Cellos made before approximately 1700 tended to be considerably larger than those made and commonly played today. Around 1680, changes in string-making technology made it possible to play lower-pitched notes on shorter strings. The cellos of Stradivari, for example, can be clearly divided into two models: the style made before 1702, characterized by larger instruments (of which only three exist in their original size and configuration), and the style made during and after 1707, when Stradivari began making smaller cellos. This later model is the design most commonly used by modern luthiers. The scale length of a cello is about . The new size offered fuller tonal projection and greater range of expression. The instrument in this form was able to contribute to more pieces musically and offered the possibility of greater physical dexterity for the player to develop technique.\n\nThere are many accessories for the cello.\n\n\n\nCellos are made by luthiers, specialists in building and repairing stringed instruments, ranging from guitars to violins. The following luthiers are notable for the cellos they have produced:\n\nA person who plays the cello is called a \"cellist\". For a list of notable cellists, see the list of cellists and .\n\nCareers in cello vary widely by genre and by region or country. Most cellists earn their living from a mixture of performance and teaching jobs. The first step to getting most performance jobs is by playing at an audition. In some styles of music, cellists may be asked to sight read printed music or perform standard repertoire with an ensemble.\n\nIn classical music, cellists audition for playing jobs in orchestras and for admission into university or Conservatory programs or degrees. At a classical cello audition, the performer typically plays a movement from a Bach suite or a movement from a concerto and a variety of excerpts from the orchestral literature. Orchestral auditions are typically held in front of a panel that includes the conductor, the Concertmaster, the Principal cellist and other principal players.\n\nThe most promising candidates are invited to return for a second or third round of auditions, which allows the conductor and the panel to compare the best candidates. Performers may be asked to sight read orchestral music. The final stage of the audition process in some orchestras is a \"test week\", in which the performer plays with the orchestra for a week or two, which allows the conductor and principal players to see if the individual can function well in an actual performance setting.\n\nPerformance jobs include playing as a freelancer in small groups, playing in a chamber music group, large ensembles, or performing solo music, either live onstage or as a session player for radio or TV broadcasts or for recordings; and working as the employee of an orchestra, big band, or recording studio. Many cello players find extra work by substituting (\"subbing\") for cellists who are double-booked or ill. It is hard for many cello players to be able to find full-time, full-year work at a single job. About the closest that a cellist can come to this is in the case of those who win an audition at a professional orchestra. Even full-time orchestra jobs do not usually last for the entire year. When the orchestra stops playing (which is often in the summer), orchestral cellists have to find other work, either as a teacher or coach, or in another group.\n\nTeaching work for cellists includes giving private lessons in the home or at colleges and universities; coaching cellists who are preparing for recordings or auditions; doing group coaching at music camps or for youth ensembles; and working as a high school music teacher. Due to the limited number of full-time orchestral jobs, many classical cellists are not able to find full-time work with a single orchestra. Some cellists increase their employ-ability by learning several different styles, such as folk or pop.\n\nIn some cases, cellists supplement their performing and teaching income with other related music jobs, such as working as a stringed instrument repairer (luthier); as a contractor who hires musicians for orchestras or big bands, composing music, songwriting, conducting, or organizing festivals (e.g., Julian Armour).\n\nSpecific instruments are famous (or become famous) for a variety of reasons. An instrument's notability may arise from its age, the fame of its maker, its physical appearance, its acoustic properties, and its use by notable performers. The most famous instruments are generally known for all of these things. The most highly prized instruments are now collector's items, and are priced beyond the reach of most musicians. These instruments are typically owned by some kind of organization or investment group, which may loan the instrument to a notable performer. (For example, the Davidov Stradivarius, which is currently in the possession of one of the most widely known living cellists, Yo-Yo Ma, is actually owned by the Vuitton Foundation.)\n\nSome notable cellos:\n\n\n\n\n\n"}
{"id": "6559", "url": "https://en.wikipedia.org/wiki?curid=6559", "title": "Control store", "text": "Control store\n\nA control store is the part of a CPU's control unit that stores the CPU's microprogram. It is usually accessed by a microsequencer. Early types of control store took the form of diode-arrays that were accessed via address decoders, but were later implemented as writable microcode that was stored in a form of read only memory called a writable control store. The outputs generally had to go through a register to prevent a race condition from occurring. The register was clocked by the clock signal of the system it was running on.\n\nEarly control stores were implemented as a diode-array accessed via address decoders, a form of read-only memory. This tradition dates back to the \"program timing matrix\" on the MIT Whirlwind, first described in 1947. Modern VLSI processors instead use matrices of field-effect transistors to build the ROM and/or PLA structures used to control the processor as well as its internal sequencer in a microcoded implementation. IBM System/360 used a variety of techniques: CCROS (Card Capacitor Read-Only Storage) on the Model 30, TROS (Transformer Read-Only Storage) on the Model 40, and BCROS (Balanced Capacitor Read-Only Storage) on the Model 50.\n\nSome computers were built using \"writable microcode\" — rather than storing the microcode in ROM or hard-wired logic, the microcode was stored in a RAM called a \"writable control store\" or \"WCS\". Such a computer is sometimes called a \"Writable Instruction Set Computer\" or \"WISC\". Many of these machines were experimental laboratory prototypes, such as the WISC CPU/16 and the RTX 32P.\n\nThe original System/360 models of IBM mainframe had read-only control store, but later System/360, System/370 and successor models loaded part or all of their microprograms from floppy disks or other DASD into a writable control store consisting of ultra-high speed random-access read-write memory. The System/370 architecture included a facility called Initial-Microprogram Load (IML or IMPL) that could be invoked from the console, as part of Power On Reset (POR) or from another processor in a tightly coupled multiprocessor complex. This permitted IBM to easily repair microprogramming defects in the field. Even when the majority of the control store is stored in ROM, computer vendors would often sell writable control store as an option, allowing the customers to customize the machine's microprogram. Other vendors, e.g., IBM, use the WCS to run microcode for emulator features and hardware diagnostics.\n\nOther commercial machines that used writable microcode include the Burroughs Small Systems (1970s and 1980s), the Xerox processors in their Lisp machines and Xerox Star workstations, the DEC VAX 8800 (\"Nautilus\") family, and the Symbolics L- and G-machines (1980s). Some DEC PDP-10 machines stored their microcode in SRAM chips (about 80 bits wide x 2 Kwords), which was typically loaded on power-on through some other front-end CPU. Many more machines offered user-programmable writable control stores as an option (including the HP 2100, DEC PDP-11/60 and Varian Data Machines V-70 series minicomputers).\nThe Mentec M11 and Mentec M1 stored its microcode in SRAM chips, loaded on power-on through another CPU.\nThe Data General Eclipse MV/8000 (\"Eagle\") had a SRAM writable control store, loaded on power-on through another CPU.\n\nWCS offered several advantages including the ease of patching the microprogram and, for certain hardware generations, faster access than ROMs could provide. User-programmable WCS allowed the user to optimize the machine for specific purposes.\n\nSome CPU designs compile the instruction set to a writable RAM or FLASH inside the CPU (such as the Rekursiv processor and the Imsys Cjip), or an FPGA (reconfigurable computing).\n\nSeveral Intel CPUs in the x86 architecture family have writable microcode, starting with the Pentium Pro in 1995.\nThis has allowed bugs in the Intel Core 2 microcode and Intel Xeon microcode to be fixed in software, rather than requiring the entire chip to be replaced.\nSuch fixes can be installed by Linux, FreeBSD, Microsoft Windows, or the motherboard BIOS.\n\nThe control store usually has a register on its outputs. The outputs that go back into the sequencer to determine the next address have to go through some sort of register to prevent the creation of a race condition. In most designs all of the other bits also go through a register. This is because the machine will work faster if the execution of the next microinstruction is delayed by one cycle. This register is known as a pipeline register. Very often the execution of the next microinstruction is dependent on the result of the current microinstruction, which will not be stable until the end of the current microcycle. It can be seen that either way, all of the outputs of the control store go into one big register. Historically it used to be possible to buy EPROMs with these register bits on the same chip.\n\nThe clock signal determining the clock rate, which is the cycle time of the system, primarily clocks this register.\n\n"}
{"id": "6561", "url": "https://en.wikipedia.org/wiki?curid=6561", "title": "Columba", "text": "Columba\n\nSaint Columba (, 'church dove'; 7 December 521 – 9 June 597) was an Irish abbot and missionary credited with spreading Christianity in what is today Scotland at the start of the Hiberno-Scottish mission. He founded the important abbey on Iona, which became a dominant religious and political institution in the region for centuries. He is the Patron Saint of Derry. He was highly regarded by both the Gaels of Dál Riata and the Picts, and is remembered today as a Christian saint and one of the Twelve Apostles of Ireland.\n\nColumba studied under some of Ireland's most prominent church figures and founded several monasteries in the country. Around 563 he and his twelve companions crossed to Dunaverty near Southend, Argyll in Kintyre before settling in Iona in Scotland, then part of the Ulster kingdom of Dál Riata, where they founded a new abbey as a base for spreading Christianity among the northern Pictish kingdoms who were pagan. He remained active in Irish politics, though he spent most of the remainder of his life in Scotland. Three surviving early medieval Latin hymns may be attributed to him.\n\nColumba was born to Fedlimid and Eithne of the Cenel Conaill in Gartan, near Lough Gartan, in modern County Donegal, in Ireland. On his father's side, he was great-great-grandson of Niall of the Nine Hostages, an Irish high king of the 5th century. He was baptised in Temple-Douglas, in the County Donegal parish of Conwal (midway between Gartan and Letterkenny), by his teacher and foster-uncle Saint Crunathan. It is not known for sure if his name at birth was Columba or if he adopted this name later in life; Adomnan of Iona thought it was his birth name but other Irish sources have claimed his name at birth was Crimthann (meaning 'fox'). In the Irish language his name means 'dove', which is the same name as the Prophet Jonah (Jonah in Hebrew is also 'dove'), which Adomnan of Iona as well as other early Irish writers were aware of, although it is not clear if he was deliberately named after Jonah or not.\nWhen sufficiently advanced in letters he entered the monastic school of Movilla, at Newtownards, under St. Finnian who had studied at St. Ninian's \"Magnum Monasterium\" on the shores of Galloway. He was about twenty, and a deacon when, having completed his training at Movilla, he travelled southwards into Leinster, where he became a pupil of an aged bard named Gemman. On leaving him, Columba entered the monastery of Clonard, governed at that time by Finnian, noted for sanctity and learning. Here he imbibed the traditions of the Welsh Church, for Finnian had been trained in the schools of St. David.\n\nIn early Christian Ireland the druidic tradition collapsed due to the spread of the new Christian faith. The study of Latin learning and Christian theology in monasteries flourished. Columba became a pupil at the monastic school at Clonard Abbey, situated on the River Boyne in modern County Meath. During the sixth century, some of the most significant names in the history of Irish Christianity studied at the Clonard monastery. It is said that the average number of scholars under instruction at Clonard was 3,000. Columba was one of twelve students of St. Finnian who became known as the Twelve Apostles of Ireland. He became a monk and eventually was ordained a priest.\n\nAnother preceptor of Columba was St. Mobhi, whose monastery at Glasnevin was frequented by such famous men as St. Canice, St. Comgall, and St. Ciaran. A pestilence which devastated Ireland in 544 caused the dispersion of Mobhi's disciples, and Columba returned to Ulster, the land of his kindred. He was a striking figure of great stature and powerful build, with a loud, melodious voice which could be heard from one hilltop to another. The following years were marked by the foundation of several important monasteries, Derry, County Londonderry; Durrow, County Offaly; Kells, County Meath; and Swords. While at Derry it is said that he planned a pilgrimage to Rome and Jerusalem, but did not proceed farther than Tours. Thence he brought a copy of those gospels that had lain on the bosom of St. Martin for the space of 100 years. This relic was deposited in Derry.\n\nTradition asserts that, sometime around 560, he became involved in a quarrel with Saint Finnian of Movilla Abbey over a psalter. Columba copied the manuscript at the scriptorium under Saint Finnian, intending to keep the copy. Saint Finnian disputed his right to keep the copy. The dispute eventually led to the pitched Battle of Cúl Dreimhne in Cairbre Drom Cliabh (now in County Sligo) in 561, during which many men were killed. A second grievance that led him to induce the clan Neill to rise and engage in battle against King Diarmait at Cooldrevny in 561 was the king's violation of the right of sanctuary belonging to Columba's person as a monk on the occasion of the murder of Prince Curnan, the saint's kinsman. Prince Curnan of Connaught, who had fatally injured a rival in a hurling match and had taken refuge with Columba, was dragged from his protector's arms and slain by Diarmaid's men, in defiance of the rights of sanctuary.\n\nA synod of clerics and scholars threatened to excommunicate him for these deaths, but St. Brendan of Birr spoke on his behalf with the result that he was allowed to go into exile instead. Columba's own conscience was uneasy, and on the advice of an aged hermit, Molaise, he resolved to expiate his offence by going into exile and win for Christ as many souls as had perished in the terrible battle of Cúl Dreimhne. He left Ireland, to return only once, many years later. Columba's copy of the psalter has been traditionally associated with the Cathach of St. Columba.\n\nIn 563, he travelled to Scotland with twelve companions (said to include Odran of Iona) in a wicker currach covered with leather. According to legend he first landed on the Kintyre Peninsula, near Southend. However, being still in sight of his native land, he moved farther north up the west coast of Scotland. The island of Iona was made over to him by his kinsman Conall mac Comgaill King of Dál Riata, who perhaps had invited him to come to Scotland in the first place. However, there is a sense in which he was not leaving his native people, as the Ulster Gaels had been colonising the west coast of Scotland for the previous couple of centuries. Aside from the services he provided guiding the only centre of literacy in the region, his reputation as a holy man led to his role as a diplomat among the tribes. There are also many stories of miracles which he performed during his work to convert the Picts, the most famous being his encounter with an unidentified animal that some have equated with the Loch Ness Monster in 565. It is said that he banished a ferocious \"water beast\" to the depths of the River Ness after it had killed a Pict and then tried to attack Columba's disciple named Lugne (see Vita Columbae Book 2 below). He visited the pagan King Bridei, King of Fortriu, at his base in Inverness, winning Bridei's respect, although not his conversion. He subsequently played a major role in the politics of the country. He was also very energetic in his work as a missionary, and, in addition to founding several churches in the Hebrides, he worked to turn his monastery at Iona into a school for missionaries. He was a renowned man of letters, having written several hymns and being credited with having transcribed 300 books. One of the few, if not the only, times he left Scotland was towards the end of his life, when he returned to Ireland to found the monastery at Durrow.\n\nColumba died on Iona and was buried in 597 by his monks in the abbey he created. In 794 the Vikings descended on Iona. Columba's relics were finally removed in 849 and divided between Scotland and Ireland. The parts of the relics which went to Ireland are reputed to be buried in Downpatrick, County Down, with St. Patrick and St. Brigid or at Saul Church neighbouring Downpatrick. (Names of Iona), Inchcolm and Eilean Chaluim Chille.\n\nSaint Columba is one of the three chief saints of Ireland, after Saint Patrick and Saint Brigid of Kildare. Columba is the patron-saint of the city of Derry, where he founded a monastic settlement in c. 540. The name of the city in Irish is \"Doire Colmcille\" and is derived from the native oak trees in the area and the city's association with Columba. The Catholic Church of Saint Columba's Long Tower, and the Church of Ireland St Augustine's Church both claim to stand at the spot of this original settlement. The Church of Ireland Cathedral in Derry is dedicated to St Columba.\nSt. Colmcilles Primary School and St. Colmcilles Community School are two schools in Knocklyon, Dublin, named after St. Colmcille, with the former having an annual day dedicated to the saint on 9 June. The Columba Press, a religious and spiritual book company based in Dublin, is named after St. Columba. Aer Lingus, Ireland's national flag carrier has named one of its Airbus A330 aircraft in commemoration of the saint (reg: EI-DUO).\n\nColumba is credited as being a leading figure in the revitalisation of monasticism. The Clan Malcom/Clan McCallum claims its name from Columba and was reputedly founded by the descendants of his original followers. It is also said that Clan Robertson are heirs of Columba. Clan MacKinnon may also have some claim to being spiritual descendants of St Columcille as after he founded his monastery on Isle Iona, the MacKinnons were the abbots of the Church for centuries. This would also account for the fact that Clan MacKinnon is amongst the ancient clans of Scotland.\n\nThe cathedral of the Catholic Diocese of Argyll and the Isles is placed under the patronage of St. Columba, as are numerous Catholic schools and parishes throughout the nation. The Scottish Episcopal Church, the Church of Scotland, and the Evangelical Lutheran Church of England also have parishes dedicated to him. The village of Kilmacolm in Renfrewshire is also derived from Columba's name.\n\nSaint Columba currently has two poems attributed to him: \"Adiutor Laborantium\" and \"Altus Prosator\". Both poems are examples of Abecedarian hymns in Latin written while Columba was at the Iona Abbey.\n\nThe shorter of the two poems, \"Adiutor Laborantium\" consists of twenty-seven lines of eight syllables each, with each line following the format of an Abecedarian hymn using the Classical Latin alphabet save for lines 10-11 and 25-27. The content of the poem addresses God as a helper, ruler, guard, defender and lifter for those who are good and an enemy of sinners whom he will punish.\n\n\"Altus Prosator\" consists of twenty-three stanzas sixteen syllables long, with the first containing seven lines and six lines in each subsequent stanza. It uses the same format and alphabet as \"Adiutor Laborantium\" except with each stanza starting with a different letter rather than each line. The poem tells a story over three parts split into the beginning of time, history of Creation, and the Apocalypse or end of time.\n\nAs of 2011, Canadians who are of Scottish ancestry are the third largest ethnic group in the country and thus Columba's name is to be found attached to Catholic, Anglican and Presbyterian parishes. This is particularly the case in eastern Canada apart from Quebec which is French-speaking.\n\nThroughout the US there are numerous parishes within the Catholic and Episcopalian denominations dedicated to Columba. Within the Protestant tradition the Presbyterian Church (which has its roots in Scottish Presbyterianism) also has parishes named in honour of Columba. There is even an Orthodox Church monastery dedicated to the saint in the Massachusetts town of Southbridge. St. Columba is the Patron Saint of the Roman Catholic Diocese of Youngstown, OH. The Cathedral there is named for him.\n\nIona College, a small Catholic liberal arts college in New Rochelle, NY, is named after the island on which Columba established his first monastery in Scotland, as is Iona College in Windsor, Ontario and Iona Presentation College, Perth.\n\nThere are at least four pipe bands named for him; one each from Tullamore, Ireland, from Derry, Northern Ireland, from Kearny, New Jersey, and from Cape Cod, Massachusetts.\n\nSt. Columba's School one of the most prominent English-Medium schools in Delhi run by the Irish Christian Brothers is also named after the Saint.\n\nThe Munich GAA is named München Colmcilles.\n\nSt. Columba's Feast Day, 9 June, has been designated as International Celtic Art Day. The Book of Kells and the Book of Durrow, great medieval masterpieces of Celtic art, are associated with Columba. \n\nThe main source of information about Saint Columba's life is the Vita Columbae (i.e. \"Life of Columba\"), a hagiography written in the style of \"saint's lives\" narratives that had become widespread throughout medieval Europe. Compiled and drafted by scribes and clergymen, these accounts were written in Latin and served as written collections of the deeds and miracles attributed to the saint, both during his or her life or after death. The canonization of a saint, especially one who had lived on the fringes of the medieval Christian world like Saint Columba, required a well-written hagiography to be submitted to Rome, but popular belief and local cults of sainthood often led to the veneration of these men and women without official approval from the Catholic Church.\n\nWriting a century after the death of Saint Columba, the author Adomnán (also known as Eunan), served as the ninth Abbot of Iona until his death in 704 A.D.\n\nJames Earle Fraser asserts that Adomnán drew extensively from an existing body of accounts regarding the life of Saint Columba, including a Latin collection entitled \"De uirtutibus sancti Columbae\", composed c. 640 A.D. This earlier work is attributed to Cummene Find, who became the abbot of Iona and served as the leader of the monastic island community from 656 until his death in 668 A.D. or 669 A.D.\n\nWhile the Vita Columbae often conflicts with contemporaneous accounts of various battles, figures, and dates, it remains the most important surviving work from early medieval Scotland and provides a wealth of knowledge regarding the Picts and other ethnic and political groups from this time period. The Vita also offers a valuable insight into the monastic practices of Iona and the daily life of the early medieval Gaelic monks.\n\nThe surviving manuscripts include:\n\n\nInstead of relying on chronological order, Adomnán categorises the events recorded in the Vita Columbae into three different books: Columba’s Prophecies, Columba’s Miracles, and Columba’s Apparitions.\n\nIn the first book, the author Adomnán lists Saint Columba's prophetic revelations, which come as a result of the saint's ability to view the present and the future simultaneously. Most of the short chapters begin with Saint Columba informing his fellow monks that a person will soon arrive on the island or an event will imminently occur.\n\nIn one notable instance, Columba appears in a dream to King Oswald of Northumbria, and announces the king's incoming victory against the King Catlon (Cadwallon of Wales) in the Battle of Heavenfield. The people of Britain promise to convert to Christianity and receive baptism after the conclusion of the war. This victory signals the re-Christianization of pagan England, and establishes King Oswald as ruler of the entirety of Britain.\n\nColumba's other prophecies can be considered vindictive at times, as when he sends a man named Batain off to perform his penance, but then Columba turns to his friends and says Batain will instead return to Scotia and be killed by his enemies. \nSeveral of Saint Columba's prophecies reflect the scribal culture in which he was immersed, such his miraculous knowledge of the missing letter \"I” from Baithene's psalter or when he prophecies that an eager man will knock over his inkhorn and spill its contents.\n\nIn the second book, Columba performs various miracles such as healing people with diseases, expelling malignant spirits, subduing wild beasts, calming storms, and even returning the dead to life. They have also made many schools in honour of St.Columba, one was founded by the Sisters of Charity.\n\nHe also performs agricultural miracles that would hold a special significance to the common people of Ireland and the British Isles, such as when he casts a demon out of a pail and restores the spilt milk to its container.\n\nThe Vita contains a story that has been interpreted as the first reference to the Loch Ness Monster. According to Adomnán, Columba came across a group of Picts burying a man who had been killed by the monster. Columba saves a swimmer from the monster with the sign of the Cross and the imprecation, \"Thou shalt go no further, nor touch the man; go back with all speed.\" The beast flees, terrified, to the amazement of the assembled Picts who glorified Columba's God. Whether or not this incident is true, Adomnan's text specifically states that the monster was swimming in the River Ness – the river flowing from the loch – rather than in Loch Ness itself.\n\nIn book three, Adomnán describes different apparitions of the Saint, both that Columba receives and those that are seen by others regarding him. He mentions that, \"For indeed after the lapse of many years, ... St. Columba was excommunicated by a certain synod for some pardonable and very trifling reasons, and indeed unjustly\" (P.79- 80).\n\nIn one of the accounts, Saint Columba, in this period of excommunication, goes to a meeting held against him in Teilte. Saint Brendan, despite of all the negative reactions among the seniors toward Columba, kisses him reverently and assures that Columba is the man of God and that he sees Holy Angels accompanying Columba on his journey through the plain.\n\nIn the last Chapter, Columba foresees his own death when speaking to his attendant: This day in the Holy Scriptures is called the Sabbath, which means rest. And this day is indeed a Sabbath to me, for it is the last day of my present laborious life, and on it I rest after the fatigues of my labours; and this night at midnight, which commenceth the solemn Lord's Day, I shall, according to the sayings of Scripture, go the way of our fathers. For already my Lord Jesus Christ deigneth to invite me; and to Him, I say, in the middle of this night shall I depart, at His invitation. For so it hath been revealed to me by the Lord himself. \n\nAnd when the bell strikes midnight, Columba goes to the church and kneels beside the altar. His attendant witnesses heavenly light in the direction of Columba, and Holy angels join the saint in his passage to the Lord: And having given them his holy benediction in this way, he immediately breathed his last. After his soul had left the tabernacle of the body, his face still continued ruddy, and brightened in a wonderful way by his vision of the angels, and that to such a degree that he had the appearance, not so much of one dead, as of one alive and sleeping. \n\nBoth the \"Vita Columbae\" and the Venerable Bede (672/673-735) record Columba's visit to Bridei. Whereas Adomnán just tells us that Columba visited Bridei, Bede relates a later, perhaps Pictish tradition, whereby the saint actually converts the Pictish king. Another early source is a poem in praise of Columba, most probably commissioned by Columba's kinsman, the King of the Uí Néill clan. It was almost certainly written within three or four years of Columba's death and is the earliest vernacular poem in European history. It consists of 25 stanzas of four verses of seven syllables each.\n\nThrough the reputation of its venerable founder and its position as a major European centre of learning, Columba's Iona became a place of pilgrimage. Columba is historically revered as a warrior saint, and was often invoked for victory in battle.\nHis relics were finally removed in 849 and divided between Alba and Ireland. Relics of Columba were carried before Scottish armies in the reliquary made at Iona in the mid-8th century, called the Brecbennoch. Legend has it that the Brecbennoch was carried to the Battle of Bannockburn (24 June 1314) by the vastly outnumbered Scots army and the intercession of Columba helped them to victory. It is widely thought that the Monymusk Reliquary is the object in question.\n\nIn the Antiphoner of Inchcolm Abbey, the \"Iona of the East\" (situated on an island in the Firth of Forth), a 14th-century prayer begins \"O Columba spes Scotorum...\" \"O Columbus, hope of the Scots\".\n\n\n\n\n\n \n"}
{"id": "6562", "url": "https://en.wikipedia.org/wiki?curid=6562", "title": "Conditional proof", "text": "Conditional proof\n\nA conditional proof is a proof that takes the form of asserting a conditional, and proving that the antecedent of the conditional necessarily leads to the consequent. \n\nThe assumed antecedent of a conditional proof is called the \"conditional proof assumption\" (CPA). Thus, the goal of a conditional proof is to demonstrate that if the CPA were true, then the desired conclusion necessarily follows. The validity of a conditional proof does not require that the CPA be actually true, only that \"if it were true\" it would lead to the consequent.\n\nConditional proofs are of great importance in mathematics. Conditional proofs exist linking several otherwise unproven conjectures, so that a proof of one conjecture may immediately imply the validity of several others. It can be much easier to show a proposition's truth to follow from another proposition than to prove it independently.\n\nA famous network of conditional proofs is the NP-complete class of complexity theory. There are a large number of interesting tasks, and while it is not known if a polynomial-time solution exists for any of them, it is known that if such a solution exists for any of them, one exists for all of them. Similarly, the Riemann hypothesis has a large number of consequences already proven.\n\nAs an example of a conditional proof in symbolic logic, suppose we want to prove A → C (if A, then C) from the first two premises below:\n\n\n"}
{"id": "6563", "url": "https://en.wikipedia.org/wiki?curid=6563", "title": "Conjunction introduction", "text": "Conjunction introduction\n\nConjunction introduction (often abbreviated simply as conjunction and also called and introduction) is a valid rule of inference of propositional logic. The rule makes it possible to introduce a conjunction into a logical proof. It is the inference that if the proposition \"p\" is true, and proposition \"q\" is true, then the logical conjunction of the two propositions \"p and q\" is true. For example, if it's true that it's raining, and it's true that I'm inside, then it's true that \"it's raining and I'm inside\". The rule can be stated:\n\nwhere the rule is that wherever an instance of \"formula_2\" and \"formula_3\" appear on lines of a proof, a \"formula_4\" can be placed on a subsequent line.\n\nThe \"conjunction introduction\" rule may be written in sequent notation:\n\nwhere formula_6 is a metalogical symbol meaning that formula_4 is a syntactic consequence if formula_2 and formula_3 are each on lines of a proof in some logical system;\n\nwhere formula_2 and formula_3 are propositions expressed in some formal system.\n"}
{"id": "6566", "url": "https://en.wikipedia.org/wiki?curid=6566", "title": "English in the Commonwealth of Nations", "text": "English in the Commonwealth of Nations\n\nThe use of the English language in most member countries of the Commonwealth of Nations was inherited from British colonisation. English is spoken as a first or second language in most of the Commonwealth. In a few countries, such as Cyprus and Malaysia, it does not have official status, but is widely used as a lingua franca. Mozambique is an exception – although English is widely spoken there, it is a former Portuguese colony which joined the Commonwealth in 1996.\n\nMany regions, notably Canada, Australia, India, New Zealand, Pakistan, South Africa, Malaysia, Brunei, Singapore and the Caribbean, have developed their own native varieties of the language.\n\nWritten English as used in the Commonwealth generally favours British spelling as opposed to American spelling, with one notable exception being Canada, where there is also a strong influence from neighbouring American English (North American English).\n\nThe report of the Inter-Governmental Group on Criteria for Commonwealth Membership states that English is a symbol of Commonwealth heritage and unity.\n\nSouthern Hemisphere native varieties of English began to develop during the 18th century, with the colonisation of Australasia and South Africa. Australian English and New Zealand English are closely related to each other, and share some similarities with South African English. The vocabularies of these dialects draw from both British and American English as well as numerous native peculiarities.\n\nCanadian English is the variety of English spoken in Canada. It contains elements of British English and American English, as well as many Canadianisms. It is the product of several waves of immigration and settlement, from the United States, Britain, Ireland, and around the world, over a period of almost two centuries. Modern Canadian English has taken significant vocabulary and spelling from the shared political and social institutions of Commonwealth countries.\n\nCaribbean English is influenced by the English-based Creole varieties spoken, but they are not one and the same. There is a great deal of variation in the way English is spoken, with a \"Standard English\" at one end of a bipolar linguistic continuum and Creole languages at the other. These dialects have roots in 17th-century English and African languages; unlike most native varieties of English, West Indian dialects often tend to be syllable-timed rather than stress-timed.\n\nSecond language varieties of English in Africa and Asia have often undergone \"indigenisation\"; that is, each English-speaking community has developed (or is in the process of developing) its own standards of usage, often under the influence of local languages. These dialects are sometimes referred to as \"New Englishes\" (McArthur, p. 36); most of them inherited non-rhoticity from Southern British English.\n\nSeveral dialects of West African English exist, with a lot of regional variation and some influence from indigenous language. West African English tends to be syllable-timed, and its phoneme inventory is much simpler than that of Received Pronunciation; this sometimes affects mutual intelligibility with native varieties of English. A distinctive East African English is spoken in countries such as Kenya or Tanzania.\n\nSmall communities of native English speakers can be found in Zimbabwe, Botswana and Namibia; the dialects spoken are similar to South African English.\n\nIn countries such as Kenya - particularly in Nairobi and other cities where there is an expanding middle class - English is increasingly being used in the home as the first language, albeit often with significant influences from Bantu languages such as Swahili. \n\nIndia has the largest English-speaking population in the Commonwealth, although comparatively few speakers of Indian English are first language speakers (indeed, the same is true of English spoken in other parts of South Asia e.g. Pakistani English, Bangladeshi English). South Asian English phonology is highly variable; stress, rhythm and intonation are generally different from those of native varieties. There are also several peculiarities at the levels of morphology, syntax and usage, some of which can also be found among educated speakers. \n\nSoutheast Asian English comprises Singapore English, Malaysian English and Brunei English; it features some influence from Malay and Chinese languages, as well as Indian English.\n\nHong Kong ceased to be part of the Commonwealth in 1997. Nonetheless, the English language still enjoys status as an official language, alongside Chinese (See Hong Kong English).\n\n\nOther languages:\n\n"}
{"id": "6569", "url": "https://en.wikipedia.org/wiki?curid=6569", "title": "Charles McCarry", "text": "Charles McCarry\n\nCharles McCarry (born in 1930, in Massachusetts, USA) is an American writer, primarily of spy fiction, and a former undercover operative for the Central Intelligence Agency who The Wall Street Journal, in 2013, described as \"the dean of American spy writers\"; The New Republic magazine calls him \"poet laureate of the CIA.\"William Zinsser calls him a \"political novelist:\" Jonathan Yardley, Pulitzer Prize winning critic for the Washington \"Post\", calls him a \"'serious' novelist\" whose work may include \"the best novel ever written about life in high-stakes Washington, DC.\" \n\nHis family is from The Berkshires area of western Massachusetts, and he currently lives in Virginia.\n\nMcCarry believes that \"the best novels are about ordinary things: love, betrayal, death, trust, loneliness, marriage, fatherhood.\" He also says \"if you write a political novel, you're writing what you believe instead of what you know.\"\n\nMcCarry writes that: \"After I resigned [from the CIA], intending to spend the rest of my life writing fiction and knowing what tricks the mind can play when the gates are thrown wide open, as they are by the act of writing, between the imagination and that part of the brain in which information is stored, I took the precaution of writing a closely remembered narrative of my clandestine experiences. After correcting the manuscript, I burned it.\nWhat I kept for my own use was the atmosphere of secret life: How it worked on the five senses and what it did to the heart and mind. All the rest went up in flames, setting me free henceforth to make it all up. In all important matters, such as the creation of characters and the invention of plots, with rare and minor exceptions, that is what I have done. And, as might be expected, when I have been weak enough to use something that really happened as an episode in a novel, it is that piece of scrap, buried in a landfill of the imaginary, readers invariably refuse to believe.\"\n\nThroughout McCarry's fiction are statements and descriptions such as \"the average intelligence officer is a sort of latter-day Marcel Proust. He lies abed in a cork-lined room, hoping to profit by secrets that other people slip under the door.\"\n\nMcCarry's first published novel came out in 1973, which means he was 43,; in contrast, most successful novelists have their first novel published while they are still in their twenties.\n\nA critic for \"Tin House\" magazine approaches McCarry through what the critic calls \"the art of the sentence,\" citing as an example a description in the opening pages of McCarry's \"The Secret Lovers\": “The sun shone feebly through the overcast, like a lamp covered by a woman’s scarf in a shabby hotel room.”\n\nMcCarry began his writing career in the United States Army as a correspondent for \"Stars and Stripes\", afterwards, in the 1950s, serving as a speechwriter for President Dwight D. Eisenhower before taking a post with the CIA for whom he traveled the globe as a deep cover operative (that his son, Nathan McCarry (CEO of Pluribus International Corporation described, in 2014, as being \"trying for the family\"), but left the CIA, in 1967, becoming a writer of spy novels. McCarry rarely speaks or writes directly about those years, saying simply, \"For a decade at the height of the Cold War, I worked abroad under cover as an intelligence agent.\"\n\nMcCarry was editor-at-large for \"National Geographic\" and has contributed pieces to \"The New York Times\", \"The Wall Street Journal\", \"The Washington Post\", the \"Saturday Evening Post,\" and other national publications.\n\nIn as essay published by the Washington \"Pos\"t, he says that \"for a writer in America, going out to dinner is like living as an American in Europe: Total strangers think they can say anything they like to you.\"\n\n\nTen of McCarry's novels involve the life story of a fictional character named Paul Christopher, who--in McCarry's telling--grew up in pre-Nazi Germany, and later became a lone operative for a U.S. government entity that is clearly the Central Intelligence Agency.\n\nThese books, in order of publication, are:\nThe novels, In chronological order of events depicted:\n\n\"It’s tempting to say that Charles McCarry’s\"The Tears of Autumn\" is the greatest espionage novel ever written by an American, if only because it’s hard to conceive of one that could possibly be better. But since no one can claim to have read every America espionage novel ever written, let’s just say that \"The Tears of Autumn\" is a perfect spy novel, and that its hero, Paul Christopher, should by all rights be known the world over as the thinking man’s James Bond — and woman’s too.\"--Brendan Bernard, \"The Great American Spy Novel,\" March 31, 2005', LA Weekly\"\n\n\"\"Old Boys\" is a large yarn that will make yummy reading between long looks at Nantucket Sound this summer. (And a boffo movie in the right hands.) But it is a tale that travels from the outlandish to the absurd. As long as readers don't expect the taut realism we have come to expect from the man, they'll be fine. If they're looking for vintage McCarry, though, this will produce unhappy campers. The book does not approach his better grownup fiction. It is not in the same league, for example, with \"The Miernik Dossier\", the small gem that made McCarry's career. Rather, it is something of a \"Treasure Island\" for lovers of spook fiction, a near-juvenile adventure that entrances adults who know better with fabulous writing. What they do get is a fleeting reprise of McCarry's great creation, Paul Christopher. Christopher, the spy whom many first met in McCarry's bestseller \"The Tears of Autumn\", is now an opaque older man and an ascetic survivor of a Chinese prison camp.\"--Sam Allis, \"McCarry's thriller 'Old Boys' is a trip past believable,\" Boston \"Globe\", July 26, 2004\n\n\n\n--Computer algorithms that analysis media content and specify--with accuracy--when a physical war between two countries will break out. \"The Better Angels\", 1979.\n\n--Jacob Heilbrunn writes in the N.Y. \"Times\" (2006): \"McCarry never succumbs to a bogus moral equivalence in which Western operatives are as nefarious as their Communist counterparts. He instructs us that the real problem is not so much moral quicksand as incompetent scheming. At a moment when the C.I.A.'s travails are evoking nostalgia for a golden age when it supposedly operated effectively, McCarry offers a useful reminder that such an era never existed.\"\n\n\nGreene, le Carre, Maugham, and McCarry,\" writes Alan Furst, \"write with a kind of cloaked anger, a belief that the world is a place where political power is maintained by treachery and betrayal...\" In its subtitle, Furst's book calls such writing \"literary espionage.\"\n\n\nThe film \"Wrong is Right\" (1982), starring Sean Connery, was loosely based on McCarry's novel, \"The Better Angels\" (1979).\n\nMcCarry is an admirer of the work of W. Somerset Maugham, especially the stories. He was also an admirer of Richard Condon, author of \"The Manchurian Candidate\" (1959), \"Prizzi's Honor\" (1982), and numerous other novels.\n\n\n--Stories include: In March 1981, shortly after taking office, Ronald Reagan was shot; Secretary of State Haig appeared in the White House press room and announced, \"I am in charge here!\"\n\nHarlan Coben, ed. \"'The Best American Mystery Stories: 2011\", incldudes \"The End of the String.\"\n\nAlan Furst, ed. \"The Book of Spies\",includes excerpt from T\"he Tears of Autumn\".\n\nOtto Penzler, ed. \"Agents of Treachery\", includes \"The End of the Sting.\"\n\n\n"}
{"id": "6571", "url": "https://en.wikipedia.org/wiki?curid=6571", "title": "Cimbri", "text": "Cimbri\n\nThe Cimbri were an ancient people, either Germanic or Celtic who, together with the Teutones and the Ambrones, fought the Roman Republic between 113 and 101 BC. The Cimbri were initially successful, particularly at the Battle of Arausio, in which a large Roman army was routed, after which they raided large areas in Gaul and Hispania. In 101 BC, during an attempted invasion of Italy, the Cimbri were decisively defeated by Gaius Marius, and their king, Boiorix, was killed. Some of the surviving captives are reported to have been among the rebelling gladiators in the Third Servile War.\n\nRoman sources such as Strabo and Tacitus identify these Cimbri with a group living in Jutland, but strong evidence for this connection is lacking.\n\nArchaeologists have not found any clear indications of a mass migration from Jutland in the early Iron Age. The Gundestrup Cauldron, which was deposited in a bog in Himmerland in the 2nd or 1st century BC, shows that there was some sort of contact with southeastern Europe, but it is uncertain if this contact can be associated with the Cimbrian expedition.\n\nAdvocates for a northern homeland point to Greek and Roman sources that associate the Cimbri with the peninsula of Jutland. According to the \"Res gestae\" (ch. 26) of Augustus, the Cimbri were still found in the area around the turn of the 1st century AD:\n\nThe contemporary Greek geographer Strabo testifies that the Cimbri still existed as a Germanic tribe, presumably in the \"Cimbric peninsula\" (since they are said to live by the North Sea and to have paid tribute to Augustus):\n\nOn the map of Ptolemy, the \"Kimbroi\" are placed on the northernmost part of the peninsula of Jutland., i.e., in the modern landscape of Himmerland south of Limfjorden (since Vendsyssel-Thy north of the fjord was at that time a group of islands). \"Himmerland\" (Old Danish \"Himbersysel\") is generally thought to preserve their name, in an older form without Grimm's law (PIE \"k\" > Germ. \"h\"). Alternatively, Latin \"c-\" represents an attempt to render the unfamiliar Proto-Germanic \"h\" = (Latin \"h\" was but was becoming silent in common speech at the time), perhaps due to Celtic-speaking interpreters (a Celtic intermediary would also explain why Germanic *\"Þeuðanōz\" became Latin \"Teutones\").\n\nThe origin of the name \"Cimbri\" is unknown. One etymology is PIE ' \"inhabitant\", from ' \"home\" (> English \"home\"), itself a derivation from \"\" \"live\" (> Greek , Latin \"sinō\"); then, the Germanic *\"himbra-\" finds an exact cognate in Slavic \"sębrъ\" \"farmer\" (> Croatian, Serbian \"sebar\", Russian сябёр \"syabyor\").\n\nBecause of the similarity of the names, the Cimbri have been at times associated with Cymry, the Welsh name for themselves. However, this word is derived from Brittonic *\"Kombrogi\", meaning “compatriots”, and is unrelated to Cimbri. The name has also been related to the word \"kimme\" meaning “rim”, i.e., the people of the coast. Finally, since Antiquity, the name has been related to that of the Cimmerians. Moreover, an assumption exists that the Cimbri as the Cimmerians too were an Iranian people.\n\nSome time before 100 BC many of the Cimbri, as well as the Teutons and Ambrones migrated south-east. After several unsuccessful battles with the Boii and other Celtic tribes, they appeared ca 113 BC in Noricum, where they invaded the lands of one of Rome's allies, the Taurisci.\n\nOn the request of the Roman consul Gnaeus Papirius Carbo, sent to defend the Taurisci, they retreated, only to find themselves deceived and attacked at the Battle of Noreia, where they defeated the Romans. Only a storm, which separated the combatants, saved the Roman forces from complete annihilation.\n\nNow the road to Italy was open, but they turned west towards Gaul. They came into frequent conflict with the Romans, who usually came out the losers. In Commentarii de Bello Gallico the Aduaticii—Belgians of Cimbrian origin—repeatedly sided with Rome's enemies. In 109 BC, they defeated a Roman army under the consul Marcus Junius Silanus, who was the commander of Gallia Narbonensis. In 107 BC they defeated another Roman army under the consul Gaius Cassius Longinus, who was killed at the Battle of Burdigala (modern day Bordeaux) against the Tigurini, who were allies of the Cimbri.\n\nIt was not until 105 BC that they planned an attack on the Roman Republic itself. At the Rhône, the Cimbri clashed with the Roman armies. Discord between the Roman commanders, the proconsul Quintus Servilius Caepio and the consul Gnaeus Mallius Maximus, hindered Roman coordination and so the Cimbri succeeded in first defeating the legate Marcus Aurelius Scaurus and later inflicted a devastating defeat on Caepio and Maximus at the Battle of Arausio. The Romans lost as many as 80,000 men, according to Livy, Mommsen thought that excluded auxiliary cavalry and non-combatants who brought the total loss closer to 112,000. Other estimates are much smaller, but by any account a large Roman army was routed.\n\nRome was in panic, and the \"terror cimbricus\" became proverbial. Everyone expected to soon see the \"new Gauls\" outside of the gates of Rome. Desperate measures were taken: contrary to the Roman constitution, Gaius Marius, who had defeated Jugurtha, was elected consul and supreme commander for five years in a row (104-100 BC).\n\nIn 104-103 BC, the Cimbri had turned to the Iberian Peninsula where they pillaged far and wide, until they were confronted by a coalition of Celtiberians. Defeated, the Cimbri returned to Gaul, where they joined their allies, the Teutons. During this time C. Marius had the time to prepare and, in 102 BC, he was ready to meet the Teutons and the Ambrones at the Rhône. These two tribes intended to pass into Italy through the western passes, while the Cimbri and the Tigurines were to take the northern route across the Rhine and later across the Central Eastern Alps.\n\nAt the estuary of the Isère River, the Teutons and the Ambrones met Marius, whose well-defended camp they did not manage to overrun. Instead, they pursued their route, and Marius followed them. At Aquae Sextiae, the Romans won two battles and took the Teuton king Teutobod prisoner.\n\nThe Cimbri had penetrated through the Alps into northern Italy. The consul Quintus Lutatius Catulus had not dared to fortify the passes, but instead he had retreated behind the River Po, and so the land was open to the invaders. The Cimbri did not hurry, and the victors of Aquae Sextiae had the time to arrive with reinforcements. At the Battle of Vercellae, at the confluence of the Sesia River with the Po River, in 101 BC, the long voyage of the Cimbri also came to an end.\n\nIt was a devastating defeat, two chieftains, Lugius and Boiorix, died on the field, while the other chieftains Caesorix and Claodicus were captured. The women killed both themselves and their children in order to avoid slavery. The Cimbri were annihilated, although some may have survived to return to the homeland where a population with this name was residing in northern Jutland in the 1st century AD, according to the sources quoted above. Some of the surviving captives are reported to have been among the rebelling Gladiators in the Third Servile War.\n\nHowever, Justin's epitome of Trogus, 38.4, has Mithridates the Great state that the Cimbri are ravaging Italy while the Social War is going on, i.e. at some time in 90 - 88 BCE, thus more than a decade later, after having sent ambassadors to the Cimbri to request military aid; judging from the context they must then have been living in North Eastern Europe at the time.\n\nAccording to Julius Caesar, the Belgian tribe of the Atuatuci \"was descended from the Cimbri and Teutoni, who, upon their march into our province and Italy, set down such of their stock and stuff as they could not drive or carry with them on the near (i.e. west) side of the Rhine, and left six thousand men of their company there as guard and garrison\" (\"Gall.\" 2.29, trans. Edwards). They founded the city of Atuatuca in the land of the Belgic Eburones, whom they dominated. Thus Ambiorix king of the Eburones paid tribute and gave his son and nephew as hostages to the Atuatuci (\"Gall.\" 6.27). In the first century AD, the Eburones were replaced or absorbed by the Germanic Tungri, and the city was known as Atuatuca Tungrorum, i.e. the modern city of Tongeren.\n\nThe population of modern-day Himmerland claims to be the heirs of the ancient Cimbri. The adventures of the Cimbri are described by the Danish Nobel Prize–winning author Johannes V. Jensen, himself born in Himmerland, in the novel \"Cimbrernes Tog\" (1922), included in the epic cycle \"Den lange Rejse\" (English \"The Long Journey\", 1923). The so-called Cimbrian bull (\"Cimbrertyren\"), a sculpture by Anders Bundgaard, was erected on 14 April 1937 in a central town square in Aalborg, the capital of the region of North Jutland.\n\nA German ethnic minority speaking the Cimbrian language, having settled in the mountains between Vicenza, Verona, and Trento in Italy (also known as Seven Communities), is also called the . For hundreds of years this isolated population and its present 4,400 inhabitants have claimed to be the direct descendants of the Cimbri retreating to this area after the Roman victory over their tribe. However, it is more likely that Bavarians settled here in the Middle Ages. Most linguists remain committed to the hypothesis of a medieval (11th to 12th century AD) immigration to explain the presence of small German-speaking communities in the north of Italy. Some genetic studies seem to prove a Celtic, not Germanic, descent for most inhabitants in the region that is reinforced by Gaulish toponyms such as those ending with the suffix \"-ago\" < Celtic \"-*ako(n)\" (e.g. Asiago is clearly the same place name as the numerous variants - Azay, Aisy, Azé, Ezy - in France, all of which derive from \"*Asiacum\" < Gaulish \"*Asiāko(n)\"). On the other hand, the original place names in the region, from the specifically localized language known as 'Cimbro' are still in use alongside the more modern names today. These indicate a different origin (e.g., Asiago is known also by its original Cimbro name of \"Sleghe\"). The Cimbrian origin myth was popularized by humanists in the 14th century.\n\nDespite these connections to southern Germany, belief in a Himmerland origin persisted well into modern times. On one occasion in 1709, for instance, Frederick IV of Denmark paid the region's inhabitants a visit and was greeted as their king. The population, which kept its independence during the time of the Venice Republic, was later severely devastated by World War I. As a result, many Cimbri have left this mountainous region of Italy, effectively forming a worldwide diaspora.\n\nStrabo gives this vivid description of the Cimbric folklore (\"Geogr.\" 7.2.3, trans. H.L. Jones):\n\n(The priestess mentioned below was called, in Danish: en Vølve (a voelve)).\n\nThe Cimbri are depicted as ferocious warriors who did not fear death. The host was followed by women and children on carts. Aged women, priestesses, dressed in white sacrificed the prisoners of war and sprinkled their blood, the nature of which allowed them to see what was to come.\n\nIf the Cimbri did in fact come from Jutland, evidence that they practised ritualistic sacrifice may be found in the Haraldskær Woman discovered in Jutland in the year 1835. Noosemarks and skin piercing were evident and she had been thrown into a bog rather than buried or cremated. Furthermore, the Gundestrup cauldron, found in Himmerland, may be a sacrificial vessel like the one described in Strabo's text. The work itself was of Thracian origin.\n\nA major problem in determining whether the Cimbri were speaking a Celtic or a Germanic language is that at this time the Greeks and Romans tended to refer to all groups to the north of their sphere of influence as Gauls, Celts, or Germani rather indiscriminately. Caesar seems to be one of the first authors to distinguish the two groups, and he had a political motive for doing so (it was an argument in favour of the Rhine border). Yet, one cannot always trust Caesar and Tacitus when they ascribe individuals and tribes to one or the other category, although Caesar made clear distinctions between the two cultures. Most ancient sources categorize the Cimbri as a Germanic tribe, but some ancient authors include the Cimbri among the Celts.\n\nThere are few direct testimonies to the language of the Cimbri: Referring to the Northern Ocean (the Baltic or the North Sea), Pliny the Elder states: \"Philemon says that it is called Morimarusa, i.e. the Dead Sea, by the Cimbri, until the promontory of Rubea, and after that Cronium.\" The contemporary Gaulish terms for “sea” and “dead” appear to have been \"mori\" and \"*maruo-\"; compare their well-attested modern Insular Celtic cognates \"muir\" and \"marbh\" (Irish), \"môr\" and \"marw\" (Welsh), and \"mor\" and \"marv\" (Breton). The same word for “sea” is also known from Germanic, but with an \"a\" (*\"mari-\"), whereas a cognate of \"marbh\" is unknown in all dialects of Germanic. Yet, given that Pliny had not heard the word directly from a Cimbric informant, it cannot be ruled out that the word is in fact Gaulish instead.\n\nThe known Cimbri chiefs have names that look Celtic, including Boiorix (which may mean \"King of the Boii\" or, more literally, \"King of Strikers\"), Gaesorix (which means \"Spear King\"), and Lugius (which may be named after the Celtic god Lugus), although this may not mean that they are Celtic as the elements could work in Germanic (compare the name of the Vandalic king Gaiseric, which is likely identical to Gaesorix). Also, although the kings of the Cimbri and Teutones carry what look like Celtic names, the origin of a name need not say anything about the ethnicity or language of the individual carrying the name. Other evidence to the language of the Cimbri is circumstantial: thus, we are told that the Romans enlisted Gaulish Celts to act as spies in the Cimbri camp before the final showdown with the Roman army in 101 BC. Some take this as evidence in support of \"the Celtic rather than the German theory\".\n\nJean Markale wrote that the Cimbri were associated with the Helvetii, and more especially with the indisputably Celtic Tigurini. These associations may link to a common ancestry, recalled from two hundred years previous, though they may not. Henri Hubert states \"All these names are Celtic, and they cannot be anything else\". Some authors take a different perspective.\n\nCountering the argument of a Celtic origin is the literary evidence that the Cimbri originally came from northern Jutland, an area with no Celtic placenames, instead only Germanic ones. This does not rule out Cimbric Gallicization during the period when they lived in Gaul. Boiorix, who may have a Celtic name if not a Celticized Germanic name, was king of the Cimbri after they moved away for their ancestral home of northern Jutland; Boiorix and his tribe lived around Celtic peoples during his era as J. B. Rives points out in his introduction to Tacitus's Germania (book) and moreover that the name Boiorix can work in Proto-Germanic as well as Celtic.\n\n"}
{"id": "6576", "url": "https://en.wikipedia.org/wiki?curid=6576", "title": "Cleveland Browns", "text": "Cleveland Browns\n\nThe Cleveland Browns are a professional American football team based in Cleveland, Ohio. The Browns compete in the National Football League (NFL) as a member club of the American Football Conference (AFC) North division. The Browns play their home games at FirstEnergy Stadium, which opened in 1999, with administrative offices and training facilities in Berea, Ohio. The Browns' official colors are brown, orange and white. They are unique among the 32 member franchises of the NFL in that they do not have a logo on their helmets and are the only team named after a specific person, original coach Paul Brown.\n\nThe franchise was founded in 1945 by businessman Arthur B. McBride and coach Paul Brown as a charter member of the All-America Football Conference (AAFC). The Browns dominated the AAFC, compiling a 47–4–3 record in the league's four active seasons and winning its championship in each of them. When the AAFC folded after the 1949 season, the Browns joined the National Football League along with the San Francisco 49ers and the original Baltimore Colts. The Browns won a championship in their inaugural NFL season, as well as in the 1954, 1955, and 1964 seasons, and in a feat unequaled in any of the North American major professional sports, played in their league championship game in each of the Browns' first ten years of existence. From 1965 to 1995, they made the playoffs 14 times, but did not win another championship or appear in the Super Bowl in that period.\n\nIn 1995, owner Art Modell, who had purchased the Browns in 1961, announced plans to move the team to Baltimore, Maryland. After threats of legal action from the city of Cleveland and fans, a compromise was reached in early 1996 that allowed Modell to establish the Baltimore Ravens as a new franchise while retaining the contracts of all Browns personnel. The Browns' intellectual property, including team name, logos, training facility, and history, were kept in trust and the franchise was regarded by the NFL as suspended. A new team would be established by 1999 either by expansion or relocation. The Browns were announced as an expansion team in 1998 and resumed play in 1999.\n\nSince resuming operations in 1999, the Browns have struggled to find success. They have had only two winning seasons (in 2002 and 2007), one playoff appearance (2002), and no playoff wins. The franchise has also been noted for a lack of stability with quarterbacks, having started 26 players in the position over the past 18 seasons. To date, the Browns' overall win-loss record since 1999 is 88–200.\n\nThe Browns' origins date to 1944, when taxicab magnate Arthur B. \"Mickey\" McBride secured the rights to a Cleveland franchise in the newly formed All-America Football Conference (AAFC). The AAFC was to compete with the dominant National Football League (NFL) once it began operations at the end of World War II, which had forced many professional teams to curtail activity, merge, or go on hiatus as their players served in the United States Armed Forces.\n\nEarly in 1945, McBride named 36-year-old Ohio State Buckeyes coach Paul Brown as the team's head coach and general manager and gave him a share in its profits. The move surprised and upset Buckeye fans, who had hoped he would resume his successful run at the school after the war. Brown, who had built an impressive record as coach of a Massillon, Ohio, high school team and brought the Buckeyes their first national championship, at the time was serving in the U.S. Navy and coached the football team at Great Lakes Naval Station near Chicago.\n\nThe name of the team was at first left up to Paul Brown, who rejected calls for it to be christened the \"Browns\". The franchise and the local newspaper, the Cleveland Plain Dealer, then held a naming contest to publicize the team, promising a $1,000 war bond to the winner. In June 1945, a committee selected \"Panthers\" as the new team's name, named after a failed American Football League (AFL) franchise in Cleveland which only lasted part way through that professional league's single season in 1926 (although as a semi-professional team the Cleveland Panthers existed between 1919 and 1933). It is unclear whether \"Panthers\" was the highest vote-getter, or if it was second-highest behind \"Browns\", which was again rejected by Paul Brown. \nGeorge T. Jones, who been the secretary for the Panthers under the AFL team owner General Charles X. Zimmerman (who died in November 1926), had become manager of the re-established semi-professional Cleveland Panthers in 1927 and had the rights to the name. Jones apparently demanded several thousand dollars from the new franchise owner, Arthur McBride, for the use of the name. McBride refused to pay, reopened the contest, and selected the Browns name for his team. At this point, Paul Brown bowed to popular sentiment and agreed to the \"Browns\" name.\n\nBrown remained uncomfortable with the idea of the team being named after him. For some time after, he would occasionally cite an alternate history of the team name, claiming that they were actually named after boxer Joe Louis, whose nickname was \"The Brown Bomber\". This alternate history of the name was even supported by the team as being factual as recently as the mid-1990s, and it continues as an urban legend to this day. However, Paul Brown never held fast to the Joe Louis story, and later in his life admitted that it was false, invented to deflect unwanted attention arising from the team being named after him. The Browns and the NFL now both support the position that the team was indeed named after Paul Brown.\n\nAs the war began to wind down with Germany's surrender in May 1945, the team parlayed Brown's ties to college football and the military to build its roster. Negotiations with players were handled by John Brickels, the team's acting manager, as Brown was still in the Navy. The first signings were Otto Graham, a former star quarterback at Northwestern University, and Herb Coleman, a center at Notre Dame, both of whom were then in the military. The Browns later signed kicker and offensive tackle Lou Groza and wide receivers Dante Lavelli and Mac Speedie. Fullback Marion Motley and nose tackle Bill Willis, two of the earliest African Americans to play professional football, also joined the team in 1946.\n\nThe Browns' first regular-season game took place September 6, 1946, at Cleveland Municipal Stadium against the Miami Seahawks before a record crowd of 60,135. That contest, which the Browns won 44–0, kicked off an era of dominance. With Brown at the helm, the team won all four of the AAFC's championships from 1946 until the conference's dissolution in 1949, amassing a record of 52 wins, four losses, and three ties. This included the 1948 season, in which the Browns became the second unbeaten and untied team in professional football history, 24 years before the NFL's 1972 Miami Dolphins duplicated the feat. Cleveland's total undefeated streak stretched to 18 wins and included the 1947 and 1948 AAFC championship games.\n\nThe Browns had few worthy rivals among the AAFC's eight teams, but the New York Yankees and San Francisco 49ers were their closest competition. Cleveland met the Yankees in the 1946 and 1947 championships and faced the 49ers for the title in 1949, winning all of those games. One of the highlights of the AAFC years was a contest between the 49ers and Browns in 1948. Both teams came into the game undefeated, with the Browns 9–0 and the 49ers 10–0. Behind a stiff defense and helped by San Francisco turnovers, the Browns won the \"clash of the unbeaten\" by a score of 14 to 7 before a crowd of 82,769, a professional football attendance record at the time.\n\nWhile the Browns excelled on defense, Cleveland's winning ways were driven by an offense that employed Brown's version of the T formation, which emphasized speed, timing, and execution over set plays. Brown liked his players \"lean and hungry\", and championed quickness over bulk. Graham became a star under Brown's system, leading all passers in each of the AAFC's seasons and racking up 10,085 passing yards. Motley, who Brown in 1948 called \"the greatest fullback that ever lived,\" was the AAFC's all-time leading rusher. Lavelli was the league's top receiver in 1946, while Speedie won those honors in 1947 and 1949. Brown and six players from the Browns' AAFC years were elected to the Pro Football Hall of Fame: Graham, Motley, Groza, Lavelli, Willis, and center Frank Gatski.\n\nThe Cleveland area showered support on the Browns from the outset. Brown's celebrity was cresting in the late 1940s, thanks to his success with teams at the high school, college, and now professional levels. Meanwhile, the Browns unexpectedly had Cleveland to themselves; the NFL's Cleveland Rams, who had continually lost money despite winning the 1945 NFL championship, moved to Los Angeles after that season. The Browns' on-field feats only amplified their popularity, and the team saw a record-setting average attendance of 57,000 per game in its first season.\n\nThe Browns, however, became victims of their own success. Their dominance exposed a lack of balance among AAFC teams, which the league tried to correct by sending Browns players including quarterback Y. A. Tittle to the Baltimore Colts in 1948. Attendance at Browns games fell in later years as fans lost interest in lopsided victories. Despite an undefeated season in 1948, only 23,891 people showed up to see the Browns beat the Buffalo Bills in the championship game. These factors – combined with a war for players between the two leagues that raised salaries and ate into owners' profits – ultimately led to the dissolution of the AAFC and the merger of three of its teams, including the Browns, into the NFL in 1949. The NFL has so far refused to acknowledge AAFC statistics and records because the Browns' achievements – including their perfect season – did not take place in the NFL or against NFL teams, and not even in a league fully absorbed by the NFL.\n\nThe AAFC proposed match-ups with NFL teams numerous times during its four-year existence, but no inter-league game ever materialized. That made the Browns' entry into the NFL in the 1950 season the first test of whether their early supremacy could continue into a more established league. Some people suggested Cleveland was at best the dominant team in a minor league, while others were confident of its prospects in the NFL. The proof of Cleveland's mettle came quickly: its NFL regular-season opener was against the two-time defending champion Eagles on September 16 in Philadelphia. The Browns quashed any doubts about their prowess in that game, amassing 487 yards of offense—including 246 passing yards from Graham and his receivers—en route to a 35–10 win before a crowd of 71,237.\n\nBehind a potent offense that featured Graham, Groza, Motley, Lavelli, and running back Dub Jones, the Browns finished the 1950 regular season with a 10–2 record, tied for first place in their conference. Cleveland then won a playoff game 8–3 against the New York Giants on December 17 behind a pair of Groza field goals, turning the tables on a team that handed the Browns both of their regular-season losses. That set up the NFL championship match a week later in Cleveland, between the Browns and the Rams, the NFL team that had moved from Cleveland just five years earlier. The Browns won the championship game, 30–28, on a last-minute Groza field goal. Fans stormed the field after the victory, carting off the goalposts, ripping off one player's jersey, and setting a bonfire in the bleachers. \"There never was a game like this one,\" Brown said.\n\nAfter five straight championship wins in the AAFC and NFL, the Browns appeared poised to bring another trophy home in 1951. The team finished the regular season with 11 wins and a single loss in the first game of the season. Cleveland faced the Rams on December 23 in a rematch of the previous year's title game. The score was deadlocked 17–17 in the final period, but a 73-yard touchdown pass by Rams quarterback Norm van Brocklin to wide receiver Tom Fears broke the tie and gave Los Angeles the lead for good. The 24–17 loss was the Browns' first in a championship game.\n\nThe 1952 and 1953 seasons followed a similar pattern, with Cleveland reaching the championship game but losing both times to the Detroit Lions. The Browns finished 8–4 in 1952, but lost that year's championship game 17–7 after a muffed punt, several Lions defensive stands and a 67-yard touchdown run by Doak Walker scuttled their chances. The team finished 11–1 in 1953, and narrowly lost the championship game to the Lions by a score of 17–16 on a 33-yard Bobby Layne touchdown pass to Jim Doran with just over two minutes left.\nWhile the championship losses sowed bitterness among Cleveland fans who had grown accustomed to winning, the team continued to make progress. Len Ford, whom the Browns picked up from the defunct AAFC Los Angeles Dons team, emerged as a force on the defensive line, making the Pro Bowl each year between 1951 and 1953. Second-year wideout Ray Renfro became a star in 1953 with 722 yards receiving and 352 yards rushing, also reaching the Pro Bowl.\n\nMeanwhile, in a November 15, 1953 game against the 49ers, Otto Graham took an elbow from linebacker Art Michalik that put a gash on his face requiring 15 stitches. Graham's helmet was fitted with a clear plastic mask and he was sent back on the field. While the use of face masks was not unheard-of – Y.A. Tittle was using one that season as he nursed a fractured cheekbone – the injury contributed to their development. Brown is often credited with inventing and patenting the single-bar face mask. Riddell, a sports equipment manufacturer, claims the \"bar tubular\" mask was invented by G.E. Morgan for Graham in 1955, although by the end of that year many players were already wearing some form of face protection.\n\nDuring the summer before the 1953 season, the Browns' original owners sold the team for a then-unheard-of $600,000. The old stockholders were Arthur B. \"Mickey\" McBride and his son Edward, along with minority owners including McBride business associate Dan Sherby, Brown, and four others. The buyers were a group of prominent Cleveland businessmen: Homer Marshman, an attorney, Dave R. Jones, a businessman and former Cleveland Indians director, Ellis Ryan, a former Indians president, Saul Silberman, owner of the Randall Park race track, and Ralph DeChairo, an associate of Silberman. McBride said he made the deal simply because he \"had his fling\" with football and wanted to move on to other activities. McBride's tenure as owner was viewed favorably, partly because of the Browns' on-field success, but also because he gave Brown a free hand to sign players and coaches. One of the new ownership group's first acts was to assure Cleveland fans they would give Brown the same kind of leeway.\n\nThe Browns came into 1954 as one of the most powerful teams in the NFL, having reached the championship in each of their first four years in the league, but the future was far from certain. Graham, whose leadership and throwing skills had been instrumental in the Browns' championship runs, said he planned to retire after the season. Motley, the team's best rusher and blocker in its early years, retired at the beginning of the season with a bad knee. Star defensive lineman Bill Willis also retired before the season. Still, Cleveland finished the regular season 9–3 as Graham and Lavelli excelled on offense and linemen Len Ford and Don Colo held up the defense. The Browns met Detroit on December 26 in the championship game for a third consecutive time. And this time the Browns dominated on both sides of the ball, intercepting Bobby Layne six times and forcing three fumbles while Graham threw three touchdowns and ran for three more. The Browns, who lost the last game of the regular season to the Lions only a week before, won their second NFL crown 56–10. \"I saw it, but still hardly can believe it,\" Lions coach Buddy Parker said after the game. \"It has me dazed.\"\n\nThe Browns kept rolling in 1955 after Brown convinced Graham to come back and play, arguing that the team lacked a solid alternative. Chuck Noll had a productive season at linebacker with five interceptions, Graham passed for 15 touchdowns and ran for six more, and the team finished the regular season 9–2–1. The Browns went on to win their third championship game in six NFL seasons, beating the Los Angeles Rams 38–14. It was Graham's last game; the win capped a 10-year run in which he led his team to the league championship every year, winning four in the AAFC and three in the NFL.\n\nThe end of the Graham era, however, was also the end of the Browns' dominant streak. The team floundered in 1956 as it struggled to find a permanent replacement for Graham. The season began with long-time backup George Ratterman at quarterback, but Babe Parilli took his place when the starter was injured. After Parilli was hurt, relative unknown Tommy O'Connell took up the position for the remainder of the season. None of them threw more touchdowns than interceptions, and Cleveland's 5–7 finish was its first losing season ever. Dante Lavelli and Frank Gatski retired at the end of the season, leaving Groza as the only original Cleveland player still on the team.\n\nWhile the Browns' on-field play in 1956 was uninspiring, off-the-field drama developed after a Cleveland-based inventor named George Sarles let Brown test a helmet with a radio transmitter inside. After trying it out in training camp, Brown used the helmet to call in plays during a September 15 preseason game against the Lions with Ratterman behind center. The device allowed the coach to direct his quarterback on the fly, giving him an advantage over franchises which had to use messenger players to relay instructions. The Browns used the device off and on into the regular season, and other teams began to experiment with their own radio helmets. The NFL's commissioner at the time, Bert Bell, banned the device in October 1956. Today, however, all NFL teams use in-helmet radios to communicate with players.\n\nWith Otto Graham and most of the other original Browns in retirement, by 1957, the team was struggling to replenish its ranks. Cleveland was coming off a series of bad drafts, including in 1954, when the team selected quarterback Bobby Garrett with the first pick. Garrett, Graham's presumed successor, did not play a single game for Cleveland, which traded him to Green Bay, brought him back three years later, then released him for good after he could not overcome a stutter that made calling plays in the huddle difficult.\n\nIn 1957's draft, however, Cleveland took fullback Jim Brown out of Syracuse University in the first round. In his first season, Brown was the NFL's leading rusher with 942 yards in a 12-game regular season and was voted rookie of the year in a United Press poll. Led by Brown's running and quarterback Tommy O'Connell's passing, Cleveland finished 9–2–1 and again advanced to the championship game against Detroit, but the Lions dominated the game, forcing six turnovers and allowing only 112 yards passing in a 59–14 rout, Detroit's last league championship to date.\n\nBefore the 1958 season, Cleveland was again in search of a quarterback. O'Connell had played well in the previous two seasons – he led the league in passing in 1957 – but lacked the stature and durability Paul Brown wanted in a starter. He stood at just 5 feet, 10 inches tall and was hurt numerous times, including a sprained ankle and a broken bone in his leg that kept him out of the last two games of the 1957 regular season. Due in part to these injuries, O'Connell retired early in 1958 to take a coaching job in Illinois, and Milt Plum was named the starter. Cleveland, however, was relying increasingly on the running game, in contrast to its pass-happy early years under Graham. As the team built up a 9–3 regular-season record, Brown in 1958 ran for 1,527 yards – almost twice as much as any other back and a league record at the time.\n\nEntering the final game of the 1958 season, Cleveland needed to either win or tie against the New York Giants to clinch the Eastern Conference title and the right to host the championship game. On the game's first play from scrimmage, Brown raced 65 yards for a touchdown, giving Cleveland a 7–0 lead. Entering the fourth quarter, the Browns held a 10–3 advantage, but the Giants tied it then won it with two minutes left on a 49-yard field goal by Pat Summerall under snowy conditions. That set up a playoff between the Browns and Giants the following week. In that game, Brown was held to eight yards and the team committed four turnovers in a 10–0 loss. The Giants went on to play the Baltimore Colts in the championship, a game often cited as the seed of professional football's popularity surge in the U.S.\n\nCleveland's campaigns in 1959 and 1960 were noteworthy for Brown's league-leading rushing totals in both seasons. Plum became established as the starting quarterback, bringing a measure of stability to the squad not seen since Graham's retirement. The Penn State product led the team to a 7–5 record in 1959, and one year later, he turned in one of the greatest statistical seasons at his position in NFL history. Nonetheless, in 1960, the Browns dropped three games by a total of 10 points, finished with an 8–3–1 record and out of the playoffs for the second consecutive season.\n\nArt Modell, a 35-year-old advertising executive from Brooklyn, purchased the team in 1961 from a group of shareholders led by National Insurance Company. The beginnings of a power struggle between Paul Brown and Art Modell took its toll. Journalist D.L. Stewart recounted in Jeff Miller's book on the AFL, \"Going Long\", \"As you well can imagine, Jimmy Brown and Paul were not thick. The buzz was that Jimmy had Modell working for him, and Paul took exception to that.\" The season otherwise was typical: a fifth consecutive league-leading season from Jim Brown and a half-decent performance in the standings, but again, at 8–5–1, they were two games out of a berth in the championship.\n\nAfter a 7–6–1 record in 1962, Modell fired Paul Brown on January 9, 1963, and replaced him one week later with longtime assistant Blanton Collier. Many of the Browns' younger players, such as Jim Brown and quarterback Frank Ryan, had chafed under Brown's autocratic coaching style; in contrast, Collier ran the club with a much looser grip. He installed a much more open offense and allowed Ryan to call his own plays. In Collier's first season, the Browns won their first six games, but a damaging midseason slump ended up costing them the Eastern Division title as they finished one game back with a 10–4 mark. On an individual level, Jim Brown won Most Valuable Player accolades with a record 1,863 yards rushing.\n\nIn 1964, the Browns went 10–3–1 and reached their first title game in seven years. They throttled the heavily favored Baltimore Colts 27–0, with receiver Gary Collins catching three touchdown passes to earn the MVP award. The following year, the Browns again reached the title game, but came up short against the Green Bay Packers.\n\nThat 1965 title game marked the final game in a Browns uniform for Jim Brown. During the start of the subsequent training camp, Brown was in England filming scenes for \"The Dirty Dozen\" due to production delays, and on July 14, Brown announced his retirement from football to concentrate on his acting career. The Browns were able to blunt the effect of Brown's departure with the emergence of third-year running back Leroy Kelly, who rushed for more than 1,000 yards in each of the next three seasons, leading the league during the latter two years.\n\nAfter missing out on the postseason in 1966, the Browns rebounded with a 9–5 season the following year. However, they were quickly eliminated by the Dallas Cowboys, 52–14 in the first round of the playoffs for the Eastern Conference title. In each of the next two seasons, the Browns took revenge on the Cowboys in the playoffs, winning by scores of 31–20 in 1968 and 38–14 in 1969. However, both victories were in turn followed by stinging defeats, preventing them from reaching the Super Bowl.\n\nIn May 1969, the Browns, along with the Pittsburgh Steelers and Baltimore Colts, agreed to move in 1970 to the postmerger American Football Conference. Inconsistent performances throughout the 1970 campaign proved to be fatal to postseason hopes as the team finished one game behind Paul Brown's upstart Cincinnati Bengals with a 7–7 record. Late in the 1970 season, Collier officially announced his retirement due to increased hearing problems, and was replaced by the team's offensive coordinator Nick Skorich.\n\nSkorich led the Browns to a division title in and a wild-card berth in . In the latter year, the Browns battled the undefeated Miami Dolphins before losing 20–14, as the Dolphins went on to capture their first Super Bowl title with a spotless 17–0 mark. In , the Browns were handicapped by a struggling offense, but remained in contention until the closing weeks of the season, finishing with a 7–5–2 record.\n\nHowever, the team's era of success came to a crashing halt as it dropped to 4–10 in 1974. Neither quarterback Mike Phipps nor rookie Brian Sipe was effective behind center; they threw 24 combined interceptions to only 10 touchdowns. The Browns allowed 344 points, most in the league. It was only the second losing season in franchise history, and it cost Skorich his job.\nAssistant coach Forrest Gregg took over in , but the Browns stumbled out of the gate with an 0–9 start that finally came to an end on November 23 in a 35–23 comeback victory over the Cincinnati Bengals. Three weeks later, third-year running back Greg Pruitt paced the team with 214 yards rushing in a rout over the Kansas City Chiefs, helping the team finish the season 3–11.\n\nCleveland showed marked improvement with a 9–5 record in as Brian Sipe firmly took control at quarterback. Sipe had been inserted into the lineup after a Phipps injury in the season-opening win against the New York Jets on September 12. After a 1–3 start brought visions of another disastrous year, the Browns jolted the two-time defending Super Bowl champion Steelers with an 18–16 victory on October 10. Third-string quarterback Dave Mays helped lead the team to that victory, while defensive end Joe \"Turkey\" Jones' pile-driving sack of Pittsburgh quarterback Terry Bradshaw fueled the heated rivalry between the two teams. That win was the first of eight in the next nine weeks, helping put the Browns in contention for the AFC playoffs. The Browns rattled off wins over the Atlanta Falcons and San Diego Chargers, before a 21–6 loss to their cross-state rivals, the Paul Brown-owned Bengals. The Browns rebounded and picked up wins over the Houston Oilers, Philadelphia Eagles, Tampa Bay Buccaneers, Miami Dolphins, and a second victory over Houston, giving the Browns a series sweep for the season. A 39–14 blow out loss to the Kansas City Chiefs in the regular season finale cost them a share of the division title, but running back Pruitt continued his outstanding play by rushing for exactly 1,000 yards, his second-straight four-digit season. Sipe finished the season completing 178 passes in 312 attempts, for 2,113 yards. He also had 17 touchdown passes versus 14 interceptions.\n\nThe Browns continued to roll in the first half of the 1977 season, but an injury to Sipe by Steelers linebacker Jack Lambert on November 13 proved to be disastrous. Cleveland won only one of their last five games to finish at 6–8, a collapse that led to Forrest Gregg's dismissal before the final game of the season. Dick Modzelewski served as interim coach in the team's 20–19 loss to the Seattle Seahawks.\nOn December 27, 1977, Sam Rutigliano was named head coach, and he aided a healthy Sipe in throwing 21 touchdowns and garnering 2,900 yards during the 1978 NFL season. Greg Pruitt and Mike Pruitt (no relation) led a rushing attack that gained almost 2,500 yards, but problems with the team's dismal pass defense resulted in the Browns finishing 8–8 on the year.\n\nThe 1979 campaign started with four consecutive wins, three of which were in the final minute or overtime. Four more games were won by less than a touchdown. This penchant for playing close games would later earn them the nickname \"Kardiac Kids\". Sipe threw 28 touchdown passes, tying him with Steve Grogan of New England for most in the league, but his 26 interceptions were the worst in the league. Mike Pruitt had a Pro Bowl season with his 1,294 rushing yards, while the defense was still shaky, ranking near the bottom in rushing defense. The team finished 9–7, behind division rivals Houston and Pittsburgh in a tough AFC Central.\n\nThe 1980 season is still fondly remembered by Browns fans. After going 3–3 in the first six games, the Browns won three straight games with fourth-quarter comebacks, and stopped a late comeback by the Baltimore Colts to win a fourth. The Browns won two more games in that fashion by the end of the season, and even lost a game to the Minnesota Vikings on the last play when a Hail Mary pass was tipped into the waiting hands of Ahmad Rashād. Sipe passed for 4,000 yards and 30 touchdowns with only 14 interceptions (enough for him to be named the NFL MVP), behind an offensive line that sent three members to the Pro Bowl: Doug Dieken, Tom DeLeone, and Joe DeLamielleure. The \"Kardiac Kids\" name stuck. A fourth-quarter field goal by Don Cockroft in the final game against the Bengals helped the Browns capture the division with an 11–5 mark, with the Oakland Raiders their opponent in the team's first playoff game in eight years. However, a heartbreaking end to this dramatic season came in the closing seconds when Rutigliano called what became known as \"Red Right 88\" and had Sipe pass toward the end zone, only to watch Oakland's Mike Davis intercept the ball. The Raiders went on to win the Super Bowl, and \"Red Right 88\" has numbered among the list of Cleveland sports curses ever since.\n\nIf 1980 was a dream season, then was a nightmare. Sipe threw only 17 touchdowns while being picked off 25 times. The Browns went 5–11, and few of their games were particularly close. Tight end Ozzie Newsome, their only Pro Bowler, had 1,004 yards receiving for six touchdowns.\n\nIn , Sipe split quarterbacking duties with Paul McDonald, and both put up similar numbers. The Browns had little success rushing or defending against it, finishing in the bottom five teams in both yardage categories. Despite going 4–5, Cleveland was able to make the playoffs due to an expanded playoff system in the strike-shortened year. They were matched up again with the Raiders in the playoffs, but were easily defeated 27–10.\n\nSipe and the Browns got some of their spark back in . Sipe had 26 touchdown passes and 3,566 yards, while Mike Pruitt ran for 10 scores on 1,184 yards. Cleveland even won two games in overtime and another in the fourth quarter. A fourth-quarter loss to the Oilers in their second-to-last game dashed their playoff hopes. At 9–7, the Browns finished one game behind the Steelers, and lost out on a wild-card spot due to a tiebreaker.\n\nIn 1984, a rebuilding year, Brian Sipe had defected to the upstart United States Football League after the 1983 season, and Paul McDonald was named the starting quarterback. Mike Pruitt missed much of the season and later ended up with the Buffalo Bills. Coach Sam Rutigliano lost his job after a 1–7 start as Marty Schottenheimer took over. The Browns coasted to a 5–11 record.\n\nIn 1985, the Browns selected University of Miami quarterback Bernie Kosar in the supplemental draft. As a rookie, Kosar learned through trial by fire as he took over for Gary Danielson midway through the 1985 season. Progressing a bit more each Sunday, the young quarterback helped turn the struggling season around, as the Browns won four of the ten games Kosar started. Two young rushers, Earnest Byner and Kevin Mack, played a large part in the team's success, as well; each ran for 1,000+ yards. The Browns' 8–8 record gave the team the top spot in a weak AFC Central, and they looked poised to shock the heavily favored Miami Dolphins in the divisional playoff game with a 21–3 lead at halftime. It took Dan Marino's spirited second-half comeback to win the game for Miami 24–21. While the Browns' faithful may have felt the initial sting of disappointment, there was tremendous upside in the loss: Schottenheimer's team, with Kosar at quarterback, reached the playoffs each of the next five seasons, advancing to the AFC championship game in three of those years.\nThe Browns broke into the ranks of the NFL's elite—particularly on defense—with a 12–4 showing in . Behind Kosar's 3,854 yards passing and one of the league's stingiest defenses featuring five Pro Bowlers (Chip Banks, Hanford Dixon, Bob Golic, Clay Matthews, and Frank Minnifield), the Browns dominated the AFC Central with the best record in the AFC and clinched home-field advantage throughout the playoffs. In the divisional playoffs, the Browns needed some serious heroics (and a bit of luck) to overcome the New York Jets. The Jets were leading 20–10 with less than four minutes to play, with the Browns in a dire third and 24 situation. Mark Gastineau was called for roughing the passer, which gave Cleveland a first down. The drive ended with Kevin Mack running into the end zone for a touchdown. After going three-and-out, the Jets went back on defense, but allowed the rejuvenated Browns to again drive the ball deep into their end of the field. With 11 seconds remaining in regulation, Mark Moseley kicked a field goal to tie the game. In the first of two ensuing overtime periods, Moseley missed his next attempt, but later redeemed himself by ending what had become the second-longest game in NFL history, a 23–20 victory for the Browns.\n\nIn the 1986 AFC championship game, the Denver Broncos arrived in the windswept, hostile confines of Cleveland Municipal Stadium. No one knew at the time, but the Broncos would become Cleveland's nemesis of the Kosar era, having only lost once to the Browns in a span that still continues to this day. As with the divisional playoffs of the previous week, the AFC title game would also prove to be an overtime heart-stopper. But this time, John Elway and the Broncos came away the victors. Pinned in on the Denver two-yard line with 5:11 left to play and the wind in his face, Elway embarked on his now-famous 98-yard march downfield, which is now known by NFL historians as simply \"the Drive\". With 34 seconds on the clock, Elway's five-yard touchdown pass to Mark Jackson tied the game at 20 apiece. The 79,973 Browns fans in attendance were silenced when Rich Karlis' field goal attempt just made it inside the left upright to win the game 23–20 for Denver early into overtime.\n\nThe Browns' success was replicated in , with 22 touchdown passes and 3,000 yards for Kosar and eight Pro Bowlers (Kosar, Mack, Dixon, Golic, Minnifield, linebacker Clay Matthews, wide receiver Gerald McNeil, and offensive lineman Cody Risien). Cleveland won another AFC Central crown with a 10–5 record and easily defeated the Indianapolis Colts 38–21 in the divisional playoff to set up a rematch with the Broncos in the AFC championship game, this time in Denver. With the score 21–3 in favor of the Broncos at halftime, Kosar led a third-quarter comeback with two touchdowns by Earnest Byner and another by Reggie Langhorne. Early in the fourth quarter, Webster Slaughter's four-yard touchdown catch tied the game at 31–31. The Broncos regained the lead with a 20-yard Sammy Winder touchdown with less than five minutes to go, setting the stage for one final drive by the Browns. Kosar drove the Browns to the Broncos' 8-yard line with 1:12 to go, and handed off to Byner. Just when it looked like he had an open route to the end zone, Broncos defensive back Jeremiah Castille stripped him of the ball. The Broncos recovered what became known as \"The Fumble\". After taking an intentional safety, the Broncos had shocked the Browns again, 38–33.\n\nInjuries to Kosar and two of his backups sidelined them for much of the season, but the Browns still finished 10–6. A final-week comeback victory in a snowstorm at Cleveland Municipal Stadium over the Houston Oilers clinched them a wild-card playoff spot and a home game rematch against the Oilers in the first round. After Mike Pagel, in for an injured Don Strock (the recently signed ex-Dolphins quarterback), threw a touchdown pass to Webster Slaughter late in the fourth quarter to pull the Browns within a point at 24–23, the Browns had three chances to recover an onside kick (due to penalties), but the Oilers recovered and stopped the Cleveland comeback.\n\nSchottenheimer left the Browns by mutual agreement with Modell shortly after the loss to the Oilers. Modell was tired of losing in the playoffs, and Schottenheimer was tired of what he perceived as Modell's interference with his coaching personnel and game strategy. The Kansas City Chiefs quickly hired Schottenheimer for the season. Bud Carson was his replacement in Cleveland, but his tenure was short—only one and a half years.\n\nThe 1989 season opened with the Browns defeating the rival Pittsburgh Steelers at Three Rivers Stadium in Pittsburgh 51–0, which as of 2010 remains the most lopsided game in the rivalry, as well as the all-time worst loss for the Steelers. The rest of the season was headlined by Slaughter's Pro Bowl-worthy 1,236 yards receiving, and was a success at 7–3 until a 10–10 tie with Schottenheimer's Chiefs in November led to a three-game losing streak. Two comeback wins over the Minnesota Vikings and Houston Oilers in the season's final two weeks kept them in the playoff race. The tie ended up being the Browns' saving grace, with their 9–6–1 record winning them the AFC Central title and first-round bye over the Oilers and Pittsburgh Steelers at 9–7. The Browns narrowly survived a scare from the Buffalo Bills in their divisional playoff game, when Scott Norwood missed an extra point that would have pulled Buffalo within three points and, later, when Jim Kelly's desperation pass to the end zone on the final play of the game was intercepted by Clay Matthews.\n\nCleveland's 34–30 win set them up for another game with the Broncos in Denver for the AFC championship. While their two previous matchups went down to the wire, the result of this particular game was never in doubt. The Broncos led from start to finish, and a long Elway touchdown pass to Sammy Winder put the game away in the fourth quarter. Denver easily won 37–21.\n\nIn , things began to unravel. Kosar threw more interceptions (15) than touchdowns (10) for the first time in his career, and the team finished last in the league in rushing offense, and near the bottom in rushing defense. Carson was fired after a 2–7 start, and the team finished 3–13, second-worst in the league. After the season, Bill Belichick, defensive coordinator of the then-Super Bowl champion New York Giants, was named head coach.\n\nThe Browns had only a slight improvement under Belichick in , finishing 6–10. Kosar was markedly better, with a ratio of 18 touchdowns to nine interceptions, and Leroy Hoard had a breakout season. The next season, with Kosar sitting out much of the season and Mike Tomczak in under center, Cleveland was in the thick of the AFC Central race before dropping their final three games to finish 7–9.\n\nThe season had Belichick make the controversial decision of cutting Kosar while backup Vinny Testaverde, who had been signed from the Tampa Bay Buccaneers, was injured. The Browns were in first place at the time and the Browns faltered as Todd Philcox became the starter. Kosar was signed by the Dallas Cowboys and a few days later led the Cowboys to a win in place of an injured Troy Aikman. Kosar won a ring that season as the Cowboys won the Super Bowl with a healthy Aikman. Cleveland won only two of its final nine games, finishing 7–9 once again.\nCleveland managed to right the ship in , although the quarterback situation had not quite improved. A solid defense led the league for fewest yards allowed per attempt, sending four players (Rob Burnett, Pepper Johnson, Michael Dean Perry, and Eric Turner) to the Pro Bowl. The Browns finished 11–5, making the playoffs for the first time in five seasons. In the AFC wild card game against the New England Patriots, the Browns' defense picked off Drew Bledsoe three times, with Testaverde completing two-thirds of his passes, to win 20–13. Arch-rival Pittsburgh ended the Browns' season the following week, however, with a 29–9 blowout in the AFC divisional game.\n\nModell announced on November 6, 1995, that he had signed a deal to relocate the Browns to Baltimore in —a move which would return the NFL to Baltimore for the first time since the Colts relocated to Indianapolis after the 1983 season. The very next day, on November 7, 1995, Cleveland voters overwhelmingly approved an issue that had been placed on the ballot at Modell's request, before he made his decision to move the franchise, which provided $175 million in tax dollars to refurbish the outmoded and declining Cleveland Municipal Stadium. Taxpayers ultimately paid close to $300 million to demolish the old stadium and construct a new stadium for the Browns on the site of Municipal Stadium.\n\nBrowns fans reacted angrily to the news of Modell's plan to relocate the Browns. Over 100 lawsuits were filed by fans, the city of Cleveland, and a host of others. Congress held hearings on the matter. Actor/comedian Drew Carey returned to his hometown of Cleveland on November 26, 1995, to host \"Fan Jam\" in protest of the proposed move. A protest was held in Pittsburgh during the Browns' game there—one of the few instances that Steelers fans and Browns fans were supporting each other, as fans in Pittsburgh felt that Modell was robbing their team of their rivalry with the Browns. Virtually all of the team's sponsors immediately pulled their support, leaving Municipal Stadium devoid of advertising during the team's final weeks.\n\nThe season was a disaster on the field, as well. After starting 3–1, the Browns lost three straight. They split the next two games, but the announcement of the move to Baltimore cut the legs out from under the team. They finished 5–11, including a 1–6 record in the seven games after the announcement. Fans in the Dawg Pound became so unruly during their final home game against the Bengals that all offensive plays had to be run from the opposite end of the field. Rows of empty seats were torn from the stadium and thrown on the field. Stalls and sinks in the restrooms were torn from the walls. Several fans set fires in the stands, especially in the \"Dawg Pound\" section, and assaulted security officials and police officers who tried to quench the growing fires. The Browns won their final home game. Belichick was fired by Modell by telephone in February 1996, exactly one week after the switch to Baltimore was made official. The new team became the Baltimore Ravens.\n\nAfter extensive talks between the NFL, the Browns, and officials of the two cities, Cleveland accepted a legal settlement that would keep the Browns legacy in Cleveland. In February 1996, the NFL announced the Browns would be 'deactivated' for three years, and a new stadium would be built for a new Browns team, as either an expansion team or a team moved from another city, that would begin play in 1999. Modell would, in turn, be granted a new franchise, the NFL's 31st, for Baltimore, the Baltimore Ravens, retaining the current contracts of players and personnel. The Browns ceased play at the end of the 1995 season when Modell relocated the organization to Baltimore. The Browns franchise was then reactivated, and its roster restocked by an expansion draft before resuming play in the 1999 season. The team would be new, but the Browns' name, colors, history, records, awards, and archives would all remain in Cleveland. The move fueled a proliferation of 12 new stadiums throughout the NFL. Using the NFL–City of Cleveland agreement's promise to supply a team to Cleveland by 1999, several NFL franchises used the threat of relocation to coerce their respective cities to build new stadiums with public funds.\n\nCleveland NFL Football LLC (Cleveland Browns Trust) was formed by the NFL. The president of the trust was Bill Futterer, and NFL Commissioner Paul Tagliabue was the trustee. The trust represented the NFL in the stadium design and construction, managed the sale of suites and club seats, and sold permanent seat licenses and season tickets. Additionally, the trust reorganized the Browns Backers fan clubs across the United States, resumed coaches' shows on television and radio throughout the state of Ohio, and conducted a dramatic one-year countdown celebration that incorporated the first live Internet broadcast in NFL history. The trust operated its campaign under a \"Countdown to '99\" theme, using Hall of Famers such as Lou Groza and Jim Brown extensively, and sold nearly 53,000 season tickets—a team record in 1998. It remains the only time in professional American football history that a league operated a team \"in absentia\" to preserve the history of the franchise and to build value in that franchise for the future owner. The NFL sold the Browns as an expansion team in 1998 to former Browns minority owner Al Lerner. The purchase price was a then- North American record $530 million, more than double any previous selling price for a professional sports team. Commissioner Tagliabue announced that the Browns would be an expansion team, rather than a relocated team, at the owners' meeting in March 1998.\n\nOfficially, the National Football League, Pro Football Hall of Fame, Browns, and Ravens all recognize the current Browns team as a continuation of the team founded in 1946. The Ravens \"1998 Fan and Media Guide\" referred to longtime staffers as having worked for \"the Modell organization\" before the Ravens were created in 1996.\n\nCleveland returned to the NFL in 1999 with high hopes and expectations, featuring deep-pocketed ownership in Al Lerner. The team's football operations appeared to be in solid hands in the form of president and CEO Carmen Policy and general manager Dwight Clark, both of whom had come from the San Francisco 49ers. Chris Palmer, former offensive coordinator of the Jacksonville Jaguars, was hired as head coach. The team was rebuilt from a special expansion draft and the regular NFL draft; the latter included the number one selection, QB Tim Couch.\n\nThe resurrected Browns were expected to struggle at first, as for all practical purposes, they were an expansion team. However, the Browns' first two seasons were awful even by expansion standards. The 1999 season started with a home game against the rival Steelers on \"ESPN Sunday Night Football\", with Cleveland native Drew Carey participating in the opening-game coin toss. However, that was the only highlight for the Browns that night. The Steelers beat the Browns 43–0 in their first game back. It was the team's worst shutout loss ever, topped later only by a 48–0 loss to Jacksonville on December 3, 2000.\n\nThe 1999 season Browns start was 0–7 en route to a 2–14 finish, the worst in franchise history until 2016. The season was slightly better, with a 3–13 finish—the lone highlight being the Browns' first home win in five years, against the Steelers on September 17. Compounding the fans' frustration was the Baltimore Ravens' win over the New York Giants in Super Bowl XXXV that season. Though the Ravens were considered a \"new franchise\", the team still had players such as Matt Stover and Rob Burnett, who had played for the Browns before the Modell move. Palmer was fired after the season and replaced by University of Miami coach Butch Davis.\n\nUnder Davis, the Browns became more competitive, finishing 7–9 in , three games out of the playoffs. With the team apparently close to being a contender again, Clark was forced to resign after the season, and Davis was named general manager, as well as coach. In , the Browns finished 9–7, and due to multiple tiebreakers, made the playoffs for the first time since 1994. Facing Pittsburgh in the first round, the Browns led 33–21 with five minutes to go, but ultimately lost 36–33. Their largest lead in the game was 17 points—they led 24–7 in the third quarter; after that, the Steelers outscored them 29–9. Also during the 2002 season, owner Al Lerner died on October 2, and ownership then was taken over by his son Randy.\n\nThe Browns could not sustain the momentum from 2002, finishing with double-digit losing records in , , , and . Davis resigned November 30, 2004, with the team shouldering a 3–8 record; Policy had resigned earlier in the year. Offensive Coordinator Terry Robiskie was named interim head coach for the remainder of the 2004 season.\n\nBefore the 2005 season began, Romeo Crennel, a one-time Browns assistant coach under Chris Palmer and, at the time, defensive coordinator for the New England Patriots, was named the Browns head coach. The team also hired Phil Savage – who had been Ravens VP/GM Ozzie Newsome's top aide – as a new general manager. But despite the changes, the Browns losing trend continued for 2005 and 2006, with records of 6–10 and 4–12, respectively. Prior to the Browns' final game of the 2005 season, ESPN reported that team president John Collins was going to fire Savage. However, the resulting uproar from fans and local media was strong, and on January 3, 2006, Collins resigned instead. The role of team \"President and CEO\" was vacated until 2008, with owner Randy Lerner filling in as \"de facto\" CEO until Michael Keenan was hired.\n\nIn the 2007 season, the team had a remarkable turnaround on the field. After opening the season with a 34–7 defeat by the Steelers, the Browns traded starting quarterback Charlie Frye to the Seattle Seahawks, with backup Derek Anderson assuming the starting role. In his first start, Anderson led the Browns to a 51–45 win over the Cincinnati Bengals, tying the franchise record of five touchdown passes in a single game. The Browns finished the 2007 season a surprising 10–6, but missed the playoffs due to a tie-breaker. Nevertheless, the record was the team's best since 1994. Six players earned Pro Bowl recognition, with Anderson starting for the AFC in place of New England Patriots quarterback Tom Brady. Coach Crennel agreed to a two-year contract extension.\n\nThe Browns entered the 2008 season with high expectations, and many pundits predicted the team would win the division. The highlight of the season was an upset of the defending Super Bowl champion New York Giants on \"Monday Night Football\". However, inconsistent play and key injuries led to a disappointing 4–12 record. The Browns ended up using four starting quarterbacks during the season: Derek Anderson, Brady Quinn, and Ken Dorsey were lost to injury; the fourth, Bruce Gradkowski, was signed midseason. Ending with six straight losses, the Browns finished with a franchise-first two consecutive shutouts. Savage and Crennel were subsequently fired.\n\nOn January 5, 2009, the Browns hired former New York Jets coach Eric Mangini as head coach. Mangini, who started his career as a ballboy in Cleveland, worked as an assistant under former Browns coach Bill Belichick until becoming head coach of the Jets in 2006. On January 25, the team hired George Kokinis as the team's general manager. The Browns continued to struggle as they became accustomed to a completely new coaching staff. Throughout the preseason, Brady Quinn and Derek Anderson competed for the starting quarterback position. Quinn ended up winning the job, but after three games marked by team inconsistency (and an 0–3 record), he was benched in favor of Anderson. On November 1, the team announced the firing of GM Kokinis after only eight regular-season games (and a 1–7 record), with his duties essentially being assumed by Mangini. Soon afterwards, Mangini decided that a quarterback switch was to be made again, and Quinn given the starting job back. After being 1–11 at the three-quarters point in the season, the team went on a four-game winning streak and finished with a record of 5–11, highlighted by the team beating the Steelers after 12 consecutive losses against their rival.\n\nOn December 21, 2009, as Mangini's first season was coming to a close, former Green Bay Packers and Seattle Seahawks head coach Mike Holmgren was hired as team president and was given authority over the team's football operations. This hire was made after Browns owner Randy Lerner announced he wished to bring in a \"serious, credible leader\" to steer the team in the right direction. After much public speculation by the media that Holmgren and Mangini would not be able to coexist, Holmgren announced the retention of Mangini and the entire coaching staff for the 2010 season. The following week, Holmgren hired former Philadelphia Eagles general manager Tom Heckert to become the new GM for the Browns.\n\nAfter taking control as president, Holmgren decided to release Anderson and trade away Quinn (getting back eventual 1,100+ yard rusher and fan favorite running back Peyton Hillis in return). He signed veteran quarterback Jake Delhomme, who had led the Carolina Panthers to the Super Bowl in 2003, along with veteran backup Seneca Wallace from the Seattle Seahawks. During the 2010 draft, the team of Holmgren, Heckert, and Mangini focused mostly on improving the team's defensive secondary, although they also managed to acquire the University of Texas's Colt McCoy in the third round; McCoy has the most recorded wins as a starting quarterback in NCAA history.\n\nDespite heading into the 2010 season with an overall sense of optimism, the Browns started off poorly. They set an NFL record when they lost their first three games after leading in the fourth quarter. They finally won their first game against the Cincinnati Bengals in week 4. However, both Delhomme and Wallace injured their ankles over the first five games, forcing Colt McCoy to start in week 6 against the Steelers, though Mike Holmgren stated he would sit and learn the entire season. Though McCoy lost his first NFL start against the Steelers, he was able to win the following week when the Browns upset the defending Super Bowl champions, the New Orleans Saints. With this victory, the Browns defeated the defending Super Bowl Champions three years in a row, becoming the seventh NFL team to achieve this feat. The Browns continued this positive streak by outplaying the New England Patriots for a 34–14 victory in their next game. However, they lost to the New York Jets in overtime the following week, despite a late fourth-quarter, game-tying touchdown drive by McCoy. On January 3, 2011, after losing four games in a row to end the season, Holmgren and the Browns decided to fire head coach Eric Mangini, who posted a record of 10–22 in his two seasons as head coach. Eleven days later, Holmgren signed St. Louis Rams offensive coordinator Pat Shurmur to become the new head coach, and former Buffalo Bills and Chicago Bears head coach Dick Jauron as their defensive coordinator.\n\nWhile potentially hamstrung in attempts to install a new offensive system by the NFL lockout, the Browns played through the first half of the 2011 season at or near the bottom of the league in almost every offensive category. Starting several rookies, Shurmur's team was frequently beset by confusion in personnel and play-calling at critical junctures. Early in the year, the defense surrendered a touchdown on a failure to break the defensive huddle, and the team's chances in several games were compromised by a host of special-teams mistakes and meltdowns. At the midpoint of the season, in a telling series in which Shurmur called for a conservative ball-protection strategy, the Browns recovered their own fumble resulting from a mistakenly called handoff to a third-string tight end lined up at fullback, only to botch a short go-ahead field goal attempt with a failed snap and uncoordinated line movement.\n\nThe Browns went 4–12 in Shumur's first season, including losing six in a row to end the 2011 campaign. During that same season, comedian and frustrated Browns fan Mike Polk made a video to complain about the team's futility, screaming \"You are a factory of sadness!\" to the Browns' home stadium; the colloquial \"Factory of Sadness\" name for the stadium has stuck ever since.\n\nNew owner Jimmy Haslam announced on October 16, 2012, that Holmgren would stay on through the 2012 season in a lesser role (though still officially listed as president), and would then retire. On November 25, CBS reported and the Browns confirmed Holmgren would officially step down following the Browns 20–14 win over the Steelers.\n\nIn July 2012, owner Randy Lerner announced he planned to sell the Browns to businessman Jimmy Haslam. The sale was finalized on August 2, 2012, in excess of $1 billion. Haslam officially was approved as the new owner on October 16, 2012, at the NFL owners' meetings, and the very next day former Eagles president Joe Banner was named as the Browns' new CEO.\n\nThe Browns began the 2012 season by losing their first five games. Having lost their last six games to end the 2011 season, this marked an 11-game losing streak, tied for the longest in team history with the 1974–75 teams. On October 12, the Browns defeated the Bengals 34–24 in Cleveland, behind two touchdown passes from rookie quarterback Brandon Weeden (on his 29th birthday) to end the streak. On December 31, 2012, head coach Pat Shurmur and general manager Tom Heckert were fired. Shurmur went 9–23 in his two seasons as head coach.\n\nAfter interviewing numerous candidates such as Chip Kelly and Ken Whisenhunt, the Browns decided to hire former offensive coordinator and tight ends coach, Rob Chudzinski, on January 10, 2013.\n\nOn January 15, 2013, Haslam / Banner announced the naming rights to Cleveland Browns Stadium were sold to FirstEnergy, and the stadium would be renamed FirstEnergy Stadium. The name change officially received Cleveland City Council approval on February 15, 2013.\n\nOn January 18, 2013, the Browns hired Michael Lombardi – who had a previous stint with the Browns in the player personnel department in the 1980s and 90s – as Vice President of Player Personnel (two months later he was officially given the title of general manager), making him Tom Heckert's replacement.\n\nThe Browns would finish with a 4-12 record in the first season under the new regime, finishing last in the AFC North Division, and losing seven in a row to finish the 2013 campaign. Following the 2013 season finale on December 29, 2013, the Browns fired Chudzinski after only one year as head coach.\n\nOn January 24, 2014, the Browns hired Bills defensive coordinator Mike Pettine as the 15th full-time head coach in team history.\n\nOn February 11, 2014 the Browns announced that Lombardi would be replaced by Ray Farmer as GM, and that Joe Banner would resign as CEO.\n\nIn the first round of the 2014 NFL draft, the Browns selected cornerback Justin Gilbert from Oklahoma State with the eighth pick, and Heisman Trophy-winning quarterback Johnny Manziel of Texas A&M with the 22nd overall pick.\n\nBeginning in the 2014 season, the Browns use a live bullmastiff named \"Swagger\" as their new mascot.\n\nOn October 5, 2014, the Browns staged the largest rally in team history, when after trailing the Tennessee Titans 28-3 with 1:09 left in the second quarter, Cleveland scored 26 unanswered points to win the game 29-28. This was also the largest rally by a road team in NFL history. After a 7-4 start, the Browns would lose their final five games to finish the 2014 season at 7-9, last in the AFC North.\n\nIn February 2015, the team made headlines when two high-profile players were in the news due to substance abuse issues. On Monday February 2, it was announced quarterback Johnny Manziel had checked himself into a treatment center, reportedly for alcoholism. The following day, wide receiver Josh Gordon was suspended for the 2015 season due to failing a drug test.\n\nOn February 28, it was revealed that former Tampa Bay Buccaneers quarterback Josh McCown had signed a three-year deal with the Browns.\n\nOn March 30, the NFL announced that Browns general manager Ray Farmer would be suspended for the first four regular season games, and that the team would be fined $250,000 (U.S.) for Farmer text messaging the coaching staff during games in the 2014 season, which is against NFL rules. The story had been dubbed \"Textgate\" due to its scandalous nature.\n\nOn April 14 at a ceremony at the Huntington Convention Center of Cleveland, the team unveiled their new uniforms. They feature brown, white, and orange jerseys along with brown, white, and orange pants that can be worn in any combination. Unique features include the word \"Cleveland\" across the front of the jersey, the word \"Browns\" going down the pant leg, and the words \"Dawg Pound\" on the inside collar - all first of their kind features on NFL uniforms. Browns President Alec Scheiner compared these new jerseys to those of the Oregon Ducks football team, as the Ducks are known for their various uniform combinations.\n\nIn the 2015 NFL draft, the Browns had two first round picks, selecting nose tackle Danny Shelton from Washington at #12, and offensive lineman Cameron Erving from Florida State at #19.\n\nOn September 8, 2015, the Browns announced that they indefinitely suspended offensive line coach Andy Moeller after an alleged domestic assault incident at his home during Labor Day weekend. This meant that at the beginning of the 2015 regular season, the team had a player (Josh Gordon), a coach (Moeller), and a front office executive (Ray Farmer) all suspended for various league and legal infractions. Moeller would subsequently be fired on September 29.\n\nAfter starting 2-3, the Browns lost 10 of their last 11 games to finish the 2015 season at 3-13. This stretch included a 33-27 home loss to the Baltimore Ravens in which Ravens safety Will Hill return a blocked field goal 64 yards for a touchdown on the game's final play. The Browns lost at home 37-3 to the division-rival Cincinnati Bengals the following week, dropping the team´s record to 2-10 and making them the first team in the 2015 season to be mathematically eliminated from playoff contention. On January 3, 2016, soon after the final game of the season (a 28-12 loss to the Pittsburgh Steelers), both Ray Farmer and Mike Pettine were fired from their respective positions as GM and head coach.\n\nIn January 2016, the Browns made headlines when after firing Farmer and Pettine, promoted general counsel Sashi Brown to Executive VP of Football Operations, and hired longtime baseball executive Paul DePodesta as chief strategy officer. These moves were viewed nationally as the Browns trying to take a more analytics intensive approach to building the team, taking a page from the \"Moneyball\" style of Major League Baseball teams like the Oakland Athletics - of which DePodesta helped pioneer during his time as an assistant to Athletics GM Billy Beane. With Brown essentially taking over GM duties, this marks the fourth different head of personnel (either as GM or similar job title) under the Haslam ownership era, which began in 2012.\n\nOn January 13, 2016, the Browns hired Bengals offensive coordinator (and former Oakland head coach) Hue Jackson as head coach - making him the eighth full-time head coach since the team's return in 1999 and fourth since 2012, when the Haslam ownership era began.\n\nOn January 28, the Browns hired Andrew Berry - a longtime scout with the Indianapolis Colts - as VP of Personnel. Berry, being a Harvard alumnus like DePodesta and Sashi Brown, has been noted as furthering the Browns new analytic approach, and the trio has been dubbed as the \"Harvard Connection\" (and other similar monikers) by local and national media.\n\nOn March 4, team president Alex Scheiner announced he would be stepping down from his post effective March 31, and would remain with the team as a consultant for the rest of the year. With this move, Paul DePodesta essentially became the top ranked executive of the team in his role as Chief Strategy Officer. This makes DePodesta the fourth different top executive of the team under the Haslams' ownership.\n\nOn March 11, following two seasons of inconsistent play on the field and numerous highly publicized incidents off the field, the Browns waived quarterback Johnny Manziel.\n\nOn March 24, the Browns signed quarterback Robert Griffin III to a two-year contract.\n\nGoing into the 2016 NFL Draft, the Browns had the #2 overall pick. They traded that pick to Philadelphia in exchange for the #8 pick in the first round (along with various later round 2016 picks, and Philadelphia's first round pick in 2017). On draft night they traded the #8 pick to Tennessee in exchange for the #15 pick in the first round (and later round picks). With the #15 pick in the 2016 draft, the Browns selected wide receiver Corey Coleman from Baylor.\n\nThe 2016 season began with the Browns losing their first 14 games, which combined with losing their last three games in 2015, gave the team a franchise record 17 game losing streak. On December 24, in a game that has since been dubbed \"The Christmas Miracle\", the Browns defeated the San Diego Chargers 20-17. The Browns would lose their last game of the season to finish 1-15 - the worst record in team history. With that final game loss, the Browns clinched the #1 pick in the 2017 NFL Draft, with which the Browns selected Myles Garrett, a defensive end from Texas A&M.\n\nThe Browns are the only National Football League team without a helmet logo. The logoless helmet serves as the Browns' official logo. The organization has used several promotional logos throughout the years; players' numbers were painted on the helmets from the 1957 to 1960; and an unused \"CB\" logo was created in 1965, But for much of their history, the Browns' helmets have been an unadorned burnt orange color with a top stripe of dark brown (officially called \"seal brown\") divided by a white stripe.\n\nThe team has had various promotional logos throughout the years, such as the \"Brownie Elf\" mascot or a Brown \"B\" in a white football. While Art Modell did away with the Brownie Elf in the mid-1960s, believing it to be too childish, its use has been revived under the current ownership. The popularity of the Dawg Pound section at First Energy Stadium has led to a brown and orange dog being used for various Browns functions. But overall, the orange, logo-less helmet continues as the primary trademark of the Cleveland Browns.\n\nOn February 24, 2015, the team unveiled its new logos and word marks, the only differences being minor color changes to the helmet with the helmet logo remaining largely as is.\n\nThe original designs of the jerseys, pants, and socks remained mostly the same, but the helmets went through many significant revisions throughout the years. The Browns uniforms saw their first massive change prior to the 2015 season.\n\nJerseys: 1. Brown: Brown (officially \"seal brown\") with orange colored numbers and writing, and an orange-white-orange stripe sequence on the sleeves. 2. White (Away): white with orange numbers and writing, with a brown-orange-brown stripe sequence. 3. Orange: Orange with white numerals and writing, and a brown-white-brown stripe sequence.\n\nPants: 1. Brown - Brown pants with an orange-white-orange stripe sequence down 2/3rds the length of the pants. the other third is the word \"BROWNS,\" written in orange. 2. White - white pants with a brown-orange-brown stripes. \"BROWNS\" Is written in Brown. 3. Orange - Orange pants with a brown-white-brown stripe sequence. \"BROWNS\" is written in brown.\n\nSocks: 1. Solid Brown. 2. Solid white. 3. solid Orange.\n\nHelmet: Solid white (1946–1949); solid white for day games and solid orange for night games (1950–51); orange with a single white stripe (1952–56); orange with a single white stripe and brown numerals on the sides (1957–59); orange with a brown-white-brown stripe sequence and brown numerals on the sides (1960); orange with a brown-white-brown stripe sequence (1961–95 and 1999–present).\n\nOver the years, the Browns have had on-again / off-again periods of wearing white for their home games, particularly in the 1970s and 80s, as well as in the early 2000s after the team returned to the league. Until recently, when more NFL teams have started to wear white at home at least once a season, the Browns were the only non-subtropical team north of the Mason-Dixon line to wear white at home on a regular basis.\n\nNumerals first appeared on the jersey sleeves in 1961. Over the years, there have been minor revisions to the sleeve stripes, the first occurring in 1968 (brown jerseys worn in early season) and 1969 (white and brown jerseys) when stripes began to be silk screened onto the sleeves and separated from each other to prevent color bleeding. However, the basic five-stripe sequence has remained intact (with the exception of the 1984 season). A recent revision was the addition of the initials \"AL\" to honor team owner Al Lerner who died in 2002; this was removed in 2013 upon Jimmy Haslam assuming ownership of the team.\n\nOrange pants with a brown-white-brown stripe sequence were worn from 1975 to 1983 and become symbolic of the \"Kardiac Kids\" era. The orange pants were worn again occasionally in 2003 and 2004.\n\nOther than the helmet, the uniform was completely redesigned for the 1984 season. New striping patterns appeared on the white jerseys, brown jerseys and pants. Solid brown socks were worn with brown jerseys and solid orange socks were worn with white jerseys. Brown numerals on the white jerseys were outlined in orange. White numerals on the brown jerseys were double outlined in brown and orange. (Orange numerals double outlined in brown and white appeared briefly on the brown jerseys in one pre-season game.) However, this particular uniform set was not popular with the fans, and in 1985 the uniform was returned to a look similar to the original design. It remained that way until 1995.\n\nIn 1999, the expansion Browns adopted the traditional design with two exceptions: first, jersey-sleeve numbers were moved to the shoulders; and second, the orange-brown-orange pants stripes were significantly widened.\n\nExperimentation with the uniform design began in 2002. An alternate orange jersey was introduced that season as the NFL encouraged teams to adopt a third jersey, and a major design change was made when solid brown socks appeared for the first time since 1984 and were used with white, brown and orange jerseys. Other than 1984, striped socks (matching the jersey stripes) had been a signature design element in the team's traditional uniform. The white striped socks appeared occasionally with the white jerseys in 2003–2005 and 2007.\n\nExperimentation continued in 2003 and 2004 when the traditional orange-brown-orange stripes on the white pants were replaced by two variations of a brown-orange-brown sequence, one in which the stripes were joined (worn with white jerseys) and the other in which they were separated by white (worn with brown jerseys). The joined sequence was used exclusively with both jerseys in 2005. In 2006, the traditional orange-brown-orange sequence returned.\n\nAdditionally in 2006, the team reverted to an older uniform style, featuring gray face masks; the original stripe pattern on the brown jersey sleeves (The white jersey has had that sleeve stripe pattern on a consistent basis since the 1985 season.) and the older, darker shade of brown.\n\nThe Browns wore brown pants for the first time in team history on August 18, 2008, preseason game against the New York Giants. The pants contain no stripes or markings. The team had the brown pants created as an option for their away uniform when they integrated the gray facemask in 2006. They were not worn again until the Browns \"family\" scrimmage on August 9, 2009 with white-striped socks. The Browns have continued to wear the brown pants throughout the 2009 season. Browns quarterback Brady Quinn supported the team's move to wearing the brown pants full-time, claiming that the striped pattern on the white pants \"prohibit[ed] mobility\".\nHowever, the fans generally did not like the brown pants, and after being used for only one season, the team returned to their white shirt-on-white pants in 2010. Coach Eric Mangini told \"The Plain Dealer\" the Browns won't use the brown pants anymore. \"It wasn't very well-received,\" Mangini said. \"I hope we can get to the point where we can wear fruit on our heads and people wouldn't notice.\" At the time, the brown pants weren't officially dropped by the team, but simply not used.\n\nThe Browns chose to wear white at home for the 2011 season, and wound up wearing white for all 16 games as when they were on the road, the home team would wear their darker colored uniform.\n\nThe Browns brought back the brown pants in their home game against the Buffalo Bills on October 3, 2013 on \"Thursday Night Football\", pairing them with the brown jerseys. It marked the first time the team wore an all-brown combination in team history.\n\nOn April 14, 2015, the Cleveland Browns unveiled their new uniform combinations, consisting of the team's colors of orange, brown and white.\n\nThe Cleveland Browns have rivalries with all three of its AFC North opponents. In addition, the team has had historical rivalries with the Indianapolis Colts, Denver Broncos, Buffalo Bills, and Detroit Lions.\n\nThe team's biggest rival in the AAFC was the San Francisco 49ers, though this has cooled and in some cases turned into a friendly relationship, as the Browns now play in AFC and the 49ers play in the NFC. Additionally, many 49ers personnel helped the Browns relaunch in 1999 as well as former team President Mike Holmgren having started his NFL career in San Francisco. Also, 49ers owners John York & Denise DeBartolo York reside in Youngstown, Ohio 60 miles southeast of Cleveland. Former long-time veteran placekicker and fan favorite, Phil Dawson, signed with the 49ers in 2014, along with backup QB Colt McCoy.\n\nOften called the \"Turnpike Rivalry\", the Browns' main rival has long been the Pittsburgh Steelers. Though the Browns dominated this rivalry early in the series (winning the first eight matchups), the Steelers currently have the all-time edge 68–58, making it the oldest rivalry in the AFC. Former Browns owner Art Modell scheduled home games against the Steelers on Saturday night from 1964 to 1970 to help fuel the rivalry. The rivalry has been fueled by the proximity of the two teams, number of championships both teams have won, players and personnel having played and/or coached for both sides, and personal bitterness. Though the rivalry has cooled in Pittsburgh due to the Modell move (as well as the Browns having a 5–24 record against the Steelers since returning to the league in 1999), the Steelers are still top rival for Cleveland.\n\nOriginally conceived due to the personal animosity between Paul Brown and Art Modell, the \"Battle of Ohio\" between the Browns and the Cincinnati Bengals has been fueled by the sociocultural differences between Cincinnati and Cleveland, a shared history between the two teams, and even similar team colors, since Brown used the exact shade of orange for the Bengals that he used for the Browns. (Though this has changed since then, as the Bengals now use a brighter shade of orange.) Modell, in fact, moved the Browns to the AFC after the AFL–NFL merger in order to have a rivalry with the Bengals. The rivalry has also produced two of the eight highest-scoring games in NFL history. Cincinnati has the all-time edge 46–39, having won the majority of games against the Browns since they returned to the NFL in 1999.\n\nCreated as a result of the Cleveland Browns relocation controversy, the rivalry between the Browns and Ravens was more directed at Art Modell than the team itself, and is simply considered a divisional game in Baltimore. Unlike the other two rivalries, this one is more lopsided: the Ravens lead 25-9. Additionally, this matchup is more bitter for Cleveland than the others due to the fact that the draft picks for 1995 to 1998 resulted in the rosters that won Super Bowl for the Ravens in 2000. Had the Browns stayed in Cleveland, these teams (drafted by GM Ozzie Newsome) might have given the Browns the title after a 35-year drought. This bitterness was compounded when the Ravens won their second world title in 2012.\n\nThe Lions rivalry began in the 1950s, when the Browns and Lions played each other in four NFL championships. The Lions won three of those championships, while the Browns won one. This was arguably one of the NFL's best rivalries in the 1950s. From 2002 to 2014, the two teams played an annual preseason game known as the \"Great Lakes Classic\".\n\nThe Bills rivalry had its roots back to the days of the AAFC, when there was a team from Buffalo with the same name in that league. The Browns and AAFC Bills played six games, including a league championship game, before the Browns were selected to merge into the NFL and the Bills left out. After the current incarnation of the Bills joined the NFL, the Browns and Bills have played each other from time to time. Though the Browns and Bills are in different AFC divisions, a mellow rivalry has since developed between the teams due to the similarities between Buffalo and Cleveland and shared misfortune between the teams. There have been some competitive moments between the Bills and Browns as well, such as a playoff game in 1990 and two games with playoff-implications in 2007 and 2014.\n\nThe Colts rivalry was hot in the 1960s. The Browns upset the Colts in the 1964 NFL championship (the last championship to date for the Browns) and the Colts got revenge by beating the Browns in the 1968 NFL Championship. The Browns also beat the Indianapolis Colts in a 1987 divisional playoff game.\n\nThe Browns had a brief rivalry with the Broncos that arose from three AFC championship games from 1986 to 1989. In the 1986 AFC Championship, quarterback John Elway led The Drive to secure a tie in the waning moments at Cleveland Municipal Stadium; the Broncos went on to win in 23–20 in overtime. One year later, the two teams met again in the 1987 AFC Championship game at Mile High Stadium. Denver took a 21–3 lead, but Browns' quarterback Bernie Kosar threw four touchdown passes to tie the game at 31–31 halfway through the 4th quarter. After a long drive, John Elway threw a 20-yard touchdown pass to running back Sammy Winder to give Denver a 38–31 lead. Cleveland advanced to Denver's 8-yard line with 1:12 left, but Broncos' safety Jeremiah Castille stripped Browns' running back Earnest Byner of the football at the 2-yard line—a play that has been called The Fumble by Browns' fans. The Broncos recovered it, gave Cleveland an intentional safety, and went on to win 38–33. The two teams met yet again in the 1989 AFC Championship at Mile High Stadium, which the Broncos easily won by a score of 37–21.\n\nA 2006 study conducted by \"Bizjournal\" determined that Browns fans are the most loyal fans in the NFL. The study, while not scientific, was largely based on fan loyalty during winning and losing seasons, attendance at games, and challenges confronting fans (such as inclement weather or long-term poor performance of their team). The study noted that Browns fans filled 99.8% of the seats at Cleveland Browns Stadium during the last seven seasons, despite a combined record of 36-76 over that span.\n\nPerhaps the most visible Browns fans are those that can be found in the Dawg Pound. Originally the name for the bleacher section located in the open (east) end of old Cleveland Municipal Stadium, the current incarnation is likewise located in the east end of FirstEnergy Stadium and still features hundreds of orange and brown clad fans sporting various canine-related paraphernalia. The fans adopted that name in 1984 after members of the Browns defense used it to describe the team's defense.\n\nRetired cornerback Hanford Dixon, who played his entire career for the Browns (1981–89), is credited with naming the Cleveland Browns defense 'The Dawgs' in the mid-1980s. Dixon and teammates Frank Minnifield and Eddie Johnson would bark at each other and to the fans in the bleachers at the Cleveland Stadium to fire them up. It was from Dixon's naming that the \"Dawg Pound\" subsequently took its title. The fans adopted that name in the years after. Due to this nickname, since the team's revival the Browns have used a bulldog as an alternate logo.\n\nThe most prominent organization of Browns fans is the \"Browns Backers Worldwide\" (BBW). The organization has approximately 305,000 members and Browns Backers clubs can be found in every major city in the United States, and in a number of military bases throughout the world, with the largest club being in Phoenix, Arizona. In addition, the organization has a sizable foreign presence in places as far away as Egypt, Australia, Japan, Sri Lanka, and McMurdo Station in Antarctica. According to The Official Fan Club of the Cleveland Browns, the two largest international fan clubs are in Alon Shvut, West Bank and Niagara, Canada, with Alon Shvut having 129 members and Niagara having 310.\n\nFollowing former Browns owner Randy Lerner's acquisition of English football club Aston Villa, official Villa outlets started selling Cleveland Browns goods such as jerseys and NFL footballs. This has raised interest in England and strengthened the link between the two sporting clubs. Aston Villa supporters have set up an organization known as the Aston (Villa) Browns Backers of Birmingham.\n\nThe Cleveland Browns were the favorite team of Elvis Presley. This was because his friend Gene Hickerson - with whom he had played football in their common youth in Memphis - was contracted by the Browns in 1957 and played there during his entire career until 1973. Also defender Bobby Franklin, who had played from 1960 to 1966 for the Browns, was a friend of Presley. WWE Hall of Fame wrestler and commentator Jerry \"The King\" Lawler - though he has spent most of his life in Memphis - spent part of his childhood in the Cleveland area and is a fan of the Browns. Fellow WWE wrestlers The Miz and Dolph Ziggler (both Cleveland natives) are also fans. Another fan of the team is Baseball-legend Hank Aaron. Other famous Browns fans include Arsenio Hall, Drew Carey, Patricia Heaton (her father, Chuck Heaton, was a sportswriter for \"The Plain Dealer\", which covered the Browns and wrote two books about the team), Terri Garr, Martin Mull, Condoleezza Rice, Valerie Bertinelli (her husband is from the Northeast Ohio area), Machine Gun Kelly, Paul Adelstein, Iron Chef Michael Symon, ESPN sportscaster Jay Crawford and Brad Paisley.\n\nThe Cleveland Browns have the fourth largest number of players enshrined in the Pro Football Hall of Fame with a total of 16 enshrined players elected based on their performance with the Browns, and eight more players or coaches elected who spent at least one year with the Browns franchise. No Browns players were inducted in the inaugural induction class of 1963. Otto Graham was the first Browns player to be enshrined as a member of the class of 1965, and the most recent Browns player to be included in the Pro Football Hall of Fame is Gene Hickerson, who was a member of the class of 2007.\nThe Cleveland Browns legends program honors former Browns who made noteworthy contributions to the history of the franchise. In addition to all the Hall of Famers listed above, the Legends list includes:\n\nFrom 1993 to 2013, number 19 was unofficially retired for Bernie Kosar aside from Frisman Jackson briefly wearing it in 2004, later changing it due to fan outcry over the number being used. In 2014, Miles Austin asked for and received permission from Kosar to wear 19, after which 19 returned to regular circulation for the Browns.\n\nBeginning in 2010, the Browns established a Ring of Honor, honoring the greats from the past by having their names displayed around the upper deck of FirstEnergy Stadium. The inaugural class in the Browns Ring of Honor was unveiled during the home opener on September 19, 2010, and featured the 16 Hall of Famers listed above who went into the Hall of Fame as Browns.\n\nThe team has honored two of its alumni with statues - late owner Alfred Lerner (in front of the team's headquarters/practice facility), and Hall of Fame running back Jim Brown (in front of First Energy Stadium).\n\nWKNR (850 AM), WKRK-FM (92.3 FM), and WNCX (98.5 FM) serve as co-flagship stations for the Browns Radio Network. Games are covered on-site by play-by-play announcer Jim Donovan (sports director/news anchor for WKYC channel 3), color commentator Doug Dieken (a former Browns Pro Bowl left tackle), and sideline reporter Nathan Zegura. WKRK-FM personalities Ken Carman and Dustin Fox, along with WKNR personalities Tony Rizzo, Matt Wilhelm, and Je'Rod Cherry host the network pregame show (heard in Cleveland on WKNR exclusively), while WKRK host Jeff Phelps and Fox host the network postgame show (heard in Cleveland on WKRK-FM exclusively).\n\nThe team also produces \"Cleveland Browns Daily\", a year-round daily radio show hosted by Zegura and Wilhelm which airs weekday afternoons on WKNR. A weekly version of the show airs Saturday mornings on WKRK-FM. WKNR and WKRK also simulcast a Wednesday night preview show with Carman and Cherry, and Hue Jackson's Thursday night coach's show.\n\nWEWS-TV serves as the broadcast TV home of the Browns, airing year-round team programming as well as non-network preseason games. Mike Patrick is the play-by-play announcer, while Solomon Wilcots serves as color commentator, and WEWS-TV sports director/WKRK-FM midday host Andy Baskin serves as pregame/halftime host and sideline reporter.\n\nSportsTime Ohio (STO) is the cable outlet for the team, airing various Browns related programming during the season, STO had previously served as the team's cable outlet from it's founding in 2006 until 2014. \n\nThe Browns in-house production team won a pair of Lower Great Lakes Emmy Awards in 2005. One was for a primetime special honoring the 1964 NFL Championship team (\"The 1964 Championship Show\") and one was for a commercial spot (\"The Paperboy\")\nThe Browns have (either directly or indirectly) been featured in various movies and TV shows over the years. Notable examples include:\n\n\n"}
{"id": "6579", "url": "https://en.wikipedia.org/wiki?curid=6579", "title": "Carbine", "text": "Carbine\n\nA carbine ( or ), from French \"carabine\", is a long arm firearm but with a shorter barrel than a rifle or musket. Many carbines are shortened versions of full-length rifles, shooting the same ammunition, while others fire lower-powered ammunition, including types designed for pistols.\n\nThe smaller size and lighter weight of carbines make them easier to handle. They are typically issued to high-mobility troops such as special-operations soldiers and paratroopers, as well as to mounted, artillery, logistics, or other non-infantry personnel whose roles do not require full-sized rifles, although there is a growing tendency for carbines to be issued to front-line soldiers to offset the increasing weight of other issued equipment. An example of this is the US Army's M4 carbine, which is standard-issue.\n\nSome sources derive the name of the weapon from its first users — cavalry troopers called \"carabiniers\", from the French \"carabine\", from Old French \"carabin\" (soldier armed with a musket), perhaps from \"escarrabin\", gravedigger, which derives from \"scarabee\", scarab beetle.\n\nThe carbine was originally a lighter, shortened weapon developed for the cavalry. Carbines were short enough to be loaded and fired from horseback, but this was rarely donea moving horse is a very unsteady platform, and once halted, a soldier can load and fire more easily if dismounted, which also makes him a smaller target (Napoleonic-era and earlier cavalry did fight from horseback, but they fought with sabers and large muzzle-loading \"horse pistols\", so called because their large size meant they were most easily carried in a saddle holster, much like the later Colt-Walker revolver). After the Napoleonic Wars, cavalry began fighting dismounted, using the horses only for greater mobility, an early form of what is today known as mechanized infantry. By the American Civil War, dismounted cavalry were mostly the rule. The principal advantage of the carbine was that its length made it very portable. Troops could carry full-length muskets comfortably enough on horseback if just riding from A to B (the practice of the original dragoons and other mounted infantry). Cavalry proper (a \"Regiment of Horse\") had to ride with some agility and engage in sword-wielding melées with opposing cavalry or pursue running infantry, so carrying anything long would be a dangerous encumbrance. A carbine was typically no longer than a sheathed sabre, and like a sheathed sabre was carried arranged to hang clear of the rider's elbows and horse's legs.\n\nCarbines were usually less accurate and less powerful than the longer muskets (and later rifles) of the infantry, due to a shorter sight plane and lower velocity of bullets fired from the shortened barrel. With the advent of fast-burning smokeless powder, the velocity disadvantages of a shorter barrel became less of an issue (see internal ballistics). Eventually, the use of horse-mounted cavalry would decline. But carbines continued to be issued and used by many who preferred a lighter, more compact weapon even at the cost of reduced long-range accuracy and power, such as artillery troops, who might need to defend themselves from attack but would be hindered by keeping full-sized rifles around; thus, a common title for many short rifles in the late 19th century was \"artillery carbine\".\n\nDuring the early 19th century, carbines were often developed separately from the infantry rifles and, in many cases, did not even use the same ammunition, which made for supply difficulties. A notable weapon developed towards the end of the American Civil War by the Union was the Spencer carbine, one of the very first breechloading, repeating weapons. It had a spring-powered, removable tube magazine in the buttstock which held seven rounds and could be reloaded by inserting spare tubes. It was intended to give the cavalry a replacement weapon which could be fired from horseback without the need for awkward reloading after each shot (although it saw service mostly with dismounted troopers and infantrymen, as was typical of cavalry weapons during that war). In the late 19th century, it became common for a number of nations to make bolt-action rifles in both full-length and carbine versions. One of the most popular and recognizable carbines were the lever-action Winchester carbines, with several versions available firing revolver cartridges. This made it an ideal choice for cowboys and explorers, as well as other inhabitants of the American West, who could carry a revolver and a carbine, both using the same ammunition.\n\nIn the decades following World War I, the standard battle rifle used by armies around the world had been growing shorter, either by redesign or by the general issue of carbine versions instead of full-length rifles. This move was initiated by the US Model 1903 Springfield, which was originally produced in 1907 with a short 24-inch barrel, providing a short rifle that was longer than a carbine but shorter than a typical rifle, so it could be issued to all troops without need for separate versions. Other nations followed suit after World War I, when they learned that their traditional long-barreled rifles provided little benefit in the trenches and merely proved a hindrance to the soldiers. Examples include the Russian Model 1891 rifle, originally with an barrel, later shortened to in 1930, and to , and in 1938, the German Mauser Gewehr 98 rifles went from in 1898 to in 1935 as the \"Karabiner 98k\" (K98k or Kar98k), or \"short carbine\". The barrel lengths in rifles used by the United States did not change between the bolt-action M1903 rifle of World War I and the World War II M1 Garand rifle, because the barrel on the M1903 was still shorter than even the shortened versions of the Model 1891 and Gewehr 98. The US M1 carbine was more of a traditional carbine in that it was significantly shorter and lighter, with a barrel, than the M1 Garand rifle, and that it was intended for rear-area troops who couldn't be hindered with full-sized rifles but needed something more powerful and accurate than a Model 1911 pistol (although this didn't stop soldiers from using them on the front line). Contrary to popular belief, and even what some books claim, in spite of both being designated \"M1\", the M1 Carbine was \"not\" a shorter version of the .30-06 M1 Garand, as is typical for most rifles and carbines, but a wholly different design firing a smaller, less-powerful cartridge. The \"M1\" designates each as the first model in the new US designation system, which no longer used the year of introduction, but a sequential series of numbers starting at \"1\": the M1 \"Carbine\" and M1 \"Rifle\".\n\nThe United Kingdom also developed a \"Jungle Carbine\" version of their Lee–Enfield service rifle, featuring a shorter barrel, flash suppressor, and manufacturing modifications designed to decrease the rifle's weight. Officially titled \"Rifle, No. 5 Mk I\", it was introduced in the closing months of World War II, but it did not see widespread service until the Korean War, the Mau Mau Uprising, and the Malayan Emergency.\n\nA shorter weapon was more convenient when riding in a truck, armored personnel carrier, helicopter, or aircraft, and also when engaged in close-range combat. Based on the combat experience of World War II, the criteria used for selecting infantry weapons began to change. Unlike previous wars, which were often fought mainly from fixed lines and trenches, World War II was a highly mobile war, often fought in cities, forests, or other areas where mobility and visibility were restricted. In addition, improvements in artillery made moving infantry in open areas even less practical than it had been.\n\nThe majority of enemy contacts were at ranges of less than , and the enemy was exposed to fire for only short periods of time as they moved from cover to cover. Most rounds fired were not aimed at an enemy combatant, but instead fired in the enemy's direction to keep them from moving and firing back (see suppressive fire). These situations did not require a heavy rifle, firing full-power rifle bullets with long-range accuracy. A less-powerful weapon would still produce casualties at the shorter ranges encountered in actual combat, and the reduced recoil would allow more shots to be fired in the short amount of time an enemy was visible. The lower-powered round would also weigh less, allowing a soldier to carry more ammunition. With no need of a long barrel to fire full-power ammunition, a shorter barrel could be used. A shorter barrel made the weapon weigh less, was easier to handle in tight spaces, and was easier to shoulder quickly to fire a shot at an unexpected target. Full-automatic fire was also considered a desirable feature, allowing the soldier to fire short bursts of three to five rounds, increasing the probability of a hit on a moving target.\n\nThe Germans had experimented with selective-fire carbines firing rifle cartridges during the early years of World War II. These were determined to be less than ideal, as the recoil of full-power rifle cartridges caused the weapon to be uncontrollable in full-automatic fire. They then developed an intermediate-power cartridge round, which was accomplished by reducing the power and the length of the standard 7.92×57mm Mauser rifle cartridge to create the 7.92×33mm \"Kurz\" (Short) cartridge. A selective-fire weapon was developed to fire this shorter cartridge, eventually resulting in the Sturmgewehr 44, later translated as \"assault rifle\" (also frequently called \"machine carbines\" by Allied intelligence, a quite accurate assessment, in fact). Very shortly after World War II, the USSR would adopt a similar weapon, the ubiquitous AK-47, the first model in the famed Kalashnikov-series, which became the standard Soviet infantry weapon, and which has been produced and exported in extremely large numbers up until the present day. Although the United States had developed the M2 Carbine, a selective-fire version of the M1 Carbine during WW2, the .30 Carbine cartridge was closer to a pistol round in power, making it more of a submachine gun than an assault rifle. It was also adopted only in very small numbers and issued to few troops (the semi-automatic M1 carbine was produced in a 10-to-1 ratio to the M2), while the AK47 was produced by the millions and was standard-issue to all Soviet troops, as well as those of many other nations. The US was slow to follow suit, insisting on retaining a full-power, 7.62×51mm NATO rifle, the M14 (although this \"was\" selective fire), until too-hastily adopting the 5.56mm M16 rifle in the mid-1960s, with initially poor results due to the rapidity of its introduction (but later to become a highly successful line of rifles and carbines).\n\nIn the 1950s, the British developed the .280 British, an intermediate cartridge, and a select-fire bullpup assault rifle to fire it, the EM-2. They pressed for the US to adopt it so it could become a NATO-standard round, but the US insisted on retaining a full-power, .30 caliber round. This forced NATO to adopt the 7.62×51mm NATO round (which in reality is only slightly different ballistically to the .30-06 Springfield), to maintain commonality. The British eventually adopted the 7.62mm FN FAL, and the US adopted the 7.62mm M14. These rifles are both what is known as \"battle rifles\" and were a few inches shorter than the standard-issue rifles they replaced (22\" barrel as opposed to 24\" for the M1 Garand), although they were still full-powered rifles, with selective fire capability. These can be compared to the even shorter, less-powerful assault rifle, which might be considered the \"carbine branch of weapons development\", although indeed, there are now carbine variants of many of the assault rifles which had themselves seemed quite small and light when adopted.\nBy the 1960s, after becoming involved in War in Vietnam, the US did an abrupt about-face and decided to standardize on the intermediate 5.56×45mm round (based on the .223 Remington varmint cartridge) fired from the new, lightweight M16 rifle, leaving NATO to hurry and catch up. Many of the NATO countries couldn't afford to re-equip so soon after the recent 7.62mm standardization, leaving them armed with full-power 7.62mm battle rifles for some decades afterwards, although by this point, the 5.56mm has been adopted by almost all NATO countries and many non-NATO nations as well. This 5.56mm NATO round was even lighter and smaller than the Soviet 7.62×39mm AK-47 cartridge, but possessed higher velocity. In U.S. service, the M16 assault rifle replaced the M14 as the standard infantry weapon, although the M14 continued to be used by designated marksmen. Although at 20\", the barrel of the M16 was shorter than that of the M14, it was still designated a \"rifle\" rather than a \"carbine\", and it was still longer than the AK, which used a 16\" barrel. (It is interesting to note that the SKS – an interim, semi-automatic, weapon adopted a few years before the AK-47 was put into service – was designated a carbine, even though it's 20\" barrel was significantly longer than the AK series' 16.3\". This is because of the Kalashnikov's revolutionary nature, which altered the old paradigm. Compared to previous rifles, particularly the Soviets' initial attempts at semi-automatic rifles, such as the 24\" SVT-40, the SKS was significantly shorter. The Kalashnikov altered traditional notions and ushered in a change in what was considered a \"rifle\" in military circles.)\n\nIn 1974, shortly after the introduction of the 5.56mm NATO, the USSR began to issue a new Kalashnikov variant, the AK-74, chambered in the small-bore 5.45×39mm cartridge, which was a standard 7.62×39mm necked down to take a smaller, lighter, faster bullet. It soon became standard issue in Soviet nations, although many of the nations with export Kalashnikovs retained the larger 7.62×39mm round. In 1995, the People's Republic of China adopted a new 5.8×42mm cartridge to match the modern trend in military ammunition, replacing the previous 7.62×39mm round as standard.\n\nLater, even lighter carbines variants of many of these short-barreled assault rifles came to be adopted as the standard infantry weapon. In much modern tactical thinking, only a certain number of soldiers now need to retain longer-range weapons, these serving as designated marksmen. The rest can carry lighter, shorter-ranged weapons for close-quarters combat and suppressive fire. This is basically a more extreme extension of the idea that brought the original assault rifle. Another factor is that with the increasing weight of technology, sighting systems, ballistic armor, etc., the only way to reduce the burden on the modern soldier was to equip him/her with a smaller, lighter weapon. Also, modern soldier rely a great deal on vehicles and helicopters to transport them around the battle area, and a longer weapon can be a serious hindrance to entering and exiting these vehicles. Development of lighter assault rifles continued, matched by developments in even lighter carbines. In spite of the short barrels of the new assault rifles, carbines variants like the 5.45×39mm AKS-74U and Colt Commando were being developed for use when mobility was essential and a submachine gun wasn't sufficiently powerful. The AKS-74U featured an extremely short 8.1\" barrel which necessitated redesigning and shortening the gas-piston and integrating front sights onto the gas tube; the Colt Commando was a bit longer, at 11.5\". Neither was adopted as standard issue, although the US did later adopt the somewhat-longer M4 carbine, with a 14.5\" barrel.\n\nBy the 1990s, the US had adopted the M4 carbine, a derivative of the M16 family which fired the same 5.56mm cartridge but was lighter and shorter (in overall length and barrel length), resulting in marginally reduced range and power, although offering better mobility and lighter weight to offset the weight of equipment and armor that a modern soldier has to carry.\n\nHowever, in spite of the benefits of the modern carbine, many armies are experiencing a certain backlash against the universal equipping of soldiers with carbines and lighter rifles in general, and are equipping selected soldiers, usually called Designated Marksmen, or DM, with higher-power rifles. Another problem comes from the loss of muzzle velocity caused by the shorter barrel, which when coupled with the typical small, lightweight bullets, causes effectiveness to be diminished; a 5.56mm gets its lethality from its high velocity, and when fired from the 14.5\" M4 carbine, its power, penetration, and range are diminished. Thus, there has been a move towards adopting a slightly more powerful round tailored for high performance from both long and short barrels. The US has done experiments regarding adopting a new, slightly larger and heavier caliber such as the 6.5mm Grendel or 6.8mm Remington SPC, which are heavier and thus retain more effectiveness at lower muzzle velocities, but has for the time decided to retain the 5.56mm NATO round as standard issue.\n\nWhile the US Army adopted the M4 carbine in the 1990s, the US Marine Corps retained their 20\" barrel M16A4 rifles long afterwards, citing the increased range and effectiveness over the carbine version; officers were required to carry an M4 carbine rather than an M9 pistol, as Army officers do. Due to the Marine Corps emphasis on being riflemen, the lighter carbine was considered a suitable compromise between a rifle and a pistol. Marines with restricted mobility such as vehicle operators, or a greater need for mobility such as squad leaders, were also issued M4 carbines. In July 2015, the Marine Corps approved the M4 carbine for standard issue to front-line Marines, replacing the M16A4 rifle. The rifles will be issued to support troops while the carbines go to the front-line Marines, in a reversal of the traditional roles of \"rifles for the front line, carbines for the rear\".\n\nSpecial forces need to perform fast, decisive operations, frequently airborne or boat-mounted. A pistol, though light and quick to operate, is viewed as not having enough power, firepower, or range. A submachine gun has selective fire, but firing a pistol cartridge and having a short barrel and sight radius, it is not accurate or powerful enough at longer ranges. Submachine guns also tend to have poorer armor and cover penetration than rifles and carbines firing rifle ammunition. Consequently, carbines have gained wide acceptance among SOCOM, UKSF, and other communities, having relatively light weight, large magazine capacity, selective fire, and much better range and penetration than a submachine gun.\n\nThe smaller size and relative lighter weight of carbines makes them easier to handle in close-quarter situations such as urban engagements, when deploying from military vehicles, or in any situation where space is confined. The disadvantages of carbines relative to rifles include inferior long-range accuracy and a shorter effective range (when referring to carbines of the same power and class as the rifle). Larger than a submachine gun, they are harder to maneuver in tight encounters where superior range and stopping power at distance are not great considerations. Firing the same ammunition as standard-issue rifles or pistols gives carbines the advantage of standardization over those personal defense weapons (PDWs) that require proprietary cartridges.\n\nThe modern usage of the term carbine covers much the same scope as it always had, namely lighter weapons (generally rifles) with barrels up to 20 inches in length. These weapons can be considered carbines, while rifles with barrels longer than 20 inches are generally not considered carbines unless specifically named so. Conversely, many rifles have barrels \"shorter\" than 20\", yet aren't considered carbines. The AK series rifles has an almost universal barrel length of 16.3\", well within carbine territory, yet has always been considered a rifle, perhaps because it was designed as such and not shortened from a longer weapon. Modern carbines use ammunition ranging from that used in light pistols up to powerful rifle cartridges, with the usual exception of high-velocity magnum cartridges. In the more powerful cartridges, the short barrel of a carbine has significant disadvantages in velocity, and the high residual pressure, and frequently still-burning powder and gases, when the bullet exits the barrel results in substantially greater muzzle blast. Flash suppressors are a common, partial solution to this problem, although even the best flash suppressors are hard put to deal with the excess flash from the still-burning powder leaving the short barrel (and they also add several inches to the length of the barrel, diminishing the purpose of having a short barrel in the first place). The shorter the barrel, the more difficult it is to hide the flash; the AKS-74U has a complex, effective muzzle-booster/flash suppressor, yet it still suffers from extreme muzzle flash.\n\nOne of the more atypical classes of carbine is the pistol caliber carbine or PCC. These first appeared soon after metallic cartridges became common. These were developed as \"companions\" to the popular revolvers of the day, firing the same cartridge but allowing more velocity and accuracy than the revolver. These were carried by cowboys, lawmen, and others in the Old West. The classic combination would be a Winchester lever-action carbine and a Colt Single Action Army revolver in .44-40 or .38-40. During the 20th century, this trend continued with more modern and powerful smokeless revolver cartridges, in the form of Winchester and Marlin lever action carbines chambered in .38 Special/.357 Magnum and .44 Special/.44 Magnum.\n\nModern equivalents, such as the discontinued Ruger Police Carbine, which uses the same magazine as the Ruger pistols of the same caliber, as well as the (also discontinued) Marlin Camp Carbine (which, in .45ACP, used M1911 magazines). The Ruger Model 44 and Ruger Deerfield Carbine were both carbines chambered in .44 Magnum. The Beretta Cx4 Storm shares magazines with many Beretta pistols and is designed to be complementary to the Beretta Px4 Storm pistol. The Hi-Point 995 Carbine is a popular, cheap and reliable alternative to other pistol caliber carbines in the United States, and its magazines can be used in the Hi-Point C-9 pistol. Another example is the Kel-Tec SUB-2000 series chambered in either 9 mm Luger or .40S&W, which can be configured to accept Glock, Beretta, S&W, or SIG pistol magazines. The SUB-2000 also has the somewhat unusual (although not unique) ability to fold in half.\nThe primary advantage of a carbine over a pistol using the same ammunition is controllability. The combination of firing from the shoulder, longer sight-radius, 3 points of contact (firing hand, support hand & shoulder), and precision offer a significantly more user-friendly platform. Carbines like the Kel-Tec SUB-2000 and the Beretta Cx4 Storm have the ability to mount user friendly optics, lights and lasers thanks to them having accessory rails, which make target acquisition and engagement much easier.\n\nThe longer barrel can offer increased velocity and, with it, greater energy and effective range due to the propellant having more time to burn. However, loss in bullet velocity can happen where the propellant is utilised before the bullet reaches the muzzle, combined with the friction from the barrel on the bullet. As long guns, pistol-caliber carbines may be less legally restricted than handguns in some jurisdictions. Compared to carbines chambered in intermediate or rifle calibers, such as .223 Remington and 7.62×54mmR, pistol-caliber carbines generally experience less of an increase in external ballistic properties as a result of the propellant. The drawback is that one loses the primary benefits of a handgun, i.e. portability and concealability, resulting in a weapon almost the size of, but less accurate than, a long-gun, but not much more powerful than a pistol.\n\nAlso widely produced are semi-automatic and typically longer-barreled derivatives of select-fire submachine guns, such as the FN PS90, HK USC, KRISS Vector, Thompson carbine, and the Uzi carbine. In order to be sold legally in many countries, the barrel must meet a minimum length (16\" in the USA). So the original submachine gun in given a legal-length barrel and made into a semi-automatic, transforming it into a carbine. Though less common, pistol-caliber conversions of centerfire rifles like the AR-15 are commercially available.\n\nSome handguns used to come from the factory with mounting lugs for a shoulder stock, notably including the \"Broomhandle\" Mauser C96, Luger P.08, and Browning Hi-Power. In the case of the first two, the pistol could come with a hollow wooden stock that doubled as a holster.\n\nCarbine conversion kits are commercially available for many other pistols, including M1911 and most Glocks. These can either be simple shoulder stocks fitted to a pistol or full carbine conversion kits, which are at least long and replace the pistol's barrel with one at least long for compliance with the US law. In the US, fitting a shoulder stock to a handgun with a barrel less than 16\" long legally turns it into a short-barreled rifle, which is in violation of the National Firearms Act.\n\nUnder the National Firearms Act of 1934, firearms with shoulder stocks or originally manufactured as a rifle and barrels less than in length are classified as short-barreled rifles. Short-barreled rifles are restricted similarly to short-barreled shotguns, requiring a $200 tax paid prior to manufacture or transfer – a process which can take several months. Because of this, firearms with barrels of less than and a shoulder stock are uncommon. A list of firearms not covered by the NFA due to their antique status may be found here or due to their \"Curio and Relic\" status may be found here; these lists includes a number of carbines with barrels less than the minimum legal length and firearms that are \"primarily collector's items and are not likely to be used as weapons and, therefore, are excluded from the provisions of the National Firearms Act.\" Machine guns, as their own class of firearm, are not subject to requirements of other class firearms.\n\nDistinct from simple shoulder stock kits, full carbine conversion kits are not classified as short-barreled rifles. By replacing the pistol barrel with one at least in length and having an overall length of at least , a carbine converted pistol may be treated as a standard rifle under Title I of the Gun Control Act of 1968 (GCA). However, certain \"Broomhandle\" Mauser C96, Luger, and Browning Hi-Power Curio & Relic pistols with their originally issued stock attached only may retain their pistol classification.\n\nCarbines without a stock and not originally manufactured as a rifle are not classified as rifles or short barreled rifles. A carbine manufactured under in length without a forward vertical grip will be a pistol and, state law notwithstanding, can be carried concealed without creating an unregistered Any Other Weapon. A nearly identical carbine with an overall length of or greater is simply an unclassified firearm under Title I of the Gun Control Act of 1968, as the Any Other Weapon catch-all only applies to firearms under or that have been concealed. However, a modification intending to fire from the shoulder and bypass the regulation of short-barreled rifles is considered the unlawful possession and manufacture of an unregistered short-barreled rifle.\n\nIn some historical cases, the term \"machine carbine\" was the official title for submachine guns, such as the British Sten and Australian Owen guns. The semiautomatic-only version of the Sterling submachine gun was also officially called a \"carbine\". The original Sterling semi-auto would be classed a \"short barrel rifle\" under the U.S. National Firearms Act, but fully legal long-barrel versions of the Sterling have been made for the U.S. collector market.\n\n\n"}

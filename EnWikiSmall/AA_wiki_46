{"id": "6251", "url": "https://en.wikipedia.org/wiki?curid=6251", "title": "Professional certification", "text": "Professional certification\n\nProfessional certification, trade certification, or professional designation, often called simply \"certification\" or \"qualification\", is a designation earned by a person to assure qualification to perform a job or task. Not all certifications that use post-nominal letters are an acknowledgement of educational achievement, or an agency appointed to safeguard the public interest.\n\nA certification is a third-party attestation of an individual's level of knowledge or proficiency in a certain industry or profession. They are granted by authorities in the field, such as professional societies and universities, or by private certificate-granting agencies. Most certifications are time-limited; some expire after a period of time (e.g., the lifetime of a product that required certification for use), while others can be renewed indefinitely as long as certain requirements are met. Renewal usually requires ongoing education to remain up-to-date on advancements in the field, evidenced by earning the specified number of continuing education credits (CECs), or continuing education units (CEUs), from approved professional development courses.\n\nMany certification programs are affiliated with professional associations, trade organizations, or private vendors interested in raising industry standards. Certificate programs are often created or endorsed by professional associations, but are typically completely independent from membership organizations. Certifications are very common in fields such as aviation, construction, technology, environment, and other industrial sectors, as well as healthcare, business, real estate, and finance.\nAccording to \"The Guide to National Professional Certification Programs\" (1997) by Phillip Barnhart, \"certifications are portable, since they do not depend on one company's definition of a certain job\" and they provide protential employers with \"an impartial, third-party endorsement of an individual's professional knowledge and experience\".\n\nCertification is different from professional licensure. In the United States, licenses are typically issued by state agencies, whereas certifications are usually awarded by professional societies or educational institutes. Obtaining a certificate is voluntary in some fields, but in others, certification from a government-accredited agency may be legally required to perform certain jobs or tasks. In other countries, licenses are typically granted by professional societies or universities and require a certificate after about three to five years and so on thereafter. The assessment process for certification may be more comprehensive than that of licensure, though sometimes the assessment process is very similar or even the same, despite differing in terms of legal status.\n\nThe American National Standards Institute (ANSI) defines the standard for being a certifying agency as meeting the following two requirements:\n\nThe Institute for Credentialing Excellence (ICE) is a U.S.-based organization that sets standards for the accreditation of personnel certification and certificate programs based on the \"Standards for Educational and Psychological Testing\", a joint publication of the American Educational Research Association (AERA), the American Psychological Association (APA), and the National Council on Measurement in Education (NCME). Many members of the Association of Test Publishers (ATP) are also certification organizations.\n\nThere are three general types of certification. Listed in order of development level and portability, they are: corporate (internal), product-specific, and profession-wide.\n\nCorporate, or \"internal\" certifications, are made by a corporation or low-stakes organization for internal purposes. For example, a corporation might require a one-day training course for all sales personnel, after which they receive a certificate. While this certificate has limited portability – to other corporations, for example – it is the most simple to develop.\n\nProduct-specific certifications are more involved, and are intended to be referenced to a product across all applications. This approach is very prevalent in the information technology (IT) industry, where personnel are certified on a version of software or hardware. This type of certification is portable across locations (for example, different corporations that use that software), but not across other products. Another example could be the certifications issued for shipping personnel, which are under international standards even for the recognition of the certification body, under the International Maritime Organization (IMO).\n\nThe most general type of certification is profession-wide. Certification in the medical profession is often offered by particular specialties. In order to apply professional standards, increase the level of practice, and protect the public, a professional organization might establish a certification. This is intended to be portable to all places a certified professional might work. Of course, this generalization increases the cost of such a program; the process to establish a legally defensible assessment of an entire profession is very extensive. An example of this is a Certified Public Accountant (CPA), which would not be certified for just one corporation or one piece of accountancy software but for general work in the profession.\n\nMany universities grant professional certificates as an award for the completion of an educational program. The curriculum of a professional certificate is most often in a focused subject matter. Many professional certificates have the same curriculum as master's degrees in the same subject. Many other professional certificates offer the same courses as master's degrees in the same subject, but require the student to take fewer total courses to complete the program. Some professional certificates have a curriculum that more closely resembles a baccalaureate major in the same field. The typical professional certificate program is between 200-300 class-hours in size. It is uncommon for a program to be larger or smaller than that. Most professional certificate programs are open enrollment, but some have admissions processes. A few universities put some of their professional certificates into a subclass they refer to as advanced professional certificates.\n\nSome of the more commonly offered professional certificates include:\n\n\"Advanced professional certificates\" are professional credentials designed to help professionals enhance their job performance and marketability in their respective fields. In many other countries, certificates are qualifications in higher education. In the United States, a certificate may be offered by an institute of higher education. These certificates usually signify that a student has reached a standard of knowledge of a certain vocational subject. Certificate programs can be completed more quickly than associate degrees and often do not have general education requirements.\n\nAn advanced professional certificate is a result of an educational process designed for individuals. Certificates are designed for both newcomers to the industry as well as seasoned professionals. Certificates are awarded by an educational program or academic institution. Completion of a certificate program indicates completion of a course or series of courses with a specific concentration that is different from an educational degree program. Course content for an advanced certificate is set forth through a variety of sources i.e. faculty, committee, instructors, and other subject matter experts in a related field. The end goal of an advanced professional certificate is so that professionals may demonstrate knowledge of course content at the end of a set period in time.\n\nInstitutions that offer advance professional certificates in various fields and industries include:\n\nThere are many professional bodies for accountants and auditors throughout the world; some of them are legally recognized in their jurisdictions.\nPublic accountants are the accountancy and control experts that are legally certified in different jurisdictions to work in public practices, certifying accounts as statutory auditors, eventually selling advice and services to other individuals and businesses. Today, however, many work within private corporations, financial industry, and government bodies.\n\nCf. Accountancy qualifications and regulation\n\n\n\n\n\n\n\nAviators are certified through theoretical and in-flight examinations. Requirements for certifications are quite equal in most countries and are regulated by each National Aviation Authority. The existing certificates or pilot licenses are:\n\nLicensing in these categories require not only examinations but also a minimum number of flight hours. All categories are available for Fixed-Wing Aircraft (airplanes) and Rotatory-Wing Aircraft (helicopters). Within each category, aviators may also obtain certifications in:\nUsually, aviators must be certified also in their log books for the type and model of aircraft they are allowed to fly. Currency checks as well as regular medical check-ups with a frequency of 6 months, 12 months, or 36 months, depending on the type of flying permitted, are obligatory. An aviator can fly only if holding:\n\nIn Europe, the ANSP, ATCO & ANSP technicians are certified according to ESARRs (according to EU regulation 2096/2005 \"Common Requirements\").\n\n\nIn the United States, several communications certifications are conferred by the Electronics Technicians Association.\n\nCertification is often used in the professions of software engineering and information technology.\n\n\nConferred by the International Dance Council CID at UNESCO, the International Certification of Dance Studies is awarded to students who have completed 150 hours of classes in a specific form of dance for Level 1. Another 150 hours are required for Level 2 and so on till Level 10. This is the only international certification for dance since the International Dance Council CID is the official body for all forms of dance; it is usually given in addition to local or national certificates, that is why it is colloquially called \"the dancer's passport\". Students cannot apply for this certification directly - they have to ask their school to apply on their behalf. This certification is awarded free of charge, there is no cost other than membership fees.\n\nInternational Dance Council CID at UNESCO administers the International Certification of Dance Studies.\n\nIn the United States, several electronics certifications are provided by the Electronics Technicians Association.\n\nThe Federal Emergency Management Agency's EMI offers credentials and training opportunities for United States citizens. Students do not have to be employed by FEMA or be federal employees for some of the programs.\n\nProfessional engineering is any act of planning, designing, composing, measuring, evaluating, inspecting, advising, reporting, directing or supervising, or managing any of the foregoing, that requires the application of engineering principles and that concerns the safeguarding of life, health, property, economic interests, the public interest or the environment.\n\nFacility management can be defined as an aspect of engineering management science that deals with the planning, designing, coordination of space and maintenance of a built environment to enhance quality service management system. Service Quality System includes activities like security, maintenance, catering, and external as well as internal cleaning. In general, it is also the coordination and harmonization of various specialist disciplines to create the best possible working environment for staff.\n\nFacility management is an interdisciplinary field devoted to the coordination of space, infrastructure, people and organization, often associated with the administration of office blocks, arenas, schools, convention centers, shopping complexes, hospitals, hotels, etc. However, FM facilitates on a wider range of activities than just business services and these are referred to as non-core functions.\n\nA warehouse management system (WMS) is a part of the supply chain and primarily aims to control the movement and storage of materials within a warehouse and process the associated transactions, including shipping, receiving, putaway and picking. The systems also direct and optimize stock putaway based on real-time information about the status of bin utilization. A WMS monitors the progress of products through the warehouse. It involves the physical warehouse infrastructure, tracking systems, and communication between product stations.\n\nMore precisely, warehouse management involves the receipt, storage and movement of goods, (normally finished goods), to intermediate storage locations or to a final customer. In the multi-echelon model for distribution, there may be multiple levels of warehouses. This includes a central warehouse, a regional warehouses (serviced by the central warehouse) and potentially retail warehouses (serviced by the regional warehouses).\n\n\n\n\nIECEx covers the highly specialized field of explosion protection associated with the use of equipment in areas where flammable gases, liquids and combustible dusts may be present. This system provides the assurance that equipment is manufactured to meet safety standards, and that services such as installation, repair and overhaul also comply with IEC International Standards on safety. The United Nations, via UNECE (United Nations Economic Commission for Europe), recommends the IEC and IECEx as the world’s best practice model for the verification of conformity to International Standards. It published a “Common Regulatory Framework” encompassing the use of IEC International Standards developed by IEC TC (Technical Committee) 31: Equipment for explosive atmospheres, with proof of compliance demonstrated by IECEx.\n\n\nAG (Accredited Genealogist) conferred by the International Commission for the Accreditation of Professional Genealogists (ICAPGen).\nCG (Certified Genealogist) conferred by the Board for Certification of Genealogists (BCG).\nCGL (Certified Genealogical Lecturer) conferred by the Board for Certification of Genealogists (BCG).\n\n\n\n\n\n\n\nIn the United States, insurance professionals are licensed separately by each state. Many individuals seek one or more certifications to distinguish themselves from their peers. The most recognizable certifications are issued by five organizations:\n\nNational Association of Mutual Insurance Companies\n\nNational Registry of Workers' Compensation Specialists\n\nProfessional Liability Underwriting Society (PLUS)\n\nTESOL is a large field of employment with widely varying degrees of regulation. Most provision worldwide is through the state school system of each individual country, and as such, the instructors tend to be trained primary- or secondary school teachers who are native speakers of the language of their pupils, and not of English. Though native speakers of English have been working in non-English speaking countries in this capacity for years, it was not until the last twenty-five years or so that there was any widespread focus on training particularly for this field. Previously, workers in this sort of job were anyone from backpackers hoping to earn some extra travel money to well-educated professionals in other fields doing volunteer work, or retired people. These sort of people are certainly still to be found, but there are many who consider TESOL their main profession.\n\nOne of the problems facing these full-time teachers is the absence of an international governing body for the certification or licensure of English language teachers. However, Cambridge University and its subsidiary body UCLES are pioneers in trying to get some degree of accountability and quality control to consumers of English courses, through their CELTA and DELTA programs. Trinity College London has equivalent programs, the CertTESOL and the LTCL DipTESOL. They offer initial certificates in teaching, in which candidates are trained in language awareness and classroom techniques, and given a chance to practice teaching, after which feedback is reported. Both institutions have as a follow-up a professional diploma, usually taken after a year or two in the field. Although the initial certificate is available to anyone with a high school education, the diploma is meant to be a post-graduate qualification and in fact can be incorporated into a master's degree program.\n\nAn increasing number of attorneys are choosing to be recognized as having special expertise in certain fields of law. According to the American Bar Association, a lawyer who is a certified specialist has been recognized by an independent professional certifying organization as having an enhanced level of skill and expertise, as well as substantial involvement in an established legal specialty. These organizations require a lawyer to demonstrate special training, experience and knowledge to ensure that the lawyer's recognition is meaningful and reliable. Lawyer conduct with regard to specialty certification is regulated by the states.\n\nNBLSC is an American Bar Association (ABA) accredited organization providing Board Certification for US Lawyers. Board Certification is a rigorous testing and approval process that officially recognizes the extensive education and courtroom experience of attorneys. NBLSC provides Board Certification for Trial Lawyers & Trial Attorneys, Civil Lawyers, Criminal Lawyers, Family Lawyers and Social Security Disability Lawyers.\n\nLogistician is the profession in the logistics and transport sectors, including sea, air, land and rail modes. Professional qualification for logisticians usually carries post-nominal letters. Common examples include:\n (CSCB),\n\n\n\nChurches have their own process of who may use various religious titles. Protestant churches typically require a Masters of Divinity, accreditation by the denomination and ordination by the local church in order for a minister to become a \"Reverend\". Those qualifications may or may not also give government authorization to solemnize marriages\n\nBoard certification is the process by which a physician in the United States documents by written, practical or computer based testing, illustrating a mastery of knowledge and skills that define a particular area of medical specialization. The American Board of Medical Specialties, a not-for-profit organization, assists 24 approved medical specialty boards in the development and use of standards in the ongoing evaluation and certification of physicians.\n\nMedical specialty certification in the United States is a voluntary process. While medical licensure sets the minimum competency requirements to diagnose and treat patients, it is not specialty specific. Board certification demonstrates a physician’s exceptional expertise in a particular specialty or sub-specialty of medical practice.\n\nPatients, physicians, health care providers, insurers and quality organizations regard certification as an important measure of a physician’s knowledge, experience and skills to provide quality health care within a given specialty.\n\nOther professional certifications include certifications such as medical licenses, Membership of the Royal College of Physicians, Fellowship of the Royal College of Physicians and Surgeons of Canada, nursing board certification, diplomas in social work. The Commission for Certification in Geriatric Pharmacy certifies pharmacists that are knowledgeable about principles of geriatric pharmacotherapy and the provision of pharmaceutical care to the elderly. Additional certifying bodies relating to the medical field include:\n\n\nNCPRP stands for “National Certified Peer Recovery Professional”, and the NCPRP credential and exam were developed in collaboration with the International Certification Board of Recovery Professionals (ICBRP) and is currently being administered by PARfessionals.\n\nPARfessionals is a professional organization and all of the available courses are professional development and pre-certification courses.\n\nThe NCPRP credential and exam focus primarily on the concept of peer recovery through mental health and addiction recovery. It has the main purpose of training student-candidates on how to become peer recovery professionals who can provide guidance, knowledge or assistance for individuals who have had similar experiences.\n\nThrough the support of the SJM Family Foundation, PARfessionals has developed the first globally recognized online training program for peer recovery professionals.\n\nEach student-candidate must complete several key steps which include initial registration; the pre-certification review course; and all applicable sections of the official application in order to become eligible to complete the final step, which is the NCPRP certification exam.\n\nThe NCPRP credential is obtained once a participant successfully passes the NCPRP certification exam by the second attempt and is valid for five years.\n\n\n\nCertification is of significant importance in the project management (PM) industry. Certification refers to the evaluation and recognition of the skills, knowledge and competence of a practitioner in the field.\n\nProject management certifications come in a variety of flavors:\n\nCombination of competence-based, knowledge-based, and experience-based\nKnowledge-based\n\nThere are 15 professional associations from around the world offering the 'Accredited in Public Relations' (APR) designation and one offering the 'Accredited Business Communicator' (ABC) designation. See Communications above for communications-related certifications (CMP and SCMP)\n\n\n\n\n\n\n\nCustomer relationship management (CRM) is a system for managing a company’s interactions with current and future customers. It often involves using technology to organize, automate and synchronize sales, marketing, customer service, and technical support.\n\n\nConferred by the National Speakers Association, the Certified Speaking Professional (CSP) is the speaking profession's international measure of professional platform competence. This certification is awarded by the National Speakers Association. Only about 10% of the speakers who belong to the Global Speakers Federation (GSF) hold this designation. Those who have earned their certification have done so by demonstrating a track record of experience and expertise.\n\nSupply chain management (SCM) is the management of the flow of goods. It includes the movement and storage of raw materials, work-in-process inventory, and finished goods from point of origin to point of consumption. Interconnected or interlinked networks, channels and node businesses are involved in the provision of products and services required by end customers in a supply chain.[2] Supply chain management has been defined as the \"design, planning, execution, control, and monitoring of supply chain activities with the objective of creating net value, building a competitive infrastructure, leveraging worldwide logistics, synchronizing supply with demand and measuring performance globally.\n\nSCM draws heavily from the areas of operations management, logistics, procurement, and information technology, and strives for an integrated approach.\n\nAustralian Institute of Certified Practising Trainers administers the Certified Practising Trainer (CPT).\nConferred by the Australian Institute of Certified Practising Trainers, this certification is the hallmark for professional trainers.\n\n\nMany political commentators, often criticize professional or occupational licensing, especially medical and legal licensing, for restricting the supply of services and therefore making them more expensive, often putting them out of reach of the poor.\n\nThe current proliferation of IT certifications (both offered and attained), like the FSI's IT baseline protection certification, has led some technologists to question their value. Proprietary content that has been distributed on the Internet allows some to gain credentials without the implied depth or breadth of expertise. Certifying agencies have responded in various ways: Some now incorporate hands-on elements, anti-cheating methodologies or have expanded their content. Others have expired and restructured their certificate programs or raised their fees to deter abuse.\n\nCertification programs that take into account length of service and demonstrated experience via industry peer and employer recommendation avoid some of the issues associated with purely passing an examination; however, certification remains a contentious issue.\n\nAlso, some professional certifications require a criminal record check before the certification can be approved. The presence of a criminal history when applying for certification may be grounds for denial of certification.\n\n\n"}
{"id": "6255", "url": "https://en.wikipedia.org/wiki?curid=6255", "title": "Carl Menger", "text": "Carl Menger\n\nCarl Menger (; February 23, 1840 – February 26, 1921) was an Austrian economist and the founder of the Austrian School of economics. Menger contributed to the development of the theory of marginalism, (marginal utility), which rejected the cost-of-production theories of value, such as were developed by the classical economists such as Adam Smith and David Ricardo. Menger used his “Subjective Theory of Value” to arrive at what he considered one of the most powerful insights in economics: both sides gain from exchange.\n\nMenger was born in the city of Neu-Sandez in Austrian Galicia, which is now Nowy Sącz in Poland. He was the son of a wealthy family of minor nobility; his father, Anton, was a lawyer. His mother, Caroline, was the daughter of a wealthy Bohemian merchant. He had two brothers, Anton and Max, both prominent as lawyers. His son, Karl Menger, was a mathematician who taught for many years at Illinois Institute of Technology.\n\nAfter attending \"Gymnasium\" he studied law at the Universities of Prague and Vienna and later received a doctorate in jurisprudence from the Jagiellonian University in Kraków. In the 1860s Menger left school and enjoyed a stint as a journalist reporting and analyzing market news, first at the \"Lemberger Zeitung\" in Lwów, Ukraine and later at the \"Wiener Zeitung\" in Vienna.\n\nDuring the course of his newspaper work he noticed a discrepancy between what the classical economics he was taught in school said about price determination and what real world market participants believed. In 1867 Menger began a study of political economy which culminated in 1871 with the publication of his \"Principles of Economics\" \"(Grundsätze der Volkswirtschaftslehre),\" thus becoming the father of the Austrian School of economic thought. It was in this work that he challenged classical cost-based theories of value with his theory of marginality – that price is determined at the margin.\n\nIn 1872 Menger was enrolled into the law faculty at the University of Vienna and spent the next several years teaching finance and political economy both in seminars and lectures to a growing number of students. In 1873 he received the university's chair of economic theory at the very young age of 33.\n\nIn 1876 Menger began tutoring Archduke Rudolf von Habsburg, the Crown Prince of Austria in political economy and statistics. For two years Menger accompanied the prince in his travels, first through continental Europe and then later through the British Isles. He is also thought to have assisted the crown prince in the composition of a pamphlet, published anonymously in 1878, which was highly critical of the higher Austrian aristocracy. His association with the prince would last until Rudolf's suicide in 1889 (see the Mayerling Affair).\n\nIn 1878 Rudolf's father, Emperor Franz Josef, appointed Menger to the chair of political economy at Vienna. The title of \"Hofrat\" was conferred on him, and he was appointed to the Austrian \"Herrenhaus\" in 1900.\n\nEnsconced in his professorship he set about refining and defending the positions he took and methods he utilized in \"Principles,\" the result of which was the 1883 publication of \"Investigations into the Method of the Social Sciences with Special Reference to Economics (Untersuchungen über die Methode der Socialwissenschaften und der politischen Oekonomie insbesondere).\" The book caused a firestorm of debate, during which members of the Historical school of economics began to derisively call Menger and his students the \"Austrian School\" to emphasize their departure from mainstream economic thought in Germany – the term was specifically used in an unfavorable review by Gustav von Schmoller.\n\nIn 1884 Menger responded with the pamphlet \"The Errors of Historicism in German Economics\" and launched the infamous \"Methodenstreit,\" or methodological debate, between the Historical School and the Austrian School. During this time Menger began to attract like-minded disciples who would go on to make their own mark on the field of economics, most notably Eugen von Böhm-Bawerk, and Friedrich von Wieser.\n\nIn the late 1880s Menger was appointed to head a commission to reform the Austrian monetary system. Over the course of the next decade he authored a plethora of articles which would revolutionize monetary theory, including \"The Theory of Capital\" (1888) and \"Money\" (1892). Largely due to his pessimism about the state of German scholarship, Menger resigned his professorship in 1903 to concentrate on study.\n\nMenger used his “Subjective Theory of Value” to arrive at what he considered one of the most powerful insights in economics: both sides gain from exchange.\nUnlike William Jevons, Menger did not believe that goods provide “utils,” or units of utility. Rather, he wrote, goods are valuable because they serve various uses whose importance differs. Menger also came up with an explanation of how money develops that is still accepted by some schools of thought today.\n\n\n\n\n"}
{"id": "6256", "url": "https://en.wikipedia.org/wiki?curid=6256", "title": "List of cartoonists", "text": "List of cartoonists\n\nThis is a list of cartoonists, visual artists who specialize in drawing cartoons. This list includes only notable cartoonists and is not meant to be exhaustive.\n\n"}
{"id": "6258", "url": "https://en.wikipedia.org/wiki?curid=6258", "title": "Civilization", "text": "Civilization\n\nA civilization (UK and US) or civilisation (British English variant) is any complex society characterized by urban development, social stratification, symbolic communication form (typically, writing systems) and a perceived separation from and domination over the natural environment, and social domination by cultural elite.\n\nCivilizations are intimately associated with and often further defined by other socio-politico-economic characteristics, including centralization, the domestication of both humans and other organisms, specialization of labour, culturally ingrained ideologies of progress and supremacism, monumental architecture, taxation, societal dependence upon farming and expansionism. Historically, a civilization was a so-called \"advanced\" culture in contrast to more supposedly primitive cultures. In this broad sense, a civilization contrasts with non-centralized tribal societies, including the cultures of nomadic pastoralists, egalitarian horticultural subsistence neolithic societies or hunter-gatherers. As an uncountable noun, civilization also refers to the process of a society developing into a centralized, urbanized, stratified structure. Civilizations are organized in densely populated settlements divided into hierarchical social classes with a ruling elite and subordinate urban and rural populations, which engage in intensive agriculture, mining, small-scale manufacture and trade. Civilization concentrates power, extending human control over the rest of nature, including over other human beings.\n\nThe earliest emergence of civilizations is generally associated with the final stages of the Neolithic Revolution, culminating in the relatively rapid process of urban revolution and state formation, a political development associated with the appearance of a governing elite. The earlier neolithic technology and lifestyle was established first in the Middle East (for example at Göbekli Tepe, from about 9,130 BCE), and later in the Yellow River and Yangtze basins in China (for example the Pengtoushan culture from 7,500 BCE), and later spread. Similar pre-civilized \"neolithic revolutions\" also began independently from 7,000 BCE in such places as northwestern South America (the Norte Chico civilization) and Mesoamerica. These were among the six civilizations worldwide that arose independently. Mesopotamia is the site of the earliest developments of the Neolithic Revolution from around 10,000 BCE, with civilizations developing from 6,500 years ago. This area has been identified as having \"inspired some of the most important developments in human history including the invention of the wheel, the development of cursive script, mathematics, astronomy and agriculture.\"\n\nThe civilized urban revolution in turn was dependent upon the development of sedentarism, the domestication of grains and animals and development of lifestyles that facilitated economies of scale and accumulation of surplus production by certain social sectors. The transition from \"complex cultures\" to \"civilizations\", while still disputed, seems to be associated with the development of state structures, in which power was further monopolized by an elite ruling class who practised human sacrifice. Towards the end of the Neolithic period, various elitist Chalcolithic civilizations began to rise in various \"cradles\" from around 3300 BCE. Chalcolithic civilizations, as defined above, also developed in Pre-Columbian Americas and, despite an early start in Egypt, Axum and Kush, much later in Iron Age sub-Saharan Africa. The Bronze Age collapse was followed by the Iron Age around 1200 BCE, during which a number of new civilizations emerged, culminating in a period from the 8th to the 3rd century BCE which German psychiatrist and philosopher Karl Jaspers termed the Axial Age, and which he claimed was a critical transitional phase leading to Classical civilization. A major technological and cultural transition to modernity began approximately 1500 CE in Western Europe, and from this beginning new approaches to science and law spread rapidly around the world, incorporating earlier cultures into the industrial and technological civilization of the present.\n\nAnthropologist Layla AbdelRahim defines civilization as the material manifestation of \"the sum of domesticated relationships with everything material and symbolic that issues from the labour and consumption of those categorized as resources and the (necessarily) unequal value for that labour, victimhood, and lives\". This economic and socio-environmental paradigm is unsustainable, says AbdelRahim, because it is constantly growing and demanding the colonization of new territories and human and nonhuman lives.\n\nThe English word \"civilization\" comes from the 16th-century French \"civilisé\" (\"civilized\"), from Latin \"civilis\" (\"civil\"), related to \"civis\" (\"citizen\") and \"civitas\" (\"city\"). The fundamental treatise is Norbert Elias's \"The Civilizing Process\" (1939), which traces social mores from medieval courtly society to the Early Modern period. In \"The Philosophy of Civilization\" (1923), Albert Schweitzer outlines two opinions: one purely material and the other material and ethical. He said that the world crisis was from humanity losing the ethical idea of civilization, \"the sum total of all progress made by man in every sphere of action and from every point of view in so far as the progress helps towards the spiritual perfecting of individuals as the progress of all progress\".\n\nAdjectives like \"civility\" developed in the mid-16th century. The abstract noun \"civilization\", meaning \"civilized condition\", came in the 1760s, again from French. The first known use in French is in 1757, by Victor Riqueti, marquis de Mirabeau, and the first use in English is attributed to Adam Ferguson, who in his 1767 \"Essay on the History of Civil Society\" wrote, \"Not only the individual advances from infancy to manhood, but the species itself from rudeness to civilisation\". The word was therefore opposed to barbarism or rudeness, in the active pursuit of progress characteristic of the Age of Enlightenment.\n\nIn the late 1700s and early 1800s, during the French Revolution, \"civilization\" was used in the singular, never in the plural, and meant the progress of humanity as a whole. This is still the case in French. The use of \"civilizations\" as a countable noun was in occasional use in the 19th century, but has become much more common in the later 20th century, sometimes just meaning culture (itself in origin an uncountable noun, made countable in the context of ethnography). Only in this generalized sense does it become possible to speak of a \"medieval civilization\", which in Elias's sense would have been an oxymoron.\n\nAlready in the 18th century, civilization was not always seen as an improvement. One historically important distinction between culture and civilization is from the writings of Rousseau, particularly his work about education, \"\". Here, civilization, being more rational and socially driven, is not fully in accord with human nature, and \"human wholeness is achievable only through the recovery of or approximation to an original prediscursive or prerational natural unity\" (see noble savage). From this, a new approach was developed, especially in Germany, first by Johann Gottfried Herder, and later by philosophers such as Kierkegaard and Nietzsche. This sees cultures as natural organisms, not defined by \"conscious, rational, deliberative acts\", but a kind of pre-rational \"folk spirit\". Civilization, in contrast, though more rational and more successful in material progress, is unnatural and leads to \"vices of social life\" such as guile, hypocrisy, envy and avarice. In World War II, Leo Strauss, having fled Germany, argued in New York that this opinion of civilization was behind Nazism and German militarism and nihilism.\n\nSocial scientists such as V. Gordon Childe have named a number of traits that distinguish a civilization from other kinds of society. Civilizations have been distinguished by their means of subsistence, types of livelihood, settlement patterns, forms of government, social stratification, economic systems, literacy and other cultural traits. Andrew Nikiforuk argues that \"civilizations relied on shackled human muscle. It took the energy of slaves to plant crops, clothe emperors, and build cities\" and considers slavery to be a common feature of pre-modern civilizations.\n\nAll civilizations have depended on agriculture for subsistence. Grain farms can result in accumulated storage and a surplus of food, particularly when people use intensive agricultural techniques such as artificial fertilization, irrigation and crop rotation. It is possible but more difficult to accumulate horticultural production, and so civilizations based on horticultural gardening have been very rare. Grain surpluses have been especially important because they can be stored for a long time. A surplus of food permits some people to do things besides produce food for a living: early civilizations included soldiers, artisans, priests and priestesses, and other people with specialized careers. A surplus of food results in a division of labour and a more diverse range of human activity, a defining trait of civilizations. However, in some places hunter-gatherers have had access to food surpluses, such as among some of the indigenous peoples of the Pacific Northwest and perhaps during the Mesolithic Natufian culture. It is possible that food surpluses and relatively large scale social organization and division of labour predates plant and animal domestication.\n\nCivilizations have distinctly different settlement patterns from other societies. The word \"civilization\" is sometimes simply defined as \"'living in cities'\". Non-farmers tend to gather in cities to work and to trade.\n\nCompared with other societies, civilizations have a more complex political structure, namely the state. State societies are more stratified than other societies; there is a greater difference among the social classes. The ruling class, normally concentrated in the cities, has control over much of the surplus and exercises its will through the actions of a government or bureaucracy. Morton Fried, a conflict theorist and Elman Service, an integration theorist, have classified human cultures based on political systems and social inequality. This system of classification contains four categories\n\nEconomically, civilizations display more complex patterns of ownership and exchange than less organized societies. Living in one place allows people to accumulate more personal possessions than nomadic people. Some people also acquire landed property, or private ownership of the land. Because a percentage of people in civilizations do not grow their own food, they must trade their goods and services for food in a market system, or receive food through the levy of tribute, redistributive taxation, tariffs or tithes from the food producing segment of the population. Early human cultures functioned through a gift economy supplemented by limited barter systems. By the early Iron Age, contemporary civilizations developed money as a medium of exchange for increasingly complex transactions. In a village, the potter makes a pot for the brewer and the brewer compensates the potter by giving him a certain amount of beer. In a city, the potter may need a new roof, the roofer may need new shoes, the cobbler may need new horseshoes, the blacksmith may need a new coat and the tanner may need a new pot. These people may not be personally acquainted with one another and their needs may not occur all at the same time. A monetary system is a way of organizing these obligations to ensure that they are fulfilled. From the days of the earliest monetarized civilizations, monopolistic controls of monetary systems have benefited the social and political elites.\n\nWriting, developed first by people in Sumer, is considered a hallmark of civilization and \"appears to accompany the rise of complex administrative bureaucracies or the conquest state\". Traders and bureaucrats relied on writing to keep accurate records. Like money, writing was necessitated by the size of the population of a city and the complexity of its commerce among people who are not all personally acquainted with each other. However, writing is not always necessary for civilization, as shown the Inca civilization of the Andes, which did not use writing at all except from a complex recording system consisting of cords and nodes instead: the \"Quipus\", whose still functioned as a civilized society.\n\nAided by their division of labour and central government planning, civilizations have developed many other diverse cultural traits. These include organized religion, development in the arts, and countless new advances in science and technology.\n\nThrough history, successful civilizations have spread, taking over more and more territory, and assimilating more and more previously-uncivilized people. Nevertheless, some tribes or people remain uncivilized even to this day. These cultures are called by some \"primitive\", a term that is regarded by others as pejorative. \"Primitive\" implies in some way that a culture is \"first\" (Latin = \"primus\"), that it has not changed since the dawn of humanity, though this has been demonstrated not to be true. Specifically, as all of today's cultures are contemporaries, today's so-called primitive cultures are in no way antecedent to those we consider civilized. Anthropologists today use the term \"non-literate\" to describe these peoples.\n\nCivilization has been spread by colonization, invasion, religious conversion, the extension of bureaucratic control and trade, and by introducing agriculture and writing to non-literate peoples. Some non-civilized people may willingly adapt to civilized behaviour. But civilization is also spread by the technical, material and social dominance that civilization engenders.\n\nAssessments of what level of civilization a polity has reached are based on comparisons of the relative importance of agricultural as opposed to trade or manufacturing capacities, the territorial extensions of its power, the complexity of its division of labour, and the carrying capacity of its urban centres. Secondary elements include a developed transportation system, writing, standardized measurement, currency, contractual and tort-based legal systems, art, architecture, mathematics, scientific understanding, metallurgy, political structures and organized religion.\n\nTraditionally, polities that managed to achieve notable military, ideological and economic power defined themselves as \"civilized\" as opposed to other societies or human groupings outside their sphere of influence—calling the latter barbarians, savages, and primitives. In a modern-day context, \"civilized people\" have been contrasted with indigenous people or tribal societies.\n\n\"Civilization\" can also refer to the culture of a complex society, not just the society itself. Every society, civilization or not, has a specific set of ideas and customs, and a certain set of manufactures and arts that make it unique. Civilizations tend to develop intricate cultures, including a state-based decision making apparatus, a literature, professional art, architecture, organized religion and complex customs of education, coercion and control associated with maintaining the elite.\n\nThe intricate culture associated with civilization has a tendency to spread to and influence other cultures, sometimes assimilating them into the civilization (a classic example being Chinese civilization and its influence on nearby civilizations such as Korea, Japan and Vietnam). Many civilizations are actually large cultural spheres containing many nations and regions. The civilization in which someone lives is that person's broadest cultural identity.\n\nMany historians have focused on these broad cultural spheres and have treated civilizations as discrete units. Early twentieth-century philosopher Oswald Spengler, uses the German word \"Kultur\", \"culture\", for what many call a \"civilization\". Spengler believes a civilization's coherence is based on a single primary cultural symbol. Cultures experience cycles of birth, life, decline and death, often supplanted by a potent new culture, formed around a compelling new cultural symbol. Spengler states civilization is the beginning of the decline of a culture as \"the most external and artificial states of which a species of developed humanity is capable\".\n\nThis \"unified culture\" concept of civilization also influenced the theories of historian Arnold J. Toynbee in the mid-twentieth century. Toynbee explored civilization processes in his multi-volume \"A Study of History,\" which traced the rise and, in most cases, the decline of 21 civilizations and five \"arrested civilizations\". Civilizations generally declined and fell, according to Toynbee, because of the failure of a \"creative minority\", through moral or religious decline, to meet some important challenge, rather than mere economic or environmental causes.\n\nSamuel P. Huntington defines civilization as \"the highest cultural grouping of people and the broadest level of cultural identity people have short of that which distinguishes humans from other species\". Huntington's theories about civilizations are discussed below.\n\nAnother group of theorists, making use of systems theory, looks at a civilization as a complex system, i.e., a framework by which a group of objects can be analysed that work in concert to produce some result. Civilizations can be seen as networks of cities that emerge from pre-urban cultures and are defined by the economic, political, military, diplomatic, social and cultural interactions among them. Any organization is a complex social system and a civilization is a large organization. Systems theory helps guard against superficial but misleading analogies in the study and description of civilizations.\n\nSystems theorists look at many types of relations between cities, including economic relations, cultural exchanges and political/diplomatic/military relations. These spheres often occur on different scales. For example, trade networks were, until the nineteenth century, much larger than either cultural spheres or political spheres. Extensive trade routes, including the Silk Road through Central Asia and Indian Ocean sea routes linking the Roman Empire, Persian Empire, India and China, were well established 2000 years ago, when these civilizations scarcely shared any political, diplomatic, military, or cultural relations. The first evidence of such long distance trade is in the ancient world. During the Uruk period, Guillermo Algaze has argued that trade relations connected Egypt, Mesopotamia, Iran and Afghanistan. Resin found later in the Royal Cemetery at Ur is suggested was traded northwards from Mozambique.\n\nMany theorists argue that the entire world has already become integrated into a single \"world system\", a process known as globalization. Different civilizations and societies all over the globe are economically, politically, and even culturally interdependent in many ways. There is debate over when this integration began, and what sort of integration – cultural, technological, economic, political, or military-diplomatic – is the key indicator in determining the extent of a civilization. David Wilkinson has proposed that economic and military-diplomatic integration of the Mesopotamian and Egyptian civilizations resulted in the creation of what he calls the \"Central Civilization\" around 1500 BCE. Central Civilization later expanded to include the entire Middle East and Europe, and then expanded to a global scale with European colonization, integrating the Americas, Australia, China and Japan by the nineteenth century. According to Wilkinson, civilizations can be culturally heterogeneous, like the Central Civilization, or homogeneous, like the Japanese civilization. What Huntington calls the \"clash of civilizations\" might be characterized by Wilkinson as a clash of cultural spheres within a single global civilization. Others point to the Crusades as the first step in globalization. The more conventional viewpoint is that networks of societies have expanded and shrunk since ancient times, and that the current globalized economy and culture is a product of recent European colonialism. \n\nHistorically civilizations were assumed by writers such as Aristotle to be the natural state of humanity, so no origin for the Greek polis was considered to be needed. The Sumerian King List for instance, sees the origin of their civilization as descending from heaven. However the great age of maritime discovery exposed the states of Western Europe to hunter-gatherer and simple horticultural cultures that were not civilized. To explain the differences observed, early theorists turned to racist theories of cultural superiority, theories of geographic determinism, or accidents of culture. After the Second World War, these theories were rejected on various grounds and other explanations sought. Four schools have developed in the modern period.\n\nThe process of sedentarization is first thought to have occurred around 12,000 BCE in the Levant region of southwest Asia though other regions around the world soon followed. The emergence of civilization is generally associated with the Neolithic, or Agricultural Revolution, which occurred in various locations between 8,000 and 5,000 BCE, specifically in southwestern/southern Asia, northern/central Africa and Central America. At first, the Neolithic was associated with shifting subsistence cultivation, where continuous farming led to the depletion of soil fertility resulting in the requirement to cultivate fields further and further removed from the settlement, eventually compelling the settlement itself to move. In major semi-arid river valleys, annual flooding renewed soil fertility every year, with the result that population densities could rise significantly. This encouraged a secondary products revolution in which people used domesticated animals not just for meat, but also for milk, wool, manure and pulling ploughs and carts—a development that spread through the Eurasian Oecumene. The 8.2 Kiloyear Arid Event and the 5.9 Kiloyear Interpluvial saw the drying out of semiarid regions and a major spread of deserts. This climate change shifted the cost-benefit ratio of endemic violence between communities, which saw the abandonment of unwalled village communities and the appearance of walled cities, associated with the first civilizations. This \"urban revolution\" marked the beginning of the accumulation of transferrable surpluses, which helped economies and cities develop. It was associated with the state monopoly of violence, the appearance of a soldier class and endemic warfare, rapid development of hierarchies, the appearance of human sacrifice and a fall in the status of women.\n\n\n\nThe Iron Age is the period generally occurring after the Bronze Age, marked by the prevalent use of iron. The early period of the age is characterized by the widespread use of iron or steel. The adoption of such material coincided with other changes in society, including differing agricultural practices, religious beliefs and artistic styles. The Iron Age as an archaeological term indicates the condition as to civilization and culture of a people using iron as the material for their cutting tools and weapons. The Iron Age is the third principal period of the three-age system created by Christian Thomsen (1788–1865) for classifying ancient societies and prehistoric stages of progress.\n\nKarl Jaspers, the German historical philosopher, proposed that the ancient civilizations were affected greatly by an Axial Age in the period between 800 BCE–200 BCE during which a series of male sages, prophets, religious reformers and philosophers, from China, India, Iran, Israel and Greece, changed the direction of civilizations. William Hardy McNeill proposed that this period of history was one in which culture contact between previously separate civilizations saw the \"closure of the oecumene\" and led to accelerated social change from China to the Mediterranean, associated with the spread of coinage, larger empires and new religions. This view has recently been championed by Christopher Chase-Dunn and other world systems theorists.\n\nThe spread of the Higher Religions, beginning with Zoroastrianism, Confucianism and Buddhism was linked to the developments of the Axial Age. Principal amongst this was the creation of large militaristic territorial states, which saw an increase in the state as a powerful unit monopolizing the use of violence. It was also linked with the spread of coinage and monetary economies, which had the effect of dissolving the previous local community traditions. The rise of the confessional religious brought the ability to unify larger states than had existed previously.\n\n\n\nCivilizations have generally ended in one of two ways; either through being incorporated into another expanding civilization (e.g. As Ancient Egypt was incorporated into Hellenistic Greek, and subsequently Roman civilizations), or by collapse and reversion to a simpler form, as happens in what are called Dark Ages.\n\nThere have been many explanations put forward for the collapse of civilization. Some focus on historical examples, and others on general theory.\n\nPolitical scientist Samuel Huntington has argued that the defining characteristic of the 21st century will be a clash of civilizations. According to Huntington, conflicts between civilizations will supplant the conflicts between nation-states and ideologies that characterized the 19th and 20th centuries. These views have been strongly challenged by others like Edward Said, Muhammed Asadi and Amartya Sen. Ronald Inglehart and Pippa Norris have argued that the \"true clash of civilizations\" between the Muslim world and the West is caused by the Muslim rejection of the West's more liberal sexual values, rather than a difference in political ideology, although they note that this lack of tolerance is likely to lead to an eventual rejection of (true) democracy. In \"Identity and Violence\" Sen questions if people should be divided along the lines of a supposed \"civilization\", defined by religion and culture only. He argues that this ignores the many others identities that make up people and leads to a focus on differences.\n\nCultural Historian Morris Berman suggests in \"Dark Ages America: the End of Empire\" that in the corporate consumerist United States, the very factors that once propelled it to greatness―extreme individualism, territorial and economic expansion, and the pursuit of material wealth―have pushed the United States across a critical threshold where collapse is inevitable. Politically associated with over-reach, and as a result of the environmental exhaustion and polarization of wealth between rich and poor, he concludes the current system is fast arriving at a situation where continuation of the existing system saddled with huge deficits and a hollowed-out economy is physically, socially, economically and politically impossible. Although developed in much more depth, Berman's thesis is similar in some ways to that of Urban Planner, Jane Jacobs who argues that five pillars of United States culture that are in serious decay: community and family; higher education; the effective practice of science; taxation and government; and the self-regulation of the learned professions. The corrosion of these pillars, Jacobs argues, is linked to societal ills such as environmental crisis, racism and the growing gulf between rich and poor.\n\nSome environmental scientists also see the world entering a Planetary Phase of Civilization, characterized by a shift away from independent, disconnected nation-states to a world of increased global connectivity with worldwide institutions, environmental challenges, economic systems, and consciousness. In an attempt to better understand what a Planetary Phase of Civilization might look like in the current context of declining natural resources and increasing consumption, the Global scenario group used scenario analysis to arrive at three archetypal futures: Barbarization, in which increasing conflicts result in either a fortress world or complete societal breakdown; Conventional Worlds, in which market forces or Policy reform slowly precipitate more sustainable practices; and a Great Transition, in which either the sum of fragmented Eco-Communalism movements add up to a sustainable world or globally coordinated efforts and initiatives result in a new sustainability paradigm.\n\nCultural critic and author Derrick Jensen argues that modern civilization is directed towards the domination of the environment and humanity itself in an intrinsically harmful, unsustainable, and self-destructive fashion. Defending his definition both linguistically and historically, he defines civilization as \"a culture... that both leads to and emerges from the growth of cities\", with \"cities\" defined as \"people living more or less permanently in one place in densities high enough to require the routine importation of food and other necessities of life\". This need for civilizations to import ever more resources, he argues, stems from their over-exploitation and diminution of their own local resources. Therefore, civilizations inherently adopt imperialist and expansionist policies and, to maintain these, highly militarized, hierarchically structured, and coercion-based cultures and lifestyles.\n\nThe Kardashev scale classifies civilizations based on their level of technological advancement, specifically measured by the amount of energy a civilization is able to harness. The Kardashev scale makes provisions for civilizations far more technologically advanced than any currently known to exist (see also: Civilizations and the Future and Space civilization).\n\n\n"}
{"id": "6259", "url": "https://en.wikipedia.org/wiki?curid=6259", "title": "Civilization (video game)", "text": "Civilization (video game)\n\nSid Meier's Civilization is the first in a series of turn-based \"4X\"-type strategy video game created by Sid Meier and Bruce Shelley for MicroProse in 1991. The game's objective is to \"Build an empire to stand the test of time\": it begins in 4000 BC and the players attempt to expand and develop their empires through the ages from the ancient era until modern and near-future times.\n\n\"Civilization\" was originally developed for MS-DOS running on a PC. It has undergone numerous revisions for various platforms (including Windows, Macintosh, Amiga, Atari ST, Super NES, Sega Saturn, PlayStation and N-Gage) and now exists in several versions. A multiplayer remake, Sid Meier's CivNet was released for the PC in 1995. The N-Gage version was the 17th game released for the system in North America.\n\n\"Civilization\" is a turn-based single- or multiplayer strategy game. The player takes on the role of the ruler of a civilization, starting with one (or occasionally two) settler units, and attempts to build an empire in competition with two to seven other civilizations. The game requires a fair amount of micromanagement (although less than other simulation games). Along with the larger tasks of exploration, warfare and diplomacy, the player has to make decisions about where to build new cities, which improvements or units to build in each city, which advances in knowledge should be sought (and at what rate), and how to transform the land surrounding the cities for maximum benefit. From time to time the player's towns may be harassed by barbarians, units with no specific nationality and no named leader. These threats only come from unclaimed land or sea, so that over time there are fewer and fewer places from which barbarians will emanate.\n\nBefore the game begins, the player chooses which historical or current civilization to play. In contrast to later games in the \"Civilization\" series, this is largely a cosmetic choice, affecting titles, city names, musical heralds, and color. The choice does affect their starting position on the \"Play on Earth\" map, and thus different resources in one's initial cities, but has no effect on starting position when starting a random world game or a customized world game. The player's choice of civilization also prevents the computer from being able to play as that civilization or the other civilization of the same color, and since computer-controlled opponents display certain traits of their civilizations this affects gameplay as well. The Aztecs are both fiercely expansionist and generally extremely wealthy, for example. Other civilizations include the Americans, the Mongols, and Romans. Each civilization is led by a famous historical figure, such as Mohandas K. Gandhi for India.\n\nThe scope of \"Civilization\" is larger than most other games. The game begins in 4000 BC, before the Bronze Age, and can last through to AD 2100 (on the easiest setting) with Space Age and \"future technologies\". At the start of the game there are no cities anywhere in the world: the player controls one or two settler units, which can be used to found new cities in appropriate sites (and those cities may build other settler units, which can go out and found new cities, thus expanding the empire). Settlers can also alter terrain, build improvements such as mines and irrigation, build roads to connect cities, and later in the game they can construct railroads which offer unlimited movement.\n\nAs time advances, new technologies are developed; these technologies are the primary way in which the game changes and grows. At the start, players choose from advances such as pottery, the wheel, and the alphabet to, near the end of the game, nuclear fission and spaceflight. Players can gain a large advantage if their civilization is the first to learn a particular technology (the secrets of flight, for example) and put it to use in a military or other context. Most advances give access to new units, city improvements or derivative technologies: for example, the chariot unit becomes available after the wheel is developed, and the granary building becomes available to build after pottery is developed. The whole system of advancements from beginning to end is called the technology tree, or simply the Tech tree; this concept has been adopted in many other strategy games. Since only one tech may be \"researched\" at any given time, the order in which technologies are chosen makes a considerable difference in the outcome of the game and generally reflects the player's preferred style of gameplay.\n\nPlayers can also build \"Wonders of the World\" in each of the epochs of the game, subject only to obtaining the prerequisite knowledge. These wonders are important achievements of society, science, culture and defense, ranging from the Pyramids and the Great Wall in the Ancient age, to Copernicus' Observatory and Magellan's Expedition in the middle period, up to the Apollo program, the United Nations, and the Manhattan Project in the modern era. Each wonder can only be built once in the world, and requires a lot of resources to build, far more than most other city buildings or units. Wonders provide unique benefits to the controlling civilization. For example, Magellan's Expedition increases the movement rate of naval units. Wonders typically affect either the city in which they are built (for example, the Colossus), every city on the continent (for example, J.S. Bach's Cathedral), or the civilization as a whole (for example, Darwin's Voyage). Some wonders are made obsolete by new technologies.\n\nThe game can be won by conquering all other civilizations or by winning the space race by reaching the star system of Alpha Centauri.\n\nBritish designer Francis Tresham released his \"Civilization\" board game in 1980 under his company Hartland Trefoil. Avalon Hill had obtained the rights to publish it in the United States in 1981.\n\nThere were at least two attempts to make a computerized version of Tresham's game prior to 1990. Danielle Bunten Berry planned to start work on the game after completing \"M.U.L.E.\" in 1983, and again in 1985, after completing \"The Seven Cities of Gold\" at Electronic Arts. In 1983 Bunten and producer Joe Ybarra opted to first do \"Seven Cities of Gold\". The success of \"Seven Cities\" in 1985 in turn led to a sequel, \"Heart of Africa\". Bunten never returned to the idea of \"Civilization\". Don Daglow, designer of \"Utopia\", the first simulation game, began work programming a version of \"Civilization\" in 1987. He dropped the project, however, when he was offered an executive position at Brøderbund, and never returned to the game.\n\nSid Meier and Bill Stealey co-founded MicroProse in 1982 to develop flight simulators and other military strategy video games based on Stealey's past experiences as a United States Air Force pilot. Around 1989, Meier wanted to expand his repertoire beyond these types of games, as just having finished \"F-19 Stealth Fighter\" (1988, 1990), he said \"Everything I thought was cool about a flight simulator had gone into that game.\" He took to heart the success of the new god game genre in particular \"SimCity\" (1989) and \"Populous\" (1989). Specifically with \"SimCity\", Meier recognized that video games could still be entertaining based on building something up. By then, Meier was not an official employee of MicroProse but worked under contract where the company paid him upfront for game development, a large payment on delivery of the game, and additional royalties on each game of his sold.\n\nMicroProse had hired a number of Avalon Hill game designers, including Bruce Shelley. Among other works, Shelley had been responsible for adapting the railroad-based \"1829\" board game developed by Tresham into \"\". Shelley had joined MicroProse finding that the board game market was weakening in contrast to the video game market, and initially worked on \"F-19 Stealth Fighter\". Meier recognized Shelley's abilities and background in game design and took him on as personal assistant designer to brainstorm new game ideas. The two initially worked on ideas for \"Covert Action\", but had put these aside when they came up with the concepts for \"Railroad Tycoon\" (1990), based loosely on the \"1829\"/\"1830\" board games. \"Railroad Tycoon\" was generally well received at its release, but the title did not fit within the nature of flight simulators and military strategy from MicroProse's previous catalog. Meier and Shelley had started a sequel to \"Railroad Tycoon\" shortly after its release, but Stealey canceled the project.\n\nOne positive aspect both had taken from \"Railroad Tycoon\" was the idea of multiple smaller systems working together at the same time and the player having to manage them. Both Meier and Shelley recognized that the complex interactions between these systems led players to \"make a lot of interesting decisions\", and that ruling a whole civilization would readily work well with these underlying systems. Some time later, both discussed their love of the original \"Empire\" computer games, and Meier challenged Shelley to give him ten things he'd change about \"Empire\"; Shelley provided him with twelve. Around May 1990, Meier presented Shelley with a 5-1/4\" floppy disk which contained the first prototype of \"Civilization\" based on their past discussions and Shelley's list. \n\nMeier's prototype took elements from \"Empire\", \"Railroad Tycoon\", \"SimCity\" and the \"Civilization\" board game. This initial version of this game was a real-time simulation, with the player defining zones for their population to grow similar to zoning in \"SimCity\". Meier and Shelley went back and forth with this, with Shelley providing suggestions based on his playthrough and acting as the game's producer, and Meier coding and reworking the game to address these points, and otherwise without involvement of other MicroProse staff. During this period, Stealey and the other managers became concerned that this game did not fit MicroProse's general catalog as computer strategies games had yet proven successful. A few months into the development, Stealey requested them to put the project on hold and complete \"Covert Action\", after which they could go back to their new game. Meier and Shelley completed \"Covert Action\" which was published in 1990.\nOnce \"Covert Action\" was released, Meier and Shelley returned to the prototype. The time away from the project allowed them to recognize that the real-time aspect was not working well, and reworked the game to become turn-based and dropped the zoning aspect. They incorporated elements of city management and military aspect from \"Empire\", including creating individual military units as well as settler units that replaced the functionality of the zoning approach. Meier felt adding military and combat to the game was necessary as \"The game really isn't about being civilized. The competition is what makes the game fun and the players play their best. At times, you have to make the player uncomfortable for the good of the player\". Meier also opted to include a technology tree that would help to open the game to many more choices to the player as it continued, creating a non-linear experience. Meier felt players would be able to use the technology tree to adopt a style of play and from which they could use technologies to barter with the other opponents. While the game relies on established recorded history, Meier admitted he did not spend much time in research, usually only to assure the proper chronology or spellings; Shelley noted that they wanted to design for fun, not accuracy, and that \"Everything we needed was pretty much available in the children’s section of the library.\"\n\nMeier eliminated the potential for any civilization to fall on its own, believing this would be punishing to the player. Meier omitted multiplayer alliances because the computer used them too effectively, causing players to think that it was cheating. He said that by contrast, minefields and minesweepers caused the computer to do \"stupid things ... If you've got a feature that makes the AI look stupid, take it out. It's more important not to have stupid AI than to have good AI\". Meier also omitted jets and helicopters because he thought players would not find obtaining new technologies in the endgame useful, and online multiplayer support because of the small number of online players (\"if you had friends, you wouldn't need to play computer games\"); he also did not believe that online play worked well with turn-based play. The game was developed for the IBM PC platform, which at the time had support for both 16-color EGA to 256-color VGA; Meier opted to stay with the basic EGA support to allow the game to run on both EGA and VGA systems.\n\nMeier and Shelley neared the end of their development and started presenting the game to the rest of MicroProse for feedback towards publication. This process was slowed by the current vice president of development, who had taken over Meier's former position at the company. This vice president did not receive any financial bonuses for successful publication of Meier's games due to Meier's contract terms, forgoing any incentive to provide the needed resources to finish the game. The management had also expressed issue with the lack of a firm completion date, as according to Shelley, Meier would consider a game completed only when he felt he had completed it. Eventually the two got the required help for publication, with Shelley overseeing these processes and Meier making the necessary coding changes. Playtesting revealed that their chosen map size was too large and made for boring and repetitive gameplay. The size was reduced and other automated features, like city management, were made to require more player involvement. They also eliminated a large branch of their technology tree and spent time reworking the existing technologies and units to make sure they felt appropriate and did not break the game. Most of the game was originally developed with art crafted by Meier, and MicroProse's art department helped to create most of the final assets, though some of Meier's original art was used. Shelley wrote out the \"Civilopedia\" entries for all the elements of the game and the game's large manual.\n\nThe name \"Civilization\" came late in the development process. MicroProse recognized at this point the 1980 \"Civilization\" board game may conflict with their video game, as it shared a similar theme including the technology tree. Meier had noted the board game's influence but considered it not as great as \"Empire\" or \"SimCity\", while others have noted significant differences that made the video game far different from the board game such as the non-linearity introduced by Meier's technology tree. To avoid any potential legal issues, MicroProse negotiated a license to use the \"Civilization\" name from Avalon Hill. The addition of Meier's name to the title was from a current practice established by Stealey to attach games like \"Civilization\" that diverged from MicroProse's past catalog to Meier's name, so that players that played Meier's combat simulators and recognized Meier's name would give these new games a try. This approach worked, according to Meier, and he would continue this naming scheme for other titles in the future as a type of branding.\n\nBy the time the game was completed and ready for release, Meier estimated that it had cost $170,000 in development. \"Civilization\" was released in early 1991. Because of the animosity that MicroProse's management had towards Meier's games, there was very little promotion of the title, though interest in the game through word-of-mouth helped to boost sales. Following the release on the IBM PC, the game was ported to other platforms; Meier and Shelley provided this code to contractors hired by MicroProse to complete the ports.\n\n\"Civilization\" was released with only single-player support, with the player working against multiple computer opponents. In 1991, Internet or online gaming was still in its infancy, so this option was not considered in \"Civilization\"s release. Over the next few years, as home Internet accessibility took off, MicroProse looked to develop an online version of \"Civilization\". This led to the 1995 release of \"Sid Meier's CivNet\". \"CivNet\" allowed for up to eight players to play the game, with computer opponents available to obtain up to eight active civilizations. Games could be played either on a turn-based mode, or in a simultaneous mode where each player took their turn at the same time and only progressing to the next turn once all players have confirmed being finished that turn. The game, in addition to better support for Windows 3.1 and Windows 95, supported connectivity through LAN, primitive Internet play, modem, and direct serial link, and included a local hotseat mode. \"CivNet\" also included a map editor and a \"king builder\" to allow a player to customize the names and looks of their civilization as seen by other players.\n\nAccording to Brian Reynolds, who led the development of \"Civilization II\", MicroProse \"sincerely believed that \"CivNet\" was going to be a much more important product\" than the next single-player \"Civilization\" game that he and Jeff Briggs had started working on. Reynolds said that because their project was seen as a side effort with little risk, they were able to innovate new ideas into \"Civilization II\". As a net result, \"CivNet\" was generally overshadowed by \"Civilization II\" which was released in the following year.\n\n\"Civilization\"s critical success created a \"golden period of MicroProse\" where there was more potential for similar strategy games to succeed, according to Meier. This put stress on the company's direction and culture. Stealey wanted to continue to pursue the military-themed titles, while Meier wanted to continue his success with simulation games. Shelley left MicroProse in 1992 and joined Ensemble Studios, where he used his experience with \"Civilization\" to design the \"Age of Empire\" games. Stealey had pushed MicroProse to develop console and arcade-based versions of their games, but this put the company into debt, and Stealey eventually sold the company to Spectrum HoloByte in 1993; Spectrum HoloByte kept MicroProse as a separate company on acquisition.\n\nMeier would continue and develop \"Civilization II\" along with Brian Reynolds, who served in a similar role to Shelley as design assistant, as well as help from Jeff Briggs and Douglas Kaufman. This game was released in early 1996, and is considered the first sequel of any Sid Meier game. Stealey eventually sold his shares in MicroProse and left the company, and Spectrum HoloByte opted to consolidate the two companies under the name MicroProse in 1996, eliminating numerous positions at MicroProse in the process. As a result, Meier, Briggs, and Reynolds all opted to leave the company and founded Firaxis, which by 2005 became a subsidiary of Take-Two. After a number of acquisitions and legal actions, the \"Civilization\" brand (both as a board game and video game) is now owned by Take-Two, and Firaxis, under Meier's oversight, continues to develop games in the \"Civilization\" series.\n\n\"Civilization\" has been called one of the most important strategy games of all time, and has a loyal following of fans. This high level of interest has led to the creation of a number of free and open source versions and inspired similar games by other commercial developers.\n\nThe game was reviewed in 1992 in \"Dragon\" #183 by Hartley, Patricia, and Kirk Lesser in \"The Role of Computers\" column. The reviewers gave the game 5 out of 5 stars. They commented: \"\"Civilization\" is one of the highest dollar-to-play-ratio entertainments we've enjoyed. The scope is enormous, the strategies border on being limitless, the excitement is genuinely high, and the experience is worth every dime of the game's purchase price.\"\n\n\"Civilization\" won the Origins Award in the category Best Military or Strategy Computer Game of 1991. A 1992 \"Computer Gaming World\" survey of wargames with modern settings gave the game five stars out of five, describing it as \"more addictive than crack ... so rich and textured that the documentation is incomplete\". In 1992 the magazine named it the Overall Game of the Year, in 1993 added the game to its Hall of Fame, and in 1996 chose \"Civilization\" as the best game of all time:\n\nA critic for \"Next Generation\" judged the Super NES version to be a disappointing port, with a cumbersome menu system (particularly that the \"City\" and \"Production\" windows are on separate screens), an unintuitive button configuration, and ugly scaled down graphics. However, he gave it a positive recommendation due to the strong gameplay and strategy of the original game: \"if you've never taken a crack at this game before, be prepared to lose hours, even days, trying to conquer those pesky Babylonians.\" Sir Garnabus of \"GamePro\", in contrast, was pleased with the Super NES version's interface, and said the graphics and audio are above that of a typical strategy game. He also said the game stood out among the Super NES's generally action-oriented library.\n\nIn 2000, GameSpot rated \"Civilization\" as the seventh most influential video game of all time. It was also ranked at fourth place on IGN's 2000 list of the top PC games of all time. In 2004, readers of \"Retro Gamer\" voted it as the 29th top retro game. In 2007, it was named one of the 16 most influential games in history at a German technology and games trade show Telespiele. In Poland, it was included in the retrospective lists of the best Amiga games by Wirtualna Polska (ranked ninth) and \"CHIP\" (ranked fifth). In 2012, \"Time\" named it one of the 100 greatest video games of all time. In 1994, \"PC Gamer US\" named \"Civilization\" the second best computer game ever. The editors wrote, \"The depth of strategies possible is impressive, and the look and feel of the game will keep you playing and exploring for months. Truly a remarkable title.\" That same year, \"PC Gamer UK\" named its Windows release the sixth best computer game of all time, calling it Sid Meier's \"crowning glory.\"\n\nOn March 12, 2007, \"The New York Times\" reported on a list of the ten most important video games of all time, the so-called game canon, which included \"Civilization\".\n\nBy the release of \"Civilization II\" in 1996, \"Civilization\" had sold over 850,000 copies. Shelley stated in a 2016 interview that \"Civilization\" had sold 1.5 million copies.\n\nThere have been several sequels to \"Civilization\", including \"Civilization II\" (1996), \"Civilization III\" (2001), \"Civilization IV\" (2005), \"Civilization Revolution\" (2008), \"Civilization V\" (2010), and \"Civilization VI\" in 2016. In 1994, Meier produced a similar game titled \"Colonization\". As an easter egg in these latter games referencing an integer underflow bug in \"Civilization\", the computer-controlled Gandhi, normally run in a highly-peaceful manner, becomes a nuclear warmonger once this technology was unlocked. The original game had set Gandhi's \"aggressiveness\" to 1 out of a maximum 255 possible for a 8-bit unsigned integer, making the opponent extremely peaceful. However, once a player achieved the Democracy government, this reduced all aggression levels by 2; Gandhi's \"1\" would be reduced to \"-1\" which actually wraps around to \"255\", making him the most aggressive opponent in the game. Another relic of \"Civilization\" was the nature of combat where a military unit from earlier civilization periods could remain in play through modern times, gaining combat bonuses due to veteran proficiency, leading to these primitive units easily beating out modern technology against all common sense, with the common example of a veteran spearmen unit able to fend off a battleship. Meier noted that this resulted from not anticipating how players would use units, expecting them to have used their forces more like a war-based board game to protect borders and maintain zones of control rather than creating \"stacks of doom\". Future civilization games have had many changes in combat systems to prevent such oddities, though these games do allow for such random victories.\n\nThe 1999 game \"Sid Meier's Alpha Centauri\" was also created by Meier and is in the same genre, but with a futuristic/space theme; many of the interface and gameplay innovations in this game eventually made their way into \"Civilization III\" and \"IV\". \"Alpha Centauri\" is not actually a sequel to \"Civilization\", despite beginning with the same event that ends \"Civilization\" and \"Civilization II\": a manned spacecraft from Earth arrives in the Alpha Centauri star system. Firaxis' 2014 game \"\", although bearing the name of the main series, is a reimagining of \"Alpha Centauri\" running on the engine of \"Civilization V\".\n\nIn 1994, MicroProse published \"Master of Magic\", a similar game but embedded in a medieval-fantasy setting where instead of technologies the player (a powerful wizard) develops spells, among other things. The game also shared many things with the popular fantasy card-trading game \"\". In 1999, Activision released \"\", a sequel of sorts to \"Civilization II\" but created by a completely different design team. \"Call to Power\" spawned a sequel in 2000, but by then Activision had lost the rights to the \"Civilization\" name and could only call it \"Call to Power II\".\n\nAn open source clone of \"Civilization\" has been developed under the name of \"Freeciv\", with the slogan \"'Cause civilization should be free.\" This game can be configured to match the rules of either \"Civilization\" or \"Civilization II\". Another game that partially clones Civilization is a public domain game called \"C-evo\".\n\n\n"}
{"id": "6260", "url": "https://en.wikipedia.org/wiki?curid=6260", "title": "Claude Debussy", "text": "Claude Debussy\n\nAchille-Claude Debussy (, 22 August 1862 – 25 March 1918), known since the 1890s as Claude-Achille Debussy or Claude Debussy, was a French composer. He and Maurice Ravel were the most prominent figures associated with Impressionist music, though Debussy disliked the term when applied to his compositions. He was made Chevalier of the Legion of Honour in 1903. He was among the most influential composers of the late 19th and early 20th centuries, and his use of non-traditional scales and chromaticism influenced many composers who followed.\n\nDebussy's music is noted for its sensory content and frequent usage of nontraditional tonalities. The prominent French literary style of his period was known as Symbolism, and this movement directly inspired Debussy both as a composer and as an active cultural participant.\n\nDebussy, the eldest of five children, was born Achille-Claude Debussy (he later reversed his forenames) on 22 August 1862 in Saint-Germain-en-Laye, France. His father, Manuel-Achille Debussy, owned a china shop there; his mother, Victorine Manoury Debussy, was a seamstress. The family moved to Paris in 1867, but in 1870 Debussy's pregnant mother fled with Claude to his paternal aunt's home in Cannes to escape the Franco-Prussian War. At the age of seven, he began piano lessons with an Italian violinist in his early 40s named Jean Cerutti, and his aunt paid for his lessons. In 1871 he drew the attention of Marie Mauté de Fleurville, who claimed to have been a pupil of Frédéric Chopin. Debussy always believed her, although there is no independent evidence to support her claim. His talents soon became evident, and in 1872, at age ten, Debussy entered the Paris Conservatoire, where he spent the next 11 years. During his time there he studied composition with Ernest Guiraud, music history/theory with Louis-Albert Bourgault-Ducoudray, harmony with Émile Durand, piano with Antoine François Marmontel, organ with César Franck, and solfège with Albert Lavignac, as well as other significant figures of the era. He also became a lifelong friend of fellow student and distinguished pianist Isidor Philipp. After Debussy's death, many pianists sought Philipp's advice on playing his works.\n\nDebussy was experimental from the outset, favouring dissonances and intervals that were not taught at the Academy. Like Georges Bizet, he was a brilliant pianist and an outstanding sight reader, who could have had a professional career had he so wished. The pieces he played in public at this time included sonata movements by Beethoven, Schumann and Weber, and Chopin's Ballade No. 2, a movement from the Piano Concerto No. 1, and the \"Allegro de concert\".\n\nDuring the summers of 1880, 1881, and 1882, he accompanied Nadezhda von Meck, the wealthy patroness of Pyotr Ilyich Tchaikovsky, as she travelled with her family in Europe. The young composer's many musical activities during these vacations included playing four-hand pieces with von Meck at the piano, giving music lessons to her children, and performing in private concerts with some of her musician friends. Despite von Meck's closeness to Tchaikovsky, the Russian master appears to have had minimal effect on Debussy. In September 1880 she sent his \"Danse bohémienne\" for Tchaikovsky's perusal; a month later Tchaikovsky wrote back to her: \"It is a very pretty piece, but it is much too short. Not a single idea is expressed fully, the form is terribly shriveled, and it lacks unity.\" Debussy did not publish the piece, and the manuscript remained in the von Meck family; it was eventually sold to B. Schott's Sohne in Mainz, and published by them in 1932.\n\nA greater influence was Debussy's close friendship with Marie-Blanche Vasnier, a singer he met when he began working as an accompanist to earn some money, embarking on an eight-year affair together. She and her husband, Parisian civil servant Henri, gave Debussy emotional and professional support. Henri Vasnier introduced him to the writings of influential French writers of the time, which gave rise to his first songs, settings of poems by Paul Verlaine (the son-in-law of his former teacher Mme. Mauté de Fleurville).\nAs the winner of the 1884 Prix de Rome with his composition \"L'enfant prodigue\", he received a scholarship to the Académie des Beaux-Arts, which included a four-year residence at the Villa Medici, the French Academy in Rome, to further his studies (1885–1887). According to letters to Marie-Blanche Vasnier, perhaps in part designed to gain her sympathy, he found the artistic atmosphere stifling, the company boorish, the food bad, and the monastic quarters \"abominable\". Neither did he delight in Italian opera, as he found the operas of Donizetti and Verdi not to his taste. Debussy was often depressed and unable to compose, but he was inspired by Franz Liszt, whose command of the keyboard he found admirable. In June 1885, he wrote of his desire to follow his own way, saying, \"I am sure the Institute would not approve, for, naturally it regards the path which it ordains as the only right one. But there is no help for it! I am too enamoured of my freedom, too fond of my own ideas!\"\n\nDebussy finally composed four pieces that were sent to the Academy: the symphonic ode \"Zuleima\" (based on a text by Heinrich Heine); the orchestral piece \"Printemps\"; the cantata \"La Damoiselle élue\" (1887–1888) (which was criticized by the Academy as \"bizarre\", although it was the first piece in which the stylistic features of his later style began to emerge); and the \"Fantaisie\" for piano and orchestra, which was heavily based on César Franck's music and therefore eventually withdrawn by Debussy. The Academy chided him for \"courting the unusual\" and hoped for something better from the gifted student. Although Debussy's works showed the influence of Jules Massenet, Massenet concluded, \"He is an enigma.\"\n\nDuring his visits to Bayreuth in 1888–9, Debussy was exposed to Wagnerian opera, which would have a lasting impact on his work. Like many young musicians of the time, he responded positively to Richard Wagner's sensuousness, mastery of form, and striking harmonies. Wagner's extroverted emotionalism was not to be Debussy's way, but the German composer's influence is evident in \"La damoiselle élue\" and the 1889 piece \"Cinq poèmes de Charles Baudelaire\". Other songs of the period, notably the settings of Verlaine – \"Ariettes oubliées\", \"Trois mélodies\", and \"Fêtes galantes\" – are all in a more capricious style.\n\nAround this time he met Erik Satie, who proved a kindred spirit in his experimental approach to composition and to naming his pieces. Both musicians were bohemians during this period, enjoying the same cafe society and struggling to stay afloat financially.\n\nIn 1889, at the Exposition Universelle in Paris, Debussy first heard Javanese gamelan music. He incorporated gamelan scales, melodies, rhythms, and ensemble textures into some of his compositions, most notably \"Pagodes\" from his piano collection \"Estampes\".\n\nDebussy's private life was often turbulent. At the age of 18 he began an eight-year affair with Marie-Blanche Vasnier, the wife of Parisian civil servant Henri Vasnier. The relationship eventually faltered following his winning of the Prix de Rome in 1884 and obligatory residence in Rome.\n\nOn his permanent return to Paris and his parents' home on the rue de Berlin (now rue de Liège) he began a tempestuous relationship with Gabrielle ('Gaby') Dupont, a tailor's daughter from Lisieux, soon living with her on the rue de Londres, and later the rue Gustave Doré. During this time he also had an affair with the singer Thérèse Roger, to whom he was briefly engaged. Such cavalier behaviour was widely condemned, and precipitated the end of his long friendship with Ernest Chausson.\n\nHe ultimately left Dupont for her friend Rosalie ('Lilly') Texier, a fashion model whom he married in 1899, after threatening suicide if she refused him. However, although Texier was affectionate, practical, straightforward, and well liked by Debussy's friends and associates, he became increasingly irritated by her intellectual limitations and lack of musical sensitivity. Moreover, her looks had prematurely aged, and she was unable to bear children.\n\nIn 1904 Debussy was introduced to Emma Bardac, wife of Parisian banker Sigismond Bardac, by her son Raoul, who was one of his students. In contrast to Texier, Bardac was a sophisticate, a brilliant conversationalist, and an accomplished singer. After dispatching Lilly to her father's home at Bichain in Villeneuve-la-Guyard on 15 July 1904, Debussy secretly took Bardac to Jersey for a holiday. On their return to France, he wrote to Texier on 11 August from Dieppe, informing her that their marriage was over, but still making no mention of Bardac. He briefly moved to an apartment at 10 avenue Alphand. On 14 October, five days before their fifth wedding anniversary, Texier attempted suicide, shooting herself in the chest with a revolver while standing in the Place de la Concorde; she survived, although the bullet remained lodged in her vertebrae for the rest of her life. The ensuing scandal was to alienate Debussy from many of his friends, whilst Bardac was disowned by her family.\n\nIn the spring of 1905, finding the hostility towards them intolerable, Debussy and Bardac (now pregnant) fled to England, via Jersey. Bardac's divorce was finalized in May. The couple settled at the Grand Hotel, Eastbourne, from 24 July to 30 August 1905, where Debussy corrected proofs to his symphonic suite \"La mer\", celebrating his divorce from Texier on 2 August.\n\nAfter a brief visit to London, the couple returned to Paris in September, buying a house in a courtyard development off the Avenue du Bois de Boulogne (now Avenue Foch) where Debussy resided for the rest of his life. Their daughter (the composer's only child) Claude-Emma was born there on 30 October. Her parents eventually married in 1908, their troubled union enduring until Debussy's death in 1918. Claude-Emma, more affectionately known as 'Chouchou', was a great musical inspiration to the composer (she was the dedicatee of his \"Children's Corner\" suite). Claude-Emma outlived her father by scarcely a year, succumbing to the diphtheria epidemic of 1919 after her doctor administered the wrong treatment.\n\nMary Garden, who played the part of Melisande in the original production of \"Pelléas et Mélisande\" in 1902, was to write of him: \"I honestly don’t know if Debussy ever loved anybody really. He loved his music – and perhaps himself. I think he was wrapped up in his genius... He was a very, very strange man.\" \n\nDebussy died of rectal cancer at his Paris home on 25 March 1918, at the age of 55. He had been diagnosed with the cancer in 1909 after experiencing bleeding, and in December 1915 underwent one of the earliest colostomy operations ever performed. The operation achieved only a temporary respite, and occasioned him considerable frustration (he was to liken dressing in the morning to \"all the labours of Hercules in one\"). His death occurred in the midst of the aerial and artillery bombardment of Paris during the German Spring Offensive of World War I. The funeral procession made its way through deserted streets to Père Lachaise Cemetery as the German guns bombarded the city. The military situation in France was critical, and did not permit the honour of a public funeral with ceremonious graveside orations. His body was reinterred the following year in the small Passy Cemetery sequestered behind the Trocadéro, fulfilling his wish to rest \"among the trees and the birds\"; his wife and daughter are buried with him.\n\nRudolph Reti points out the following features of Debussy's music, which \"established a new concept of tonality in European music\":\nHe concludes that Debussy's achievement was the synthesis of monophonic based \"melodic tonality\" with harmonies, albeit different from those of \"harmonic tonality\".\n\nThe application of the term \"Impressionist\" to Debussy and the music he influenced is a matter of intense debate within academic circles. One side argues that the term is a misnomer, an inappropriate label which the composer himself opposed. In a letter of 1908 he wrote: \"I am trying to do 'something different' – an effect of reality... what the imbeciles call 'impressionism', a term which is as poorly used as possible, particularly by the critics, since they do not hesitate to apply it to [J.M.W.] Turner, the finest creator of mysterious effects in all the world of art.\"\n\n\nFrom the 1890s Debussy began to develop his own musical language, which was largely independent of Wagner's style, coloured in part from the dreamy, sometimes morbid romanticism of the Symbolist movement. He became a frequent participant at Stéphane Mallarmé's Symbolist gatherings, where Wagnerism dominated the discussion. However, in contrast to the enormous works of Wagner and other late romantic composers around this time, he chose to write in smaller, more accessible forms.\nThe \"Deux arabesques\" is an example of one of his earliest works, already developing his musical language. \"Suite bergamasque\" (1890) recalls rococo decorousness with a modern cynicism and puzzlement, and contains one of his most popular pieces, \"Clair de Lune\". His String Quartet in G minor (1893) paved the way for his later more daring harmonic exploration, using the Phrygian mode as well as less standard scales such as the whole-tone, which creates a sense of floating, ethereal harmony. He was beginning to employ a single, continuous theme, breaking away from the traditional A–B–A form with its restatements and amplifications, which had been a mainstay of classical music since Joseph Haydn.\n\nDebussy wrote one of his most famous works under the influence of Mallarmé, the revolutionary \"Prélude à l'après-midi d'un faune\", which is truly original in form and execution. In contrast to the large orchestras so favoured by late romanticism, he wrote this piece for a smaller ensemble, emphasizing instrumental colour and timbre. Despite Mallarmé himself and colleague and friend Paul Dukas having been impressed by the piece, it was controversial at its premiere, but nevertheless established Debussy as one of the leading composers of the era.\n\nThe three \"Nocturnes\" (1899) include characteristic studies: in \"Nuages\", using veiled harmony and texture; \"Fêtes\", in exuberance; and \"Sirènes\", using whole-tones. Debussy's only complete opera \"Pelléas et Mélisande\" premiered in 1902, after ten years of work, and contrasted sharply with Wagnerian opera. Based on the play by Maurice Maeterlinck, the opera proved to be an immediate success and immensely influential to younger French composers, including Maurice Ravel. These works brought a fluidity of rhythm and colour quite new to Western music.\n\n\"La mer\" (1903–1905) essays a more symphonic form, with a finale that works themes from the first movement, although the middle movement, \"Jeux de vagues\", proceeds much less directly and with more variety of colour. The reviews were once again sharply divided. Some critics thought the treatment to be less subtle and less mysterious than his previous works, and even a step backward, with Pierre Lalo complaining \"I neither hear, nor see, nor feel the sea.\" Others extolled its \"power and charm\", its \"extraordinary verve and brilliant fantasy\", and its strong colors and definite lines.\n\nHe wrote much for the piano during this period. His first volume of \"Images pour piano\" (1904–1905) combines harmonic innovation with poetic suggestion: \"Reflets dans l'eau\" is a musical description of rippling water, while the second piece \"Hommage à Rameau\" is slow and yearningly nostalgic, taking a melody from Jean-Philippe Rameau's 1737 \"Castor et Pollux\" as its inspiration.\n\nThe evocative \"Estampes\" for piano (1903) give impressions of exotic locations. Debussy came into contact with Javanese gamelan music during the 1889 Paris \"Exposition Universelle\". \"Pagodes\" is the directly inspired result, aiming for an evocation of the pentatonic structures employed by Javanese music.\n\nHe wrote his famous \"Children's Corner Suite\" (1908) for his beloved daughter, Claude-Emma, whom he nicknamed \"Chouchou\". The suite recalls classicism – the opening piece \"Doctor Gradus ad Parnassum\" refers to Muzio Clementi's collection of instructional piano compositions \"Gradus ad Parnassum\" – as well as a new wave of American ragtime music. In the popular final piece of the suite, \"Golliwogg's Cakewalk\", Debussy also pokes fun at Richard Wagner by mimicking the opening bars of Wagner's prelude to \"Tristan und Isolde\".\nThe first book of \"Préludes\" (1910), twelve in total, proved to be his most successful work for piano. The Preludes are frequently compared to those of Chopin. Debussy's preludes are replete with rich, unusual and daring harmonies. They include the popular \"La fille aux cheveux de lin\" (The Girl with the Flaxen Hair) and \"La Cathédrale Engloutie\" (The Engulfed Cathedral), although since he wanted people to respond intuitively to these pieces, their titles were placed at the end of each one in the hope that listeners would not make stereotype images as they listened.\n\nLarger scale works included his orchestral piece \"Iberia\" (1907), a triptych medley of Spanish allusions and fleeting impressions which was begun as a work for two pianos, and also the music for Gabriele D'Annunzio's mystery play \"Le Martyre de saint Sébastien\" (1911). A lush and dramatic work, written in only two months, it is remarkable in sustaining a late antique modal atmosphere that was otherwise touched only in relatively short piano pieces.\n\nAs Debussy's popularity increased, he was often engaged as a conductor throughout Europe during this period, most often performing \"Pelléas\", \"La Mer\", and \"Prélude à l'après-midi d'un faune\". He was also an occasional music critic, to supplement his conducting fees and piano lessons, writing under the pseudonym \"Monsieur Croche\". He avoided analytical dissection and attempts to force images from music, saying \"Let us at all costs preserve this magic peculiar to music, since of all the arts it is most susceptible to magic.\" He could be caustic and witty, sometimes sloppy and ill-informed. He was for the most part enthusiastic about Richard Strauss and Stravinsky, and worshipful of Chopin and Bach, the latter being acknowledged as \"the one great master.\" His relationship to Beethoven was a complex one; he was said to refer to him as \"\"le vieux sourd\"\" (the old deaf one) and adjured one young pupil never to play Beethoven's music for \"it is like somebody dancing on my grave.\" It was said that \"Debussy liked Mozart, and he believed that Beethoven had terrifically profound things to say, but that he did not know how to say them, because he was imprisoned in a web of incessant restatement and of German aggressiveness.\" He also admired the works of Charles-Valentin Alkan. Schubert and Mendelssohn fared much worse, the latter being described as a \"facile and elegant notary\".\n\nDebussy's harmonies and chord progressions frequently exploit dissonances without any formal resolution. Unlike in his earlier work, he no longer hides discords in lush harmonies, and the forms are far more irregular and fragmented. These chords that seemingly had no resolution were described by Debussy himself as \"floating chords\", and were used to set tone and mood in many of his works. The whole tone scale dominates much of his late music.\n\nHis two final volumes of works for the piano, the \"Études\" (1915), interpret similar varieties of style and texture purely as pianistic exercises, and include pieces that develop irregular form to an extreme, as well as others influenced by the young Igor Stravinsky (a presence too in the suite \"En blanc et noir\" for two pianos, 1915). The rarefaction of these works is a feature of the last set of songs, the \"Trois poèmes de Mallarmé\" (1913), and of the Sonata for flute, viola and harp (1915), though the sonata and its companions also recapture the inquisitive Verlainian classicism.\nWith the sonatas of 1915–1917 there is a sudden shift in the style. These works recall Debussy's earlier music in part, but also look forward, with leaner, simpler structures. Despite the thinner textures of the Violin Sonata (1917), there remains an undeniable richness in the chords themselves. This shift parallels the movement commonly known as neo-classicism, which became popular after his death in 1918. He planned a set of six sonatas, but had only completed three (cello, flute-viola-harp, and violin) before he died.\n\nThe final orchestral work by Debussy, the ballet \"Jeux\" (1912) written for Sergei Diaghilev's Ballets Russes, contains some of his strangest harmonies and textures in a form that moves freely over its own field of motivic connection. At first, \"Jeux\" was overshadowed by Igor Stravinsky's \"The Rite of Spring\", which was composed in the same year as \"Jeux\", and was premiered only two weeks later by the same ballet company. Decades later, composers such as Pierre Boulez and Jean Barraqué pointed out parallels to Anton Webern's serialism in this work.\n\nOther late stage works, including the ballets \"Khamma\" (1912) and \"La boîte à joujoux\" (1913), were left with the orchestration incomplete, and were later completed by Charles Koechlin and André Caplet, who also helped him with the orchestration of \"Gigues\" (from \"Images pour orchestre\") and \"Le martyre de St. Sébastien\".\n\nThe second set of \"Préludes\" for piano (1913) features Debussy at his most avant-garde, where he uses dissonant harmonies to evoke specific moods and images. He consciously gives titles to each prelude which amplify the preludes' tonal ambiguity and dissonance. He uses scales such as the whole tone scale, musical modes, and the octatonic scale in his preludes which exaggerate this tonal ambiguity, making the key of each prelude almost indistinguishable at times. The second book of Preludes for piano represents his strong interest in the indefinite and esoteric.\nAlthough \"Pelléas\" was Debussy's only completed opera, he began several opera projects which remained unfinished, perhaps due to his fading concentration, increasing procrastination, and failing health. He had finished some partial musical sketches and some unpublished libretti for operas based on Poe's \"The Devil in the Belfry\" (\"Le diable dans le beffroi\", 1902–?1912) and \"The Fall of the House of Usher\" (\"La chute de la maison Usher\", 1908–1917) as well as considering projects for operas based on Shakespeare's \"As You Like It\" and Joseph Bedier's \"La Legende de Tristan\".\n\nFurther plans, such as an American tour, more ballet scores, and revisions of Chopin and Bach works for re-publication, were all cut short by poor health and the outbreak of World War I.\n\nSome people have contended that Debussy structured parts of his music mathematically. Roy Howat, for instance, has published a book contending that Debussy's works are structured around mathematical models even while using an apparent classical structure such as sonata form. Howat suggests that some of Debussy's pieces can be divided into sections that reflect the golden ratio, frequently by using the numbers of the standard Fibonacci sequence.\n\nDebussy's influences were wide-ranging. He acquired a taste for parallel motion in fifths, fourths and octaves from medieval music, and an appreciation for figuration and arabesque from the Baroque masters. He especially had a great love for the French clavier composers Couperin and Rameau, as well as J. S. Bach. Chopin and Liszt were also powerful influences, not only in terms of pianistic layout and harmonic ingenuity, but also because of their willingness to create new forms to accommodate their material.\n\nAmong the Russian composers of his time, the most prominent influences were Tchaikovsky, Balakirev, Rimsky-Korsakov, Borodin and Mussorgsky. It can be inferred that from the Russians \"Debussy acquired his taste for ancient and oriental modes and for vivid colorations, and a certain disdain for academic rules\". Mussorgsky's opera \"Boris Godunov\" directly influenced one of Debussy's most famous works, \"Pelléas et Mélisande\". In addition to the Russian composers, one of Debussy's biggest influences was Richard Wagner. According to Pierre Louys, Debussy \"did not see 'what anyone can do beyond Tristan.\nAfter Debussy's Wagner phase, he started to become immensely interested in non-Western music and its unorthodox approaches to composition. Specifically, he was drawn to the Javanese Gamelan: a musical ensemble from the island of Java that played an array of unique instrumentation, including gongs and metallophones. He first heard the gamelan at the 1889 Paris Exposition. He was not interested in directly quoting his non-Western influences, but instead allowed this non-Western aesthetic to generally influence his own musical work, for example, by frequently using quiet, unresolved dissonances, coupled with the damper pedal, to emulate the \"shimmering\" effect created by a gamelan ensemble.\n\nDebussy was just as influenced by other art forms as he was by music, if not more so. He took a strong interest in literature and visual art, and used these mediums to help shape his unique musical style. He was heavily influenced by the French symbolist movement of the 1880s, which encompassed poetry, visual art, and theatre. He shared the movement's interest in the esoteric and indefinite and their rejection of naturalism and realism. Specifically, \"the development of free verse in poetry and the disappearance of the subject or model in painting influenced him to think about issues of musical form.\" He became personally acquainted with writers and painters of the movement, and based some of his own works on those of the symbolists. The poet Stéphane Mallarmé was a major influence, who in talking of \"a 'musicalization' of poetry\" laid claim to a strong connection between music and his own poetry. His \"Prélude à l'après-midi d'un faune\" was directly influenced by Mallarmé's poem \"Afternoon of a Faun\". Like the symbolists in respect to their own art forms, Debussy aimed to reject common techniques and approaches to composition and attempted to evoke more of a sensorial experience for the listener with his works. Since his time at the Paris Conservatoire, he believed he had much more to learn from artists than from musicians, who were primarily interested in their musical careers.\n\nAbove all, Debussy was inspired by nature and the impression it made on the mind, making a pantheistic profession of faith when he called \"mysterious Nature\" his religion. 'I do not practice religion in accordance with the sacred rites. I have made mysterious Nature my religion. I do not believe that a man is any nearer to God for being clad in priestly garments, nor that one place in a town is better adapted to meditation than another. When I gaze at a sunset sky and spend hours contemplating its marvellous ever-changing beauty, an extraordinary emotion overwhelms me. Nature in all its vastness is truthfully reflected in my sincere though feeble soul. Around me are the trees stretching up their branches to the skies, the perfumed flowers gladdening the meadow, the gentle grass-carpeted earth, ... and my hands unconsciously assume an attitude of adoration. ... To feel the supreme and moving beauty of the spectacle to which Nature invites her ephemeral guests! ... that is what I call prayer.'\n\nContemporary painter James Abbott McNeill Whistler (who lived in France between 1855 and 1859) had a profound influence on the composer. In 1894, Debussy wrote to violinist Eugène Ysaÿe describing his \"Nocturnes\" as \"an experiment in the different combinations that can be obtained from one color – what a study in grey would be in painting.\"\n\nDebussy is widely regarded as one of the most influential composers of the 20th century. His innovative harmonies were influential to almost every other major 20th-century composer, particularly Maurice Ravel, Igor Stravinsky, Olivier Messiaen, Béla Bartók, Pierre Boulez, Heitor Villa-Lobos, Henri Dutilleux, Ned Rorem, George Gershwin, and the minimalist music of Steve Reich and Philip Glass as well as the influential Japanese composer Toru Takemitsu. He also influenced many jazz musicians, including Duke Ellington, Bix Beiderbecke, Branford Marsalis, and Steve Kuhn. He also had a profound impact on modern soundtrack composers such as John Williams, because Debussy's colourful and evocative style translated easily into an emotional language for use in motion picture scores.\n\nA number of posthumous discoveries bear Debussy's name. These include:\n\nIn 1904, Debussy participated in a handful of recordings made together with Scottish soprano Mary Garden. He also made some piano rolls for Welte-Mignon in 1913.\n\n\n\n"}
{"id": "6261", "url": "https://en.wikipedia.org/wiki?curid=6261", "title": "Charles Baxter (author)", "text": "Charles Baxter (author)\n\nCharles Baxter (born May 13, 1947) is an American novelist, essayist, and poet.\n\nBaxter was born in Minneapolis, Minnesota, to John and Mary Barber (Eaton) Baxter. He graduated from Macalester College in Saint Paul. In 1974 he received his PhD in English from the University at Buffalo with a thesis on Djuna Barnes, Malcolm Lowry, and Nathanael West.\n\nBaxter taught high school in Pinconning, Michigan for a year before beginning his university teaching career at Wayne State University in Detroit, Michigan. He then moved to the University of Michigan, where for many years he directed the Creative Writing MFA program. He currently teaches at the University of Minnesota and in the Warren Wilson College MFA Program for Writers.\n\n\n\n\n\n\n"}
{"id": "6262", "url": "https://en.wikipedia.org/wiki?curid=6262", "title": "Ceres", "text": "Ceres\n\nCeres commonly refers to:\n\nCeres may also refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "6267", "url": "https://en.wikipedia.org/wiki?curid=6267", "title": "Cultural imperialism", "text": "Cultural imperialism\n\nCultural imperialism comprises the cultural aspects of imperialism. Imperialism here refers to the creation and maintenance of unequal relationships between civilizations, favoring the more powerful civilization. Thus, cultural imperialism is the practice of promoting and imposing a culture, usually that of a politically powerful nation, over a less powerful society; in other words, the cultural hegemony of industrialized or economically influential countries which determine general cultural values and standardize civilizations throughout the world. The term is employed especially in the fields of history, cultural studies, and postcolonial theory. It is usually used in a pejorative sense, often in conjunction with calls to reject such influence. Cultural imperialism can take various forms, such as an attitude, a formal policy, or military action, insofar as it reinforces cultural hegemony.\n\nAlthough the \"Oxford English Dictionary\" has a 1921 reference to the \"cultural imperialism of the Russians\", John Tomlinson, in his book on the subject, writes that the term emerged in the 1960s and has been a focus of research since at least the 1970s. Terms such as \"media imperialism\", \"structural imperialism\", \"cultural dependency and domination\", \"cultural synchronization\", \"electronic colonialism\", \"ideological imperialism\", and \"economic imperialism\" have all been used to describe the same basic notion of cultural imperialism.\n\nVarious academics give various definitions of the term. American media critic Herbert Schiller wrote: \"The concept of cultural imperialism today [1975] best describes the sum of the processes by which a society is brought into the modern world system and how its dominating stratum is attracted, pressured, forced, and sometimes bribed into shaping social institutions to correspond to, or even promote, the values and structures of the dominating centre of the system. The public media are the foremost example of operating enterprises that are used in the penetrative process. For penetration on a significant scale the media themselves must be captured by the dominating/penetrating power. This occurs largely through the commercialization of broadcasting.\"\n\nTom McPhail defined \"Electronic colonialism as the dependency relationship established by the importation of communication hardware, foreign-produced software, along with engineers, technicians, and related information protocols, that vicariously establish a set of foreign norms, values, and expectations which, in varying degrees, may alter the domestic cultures and socialization processes.\" Sui-Nam Lee observed that \"communication imperialism can be defined as the process in which the ownership and control over the hardware and software of mass media as well as other major forms of communication in one country are singly or together subjugated to the domination of another country with deleterious effects on the indigenous values, norms and culture.\" Ogan saw \"media imperialism often described as a process whereby the United States and Western Europe produce most of the media products, make the first profits from domestic sales, and then market the products in Third World countries at costs considerably lower than those the countries would have to bear to produce similar products at home.\"\n\nDowning and Sreberny-Mohammadi state: \"Imperialism is the conquest and control of one country by a more powerful one. Cultural imperialism signifies the dimensions of the process that go beyond economic exploitation or military force. In the history of colonialism, (i.e., the form of imperialism in which the government of the colony is run directly by foreigners), the educational and media systems of many Third World countries have been set up as replicas of those in Britain, France, or the United States and carry their values. Western advertising has made further inroads, as have architectural and fashion styles. Subtly but powerfully, the message has often been insinuated that Western cultures are superior to the cultures of the Third World.\"\nNeedless to say, all these authors agree that cultural imperialism promotes the interests of certain circles within the imperial powers, often to the detriment of the target societies.\n\nThe issue of cultural imperialism emerged largely from communication studies. However, cultural imperialism has been used as a framework by scholars to explain phenomena in the areas of international relations, anthropology, education, science, history, literature, and sports.\n\nMany of today's academics that employ the term, \"cultural imperialism,\" are heavily informed by the work of Foucault, Derrida, Said, and other poststructuralist and postcolonialist theorists. Within the realm of postcolonial discourse, \"cultural imperialism\" can be seen as the cultural legacy of colonialism, or forms of social action contributing to the continuation of Western hegemony. To some outside of the realm of this discourse, the term is critiqued as being unclear, unfocused, and/or contradictory in nature.\n\nThe work of French philosopher and social theorist Michel Foucault has heavily influenced use of the term \"cultural imperialism,\" particularly his philosophical interpretation of power and his concept of governmentality.\n\nFollowing an interpretation of power similar to that of Machiavelli, Foucault defines power as immaterial, as a \"certain type of relation between individuals\" that has to do with complex strategic social positions that relate to the subject's ability to control its environment and influence those around itself. According to Foucault, power is intimately tied with his conception of truth. \"Truth,\" as he defines it, is a \"system of ordered procedures for the production, regulation, distribution, circulation, and operation of statements\" which has a \"circular relation\" with systems of power. Therefore, inherent in systems of power, is always \"truth,\" which is culturally specific, inseparable from ideology which often coincides with various forms of hegemony. \"Cultural imperialism\" may be an example of this.\n\nFoucault's interpretation of governance is also very important in constructing theories of transnational power structure. In his lectures at the Collège de France, Foucault often defines governmentality as the broad art of \"governing,\" which goes beyond the traditional conception of governance in terms of state mandates, and into other realms such as governing \"a household, souls, children, a province, a convent, a religious order, a family\". This relates directly back to Machiavelli's The Prince, and Foucault's aforementioned conceptions of truth and power. (i.e. various subjectivities are created through power relations that are culturally specific, which lead to various forms of culturally specific governmentality such as neoliberal governmentality.)\n\nInformed by the works of Noam Chomsky, Foucault, and Antonio Gramsci, Edward Saïd is a founding figure of postcolonialism, established with the book \"Orientalism\" (1978), a humanist critique of The Enlightenment, which criticizes Western knowledge of \"The East\" — specifically the English and the French constructions of what is and what is not \"Oriental\". Whereby said \"knowledge\" then led to cultural tendencies towards a binary opposition of the Orient vs. the Occident, wherein one concept is defined in opposition to the other concept, and from which they emerge as of unequal value. In \"Culture and Imperialism\" (1993), the sequel to \"Orientalism\", Saïd proposes that, despite the formal end of the “age of empire” after the Second World War (1939–45), colonial imperialism left a cultural legacy to the (previously) colonized peoples, which remains in their contemporary civilizations; and that said \"cultural imperialism\" is very influential in the international systems of power.\n\nA self-described \"practical Marxist-feminist-deconstructionist\" Gayatri Chakravorty Spivak has published a number of works challenging the \"legacy of colonialism\" including \"A Critique of Postcolonial Reason: Towards a History of the Vanishing Present\" (1999), \"Other Asias\" (2005), and \"Can the Subaltern Speak?\" (1988).\n\nIn \"Can the Subaltern Speak?\" Spivak critiques common representations in the West of the Sati, as being controlled by authors other than the participants (specifically English colonizers and Hindu leaders). Because of this, Spivak argues that the subaltern, referring to the communities that participate in the Sati, are not able to represent themselves through their own voice. Spivak says that cultural imperialism has the power to disqualify or erase the knowledge and mode of education of certain populations that are low on the social hierarchy.\n\nThroughout \"Can the Subaltern Speak?\", Spivak is cites the works of Karl Marx, Michel Foucault, Walter Benjamin, Louis Althusser, Jacques Derrida, and Edward Said, among others.\n\nIn \"A critique of Postcolonial Reason\", Spivak argues that Western philosophy has a history of not only exclusion of the subaltern from discourse, but also does not allow them to occupy the space of a fully human subject.\n\n\"Cultural imperialism\" can refer to either the forced acculturation of a subject population, or to the voluntary embracing of a foreign culture by individuals who do so of their own free will. Since these are two very different referents, the validity of the term has been called into question.\n\nCultural influence can be seen by the \"receiving\" culture as either a threat to or an enrichment of its cultural identity. It seems therefore useful to distinguish between cultural imperialism as an (active or passive) attitude of superiority, and the position of a culture or group that seeks to complement its own cultural production, considered partly deficient, with imported products.\n\nThe imported products or services can themselves represent, or be associated with, certain values (such as consumerism). According to one argument, the \"receiving\" culture does not necessarily perceive this link, but instead absorbs the foreign culture passively through the use of the foreign goods and services. Due to its somewhat concealed, but very potent nature, this hypothetical idea is described by some experts as \"\"banal imperialism\".\" For example, it is argued that while \"American companies are accused of wanting to control 95 percent of the world's consumers\", \"cultural imperialism involves much more than simple consumer goods; it involved the dissemination of American principles such as freedom and democracy\", a process which \"may sound appealing\" but which \"masks a frightening truth: many cultures around the world are disappearing due to the overwhelming influence of corporate and cultural America\".\n\nSome believe that the newly globalised economy of the late 20th and early 21st century has facilitated this process through the use of new information technology. This kind of cultural imperialism is derived from what is called \"soft power\". The theory of electronic colonialism extends the issue to global cultural issues and the impact of major multi-media conglomerates, ranging from Viacom, Time-Warner, Disney, News Corp, to Google and Microsoft with the focus on the hegemonic power of these mainly United States-based communication giants.\n\nOne of the reasons often given for opposing any form of cultural imperialism, voluntary or otherwise, is the preservation of cultural diversity, a goal seen by some as analogous to the preservation of ecological diversity. Proponents of this idea argue either that such diversity is valuable in itself, to preserve human historical heritage and knowledge, or instrumentally valuable because it makes available more ways of solving problems and responding to catastrophes, natural or otherwise.\n\nOf all the areas of the world that scholars have claimed to be adversely affected by imperialism, Africa is probably the most notable. In the expansive \"age of imperialism\" of the nineteenth century, scholars have argued that European colonization in Africa has led to the elimination of many various cultures, worldviews, and epistemologies. This, arguably has led to uneven development, and further informal forms of social control having to do with culture and imperialism. A variety of factors, scholars argue, lead to the elimination of cultures, worldviews, and epistemologies, such as \"de-linguicization\" (replacing native African languages with European ones) and devaluing ontologies that are not explicitly individualistic. One scholar, Ali A. Obdi, claims that imperialism inherently \"involve[s] extensively interactive regimes and heavy contexts of identity deformation, misrecognition, loss of self-esteem, and individual and social doubt in self-efficacy.\"(2000: 12) Therefore, all imperialism would always, already be cultural.\n\nNeoliberalism is often critiqued by sociologists, anthropologists, and cultural studies scholars as being culturally imperialistic. Critics of neoliberalism, at times, claim that it is the newly predominant form of imperialism. Other Scholars, such as Elizabeth Dunn and Julia Elyachar have claimed that neoliberalism requires and creates its own form of governmentality.\n\nIn Dunn's work, \"Privatizing Poland\", she argues that the expansion of the multinational corporation, Gerber, into Poland in the 1990s imposed Western, neoliberal governmentality, ideologies, and epistemologies upon the post-soviet persons hired. Cultural conflicts occurred most notably the company's inherent individualistic policies, such as promoting competition among workers rather than cooperation, and in its strong opposition to what the company owners claimed was bribery.\n\nIn Elyachar's work, \"Markets of Dispossession\", she focuses on ways in which, in Cairo, NGOs along with INGOs and the state promoted neoliberal governmentality through schemas of economic development that relied upon \"youth microentrepreneurs.\" Youth microentrepreneurs would receive small loans to build their own businesses, similar to the way that microfinance supposedly operates. Elyachar argues though, that these programs not only were a failure, but that they shifted cultural opinions of value (personal and cultural) in a way that favored Western ways of thinking and being.\n\nOften, methods of promoting development and social justice to are critiqued as being imperialistic, in a cultural sense. For example, Chandra Mohanty has critiqued Western feminism, claiming that it has created a misrepresentation of the \"third world woman\" as being completely powerless, unable to resist male dominance. Thus, this leads to the often critiqued narrative of the \"white man\" saving the \"brown woman\" from the \"brown man.\" Other, more radical critiques of development studies, have to do with the field of study itself. Some scholars even question the intentions of those developing the field of study, claiming that efforts to \"develop\" the Global South were never about the South itself. Instead, these efforts, it is argued, were made in order to advance Western development and reinforce Western hegemony.\n\nThe core of cultural imperialism thesis is integrated with the political-economy traditional approach in media effects research. Critics of cultural imperialism commonly claim that non-Western cultures, particularly from the Third World, will forsake their traditional values and lose their cultural identities when they are solely exposed to Western media. Nonetheless, Michael B. Salwen, in his book \"Critical Studies in Mass Communication\" (1991), claims that cross-consideration and integration of empirical findings on cultural imperialist influences is very critical in terms of understanding mass media in the international sphere. He recognizes both of contradictory contexts on cultural imperialist impacts. \nThe first context is where cultural imperialism imposes socio-political disruptions on developing nations. Western media can distort images of foreign cultures and provoke personal and social conflicts to developing nations in some cases. \nAnother context is that peoples in developing nations resist to foreign media and preserve their cultural attitudes. Although he admits that outward manifestations of Western culture may be adopted, but the fundamental values and behaviors remain still. Furthermore, positive effects might occur when male-dominated cultures adopt the “liberation” of women with exposure to Western media and it stimulates ample exchange of cultural exchange.\n\nCritics of scholars who discuss cultural imperialism have a number of critiques. \"Cultural imperialism\" is a term that is only used in discussions where cultural relativism and constructivism are generally taken as true. (One cannot critique promoting Western values if one believes that said values are absolutely correct. Similarly, one cannot argue that Western epistemology is unjustly promoted in non-Western societies if one believes that those epistemologies are absolutely correct.) Therefore, those who disagree with cultural relativism and/or constructivism may critique the employment of the term, \"cultural imperialism\" on those terms.\n\nJohn Tomlinson provides a critique of cultural imperialism theory and reveals major problems in the way in which the idea of cultural, as opposed to economic or political, imperialism is formulated. In his book \"Cultural Imperialism: A Critical Introduction\", he delves into the much debated “media imperialism” theory. Summarizing research on the Third World’s reception of American television shows, he challenges the cultural imperialism argument, conveying his doubts about the degree to which US shows in developing nations actually carry US values and improve the profits of US companies. Tomlinson suggests that cultural imperialism is growing in some respects, but local transformation and interpretations of imported media products propose that cultural diversification is not at an end in global society. He explains that one of the fundamental conceptual mistakes of cultural imperialism is to take for granted that the distribution of cultural goods can be considered as cultural dominance. He thus supports his argument highly criticizing the concept that Americanization is occurring through global overflow of American television products. He points to a myriad of examples of television networks who have managed to dominate their domestic markets and that domestic programs generally top the ratings. He also doubts the concept that cultural agents are passive receivers of information. He states that movement between cultural/geographical areas always involves translation, mutation, adaptation, and the creation of hybridity.\n\nOther major critiques are that the term is not defined well, and employs further terms that are not defined well, and therefore lacks explanatory power, that \"cultural imperialism\" is hard to measure, and that the theory of a legacy of colonialism is not always true.\n\nDavid Rothkopf, managing director of Kissinger Associates and an adjunct professor of international affairs at Columbia University (who also served as a senior US Commerce Department official in the Clinton Administration), wrote about cultural imperialism in his provocatively titled \"In Praise of Cultural Imperialism?\" in the summer 1997 issue of \"Foreign Policy\" magazine. Rothkopf says that the United States should embrace \"cultural imperialism\" as in its self-interest. But his definition of cultural imperialism stresses spreading the values of tolerance and openness to cultural change in order to avoid war and conflict between cultures as well as expanding accepted technological and legal standards to provide free traders with enough security to do business with more countries. Rothkopf's definition almost exclusively involves allowing individuals in other nations to accept or reject foreign cultural influences. He also mentions, but only in passing, the use of the English language and consumption of news and popular music and film as cultural dominance that he supports. Rothkopf additionally makes the point that globalization and the Internet are accelerating the process of cultural influence.\n\nCulture is sometimes used by the organizers of society — politicians, theologians, academics, and families — to impose and ensure order, the rudiments of which change over time as need dictates. One need only look at the 20th century's genocides. In each one, leaders used culture as a political front to fuel the passions of their armies and other minions and to justify their actions among their people.\n\nRothkopf then cites genocide and s in Armenia, Russia, the Holocaust, Cambodia, Bosnia and Herzegovina, Rwanda and East Timor as examples of culture (in some cases expressed in the ideology of \"political culture\" or religion) being misused to justify violence. He also acknowledges that cultural imperialism in the past has been guilty of forcefully eliminating the cultures of natives in the Americas and in Africa, or through use of the Inquisition, \"\"and during the expansion of virtually every empire.\"\".The most important way to deal with cultural influence in any nation, according to Rothkopf, is to promote tolerance and allow, or even promote, cultural diversities that are compatible with tolerance and to eliminate those cultural differences that cause violent conflict:\n\nAlthough the term was popularized in the 1960s, and was used by its original proponents to refer to cultural hegemonies in a post-colonial world, cultural imperialism has also been used to refer to times further in the past.\n\nThe Roman empire has been seen as an early example of cultural imperialism.\n\nEarly Rome, in its conquest of Italy, assimilated the people of Etruria by replacing the Etruscan language with Latin, which led to the demise of that language and many aspects of Etruscan civilization.\n\nCultural Romanization was imposed on many parts of Rome's empire by \"many regions receiving Roman culture unwillingly, as a form of cultural imperialism.\" For example, when Greece was conquered by the Roman armies, Rome set about altering the culture of Greece to conform with Roman ideals. For instance, the Greek habit of stripping naked, in public, for exercise, was looked on askance by Roman writers, who considered the practice to be a cause of the Greeks' effeminacy and enslavement.\n\nThe Pax Romana was secured in the empire, in part, by the \"forced acculturation of the culturally diverse populations that Rome had conquered.\"\n\nBritish worldwide expansion in the 18th and 19th centuries was an economic and political phenomenon. However, \"there was also a strong social and cultural dimension to it, which Rudyard Kipling termed the 'white man's burden'.\" One of the ways this was carried out was by religious proselytising, by, amongst others, the London Missionary Society, which was \"an agent of British cultural imperialism.\" Another way, was by the imposition of educational material on the colonies for an \"imperial curriculum\". Morag Bell writes, \"The promotion of empire through books, illustrative materials, and educational syllabuses was widespread, part of an education policy geared to cultural imperialism\". This was also true of science and technology in the empire. Douglas M. Peers and Nandini Gooptu note that \"Most scholars of colonial science in India now prefer to stress the ways in which science and technology worked in the service of colonialism, as both a 'tool of empire' in the practical sense and as a vehicle for cultural imperialism. In other words, science developed in India in ways that reflected colonial priorities, tending to benefit Europeans at the expense of Indians, while remaining dependent on and subservient to scientific authorities in the colonial metropolis.\"\n\nThe analysis of cultural imperialism carried out by Edward Said drew principally from a study of the British Empire. According to Danilo Raponi, the cultural imperialism of the British in the 19th century had a much wider effect than only in the British Empire. He writes, \"To paraphrase Said, I see cultural imperialism as a complex cultural hegemony of a country, Great Britain, that in the 19th century had no rivals in terms of its ability to project its power across the world and to influence the cultural, political and commercial affairs of most countries. It is the 'cultural hegemony' of a country whose power to export the most fundamental ideas and concepts at the basis of its understanding of 'civilisation' knew practically no bounds.\" In this, for example, Raponi includes Italy.\n\nThe New Cambridge Modern History writes about the cultural imperialism of Napoleonic France. Napoleon used the Institut de France \"as an instrument for transmuting French universalism into cultural imperialism.\" Members of the Institute (who included Napoleon), descended upon Egypt in 1798. \"Upon arrival they organised themselves into an Institute of Cairo. The Rosetta Stone is their most famous find. The science of Egyptology is their legacy.\"\n\nAfter the First World War, Germans were worried about the extent of French influence in the annexed Rhineland, with the French occupation of the Ruhr Valley in 1923. An early use of the term appeared in an essay by Paul Ruhlmann (as \"Peter Hartmann\") at that date, entitled \"French Cultural Imperialism on the Rhine\".\n\n\"Cultural imperialism\" has also been used in connection with the expansion of German influence under the Nazis in the middle of the twentieth century. Alan Steinweis and Daniel Rogers note that even before the Nazis came to power, \"Already in the Weimar Republic, German academic specialists on eastern Europe had contributed through their publications and teaching to the legitimization Of German territorial revanchism and cultural imperialism. These scholars operated primarily in the disciplines Of history, economics, geography, and literature.\"\n\nIn the area of music, Michael Kater writes that during the WWII German occupation of France, Hans Rosbaud, a German conductor based by the Nazi regime in Strasbourg, became \"at least nominally, a servant of Nazi cultural imperialism directed against the French.\"\n\nIn Italy during the war, Germany pursued \"a European cultural front that gravitates around German culture\". The Nazi propaganda minister Joseph Goebbels set up the European Union of Writers, \"one of Goebbels's most ambitious projects for Nazi cultural hegemony. Presumably a means of gathering authors from Germany, Italy, and the occupied countries to plan the literary life of the new Europe, the union soon emerged as a vehicle of German cultural imperialism.\"\n\nFor other parts of Europe, Robert Gerwarth, writing about cultural imperialism and Reinhard Heydrich, states that the \"Nazis' Germanization project was based on a historically unprecedented programme of racial stock-taking, theft, expulsion and murder.\" Also, \"The full integration of the [Czech] Protectorate into this New Order required the complete Germanization of the Protectorate's cultural life and the eradication of indigenous Czech and Jewish culture.\"\n\nThe actions by Nazi Germany reflect on the notion of race and culture playing a significant role in imperialism. The idea that there is a distinction between the Germans and the Jews has created the illusion of Germans believing they were superior to the Jewish inferiors, the notion of us/them and self/others.\n\n\n\n"}
{"id": "6271", "url": "https://en.wikipedia.org/wiki?curid=6271", "title": "Chemical reaction", "text": "Chemical reaction\n\nA chemical reaction is a process that leads to the transformation of one set of chemical substances to another. Classically, chemical reactions encompass changes that only involve the positions of electrons in the forming and breaking of chemical bonds between atoms, with no change to the nuclei (no change to the elements present), and can often be described by a chemical equation. Nuclear chemistry is a sub-discipline of chemistry that involves the chemical reactions of unstable and radioactive elements where both electronic and nuclear changes can occur.\n\nThe substance (or substances) initially involved in a chemical reaction are called reactants or reagents. Chemical reactions are usually characterized by a chemical change, and they yield one or more products, which usually have properties different from the reactants. Reactions often consist of a sequence of individual sub-steps, the so-called elementary reactions, and the information on the precise course of action is part of the reaction mechanism. Chemical reactions are described with chemical equations, which symbolically present the starting materials, end products, and sometimes intermediate products and reaction conditions.\n\nChemical reactions happen at a characteristic reaction rate at a given temperature and chemical concentration. Typically, reaction rates increase with increasing temperature because there is more thermal energy available to reach the activation energy necessary for breaking bonds between atoms.\n\nReactions may proceed in the forward or reverse direction until they go to completion or reach equilibrium. Reactions that proceed in the forward direction to approach equilibrium are often described as spontaneous, requiring no input of free energy to go forward. Non-spontaneous reactions require input of free energy to go forward (examples include charging a battery by applying an external electrical power source, or photosynthesis driven by absorption of electromagnetic radiation in the form of sunlight).\n\nDifferent chemical reactions are used in combinations during chemical synthesis in order to obtain a desired product. In biochemistry, a consecutive series of chemical reactions (where the product of one reaction is the reactant of the next reaction) form metabolic pathways. These reactions are often catalyzed by protein enzymes. Enzymes increase the rates of biochemical reactions, so that metabolic syntheses and decompositions impossible under ordinary conditions can occur at the temperatures and concentrations present within a cell.\n\nThe general concept of a chemical reaction has been extended to reactions between entities smaller than atoms, including nuclear reactions, radioactive decays, and reactions between elementary particles as described by quantum field theory.\n\nChemical reactions such as combustion in fire, fermentation and the reduction of ores to metals were known since antiquity. Initial theories of transformation of materials were developed by Greek philosophers, such as the Four-Element Theory of Empedocles stating that any substance is composed of the four basic elements – fire, water, air and earth. In the Middle Ages, chemical transformations were studied by Alchemists. They attempted, in particular, to convert lead into gold, for which purpose they used reactions of lead and lead-copper alloys with sulfur.\n\nThe production of chemical substances that do not normally occur in nature has long been tried, such as the synthesis of sulfuric and nitric acids attributed to the controversial alchemist Jābir ibn Hayyān. The process involved heating of sulfate and nitrate minerals such as copper sulfate, alum and saltpeter. In the 17th century, Johann Rudolph Glauber produced hydrochloric acid and sodium sulfate by reacting sulfuric acid and sodium chloride. With the development of the lead chamber process in 1746 and the Leblanc process, allowing large-scale production of sulfuric acid and sodium carbonate, respectively, chemical reactions became implemented into the industry. Further optimization of sulfuric acid technology resulted in the contact process in the 1880s, and the Haber process was developed in 1909–1910 for ammonia synthesis.\n\nFrom the 16th century, researchers including Jan Baptist van Helmont, Robert Boyle and Isaac Newton tried to establish theories of the experimentally observed chemical transformations. The phlogiston theory was proposed in 1667 by Johann Joachim Becher. It postulated the existence of a fire-like element called \"phlogiston\", which was contained within combustible bodies and released during combustion. This proved to be false in 1785 by Antoine Lavoisier who found the correct explanation of the combustion as reaction with oxygen from the air.\n\nJoseph Louis Gay-Lussac recognized in 1808 that gases always react in a certain relationship with each other. Based on this idea and the atomic theory of John Dalton, Joseph Proust had developed the law of definite proportions, which later resulted in the concepts of stoichiometry and chemical equations.\n\nRegarding the organic chemistry, it was long believed that compounds obtained from living organisms were too complex to be obtained synthetically. According to the concept of vitalism, organic matter was endowed with a \"vital force\" and distinguished from inorganic materials. This separation was ended however by the synthesis of urea from inorganic precursors by Friedrich Wöhler in 1828. Other chemists who brought major contributions to organic chemistry include Alexander William Williamson with his synthesis of ethers and Christopher Kelk Ingold, who, among many discoveries, established the mechanisms of substitution reactions.\n\nChemical equations are used to graphically illustrate chemical reactions. They consist of chemical or structural formulas of the reactants on the left and those of the products on the right. They are separated by an arrow (→) which indicates the direction and type of the reaction; the arrow is read as the word \"yields\". The tip of the arrow points in the direction in which the reaction proceeds. A double arrow () pointing in opposite directions is used for equilibrium reactions. Equations should be balanced according to the stoichiometry, the number of atoms of each species should be the same on both sides of the equation. This is achieved by scaling the number of involved molecules (<ce>A, B, C</ce> and <ce>D</ce> in a schematic example below) by the appropriate integers \"a, b, c\" and \"d\".\n\nMore elaborate reactions are represented by reaction schemes, which in addition to starting materials and products show important intermediates or transition states. Also, some relatively minor additions to the reaction can be indicated above the reaction arrow; examples of such additions are water, heat, illumination, a catalyst, etc. Similarly, some minor products can be placed below the arrow, often with a minus sign.\nRetrosynthetic analysis can be applied to design a complex synthesis reaction. Here the analysis starts from the products, for example by splitting selected chemical bonds, to arrive at plausible initial reagents. A special arrow (⇒) is used in retro reactions.\n\nThe elementary reaction is the smallest division into which a chemical reaction can be decomposed, it has no intermediate products. Most experimentally observed reactions are built up from many elementary reactions that occur in parallel or sequentially. The actual sequence of the individual elementary reactions is known as reaction mechanism. An elementary reaction involves a few molecules, usually one or two, because of the low probability for several molecules to meet at a certain time.\nThe most important elementary reactions are unimolecular and bimolecular reactions. Only one molecule is involved in a unimolecular reaction; it is transformed by an isomerization or a dissociation into one or more other molecules. Such reactions require the addition of energy in the form of heat or light. A typical example of a unimolecular reaction is the cis–trans isomerization, in which the cis-form of a compound converts to the trans-form or vice versa.\n\nIn a typical dissociation reaction, a bond in a molecule splits (ruptures) resulting in two molecular fragments. The splitting can be homolytic or heterolytic. In the first case, the bond is divided so that each product retains an electron and becomes a neutral radical. In the second case, both electrons of the chemical bond remain with one of the products, resulting in charged ions. Dissociation plays an important role in triggering chain reactions, such as hydrogen–oxygen or polymerization reactions.\n\nFor bimolecular reactions, two molecules collide and react with each other. Their merger is called chemical synthesis or an addition reaction.\nAnother possibility is that only a portion of one molecule is transferred to the other molecule. This type of reaction occurs, for example, in redox and acid-base reactions. In redox reactions, the transferred particle is an electron, whereas in acid-base reactions it is a proton. This type of reaction is also called metathesis.\nfor example\n\nMost chemical reactions are reversible, that is they can and do run in both directions. The forward and reverse reactions are competing with each other and differ in reaction rates. These rates depend on the concentration and therefore change with time of the reaction: the reverse rate gradually increases and becomes equal to the rate of the forward reaction, establishing the so-called chemical equilibrium. The time to reach equilibrium depends on such parameters as temperature, pressure and the materials involved, and is determined by the minimum free energy. In equilibrium, the Gibbs free energy must be zero. The pressure dependence can be explained with the Le Chatelier's principle. For example, an increase in pressure due to decreasing volume causes the reaction to shift to the side with the fewer moles of gas.\n\nThe reaction yield stabilizes at equilibrium, but can be increased by removing the product from the reaction mixture or changed by increasing the temperature or pressure. A change in the concentrations of the reactants does not affect the equilibrium constant, but does affect the equilibrium position.\n\nChemical reactions are determined by the laws of thermodynamics. Reactions can proceed by themselves if they are exergonic, that is if they release energy. The associated free energy of the reaction is composed of two different thermodynamic quantities, enthalpy and entropy:\n\nReactions can be exothermic, where ΔH is negative and energy is released. Typical examples of exothermic reactions are precipitation and crystallization, in which ordered solids are formed from disordered gaseous or liquid phases. In contrast, in endothermic reactions, heat is consumed from the environment. This can occur by increasing the entropy of the system, often through the formation of gaseous reaction products, which have high entropy. Since the entropy increases with temperature, many endothermic reactions preferably take place at high temperatures. On the contrary, many exothermic reactions such as crystallization occur at low temperatures. Changes in temperature can sometimes reverse the sign of the enthalpy of a reaction, as for the carbon monoxide reduction of molybdenum dioxide:\nThis reaction to form carbon dioxide and molybdenum is endothermic at low temperatures, becoming less so with increasing temperature. ΔH° is zero at , and the reaction becomes exothermic above that temperature.\n\nChanges in temperature can also reverse the direction tendency of a reaction. For example, the water gas shift reaction\nis favored by low temperatures, but its reverse is favored by high temperature. The shift in reaction direction tendency occurs at .\n\nReactions can also be characterized by the internal energy which takes into account changes in the entropy, volume and chemical potential. The latter depends, among other things, on the activities of the involved substances.\n\nThe speed at which reactions takes place is studied by reaction kinetics. The rate depends on various parameters, such as:\n\n\nSeveral theories allow calculating the reaction rates at the molecular level. This field is referred to as reaction dynamics. The rate \"v\" of a first-order reaction, which could be disintegration of a substance A, is given by:\n\nIts integration yields:\n\nHere k is first-order rate constant having dimension 1/time, [A](t) is concentration at a time \"t\" and [A] is the initial concentration. The rate of a first-order reaction depends only on the concentration and the properties of the involved substance, and the reaction itself can be described with the characteristic half-life. More than one time constant is needed when describing reactions of higher order. The temperature dependence of the rate constant usually follows the Arrhenius equation:\n\nwhere E is the activation energy and k is the Boltzmann constant. One of the simplest models of reaction rate is the collision theory. More realistic models are tailored to a specific problem and include the transition state theory, the calculation of the potential energy surface, the Marcus theory and the Rice–Ramsperger–Kassel–Marcus (RRKM) theory.\n\nIn a synthesis reaction, two or more simple substances combine to form a more complex substance. These reactions are in the general form:\n\nTwo or more reactants yielding one product is another way to identify a synthesis reaction. One example of a synthesis reaction is the combination of iron and sulfur to form iron(II) sulfide:\n\nAnother example is simple hydrogen gas combined with simple oxygen gas to produce a more complex substance, such as water.\n\nA decomposition reaction is when a more complex substance breaks down into its more simple parts. It is thus the opposite of a synthesis reaction, and can be written as\n\nOne example of a decomposition reaction is the electrolysis of water to make oxygen and hydrogen gas:\n\nIn a single replacement reaction, a single uncombined element replaces another in a compound; in other words, one element trades places with another element in a compound These reactions come in the general form of:\n\nOne example of a single displacement reaction is when magnesium replaces hydrogen in water to make magnesium hydroxide and hydrogen gas:\n\nIn a double replacement reaction, the anions and cations of two compounds switch places and form two entirely different compounds. These reactions are in the general form:\n\nFor example, when barium chloride (BaCl) and magnesium sulfate (MgSO) react, the SO anion switches places with the 2Cl anion, giving the compounds BaSO and MgCl.\n\nAnother example of a double displacement reaction is the reaction of lead(II) nitrate with potassium iodide to form lead(II) iodide and potassium nitrate:\n\n \nRedox reactions can be understood in terms of transfer of electrons from one involved species (reducing agent) to another (oxidizing agent). In this process, the former species is \"oxidized\" and the latter is \"reduced\". Though sufficient for many purposes, these descriptions are not precisely correct. Oxidation is better defined as an increase in oxidation state, and reduction as a decrease in oxidation state. In practice, the transfer of electrons will always change the oxidation state, but there are many reactions that are classed as \"redox\" even though no electron transfer occurs (such as those involving covalent bonds).\n\nIn the following redox reaction, hazardous sodium metal reacts with toxic chlorine gas to form the ionic compound sodium chloride, or common table salt:\n\nIn the reaction, sodium metal goes from an oxidation state of 0 (as it is a pure element) to +1: in other words, the sodium lost one electron and is said to have been oxidized. On the other hand, the chlorine gas goes from an oxidation of 0 (it is also a pure element) to −1: the chlorine gains one electron and is said to have been reduced. Because the chlorine is the one reduced, it is considered the electron acceptor, or in other words, induces oxidation in the sodium – thus the chlorine gas is considered the oxidizing agent. Conversely, the sodium is oxidized or is the electron donor, and thus induces reduction in the other species and is considered the \"reducing agent\".\n\nWhich of the involved reactants would be reducing or oxidizing agent can be predicted from the electronegativity of their elements. Elements with low electronegativity, such as most metals, easily donate electrons and oxidize – they are reducing agents. On the contrary, many ions with high oxidation numbers, such as , , , , can gain one or two extra electrons and are strong oxidizing agents.\n\nThe number of electrons donated or accepted in a redox reaction can be predicted from the electron configuration of the reactant element. Elements try to reach the low-energy noble gas configuration, and therefore alkali metals and halogens will donate and accept one electron respectively. Noble gases themselves are chemically inactive.\n\nAn important class of redox reactions are the electrochemical reactions, where electrons from the power supply are used as the reducing agent. These reactions are particularly important for the production of chemical elements, such as chlorine or aluminium. The reverse process in which electrons are released in redox reactions and can be used as electrical energy is possible and used in batteries.\n\nIn complexation reactions, several ligands react with a metal atom to form a coordination complex. This is achieved by providing lone pairs of the ligand into empty orbitals of the metal atom and forming dipolar bonds. The ligands are Lewis bases, they can be both ions and neutral molecules, such as carbon monoxide, ammonia or water. The number of ligands that react with a central metal atom can be found using the 18-electron rule, saying that the valence shells of a transition metal will collectively accommodate 18 electrons, whereas the symmetry of the resulting complex can be predicted with the crystal field theory and ligand field theory. Complexation reactions also include ligand exchange, in which one or more ligands are replaced by another, and redox processes which change the oxidation state of the central metal atom.\n\nIn the Brønsted–Lowry acid–base theory, an acid-base reaction involves a transfer of protons (H) from one species (the acid) to another (the base). When a proton is removed from an acid, the resulting species is termed that acid's conjugate base. When the proton is accepted by a base, the resulting species is termed that base's conjugate acid. In other words, acids act as proton donors and bases act as proton acceptors according to the following equation:\n\nThe reverse reaction is possible, and thus the acid/base and conjugated base/acid are always in equilibrium. The equilibrium is determined by the acid and base dissociation constants (\"K\" and \"K\") of the involved substances. A special case of the acid-base reaction is the neutralization where an acid and a base, taken at exactly same amounts, form a neutral salt.\n\nAcid-base reactions can have different definitions depending on the acid-base concept employed. Some of the most common are:\n\nPrecipitation is the formation of a solid in a solution or inside another solid during a chemical reaction. It usually takes place when the concentration of dissolved ions exceeds the solubility limit and forms an insoluble salt. This process can be assisted by adding a precipitating agent or by removal of the solvent. Rapid precipitation results in an amorphous or microcrystalline residue and slow process can yield single crystals. The latter can also be obtained by recrystallization from microcrystalline salts.\n\nReactions can take place between two solids. However, because of the relatively small diffusion rates in solids, the corresponding chemical reactions are very slow in comparison to liquid and gas phase reactions. They are accelerated by increasing the reaction temperature and finely dividing the reactant to increase the contacting surface area.\n\nReaction can take place at the solid|gas interface, surfaces at very low pressure such as ultra-high vacuum. Via scanning tunneling microscopy, it is possible to observe reactions at the solid|gas interface in real space, if the time scale of the reaction is in the correct range. Reactions at the solid|gas interface are in some cases related to catalysis.\n\nIn photochemical reactions, atoms and molecules absorb energy (photons) of the illumination light and convert into an excited state. They can then release this energy by breaking chemical bonds, thereby producing radicals. Photochemical reactions include hydrogen–oxygen reactions, radical polymerization, chain reactions and rearrangement reactions.\n\nMany important processes involve photochemistry. The premier example is photosynthesis, in which most plants use solar energy to convert carbon dioxide and water into glucose, disposing of oxygen as a side-product. Humans rely on photochemistry for the formation of vitamin D, and vision is initiated by a photochemical reaction of rhodopsin. In fireflies, an enzyme in the abdomen catalyzes a reaction that results in bioluminescence. Many significant photochemical reactions, such as ozone formation, occur in the Earth atmosphere and constitute atmospheric chemistry.\n\nIn catalysis, the reaction does not proceed directly, but through reaction with a third substance known as catalyst. Although the catalyst takes part in the reaction, it is returned to its original state by the end of the reaction and so is not consumed. However, it can be inhibited, deactivated or destroyed by secondary processes. Catalysts can be used in a different phase (heterogeneous) or in the same phase (homogeneous) as the reactants. In heterogeneous catalysis, typical secondary processes include coking where the catalyst becomes covered by polymeric side products. Additionally, heterogeneous catalysts can dissolve into the solution in a solid–liquid system or evaporate in a solid–gas system. Catalysts can only speed up the reaction – chemicals that slow down the reaction are called inhibitors. Substances that increase the activity of catalysts are called promoters, and substances that deactivate catalysts are called catalytic poisons. With a catalyst, a reaction which is kinetically inhibited by a high activation energy can take place in circumvention of this activation energy.\n\nHeterogeneous catalysts are usually solids, powdered in order to maximize their surface area. Of particular importance in heterogeneous catalysis are the platinum group metals and other transition metals, which are used in hydrogenations, catalytic reforming and in the synthesis of commodity chemicals such as nitric acid and ammonia. Acids are an example of a homogeneous catalyst, they increase the nucleophilicity of carbonyls, allowing a reaction that would not otherwise proceed with electrophiles. The advantage of homogeneous catalysts is the ease of mixing them with the reactants, but they may also be difficult to separate from the products. Therefore, heterogeneous catalysts are preferred in many industrial processes.\n\nIn organic chemistry, in addition to oxidation, reduction or acid-base reactions, a number of other reactions can take place which involve covalent bonds between carbon atoms or carbon and heteroatoms (such as oxygen, nitrogen, halogens, etc.). Many specific reactions in organic chemistry are name reactions designated after their discoverers.\n\nIn a substitution reaction, a functional group in a particular chemical compound is replaced by another group. These reactions can be distinguished by the type of substituting species into a nucleophilic, electrophilic or radical substitution.\nIn the first type, a nucleophile, an atom or molecule with an excess of electrons and thus a negative charge or partial charge, replaces another atom or part of the \"substrate\" molecule. The electron pair from the nucleophile attacks the substrate forming a new bond, while the leaving group departs with an electron pair. The nucleophile may be electrically neutral or negatively charged, whereas the substrate is typically neutral or positively charged. Examples of nucleophiles are hydroxide ion, alkoxides, amines and halides. This type of reaction is found mainly in aliphatic hydrocarbons, and rarely in aromatic hydrocarbon. The latter have high electron density and enter nucleophilic aromatic substitution only with very strong electron withdrawing groups. Nucleophilic substitution can take place by two different mechanisms, S1 and S2. In their names, S stands for substitution, N for nucleophilic, and the number represents the kinetic order of the reaction, unimolecular or bimolecular.\nThe S1 reaction proceeds in two steps. First, the leaving group is eliminated creating a carbocation. This is followed by a rapid reaction with the nucleophile.\n\nIn the S2 mechanism, the nucleophile forms a transition state with the attacked molecule, and only then the leaving group is cleaved. These two mechanisms differ in the stereochemistry of the products. S1 leads to the non-stereospecific addition and does not result in a chiral center, but rather in a set of geometric isomers (\"cis/trans\"). In contrast, a reversal (Walden inversion) of the previously existing stereochemistry is observed in the S2 mechanism.\n\nElectrophilic substitution is the counterpart of the nucleophilic substitution in that the attacking atom or molecule, an electrophile, has low electron density and thus a positive charge. Typical electrophiles are the carbon atom of carbonyl groups, carbocations or sulfur or nitronium cations. This reaction takes place almost exclusively in aromatic hydrocarbons, where it is called electrophilic aromatic substitution. The electrophile attack results in the so-called σ-complex, a transition state in which the aromatic system is abolished. Then, the leaving group, usually a proton, is split off and the aromaticity is restored. An alternative to aromatic substitution is electrophilic aliphatic substitution. It is similar to the nucleophilic aliphatic substitution and also has two major types, S1 and S2\n\nIn the third type of substitution reaction, radical substitution, the attacking particle is a radical. This process usually takes the form of a chain reaction, for example in the reaction of alkanes with halogens. In the first step, light or heat disintegrates the halogen-containing molecules producing the radicals. Then the reaction proceeds as an avalanche until two radicals meet and recombine.\n\nThe addition and its counterpart, the elimination, are reactions which change the number of substitutents on the carbon atom, and form or cleave multiple bonds. Double and triple bonds can be produced by eliminating a suitable leaving group. Similar to the nucleophilic substitution, there are several possible reaction mechanisms which are named after the respective reaction order. In the E1 mechanism, the leaving group is ejected first, forming a carbocation. The next step, formation of the double bond, takes place with elimination of a proton (deprotonation). The leaving order is reversed in the E1cb mechanism, that is the proton is split off first. This mechanism requires participation of a base. Because of the similar conditions, both reactions in the E1 or E1cb elimination always compete with the S1 substitution.\n\nThe E2 mechanism also requires a base, but there the attack of the base and the elimination of the leaving group proceed simultaneously and produce no ionic intermediate. In contrast to the E1 eliminations, different stereochemical configurations are possible for the reaction product in the E2 mechanism, because the attack of the base preferentially occurs in the anti-position with respect to the leaving group. Because of the similar conditions and reagents, the E2 elimination is always in competition with the S2-substitution.\nThe counterpart of elimination is the addition where double or triple bonds are converted into single bonds. Similar to the substitution reactions, there are several types of additions distinguished by the type of the attacking particle. For example, in the electrophilic addition of hydrogen bromide, an electrophile (proton) attacks the double bond forming a carbocation, which then reacts with the nucleophile (bromine). The carbocation can be formed on either side of the double bond depending on the groups attached to its ends, and the preferred configuration can be predicted with the Markovnikov's rule. This rule states that \"In the heterolytic addition of a polar molecule to an alkene or alkyne, the more electronegative (nucleophilic) atom (or part) of the polar molecule becomes attached to the carbon atom bearing the smaller number of hydrogen atoms.\"\n\nIf the addition of a functional group takes place at the less substituted carbon atom of the double bond, then the electrophilic substitution with acids is not possible. In this case, one has to use the hydroboration–oxidation reaction, where in the first step, the boron atom acts as electrophile and adds to the less substituted carbon atom. At the second step, the nucleophilic hydroperoxide or halogen anion attacks the boron atom.\n\nWhile the addition to the electron-rich alkenes and alkynes is mainly electrophilic, the nucleophilic addition plays an important role for the carbon-heteroatom multiple bonds, and especially its most important representative, the carbonyl group. This process is often associated with an elimination, so that after the reaction the carbonyl group is present again. It is therefore called addition-elimination reaction and may occur in carboxylic acid derivatives such as chlorides, esters or anhydrides. This reaction is often catalyzed by acids or bases, where the acids increase by the electrophilicity of the carbonyl group by binding to the oxygen atom, whereas the bases enhance the nucleophilicity of the attacking nucleophile.\n\nNucleophilic addition of a carbanion or another nucleophile to the double bond of an alpha, beta unsaturated carbonyl compound can proceed via the Michael reaction, which belongs to the larger class of conjugate additions. This is one of the most useful methods for the mild formation of C–C bonds.\n\nSome additions which can not be executed with nucleophiles and electrophiles, can be succeeded with free radicals. As with the free-radical substitution, the radical addition proceeds as a chain reaction, and such reactions are the basis of the free-radical polymerization.\n\nIn a rearrangement reaction, the carbon skeleton of a molecule is rearranged to give a structural isomer of the original molecule. These include hydride shift reactions such as the Wagner-Meerwein rearrangement, where a hydrogen, alkyl or aryl group migrates from one carbon to a neighboring carbon. Most rearrangements are associated with the breaking and formation of new carbon-carbon bonds. Other examples are sigmatropic reaction such as the Cope rearrangement.\n\nCyclic rearrangements include cycloadditions and, more generally, pericyclic reactions, wherein two or more double bond-containing molecules form a cyclic molecule. An important example of cycloaddition reaction is the Diels–Alder reaction (the so-called [4+2] cycloaddition) between a conjugated diene and a substituted alkene to form a substituted cyclohexene system.\n\nWhether a certain cycloaddition would proceed depends on the electronic orbitals of the participating species, as only orbitals with the same sign of wave function will overlap and interact constructively to form new bonds. Cycloaddition is usually assisted by light or heat. These perturbations result in different arrangement of electrons in the excited state of the involved molecules and therefore in different effects. For example, the [4+2] Diels-Alder reactions can be assisted by heat whereas the [2+2] cycloaddition is selectively induced by light. Because of the orbital character, the potential for developing stereoisomeric products upon cycloaddition is limited, as described by the Woodward–Hoffmann rules.\n\nBiochemical reactions are mainly controlled by enzymes. These proteins can specifically catalyze a single reaction, so that reactions can be controlled very precisely. The reaction takes place in the active site, a small part of the enzyme which is usually found in a cleft or pocket lined by amino acid residues, and the rest of the enzyme is used mainly for stabilization. The catalytic action of enzymes relies on several mechanisms including the molecular shape (\"induced fit\"), bond strain, proximity and orientation of molecules relative to the enzyme, proton donation or withdrawal (acid/base catalysis), electrostatic interactions and many others.\n\nThe biochemical reactions that occur in living organisms are collectively known as metabolism. Among the most important of its mechanisms is the anabolism, in which different DNA and enzyme-controlled processes result in the production of large molecules such as proteins and carbohydrates from smaller units. Bioenergetics studies the sources of energy for such reactions. An important energy source is glucose, which can be produced by plants via photosynthesis or assimilated from food. All organisms use this energy to produce adenosine triphosphate (ATP), which can then be used to energize other reactions.\n\nChemical reactions are central to chemical engineering where they are used for the synthesis of new compounds from natural raw materials such as petroleum and mineral ores. It is essential to make the reaction as efficient as possible, maximizing the yield and minimizing the amount of reagents, energy inputs and waste. Catalysts are especially helpful for reducing the energy required for the reaction and increasing its reaction rate.\n\nSome specific reactions have their niche applications. For example, the thermite reaction is used to generate light and heat in pyrotechnics and welding. Although it is less controllable than the more conventional oxy-fuel welding, arc welding and flash welding, it requires much less equipment and is still used to mend rails, especially in remote areas.\n\nMechanisms of monitoring chemical reactions depend strongly on the reaction rate. Relatively slow processes can be analyzed in situ for the concentrations and identities of the individual ingredients. Important tools of real time analysis are the measurement of pH and analysis of optical absorption (color) and emission spectra. A less accessible but rather efficient method is introduction of a radioactive isotope into the reaction and monitoring how it changes over time and where it moves to; this method is often used to analyze redistribution of substances in the human body. Faster reactions are usually studied with ultrafast laser spectroscopy where utilization of femtosecond lasers allows short-lived transition states to be monitored at time scaled down to a few femtoseconds.\n\n\n"}
{"id": "6272", "url": "https://en.wikipedia.org/wiki?curid=6272", "title": "Charleston", "text": "Charleston\n\nCharleston most commonly refers to:\n\n\nCharleston may also refer to:\n\nIn Australia:\nIn Canada:\n\nIn New Zealand:\n\nIn the United Kingdom:\n\nIn the United States:\n\n\n\n\n\n\n\n"}
{"id": "6276", "url": "https://en.wikipedia.org/wiki?curid=6276", "title": "Casiquiare canal", "text": "Casiquiare canal\n\nThe Casiquiare river is a distributary of the upper Orinoco flowing southward into the Rio Negro, in Venezuela, South America. As such, it forms a unique natural canal between the Orinoco and Amazon river systems. It is the world's largest river of the kind that links two major river systems, a so-called bifurcation. The area forms a water divide, more dramatically at regional flood stage.\n\nIn 1744 a Jesuit priest named Father Roman, while ascending the Orinoco River, met some Portuguese slave-traders from the settlements on the Rio Negro. He accompanied them on their return, by way of the Casiquiare canal, and afterwards retraced his route to the Orinoco. Charles Marie de La Condamine, seven months later, was able to give to the \"Académie française\" an account of Father Roman's voyage, and thus confirm the existence of this waterway, first reported by Father Acuña in 1639.\n\nLittle credence was given to Father Roman's statement until it was verified, in 1756, by the Spanish Boundary-line Commission of Yturriaga and Solano. In 1800 German scientist Alexander von Humboldt and French botanist Aimé Bonpland explored the river. During a 1924–25 expedition, Alexander H. Rice, Jr. of Harvard University traveled up the Orinoco, traversed the Casiquiare canal, and descended the Rio Negro to the Amazon at Manaus. It was the first expedition to use aerial photography and shortwave radio for mapping of the region. In 1968 the Casiquiare was navigated by an SRN6 hovercraft during a National Geographic expedition.\n\nThe origin of the Casiquiare, at the River Orinoco, is below the mission of La Esmeralda at , and about above sea level. Its mouth at the Rio Negro, an affluent of the Amazon River, is near the town of San Carlos and is above sea level.\n\nThe general course is south-west, and its length, including windings, is about . Its width, at its bifurcation with the Orinoco, is approximately , with a current towards the Rio Negro of . However, as it gains in volume from the very numerous tributary streams, large and small, that it receives en route, its velocity increases, and in the wet season reaches , even in certain stretches. It broadens considerably as it approaches its mouth, where it is about wide. The volume of water the Casiquiare captures from the Orinoco is small in comparison to what it accumulates in its course.\n\nIn flood time, it is said to have a second connection with the Rio Negro by a branch, which it throws off to the westward, called the Itinivini, which leaves it at a point about above its mouth. In the dry season, it has shallows, and is obstructed by sandbanks, a few rapids and granite rocks. Its shores are densely wooded, and the soil more fertile than that along the Rio Negro. The general slope of the plains through which the canal runs is south-west, but those of the Rio Negro slope south-east.\n\nThe Casiquiare is not a sluggish canal on a flat tableland, but a great, rapid river which, if its upper waters had not found contact with the Orinoco, perhaps by cutting back, would belong entirely to the Negro branch of the Amazon.\n\nTo the west of the Casiquiare, there is a much shorter and easier portage between the Orinoco and Amazon basins, called the isthmus of Pimichin, which is reached by ascending the Terni branch of the Atabapo River, an affluent of the Orinoco. Although the Terni is somewhat obstructed, it is believed that it could easily be made navigable for small craft. The isthmus is across, with undulating ground, nowhere over high, with swamps and marshes. It is much used for the transit of large canoes, which are hauled across it from the Terni river, and which reach the Rio Negro by the little stream called the Pimichin.\n\nThe Casiquiare canal – Orinoco River hydrographic divide is a representation of the hydrographic water divide that delineates the separation between the Orinoco Basin and the Amazon Basin. (The Orinoco Basin flows west–north–northeast into the Caribbean; the Amazon Basin flows east into the western Atlantic in the extreme northeast of Brazil.)\n\nEssentially the river divide is a west-flowing, upriver section of Venezuela's Orinoco River with an outflow to the south into the Amazon Basin. This named outflow is the Casiquiare canal, which, as it heads downstream (southerly), picks up speed and also accumulates water volume.\n\nThe greatest manifestation of the divide is during floods. During flood stage, the Casiquiare's main outflow point into the Rio Negro is supplemented by an overflow that is a second, and more minor, entry river bifurcation into the Rio Negro and upstream from its major, common low-water entry confluence with the Rio Negro. At flood, the river becomes an area flow source, far more than a narrow confined river.\n\nThe Casiquiare canal connects the upper Orinoco, below the mission of Esmeraldas, with the Rio Negro affluent of the Amazon River near the town of San Carlos.\n\nThe simplest description (besides the entire area-floodplain) of the water divide is a \"south-bank Orinoco River strip\" at the exit point of the Orinoco, also the origin of the Casiquiare canal. However during the Orinoco's flood stage, that single, simply defined \"origin of the canal\" is turned into a region, and an entire strip along the southern bank of the Orinoco River.\n\n\n\n"}
{"id": "6279", "url": "https://en.wikipedia.org/wiki?curid=6279", "title": "Capetian dynasty", "text": "Capetian dynasty\n\nThe Capetian dynasty , also known as the House of France, is a dynasty of Frankish origin, founded by Hugh Capet. It is among the largest and oldest royal houses in Europe and the world, and consisting of Hugh Capet's male-line descendants. The senior line ruled in France as the House of Capet from the election of Hugh Capet in 987 until the death of Charles IV in 1328. They were succeeded by cadet branches, the Houses of Valois and Bourbon, which ruled until the French Revolution.\n\nThe dynasty had a crucial role in the formation of the French state. Initially obeyed only in their own demesne, the Île-de-France, the Capetian kings slowly but steadily increased their power and influence until it grew to cover the entirety of their realm. For a detailed narration on the growth of French royal power, see \"Crown lands of France\".\n\nMembers of the dynasty were traditionally Catholic. The early Capetians had an alliance with the Church. The French were also the most active participants in the Crusades, culminating in a series of five Crusader Kings – Louis VII, Philip Augustus, Louis VIII, Saint Louis, and Philip III. The Capetian alliance with the papacy suffered a severe blow after the disaster of the Aragonese Crusade. Philip III's son and successor, Philip IV, humiliated a pope and brought the papacy under French control. The later Valois, starting with Francis I, ignored religious differences and allied with the Ottoman Sultan to counter the growing power of the Holy Roman Empire. Henry IV was a Protestant at the time of his accession, but realized the necessity of conversion after four years of religious warfare.\n\nThe Capetians generally enjoyed a harmonious family relationship. By tradition, younger sons and brothers of the King of France are given appanages for them to maintain their rank and to dissuade them from claiming the French crown itself. When Capetian cadets did aspire for kingship, their ambitions were directed not at the French throne, but at foreign thrones. Through this, the Capetians spread widely over Europe.\n\nIn modern times, both King Felipe VI of Spain and Grand Duke Henri of Luxembourg are members of this family, both through the Bourbon branch of the dynasty. Along with the House of Habsburg, it was one of the two most powerful continental European royal families, dominating European politics for nearly five centuries.\n\nThe name of the dynasty derives from its founder, Hugh, who was known as \"Hugh Capet\". The meaning of \"Capet\" (a nickname rather than a surname of the modern sort) is unknown. While folk etymology identifies it with \"cape\", other suggestions suggest it to be connected to the Latin word \"caput\" (\"head\"), and thus explain it as meaning \"chief\" or \"head\".\n\nHistorians in the 19th century (see House of France) came to apply the name \"Capetian\" to both the ruling house of France and to the wider-spread male-line descendants of Hugh Capet. It was not a contemporary practice. The name \"Capet\" has also been used as a surname for French royalty, particularly but not exclusively those of the House of Capet. One notable use was during the French Revolution, when the dethroned King Louis XVI (a member of the House of Bourbon and a direct male-line descendant of Hugh Capet) and Queen Marie Antoinette (a member of the House of Habsburg-Lorraine) were referred to as \"Louis and Antoinette Capet\" (the queen being addressed as \"the Widow Capet\" after the execution of her husband).\n\nThe dynastic surname now used to describe Hugh Capet's family prior to his election as King of France is \"Robertians\" or \"Robertines.\" The name is derived from the family's first certain ancestor, Robert the Strong (b. 820), the count of Paris. Robert was probably son of Robert III of Worms (b. 800) and grandson of Robert of Hesbaye (b. 770). The Robertians probably originated in the county Hesbaye, around Tongeren in modern-day Belgium.\n\nThe sons of Robert the Strong were Odo and Robert, who both ruled as king of Western Francia. The family became Counts of Paris under Odo and Dukes of the Franks under Robert, possessing large parts of Neustria.\n\nThe Carolingian dynasty ceased to rule France upon the death of Louis V. After the death of Louis V, the son of Hugh the Great, Hugh Capet, was elected by the nobility as king of France. Hugh was crowned at Noyon on 3 July 987 with the full support from Holy Roman Emperor Otto III. With Hugh's coronation, a new era began for France, and his descendants came to be named the \"Capetians,\" with the Capetian dynasty ruling France for more than 800 years (987–1848, with some interruptions).\n\n\nOver the succeeding centuries, Capetians spread throughout Europe, ruling every form of provincial unit from kingdoms to manors.\n\nSalic law, reestablished during the Hundred Years' War from an ancient Frankish tradition, caused the French monarchy to permit only male (agnatic) descendants of Hugh to succeed to the throne of France.\n\nWithout Salic law, upon the death of John I, the crown would have passed to his half-sister, Joan (later Joan II of Navarre). However, Joan's paternity was suspect due to her mother's adultery in the Tour de Nesle Affair; the French magnates adopted Salic law to avoid the succession of a possible bastard.\n\nIn 1328, King Charles IV of France died without male heirs, as his brothers did before him. Philip of Valois, the late king's first cousin acted as regent, pending the birth of the king's posthumous child, which proved to be a girl. Isabella of France, sister of Charles IV, claimed the throne for her son, Edward III of England. The English king did not find support among the French lords, who made Philip of Valois their king. From then on the French succession not only excluded females, but also rejected claims based on the female line of descent.\n\nThus the French crown passed from the House of Capet after the death of Charles IV to Philip VI of France of the House of Valois, a cadet branch of the Capetian dynasty,\n\nThis did not affect monarchies not under that law such as Portugal, Spain, Navarre, and various smaller duchies and counties. Therefore, many royal families appear and disappear in the French succession or become cadet branches upon marriage. A complete list of the senior-most line of Capetians is available below.\n\nThe Capetian Dynasty has been broken many times into (sometimes rival) cadet branches. A cadet branch is a line of descent from another line than the senior-most. This list of cadet branches shows most of the Capetian cadet lines and designating their royal French progenitor, although some sub-branches are not shown.\n\n\n\n\n\n\n\n\nThroughout most of history, the Senior Capet and the King of France were synonymous terms. Only in the time before Hugh Capet took the crown for himself and after the reign of Charles X is the term necessary to identify which. However, since primogeniture and the Salic law provided for the succession of the French throne for most of French history, here is a list of all the predecessors of the French monarchy, all the French kings from Hugh until Charles, and all the Legitimist pretenders thereafter. All dates are for seniority, not reign. It is important to note that historians class the predecessors of Hugh Capet as \"Robertians\", not \"Capetians\".\n\nNoblemen in Neustria and their descendants (dates uncertain):\n\nCount in the Upper Rhine Valley and Wormgau:\n\nKing of France:\n\nCount of Paris:\n\nKing of France:\n\nDuke of Angoulême:\n\nCount of Chambord:\n\nCount of Montizón:\nDuke of Madrid:\nDuke of Anjou and Madrid:\nDuke of San Jaime:\nKing of Spain:\nDuke of Anjou and Segovia:\nDuke of Anjou and Cádiz:\nDuke of Anjou:\n\nMany years have passed since the Capetian monarchs ruled a large part of Europe; however, they still remain as kings, as well as other titles. Currently two Capetian monarchs still rule in Spain and Luxembourg. In addition, seven pretenders represent exiled dynastic monarchies in Brazil, France, Spain, Portugal, Parma and Two Sicilies. The current legitimate, senior family member is Louis-Alphonse de Bourbon, known by his supporters as Duke of Anjou, who also holds the Legitimist (\"Blancs d'Espagne\") claim to the French throne. Overall, dozens of branches of the Capetian dynasty still exist throughout Europe.\n\nExcept for the House of Braganza (founded by an illegitimate son of King John I of Portugal, who was himself illegitimate), all current major Capetian branches are of the Bourbon cadet branch. Within the House of Bourbon, many of these lines are themselves well-defined cadet lines of the House.\n\n\n\nIt is estimated that the agnatic descendants of the Capetian dynasty consists of 6,500 people (dead and alive).\n\nThe small number of agnatic descendants of the kings of France, compared with a theoretical number, is explained by the frequent marriages between Capetian cousins between the 12th and 20th centuries. Some examples of considerable inbreeding among descendants of the kings of France are:\n\n\n\n"}
{"id": "6280", "url": "https://en.wikipedia.org/wiki?curid=6280", "title": "Cuboctahedron", "text": "Cuboctahedron\n\nIn geometry, a cuboctahedron is a polyhedron with 8 triangular faces and 6 square faces. A cuboctahedron has 12 identical vertices, with 2 triangles and 2 squares meeting at each, and 24 identical edges, each separating a triangle from a square. As such, it is a quasiregular polyhedron, i.e. an Archimedean solid that is not only vertex-transitive but also edge-transitive.\n\nIts dual polyhedron is the rhombic dodecahedron.\n\nThe cuboctahedron was probably known to Plato: Heron's \"Definitiones\" quotes Archimedes as saying that Plato knew of a solid made of 8 triangles and 6 squares.\n\n\nThe area \"A\" and the volume \"V\" of the cuboctahedron of edge length \"a\" are:\n\nThe \"cuboctahedron\" has four special orthogonal projections, centered on a vertex, an edge, and the two types of faces, triangular and square. The last two correspond to the B and A Coxeter planes. The skew projections show a square and hexagon passing through the center of the cuboctahedron.\nThe cuboctahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.\n\nThe Cartesian coordinates for the vertices of a cuboctahedron (of edge length ) centered at the origin are:\n\nAn alternate set of coordinates can be made in 4-space, as 12 permutations of:\n\nThis construction exists as one of 16 orthant facets of the cantellated 16-cell.\n\nThe cuboctahedron's 12 vertices can represent the root vectors of the simple Lie group A. With the addition of 6 vertices of the octahedron, these vertices represent the 18 root vectors of the simple Lie group B.\n\nThe \"cuboctahedron\" can be dissected into two triangular cupolas by a common hexagon passing through the center of the cuboctahedron. If these two triangular cupolas are twisted so triangles and squares line up, Johnson solid J, the triangular orthobicupola, is created.\n\nThe cuboctahedron can also be dissected into 6 square pyramids and 8 tetrahedra meeting at a central point. This dissection is expressed in the alternated cubic honeycomb where pairs of square pyramids are combined into octahedra.\n\nThe cuboctahedron is the unique convex polyhedron in which the long radius (center to vertex) is the same as the edge length; thus its long diameter (vertex to opposite vertex) is 2 edge lengths. This special symmetry is a property of only a few polytopes: the two-dimensional hexagon, the three-dimensional cuboctahedron, and the four-dimensional 24-cell and 8-cell (tesseract). Only these polytopes can be constructed, with their radii, from equilateral triangles which meet at the center of the polytope, each contributing two radii and an edge. More generally, all the interior elements which meet at the center of these polytopes have special symmetry, as in the dissection of the cuboctahedron into 6 square pyramids and 8 tetrahedra. Each polytope also occurs as cells of a characteristic space-filling tessellation: the tiling of regular hexagons, the rectified cubic honeycomb (of alternating cuboctahedra and octahedra), the 24-cell honeycomb and the tesseractic honeycomb. Each tessellation has a dual tessellation; the cell centers in a tessellation are cell vertices in its dual tessellation. The densest regular sphere-packing in two, three and four dimensions uses the cell centers of one of these tessellations as sphere centers.\n\nA cuboctahedron can be obtained by taking an equatorial cross section of a four-dimensional 24-cell or 16-cell. A hexagon can be obtained by taking a equatorial cross section of a cuboctahedron.\n\nA cuboctahedron has octahedral symmetry. Its first stellation is the compound of a cube and its dual octahedron, with the vertices of the cuboctahedron located at the midpoints of the edges of either.\n\nThe cuboctahedron is a rectified cube and also a rectified octahedron.\n\nIt is also a cantellated tetrahedron. With this construction it is given the Wythoff symbol: . \n\nA skew cantellation of the tetrahedron produces a solid with faces parallel to those of the cuboctahedron, namely eight triangles of two sizes, and six rectangles. While its edges are unequal, this solid remains \"vertex-uniform\": the solid has the full tetrahedral symmetry group and its vertices are equivalent under that group.\n\nThe edges of a cuboctahedron form four regular hexagons. If the cuboctahedron is cut in the plane of one of these hexagons, each half is a triangular cupola, one of the Johnson solids; the cuboctahedron itself thus can also be called a triangular gyrobicupola, the simplest of a series (other than the gyrobifastigium or \"digonal gyrobicupola\"). If the halves are put back together with a twist, so that triangles meet triangles and squares meet squares, the result is another Johnson solid, the triangular orthobicupola, also called an anticuboctahedron.\n\nBoth triangular bicupolae are important in sphere packing. The distance from the solid's center to its vertices is equal to its edge length. Each central sphere can have up to twelve neighbors, and in a face-centered cubic lattice these take the positions of a cuboctahedron's vertices. In a hexagonal close-packed lattice they correspond to the corners of the triangular orthobicupola. In both cases the central sphere takes the position of the solid's center.\n\nCuboctahedra appear as cells in three of the convex uniform honeycombs and in nine of the convex uniform 4-polytopes.\n\nThe volume of the cuboctahedron is of that of the enclosing cube and of that of the enclosing octahedron.\n\nThe cuboctahedron shares its edges and vertex arrangement with two nonconvex uniform polyhedra: the cubohemioctahedron (having the square faces in common) and the octahemioctahedron (having the triangular faces in common). It also serves as a cantellated tetrahedron, as being a rectified tetratetrahedron.\n\nThe cuboctahedron 2-covers the tetrahemihexahedron, which accordingly has the same abstract vertex figure (two triangles and two squares: 3.4.3.4) and half the vertices, edges, and faces. (The actual vertex figure of the tetrahemihexahedron is 3.4..4, with the factor due to the cross.)\nThe cuboctahedron is one of a family of uniform polyhedra related to the cube and regular octahedron.\nThe cuboctahedron also has tetrahedral symmetry with two colors of triangles.\nThe cuboctahedron exists in a sequence of symmetries of quasiregular polyhedra and tilings with vertex configurations (3.\"n\"), progressing from tilings of the sphere to the Euclidean plane and into the hyperbolic plane. With orbifold notation symmetry of *\"n\"32 all of these tilings are wythoff construction within a fundamental domain of symmetry, with generator points at the right angle corner of the domain.\n\nThis polyhedron is topologically related as a part of sequence of cantellated polyhedra with vertex figure (3.4.\"n\".4), and continues as tilings of the hyperbolic plane. These vertex-transitive figures have (*\"n\"32) reflectional symmetry.\n\nThe cuboctahedron can be decomposed into a regular octahedron and eight irregular but equal octahedra in the shape of the convex hull of a cube with two opposite vertices removed. This decomposition of the cuboctahedron corresponds with the cell-first parallel projection of the 24-cell into three dimensions. Under this projection, the cuboctahedron forms the projection envelope, which can be decomposed into six square faces, a regular octahedron, and eight irregular octahedra. These elements correspond with the images of six of the octahedral cells in the 24-cell, the nearest and farthest cells from the 4D viewpoint, and the remaining eight pairs of cells, respectively.\n\nIn the mathematical field of graph theory, a cuboctahedral graph is the graph of vertices and edges of the cuboctahedron, one of the Archimedean solids. It has 12 vertices and 24 edges, and is a quartic Archimedean graph.\n\n\n"}
{"id": "6281", "url": "https://en.wikipedia.org/wiki?curid=6281", "title": "Canton", "text": "Canton\n\nCanton may refer to:\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "6282", "url": "https://en.wikipedia.org/wiki?curid=6282", "title": "Class", "text": "Class\n\nClass may refer to:\n\n\n\n\n\n\n\n\n"}
{"id": "6283", "url": "https://en.wikipedia.org/wiki?curid=6283", "title": "Critical point", "text": "Critical point\n\nCritical point may refer to:\n\n"}
{"id": "6285", "url": "https://en.wikipedia.org/wiki?curid=6285", "title": "Cube", "text": "Cube\n\nIn geometry, a cube is a three-dimensional solid object bounded by six square faces, facets or sides, with three meeting at each vertex.\n\nThe cube is the only regular hexahedron and is one of the five Platonic solids. It has 6 faces, 12 edges, and 8 vertices.\n\nThe cube is also a square parallelepiped, an equilateral cuboid and a right rhombohedron. It is a regular square prism in three orientations, and a trigonal trapezohedron in four orientations.\n\nThe cube is dual to the octahedron. It has cubical or octahedral symmetry.\n\nThe \"cube\" has four special orthogonal projections, centered, on a vertex, edges, face and normal to its vertex figure. The first and third correspond to the A and B Coxeter planes.\nThe cube can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.\n\nFor a cube centered at the origin, with edges parallel to the axes and with an edge length of 2, the Cartesian coordinates of the vertices are\n\nwhile the interior consists of all points (\"x\", \"x\", \"x\") with −1 < \"x\" < 1.\n\nIn analytic geometry, a cube's surface with center (\"x\", \"y\", \"z\") and edge length of \"2a\" is the locus of all points (\"x\", \"y\", \"z\") such that\n\nFor a cube of edge length formula_2:\nAs the volume of a cube is the third power of its sides formula_3, third powers are called \"cubes\", by analogy with squares and second powers.\n\nA cube has the largest volume among cuboids (rectangular boxes) with a given surface area. Also, a cube has the largest volume among cuboids with the same total linear size (length+width+height).\n\nFor a cube whose circumscribing sphere has radius \"R\", and for a given point in its 3-dimensional space with distances \"d\" from the cube's eight vertices, we have:\n\nDoubling the cube, or the \"Delian problem\", was the problem posed by ancient Greek mathematicians of using only a compass and straightedge to start with the length of the edge of a given cube and to construct the length of the edge of a cube with twice the volume of the original cube. They were unable to solve this problem, and in 1837 Pierre Wantzel proved it to be impossible because the cube root of 2 is not a constructible number.\n\nThe cube has three uniform colorings, named by the colors of the square faces around each vertex: 111, 112, 123.\n\nThe cube has three classes of symmetry, which can be represented by vertex-transitive coloring the faces. The highest octahedral symmetry O has all the faces the same color. The dihedral symmetry D comes from the cube being a prism, with all four sides being the same color. The lowest symmetry D is also a prismatic symmetry, with sides alternating colors, so there are three colors, paired by opposite sides. Each symmetry form has a different Wythoff symbol.\nA cube has eleven nets (one shown above): that is, there are eleven ways to flatten a hollow cube by cutting seven edges. To color the cube so that no two adjacent faces have the same color, one would need at least three colors.\n\nThe cube is the cell of the only regular tiling of three-dimensional Euclidean space. It is also unique among the Platonic solids in having faces with an even number of sides and, consequently, it is the only member of that group that is a zonohedron (every face has point symmetry).\n\nThe cube can be cut into six identical square pyramids. If these square pyramids are then attached to the faces of a second cube, a rhombic dodecahedron is obtained (with pairs of coplanar triangles combined into rhombic faces).\n\nThe analogue of a cube in four-dimensional Euclidean space has a special name—a tesseract or hypercube. More properly, a hypercube (or \"n\"-dimensional cube or simply \"n\"-cube) is the analogue of the cube in \"n\"-dimensional Euclidean space and a tesseract is the order-4 hypercube. A hypercube is also called a \"measure polytope\".\n\nThere are analogues of the cube in lower dimensions too: a point in dimension 0, a segment in one dimension and a square in two dimensions.\n\nThe quotient of the cube by the antipodal map yields a projective polyhedron, the hemicube.\n\nIf the original cube has edge length 1, its dual polyhedron (an octahedron) has edge length formula_5.\n\nThe cube is a special case in various classes of general polyhedra:\nThe vertices of a cube can be grouped into two groups of four, each forming a regular tetrahedron; more generally this is referred to as a demicube. These two together form a regular compound, the stella octangula. The intersection of the two forms a regular octahedron. The symmetries of a regular tetrahedron correspond to those of a cube which map each tetrahedron to itself; the other symmetries of the cube map the two to each other.\n\nOne such regular tetrahedron has a volume of of that of the cube. The remaining space consists of four equal irregular tetrahedra with a volume of of that of the cube, each.\n\nThe rectified cube is the cuboctahedron. If smaller corners are cut off we get a polyhedron with six octagonal faces and eight triangular ones. In particular we can get regular octagons (truncated cube). The rhombicuboctahedron is obtained by cutting off both corners and edges to the correct amount.\n\nA cube can be inscribed in a dodecahedron so that each vertex of the cube is a vertex of the dodecahedron and each edge is a diagonal of one of the dodecahedron's faces; taking all such cubes gives rise to the regular compound of five cubes.\n\nIf two opposite corners of a cube are truncated at the depth of the three vertices directly connected to them, an irregular octahedron is obtained. Eight of these irregular octahedra can be attached to the triangular faces of a regular octahedron to obtain the cuboctahedron.\n\nThe cube is topologically related to a series of spherical polyhedra and tilings with order-3 vertex figures.\nThe cuboctahedron is one of a family of uniform polyhedra related to the cube and regular octahedron.\nThe cube is topologically related as a part of sequence of regular tilings, extending into the hyperbolic plane: {4,p}, p=3,4,5...\nWith dihedral symmetry, Dih, the cube is topologically related in a series of uniform polyhedra and tilings 4.2n.2n, extending into the hyperbolic plane:\nAll these figures have octahedral symmetry.\n\nThe cube is a part of a sequence of rhombic polyhedra and tilings with [\"n\",3] Coxeter group symmetry. The cube can be seen as a rhombic hexahedron where the rhombi are squares.\nThe cube is a square prism:\nAs a trigonal trapezohedron, the cube is related to the hexagonal dihedral symmetry family.\n\nIt is an element of 9 of 28 convex uniform honeycombs: \nIt is also an element of five four-dimensional uniform polychora: \n\nThe skeleton of the cube (the vertices and edges) form a graph, with 8 vertices, and 12 edges. It is a special case of the hypercube graph. It is one of 5 Platonic graphs, each a skeleton of its Platonic solid.\n\nAn extension is the three dimensional \"k\"-ary Hamming graph, which for \"k\" = 2 is the cube graph. Graphs of this sort occur in the theory of parallel processing in computers.\n\n\nMiscellaneous cubes\n\n\n"}
{"id": "6286", "url": "https://en.wikipedia.org/wiki?curid=6286", "title": "Commuter rail", "text": "Commuter rail\n\nCommuter rail, also called suburban rail, is a passenger rail transport service that primarily operates between a city centre and middle to outer suburbs beyond 15 km (10 miles) and commuter towns or other locations that draw large numbers of commuters—people who travel on a daily basis. Trains operate following a schedule at speeds varying from 50 to 200 km/h (30 to 125 mph). Distance charges or zone pricing may be used.\n\nNon-English names include \"Treno suburbano\" in Italian, \"Cercanías\" in Spanish, \"Rodalies\" in Catalan, \"Proastiakos\" in Greek, \"S-Bahn\" in German (although \"Regionalbahn\" or stopping services occasionally also operate as commuter trains), \"Train de banlieue\" in French, \"Příměstský vlak\" or \"Esko\" in Czech, \"Elektrichka\" in Russian, \"Pociąg podmiejski \" in Polish and \"Pendeltåg\" in Swedish. The development of commuter rail services has become popular today, with the increased public awareness of congestion, dependence on fossil fuels, and other environmental issues, as well as the rising costs of owning, operating and parking automobiles.\n\nMost commuter (or suburban) trains are built to main line rail standards, differing from light rail or rapid transit (metro rail) systems by:\n\nCompared to rapid transit (or metro rail), commuter/suburban rail has lower frequency, following a schedule rather than fixed intervals, and fewer stations spaced further apart. They primarily serve lower density suburban areas (non inner-city), and often share right-of-way with intercity or freight trains. Some services operate only during peak hours and others uses fewer departures during off peak hours and weekends. Average speeds are high, often 50 km/h (30 mph) or higher. These higher speeds better serve the longer distances involved. Some services include express services which skip some stations in order to run faster and separate longer distance riders from short-distance ones.\n\nThe general range of commuter trains' distance varies between 15 and 200 km (10 and 125 miles). Sometimes long distances can be explained by that the train runs between two or several cities (e.g. S-Bahn in the Ruhr area of Germany). Distances between stations may vary, but are usually much longer than those of urban rail systems. In city centers the train either has a terminal station or passes through the city centre with notably fewer station stops than those of urban rail systems. Toilets are often available on-board trains and in stations.\n\nTheir ability to coexist with freight or intercity services in the same right-of-way can drastically reduce system construction costs. However, frequently they are built with dedicated tracks within that right-of-way to prevent delays, especially where service densities have converged in the inner parts of the network.\n\nMost such trains run on the local standard gauge track. Some light rail systems may run on a narrower gauge. Examples of narrow gauge systems are found in Japan, Indonesia, Switzerland, in the Brisbane (Queensland Rail's City network) and Perth (Transperth) systems in Australia, in some commuter rail systems in Sweden, and on the in Italy. Some countries, including Finland, India, Pakistan, Russia, Brazil and Sri Lanka, as well as San Francisco (BART) in the USA and Melbourne and Adelaide in Australia, use broad gauge track.\n\nMetro rail or rapid transit usually covers a smaller inner-urban area ranging outwards to between 12 km to 20 km (or 8 to 14 miles), has a higher train frequency and runs on separate tracks (underground or elevated), whereas commuter rail often shares tracks, technology and the legal framework within mainline railway systems.\n\nHowever, the classification as a metro or rapid rail can be difficult as both may typically cover a metropolitan area exclusively, run on separate tracks in the centre, and often feature purpose-built rolling stock. The fact that the terminology is not standardised across countries (even across English-speaking countries) further complicates matters. This distinction is most easily made when there are two (or more) systems such as New York's subway and the LIRR, Metro-North along with PATH, Paris' Métro and RER along with Transilien, London's tube lines of the Underground and the Overground, (future) Crossrail, Thameslink along with other commuter rail operators, Madrid's Metro and Cercanías, Barcelona's Metro and Rodalies, and Tokyo's subway and the JR lines along with various privately owned and operated commuter rail systems.\n\nIn Germany the S-Bahn is regarded as a train category of its own, and exists in many large cities and in some other areas, but there are differing service and technical standards from city to city. \nMost S-bahns typically behave like commuter rail with most trackage not separated from other trains, and long lines with trains running between cities and suburbs rather than within a city. The distances between stations however, are usually short. In larger systems there is usually a high frequency metro-like central corridor in the city center where all the lines converge into. Typical examples of large city S-Bahns include Munich and Frankfurt. S-Bahns do also exist in some mid-size cities like Rostock and Magdeburg but behave more like typical commuter rail with lower frequencies and very little exclusive trackage. In Berlin, the S-Bahn systems arguably fulfill all considerations of a true metro system (despite the existence of U-Bahns as well) – the trains run on tracks that are entirely separated from other trains, short distances between stations, high frequency and uses tunnels but do run a bit further out from the city centre, compared with U-Bahn. A similar network exists in Copenhagen called the S-tog. (where a metro system also exists). In Hamburg and Copenhagen, other, diesel driven trains, do continue where the S-Bahn ends (\"A-Bahn\" in Hamburg area, and \"L-tog\" in Copenhagen).\n\nRegional rail usually provides rail services between towns and cities, rather than purely linking major population hubs in the way inter-city rail does. Regional rail operates outside major cities. Unlike Inter-city, it stops at most or all stations between cities. It provides a service between smaller communities along the line, and also connections with long-distance services at interchange stations located at junctions or at larger towns along the line. Alternative names are \"local train\" or \"stopping train\". Examples include the former BR's Regional Railways, France's TER (\"Transport express régional\"), Germany's DB Regio and South Korea's Tonggeun services.\n\nRegional rail does not exist in this sense in the United States, so the term \"regional rail\" has become synonymous with commuter rail there, although the two are more clearly defined in Europe.\n\nIn some European countries the distinction between commuter trains and long-distance/intercity trains is very hard to make, because of the relatively short distances involved. For example, so-called \"intercity\" trains in Belgium and the Netherlands carry many commuters and their equipment, range and speeds are similar to those of commuter trains in some larger countries. In the United Kingdom there is no real division of organisation and brand name between commuter, regional and inter-city trains, making it hard to categorize train connections.\n\nRussian commuter trains, on the other hand, frequently cover areas larger than Belgium itself, although these are still short distances by Russian standards. They have a different ticketing system from long-distance trains, and in major cities they often operate from a separate section of the train station.\n\nThe easiest way to identify these \"inter-city\" services is that they tend to operate as express services - only linking the main stations in the cities they link, not stopping at any other stations. However, this term is used in Australia (Sydney for example) to describe the regional trains operating beyond the boundaries of the suburban services, even though some of these \"inter-city\" services stop all stations similar to German regional services. In this regard, the German service delineations and corresponding naming conventions are clearer and better used for academic purposes.\n\nSometimes high-speed rail can serve daily use of commuters. The Japanese Shinkansen high speed rail system is heavily used by commuters in the Greater Tokyo Area. They commute between 100 and 200 km by Shinkansen. To meet the demand of commuters, JR sells commuter discount passes and operates 16-car bilevel E4 Series Shinkansen trains at rush hour, providing a capacity of 1,600 seats. Several lines in China, such as the Beijing–Tianjin Intercity Railway and the Shanghai–Nanjing High-Speed Railway, serve a similar role with many more under construction or planned.\n\nThe high-speed services linking Zürich, Bern and Basel in Switzerland () have brought the Central Business Districts (CBDs) of these three cities within 1 hour of each other. This has resulted in unexpectedly high demand for new commuter trips between the three cities and a corresponding increase in suburban rail passengers accessing the high-speed services at the main city-centre stations (or Hauptbahnhof). The Regional-Express commuter service between Munich and Nuremberg in Germany go in () along a 300 km/h high-speed line.\n\nThe regional trains Stockholm–Uppsala, Stockholm–Västerås, Stockholm–Eskilstuna and Gothenburg–Trollhättan in Sweden reach and have many daily commuters.\n\nCommuter/suburban trains are usually optimized for maximum passenger volume, in most cases without sacrificing too much comfort and luggage space, though they seldom have all the amenities of long-distance trains. Cars may be single- or double-level, and aim to provide seating for all. Compared to intercity trains, they have less space, fewer amenities and limited baggage areas.\n\nCommuter rail trains are usually composed of multiple units, which are self-propelled, bidirectional, articulated passenger rail cars with driving motors on each (or every other) bogie. Depending on local circumstances and tradition they may be powered either by diesel engines located below the passenger compartment (diesel multiple units) or by electricity picked up from third rails or overhead lines (electric multiple units). Multiple units are almost invariably equipped with control cabs at both ends, which is why such units are so frequently used to provide commuter services, due to the associated short turn-around time.\n\nLocomotive hauled services are used in some countries or locations. This is often a case of asset sweating, by using a single large combined fleet for intercity and regional services. Loco hauled services are usually run in push-pull formation, that is, the train can run with the locomotive at the \"front\" or \"rear\" of the train (pushing or pulling). Trains are often equipped with a control cab at the other end of the train from the locomotive, allowing the train operator to operate the train from either end. The motive power for locomotive-hauled commuter trains may be either electric or Diesel-electric, although some countries, such as Germany and some of the former Soviet-bloc countries, also use diesel-hydraulic locomotives.\n\nIn the USA and some other countries, a three-and-two seat plan is used. However, few people sit in the middle seat on these trains because they feel crowded and uncomfortable. It is said one industrial designer for one of New York City's commuter railroads, Metro-North, told people: \"I designed the aisle seat with a half-back and no upholstery, so it will be very uncomfortable to sit there. They'll move in and take the center seat!\" (This seating design can also be found on older NJ Transit and Long Island Rail Road rolling stock.)\n\nIn Japan, longitudinal (sideways window-lining) seating is widely used in many commuter rail trains to increase capacity in rush hours. Carriages are usually not organized to increase seating capacity (although in some trains at least one carriage would feature more doors to facilitate easier boarding and alighting and bench seats so that they can be folded up during rush hour to provide more standing room) even in the case of commuting longer than 50 km and commuters in the Greater Tokyo Area have to stand in the train for more than an hour.\n\nCurrently there are not many examples of commuter rail in Africa. Metrorail operates in the major cities of South Africa, and there are some commuter rail services in Algeria, Kenya, Morocco, Alexandria, Egypt and Tunisia.\nIn Algeria, SNTF operates commuter-rail lines between the capital Algiers and its southern and eastern suburbs. They also serve to connect Algiers' main universities to each other. The Dar es Salaam commuter rail offers intracity services in Dar es Salaam, Tanzania.\n\nIn Japan, commuter rail systems have extensive network and frequent service and are heavily used. In many cases, Japanese commuter rail is operationally more like a typical metro system (with very high operating frequencies, an emphasis on standing passengers, short station spacing) than it is like commuter rail in other countries. Japanese commuter rail also tends to be heavily interlined with subway lines, with commuter rail trains continuing into the subway network, and then out onto different commuter rail systems on the other side of the city. Many Japanese commuter systems operate several levels of express trains to reduce the travel time to distant locations, often using station bypass tracks instead of dedicated express tracks. It is notable that the larger Japanese commuter rail systems are owned and operated by for-profit private railway companies, without public subsidy.\n\nCommuter rail systems have been inaugurated in several cities in China such as Beijing, Shanghai, Zhengzhou, Wuhan, Changsha and the Pearl River Delta. With plans for large systems in northeastern Zhejiang, Jingjinji, and Yangtze River Delta areas. The level of service varies considerably from line to line ranging high to near high speeds. More developed and established lines such as the Guangshen Railway have more frequent metro like service. Hong Kong MTR's East Rail Line and West Rail Line were built to commuter rail standards but are operated as a metro system. \n\nIn Taiwan, Western Line in Taipei-Taoyuan Metropolitan Area, Taichung Metropolitan Area, Tainan-Kaohsiung Metropolitan Area as well as Neiwan-Liujia Line in Hsinchu Area is considered commuter rail.\n\nOther examples in Asia include Seoul Metropolitan Subway of which some lines are suburban lines operated by Korail in South Korea.\n\nIn Indonesia, the KA Commuter Jabodetabek is the largest commuter rail system in the country, serving Jakarta metropolitan area. It connects Jakarta city center with surrounding cities and sub-urbans in Banten and West Java provinces, including Depok, Bogor, Tangerang, Bekasi, Serpong and Maja. In July 2015, KA Commuter Jabodetabek served more than 850,000 passengers per day, which is almost triple of the 2011 figures, but still less than 3.5% of all Jabodetabek commutes.\n\nOther examples include KTM Komuter in Malaysia, and the Philippine National Railways orange line in Metro Manila, Philippines.\n\nIn India, commuter rail systems are present in major cities. Mumbai Suburban Railway, the oldest suburban rail system in Asia, carries more than 7.24 million commuters on a daily basis which constitutes more than half of the total daily passenger capacity of the Indian Railways itself. The Chennai Suburban Railway along with MRTS is another railway of comparison where more than 1 million people travel daily to different areas in Chennai. In Hyderabad, the MMTS mainly transports people from the city centre to HI-TEC city, the city's Information Technology hub. Kolkata Suburban Railway is the biggest Suburban Railway network in India covering 348 stations. Other commuter railways in India include Delhi Suburban Railway, Pune Suburban Railway and Lucknow-Kanpur Suburban Railway.\n\nIn Iran, SYSTRA has done a \"Tehran long term urban rail study\". SYSTRA proposed 4 express lines similar to RER suburban lines in Paris.\nTehran Metro is going to construct express lines. For instance, the Rahyab Behineh, a consultant for Tehran Metro, is studying Tehran Express Line 2. Tehran Metro currently has a commuter line between Tehran and Karaj. Esfahan has two lines to its suburbs Baharestan and Fuladshahr under construction, and a third line to Shahinshahr is planned.\n\nMajor metropolitan areas in most European countries are usually served by extensive commuter/suburban rail systems. Well-known examples include Beovoz in Belgrade (Serbia), S-Bahn in Germany and German-speaking areas of Switzerland and Austria, Proastiakos in Greece, RER in France and Belgium, suburban lines in Milan (Italy), Cercanías in Spain, CP Urban Services in Portugal, Esko in Prague and Ostrava (Czech Republic), HÉV in Budapest (Hungary) and DART in Dublin (Ireland).\n\nIn Russia, Ukraine and some other countries of the former Soviet Union, electrical multiple unit passenger suburban trains called Elektrichka are widespread.\n\nIn Sweden, electrified commuter rail systems known as \"Pendeltåg\" are present in the cities of Stockholm and Gothenburg. The Stockholm commuter rail system, which began in 1968, is similar to the S-Bahn train systems of Munich and Frankfurt such that it may share railway tracks with inter-city trains and freight trains, but for the most part run on its own dedicated tracks, and that it is mainly used to transport passengers from nearby towns and other suburban areas into the city centre, not for transportation inside the city centre. The Gothenburg commuter rail system, which began in 1960, is similar to the Stockholm system, but does fully share tracks with long-distance trains. Other train systems that are also considered as commuter rail but not counted as \"pendeltåg\" include Roslagsbanan and Saltsjöbanan in Stockholm, Östgötapendeln in Östergötland County, Upptåget in Uppsala County and Skåne Commuter Rail. Skåne Commuter Rail (\"Pågatågen\") in Skåne County acts also as a regional rail system, as it serves cities over 100 km (62 miles) and over one hour from the principal city of Malmö.\n\nIn Norway, the Oslo commuter rail system is the largest, which mostly shares tracks with more long-distance trains, but also runs on some local railways without other traffic. Oslo has the largest commuter rail system in Scandinavia measured as line lengths or number of stations. But some lines have travel times (over an hour from Oslo) and frequencies (once per hour) which are more like regional trains. Also Bergen, Stavanger and Trondheim have commuter rail systems. These have only one or two lines each and they share tracks with other trains.\n\nIn Poland, commuter rail systems exist in Tricity, Warsaw, Cracow and Katowice urban area. There is also a similar system planned in Wrocław.\n\nIn the United States, Canada, Costa Rica, El Salvador and Mexico regional passenger rail services are provided by governmental or quasi-governmental agencies, with a limited number of metropolitan areas served.\n\nExamples include an commuter system in the Buenos Aires metropolitan area, the long Supervia in Rio de Janeiro, and the Metrotrén in Santiago, Chile. Another example is Companhia Paulista de Trens Metropolitanos (CPTM) in Greater São Paulo, Brazil. CPTM has 93 stations with six lines, numbered starting on 7 (the lines 1 to 6 belong to the São Paulo Metro), with a total length of .\n\nThe five major cities in Australia have suburban railway systems in their metropolitan areas. These networks have frequent services, with frequencies varying from every 10 to every 30 minutes on most suburban lines, and up to 3–5 minutes in peak on bundled underground lines in the city centres of Sydney, Perth and Melbourne. The networks in each state developed from mainline railways and have never been completely operationally separate from long distance and freight traffic, unlike metro systems in some comparable countries, but nevertheless have cohesive identities and are the backbones of their respective cities' public transport system. The suburban networks are all completely electrified, apart from Adelaide's, which still operates diesel services on some of its lines.\n\nThe main operators of suburban rail in Australia are:\n\nNew Zealand has two frequent suburban rail services comparable to Australia's, one using 25,000 V AC in Auckland and the other one using 1,500 V DC in Wellington (formerly operaterd by Tranz Metro).\n\n"}
{"id": "6288", "url": "https://en.wikipedia.org/wiki?curid=6288", "title": "Cambridgeshire", "text": "Cambridgeshire\n\nCambridgeshire ( or ; abbreviated Cambs.), is an East Anglian county in England, bordering Lincolnshire to the north, Norfolk to the north-east, Suffolk to the east, Essex and Hertfordshire to the south, and Bedfordshire and Northamptonshire to the west. The city of Cambridge is the county town. Modern Cambridgeshire was formed in 1974 as an amalgamation of the counties of Cambridgeshire and Isle of Ely and Huntingdon and Peterborough, which had been created in 1965 from the two historic counties of Cambridgeshire (including the Isle of Ely) and Huntingdonshire, and the Soke of Peterborough. It contains most of the region known as Silicon Fen.\n\nLocal government is divided between Cambridgeshire County Council and Peterborough City Council, which is a separate unitary authority. Under the county council, there are five district councils, Cambridge City Council, South Cambridgeshire District Council, East Cambridgeshire District Council, Huntingdonshire District Council and Fenland District Council.\n\nCambridgeshire is noted as the site of Flag Fen in Fengate, one of the earliest-known Neolithic permanent settlements in the United Kingdom, compared in importance to Balbridie in Aberdeen, Scotland. A great quantity of archaeological finds from the Stone Age, the Bronze Age and the Iron Age were made in East Cambridgeshire. Most items were found in Isleham.\n\nCambridgeshire was recorded in the Domesday Book as \"Grantbridgeshire\" (or rather \"Grentebrigescire\") (related to the river Granta).\n\nCovering a large part of East Anglia, Cambridgeshire today is the result of several local government unifications. In 1888 when county councils were introduced, separate councils were set up, following the traditional division of Cambridgeshire, for\nIn 1965, these two administrative counties were merged to form Cambridgeshire and the Isle of Ely.\nUnder the Local Government Act 1972 this merged with the county to the west, Huntingdon and Peterborough. (The latter had been organised in 1965 by the merger of Huntingdonshire with the Soke of Peterborough – previously a part of Northamptonshire which had its own county council). The resulting county was called simply Cambridgeshire.\n\nSince 1998, the City of Peterborough has been a separately administered area, as a unitary authority. It is associated with Cambridgeshire for ceremonial purposes such as Lieutenancy, and joint functions such as policing and the fire service.\n\nIn 2002, the conservation charity Plantlife unofficially designated Cambridgeshire's county flower as the Pasqueflower.\n\nThe Cambridgeshire Regiment (or Fen Tigers), the county-based army unit, fought in the Boer War of South Africa, the First World War and Second World War.\n\nDue to the county's flat terrain and proximity to the continent, during the Second World War the military built many airfields here for RAF Bomber Command, RAF Fighter Command, and the allied USAAF. In recognition of this collaboration, the Cambridge American Cemetery and Memorial is located in Madingley. It is the only WWII burial ground in England for American servicemen who died during that event.\n\nMost English counties have nicknames for their people, such as a \"Tyke\" from Yorkshire and a \"Yellowbelly\" from Lincolnshire. The traditional nicknames for people from Cambridgeshire are \"Cambridgeshire Camel\" or \"Cambridgeshire Crane\", referring to the wildfowl that were once abundant in the fens. The term \"Fenners\" was often applied to those who come from the flat country to the north of Cambridge. Since the late 20th century, this term is considered to be derogatory and has been discouraged in use.\n\nOriginal historical documents relating to Cambridgeshire are held by Cambridgeshire Archives and Local Studies.\n\nLarge areas of the county are extremely low-lying and Holme Fen is notable for being the UK's lowest physical point at 2.75 m (9 ft) below sea level. The highest point is in the village of Great Chishill at 146 m (480 ft) above sea level. Other prominent hills are Little Trees Hill and Wandlebury Hill (both at ) in the Gog Magog Hills, Rivey Hill above Linton, Rowley's Hill and the Madingley Hills.\n\nCambridgeshire contains seven Parliamentary constituencies:\n\nThis is a chart of trend of regional gross value added of Cambridgeshire at current basic prices published (pp. 240–253) by \"Office for National Statistics\" with figures in millions of English Pounds Sterling.\nAWG plc is based in Huntingdon. The RAF has several stations in the Huntingdon and St Ives area. RAF Waterbeach, 6 miles north of Cambridge, is a former RAF airfield, now used as an army barracks. RAF Alconbury, 3 miles north of Huntingdon, is being reorganised after a period of obsolescence following the departure of the USAF, to be the focus of RAF/USAFE intelligence operations, with activities at Upwood and Molesworth being transferred there. Most of Cambridgeshire is agricultural. Close to Cambridge is the so-called Silicon Fen area of high-technology (electronics, computing and biotechnology) companies. ARM Limited is based in Cherry Hinton.\n\nCambridgeshire has a completely comprehensive education system with 12 independent schools and over 240 state schools, not including sixth form colleges.\n\nSome of the secondary schools act as Village Colleges, institutions unique to Cambridgeshire. For example, Bottisham Village College.\n\nCambridgeshire is home to a number of institutes of higher education:\n\n\nIn addition, Cambridge Regional College and Huntingdonshire Regional College both offer a limited range of higher education courses in conjunction with partner universities.\n\nThese are the settlements in Cambridgeshire with a town charter, city status or a population over 5,000; for a complete list of settlements see list of places in Cambridgeshire.\n\nSee the List of Cambridgeshire settlements by population page for more detail.\n\nThe town of Newmarket is surrounded on three sides by Cambridgeshire, being connected by a narrow strip of land to the rest of Suffolk.\n\nCambridgeshire has seen 32,869 dwellings created from 2002–2013 and there are a further 35,360 planned new dwellings between now and 2023.\n\nCambridgeshire has a maritime temperate climate which is broadly similar to the rest of the United Kingdom, though it is drier than the UK average due to its low altitude and easterly location, the prevailing southwesterly winds having already deposited moisture on higher ground further west. Average winter temperatures are cooler than the English average, due to Cambridgeshire's inland location and relative nearness to continental Europe, which results in the moderating maritime influence being less strong. Snowfall is slightly more common than in western areas, due to the relative winter coolness and easterly winds bringing occasional snow from the North Sea. In summer temperatures are average or slightly above, due to less cloud cover. It reaches on around 10 days each year, and is comparable to parts of Kent and East Anglia.\n\nVarious forms of football have been popular in Cambridgeshire since medieval times at least. In 1579 one match played at Chesterton between townspeople and Cambridge University students ended in a violent brawl that led the Vice-Chancellor to issue a decree forbidding them to play \"footeball” outside of college grounds. Despite this and other decrees, football continued to be popular. George Elwes Corrie, Master of Jesus College, observed in 1838, that while walking past a park named Parker's Piece he \"saw some forty Gownsmen playing at football. The novelty and liveliness of the scene were amusing!\" By 1839, Albert Pell was organising football matches at the university; because each town or school had different rules, students had to devise a compromise set of rules.\n\nAt Cambridge University in 1846, H. de Winton and J. C. Thring formed a pioneering football club. Only a few matches were played, but in 1848 interest in football increased and that year the Cambridge rules, the first attempt to codify a form of football were drawn up in Cambridge. The Cambridge rules are generally regarded as the main precursor of Association football.\n\nAs a result of its role in the formation of the first football rules, Parker's Piece remains hallowed turf for football fans and historians.\nIn commemoration of the creation of Football; a statue is to be raised in the middle of the park where the game was invented.\n\nCambridgeshire is also the birthplace of bandy, now an IOC accepted sport. According to documents from 1813, Bury Fen Bandy Club was undefeated for 100 years. A member of the club, Charles Goodman Tebbutt, wrote down the first official rules in 1882. Tebbutt was instrumental in spreading the sport to many countries. England Bandy Federation is based in Cambridgeshire.\n\nOn 6–7 June 2015, the inaugural Tour of Cambridgeshire cycle race took place on closed roads across the county. The event was an official UCI qualification event, and consisted of a Time Trial on the 6th, and a Gran Fondo event on the 7th. The Gran Fondo event was open to the public, and over 6000 riders took part in the race.\n\nCambridge is home to the Kettle's Yard gallery and the artist run Aid and Abet project Space. Nine miles west of Cambridge next to the village of Bourn is Wysing Arts Centre.\n\n\"See \n\n\n"}
{"id": "6290", "url": "https://en.wikipedia.org/wiki?curid=6290", "title": "Christian Goldbach", "text": "Christian Goldbach\n\nChristian Goldbach (March 18, 1690 – November 20, 1764) was a German mathematician who also studied law. He is remembered today for Goldbach's conjecture. \n\nBorn in the Duchy of Prussia's capital Königsberg, part of Brandenburg-Prussia, Goldbach was the son of a pastor. He studied at the Royal Albertus University.\nAfter finishing his studies he went on long educational voyages from 1710 to 1724 through Europe, visiting other German states, England, Holland, Italy, and France, meeting with many famous mathematicians, such as Gottfried Leibniz, Leonhard Euler, and Nicholas I Bernoulli. Back in Königsberg he got acquainted with Georg Bernhard Bilfinger and Jakob Hermann.\n\nHe went on to work at the newly opened St Petersburg Academy of Sciences in 1725, as a professor of mathematics and historian of the academy. In 1728, when Peter II became Tsar of Russia, Goldbach became his tutor. In 1742 he entered the Russian Ministry of Foreign Affairs.\n\nChristian Goldbach was multilingual – he wrote a diary which was written in German and Latin, his letters were written in German, Latin, French, and Italian and for official documents he used Russian, German and Latin.\n\nHe died on November 20, 1764 at age of 74, in \nMoscow.\n\nGoldbach is most noted for his correspondence with Leibniz, Euler, and Bernoulli, especially in his 1742 letter to Euler stating his Goldbach's conjecture. He also studied and proved some theorems on perfect powers, such as the Goldbach–Euler theorem, and made several notable contributions to analysis. He also proved a result concerning Fermat numbers that is called Goldbach's theorem.\n\n\n"}
{"id": "6291", "url": "https://en.wikipedia.org/wiki?curid=6291", "title": "Roman censor", "text": "Roman censor\n\nThe censor was an officer in ancient Rome who was responsible for maintaining the census, supervising public morality, and overseeing certain aspects of the government's finances.\n\nThe censors' regulation of public morality is the origin of the modern meaning of the words \"censor\" and \"censorship\".\n\nThe \"census\" was first instituted by Servius Tullius, sixth king of Rome. After the abolition of the monarchy and the founding of the Republic, the consuls had responsibility for the census until 443 BC. In 442 BC, no consuls were elected, but tribunes with consular power were appointed instead; this was a move by the plebeians to try to attain higher magistracies: only patricians could be elected consuls, while some military tribunes were plebeians. To avoid the possibility of plebeians obtaining control of the census, the patricians removed the right to take the census from the consuls and tribunes, and appointed for this duty two magistrates, called \"censores\" (censors), elected exclusively from the patricians in Rome. \n\nIt would not be uncommon for the patrician consulars of the early republic to intersperse public office with agricultural labour. In Cicero’s words: \"in agris erant tum senatores, id est senes\": ‘In those days senators—that is, seniors—would live on their farms’. This practice was obsolete by the 2nd century.\n\nThe magistracy continued to be controlled by patricians until 351 BC, when Gaius Marcius Rutilus was appointed the first plebeian censor. Twelve years later, in 339 BC, one of the Publilian laws required that one censor had to be a plebeian. Despite this, no plebeian censor performed the solemn purification of the people (the \"\"lustrum\"\"; \"Livy\" Periochae 13) until 280 BC. In 131 BC, for the first time, both censors were plebeians.\n\nThe reason for having two censors was that the two consuls had previously taken the census together. If one of the censors died during his term of office, another was chosen to replace him, just as with consuls. This happened only once, in 393 BC. However, the Gauls captured Rome in that \"lustrum\" (five-year period), and the Romans thereafter regarded such replacement as \"an offense against religion\". From then on, if one of the censors died, his colleague resigned, and two new censors were chosen to replace them.\n\nThe censors were elected in the Centuriate Assembly, which met under the presidency of a consul. Barthold Niebuhr suggests that the censors were at first elected by the Curiate Assembly, and that the Assembly's selections were confirmed by the Centuriate, but William Smith believes that \"there is no authority for this supposition, and the truth of it depends entirely upon the correctness of <nowiki>[Niebuhr's]</nowiki> views respecting the election of the consuls\". Both censors had to be elected on the same day, and accordingly if the voting for the second was not finished in the same day, the election of the first was invalidated, and a new assembly had to be held.\n\nThe assembly for the election of the censors was held under different auspices from those at the election of the consuls and praetors, so the censors were not regarded as their colleagues, although they likewise possessed the \"maxima auspicia\". The assembly was held by the new consuls shortly after they began their term of office; and the censors, as soon as they were elected and the censorial power had been granted to them by a decree of the Centuriate Assembly (\"lex centuriata\"), were fully installed in their office.\n\nAs a general principle, the only ones eligible for the office of censor were those who had previously been consuls, but there were a few exceptions. At first, there was no law to prevent a person being censor twice, but the only person who was elected to the office twice was Gaius Marcius Rutilus in 265 BC. In that year, he originated a law stating that no one could be elected censor twice. In consequence of this, he received the cognomen of \"Censorinus\".\n\nThe censorship differed from all other Roman magistracies in the length of office. The censors were originally chosen for a whole \"lustrum\" (the period of five years), but as early as ten years after its institution (433 BC) their office was limited to eighteen months by a law of the dictator Mamercus Aemilius Mamercinus. The censors were also unique with respect to rank and dignity. They had no \"imperium\", and accordingly no lictors. Their rank was granted to them by the Centuriate Assembly, and not by the \"curiae\", and in that respect they were inferior in power to the consuls and praetors.\n\nNotwithstanding this, the censorship was regarded as the highest dignity in the state, with the exception of the dictatorship; it was a \"sacred magistracy\" (\"sanctus magistratus\"), to which the deepest reverence was due. The high rank and dignity which the censorship obtained was due to the various important duties gradually entrusted to it, and especially to its possessing the \"regimen morum\", or general control over the conduct and the morals of the citizens. In the exercise of this power, they were regulated solely by their own views of duty, and were not responsible to any other power in the state.\n\nThe censors possessed the official stool called a \"curule chair\" (\"sella curulis\"), but some doubt exists with respect to their official dress. A well-known passage of Polybius describes the use of the \"imagines\" at funerals; we may conclude that a consul or praetor wore the purple-bordered \"toga praetexta\", one who triumphed the embroidered \"toga picta\", and the censor a purple toga peculiar to him, but other writers speak of their official dress as being the same as that of the other higher magistrates. The funeral of a censor was always conducted with great pomp and splendour, and hence a \"censorial funeral\" (\"funus censorium\") was voted even to the emperors.\n\nThe censorship continued in existence for 421 years, from 443 BC to 22 BC, but during this period, many \"lustra\" passed by without any censor being chosen at all. According to one statement, the office was abolished by Lucius Cornelius Sulla. Although the authority on which this statement rests is not of much weight, the fact itself is probable, since there was no census during the two \"lustra\" which elapsed from Sulla's dictatorship to Gnaeus Pompeius Magnus (Pompey)'s first consulship (82–70 BC), and any strict \"imposition of morals\" would have been found inconvenient to the aristocracy that supported Sulla.\n\nIf the censorship had been done away with by Sulla, it was at any rate restored in the consulship of Pompey and Marcus Licinius Crassus. Its power was limited by one of the laws of the tribune Publius Clodius Pulcher (58 BC), which prescribed certain regular forms of proceeding before the censors in expelling a person from the Roman Senate, and required that the censors be in agreement to exact this punishment. This law, however, was repealed in the third consulship of Pompey in 52 BC, on the urging of his colleague Q. Caecilius Metellus Scipio, but the office of the censorship never recovered its former power and influence.\n\nDuring the civil wars which followed soon afterwards, no censors were elected; it was only after a long interval that they were again appointed, namely in 22 BC, when Augustus caused Lucius Munatius Plancus and Aemilius Lepidus Paullus to fill the office. This was the last time that such magistrates were appointed; the emperors in future discharged the duties of their office under the name of Praefectura Morum (\"prefect of the morals\").\n\nSome of the emperors sometimes took the name of censor when they held a census of the Roman people; this was the case with Claudius, who appointed the elder Vitellius as his colleague, and with Vespasian, who likewise had a colleague in his son Titus. Domitian assumed the title of \"perpetual censor\" (\"censor perpetuus\"), but this example was not imitated by succeeding emperors. In the reign of Decius, we find the elder Valerian nominated to the censorship, but Valerian was never actually elected censor.\n\nThe duties of the censors may be divided into three classes, all of which were closely connected with one another:\n\n\nThe original business of the censorship was at first of a much more limited kind, and was restricted almost entirely to taking the census, but the possession of this power gradually brought with it fresh power and new duties, as is shown below. A general view of these duties is briefly expressed in the following passage of Cicero: \"\"Censores populi aevitates, soboles, familias pecuniasque censento: urbis templa, vias, aquas, aerarium, vectigalia tuento: populique partes in tribus distribunto: exin pecunias, aevitates, ordines patiunto: equitum, peditumque prolem describunto: caelibes esse prohibento: mores populi regunto: probrum in senatu ne relinquunto.\"\" This can be translated as: \"The Censors are to determine the generations, origins, families, and properties of the people; they are to (watch over/protect) the city's temples, roads, waters, treasury, and taxes; they are to divide the people into three parts; next, they are to (allow/approve) the properties, generations, and ranks [of the people]; they are to describe the offspring of knights and footsoldiers; they are to forbid being unmarried; they are to guide the behavior of the people; they are not to overlook abuse in the Senate.\" \n\nThe Census, the first and principal duty of the censors, was always held in the Campus Martius, and from the year 435 BC onwards, in a special building called Villa Publica, which was erected for that purpose by the second pair of censors, Gaius Furius Pacilus Fusus and Marcus Geganius Macerinus.\n\nAn account of the formalities with which the census was opened is given in a fragment of the \"Tabulae Censoriae\", preserved by Varro. After the auspices had been taken, the citizens were summoned by a public crier to appear before the censors. Each tribe was called up separately, and the names in each tribe were probably taken according to the lists previously made out by the tribunes of the tribes. Every paterfamilias had to appear in person before the censors, who were seated in their curule chairs, and those names were taken first which were considered to be of good omen, such as Valerius, Salvius, Statorius, etc.\n\nThe census was conducted according to the judgment of the censor (\"ad arbitrium censoris\"), but the censors laid down certain rules, sometimes called \"leges censui censendo\", in which mention was made of the different kinds of property subject to the census, and in what way their value was to be estimated. According to these laws, each citizen had to give an account of himself, of his family, and of his property upon oath, \"declared from the heart\".\n\nFirst he had to give his full name (praenomen, nomen, and cognomen) and that of his father, or if he were a \"Libertus\" (\"freedman\") that of his patron, and he was likewise obliged to state his age. He was then asked, \"You, declaring from your heart, do you have a wife?\" and if married he had to give the name of his wife, and likewise the number, names, and ages of his children, if any. Single women and orphans were represented by their guardians; their names were entered in separate lists, and they were not included in the sum total of heads.\n\nAfter a citizen had stated his name, age, family, etc., he then had to give an account of all his property, so far as it was subject to the census. Only such things were liable to the census (\"censui censendo\") as were property according to the Quiritarian law. At first, each citizen appears to have merely given the value of his whole property in general without entering into details; but it soon became the practice to give a minute specification of each article, as well as the general value of the whole.\n\nLand formed the most important article of the census, but public land, the possession of which only belonged to a citizen, was excluded as not being Quiritarian property. If we may judge from the practice of the imperial period, it was the custom to give a most minute specification of all such land as a citizen held according to the Quiritarian law. He had to state the name and location of the land, and to specify what portion of it was arable, what meadow, what vineyard, and what olive-ground: and of the land thus described, he had to give his assessment of its value.\n\nSlaves and cattle formed the next most important item. The censors also possessed the right of calling for a return of such objects as had not usually been given in, such as clothing, jewels, and carriages. It has been doubted by some modern writers whether the censors possessed the power of setting a higher valuation on the property than the citizens themselves gave, but when we recollect the discretionary nature of the censors' powers, and the necessity almost that existed, in order to prevent fraud, that the right of making a surcharge should be vested in somebody's hands, we can hardly doubt that the censors had this power. It is moreover expressly stated that on one occasion they made an extravagant surcharge on articles of luxury; and even if they did not enter in their books the property of a person at a higher value than he returned it, they accomplished the same end by compelling him to pay a tax upon the property at a higher rate than others. The tax was usually one per thousand upon the property entered in the books of the censors, but on one occasion the censors compelled a person to pay eight per thousand as a punishment.\n\nA person who voluntarily absented himself from the census was considered \"incensus\" and subject to the severest punishment. Servius Tullius is said to have threatened such individuals with imprisonment and death, and in the Republican period he might be sold by the state as a slave In the later times of the republic, a person who was absent from the census might be represented by another, and be thus registered by the censors. Whether the soldiers who were absent on service had to appoint a representative is uncertain. In ancient times, the sudden outbreaks of war prevented the census from being taken, because a large number of the citizens would necessarily be absent. It is supposed from a passage in Livy that in later times the censors sent commissioners into the provinces with full powers to take the census of the Roman soldiers there, but this seems to have been a special case. It is, on the contrary, probable from the way in which Cicero pleads the absence of Archias from Rome with the army under Lucullus, as a sufficient reason for his not having been enrolled in the census, that service in the army was a valid excuse for absence.\n\nAfter the censors had received the names of all the citizens with the amount of their property, they then had to make out the lists of the tribes, and also of the classes and centuries; for by the legislation of Servius Tullius the position of each citizen in the state was determined by the amount of his property (Comitia Centuriata). These lists formed a most important part of the Tabulae Censoriae, under which name were included all the documents connected in any way with the discharge of the censors' duties. These lists, insofar as they were connected with the finances of the state, were deposited in the aerarium, which was the temple of Saturn; but the regular depositary for all the archives of the censors was in earlier times the Atrium Libertatis, near the Villa publica, and in later times the temple of the Nymphs.\n\nBesides the division of the citizens into tribes, centuries, and classes, the censors had also to make out the lists of the senators for the ensuing five years, or until new censors were appointed; striking out the names of such as they considered unworthy, and making additions to the body from those who were qualified. In the same manner they held a review of the Equestrians who received a horse from public funds (\"equites equo publico\"), and added and removed names as they judged proper. They also confirmed the princeps senatus, or appointed a new one. The princeps himself had to be a former censor.\n\nAfter the lists had been completed, the number of citizens was counted up, and the sum total announced. Accordingly, we find that in the account of a census, the number of citizens is likewise usually given. They are in such cases spoken of as \"capita\" (\"heads\"), sometimes with the addition of the word \"civium\" (\"of the citizens\"), and sometimes not. Hence, to be registered in the census was the same thing as \"having a head\" (\"caput habere\").\n\nA census was sometimes taken in the provinces, even under the republic. The Emperor sent into the provinces special officers called Censitores to take the census; but the duty was sometimes discharged by the Imperial legati. The Censitores were assisted by subordinate officers, called Censuales, who made out the lists, &c. In Rome, the census was still taken under the empire, but the old ceremonies connected with it were no longer performed, and the ceremony of the lustration was not performed after the time of Vespasian. The jurists Paulus and Ulpian each wrote works on the census in the imperial period; and several extracts from these works are given in a chapter in the \"Digest\" (50 15). \n\nThe word \"census\", besides the conventional meaning of \"valuation\" of a person's estate, has other meaning in Rome; it could refer to: \n\nKeeping the public morals (\"regimen morum\", or in the empire \"cura morum\" or \"praefectura morum\") was the second most important branch of the censors' duties, and the one which caused their office to be one of the most revered and the most dreaded in the Roman state; hence they were also known as \"Castigatores\" (\"chastisers\"). It naturally grew out of the right which they possessed of excluding persons from the lists of citizens; for, as has been well remarked, \"they would, in the first place, be the sole judges of many questions of fact, such as whether a citizen had the qualifications required by law or custom for the rank which he claimed, or whether he had ever incurred any judicial sentence, which rendered him infamous: but from thence the transition was easy, according to Roman notions, to the decisions of questions of right; such as whether a citizen was really worthy of retaining his rank, whether he had not committed some act as justly degrading as those which incurred the sentence of the law.\" \n\nIn this manner, the censors gradually assumed at least nominal complete superintendence over the whole public and private life of every citizen. They were constituted as the conservators of public morality; they were not simply to prevent crime or particular acts of immorality, but rather to maintain the traditional Roman character, ethics, and habits (\"mos majorum\")—\"regimen morum\" also encompassed this protection of traditional ways, which was called in the times of the empire \"cura\" (\"supervision\") or \"praefectura\" (\"command\"). The punishment inflicted by the censors in the exercise of this branch of their duties was called \"nota\" (\"mark, letter\") or \"notatio\", or \"animadversio censoria\" (\"censorial reproach\"). In inflicting it, they were guided only by their conscientious convictions of duty; they had to take an oath that they would act biased by neither partiality nor favour; and, in addition to this, they were bound in every case to state in their lists, opposite the name of the guilty citizen, the cause of the punishment inflicted on him, \"Subscriptio censoria\".\n\nThis part of the censors' office invested them with a peculiar kind of jurisdiction, which in many respects resembled the exercise of public opinion in modern times; for there are innumerable actions which, though acknowledged by every one to be prejudicial and immoral, still do not come within the reach of the positive laws of a country; as often said, \"immorality does not equal illegality\". Even in cases of real crimes, the positive laws frequently punish only the particular offence, while in public opinion the offender, even after he has undergone punishment, is still incapacitated for certain honours and distinctions which are granted only to persons of unblemished character.\n\nHence the Roman censors might brand a man with their \"censorial mark\" (\"nota censoria\") in case he had been convicted of a crime in an ordinary court of justice, and had already suffered punishment for it. The consequence of such a nota was only \"ignominia\" and not \"infamia\". \"Infamia\" and the censorial verdict was not a \"judicium\" or \"res judicata\", for its effects were not lasting, but might be removed by the following censors, or by a \"lex\" (roughly \"law\"). A censorial mark was moreover not valid unless both censors agreed. The \"ignominia\" was thus only a transitory reduction of status, which does not even appear to have deprived a magistrate of his office, and certainly did not disqualify persons labouring under it for obtaining a magistracy, for being appointed as \"judices\" by the praetor, or for serving in the Roman armies. Mamercus Aemilius Mamercinus was thus, notwithstanding the reproach of the censors (\"animadversio censoria\"), made dictator.\n\nA person might be branded with a censorial mark in a variety of cases, which it would be impossible to specify, as in a great many instances it depended upon the discretion of the censors and the view they took of a case; and sometimes even one set of censors would overlook an offence which was severely chastised by their successors. But the offences which are recorded to have been punished by the censors are of a threefold nature.\nA person who had been branded with a \"nota censoria\", might, if he considered himself wronged, endeavour to prove his innocence to the censors, and if he did not succeed, he might try to gain the protection of one of the censors, that he might intercede on his behalf.\n\nThe punishments inflicted by the censors generally differed according to the station which a man occupied, though sometimes a person of the highest rank might suffer all the punishments at once, by being degraded to the lowest class of citizens. But they are generally divided into four classes:\n\n\nIt was this authority of the Roman censors which eventually developed into the modern meaning of \"censor\" and \"censorship\"—i.e., officials who review published material and forbid the publication of material judged to be contrary to \"public morality\" as the term is interpreted in a given political and social environment.\n\nThe administration of the state's finances was another part of the censors' office. In the first place the \"tributum\", or property-tax, had to be paid by each citizen according to the amount of his property registered in the census, and, accordingly, the regulation of this tax naturally fell under the jurisdiction of the censors. They also had the superintendence of all the other revenues of the state, the \"vectigalia\", such as the tithes paid for the public lands, the salt works, the mines, the customs, etc.\n\nThe censors typically auctioned off to the highest bidder for the space of a \"lustrum\" the collection of the tithes and taxes (tax farming). This auctioning was called \"venditio\" or \"locatio\", and seems to have taken place in the month of March, in a public place in Rome The terms on which they were let, together with the rights and duties of the purchasers, were all specified in the \"leges censoriae\", which the censors published in every case before the bidding commenced. For further particulars see Publicani.\n\nThe censors also possessed the right, though probably not without the assent of the Senate, of imposing new \"vectigalia\", and even of selling the land belonging to the state. It would thus appear that it was the duty of the censors to bring forward a budget for a five-year period, and to take care that the income of the state was sufficient for its expenditure during that time. In part, their duties resembled those of a modern minister of finance. The censors, however, did not receive the revenues of the state. All the public money was paid into the \"aerarium\", which was entirely under the jurisdiction of the senate; and all disbursements were made by order of this body, which employed the quaestors as its officers.\n\nIn one important department the censors were entrusted with the expenditure of the public money, though the actual payments were no doubt made by the quaestors. The censors had the general superintendence of all the public buildings and works (\"opera publica\"), and to meet the expenses connected with this part of their duties, the senate voted them a certain sum of money or certain revenues, to which they were restricted, but which they might at the same time employ according to their discretion. They had to see that the temples and all other public buildings were in a good state of repair, that no public places were encroached upon by the occupation of private persons, and that the aqueduct, roads, drains, etc. were properly attended to.\n\nThe repairs of the public works and the keeping of them in proper condition were let out by the censors by public auction to the lowest bidder, just as the \"vectigalia\" were let out to the highest bidder. These expenses were called \"ultrotributa\", and hence we frequently find \"vectigalia\" and \"ultrotributa\" contrasted with one another. The persons who undertook the contract were called \"conductores\", \"mancipes\", \"redemptores\", \"susceptores\", etc.; and the duties they had to discharge were specified in the Leges Censoriae. The censors had also to superintend the expenses connected with the worship of the gods, even for instance the feeding of the sacred geese in the Capitol; these various tasks were also let out on contract. It was ordinary for censors to expend large amounts of money (“by far the largest and most extensive” of the state) in their public works.\n\nBesides keeping existing public buildings and facilities in a proper state of repair, the censors were also in charge of constructing new ones, either for ornament or utility, both in Rome and in other parts of Italy, such as temples, basilicae, theatres, porticoes, fora, walls of towns, aqueducts, harbours, bridges, cloacae, roads, etc. These works were either performed by them jointly, or they divided between them the money, which had been granted to them by the senate. They were let out to contractors, like the other works mentioned above, and when they were completed, the censors had to see that the work was performed in accordance with the contract: this was called \"opus probare\" or \"in acceptum referre\".\n\nThe aediles had likewise a superintendence over the public buildings, and it is not easy to define with accuracy the respective duties of the censors and aediles, but it may be remarked in general that the superintendence of the aediles had more of a police character, while that of the censors were more financial in subject matter.\n\nAfter the censors had performed their various duties and taken the five-yearly census, the \"lustrum\", a solemn purification of the people, followed. When the censors entered upon their office, they drew lots to see which of them should perform this purification; but both censors were of course obliged to be present at the ceremony.\n\nLong after the Roman census was no longer taken, the Latin word \"lustrum\" has survived, and been adopted in some modern languages, in the derived sense of a period of five years, i.e. half a decennium.\n\n\n"}
{"id": "6292", "url": "https://en.wikipedia.org/wiki?curid=6292", "title": "Convex set", "text": "Convex set\n\nIn convex geometry, a convex set is a subset of an affine space that is closed under convex combinations. More specifically, in a Euclidean space, a convex region is a region where, for every pair of points within the region, every point on the straight line segment that joins the pair of points is also within the region. For example, a solid cube is a convex set, but anything that is hollow or has an indent, for example, a crescent shape, is not convex.\n\nThe boundary of a convex set is always a convex curve. The intersection of all convex sets containing a given subset of Euclidean space is called the convex hull of . It is the smallest convex set containing .\n\nA convex function is a real-valued function defined on an interval with the property that its epigraph (the set of points on or above the graph of the function) is a convex set. Convex minimization is a subfield of optimization that studies the problem of minimizing convex functions over convex sets. The branch of mathematics devoted to the study of properties of convex sets and convex functions is called convex analysis.\n\nThe notion of a convex set can be generalized as described below.\n\nLet be a vector space over the real numbers, or, more generally, some ordered field. This includes Euclidean spaces. A set in is said to be convex if, for all and in and all in the interval , the point also belongs to . In other words, every point on the line segment connecting and is in . This implies that a convex set in a real or complex topological vector space is path-connected, thus connected.\nFurthermore, is strictly convex if every point on the line segment connecting and other than the endpoints is inside the interior of .\n\nA set is called absolutely convex if it is convex and balanced.\n\nThe convex subsets of (the set of real numbers) are simply the intervals of . Some examples of convex subsets of the Euclidean plane are solid regular polygons, solid triangles, and intersections of solid triangles. Some examples of convex subsets of a Euclidean 3-dimensional space are the Archimedean solids and the Platonic solids. The Kepler-Poinsot polyhedra are examples of non-convex sets.\n\nA set that is not convex is called a \"non-convex set\". A polygon that is not a convex polygon is sometimes called a concave polygon, and some sources more generally use the term \"concave set\" to mean a non-convex set, but most authorities prohibit this usage.\n\nThe complement of a convex set, such as the epigraph of a concave function, is sometimes called a \"reverse convex set\", especially in the context of mathematical optimization.\n\nIf is a convex set in -dimensional space, then for any collection of , , -dimensional vectors in , and for any nonnegative numbers such that , then one has:\nA vector of this type is known as a convex combination of .\n\nThe collection of convex subsets of a vector space has the following properties:\n\nClosed convex sets are convex sets that contain all their limit points. They can be characterised as the intersections of \"closed half-spaces\" (sets of point in space that lie on and to one side of a hyperplane).\n\nFrom what has just been said, it is clear that such intersections are convex, and they will also be closed sets. To prove the converse, i.e., every convex set may be represented as such intersection, one needs the supporting hyperplane theorem in the form that for a given closed convex set and point outside it, there is a closed half-space that contains and not . The supporting hyperplane theorem is a special case of the Hahn–Banach theorem of functional analysis.\n\nLet C be a convex body in the plane. We can inscribe a rectangle r in C such that a homothetic copy R of r is circumscribed about C. The positive homothety ratio is at most 2 and:\n\nEvery subset of the vector space is contained within a smallest convex set (called the convex hull of ), namely the intersection of all convex sets containing . The convex-hull operator Conv() has the characteristic properties of a hull operator:\nThe convex-hull operation is needed for the set of convex sets to form a lattice, in which the \"\"join\"\" operation is the convex hull of the union of two convex sets\nThe intersection of any collection of convex sets is itself convex, so the convex subsets of a (real or complex) vector space form a complete lattice.\n\nIn a real vector-space, the \"Minkowski sum\" of two (non-empty) sets, and , is defined to be the set formed by the addition of vectors element-wise from the summand-sets\nMore generally, the \"Minkowski sum\" of a finite family of (non-empty) sets is the set formed by element-wise addition of vectors\n\nFor Minkowski addition, the \"zero set\"  containing only the zero vector  has special importance: For every non-empty subset S of a vector space\nin algebraic terminology, is the identity element of Minkowski addition (on the collection of non-empty sets).\n\nMinkowski addition behaves well with respect to the operation of taking convex hulls, as shown by the following proposition:\n\nLet be subsets of a real vector-space, the convex hull of their Minkowski sum is the Minkowski sum of their convex hulls\n\nThis result holds more generally for each finite collection of non-empty sets:\n\nIn mathematical terminology, the operations of Minkowski summation and of forming convex hulls are commuting operations.\n\nThe Minkowski sum of two compact convex sets is compact. The sum of a compact convex set and a closed convex set is closed.\n\nThe notion of convexity in the Euclidean space may be generalized by modifying the definition in some or other aspects. The common name \"generalized convexity\" is used, because the resulting objects retain certain properties of convex sets.\n\nLet be a set in a real or complex vector space. is star convex if there exists an in such that the line segment from to any point in is contained in . Hence a non-empty convex set is always star-convex but a star-convex set is not always convex.\n\nAn example of generalized convexity is orthogonal convexity.\n\nA set in the Euclidean space is called orthogonally convex or ortho-convex, if any segment parallel to any of the coordinate axes connecting two points of lies totally within . It is easy to prove that an intersection of any collection of orthoconvex sets is orthoconvex. Some other properties of convex sets are valid as well.\n\nThe definition of a convex set and a convex hull extends naturally to geometries which are not Euclidean by defining a geodesically convex set to be one that contains the geodesics joining any two points in the set.\n\nConvexity can be extended for a space endowed with the order topology, using the total order of the space.\n\nLet . The subspace is a convex set if for each pair of points in such that , the interval is contained in . That is, is convex if and only if for all in , implies .\n\nThe notion of convexity may be generalised to other objects, if certain properties of convexity are selected as axioms.\n\nGiven a set , a convexity over is a collection of subsets of satisfying the following axioms:\n\n\nThe elements of are called convex sets and the pair is called a convexity space. For the ordinary convexity, the first two axioms hold, and the third one is trivial.\n\nFor an alternative definition of abstract convexity, more suited to discrete geometry, see the \"convex geometries\" associated with antimatroids.\n\n"}
{"id": "6293", "url": "https://en.wikipedia.org/wiki?curid=6293", "title": "Cairo", "text": "Cairo\n\nCairo ( ; ', , ') is the capital and largest city of Egypt. The city's metropolitan area is the largest in the Middle East and the Arab world, and 15th-largest in the world, and is associated with ancient Egypt, as the famous Giza pyramid complex and the ancient city of Memphis are located in its geographical area. Located near the Nile Delta, modern Cairo was founded in 969 CE by Jawhar of the Fatimid dynasty, but the land composing the present-day city was the site of ancient national capitals whose remnants remain visible in parts of Old Cairo. Cairo has long been a center of the region's political and cultural life, and is titled \"the city of a thousand minarets\" for its preponderance of Islamic architecture. Cairo is considered a World City with a \"Beta +\" classification according to GaWC.\n\nCairo has the oldest and largest film and music industries in the Arab world, as well as the world's second-oldest institution of higher learning, Al-Azhar University. Many international media, businesses, and organizations have regional headquarters in the city; the Arab League has had its headquarters in Cairo for most of its existence.\n\nWith a population of 6.76 million spread over , Cairo is by far the largest city in Egypt. An additional 9.5 million inhabitants live in close proximity to the city. Cairo, like many other mega-cities, suffers from high levels of pollution and traffic. Cairo's metro, one of only two in Africa (the other is in Algiers, Algeria), ranks among the fifteen busiest in the world, with over 1 billion annual passenger rides. The economy of Cairo was ranked first in the Middle East in 2005, and 43rd globally on \"Foreign Policy\" 2010 Global Cities Index.\n\nEgyptians often refer to Cairo as ' (; ), the Egyptian Arabic name for Egypt itself, emphasizing the city's importance for the country. Its official name ' () means \"the Vanquisher\" or \"the Conqueror\", supposedly due to the fact that the planet Mars, \"an-Najm al-Qāhir\" (, literally \"the Conquering Star\"), was rising at the time when the city was founded, possibly also in reference to the much awaited arrival of Caliph Al-Mu'izz li-Din Allah who reached Cairo in 973 from Mahdia, the old Fatimid capital. In Coptic the city is known as Kahire (), meaning \"Place of the Sun\", possibly referring to the ancient city of Heliopolis, the main seat of worship of the solar deity Ra—(or Re). The location of the ancient city is the suburb of Ain Shams (, literally \"Sun-Eye\" or \"Eye of the Sun\"). The ancient Egyptian name for the area is thought to be Khere-Ohe, \"The Place of Combat\", supposedly in reference to a mythical battle that took place between Seth and Horus. Sometimes the city is informally referred to as ' (; ).\n\nThe area around present-day Cairo, especially Memphis, had long been a focal point of Ancient Egypt due to its strategic location just upstream from the Nile Delta. However, the origins of the modern city are generally traced back to a series of settlements in the first millennium. Around the turn of the 4th century, as Memphis was continuing to decline in importance, the Romans established a fortress town along the east bank of the Nile. This fortress, known as Babylon, remained the nucleus of the Roman, and, later, the Byzantine, city and is the oldest structure in the city today. It is also situated at the nucleus of the Coptic Orthodox community, which separated from the Roman and Byzantine church in the late 4th century. Many of Cairo's oldest Coptic churches, including the Hanging Church, are located along the fortress walls in a section of the city known as Coptic Cairo.\n\nFollowing the Muslim conquest in 640 AD the conqueror Amr ibn As settled to the north of the Babylon in an area that became known as al-Fustat. Originally a tented camp (\"Fustat\" signifies \"City of Tents\") Fustat became a permanent settlement and the first capital of Islamic Egypt.\n\nIn 750, following the overthrow of the Ummayad caliphate by the Abbasids, the new rulers created their own settlement to the northeast of Fustat which became their capital. This was known as al-Askar (the city of sections, or cantonments) as it was laid out like a military camp.\n\nA rebellion in 869 by Ahmad ibn Tulun led to the abandonment of Al Askar and the building of another settlement, which became the seat of government. This was al-Qatta'i (\"the Quarters\"), to the north of Fustat and closer to the river. Al Qatta'i was centred around a palace and ceremonial mosque, now known as the Mosque of ibn Tulun.\n\nIn 905 the Abbasids re-asserted control of the country and their governor returned to Fustat, razing al Qattai to the ground.\n\nIn 969 the Fatimid conquest saw the founding of yet another settlement, further north again, called al Qahira (\"the Victorious\", or \"the Conqueror\"), the nascent city of Cairo. However Fustat remained the capital, until 1168, when the then vizier of al Qahira transferred his government there and had Fustat destroyed by fire.\n\nAs Qahira expanded these earlier settlements were encompassed, and have since become part of the city of Cairo as it expanded and spread; they are now collectively known as \"Old Cairo\".\n\nIn 968, the Fatimids were led by General Jawhar al-Siqilli with his Kutama army, to establish a new capital for the Fatimid dynasty. Egypt was conquered from their base in Ifriqiya and a new fortified city northeast of Fustat was established. It took four years for Jawhar to build the city, initially known as al-Manṣūriyyah, which was to serve as the new capital of the caliphate. During that time, Jawhar also commissioned the construction of the al-Azhar Mosque, which developed into the third-oldest university in the world. Cairo would eventually become a centre of learning, with the library of Cairo containing hundreds of thousands of books. When Caliph al-Mu'izz li Din Allah finally arrived from the old Fatimid capital of Mahdia in Tunisia in 973, he gave the city its present name, \"al-Qahira\" (\"The Victorious\").\n\nFor nearly 200 years after Cairo was established, the administrative centre of Egypt remained in Fustat. However, in 1168 the Fatimids under the leadership of Vizier Shawar set fire to Fustat to prevent Cairo's capture by the Crusaders. Egypt's capital was permanently moved to Cairo, which was eventually expanded to include the ruins of Fustat and the previous capitals of al-Askar and al-Qatta'i. While the Fustat fire successfully protected the city of Cairo, a continuing power struggle between Shawar, King Amalric I of Jerusalem, and the Zengid general Shirkuh led to the downfall of the Fatimid establishment.\n\nIn 1169 Saladin was appointed as the new vizier of Egypt by the Fatimids and two years later he seized power from the family of the last Fatimid caliph, al-'Āḍid. As the first Sultan of Egypt, Saladin established the Ayyubid dynasty, based in Cairo, and aligned Egypt with the Abbasids, who were based in Baghdad. During his reign, Saladin also constructed the Cairo Citadel, which served as the seat of the Egyptian government until the mid-19th century.\nIn 1250 slave soldiers, known as the Mamluks, seized control of Egypt and like many of their predecessors established Cairo as the capital of their new dynasty. Continuing a practice started by the Ayyubids, much of the land occupied by former Fatimid palaces was sold and replaced by newer buildings. Construction projects initiated by the Mamluks pushed the city outward while also bringing new infrastructure to the centre of the city. Meanwhile, Cairo flourished as a centre of Islamic scholarship and a crossroads on the spice trade route among the civilisations in Afro-Eurasia. By 1340, Cairo had a population of close to half a million, making it the largest city west of China.\n\nAlthough Cairo avoided Europe's stagnation during the Late Middle Ages, it could not escape the Black Death, which struck the city more than fifty times between 1348 and 1517. During its initial, and most deadly waves, approximately 200,000 people were killed by the plague, and, by the 15th century, Cairo's population had been reduced to between 150,000 and 300,000. The city's status was further diminished after Vasco da Gama discovered a sea route around the Cape of Good Hope between 1497 and 1499, thereby allowing spice traders to avoid Cairo.\nCairo's political influence diminished significantly after the Ottomans supplanted Mamluk power over Egypt in 1517. Ruling from Constantinople, Sultan Selim I relegated Egypt to a mere province, with Cairo as its capital. For this reason, the history of Cairo during Ottoman times is often described as inconsequential, especially in comparison to other time periods. However, during the 16th and 17th centuries, Cairo remained an important economic and cultural centre. Although no longer on the spice route, the city facilitated the transportation of Yemeni coffee and Indian textiles, primarily to Anatolia, North Africa, and the Balkans. Cairene merchants were instrumental in bringing goods to the barren Hejaz, especially during the annual hajj to Mecca. It was during this same period that al-Azhar University reached the predominance among Islamic schools that it continues to hold today; pilgrims on their way to hajj often attested to the superiority of the institution, which had become associated with Egypt's body of Islamic scholars. By the 16th century, Cairo also had high-rise apartment buildings where the two lower floors were for commercial and storage purposes and the multiple stories above them were rented out to tenants.\n\nUnder the Ottomans, Cairo expanded south and west from its nucleus around the Citadel. The city was the second-largest in the empire, behind only Constantinople, and, although migration was not the primary source of Cairo's growth, twenty percent of its population at the end of the 18th century consisted of religious minorities and foreigners from around the Mediterranean. Still, when Napoleon arrived in Cairo in 1798, the city's population was less than 300,000, forty percent lower than it was at the height of Mamluk—and Cairene—influence in the mid-14th century.\n\nThe French occupation was short-lived as British and Ottoman forces, including a sizeable Albanian contingent, recaptured the country in 1801. Cairo itself was besieged by a British and Ottoman force culminating with the French surrender on 22 June 1801. The British vacated Egypt two years later, leaving the Ottomans, the Albanians, and the long-weakened Mamluks jostling for control of the country. Continued civil war allowed an Albanian named Muhammad Ali Pasha to ascend to the role of commander and eventually, with the approval of the religious establishment, viceroy of Egypt in 1805.\n\nUntil his death in 1848, Muhammad Ali Pasha instituted a number of social and economic reforms that earned him the title of founder of modern Egypt. However, while Muhammad Ali initiated the construction of public buildings in the city, those reforms had minimal effect on Cairo's landscape. Bigger changes came to Cairo under Isma'il Pasha (r. 1863–1879), who continued the modernisation processes started by his grandfather. Drawing inspiration from Paris, Isma'il envisioned a city of maidans and wide avenues; due to financial constraints, only some of them, in the area now composing Downtown Cairo, came to fruition. Isma'il also sought to modernize the city, which was merging with neighboring settlements, by establishing a public works ministry, bringing gas and lighting to the city, and opening a theater and opera house.\n\nThe immense debt resulting from Isma'il's projects provided a pretext for increasing European control, which culminated with the British invasion in 1882. The city's economic centre quickly moved west toward the Nile, away from the historic Islamic Cairo section and toward the contemporary, European-style areas built by Isma'il. Europeans accounted for five percent of Cairo's population at the end of the 19th century, by which point they held most top governmental positions.\n\nThe British occupation was intended to be temporary, but it lasted well into the 20th century. Nationalists staged large-scale demonstrations in Cairo in 1919, five years after Egypt had been declared a British protectorate. Nevertheless, while this led to Egypt's independence in 1922, British troops remained in the country until 1956. During this time, urban Cairo, spurred by new bridges and transport links, continued to expand to include the upscale neighbourhoods of Garden City, Zamalek, and Heliopolis. Between 1882 and 1937, the population of Cairo more than tripled—from 347,000 to 1.3 million—and its area increased from .\n\nThe city was devastated during the 1952 riots known as the Cairo Fire or Black Saturday, which saw the destruction of nearly 700 shops, movie theatres, casinos and hotels in Downtown Cairo. The British departed Cairo following the Egyptian Revolution of 1952, but the city's rapid growth showed no signs of abating. Seeking to accommodate the increasing population, President Gamal Abdel Nasser redeveloped Maidan Tahrir and the Nile Corniche, and improved the city's network of bridges and highways. Meanwhile, additional controls of the Nile fostered development within Gezira Island and along the city's waterfront. The metropolis began to encroach on the fertile Nile Delta, prompting the government to build desert satellite towns and devise incentives for city-dwellers to move to them.\n\nDespite these efforts, Cairo's population has doubled since the 1960s, reaching close to seven million (with an additional ten million in its urban area). Concurrently, Cairo has established itself as a political and economic hub for North Africa and the Arab world, with many multinational businesses and organisations, including the Arab League, operating out of the city.\n\nIn 1992, Cairo was hit by a damaging earthquake, that caused 545 deaths, 6,512 injuries and left 50,000 people homeless.\n\nCairo's Tahrir Square was the focal point of the 2011 Egyptian Revolution against former president Hosni Mubarak. Over 2 million protesters were at Cairo's Tahrir square. More than 50,000 protesters first occupied the square on 25 January, during which the area's wireless services were reported to be impaired. In the following days Tahrir Square continued to be the primary destination for protests in Cairo as it took place following a popular uprising that began on Tuesday, 25 January 2011 and is still continuing as of February 2012. The uprising was mainly a campaign of non-violent civil resistance, which featured a series of demonstrations, marches, acts of civil disobedience, and labour strikes. Millions of protesters from a variety of socio-economic and religious backgrounds demanded the overthrow of the regime of Egyptian President Hosni Mubarak. Despite being predominantly peaceful in nature, the revolution was not without violent clashes between security forces and protesters, with at least 846 people killed and 6,000 injured. The uprising took place in Cairo, Alexandria, and in other cities in Egypt, following the Tunisian revolution that resulted in the overthrow of the long-time Tunisian president Zine El Abidine Ben Ali. On 11 February, following weeks of determined popular protest and pressure, Hosni Mubarak resigned from office.\n\nUnder the rule of President el-Sisi, in March 2015 plans were announced for another yet-unnamed planned city to be built further east of the existing satellite city of New Cairo, intended to serve as the new capital of Egypt.\n\nCairo is located in northern Egypt, known as Lower Egypt, south of the Mediterranean Sea and west of the Gulf of Suez and Suez Canal. The city is along the Nile River, immediately south of the point where the river leaves its desert-bound valley and branches into the low-lying Nile Delta region. Although the Cairo metropolis extends away from the Nile in all directions, the city of Cairo resides only on the east bank of the river and two islands within it on a total area of .\n\nUntil the mid-19th century, when the river was tamed by dams, levees, and other controls, the Nile in the vicinity of Cairo was highly susceptible to changes in course and surface level. Over the years, the Nile gradually shifted westward, providing the site between the eastern edge of the river and the Mokattam highlands on which the city now stands. The land on which Cairo was established in 969 (present-day Islamic Cairo) was located underwater just over three hundred years earlier, when Fustat was first built.\n\nLow periods of the Nile during the 11th century continued to add to the landscape of Cairo; a new island, known as \"Geziret al-Fil\", first appeared in 1174, but eventually became connected to the mainland. Today, the site of \"Geziret al-Fil\" is occupied by the Shubra district. The low periods created another island at the turn of the 14th century that now composes Zamalek and Gezira. Land reclamation efforts by the Mamluks and Ottomans further contributed to expansion on the east bank of the river.\n\nBecause of the Nile's movement, the newer parts of the city—Garden City, Downtown Cairo, and Zamalek—are located closest to the riverbank. The areas, which are home to most of Cairo's embassies, are surrounded on the north, east, and south by the older parts of the city. Old Cairo, located south of the centre, holds the remnants of Fustat and the heart of Egypt's Coptic Christian community, Coptic Cairo. The Boulaq district, which lies in the northern part of the city, was born out of a major 16th-century port and is now a major industrial centre. The Citadel is located east of the city centre around Islamic Cairo, which dates back to the Fatimid era and the foundation of Cairo. While western Cairo is dominated by wide boulevards, open spaces, and modern architecture of European influence, the eastern half, having grown haphazardly over the centuries, is dominated by small lanes, crowded tenements, and Islamic architecture.\n\nNorthern and extreme eastern parts of Cairo, which include satellite towns, are among the most recent additions to the city, as they developed in the late-20th and early-21st centuries to accommodate the city's rapid growth. The western bank of the Nile is commonly included within the urban area of Cairo, but it composes the city of Giza and the Giza Governorate. Giza has also undergone significant expansion over recent years, and today the city, although still a suburb of Cairo, has a population of 2.7 million. The Cairo Governorate was just north of the Helwan Governorate from 2008 when some Cairo's southern districts, including Maadi and New Cairo, were split off and annexed into the new governorate, to 2011 when the Helwan Governorate was reincorporated into the Cairo Governorate.\nIn Cairo, and along the Nile River Valley, the climate is a hot desert climate (\"BWh\" according to the Köppen climate classification system), but often with high humidity as it is not very far from the Mediterranean Sea and the Nile Delta. Wind storms can be frequent, bringing Saharan dust into the city, sometimes from March to May (see Khamasin) and the air often becomes uncomfortably dry. High temperatures in winter range from , while night-time lows drop to below , often to . In summer, the highs rarely surpass , and lows drop to about . Rainfall is sparse and only happens in the colder months, but sudden showers do cause harsh flooding. Snowfall is extremely rare; a small amount of graupel, widely believed to be snow, fell on Cairo's easternmost suburbs on 13 December 2013, the first time Cairo's area received this kind of precipitation in many decades. Dewpoints in the hottest months range from in June to in August.\n\nThe Greater Cairo is the largest metropolitan area in Africa. It consists of Cairo Governorate, parts of Giza Governorate, and parts of Qalyubia Governorate.\n\n6th of October City, west of Cairo, and New Cairo, east of Cairo, are major urban developments which have been built to accommodate additional growth and development of the Cairo area. New development includes several high-end residential developments.\n\nIn March 2015, plans were announced for a yet-unnamed planned city to be built east of Cairo, in an undeveloped area of the Cairo Governorate, which would serve as the administrative and financial capital of Egypt.\n\nCairo, as well as neighbouring Giza, has been established as Egypt's main centre for medical treatment, and despite some exceptions, has the most advanced level of medical care in the country. Cairo's hospitals include the JCI-accredited As-Salaam International Hospital—Corniche El Nile, Maadi (Egypt's largest private hospital with 350 beds), Ain Shams University Hospital, Dar El Fouad Hospital, as well as Kasr El Aini Hospital.\n\nGreater Cairo has long been the hub of education and educational services for Egypt and the region.\nToday, Greater Cairo is the centre for many government offices governing the Egyptian educational system, has the largest number of educational schools, and higher learning institutes among other cities and governorates of Egypt.\n\nSome of the International Schools found in Cairo:\n\nUniversities in Greater Cairo:\n\nCairo has an extensive road network, rail system, subway system and maritime services. Road transport is facilitated by personal vehicles, taxi cabs, privately owned public buses and Cairo microbuses. Cairo, specifically Ramses Square, is the centre of almost the entire Egyptian transportation network.\n\nThe subway system, officially called \"Metro (مترو)\", is a fast and efficient way of getting around Cairo. Metro network covers Helwan and other suburbs. It can get very crowded during rush hour. Two train cars (the fourth and fifth ones) are reserved for women only, although women may ride in any car they want.\n\nTrams in Greater Cairo (Heliopolis and Nasr City) exists now, while Cairo trolleybus was closed.\n\nAn extensive road network connects Cairo with other Egyptian cities and villages. There is a new Ring Road that surrounds the outskirts of the city, with exits that reach outer Cairo districts. There are flyovers and bridges, such as the Sixth of October bridge that, when the traffic is not heavy, allow fast means of transportation from one side of the city to the other.\n\nCairo traffic is known to be overwhelming and overcrowded. Traffic moves at a relatively fluid pace. Drivers tend to be aggressive, but are more courteous at junctions, taking turns going, with police aiding in traffic control of some congested areas.\n\nOn 25 October 2009 a passenger train ran into another one near Giza, just outside Cairo. Local news agencies reported at least 25 people dead. A local resident, Samhi Saleh Abdel Al, told reporters that \"\"the first train stopped after hitting a cow and 10 minutes later the second train arrived at full speed.\"\" One of the two trains was travelling from Cairo to Assiut, while the other was said to have been en route to Fayoum from Giza. Around 55 people were injured.\n\nFootball is the most popular sport in Egypt, and Cairo has a number of sporting teams that compete in national and regional leagues. The best known teams are Al-Ahly, El Zamalek and Al-Ismaily. Al-Ahly and El Zamalek annual football tournament is perhaps the most watched sports event in Egypt as well as the African-Arab region. Both teams are known as the \"rivals\" of Egyptian football, and are the first and the second champions in Africa and the Arab world. They play their home games at Cairo International Stadium or Naser Stadium, which is Egypt's 2nd largest stadium, Cairo's largest one and one of the largest stadiums in the world.\n\nThe Cairo International Stadium was built in 1960 and its multi-purpose sports complex that houses the main football stadium, an indoor stadium, several satellite fields that held several regional, continental and global games, including the African Games, U17 Football World Championship and was one of the stadiums scheduled that hosted the 2006 Africa Cup of Nations which was played in January 2006. Egypt later won the competition and went on to win the next edition In Ghana (2008) making the Egyptian and Ghanaian national teams the only teams to win the African Nations Cup Back to back which resulted in Egypt winning the title for a record number of six times in the history of African Continental Competition. This was followed by a third consecutive win in Angola 2010, making Egypt the only country with a record 3-consecutive and 7-total Continental Football Competition winner. This achievement had also placed the Egyptian football team as the #9 best team in the world's FIFA rankings.\n\nCairo failed at the applicant stage when bidding for the 2008 Summer Olympic Games, which was hosted in Beijing, China. However, Cairo did host the 2007 Pan Arab Games.\n\nThere are several other sports teams in the city that participate in several sports including el Gezira Sporting Club, el Shams Club, el Seid Club, Heliopolis Club and several smaller clubs, but the biggest clubs in Egypt (not in area but in sports) are Al Ahly and Al Zamalek. They have the two biggest football teams in Egypt. There are new sports clubs in the area of New Cairo (one hour far from Cairo's down town), these are Al Zohour sporting club, Wadi Degla sporting club and Platinum Club.\n\nMost of the sports federations of the country are also located in the city suburbs, including the Egyptian Football Association. The headquarters of the Confederation of African Football (CAF) was previously located in Cairo, before relocating to its new headquarters in 6 October City, a small city away from Cairo's crowded districts.\n\nIn October 2008, the Egyptian Rugby Federation was officially formed and granted membership into the International Rugby Board.\n\nEgypt is internationally known for the excellence of its squash players who excel in both professional and junior divisions. Egypt currently has seven players in the top ten of the PSA men’s world rankings, and three in the women’s top ten. Mohamed El Shorbagy held the world number one position for more than a year before being overtaken by compatriot Karim Abdel Gawad, who is currently number two behind Gregory Gaultier of France. Ramy Ashour and Amr Shabana are regarded as two of the most talented squash players in history. Shabana won the World Open title four times and Ashour twice, although his recent form has been hampered by injury. Egypt’s Nour El Sherbini has won the Women’s World Championship twice and has been women’s world number one for 16 consecutive months. On 30 April 2016, she became the youngest woman to win the Women's World Championship which was held in Malaysia. On April 2017 she retained her title by winning the Women's World Championship which was held in the Egyptian resort of El Gouna. \n\nPresident Mubarak inaugurated the new Cairo Opera House of the Egyptian National Cultural Centres on 10 October 1988, 17 years after the Royal Opera House had been destroyed by fire. The National Cultural Centre was built with the help of JICA, the Japan International Co-operation Agency and stands as a prominent feature for the Japanese-Egyptian co-operation and the friendship between these two nations.\n\nThe Khedivial Opera House or Royal Opera House was the original opera house in Cairo, Egypt. It was dedicated on 1 November 1869 and burned down on 28 October 1971. After the original opera house was destroyed, Cairo was without an opera house for nearly two decades until the opening of the new Cairo Opera House in 1988.\n\nEgypt's love of the arts in general can be traced back to the rich heritage bequeathed by the ancient Egyptians. In modern times, Egypt has enjoyed a strong cinematic tradition since the art of film making was first developed, early in the 20th century. A natural progression from the active theatre scene of the time, cinema rapidly evolved into a vast motion picture industry. This together with the much older music tradition, raised Egypt to become a center for motion picture production in the Middle East and the cultural capital of the Arab world.\n\nEgypt has also been a fount of Arabic literature, producing some of the 20th century's greatest Arab writers such as Taha Hussein and Tawfiq al-Hakim to Nobel Laureate, novelist Naguib Mahfouz. Each of them has written for the cinema.\n\nWith these credentials, it was clear that Cairo should aim to hold an international film festival. This dream came true on Monday 16 August 1976, when the first Cairo International Film Festival was launched by the Egyptian Association of Film Writers and Critics, headed by Kamal El-Mallakh. The Association ran the festival for seven years until 1983.\n\nThis achievement lead to the President of the Festival again contacting the FIAPF with the request that a competition should be included at the 1991 Festival. The request was granted.\n\nIn 1998, the Festival took place under the presidency of one of Egypt's leading actors, Hussein Fahmy, who was appointed by the Minister of Culture, Farouk Hosni, after the death of Saad El-Din Wahba.\n\nFour years later, the journalist and writer Cherif El-Shoubashy became president.\n\nFor 33 years The International Festival has awarded dozens of international superstars, including John Malkovich, Nicolas Cage, Morgan Freeman, Bud Spencer, Gina Lollobrigida, Ornella Muti, Sophia Loren, Claudia Cardinale, Victoria Abril, Elizabeth Taylor, Shashi Kapoor, Alain Delon, Goldie Hawn, Kurt Russell, Susan Sarandon, Greta Scacchi, Catherine Deneuve, Peter O'Toole, Charlize Theron, Julia Ormond, Mira Sorvino, Stuart Townsend, Alicia Silverstone, Priscilla Presley, Christopher Lee, Irene Papas, Marcello Mastroianni, Salma Hayek, Lucy Liu, Samuel L. Jackson, Tom Berenger and Omar Sharif, as well as directors like Robert Wise, Elia Kazan, Vanessa Redgrave, Oliver Stone, Roland Joffé, Carlos Saura, Ismail Merchant and Michelangelo Antonioni, in an annual celebration and examination of the state of cinema in the world today. The presidents of the Festival since it was founded in 1976 are Saad El-Din Wahba, Hussein Fahmy and Sherif El Shoubashy. This year the festival a milestone of 30 years in an annual celebration and examination of the state of cinema in the world today.\n\nThe Cairo Geniza is an accumulation of almost 200,000 Jewish manuscripts that were found in the genizah of the Ben Ezra synagogue (built 882) of Fustat, Egypt (now Old Cairo), the Basatin cemetery east of Old Cairo, and a number of old documents that were bought in Cairo in the later 19th century. These documents were written from about 870 to as late as 1880 AD and have now been archived in various American and European libraries. The Taylor-Schechter collection in the University of Cambridge runs to 140,000 manuscripts, a further 40,000 manuscripts are at the Jewish Theological Seminary of America.\n\nMost residents are Sunni Muslim, while the rest of the population is mostly Christian. Al-Azhar University, based in Cairo, is considered the leading authority of Sunni Islam worldwide. Most Christians are Coptic Orthodox. Until his death in March 2012, Pope Shenouda III of Alexandria was the leader of the Coptic Orthodox Church, followed by Pope Tawadros II who became Pope on November 18, 2012, whose residence is in Cairo. Cairo has several synagogues, but few Jews remain after Israel was established and the subsequent exodus, largely due to state sponsored discrimination. Tension between members of different religions has increased recently.\n\nCairo was ranked as the \"world's most 24-hour city\" in a 2011 study conducted by the social networking site Badoo, placing it well ahead of other famous big cities such as New York, London or Paris. The study's rankings were determined by measuring the amount of online activity at night versus during the day and by comparing peak-times for such activity in cities across the world. Cairo's highly nocturnal lifestyle is attributed not only to young people in nightclubs but also to the importance of cafés, which remain very active at night as social gathering places to smoke shisha, and even to the late-night public activeness of families with children.\n\nCairo is also one of few cities in the Muslim world to have several casinos.\n\nCairo accounts for 11% of Egypt's population and 22% of its economy (PPP). Cairo is also in every respect the centre of Egypt, as it has been almost since its founding in 969 AD. The majority of the nation's commerce is generated there, or passes through the city. The great majority of publishing houses and media outlets and nearly all film studios are there, as are half of the nation's hospital beds and universities. This has fueled rapid construction in the city—one building in five is less than 15 years old.\n\nThis astonishing growth until recently surged well ahead of city services. Homes, roads, electricity, telephone and sewer services were all suddenly in short supply. Analysts trying to grasp the magnitude of the change coined terms like \"hyper-urbanization\".\n\n\nTahrir Square was founded during the mid 19th century with the establishment of modern downtown Cairo. It was first named Ismailia Square, after the 19th-century ruler Khedive Ismail, who commissioned the new downtown district's 'Paris on the Nile' design. After the Egyptian Revolution of 1919 the square became widely known as Tahrir (Liberation) Square, though it was not officially renamed as such until after the 1952 Revolution which eliminated the monarchy. Several notable buildings surround the square including, the American University in Cairo's downtown campus, the Mogamma governmental administrative Building, the headquarters of the Arab League, the Nile Ritz Carlton Hotel, and the Egyptian Museum. Being at the heart of Cairo, the square witnessed several major protests over the years. However, the most notable event in the square was being the focal point of the 2011 Egyptian Revolution against former president Hosni Mubarak.\n\nThe Museum of Egyptian Antiquities, known commonly as the Egyptian Museum, is home to the most extensive collection of ancient Egyptian antiquities in the world. It has 136,000 items on display, with many more hundreds of thousands in its basement storerooms. Among its most famous collections on display are the finds from the Tomb of Tutankhamun.\n\nThe Cairo Tower is a free-standing tower with a revolving restaurant at the top. It provides a bird's eye view of Cairo to the restaurant patrons. It stands in the Zamalek district on Gezira Island in the Nile River, in the city centre. At , it is higher than the Great Pyramid of Giza, which stands some to the southwest.\n\nThis area of Cairo is so-named as it contains the remains of the ancient Roman fortress of Babylon and also overlaps the original site of Fustat, the first Arab settlement in Egypt (7th century AD) and the predecessor of later Cairo. The area is also known as Coptic Cairo as it holds a high concentration of old Christian churches including the Hanging Church, the Greek Orthodox Church of St. George, and other Christian or Coptic buildings, most of which are located over the site of the ancient Roman fortress. It is also the location of the Coptic Museum, which showcases the history of Coptic art from Greco-Roman to Islamic times, and of the Ben Ezra Synagogue, the oldest and best-known synagogue in Cairo, where the important collection of Geniza documents were discovered in the 19th century. To the north of this Coptic enclave is the Amr ibn al-'As Mosque, the first mosque in Egypt and the most important religious center of former Fustat, founded in 642 AD right after the Arab conquest but rebuilt many times since.\n\nCairo holds one of the greatest concentrations of historical monuments of Islamic architecture in the world. The areas around the old walled city and around the Citadel are characterized by hundreds of mosques, tombs, madrasas, mansions, caravanserais, and fortifications dating from the Islamic era and are often referred to as \"Islamic Cairo\", especially in English travel literature. It is also the location of several important religious shrines such as the al-Hussein Mosque (whose shrine is believed to hold the head of Husayn ibn Ali), the Mausoleum of Imam al-Shafi'i (founder of the Shafi'i madhhab, one of the primary schools of thought in Sunni Islamic jurisprudence), the Tomb of Sayyida Ruqayya, the Mosque of Sayyida Nafisa, and others.\n\nWhile the first mosque in Egypt was the Mosque of Amr ibn al-As in Fustat, the Mosque of Ibn Tulun is the oldest mosque to retain its original form and is a rare example of Abbasid architecture, from the classical period of Islamic civilization. It was built in 876–879 AD in a style inspired by the Abbasid capital of Samarra in Iraq. It is one of the largest mosques in Cairo and is often cited as one of the most beautiful. Another Abbasid construction, the Nilometer on Rhoda Island, is the oldest original structure in Cairo, built in 862 AD. It was designed to measure the level of the Nile, which was important for agricultural and administrative purposes.\n\nThe city named Cairo (Arabic: \"al-Qahira\") was founded to the northeast of Fustat in 959 AD by the victorious Fatimid army. The Fatimids built a separate palatial city which contained their palaces and institutions of government. It was enclosed by a circuit of walls, which were rebuilt in stone in the late 11th century AD by the vizir Badr al-Gamali, parts of which survive today at Bab Zuwayla in the south and Bab al-Futuh and Bab al-Nasr in the north.\n\nOne of the most important and lasting institutions founded in the Fatimid period was the Mosque of al-Azhar, founded in 970 AD, which competes with the Qarawiyyin in Fes for the title of oldest university in the world. Today, al-Azhar University is the foremost center of Islamic learning in the world and one of Egypt's largest universities with campuses across the country. The mosque itself retains significant Fatimid elements but has been added to and expanded in subsequent centuries, notably by the Mamluk sultans Qaitbay and al-Ghuri and by Abd al-Rahman Katkhuda in the 18th century.\n\nOther extant monuments from the Fatimid era include the large Mosque of al-Hakim, the al-Aqmar mosque, Juyushi Mosque, Lulua Mosque, and the Mosque of Salih Tala'i.\n\nThe most prominent architectural heritage of medieval Cairo, however, dates from the Mamluk period, from 1250 to 1517 AD. The Mamluk sultans and elites were eager patrons of religious and scholarly life, commonly building religious or funerary complexes whose functions could include a mosque, madrasa, khanqah (for Sufis), water distribution centers (sabils), and mausoleum for themselves and their families. \nAmong the best-known examples of Mamluk monuments in Cairo are the huge Mosque-Madrasa of Sultan Hasan, the Mosque of Amir al-Maridani, the Mosque of Sultan al-Mu'ayyad (whose twin minarets were built above the gate of Bab Zuwayla), the Sultan Al-Ghuri complex, the funerary complex of Sultan Qaytbay in the Northern Cemetery, and the trio of monuments in the Bayn al-Qasrayn area comprising the complex of Sultan al-Mansur Qalawun, the Madrasa of al-Nasir Muhammad, and the Madrasa of Sultan Barquq. It is said that a lot of the columns found in mosques were taken from the Coptic churches because of their beautiful artistic carvings and placed in mosques.\n\nThe Mamluks, and the later Ottomans, also built wikalas or caravanserais to house merchants and goods due to the important role of trade and commerce in Cairo's economy. The most famous example still intact today is the Wikala al-Ghuri, which nowadays also hosts regular performances by the Al-Tannoura Egyptian Heritage Dance Troupe. The famous Khan al-Khalili (see below) is a commercial hub which also integrated caravanserais (also known as khans).\n\nThe Citadel is a fortified enclosure begun by Salah al-Din in 1176 AD on an outcrop of the Muqattam Hills as part of a large defensive system to protect both Cairo to the north and Fustat to the southwest. It was the center of Egyptian government and residence of its rulers until 1874, when Khedive Isma'il moved to 'Abdin Palace. It is still occupied by the military today, but is now open as a tourist attraction comprising, notably, the National Military Museum, the 14th century Mosque of al-Nasir Muhammad, and the 19th century Mosque of Muhammad Ali which commands a dominant position on Cairo's skyline.\n\nKhan el-Khalili is an ancient bazaar, or marketplace adjacent to the Al-Hussein Mosque. It dates back to 1385, when Amir Jarkas el-Khalili built a large caravanserai, or khan. (A caravanserai is a hotel for traders, and usually the focal point for any surrounding area.) This original carvanserai building was demolished by Sultan al-Ghuri, who rebuilt it as a new commercial complex in the early 16th century, forming the basis for the network of souqs existing today. Many medieval elements remain today, including the ornate Mamluk-style gateways. Today, the Khan el-Khalili is a major tourist attraction and popular stop for tour groups.\n\nCairo is an expanding city, which has led to many environmental problems. The air pollution in Cairo is a matter of serious concern. Greater Cairo's volatile aromatic hydrocarbon levels are higher than many other similar cities. Air quality measurements in Cairo have also been recording dangerous levels of lead, carbon dioxide, sulphur dioxide, and suspended particulate matter concentrations due to decades of unregulated vehicle emissions, urban industrial operations, and chaff and trash burning. There are over 4,500,000 cars on the streets of Cairo, 60% of which are over 10 years old, and therefore lack modern emission cutting features like catalytic converters. Cairo has a very poor dispersion factor because of lack of rain and its layout of tall buildings and narrow streets, which create a bowl effect.\nIn recent years, a mysterious black cloud (as Egyptians refer to it) appeared over Cairo every autumn and causes serious respiratory diseases and eye irritations for the city's citizens. Tourists who are not familiar with such high levels of pollution must take extra care.\n\nCairo also has many unregistered lead and copper smelters which heavily pollute the city. The results of this has been a permanent haze over the city with particulate matter in the air reaching over three times normal levels. It is estimated that 10,000 to 25,000 people a year in Cairo die due to air pollution-related diseases. Lead has been shown to cause harm to the central nervous system and neurotoxicity particularly in children. In 1995, the first environmental acts were introduced and the situation has seen some improvement with 36 air monitoring stations and emissions tests on cars. Twenty thousand buses have also been commissioned to the city to improve congestion levels, which are very high.\n\nThe city also suffers from a high level of land pollution. Cairo produces 10,000 tons of waste material each day, 4,000 tons of which is not collected or managed. This once again is a huge health hazard and the Egyptian Government is looking for ways to combat this. The Cairo Cleaning and Beautification Agency was founded to collect and recycle the waste; however, they also work with the Zabbaleen (or Zabaleen), a community that has been collecting and recycling Cairo's waste since the turn of the 20th century and live in an area known locally as Manshiyat naser. Both are working together to pick up as much waste as possible within the city limits, though it remains a pressing problem.\n\nThe city also suffers from water pollution as the sewer system tends to fail and overflow. On occasion, sewage has escaped onto the streets to create a health hazard. This problem is hoped to be solved by a new sewer system funded by the European Union, which could cope with the demand of the city. The dangerously high levels of mercury in the city's water system has global health officials concerned over related health risks.\n\n\n\n\n"}
{"id": "6295", "url": "https://en.wikipedia.org/wiki?curid=6295", "title": "Chaos theory", "text": "Chaos theory\n\nChaos theory is a branch of mathematics focused on the behavior of dynamical systems that are highly sensitive to initial conditions. 'Chaos' is an interdisciplinary theory stating that within the apparent randomness of chaotic complex systems, there are underlying patterns, constant feedback loops, repetition, self-similarity, fractals, self-organization, and reliance on programming at the initial point known as \"sensitive dependence on initial conditions\". The butterfly effect describes how a small change in one state of a deterministic nonlinear system can result in large differences in a later state, e.g. a butterfly flapping its wings in Brazil can cause a tornado in Texas.\n\nSmall differences in initial conditions (such as those due to rounding errors in numerical computation) yield widely diverging outcomes for such dynamical systems — a response popularly referred to as the butterfly effect — rendering long-term prediction of their behavior impossible in general. This happens even though these systems are deterministic, meaning that their future behavior is fully determined by their initial conditions, with no random elements involved. In other words, the deterministic nature of these systems does not make them predictable. This behavior is known as deterministic chaos, or simply chaos. The theory was summarized by Edward Lorenz as:\n\nChaotic behavior exists in many natural systems, such as weather and climate. It also occurs spontaneously in some systems with artificial components, such as road traffic. This behavior can be studied through analysis of a chaotic mathematical model, or through analytical techniques such as recurrence plots and Poincaré maps. Chaos theory has applications in several disciplines, including meteorology, sociology, physics, environmental science, computer science, engineering, economics, biology, ecology, and philosophy.\nThe theory formed the basis for such fields of study as complex dynamical systems, edge of chaos theory, self-assembly process.\n\nChaos theory concerns deterministic systems whose behavior can in principle be predicted. Chaotic systems are predictable for a while and then 'appear' to become random. The amount of time that the behavior of a chaotic system can be effectively predicted depends on three things: How much uncertainty can be tolerated in the forecast, how accurately its current state can be measured and a time scale depending on the dynamics of the system, called the Lyapunov time. Some examples of Lyapunov times are: chaotic electrical circuits, about 1 millisecond; weather systems, a few days (unproven); the solar system, 50 million years. In chaotic systems, the uncertainty in a forecast increases exponentially with elapsed time. Hence, mathematically, doubling the forecast time more than squares the proportional uncertainty in the forecast. This means, in practice, a meaningful prediction cannot be made over an interval of more than two or three times the Lyapunov time. When meaningful predictions cannot be made, the system appears random.\n\nIn common usage, \"chaos\" means \"a state of disorder\". However, in chaos theory, the term is defined more precisely. Although no universally accepted mathematical definition of chaos exists, a commonly used definition originally formulated by Robert L. Devaney says that, to classify a dynamical system as chaotic, it must have these properties:\n\n\nIn some cases, the last two properties in the above have been shown to actually imply sensitivity to initial conditions. In these cases, while it is often the most practically significant property, \"sensitivity to initial conditions\" need not be stated in the definition.\n\nIf attention is restricted to intervals, the second property implies the other two. An alternative, and in general weaker, definition of chaos uses only the first two properties in the above list.\n\nIn continuous time dynamical systems, chaos is the phenomenon of the spontaneous breakdown of topological supersymmetry which is an intrinsic property of evolution operators of all stochastic and deterministic (partial) differential equations. This picture of dynamical chaos works not only for deterministic models but also for models with external noise, which is an important generalization from the physical point of view because in reality all dynamical systems experience influence from their stochastic environments. Within this picture, the long-range dynamical behavior associated with chaotic dynamics, e.g., the butterfly effect, is a consequence of the Goldstone's theorem in the application to the spontaneous topological supersymmetry breaking. \n\nSensitivity to initial conditions means that each point in a chaotic system is arbitrarily closely approximated by other points with significantly different future paths, or trajectories. Thus, an arbitrarily small change, or perturbation, of the current trajectory may lead to significantly different future behavior.\n\nSensitivity to initial conditions is popularly known as the \"butterfly effect\", so-called because of the title of a paper given by Edward Lorenz in 1972 to the American Association for the Advancement of Science in Washington, D.C., entitled \"Predictability: Does the Flap of a Butterfly's Wings in Brazil set off a Tornado in Texas?\". The flapping wing represents a small change in the initial condition of the system, which causes a chain of events leading to large-scale phenomena. Had the butterfly not flapped its wings, the trajectory of the system might have been vastly different.\n\nA consequence of sensitivity to initial conditions is that if we start with a limited amount of information about the system (as is usually the case in practice), then beyond a certain time the system is no longer predictable. This is most familiar in the case of weather, which is generally predictable only about a week ahead. Of course, this does not mean that we cannot say anything about events far in the future; some restrictions on the system are present. With weather, we know that the temperature will not naturally reach 100 °C or fall to -130 °C on earth (during the current geologic era), but we can't say exactly what day will have the hottest temperature of the year.\n\nIn more mathematical terms, the Lyapunov exponent measures the sensitivity to initial conditions. Given two starting trajectories in the phase space that are infinitesimally close, with initial separation formula_1, the two trajectories end up diverging at a rate given by\n\nwhere t is the time and λ is the Lyapunov exponent. The rate of separation depends on the orientation of the initial separation vector, so a whole spectrum of Lyapunov exponents exist. The number of Lyapunov exponents is equal to the number of dimensions of the phase space, though it is common to just refer to the largest one. For example, the maximal Lyapunov exponent (MLE) is most often used because it determines the overall predictability of the system. A positive MLE is usually taken as an indication that the system is chaotic.\n\nAlso, other properties relate to sensitivity of initial conditions, such as measure-theoretical mixing (as discussed in ergodic theory) and properties of a K-system.\n\nTopological mixing (or topological transitivity) means that the system evolves over time so that any given region or open set of its phase space eventually overlaps with any other given region. This mathematical concept of \"mixing\" corresponds to the standard intuition, and the mixing of colored dyes or fluids is an example of a chaotic system.\n\nTopological mixing is often omitted from popular accounts of chaos, which equate chaos with only sensitivity to initial conditions. However, sensitive dependence on initial conditions alone does not give chaos. For example, consider the simple dynamical system produced by repeatedly doubling an initial value. This system has sensitive dependence on initial conditions everywhere, since any pair of nearby points eventually becomes widely separated. However, this example has no topological mixing, and therefore has no chaos. Indeed, it has extremely simple behavior: all points except 0 tend to positive or negative infinity.\n\nFor a chaotic system to have dense periodic orbits means that every point in the space is approached arbitrarily closely by periodic orbits. The one-dimensional logistic map defined by \"x\" → 4 \"x\" (1 – \"x\") is one of the simplest systems with density of periodic orbits. For example, formula_3 → formula_4 → formula_3 (or approximately 0.3454915 → 0.9045085 → 0.3454915) is an (unstable) orbit of period 2, and similar orbits exist for periods 4, 8, 16, etc. (indeed, for all the periods specified by Sharkovskii's theorem).\n\nSharkovskii's theorem is the basis of the Li and Yorke (1975) proof that any one-dimensional system that exhibits a regular cycle of period three will also display regular cycles of every other length, as well as completely chaotic orbits.\n\nSome dynamical systems, like the one-dimensional logistic map defined by \"x\" → 4 \"x\" (1 – \"x\"), are chaotic everywhere, but in many cases chaotic behavior is found only in a subset of phase space. The cases of most interest arise when the chaotic behavior takes place on an attractor, since then a large set of initial conditions leads to orbits that converge to this chaotic region.\n\nAn easy way to visualize a chaotic attractor is to start with a point in the basin of attraction of the attractor, and then simply plot its subsequent orbit. Because of the topological transitivity condition, this is likely to produce a picture of the entire final attractor, and indeed both orbits shown in the figure on the right give a picture of the general shape of the Lorenz attractor. This attractor results from a simple three-dimensional model of the Lorenz weather system. The Lorenz attractor is perhaps one of the best-known chaotic system diagrams, probably because it was not only one of the first, but it is also one of the most complex and as such gives rise to a very interesting pattern, that with a little imagination, looks like the wings of a butterfly.\n\nUnlike fixed-point attractors and limit cycles, the attractors that arise from chaotic systems, known as strange attractors, have great detail and complexity. Strange attractors occur in both continuous dynamical systems (such as the Lorenz system) and in some discrete systems (such as the Hénon map). Other discrete dynamical systems have a repelling structure called a Julia set, which forms at the boundary between basins of attraction of fixed points. Julia sets can be thought of as strange repellers. Both strange attractors and Julia sets typically have a fractal structure, and the fractal dimension can be calculated for them.\n\nDiscrete chaotic systems, such as the logistic map, can exhibit strange attractors whatever their dimensionality. In contrast, for continuous dynamical systems, the Poincaré–Bendixson theorem shows that a strange attractor can only arise in three or more dimensions. Finite-dimensional linear systems are never chaotic; for a dynamical system to display chaotic behavior, it must be either nonlinear or infinite-dimensional.\n\nThe Poincaré–Bendixson theorem states that a two-dimensional differential equation has very regular behavior. The Lorenz attractor discussed below is generated by a system of three differential equations such as:\nwhere formula_7, formula_8, and formula_9 make up the system state, formula_10 is time, and formula_11, formula_12, formula_13 are the system parameters. Five of the terms on the right hand side are linear, while two are quadratic; a total of seven terms. Another well-known chaotic attractor is generated by the Rössler equations, which have only one nonlinear term out of seven. Sprott found a three-dimensional system with just five terms, that had only one nonlinear term, which exhibits chaos for certain parameter values. Zhang and Heidel showed that, at least for dissipative and conservative quadratic systems, three-dimensional quadratic systems with only three or four terms on the right-hand side cannot exhibit chaotic behavior. The reason is, simply put, that solutions to such systems are asymptotic to a two-dimensional surface and therefore solutions are well behaved.\n\nWhile the Poincaré–Bendixson theorem shows that a continuous dynamical system on the Euclidean plane cannot be chaotic, two-dimensional continuous systems with non-Euclidean geometry can exhibit chaotic behavior. Perhaps surprisingly, chaos may occur also in linear systems, provided they are infinite dimensional. A theory of linear chaos is being developed in a branch of mathematical analysis known as functional analysis.\n\nIn physics, jerk is the third derivative of position, with respect to time. As such, differential equations of the form\nare sometimes called \"Jerk equations\". It has been shown that a jerk equation, which is equivalent to a system of three first order, ordinary, non-linear differential equations, is in a certain sense the minimal setting for solutions showing chaotic behaviour. This motivates mathematical interest in jerk systems. Systems involving a fourth or higher derivative are called accordingly hyperjerk systems.\n\nA jerk system's behavior is described by a jerk equation, and for certain jerk equations, simple electronic circuits can model solutions. These circuits are known as jerk circuits.\n\nOne of the most interesting properties of jerk circuits is the possibility of chaotic behavior. In fact, certain well-known chaotic systems, such as the Lorenz attractor and the Rössler map, are conventionally described as a system of three first-order differential equations that can combine into a single (although rather complicated) jerk equation. Nonlinear jerk systems are in a sense minimally complex systems to show chaotic behaviour; there is no chaotic system involving only two first-order, ordinary differential equations (the system resulting in an equation of second order only).\n\nAn example of a jerk equation with nonlinearity in the magnitude of formula_7 is:\n\nHere, \"A\" is an adjustable parameter. This equation has a chaotic solution for \"A\"=3/5 and can be implemented with the following jerk circuit; the required nonlinearity is brought about by the two diodes:\n\nIn the above circuit, all resistors are of equal value, except formula_17, and all capacitors are of equal size. The dominant frequency is formula_18. The output of op amp 0 will correspond to the x variable, the output of 1 corresponds to the first derivative of x and the output of 2 corresponds to the second derivative.\n\nUnder the right conditions, chaos spontaneously evolves into a lockstep pattern. In the Kuramoto model, four conditions suffice to produce synchronization in a chaotic system.\nExamples include the coupled oscillation of Christiaan Huygens' pendulums, fireflies, neurons, the London Millennium Bridge resonance, and large arrays of Josephson junctions.\n\nAn early proponent of chaos theory was Henri Poincaré. In the 1880s, while studying the three-body problem, he found that there can be orbits that are nonperiodic, and yet not forever increasing nor approaching a fixed point. In 1898 Jacques Hadamard published an influential study of the chaotic motion of a free particle gliding frictionlessly on a surface of constant negative curvature, called \"Hadamard's billiards\". Hadamard was able to show that all trajectories are unstable, in that all particle trajectories diverge exponentially from one another, with a positive Lyapunov exponent.\n\nChaos theory began in the field of ergodic theory. Later studies, also on the topic of nonlinear differential equations, were carried out by George David Birkhoff, Andrey Nikolaevich Kolmogorov, Mary Lucy Cartwright and John Edensor Littlewood, and Stephen Smale. Except for Smale, these studies were all directly inspired by physics: the three-body problem in the case of Birkhoff, turbulence and astronomical problems in the case of Kolmogorov, and radio engineering in the case of Cartwright and Littlewood. Although chaotic planetary motion had not been observed, experimentalists had encountered turbulence in fluid motion and nonperiodic oscillation in radio circuits without the benefit of a theory to explain what they were seeing.\n\nDespite initial insights in the first half of the twentieth century, chaos theory became formalized as such only after mid-century, when it first became evident to some scientists that linear theory, the prevailing system theory at that time, simply could not explain the observed behavior of certain experiments like that of the logistic map. What had been attributed to measure imprecision and simple \"noise\" was considered by chaos theorists as a full component of the studied systems.\n\nThe main catalyst for the development of chaos theory was the electronic computer. Much of the mathematics of chaos theory involves the repeated iteration of simple mathematical formulas, which would be impractical to do by hand. Electronic computers made these repeated calculations practical, while figures and images made it possible to visualize these systems. As a graduate student in Chihiro Hayashi's laboratory at Kyoto University, Yoshisuke Ueda was experimenting with analog computers and noticed, on November 27, 1961, what he called \"randomly transitional phenomena\". Yet his advisor did not agree with his conclusions at the time, and did not allow him to report his findings until 1970.\n\nEdward Lorenz was an early pioneer of the theory. His interest in chaos came about accidentally through his work on weather prediction in 1961. Lorenz was using a simple digital computer, a Royal McBee LGP-30, to run his weather simulation. He wanted to see a sequence of data again, and to save time he started the simulation in the middle of its course. He did this by entering a printout of the data that corresponded to conditions in the middle of the original simulation. To his surprise, the weather the machine began to predict was completely different from the previous calculation. Lorenz tracked this down to the computer printout. The computer worked with 6-digit precision, but the printout rounded variables off to a 3-digit number, so a value like 0.506127 printed as 0.506. This difference is tiny, and the consensus at the time would have been that it should have no practical effect. However, Lorenz discovered that small changes in initial conditions produced large changes in long-term outcome. Lorenz's discovery, which gave its name to Lorenz attractors, showed that even detailed atmospheric modelling cannot, in general, make precise long-term weather predictions.\n\nIn 1963, Benoit Mandelbrot found recurring patterns at every scale in data on cotton prices. Beforehand he had studied information theory and concluded noise was patterned like a Cantor set: on any scale the proportion of noise-containing periods to error-free periods was a constant – thus errors were inevitable and must be planned for by incorporating redundancy. Mandelbrot described both the \"Noah effect\" (in which sudden discontinuous changes can occur) and the \"Joseph effect\" (in which persistence of a value can occur for a while, yet suddenly change afterwards). This challenged the idea that changes in price were normally distributed. In 1967, he published \"How long is the coast of Britain? Statistical self-similarity and fractional dimension\", showing that a coastline's length varies with the scale of the measuring instrument, resembles itself at all scales, and is infinite in length for an infinitesimally small measuring device. Arguing that a ball of twine appears as a point when viewed from far away (0-dimensional), a ball when viewed from fairly near (3-dimensional), or a curved strand (1-dimensional), he argued that the dimensions of an object are relative to the observer and may be fractional. An object whose irregularity is constant over different scales (\"self-similarity\") is a fractal (examples include the Menger sponge, the Sierpiński gasket, and the Koch curve or \"snowflake\", which is infinitely long yet encloses a finite space and has a fractal dimension of circa 1.2619). In 1982 Mandelbrot published \"The Fractal Geometry of Nature\", which became a classic of chaos theory. Biological systems such as the branching of the circulatory and bronchial systems proved to fit a fractal model.\n\nIn December 1977, the New York Academy of Sciences organized the first symposium on chaos, attended by David Ruelle, Robert May, James A. Yorke (coiner of the term \"chaos\" as used in mathematics), Robert Shaw, and the meteorologist Edward Lorenz. The following year, independently Pierre Coullet and Charles Tresser with the article \"Iterations d'endomorphismes et groupe de renormalisation\" and Mitchell Feigenbaum with the article \"Quantitative Universality for a Class of Nonlinear Transformations\" described logistic maps. They notably discovered the universality in chaos, permitting the application of chaos theory to many different phenomena.\n\nIn 1979, Albert J. Libchaber, during a symposium organized in Aspen by Pierre Hohenberg, presented his experimental observation of the bifurcation cascade that leads to chaos and turbulence in Rayleigh–Bénard convection systems. He was awarded the Wolf Prize in Physics in 1986 along with Mitchell J. Feigenbaum for their inspiring achievements.\n\nIn 1986, the New York Academy of Sciences co-organized with the National Institute of Mental Health and the Office of Naval Research the first important conference on chaos in biology and medicine. There, Bernardo Huberman presented a mathematical model of the eye tracking disorder among schizophrenics. This led to a renewal of physiology in the 1980s through the application of chaos theory, for example, in the study of pathological cardiac cycles.\n\nIn 1987, Per Bak, Chao Tang and Kurt Wiesenfeld published a paper in \"Physical Review Letters\" describing for the first time self-organized criticality (SOC), considered one of the mechanisms by which complexity arises in nature.\n\nAlongside largely lab-based approaches such as the Bak–Tang–Wiesenfeld sandpile, many other investigations have focused on large-scale natural or social systems that are known (or suspected) to display scale-invariant behavior. Although these approaches were not always welcomed (at least initially) by specialists in the subjects examined, SOC has nevertheless become established as a strong candidate for explaining a number of natural phenomena, including earthquakes, (which, long before SOC was discovered, were known as a source of scale-invariant behavior such as the Gutenberg–Richter law describing the statistical distribution of earthquake sizes, and the Omori law describing the frequency of aftershocks), solar flares, fluctuations in economic systems such as financial markets (references to SOC are common in econophysics), landscape formation, forest fires, landslides, epidemics, and biological evolution (where SOC has been invoked, for example, as the dynamical mechanism behind the theory of \"punctuated equilibria\" put forward by Niles Eldredge and Stephen Jay Gould). Given the implications of a scale-free distribution of event sizes, some researchers have suggested that another phenomenon that should be considered an example of SOC is the occurrence of wars. These investigations of SOC have included both attempts at modelling (either developing new models or adapting existing ones to the specifics of a given natural system), and extensive data analysis to determine the existence and/or characteristics of natural scaling laws.\n\nIn the same year, James Gleick published \"\", which became a best-seller and introduced the general principles of chaos theory as well as its history to the broad public, though his history under-emphasized important Soviet contributions. Initially the domain of a few, isolated individuals, chaos theory progressively emerged as a transdisciplinary and institutional discipline, mainly under the name of nonlinear systems analysis. Alluding to Thomas Kuhn's concept of a paradigm shift exposed in \"The Structure of Scientific Revolutions\" (1962), many \"chaologists\" (as some described themselves) claimed that this new theory was an example of such a shift, a thesis upheld by Gleick.\n\nThe availability of cheaper, more powerful computers broadens the applicability of chaos theory. Currently, chaos theory remains an active area of research, involving many different disciplines (mathematics, topology, physics, social systems, population modeling, biology, meteorology, astrophysics, information theory, computational neuroscience, etc.).\n\nChaos theory was born from observing weather patterns, but it has become applicable to a variety of other situations. Some areas benefiting from chaos theory today are geology, mathematics, microbiology, biology, computer science, economics, engineering, finance, algorithmic trading, meteorology, philosophy, physics, politics, population dynamics, psychology, and robotics. A few categories are listed below with examples, but this is by no means a comprehensive list as new applications are appearing.\n\nChaos theory has been used for many years in cryptography. In the past few decades, chaos and nonlinear dynamics have been used in the design of hundreds of cryptographic primitives. These algorithms include image encryption algorithms, hash functions, secure pseudo-random number generators, stream ciphers, watermarking and steganography. The majority of these algorithms are based on uni-modal chaotic maps and a big portion of these algorithms use the control parameters and the initial condition of the chaotic maps as their keys. From a wider perspective, without loss of generality, the similarities between the chaotic maps and the cryptographic systems is the main motivation for the design of chaos based cryptographic algorithms. One type of encryption, secret key or symmetric key, relies on diffusion and confusion, which is modeled well by chaos theory. Another type of computing, DNA computing, when paired with chaos theory, offers a way to encrypt images and other information. Many of the DNA-Chaos cryptographic algorithms are proven to be either not secure, or the technique applied is suggested to be not efficient.\n\nRobotics is another area that has recently benefited from chaos theory. Instead of robots acting in a trial-and-error type of refinement to interact with their environment, chaos theory has been used to build a predictive model.\nChaotic dynamics have been exhibited by passive walking biped robots. \n\nFor over a hundred years, biologists have been keeping track of populations of different species with population models. Most models are continuous, but recently scientists have been able to implement chaotic models in certain populations. For example, a study on models of Canadian lynx showed there was chaotic behavior in the population growth. Chaos can also be found in ecological systems, such as hydrology. While a chaotic model for hydrology has its shortcomings, there is still much to learn from looking at the data through the lens of chaos theory. Another biological application is found in cardiotocography. Fetal surveillance is a delicate balance of obtaining accurate information while being as noninvasive as possible. Better models of warning signs of fetal hypoxia can be obtained through chaotic modeling.\n\nIn chemistry, predicting gas solubility is essential to manufacturing polymers, but models using particle swarm optimization (PSO) tend to converge to the wrong points. An improved version of PSO has been created by introducing chaos, which keeps the simulations from getting stuck. In celestial mechanics, especially when observing asteroids, applying chaos theory leads to better predictions about when these objects will approach Earth and other planets. Four of the five moons of Pluto rotate chaotically. In quantum physics and electrical engineering, the study of large arrays of Josephson junctions benefitted greatly from chaos theory. Closer to home, coal mines have always been dangerous places where frequent natural gas leaks cause many deaths. Until recently, there was no reliable way to predict when they would occur. But these gas leaks have chaotic tendencies that, when properly modeled, can be predicted fairly accurately.\n\nChaos theory can be applied outside of the natural sciences. By adapting a model of career counseling to include a chaotic interpretation of the relationship between employees and the job market, better suggestions can be made to people struggling with career decisions. Modern organizations are increasingly seen as open complex adaptive systems with fundamental natural nonlinear structures, subject to internal and external forces that may contribute chaos. The chaos metaphor—used in verbal theories—grounded on mathematical models and psychological aspects of human behavior\nprovides helpful insights to describing the complexity of small work groups, that go beyond the metaphor itself.\n\nIt is possible that economic models can also be improved through an application of chaos theory, but predicting the health of an economic system and what factors influence it most is an extremely complex task. Economic and financial systems are fundamentally different from those in the classical natural sciences since the former are inherently stochastic in nature, as they result from the interactions of people, and thus pure deterministic models are unlikely to provide accurate representations of the data. The empirical literature that tests for chaos in economics and finance presents very mixed results, in part due to confusion between specific tests for chaos and more general tests for non-linear relationships.\n\nTraffic forecasting also benefits from applications of chaos theory. Better predictions of when traffic will occur lets measures be taken to disperse it before it would have occurred. Combining chaos theory principles with a few other methods has led to a more accurate short-term prediction model (see the plot of the BML traffic model at right).\n\nChaos theory can be applied in psychology. For example, in modeling group behavior in which heterogeneous members may behave as if sharing to different degrees what in Wilfred Bion's theory is a basic assumption, the group dynamics is the result of the individual dynamics of the members: each individual reproduces the group dynamics in a different scale, and the chaotic behavior of the group is reflected in each member.\n\nChaos theory has been applied to environmental water cycle data (aka hydrological data), such as rainfall and streamflow. These studies have yielded controversial results, because the methods for detecting a chaotic signature are often relatively subjective. Early studies tended to \"succeed\" in finding chaos, whereas subsequent studies and meta-analyses called those studies into question and provided explanations for why these datasets are not likely to have low-dimension chaotic dynamics. \n\n\n\n\n\n\n\n\n\n"}
{"id": "6298", "url": "https://en.wikipedia.org/wiki?curid=6298", "title": "Cupola", "text": "Cupola\n\nIn architecture, a cupola is a small, most often dome-like, structure on top of a building. Often used to provide a lookout or to admit light and air, it usually crowns a larger roof or dome.\n\nThe word derives, via Italian, from the lower Latin \"cupula\" (classical Latin \"cupella\" from the Greek κύπελλον \"kupellon\") \"small cup\" (Latin \"cupa\") indicating a vault resembling an upside down cup.\nThe cupola is a development during the Renaissance of the oculus, an ancient device found in Roman architecture, but being weatherproof was superior for the wetter climates of northern Europe. The chhatri, seen in Indian architecture, fits the definition of a cupola when it is used atop a larger structure.\n\nCupolas often appear as small buildings in their own right. They often serve as a belfry, belvedere, or roof lantern above a main roof. In other cases they may crown a spire, tower, or turret.\n\nThe square, dome-like segment of a North American railroad train caboose that contains the second-level or \"angel\" seats is also called a cupola.\n\nSome armored fighting vehicles have cupolas, called commander's cupola, which is a raised dome or cylinder with armored glass to provide 360-degree vision around the vehicle.\n\n"}
{"id": "6299", "url": "https://en.wikipedia.org/wiki?curid=6299", "title": "Chupacabra", "text": "Chupacabra\n\nThe chupacabra or chupacabras (, literally \"goat-sucker\"; from \"chupar\", \"to suck\", and \"cabra\", \"goat\") is a legendary creature in the folklore of parts of the Americas, with its first purported sightings reported in Puerto Rico. The name comes from the animal's reported habit of attacking and drinking the blood of livestock, especially goats.\n\nPhysical descriptions of the creature vary. It is purportedly a heavy creature, the size of a small bear, with a row of spines reaching from the neck to the base of the tail.\n\nEyewitness sightings have been claimed as early as 1995 in Puerto Rico, and have since been reported as far north as Maine, and as far south as Chile, and even being spotted outside the Americas in countries like Russia and the Philippines, but many of the reports have been disregarded as uncorroborated or lacking evidence. Sightings in northern Mexico and the southern United States have been verified as canids afflicted by mange. According to biologists and wildlife management officials, the chupacabra is an urban legend.\n\n\"Chupacabras\" can be literally translated as \"goat-sucker\", from \"chupar\" (\"to suck\") and \"cabra\" (\"goat\"). It is known as both \"chupacabras\" and \"chupacabra\" throughout the Americas, with the former being the original word, and the latter a regularization of it. The name in Spanish can be preceded by a singular masculine article (\"el chupacabras\"), or the plural masculine article (\"los chupacabras\").\n\nThe first reported attack occurred in March 1995 in Puerto Rico. Eight sheep were discovered dead, each with three puncture wounds in the chest area and completely drained of blood. A few months later, in August, an eyewitness, Madelyne Tolentino, reported seeing the creature in the Puerto Rican town of Canóvanas, when as many as 150 farm animals and pets were reportedly killed. In 1975, similar killings in the small town of Moca were attributed to \"El Vampiro de Moca\" (\"The Vampire of Moca\"). Initially, it was suspected that the killings were committed by a Satanic cult; later more killings were reported around the island, and many farms reported loss of animal life. Each of the animals was reported to have had its body bled dry through a series of small circular incisions.\n\nPuerto Rican comedian and entrepreneur Silverio Pérez is credited with coining the term \"chupacabras\" soon after the first incidents were reported in the press. Shortly after the first reported incidents in Puerto Rico, other animal deaths were reported in other countries, such as the Dominican Republic, Argentina, Bolivia, Chile, Colombia, Honduras, El Salvador, Nicaragua, Panama, Peru, Brazil, United States, and Mexico.\n\nA five-year investigation by Benjamin Radford, documented in his 2011 book \"Tracking the Chupacabra\", concluded that the description given by the original eyewitness in Puerto Rico, Madelyne Tolentino, was based on the creature Sil in the science-fiction horror film \"Species\". The alien creature Sil is nearly identical to Tolentino’s chupacabra eyewitness account and she had seen the movie before her report: \"It was a creature that looked like the chupacabra, with spines on its back and all... The resemblance to the chupacabra was really impressive,\" Tolentino reported. Radford revealed that Tolentino \"believed that the creatures and events she saw in \"Species\" were happening in reality in Puerto Rico at the time,\" and therefore concludes that \"the most important chupacabra description cannot be trusted.\" This, Radford believes, seriously undermines the credibility of the chupacabra as a real animal.\n\nIn addition, the reports of blood-sucking by the chupacabra were never confirmed by a necropsy, the only way to conclude that the animal was drained of blood. An analysis by a veterinarian of 300 reported victims of the chupacabra found that they had not been bled dry.\n\nRadford divided the chupacabra reports into two categories: the reports from Puerto Rico and Latin America where animals were attacked and it is supposed their blood was extracted, and the reports in the United States of mammals, mostly dogs and coyotes with mange, that people call \"chupacabra\" due to their unusual appearance.\n\nIn late October 2010, University of Michigan biologist Barry O'Connor concluded that all the chupacabra reports in the United States were simply coyotes infected with the parasite \"Sarcoptes scabiei\", whose symptoms would explain most of the features of the chupacabra: they would be left with little fur, thickened skin, and rank odor. O'Connor theorized that the attacks on goats occurred \"because these animals are greatly weakened, they're going to have a hard time hunting. So they may be forced into attacking livestock because it's easier than running down a rabbit or a deer.\"\n\nAlthough several witnesses came to the conclusion that the attacks could not be the work of dogs or coyotes because they had not eaten the victim, this conclusion is incorrect. Both dogs and coyotes can kill and not consume the prey, either because they are inexperienced, or due to injury or difficulty in killing the prey. The prey can survive the attack and die afterwards from internal bleeding or circulatory shock. The presence of two holes in the neck, corresponding with the canine teeth, are to be expected since this is the only way that most land carnivores have to catch their prey.\n\nThere are reports of stray Mexican Hairless Dogs being mistaken for chupacabras.\n\nThe most common description of the chupacabra is that of a reptile-like creature, said to have leathery or scaly greenish-gray skin and sharp spines or quills running down its back. It is said to be approximately high, and stands and hops in a fashion similar to that of a kangaroo.\n\nAnother common description of the chupacabra is of a strange breed of wild dog. This form is mostly hairless and has a pronounced spinal ridge, unusually pronounced eye sockets, fangs, and claws. Unlike conventional predators, the chupacabra is said to drain all of the animal's blood (and sometimes organs) usually through three holes in the shape of a downwards-pointing triangle or through one or two holes.\n\nA popular legend in New Orleans concerns a popular lovers' lane called Grunch Road, which was said to be inhabited by \"grunches\", creatures similar in appearance to the \"Chupacabra\".\n\nThe Peuchens of Chile also share similarities in their supposed habits, but instead of being dog-like they are described as winged snakes. This legend may have originated from the vampire bat, an animal endemic to the region.\n\nIn the Philippines, another legendary creature called the Sigbin shares many of chupacabra's descriptions. The recent discovery of the cat-fox in Southeast Asia suggests that it could also have been simply sightings of this once unknown animal.\n\nThe popularity of the chupacabra has resulted in its being featured in many types of media.\n\n"}
{"id": "6309", "url": "https://en.wikipedia.org/wiki?curid=6309", "title": "Cayuga Lake", "text": "Cayuga Lake\n\nCayuga Lake ( or )  is the longest of central New York's glacial Finger Lakes, and is the second largest in surface area (marginally smaller than Seneca Lake) and second largest in volume. It is just under long. Its average width is 1.7 miles (2.7 km), and it is at its widest point near Aurora. It is approximately at its deepest point.\n\nThe city of Ithaca, site of Ithaca College and Cornell University, is located at the southern end of Cayuga Lake.\n\nVillages and settlements along the east shore of Cayuga Lake include Myers, King Ferry, Aurora, Levanna, Union Springs, and Cayuga. Settlements along the west shore of the lake include Sheldrake, Poplar Beach, and Canoga.\n\nThe lake has one small island near Union Springs, Frontenac Island. It is one of only two islands in the Finger Lakes, the other being Squaw Island in Canandaigua Lake.\n\nCayuga Lake is located at ; above sea level. Its depth, steep east and west sides with shallow north and south ends is typical of the Finger Lakes, as they were carved by glaciers during the last ice age.\n\nThe water level is regulated by the Mud Lock at the north end of the lake. It is connected to Lake Ontario by the Erie Canal and Seneca Lake by the Seneca River. The lake is drawn down as winter approaches, to minimize ice damage and to maximize its capacity to store heavy spring runoff.\n\nThe north end is dominated by shallow mudflats. An important stopover for migratory birds, the mudflats and marsh are the location of the Montezuma National Wildlife Refuge. The southern end is also shallow and often freezes during the winter.\n\nCayuga Lake is very popular among recreational boaters. The Allan H. Treman State Marine Park, a large state marina and boat launch, is located at the southern end of the lake in Ithaca. There are two yacht clubs on the western shore: Ithaca Yacht Club a few miles north of Ithaca, and Red Jacket Yacht Club just south of Canoga. There are several other marinas and boat launches scattered along the lake shore.\n\nCayuga Lake is the source of drinking water for several communities, including Lansing near the southern end of the lake along the east side, which draws water through the Bolton Point Municipal Water system. There are also several lake source cooling systems that are in operation on the lake, whereby cooler water is pumped from the depths of the lake, warmed, and circulated in a closed system back to the surface. One of these systems, which is operated by Cornell University and began operation in 2000, was controversial during the planning and building states for potential negative environmental impact. All the environmental impact reports and scientific studies have shown that the Cornell lake source cooling system has not yet had and will not likely have any measurably significant environmental impact. Furthermore, Cornell's system pumps significantly less warm water back into the lake than others further north which have been operating for decades, including the coal-fired power plant on the eastern shore.\nThe AES Cayuga electrical generating station operates in the Town of Lansing, on the east shore of Cayuga Lake. This coal-fired plant uses Cayuga Lake as a cooling source. In the late 1960s, citizens successfully opposed the construction of an 830-MW nuclear power plant on the shore of Cayuga Lake.\n\nRod Serling named his production company Cayuga Productions during the years of his TV series, \"The Twilight Zone\". Serling and his family had a summer home at Cayuga Lake.\n\nThe fish population is managed and substantial sport fishing is practiced, with anglers targeting smelt, lake trout and smallmouth bass. Fish species present in the lake include lake trout, landlocked salmon, brown trout, rainbow trout, smallmouth bass, smelt, alewife, atlantic salmon, black crappie, bluegill, pickerel, largemouth bass, northern pike, pumpkinseed sunfish, rock bass, and yellow perch. There are state owned hard surface ramps in Mudlock Canal Park, Long Point State Park, Cayuga Lake State Park, Dean's Cove State Marine Park, Taughannock Falls State Park, and Allen H. Treman Marine Park.\n\n\nThe lake is the subject of local folklore. Cornell's alma mater makes reference to its position \"Far Above Cayuga's Waters\", while that of Ithaca College references \"Cayuga's shore\".\n\nA tradition at Wells College in Aurora holds that if the lake completely freezes over, classes are canceled (though for only one day). According to Wells College records, this most recently happened in 1979 and 2015. However, other sources suggest that the only time the entire lake froze over solid end to end in the 20th century was in 1912.\n\nCayuga Lake, like nearby Seneca Lake, is also the site of a phenomenon known as the Guns of the Seneca, mysterious cannon-like booms heard in the surrounding area. Many of these booms may be attributable to bird-scarers, automated cannon-like devices used by farmers to scare birds away from the many vineyards, orchards and crops. There is however no proof of this.\n\nCayuga Lake is included in the American Viticultural Area with which it shares its name. Established in 1988, the AVA now boasts over a dozen wineries, four distilleries, a cidery, and a meadery.\n\n\n"}
{"id": "6310", "url": "https://en.wikipedia.org/wiki?curid=6310", "title": "Columbia University", "text": "Columbia University\n\nColumbia University (Columbia; officially Columbia University in the City of New York), established in 1754, is a private Ivy League research university in Upper Manhattan, New York City. It is often cited as one of the world's most prestigious universities.\n\nColumbia contains the oldest college in the state of New York and is the fifth chartered institution of higher learning in the United States, making it one of nine colonial colleges founded prior to the Declaration of Independence. It was established as King's College by royal charter of George II of Great Britain and renamed Columbia College in 1784 following the American Revolutionary War. The college has produced numerous distinguished alumni. It has additionally been amongst the most selective colleges in the United States since its founding. With an undergraduate acceptance rate of 5.8%, it currently stands as the third most selective college in the United States and second most selective college in the Ivy League.\n\nA 1787 charter placed the institution under a private board of trustees before it was renamed Columbia University in 1896 when the campus was moved from Madison Avenue to its current location in Morningside Heights occupying of land. Columbia is one of the fourteen founding members of the Association of American Universities and was the first school in the United States to grant the M.D. degree. The university administers annually the Pulitzer Prize.\n\nThe university is organized into twenty schools, including three undergraduate schools, Columbia College, the School of Engineering and Applied Science, and the School of General Studies, as well as graduate schools such as Columbia Law School, Columbia College of Physicians and Surgeons, and Columbia Business School. The university also has global research outposts in Amman, Beijing, Istanbul, Paris, Mumbai, Rio de Janeiro, Santiago, Asunción and Nairobi. It has affiliations with several other institutions nearby, including List College, Teachers College, Barnard College, and Union Theological Seminary, with joint undergraduate programs available through the Jewish Theological Seminary of America, University College London, Sciences Po, City University of Hong Kong, and the Juilliard School.\n\nThe university has graduated many notable alumni. Alumni and affiliates include 5 Founding Fathers of the United States – amongst these an author of the Declaration of Independence and an author of the United States Constitution; 10 Justices of the United States Supreme Court; 20 living billionaires; 123 Pulitzer Prize winners; 39 Academy Award winners; 3 United States Presidents; 29 heads of state; 101 National Academy members; 98 Nobel laureates; 77 National Medal of Science winners; 23 National Humanities Medal recipients. Columbia is second only to Harvard University in the number of Nobel Prize-winning affiliates.\n\nDiscussions regarding the founding of a college in the Province of New York began as early as 1704, at which time Colonel Lewis Morris wrote to the Society for the Propagation of the Gospel in Foreign Parts, the missionary arm of the Church of England, persuading the society that New York City was an ideal community in which to establish a college; however, not until the founding of the College of New Jersey (the present Princeton University) across the Hudson River in New Jersey did the City of New York seriously consider founding a college. In 1746 an act was passed by the general assembly of New York to raise funds for the foundation of a new college. In 1751, the assembly appointed a commission of ten New York residents, seven of whom were members of the Church of England, to direct the funds accrued by the state lottery towards the foundation of a college.\n\nClasses were initially held in July 1754 and were presided over by the college's first president, Dr. Samuel Johnson. Dr. Johnson was the only instructor of the college's first class, which consisted of a mere eight students. Instruction was held in a new schoolhouse adjoining Trinity Church, located on what is now lower Broadway in Manhattan. The college was officially founded on October 31, 1754, as King's College by royal charter of King George II, making it the oldest institution of higher learning in the state of New York and the fifth oldest in the United States.\n\nIn 1763, Dr. Johnson was succeeded in the presidency by Myles Cooper, a graduate of The Queen's College, Oxford, and an ardent Tory. In the charged political climate of the American Revolution, his chief opponent in discussions at the college was an undergraduate of the class of 1777, Alexander Hamilton. The American Revolutionary War broke out in 1776, and was catastrophic for the operation of King's College, which suspended instruction for eight years beginning in 1776 with the arrival of the Continental Army. The suspension continued through the military occupation of New York City by British troops until their departure in 1783. The college's library was looted and its sole building requisitioned for use as a military hospital first by American and then British forces. Loyalists were forced to abandon their King's College in New York, which was seized by the rebels and renamed Columbia College. The Loyalists, led by Bishop Charles Inglis fled to Windsor, Nova Scotia, where they founded King's Collegiate School.\n\nAfter the Revolution, the college turned to the State of New York in order to restore its vitality, promising to make whatever changes to the school's charter the state might demand. The Legislature agreed to assist the college, and on May 1, 1784, it passed \"an Act for granting certain privileges to the College heretofore called King's College.\" The Act created a Board of Regents to oversee the resuscitation of King's College, and, in an effort to demonstrate its support for the new Republic, the Legislature stipulated that \"the College within the City of New York heretofore called King's College be forever hereafter called and known by the name of Columbia College,\" a reference to Columbia, an alternative name for America. The Regents finally became aware of the college's defective constitution in February 1787 and appointed a revision committee, which was headed by John Jay and Alexander Hamilton. In April of that same year, a new charter was adopted for the college, still in use today, granting power to a private board of 24 Trustees.\n\nOn May 21, 1787, William Samuel Johnson, the son of Dr. Samuel Johnson, was unanimously elected President of Columbia College. Prior to serving at the university, Johnson had participated in the First Continental Congress and been chosen as a delegate to the Constitutional Convention. For a period in the 1790s, with New York City as the federal and state capital and the country under successive Federalist governments, a revived Columbia thrived under the auspices of Federalists such as Hamilton and Jay. Both President George Washington and Vice President John Adams attended the college's commencement on May 6, 1789, as a tribute of honor to the many alumni of the school who had been involved in the American Revolution.\n\nIn November 1813, the College agreed to incorporate its medical school with The College of Physicians and Surgeons, a new school created by the Regents of New York, forming Columbia University College of Physicians and Surgeons. The college's enrollment, structure, and academics stagnated for the majority of the 19th century, with many of the college presidents doing little to change the way that the college functioned. In 1857, the college moved from the King's College campus at Park Place to a primarily Gothic Revival campus on 49th Street and Madison Avenue, where it remained for the next forty years. During the last half of the 19th century, under the leadership of President F.A.P. Barnard, the institution rapidly assumed the shape of a modern university. By this time, the college's investments in New York real estate became a primary source of steady income for the school, mainly owing to the city's expanding population. University president Seth Low moved the campus from 49th Street to its present location, a more spacious campus in the developing neighborhood of Morningside Heights. Under the leadership of Low's successor, Nicholas Murray Butler, who served for over four decades, Columbia rapidly became the nation's major institution for research, setting the \"multiversity\" model that later universities would adopt. Prior to becoming the president of Columbia University, Butler founded Teachers College, as a school to prepare home economists and manual art teachers for the children of the poor, with philanthropist Grace Hoadley Dodge. Teachers College came under the aegis of Columbia University in 1893 and became the University's Graduate School of Education.\n\nResearch into the atom by faculty members John R. Dunning, I. I. Rabi, Enrico Fermi and Polykarp Kusch placed Columbia's Physics Department in the international spotlight in the 1940s after the first nuclear pile was built to start what became the Manhattan Project. In 1928, Seth Low Junior College was established by Columbia University in order to mitigate the number of Jewish applicants to Columbia College. The college was closed in 1938 due to the adverse effects of the Great Depression and its students were subsequently absorbed into University Extension. In 1947, the program was reorganized as an undergraduate college and designated the School of General Studies in response to the return of GIs after World War II. In 1995, the School of General Studies was again reorganized as a full-fledged liberal arts college for non-traditional students (those who have had an academic break of one year or more, or are pursuing dual-degrees) and was fully integrated into Columbia's traditional undergraduate curriculum. Within the same year, the Division of Special Programs—later the School of Continuing Education, and now the School of Professional Studies—was established to reprise the former role of University Extension. While the School of Professional Studies only offered non-degree programs for lifelong learners and high school students in its earliest stages, it now offers degree programs in a diverse range of professional and inter-disciplinary fields.\n\nIn the aftermath of World War II, the discipline of international relations became a major scholarly focus of the University, and in response, the School of International and Public Affairs was founded in 1946, drawing upon the resources of the faculties of political science, economics, and history.\n\nDuring the 1960s Columbia experienced large-scale student activism, which reached a climax in the spring of 1968 when hundreds of students occupied buildings on campus. The incident forced the resignation of Columbia's President, Grayson Kirk and the establishment of the University Senate.\n\nThough several schools within the university had admitted women for years, Columbia College first admitted women in the fall of 1983, after a decade of failed negotiations with Barnard College, the all-female institution affiliated with the university, to merge the two schools. Barnard College still remains affiliated with Columbia, and all Barnard graduates are issued diplomas authorized by both Columbia University and Barnard College.\n\nIn the 1970s, Teachers College's departments ran joint Ph.D programs with the University in the disciplines of psychology, history, and anthropology and all Teachers College diplomas were conferred by Columbia University. Teachers College was soon established as the University's Faculty of Education and Department of Education.\n\nDuring the late 20th century, the University underwent significant academic, structural, and administrative changes as it developed into a major research university. For much of the 19th century, the University consisted of decentralized and separate faculties specializing in Political Science, Philosophy, and Pure Science. In 1979, these faculties were merged into the Graduate School of Arts and Sciences. In 1991, the faculties of Columbia College, the School of General Studies, the Graduate School of Arts and Sciences, the School of the Arts, and the School of Professional Studies were merged into the Faculty of Arts and Sciences, leading to the academic integration and centralized governance of these schools. In 2010, the School of International and Public Affairs, which was previously a part of the Faculty of Arts and Sciences, became an independent faculty.\n\nAccording to \"New York\", Columbia University is the second largest landowner in New York City, after the Catholic Church.\n\nThe majority of Columbia's graduate and undergraduate studies are conducted in Morningside Heights on Seth Low's late-19th century vision of a university campus where all disciplines could be taught at one location. The campus was designed along Beaux-Arts principles by architects McKim, Mead, and White. Columbia's main campus occupies more than six city blocks, or , in Morningside Heights, New York City, a neighborhood that contains a number of academic institutions. The university owns over 7,800 apartments in Morningside Heights, housing faculty, graduate students, and staff. Almost two dozen undergraduate dormitories (purpose-built or converted) are located on campus or in Morningside Heights. Columbia University has an extensive underground tunnel system more than a century old, with the oldest portions predating the present campus. Some of these remain accessible to the public, while others have been cordoned off.\n\nThe Nicholas Murray Butler Library, commonly known simply as Butler Library, is the largest single library in the Columbia University Library System, and is one of the largest buildings on the campus. Proposed as \"South Hall\" by the university's former President Nicholas Murray Butler as expansion plans for Low Memorial Library stalled, the new library was funded by Edward Harkness, benefactor of Yale's residential college system, and designed by his favorite architect, James Gamble Rogers. It was completed in 1934 and renamed for Butler in 1946. The library design is neo-classical in style. Its facade features an arcade of columns in the Ionic order above which are inscribed the names of great writers, philosophers, and thinkers, most of whom are read by students engaged in the Core Curriculum of Columbia College. As of 2012, Columbia's library system includes over 11.9  million volumes, making it the eighth largest library system and fifth largest collegiate library system in the United States. It has also been ranked among the United States' most beautiful libraries.\n\nSeveral buildings on the Morningside Heights campus are listed on the National Register of Historic Places. Low Memorial Library, a National Historic Landmark and the centerpiece of the campus, is listed for its architectural significance. Philosophy Hall is listed as the site of the invention of FM radio. Also listed is Pupin Hall, another National Historic Landmark, which houses the physics and astronomy departments. Here the first experiments on the fission of uranium were conducted by Enrico Fermi. The uranium atom was split there ten days after the world's first atom-splitting in Copenhagen, Denmark.\n\nA statue by sculptor Daniel Chester French called \"Alma Mater\" is centered on the front steps of Low Memorial Library. McKim, Mead & White invited French to build the sculpture in order to harmonize with the larger composition of the court and library in the center of the campus. Draped in an academic gown, the female figure of Alma Mater wears a crown of laurels and sits on a throne. The scroll-like arms of the throne end in lamps, representing sapientia and doctrina. A book signifying knowledge, balances on her lap, and an owl, the attribute of wisdom, is hidden in the folds of her gown. Her right hand holds a scepter composed of four sprays of wheat, terminating with a crown of King's College which refers to Columbia's origin as a Royalist institution in 1754. A local actress named Mary Lawton was said to have posed for parts of the sculpture. The statue was dedicated on September 23, 1903, as a gift of Mr. & Mrs. Robert Goelet, and was originally covered in golden leaf. During the Columbia University protests of 1968 a bomb damaged the sculpture, but it has since been repaired. The small hidden owl on the sculpture is also the subject of many Columbia legends, the main legend being that the first student in the freshmen class to find the hidden owl on the statue will be valedictorian, and that any subsequent Columbia male who finds it will marry a Barnard student, given that Barnard is a women's college.\n\n\"The Steps\", alternatively known as \"Low Steps\" or the \"Urban Beach\", are a popular meeting area for Columbia students. The term refers to the long series of granite steps leading from the lower part of campus (South Field) to its upper terrace. With a design inspired by the City Beautiful movement, the steps of Low Library provides Columbia University and Barnard College students, faculty, and staff with a comfortable outdoor platform and space for informal gatherings, events, and ceremonies. McKim's classical facade epitomizes late 19th century new-classical designs, with its columns and portico marking the entrance to an important structure. On warm days when the weather is favorable, the Low Steps often become a popular gathering place for students to sunbathe, eat lunch, or play frisbee.\n\nIn April 2007, the university purchased more than two-thirds of a site for a new campus in Manhattanville, an industrial neighborhood to the north of the Morningside Heights campus. Stretching from 125th Street to 133rd Street, the new campus will house buildings for Columbia's Business School, School of International and Public Affairs, and the Jerome L. Greene Center for Mind, Brain, and Behavior, where research will occur on neurodegenerative diseases such as Parkinson's and Alzheimer's. The $7 billion expansion plan includes demolishing all buildings, except three that are historically significant, eliminating the existing light industry and storage warehouses, and relocating tenants in 132 apartments. Replacing these buildings will be of space for the university. Community activist groups in West Harlem fought the expansion for reasons ranging from property protection and fair exchange for land, to residents' rights. Subsequent public hearings drew neighborhood opposition. Most recently, as of December 2008, the State of New York's Empire State Development Corporation approved use of eminent domain, which, through declaration of Manhattanville's \"blighted\" status, gives governmental bodies the right to appropriate private property for public use. On May 20, 2009, the New York State Public Authorities Control Board approved the Manhanttanville expansion plan and the first buildings are under construction.\n\nNew York-Presbyterian Hospital is affiliated with the medical schools of both Columbia University and Cornell University. According to \"U.S. News & World Report\"s \"America's Best Hospitals 2009\", it is ranked sixth overall and third among university hospitals. Columbia's medical school has a strategic partnership with New York State Psychiatric Institute, and is affiliated with 19 other hospitals in the U.S. and four hospitals overseas. Health-related schools are located at the Columbia University Medical Center, a campus located in the neighborhood of Washington Heights, fifty blocks uptown. Other teaching hospitals affiliated with Columbia through the New York-Presbyterian network include the Payne Whitney Clinic in Manhattan, and the Payne Whitney Westchester, a psychiatric institute located in White Plains, New York. On the northern tip of Manhattan island (in the neighborhood of Inwood), Columbia owns Baker Field, which includes the Lawrence A. Wien Stadium as well as facilities for field sports, outdoor track, and tennis. There is a third campus on the west bank of the Hudson River, the Lamont-Doherty Earth Observatory and Earth Institute in Palisades, New York. A fourth is the Nevis Laboratories in Irvington, New York for the study of particle and motion physics. A satellite site in Paris, France holds classes at Reid Hall.\n\nIn 2006, the university established the Office of Environmental Stewardship to initiate, coordinate and implement programs to reduce the university's environmental footprint. The U.S. Green Building Council selected the university's Manhattanville plan for the Leadership in Energy and Environmental Design (LEED) Neighborhood Design pilot program. The plan commits to incorporating smart growth, new urbanism and \"green\" building design principles. Columbia is one of the 2030 Challenge Partners, a group of nine universities in the city of New York that have pledged to reduce their greenhouse emissions by 30% within the next ten years. Columbia University adopts LEED standards for all new construction and major renovations. The University requires a minimum of Silver, but through its design and review process seeks to achieve higher levels. This is especially challenging for lab and research buildings with their intensive energy use; however, the university also uses lab design guidelines that seek to maximize energy efficiency while protecting the safety of researchers.\n\nEvery Thursday and Sunday of the month, Columbia hosts a greenmarket where local farmers can sell their produce to residents of the city. In addition, from April to November Hodgson's farm, a local New York gardening center, joins the market bringing a large selection of plants and blooming flowers. The market is one of the many operated at different points throughout the city by the non-profit group GrowNYC. Dining services at Columbia spends 36 percent of its food budget on local products, in addition to serving sustainably harvested seafood and fair trade coffee on campus. Columbia has been rated \"B+\" by the 2011 College Sustainability Report Card for its environmental and sustainability initiatives.\n\nColumbia University received 36,292 applications for its undergraduate class of 2020 (entering 2016). In early decision, 620 out of 3,520 applicants were admitted, for an acceptance rate of 17.61%. In regular decision, 1,573 out of 32,772 applicants were admitted, for an acceptance rate of 4.79%. For the Class of 2021, 2,185 out of 37,389 applicants were admitted for an overall acceptance rate of 5.8%, making Columbia the third most selective college in the United States behind Stanford and Harvard and the 2nd most selective college in the Ivy League. According to the 2012 college selectivity ranking by U.S. News & World Report, which factors admission and yield rates among other criteria, Columbia was tied with Yale, Caltech and MIT as the most selective colleges in the country. Columbia is a racially diverse school, with approximately 52% of all students identifying themselves as persons of color. Additionally, 50% of all undergraduates received grants from Columbia. The average grant size awarded to these students is $46,516. In 2015–2016, annual undergraduate tuition at Columbia was $50,526 with a total cost of attendance of $65,860 (including room and board).\n\nOn April 11, 2007, Columbia University announced a $400m to $600m donation from media billionaire alumnus John Kluge to be used exclusively for undergraduate financial aid. The donation is among the largest single gifts to higher education. Its exact value will depend on the eventual value of Kluge's estate at the time of his death; however, the generous donation has helped change financial aid policy at Columbia. Annual gifts, fund-raising, and an increase in spending from the university's endowment have allowed Columbia to extend generous financial aid packages to qualifying students. As of 2008, undergraduates from families with incomes as high as $60,000 a year will have the projected cost of attending the university, including room, board, and academic fees, fully paid for by the university. That same year, the university ended loans for incoming and current students who were on financial aid, replacing loans that were traditionally part of aid packages with grants from the university. However, this does not apply to international students, transfer students, visiting students, or students in the School of General Studies. In the fall of 2010, admission to Columbia's undergraduate colleges Columbia College and the Fu Foundation School of Engineering and Applied Science (also known as SEAS or Columbia Engineering) began accepting the Common Application. The policy change made Columbia one of the last major academic institutions and the last Ivy League university to switch to the Common Application.\n\nScholarships are also given to undergraduate students by the admissions committee. Designations include John W. Kluge Scholars, John Jay Scholars, C. Prescott Davis Scholars, Global Scholars, Egleston Scholars, and Science Research Fellows. Named scholars are selected by the admission committee from first-year applicants. According to Columbia, the first four designated scholars \"distinguish themselves for their remarkable academic and personal achievements, dynamism, intellectual curiosity, the originality and independence of their thinking, and the diversity that stems from their different cultures and their varied educational experiences.\"\n\nColumbia University is an independent, privately supported, nonsectarian institution of higher education. Its official corporate name is \"The Trustees of Columbia University in the City of New York.\" The university's first Charter was granted in 1754 by King George II; however, its modern Charter was first enacted in 1787 and last amended in 1810 by the New York State Legislature. The university is governed by 24 Trustees, customarily including the President, who serves ex officio. The Trustees themselves are responsible for choosing their successors. Six of the 24 are nominated from a pool of candidates recommended by the Columbia Alumni Association. Another six are nominated by the Board in consultation with the Executive Committee of the University Senate. The remaining 12, including the President, are nominated by the Trustees themselves through their internal processes. The term of office for Trustees is six years. Generally, they serve for no more than two consecutive terms. The Trustees appoint the President and other senior administrative officers of the university, and review and confirm faculty appointments as required. They determine the university's financial and investment policies, authorize the budget, supervise the endowment, direct the management of the university's real estate and other assets, and otherwise oversee the administration and management of the university.\n\nThe University Senate was established by the Trustees after a university-wide referendum in 1969. It succeeded to the powers of the University Council, which was created in 1890 as a body of faculty, deans, and other administrators to regulate inter-Faculty affairs and consider issues of university-wide concern. The University Senate is a unicameral body consisting of 107 members drawn from all constituencies of the university. These include the president of the university, the Provost, the Deans of Columbia College and the Graduate School of Arts and Sciences, all who serve ex officio, and five additional representatives, appointed by the President, from the university's administration. The President serves as the Senate's presiding officer. The Senate is charged with reviewing the educational policies, physical development, budget, and external relations of the university. It oversees the welfare and academic freedom of the faculty and the welfare of students.\n\nThe President of Columbia University, who is selected by the Trustees in consultation with the Executive Committee of the University Senate and who serves at the Trustees' pleasure, is the chief executive officer of the university. Assisting the President in administering the University are the Provost, the Senior Executive Vice President, the Executive Vice President for Health and Biomedical Sciences, several other vice presidents, the General Counsel, the Secretary of the University, and the deans of the Faculties, all of whom are appointed by the Trustees on the nomination of the President and serve at their pleasure. Lee C. Bollinger became the 19th President of Columbia University on June 1, 2002. A prominent advocate of affirmative action, he played a leading role in the twin Supreme Court cases—Grutter v Bollinger and Gratz v Bollinger—that upheld and clarified the importance of diversity as a compelling justification for affirmative action in higher education. A leading First Amendment scholar, he is widely published on freedom of speech and press, and serves on the faculty of Columbia Law School.\n\nColumbia has three official undergraduate colleges: Columbia College (CC), the liberal arts college offering the Bachelor of Arts degree, the Fu Foundation School of Engineering and Applied Science (also known as SEAS or Columbia Engineering) is the engineering and applied science school offering the Bachelor of Science degree, and The School of General Studies (GS), the liberal arts college offering the Bachelor of Arts degree to non-traditional students undertaking full- or part-time study.\n\nThe university is affiliated with Teachers College, Barnard College, and Union Theological Seminary, all located nearby in Morningside Heights. Of these, Barnard College and Teachers College are integrated as Faculties of the university. Furthermore, Teachers College is incorporated as an academic department of Columbia University, serving as the University's Graduate School of Education and Department of Education. Joint undergraduate programs are available through the Jewish Theological Seminary of America as well as through the Juilliard School.\n\nColumbia University was ranked 3rd among U.S. national universities for 2016 by Wall Street Journal/Times Higher Education and first among Ivy League universities was ranked 4th overall among U.S. national universities for 2016 by \"U.S. News & World Report\". Individual colleges and schools were also nationally ranked by \"U.S. News & World Report\" for its 2016 edition. The Columbia Law School was ranked tied for 4th, the Mailman School of Public Health 5th, the School of Social Work 5th, the Teachers College (Columbia Graduate School of Education) 7th, the Columbia Business School 8th, the College of Physicians and Surgeons tied for 8th for research (and tied for 52nd for primary care), the Graduate School of Arts 10th, the School of Nursing tied for 11th, and the Fu Foundation School of Engineering and Applied Science (graduate) was ranked 14th.\n\nIn 2016, Columbia was ranked 9th in the world by \"Academic Ranking of World Universities\", 20th in the world by \"QS World University Rankings\", and 16th globally by \"Times Higher Education World University Rankings\".\n\nRankings by other organizations include the Graduate School of Architecture, Planning and Preservation #2, and its Graduate School of Journalism #1.\n\nBetween 1996 and 2008, 18 Columbia affiliates have won Nobel Prizes, of whom nine are faculty members while one is an adjunct senior research scientist (Daniel Tsui) and the other a Global Fellow (Kofi Annan). Columbia faculty awarded the Nobel Prize include Richard Axel, Martin Chalfie, Eric Kandel, Tsung-Dao Lee, Robert Mundell, Orhan Pamuk, Edmund S. Phelps, Joseph Stiglitz, and Horst L. Stormer. Other awards and honors won by faculty include 30 MacArthur Foundation Award winners, 4 National Medal of Science recipients, 43 National Academy of Sciences Award winners, 20 National Academy of Engineering Award winners, 38 Institute of Medicine of the National Academies Award recipients and 143 American Academy of Arts and Sciences Award winners.\n\nIn 2015, Columbia University was ranked the first in the state by average professor salaries. In 2011, the ranked Columbia 3rd best university for forming CEOs in the US and 12th worldwide.\n\nColumbia was the first North American site where the uranium atom was split. The College of Physicians and Surgeons played a central role in developing the modern understanding of neuroscience with the publication of \"Principles of Neural Science\", described by historian of science Katja Huenther as the \"neuroscience 'bible'\". The book was written by a team of Columbia researchers that included Nobel Prize winner Eric Kandel, James H. Schwartz, and Thomas Jessell. Columbia was the birthplace of FM radio and the laser. The MPEG-2 algorithm of transmitting high quality audio and video over limited bandwidth was developed by Dimitris Anastassiou, a Columbia professor of electrical engineering. Biologist Martin Chalfie was the first to introduce the use of Green Fluorescent Protein (GFP) in labeling cells in intact organisms. Other inventions and products related to Columbia include Sequential Lateral Solidification (SLS) technology for making LCDs, System Management Arts (SMARTS), Session Initiation Protocol (SIP) (which is used for audio, video, chat, instant messaging and whiteboarding), pharmacopeia, Macromodel (software for computational chemistry), a new and better recipe for glass concrete, Blue LEDs, and Beamprop (used in photonics).\nColumbia scientists have been credited with about 175 new inventions in the health sciences each year. More than 30 pharmaceutical products based on discoveries and inventions made at Columbia are on the market today. These include Remicade (for arthritis), Reopro (for blood clot complications), Xalatan (for glaucoma), Benefix, Latanoprost (a glaucoma treatment), shoulder prosthesis, homocysteine (testing for cardiovascular disease), and Zolinza (for cancer therapy). Columbia Technology Ventures (formerly Science and Technology Ventures), , manages some 600 patents and more than 250 active license agreements. Patent-related deals earned Columbia more than $230 million in the 2006 fiscal year, according to the university, more than any university in the world. Columbia owns many unique research facilities, such as the Columbia Institute for Tele-Information dedicated to telecommunications and the Goddard Institute for Space Studies, which is an astronomical observatory affiliated with NASA.\n\nIn fall 2014, Columbia University's student population was 29,870 (8,559 students in undergraduate programs and 21,311 in postgraduate programs), with 39% of the student population identifying themselves as a minority and 28% born outside of the United States. Twenty-six percent of students at Columbia have family incomes below $60,000, making it one of the most socioeconomically diverse top-tier colleges. Sixteen percent of students at Columbia receive Federal Pell Grants, which mostly go to students whose family incomes are below $40,000. Fifteen percent of students are the first member of their family to attend a four-year college.\n\nOn-campus housing is guaranteed for all four years as an undergraduate. Columbia College and the Fu Foundation School of Engineering and Applied Science (also known as SEAS or Columbia Engineering) share housing in the on-campus residence halls. First-year students usually live in one of the large residence halls situated around South Lawn: Hartley Hall, Wallach Hall (originally Livingston Hall), John Jay Hall, Furnald Hall or Carman Hall. Upperclassmen participate in a room selection process, wherein students can pick to live in a mix of either corridor- or apartment-style housing with their friends. The Columbia University School of General Studies and graduate schools have their own apartment-style housing in the surrounding neighborhood.\n\nColumbia University is home to many fraternities, sororities, and co-educational Greek organizations. Approximately 10–15% of undergraduate students are associated with Greek life. There has been a Greek presence on campus since the establishment in 1836 of the Delta Chapter of Alpha Delta Phi. The InterGreek Council is the self-governing student organization that provides guidelines and support to its member organizations within each of the three councils at Columbia, the Interfraternity Council, Panhellenic Council, and Multicultural Greek Council. The three council presidents bring their affiliated chapters together once a month to meet as one Greek community. The InterGreek Council meetings provide opportunity for member organizations to learn from each other, work together and advocate for community needs.\n\nColumbia University is home to a rich diversity of undergraduate, graduate, and professional publications. The \"Columbia Daily Spectator\" is the nation's second-oldest student newspaper; and \"The Blue and White\", a monthly literary magazine established in 1890, has recently begun to delve into campus life and local politics in print and on its daily blog, dubbed the \"Bwog\". \"The Morningside Post\" is a student-run multimedia news publication. Its content: student-written investigative news, international affairs analysis, opinion, and satire.\n\nPolitical publications include \"The Current\", a journal of politics, culture and Jewish Affairs; the \"Columbia Political Review\", the multi-partisan political magazine of the Columbia Political Union; and \"AdHoc\", which denotes itself as the \"progressive\" campus magazine and deals largely with local political issues and arts events.\n\nArts and literary publications include \"The Columbia Review\", the nation's oldest college literary magazine; \"Columbia\", a nationally regarded literary journal; the \"Columbia Journal of Literary Criticism\"; and \"The Mobius Strip\", an online arts and literary magazine. \"Inside New York\" is an annual guidebook to New York City, written, edited, and published by Columbia undergraduates. Through a distribution agreement with Columbia University Press, the book is sold at major retailers and independent bookstores.\n\nColumbia is home to numerous undergraduate academic publications. The \"Journal of Politics & Society\", is a journal of undergraduate research in the social sciences, published and distributed nationally by the Helvidius Group; \"Publius\" is an undergraduate journal of politics established in 2008 and published biannually; the \"Columbia East Asia Review\" allows undergraduates throughout the world to publish original work on China, Japan, Korea, Tibet, and Vietnam and is supported by the Weatherhead East Asian Institute; and \"The Birch\", is an undergraduate journal of Eastern European and Eurasian culture that is the first national student-run journal of its kind; the \"Columbia Political Review\", the undergraduate magazine on politics operated by the Columbia Political Union; the \"Columbia Economics Review\", the undergraduate economic journal on research and policy supported by the Columbia Economics Department; and the \"Columbia Science Review\" is a science magazine that prints general interest articles, faculty profiles, and student research papers.\n\n\"The Fed\" a triweekly satire and investigative newspaper, and the \"Jester of Columbia\", the newly (and frequently) revived campus humor magazine both inject humor into local life. Other publications include \"The Columbian\", the undergraduate colleges' annually published yearbook the \"Gadfly\", a biannual journal of popular philosophy produced by undergraduates; and \"Rhapsody in Blue\", an undergraduate urban studies magazine. Professional journals published by academic departments at Columbia University include \"Current Musicology\" and \"The Journal of Philosophy\". During the spring semester, graduate students in the Journalism School publish \"The Bronx Beat\", a bi-weekly newspaper covering the South Bronx. Teachers College publishes the \"Teachers College Record\", a journal of research, analysis, and commentary in the field of education, published continuously since 1900.\n\nFounded in 1961 under the auspices of Columbia University's Graduate School of Journalism, the \"Columbia Journalism Review\" (CJR) examines day-to-day press performance as well as the forces that affect that performance. The magazine is published six times a year, and offers a reporting, analysis, criticism, and commentary. CJR.org, its web site, delivers real-time criticism and reporting, giving CJR a presence in the ongoing conversation about the media.\n\nColumbia is home to two pioneers in undergraduate campus radio broadcasting, WKCR-FM and CTV. WKCR, the student run radio station that broadcasts to the Tri-State area, claims to be the oldest FM radio station in the world, owing to the university's affiliation with Major Edwin Armstrong. The station went operational on July 18, 1939, from a 400-foot antenna tower in Alpine, New Jersey, broadcasting the very first FM transmission in the world. Initially, WKCR wasn't a radio station, but an organization concerned with the technology of radio communications. As membership grew, however, the nascent club turned its efforts to broadcasting. Armstrong helped the students in their early efforts, donating a microphone and turntables when they designed their first makeshift studio in a dorm room. The station has its studios on the second floor of Alfred Lerner Hall on the Morningside campus with its main transmitter tower at 4 Times Square in Midtown Manhattan. Columbia Television (CTV) is the nation's second oldest Student television station and home of CTV News, a weekly live news program produced by undergraduate students.\n\nThe Philolexian Society is a literary and debating club founded in 1802, making it the oldest student group at Columbia, as well as the third oldest collegiate literary society in the country. The society annually administers the Joyce Kilmer Bad Poetry Contest. The Columbia Parliamentary Debate Team competes in tournaments around the country as part of the American Parliamentary Debate Association, and hosts both high school and college tournaments on Columbia's campus, as well as public debates on issues affecting the university.\n\nThe Columbia International Relations Council and Association (CIRCA), oversees Columbia's Model United Nations activities. CIRCA hosts college and high school Model UN conferences, hosts speakers influential in international politics to speak on campus, trains students from underprivileged schools in New York in Model UN and oversees a competitive team, which travels to colleges around the country and to an international conference every year. The competitive team consistently wins best and outstanding delegation awards and is considered one of the top teams in the country.\n\nThe Columbia University Organization of Rising Entrepreneurs (CORE) was founded in 1999. The student-run group aims to foster entrepreneurship on campus. Each year CORE hosts dozens of events, including talks, #StartupColumbia, a conference and venture competition for $250,000, and Ignite@CU, a weekend for undergrads interested in design, engineering, and entrepreneurship. Notable speakers include Peter Thiel, Jack Dorsey, Alexis Ohanian, Drew Houston, and Mark Cuban. By 2006, CORE had awarded graduate and undergraduate students over $100,000 in seed capital.\n\nCampusNetwork, an on-campus social networking site called Campus Network that preceded Facebook, was created and popularized by Columbia engineering student Adam Goldberg in 2003. Mark Zuckerberg later asked Goldberg to join him in Palo Alto to work on Facebook, but Goldberg declined the offer. The Fu Foundation School of Engineering and Applied Science offers a minor in Technical Entrepreneurship through its Center for Technology, Innovation, and Community Engagement. SEAS' entrepreneurship activities focus on community building initiatives in New York and worldwide, made possible through partners such as Microsoft Corporation.\n\nColumbia is a top supplier of young engineering entrepreneurs for New York City. Over the past 20 years, graduates of Columbia established over 100 technology companies. Mayor Bloomberg has provided over $6.7 million towards entrepreneurial programs that partner with Columbia and other universities in New York. Professor Chris Wiggins of the Fu Foundation School of Engineering and Applied Science is working in conjunction with Professors Evan Korth of New York University and Hilary Mason, chief scientist at bit.ly to facilitate the growth of student tech-startups in an effort to transform a traditionally financially centered New York City into the next Silicon Valley. Their website, hackny.org, is a gathering ground of ideas and discussions for New York's young entrepreneurial community, the Silicon Alley.\n\nOn June 14, 2010, Mayor Michael R. Bloomberg launched the NYC Media Lab to promote innovations in New York's media industry. Situated at the New York University Tandon School of Engineering, the lab is a consortium of Columbia University, New York University, and New York City Economic Development Corporation acting to connect companies with universities in new technology research. The Lab is modeled after similar ones at MIT and Stanford. A $250,000 grant from the New York City Economic Development Corporation was used to establish the NYC Media Lab. Each year, the lab will host a range of roundtable discussions between the private sector and academic institutions. It will support research projects on topics of content format, next-generation search technologies, computer animation for film and gaming, emerging marketing techniques, and new devices development. The lab will also create a media research and development database. Columbia University will coordinate the long-term direction of the media lab as well as the involvement of its faculty and those of other universities.\n\nA member institution of the National Collegiate Athletic Association (NCAA) in Division I FCS, Columbia fields varsity teams in 29 sports and is a member of the Ivy League. The football Lions play home games at the 17,000-seat Robert K. Kraft Field at Lawrence A. Wien Stadium. The Baker Athletics Complex also includes facilities for baseball, softball, soccer, lacrosse, field hockey, tennis, track and rowing, as well as the new Campbell Sports Center opened in January 2013. The basketball, fencing, swimming & diving, volleyball and wrestling programs are based at the Dodge Physical Fitness Center on the main campus.\n\nColumbia University athletics has a long history, with many accomplishments in athletic fields. In 1870, Columbia played against Rutgers University in the second football game in the history of the sport. Eight years later, Columbia crew won the famed Henley Royal Regatta in the first-ever defeat for an English crew rowing in English waters. In 1900, Olympian and Columbia College student Maxie Long set the first official world record in the 400 meters with a time of 47.8 seconds. In 1983, Columbia men's soccer went 18-0 and was ranked first in the nation, but lost to Indiana 1-0 in double overtime in the NCAA championship game; nevertheless, the team went further toward the NCAA title than any Ivy League soccer team in history. The football program unfortunately is best known for its record of futility set during the 1980s: between 1983 and 1988, the team lost 44 games in a row, which is still the record for the NCAA Football Championship Subdivision. The streak was broken on October 8, 1988, with a 16-13 victory over archrival Princeton University. That was the Lions' first victory at Wien Stadium, which had been opened during the losing streak and was already four years old. A new tradition has developed with the Liberty Cup. The Liberty Cup is awarded annually to the winner of the football game between Fordham and Columbia Universities, two of the only three NCAA Division I football teams in New York City. The tradition began in 2002, a year after the Fordham-Columbia game was postponed due to the September 11 attacks.\n\nFormer students include Baseball Hall of Famers Lou Gehrig and Eddie Collins, football Hall of Famer Sid Luckman, Marcellus Wiley, and world champion women's weightlifter Karyn Marshall. On May 17, 1939, fledgling NBC broadcast a doubleheader between the Columbia Lions and the Princeton Tigers at Columbia's Baker Field, making it the first televised regular athletic event in history.\n\nEstablished in 2003 by university president Lee C. Bollinger, the World Leaders Forum at Columbia University provides the opportunity for undergraduate and graduate students alike to listen to world leaders in government, religion, industry, finance, and academia. The World Leaders Forum is a year-around event series that strives to provide a platform for uninhibited speech among nations and cultures, while educating students about problems and progress around the globe.\n\nAll Columbia undergraduates and graduates as well as students of Barnard College and other Columbia affiliated schools can register to participate in the World Leaders Forum using their student IDs. Even for individuals who do not have the privilege to attend the event live, they can watch the forum via online videos on Columbia University's website.\n\nPast forum speakers include former President of the United States Bill Clinton, the Prime Minister of India Atal Bihari Vajpayee, Former President of Ghana John Agyekum Kufuor, President of Afghanistan Hamid Karzai, Prime Minister of Russia Vladimir Putin, President of the Republic of Mozambique Joaquim Alberto Chissano, President of the Republic of Bolivia Carlos Diego Mesa Gisbert, President of the Republic of Romania Ion Iliescu, President of the Republic of Latvia Vaira Vīķe-Freiberga, the first female President of Finland Tarja Halonen, President Yudhoyono of Indonesia, President Pervez Musharraf of the Islamic Republic of Pakistan, Iraq President Jalal Talabani, the 14th Dalai Lama, President of the Islamic Republic of Iran Mahmoud Ahmadinejad, financier George Soros, Mayor of New York City Michael R. Bloomberg, President Václav Klaus of the Czech Republic, President Cristina Fernández de Kirchner of Argentina, former Secretary-General of the United Nations Kofi Annan, and Al Gore.\n\nThe Columbia University Orchestra was founded by composer Edward MacDowell in 1896, and is the oldest continually operating university orchestra in the United States. Undergraduate student composers at Columbia may choose to become involved with Columbia New Music, which sponsors concerts of music written by undergraduate students from all of Columbia's schools.\n\nThere are a number of performing arts groups at Columbia dedicated to producing student theater, including the Columbia Players, King's Crown Shakespeare Troupe (KCST), Columbia Musical Theater Society (CMTS), NOMADS (New and Original Material Authored and Directed by Students), LateNite Theatre, Columbia University Performing Arts League (CUPAL), Black Theatre Ensemble (BTE), sketch comedy group Chowdah, and improvisational troupes Alfred and Fruit Paunch. The Columbia University Marching Band tells jokes during the campus tradition of Orgo Night.\nThe Columbia Queer Alliance is the central Columbia student organization that represents the bisexual, lesbian, gay, transgender, and questioning student population. It is the oldest gay student organization in the world, founded as the Student Homophile League in 1967 by students including lifelong activist Stephen Donaldson. Columbia University campus military groups include the U.S. Military Veterans of Columbia University and Advocates for Columbia ROTC. In the 2005–06 academic year, the Columbia Military Society, Columbia's student group for ROTC cadets and Marine officer candidates, was renamed the Hamilton Society for \"students who aspire to serve their nation through the military in the tradition of Alexander Hamilton\".\n\nThe university also houses an independent nonprofit organization, Community Impact, which strives to serve disadvantaged people in the Harlem, Washington Heights, and Morningside Heights communities. From its earliest inception as a single service initiative formed in 1981 by Columbia University undergraduates, Community Impact has grown into Columbia University's largest student service organization. CI provides food, clothing, shelter, education, job training, and companionship for residents in its surrounding communities. CI consists of a dedicated corps of about 950 Columbia University student volunteers participating in 25 community service programs, which serve more than 8,000 people each year.\n\nStudents initiated a major demonstration in 1968 over two main issues. The first was Columbia's proposed gymnasium in neighboring Morningside Park; this was seen by the protesters to be an act of aggression aimed at the black residents of neighboring Harlem. A second issue was the Columbia administration's failure to resign its institutional membership in the Pentagon's weapons research think-tank, the Institute for Defense Analyses (IDA). Students barricaded themselves inside Low Library, Hamilton Hall, and several other university buildings during the protests, and New York City police were called onto the campus to arrest or forcibly remove the students.\n\nThe protests achieved two of their stated goals. Columbia disaffiliated from the IDA and scrapped the plans for the controversial gym, building a subterranean physical fitness center under the north end of campus instead. A popular myth states that the gym's plans were eventually used by Princeton University for the expansion of its athletic facilities, but as Jadwin Gymnasium was already 50% complete by 1966 (when the Columbia gym was announced) this was clearly not correct. At least 30 Columbia students were suspended by the administration as a result of the protests. Many of the Class of '68 walked out of their graduation and held a countercommencement on Low Plaza with a picnic following at Morningside Park, the place where the protests began. The protests hurt Columbia financially as many potential students chose to attend other universities and some alumni refused to donate money to the school. Allan Bloom, a professor of philosophy at the University of Chicago,\nbelieved that the protest efforts at Columbia were responsible for pushing higher education further toward the liberal left. As a result of the protests, Bloom stated, \"American universities were no longer places of intellectual and academic debate, but rather places of 'political correctness' and liberalism.\" \n\nFurther student protests, including hunger strike and more barricades of Hamilton Hall and the Business School during the late 1970s and early 1980s, were aimed at convincing the university trustees to divest all of the university's investments in companies that were seen as active or tacit supporters of the apartheid regime in South Africa. A notable upsurge in the protests occurred in 1978, when following a celebration of the tenth anniversary of the student uprising in 1968, students marched and rallied in protest of university investments in South Africa. The Committee Against Investment in South Africa (CAISA) and numerous student groups including the Socialist Action Committee, the Black Student Organization and the Gay Students group joined together and succeeded in pressing for the first partial divestment of a U.S. university.\n\nThe initial (and partial) Columbia divestment,\nfocused largely on bonds and financial institutions directly involved with the South African regime. It followed a year-long campaign first initiated by students who had worked together to block the appointment of former United States Secretary of State Henry Kissinger to an endowed chair at the university in 1977.\n\nBroadly backed by student groups and many faculty members the Committee Against Investment in South Africa held teach-ins and demonstrations through the year focused on the trustees ties to the corporations doing business with South Africa. Trustee meetings were picketed and interrupted by demonstrations culminating in May 1978 in the takeover of the Graduate School of Business.\n\nThe School of International and Public Affairs extends invitations to heads of state and heads of government who come to New York City for the opening of the fall session of the United Nations General Assembly. In 2007, Iranian President Mahmoud Ahmadinejad was one of those invited to speak on campus. Ahmadinejad accepted his invitation and spoke on September 24, 2007, as part of Columbia University's World Leaders Forum. The invitation proved to be highly controversial. Hundreds of demonstrators swarmed the campus on September 24 and the speech itself was televised worldwide. University President Lee C. Bollinger tried to allay the controversy by letting Ahmadenijad speak, but with a negative introduction (given personally by Bollinger). This did not mollify those who were displeased with the fact that the Iranian leader had been invited onto the campus. Columbia students, though, turned out en masse to listen to the speech on the South Lawn. An estimated 2,500 undergraduates and graduates came out for the historic occasion.\n\nDuring his speech, Ahmadinejad criticized Israel's policies towards the Palestinians; called for research on the historical accuracy of the Holocaust; raised questions as to who initiated the 9/11 attacks; defended Iran's nuclear power program, criticizing the UN's policy of sanctions on his country; and attacked U.S. foreign policy in the Middle East. In response to a question about Iran's treatment of women and homosexuals, he asserted that women are respected in Iran and that \"In Iran, we don't have homosexuals like in your country... In Iran, we do not have this phenomenon. I don't know who told you this.\" The latter statement drew laughter from the audience. The Manhattan District Attorney's Office accused Columbia of accepting grant money from the Alavi Foundation to support faculty \"sympathetic\" to Iran's Islamic republic.\n\nBeginning in 1969, during the Vietnam War, the university did not allow the U.S. military to have Reserve Officers' Training Corps (ROTC) programs on campus, though Columbia students could participate in ROTC programs at other local colleges and universities. At a forum at the university during the 2008 presidential election campaign, both John McCain and Barack Obama said that the university should consider reinstating ROTC on campus. After the debate, the President of the University, Lee C. Bollinger, stated that he did not favor reinstating Columbia's ROTC program, because of the military's anti-gay policies. In November 2008, Columbia's undergraduate student body held a referendum on the question of whether or not to invite ROTC back to campus, and the students who voted were almost evenly divided on the issue. ROTC lost the vote (which would not have been binding on the administration, and did not include graduate students, faculty, or alumni) by a fraction of a percentage point.\n\nIn April 2010 during Admiral Mike Mullen's address at Columbia, President Lee C. Bollinger stated that the ROTC would be readmitted to campus if the admiral's plans for revoking the don't ask, don't tell policy were successful. In February 2011 during one of three town-hall meetings on the ROTC ban, former Army staff sergeant Anthony Maschek, a Purple Heart recipient for injuries sustained during his service in Iraq, was booed and hissed at by some students during his speech promoting the idea of allowing the ROTC on campus. In April 2011 the Columbia University Senate voted to welcome the ROTC program back on campus. Secretary of the Navy Ray Mabus and Columbia University President Lee C. Bollinger signed an agreement to reinstate Naval Reserve Officers Training Corps (NROTC) program at Columbia for the first time in more than 40 years on May 26, 2011. The agreement was signed at a ceremony on board the USS Iwo Jima, docked in New York for the Navy's annual Fleet Week.\n\nFrom 1975 until 2016, on the day before the Organic Chemistry exam—which is often on the first day of finals—at precisely the stroke of midnight, the Columbia University Marching Band occupied Butler Library to distract diligent students from studying. After a forty-five minutes or so of jokes and music, the procession then moved out to the lawn in front of Hartley, Wallach and John Jay residence halls to entertain the residents there. The band then plays at other locations around Morningside Heights, including the residential quadrangle of Barnard College, where students of the all-women's school, in mock-consternation, rain trash – including notes and course packets – and water balloons upon them from their dormitories above. The band tends to close their Orgo Night performances before Furnald Hall, known among students as the more studious and reportedly \"anti-social\" residence hall, where the underclassmen in the band serenade the graduating seniors with an entertaining, though vulgar, mock-hymn to Columbia, composed of quips that poke fun at the stereotypes about the Columbia student body.\n\nIn December 2016, University administrators banned the Marching Band from performing its Orgo Night show in its traditional Butler Library location, setting off a storm of protests and accusations that University President Lee C. Bollinger was censoring the Band's speech. Concerned alumni initiated an organized campaign to restore the Orgo Night show by publishing a series of pamphlets addressing the issues. University administration continued to prohibit the traditional Orgo Night performance during the spring semester of 2017, prompting the Marching Band to again stage the show outside the library on May 4, 2017 as continued objections from alumni persist. University administrators sent a letter to concerned alumni on April 27, 2017 claiming to be \"working closely\" with the current Band leadership concerning the future of Orgo Night, but the statement turned out to be false. Band leadership had received no communications or outreach from the university and the administrators had refused to engage in any discussion about the future of the Orgo Night tradition.\n\nThe campus Tree-Lighting Ceremony was inaugurated in 1998. It celebrates the illumination of the medium-sized trees lining College Walk in front of Kent and Hamilton Halls on the east end and Dodge and Journalism Halls on the west, just before finals week in early December. The lights remain on until February 28. Students meet at the sun-dial for free hot chocolate, performances by \"a cappella\" groups, and speeches by the university president and a guest.\n\nImmediately following the College Walk festivities is one of Columbia's older holiday traditions, the lighting of the Yule Log. The Christmas ceremony dates to a period prior to the American Revolutionary War, but lapsed before being revived by University President Nicholas Murray Butler in the early 20th century. A troop of students dressed as Continental Army soldiers carry the eponymous log from the sun-dial to the lounge of John Jay Hall, where it is lit amid the singing of seasonal carols. The Christmas ceremony is accompanied by a reading of \"A Visit From St. Nicholas\" by Clement Clarke Moore and \"Yes, Virginia, There is a Santa Claus\" by Francis Pharcellus Church.\n\nThe Varsity Show is an annual musical written by and for students and was established in 1894, making it one of Columbia's oldest traditions. Past writers and directors have included Columbians Richard Rodgers and Oscar Hammerstein, Lorenz Hart, I.A.L. Diamond, and Herman Wouk. The show has one of the largest operating budgets of all university events.\n\nAs of 2011, Columbia alumni included three United States Presidents, 26 foreign Heads of State, ten Justices of the Supreme Court of the United States (including three Chief Justices) and 39 Nobel winners. As of 2011, alumni also have received more than 35 National Book Awards and 123 Pulitzer Prizes. Today, two United States Senators and 16 Chief Executives of Fortune 500 companies hold Columbia degrees, as do three of the 25 richest Americans and 20 living billionaires. Five Founding Fathers including a member of the Committee of Five are alumni of Columbia University, then named \"King's College\".\n\nFormer U.S. Presidents Theodore Roosevelt and Franklin Delano Roosevelt attended the law school. Other more recent political figures educated at Columbia include former U.S President Barack Obama, Associate Justice of the U.S. Supreme Court Ruth Bader Ginsburg, former U.S. Secretary of State Madeleine Albright, former chairman of the U.S. Federal Reserve Bank Alan Greenspan, U.S. Attorney General Eric Holder, and U.S. Solicitor General Donald Verrilli Jr. Dwight D. Eisenhower served as the thirteenth president of Columbia University from 1948 to 1953. The university has also educated 26 foreign heads of state, including President of Georgia Mikheil Saakashvili, President of East Timor Jose Ramos Horta, President of Estonia Toomas Hendrik Ilves and other historical figures such as Wellington Koo, Radovan Karadžić, Gaston Eyskens, and T. V. Soong. The author of India's constitution and Dalit leader Dr. B. R. Ambedkar was also an alumnus of Columbia.\n\nAlumni of Columbia have occupied top positions in Wall Street and the rest of the business world. Notable members of the Astor family attended Columbia, while some recent business graduates include investor Warren Buffett, former CEO of PBS and NBC Larry Grossman, and chairman of Wal-Mart S. Robson Walton. CEO's of top Fortune 500 companies include James P. Gorman of Morgan Stanley, Robert J. Stevens of Lockheed Martin, Philippe Dauman of Viacom, Ursula Burns of Xerox, and Vikram Pandit of Citigroup. Notable labor organizer and women's educator Louise Leonard McLaren received her degree of Master of Arts from Columbia.\n\nIn science and technology, Columbia alumni include: founder of IBM Herman Hollerith; inventor of FM radio Edwin Armstrong; Francis Mechner; integral in development of the nuclear submarine Hyman Rickover; founder of Google China Kai-Fu Lee; scientists Stephen Jay Gould, Robert Millikan, Helium–neon laser inventor Ali Javan and Mihajlo Pupin; chief-engineer of the New York City Subway, William Barclay Parsons; philosophers Irwin Edman and Robert Nozick; economist Milton Friedman; and psychologist Harriet Babcock.\n\nMany Columbia alumni have gone on to renowned careers in the arts, such as the composers Richard Rodgers, Oscar Hammerstein II, Lorenz Hart, and Art Garfunkel. Four United States Poet Laureates received their degrees from Columbia. Columbia alumni have made an indelible mark in the field of American poetry and literature, with such people as Jack Kerouac and Allen Ginsberg, pioneers of the Beat Generation, and Langston Hughes, a seminal figure in the Harlem Renaissance, all having attended the university. Other notable writers who attended Columbia include authors Isaac Asimov, J.D. Salinger, Upton Sinclair, Danielle Valore Evans, and Hunter S. Thompson.\n\nUniversity alumni have also been very prominent in the film industry, with 28 alumni and former students winning a combined 39 Academy Awards (as of 2011), second in the world only to New York University (NYU). Some notable Columbia alumni that have gone on to work in film include directors Sidney Lumet (\"12 Angry Men\") and Kathryn Bigelow (\"The Hurt Locker\"), screenwriters Howard Koch (\"Casablanca\") and Joseph L. Mankiewicz (\"All About Eve\"), and actors James Cagney and Ed Harris.\n\n\n"}
{"id": "6312", "url": "https://en.wikipedia.org/wiki?curid=6312", "title": "Cell wall", "text": "Cell wall\n\nA cell wall is a structural layer surrounding some types of cells, situated outside the cell membrane. It can be tough, flexible, and sometimes rigid. It provides the cell with both structural support and protection, and also acts as a filtering mechanism. Cell walls are present in most prokaryotes (except mycoplasma bacteria), in algae, plants and fungi but rarely in other eukaryotes including animals. A major function is to act as pressure vessels, preventing over-expansion of the cell when water enters.\n\nThe composition of cell walls varies between species and may depend on cell type and developmental stage. The primary cell wall of land plants is composed of the polysaccharides cellulose, hemicellulose and pectin. Often, other polymers such as lignin, suberin or cutin are anchored to or embedded in plant cell walls. Algae possess cell walls made of glycoproteins and polysaccharides such as carrageenan and agar that are absent from land plants. In bacteria, the cell wall is composed of peptidoglycan. The cell walls of archaea have various compositions, and may be formed of glycoprotein S-layers, pseudopeptidoglycan, or polysaccharides. Fungi possess cell walls made of the glucosamine polymer chitin. Unusually, diatoms have a cell wall composed of biogenic silica.\n\nA plant cell wall was first observed and named (simply as a \"wall\") by Robert Hooke in 1665. However, \"the dead excrusion product of the living protoplast\" was forgotten, for almost three centuries, being the subject of scientific interest mainly as a resource for industrial processing or in relation to animal or human health.\n\nIn 1804, Karl Rudolphi and J.H.F. Link proved that cells had independent cell walls. Before, it had been thought that cells shared walls and that fluid passed between them this way.\n\nThe mode of formation of the cell wall was controversial in the 19th century. Hugo von Mohl (1853, 1858) advocated the idea that the cell wall grows by apposition. Carl Nägeli (1858, 1862, 1863) believed that the growth of the wall in thickness and in area was due to a process termed intussusception. Each theory was improved in the following decades: the apposition (or lamination) theory by Eduard Strasburger (1882, 1889), and the intussusception theory by Julius Wiesner (1886).\n\nIn 1930, Ernst Münch coined the term \"apoplast\" in order to separate the \"living\" symplast from the \"dead\" plant region, the latter of which included the cell wall.\n\nBy the 1980s, some authors suggested replacing the term \"cell wall\", particularly as it was used for plants, with the more precise term \"extracellular matrix\", as used for animal cells, but others preferred the older term.\n\nCell walls serve similar purposes in those organisms that possess them. They may give cells rigidity and strength, offering protection against mechanical stress. In multicellular organisms, they permit the organism to build and hold a definite shape (morphogenesis). Cell walls also limit the entry of large molecules that may be toxic to the cell. They further permit the creation of stable osmotic environments by preventing osmotic lysis and helping to retain water. Their composition, properties, and form may change during the cell cycle and depend on growth conditions.\n\nIn most cells, the cell wall is flexible, meaning that it will bend rather than holding a fixed shape, but has considerable tensile strength. The apparent rigidity of primary plant tissues is enabled by cell walls, but is not due to the walls' stiffness. Hydraulic turgor pressure creates this rigidity, along with the wall structure. The flexibility of the cell walls is seen when plants wilt, so that the stems and leaves begin to droop, or in seaweeds that bend in water currents. As John Howland explains:\nThe apparent rigidity of the cell wall thus results from inflation of the cell contained within. This inflation is a result of the passive uptake of water.\n\nIn plants, a secondary cell wall is a thicker additional layer of cellulose which increases wall rigidity. Additional layers may be formed by lignin in xylem cell walls, or suberin in cork cell walls. These compounds are rigid and waterproof, making the secondary wall stiff. Both wood and bark cells of trees have secondary walls. Other parts of plants such as the leaf stalk may acquire similar reinforcement to resist the strain of physical forces.\n\nThe primary cell wall of most plant cells is freely permeable to small molecules including small proteins, with size exclusion estimated to be 30-60 kDa. The pH is an important factor governing the transport of molecules through cell walls.\n\nCell walls evolved independently in many groups, even in the photosynthetic eukaryotes. In these lineages, the cell wall is closely related to the evolution of multicellularity, terrestrialization and vascularization.\n\nThe walls of plant cells must have sufficient tensile strength to withstand internal osmotic pressures of several times atmospheric pressure that result from the difference in solute concentration between the cell interior and external solutions. Plant cell walls vary from 0.1 to several µm in thickness.\n\nUp to three strata or layers may be found in plant cell walls:\n\n\nIn the primary (growing) plant cell wall, the major carbohydrates are cellulose, hemicellulose and pectin. The cellulose microfibrils are linked via hemicellulosic tethers to form the cellulose-hemicellulose network, which is embedded in the pectin matrix. The most common hemicellulose in the primary cell wall is xyloglucan. In grass cell walls, xyloglucan and pectin are reduced in abundance and partially replaced by glucuronarabinoxylan, another type of hemicellulose. Primary cell walls characteristically extend (grow) by a mechanism called acid growth, which involves turgor-driven movement of the strong cellulose microfibrils within the weaker hemicellulose/pectin matrix, catalyzed by expansin proteins. The outer part of the primary cell wall of the plant epidermis is usually impregnated with cutin and wax, forming a permeability barrier known as the plant cuticle.\n\nSecondary cell walls contain a wide range of additional compounds that modify their mechanical properties and permeability. The major polymers that make up wood (largely secondary cell walls) include:\n\nAdditionally, structural proteins (1-5%) are found in most plant cell walls; they are classified as hydroxyproline-rich glycoproteins (HRGP), arabinogalactan proteins (AGP), glycine-rich proteins (GRPs), and proline-rich proteins (PRPs). Each class of glycoprotein is defined by a characteristic, highly repetitive protein sequence. Most are glycosylated, contain hydroxyproline (Hyp) and become cross-linked in the cell wall. These proteins are often concentrated in specialized cells and in cell corners. Cell walls of the epidermis may contain cutin. The Casparian strip in the endodermis roots and cork cells of plant bark contain suberin. Both cutin and suberin are polyesters that function as permeability barriers to the movement of water. The relative composition of carbohydrates, secondary compounds and proteins varies between plants and between the cell type and age. Plant cells walls also contain numerous enzymes, such as hydrolases, esterases, peroxidases, and transglycosylases, that cut, trim and cross-link wall polymers.\n\nSecondary walls - especially in grasses - may also contain microscopic silica crystals, which may strengthen the wall and protect it from herbivores.\n\nCell walls in some plant tissues also function as storage deposits for carbohydrates that can be broken down and resorbed to supply the metabolic and growth needs of the plant. For example, endosperm cell walls in the seeds of cereal grasses, nasturtium\nand other species, are rich in glucans and other polysaccharides that are readily digested by enzymes during seed germination to form simple sugars that nourish the growing embryo.\n\nThe middle lamella is laid down first, formed from the cell plate during cytokinesis, and the primary cell wall is then deposited inside the middle lamella. The actual structure of the cell wall is not clearly defined and several models exist - the covalently linked cross model, the tether model, the diffuse layer model and the stratified layer model. However, the primary cell wall, can be defined as composed of cellulose microfibrils aligned at all angles. Cellulose microfibrils are produced at the plasma membrane by the cellulose synthase complex, which is proposed to be made of a hexameric rosette that contains three cellulose synthase catalytic subunits for each of the six units. Microfibrils are held together by hydrogen bonds to provide a high tensile strength. The cells are held together and share the gelatinous membrane called the \"middle lamella\", which contains magnesium and calcium pectates (salts of pectic acid). Cells interact though plasmodesmata, which are inter-connecting channels of cytoplasm that connect to the protoplasts of adjacent cells across the cell wall.\n\nIn some plants and cell types, after a maximum size or point in development has been reached, a \"secondary wall\" is constructed between the plasma membrane and primary wall. Unlike the primary wall, the cellulose microfibrils are aligned parallel in layers, the orientation changing slightly with each additional layer so that the structure becomes helicoidal. Cells with secondary cell walls can be rigid, as in the gritty sclereid cells in pear and quince fruit. Cell to cell communication is possible through pits in the secondary cell wall that allow plasmodesmata to connect cells through the secondary cell walls.\n\nThere are several groups of organisms that have been called \"fungi\". Some of these groups (Oomycete and Myxogastria) have been transferred out of the Kingdom Fungi, in part because of fundamental biochemical differences in the composition of the cell wall. Most true fungi have a cell wall consisting largely of chitin and other polysaccharides. True fungi do not have cellulose in their cell walls.\n\nIn fungi, the cell wall is the outer-most layer, external to the plasma membrane. The fungal cell wall is a matrix of three main components:\n\nLike plants, algae have cell walls. Algal cell walls contain either polysaccharides (such as cellulose (a glucan)) or a variety of glycoproteins (Volvocales) or both. The inclusion of additional polysaccharides in algal cells walls is used as a feature for algal taxonomy.\n\n\nOther compounds that may accumulate in algal cell walls include sporopollenin and calcium ions.\n\nThe group of algae known as the diatoms synthesize their cell walls (also known as frustules or valves) from silicic acid (specifically orthosilicic acid, HSiO). The acid is polymerised intra-cellularly, then the wall is extruded to protect the cell. Significantly, relative to the organic cell walls produced by other groups, silica frustules require less energy to synthesize (approximately 8%), potentially a major saving on the overall cell energy budget and possibly an explanation for higher growth rates in diatoms.\n\nIn brown algae, phlorotannins may be a constituent of the cell walls.\n\nThe group Oomycetes, also known as water molds, are saprotrophic plant pathogens like fungi. Until recently they were widely believed to be fungi, but structural and molecular evidence has led to their reclassification as heterokonts, related to autotrophic brown algae and diatoms. Unlike fungi, oomycetes typically possess cell walls of cellulose and glucans rather than chitin, although some genera (such as \"Achlya\" and \"Saprolegnia\") do have chitin in their walls. The fraction of cellulose in the walls is no more than 4 to 20%, far less than the fraction of glucans. Oomycete cell walls also contain the amino acid hydroxyproline, which is not found in fungal cell walls.\n\nThe dictyostelids are another group formerly classified among the fungi. They are slime molds that feed as unicellular amoebae, but aggregate into a reproductive stalk and sporangium under certain conditions. Cells of the reproductive stalk, as well as the spores formed at the apex, possess a cellulose wall. The spore wall has been shown to possess three layers, the middle of which is composed primarily of cellulose, and the innermost is sensitive to cellulase and pronase.\n\nAround the outside of the cell membrane is the bacterial cell wall. Bacterial cell walls are made of peptidoglycan (also called murein), which is made from polysaccharide chains cross-linked by unusual peptides containing D-amino acids. Bacterial cell walls are different from the cell walls of plants and fungi which are made of cellulose and chitin, respectively. The cell wall of bacteria is also distinct from that of Archaea, which do not contain peptidoglycan. The cell wall is essential to the survival of many bacteria, although L-form bacteria can be produced in the laboratory that lack a cell wall. The antibiotic penicillin is able to kill bacteria by preventing the cross-linking of peptidoglycan and this causes the cell wall to weaken and lyse. The lysozyme enzyme can also damage bacterial cell walls.\n\nThere are broadly speaking two different types of cell wall in bacteria, called Gram-positive and Gram-negative. The names originate from the reaction of cells to the Gram stain, a test long-employed for the classification of bacterial species.\n\nGram-positive bacteria possess a thick cell wall containing many layers of peptidoglycan and teichoic acids. In contrast, Gram-negative bacteria have a relatively thin cell wall consisting of a few layers of peptidoglycan surrounded by a second lipid membrane containing lipopolysaccharides and lipoproteins. Most bacteria have the Gram-negative cell wall and only the Firmicutes and Actinobacteria (previously known as the low G+C and high G+C Gram-positive bacteria, respectively) have the alternative Gram-positive arrangement. These differences in structure can produce differences in antibiotic susceptibility, for instance vancomycin can kill only Gram-positive bacteria and is ineffective against Gram-negative pathogens, such as \"Haemophilus influenzae\" or \"Pseudomonas aeruginosa\".\n\nAlthough not truly unique, the cell walls of Archaea are unusual. Whereas peptidoglycan is a standard component of all bacterial cell walls, all archaeal cell walls lack peptidoglycan, with the exception of one group of methanogens. In that group, the peptidoglycan is a modified form very different from the kind found in bacteria. There are four types of cell wall currently known among the Archaea.\n\nOne type of archaeal cell wall is that composed of pseudopeptidoglycan (also called pseudomurein). This type of wall is found in some methanogens, such as \"Methanobacterium\" and \"Methanothermus\". While the overall structure of archaeal \"pseudo\"peptidoglycan superficially resembles that of bacterial peptidoglycan, there are a number of significant chemical differences. Like the peptidoglycan found in bacterial cell walls, pseudopeptidoglycan consists of polymer chains of glycan cross-linked by short peptide connections. However, unlike peptidoglycan, the sugar N-acetylmuramic acid is replaced by N-acetyltalosaminuronic acid, and the two sugars are bonded with a \"β\",1-3 glycosidic linkage instead of \"β\",1-4. Additionally, the cross-linking peptides are L-amino acids rather than D-amino acids as they are in bacteria.\n\nA second type of archaeal cell wall is found in \"Methanosarcina\" and \"Halococcus\". This type of cell wall is composed entirely of a thick layer of polysaccharides, which may be sulfated in the case of \"Halococcus\". Structure in this type of wall is complex and not fully investigated.\n\nA third type of wall among the Archaea consists of glycoprotein, and occurs in the hyperthermophiles, \"Halobacterium\", and some methanogens. In \"Halobacterium\", the proteins in the wall have a high content of acidic amino acids, giving the wall an overall negative charge. The result is an unstable structure that is stabilized by the presence of large quantities of positive sodium ions that neutralize the charge. Consequently, \"Halobacterium\" thrives only under conditions with high salinity.\n\nIn other Archaea, such as \"Methanomicrobium\" and \"Desulfurococcus\", the wall may be composed only of surface-layer proteins, known as an \"S-layer\". S-layers are common in bacteria, where they serve as either the sole cell-wall component or an outer layer in conjunction with polysaccharides. Most Archaea are Gram-negative, though at least one Gram-positive member is known.\n\nMany protists and bacteria produce other cell surface structures apart from cell walls, external (extracellular matrix) or internal. Many algae have a sheath or envelope of mucilage outside the cell made of exopolysaccharides. Diatoms build a frustule from silica extracted from the surrounding water; radiolarians, foraminiferans, testate amoebae and silicoflagellates also produce a skeleton from minerals, called test in some groups. Many green algae, such as \"Halimeda\" and the Dasycladales, and some red algae, the Corallinales, encase their cells in a secreted skeleton of calcium carbonate. In each case, the wall is rigid and essentially inorganic. It is the non-living component of cell. Some golden algae, ciliates and choanoflagellates produces a shell-like protective outer covering called lorica. Some dinoflagellates have a theca of cellulose plates, and coccolithophorids have coccoliths.\n\nAn extracellular matrix is also present in metazoans. Its composition varies between cells, but collagens are the most abundant protein in the ECM.\n\n\n"}
{"id": "6313", "url": "https://en.wikipedia.org/wiki?curid=6313", "title": "Classical element", "text": "Classical element\n\nClassical elements typically refer to the concepts in Ancient Greece, of earth, water, air, fire, and aether, which were proposed to explain the nature and complexity of all matter in terms of simpler substances. Ancient cultures in Egypt, Babylonia, Japan, Tibet, and India had similar lists, sometimes referring in local languages to \"air\" as \"wind\" and the fifth element as \"void\". The Chinese Wu Xing system lists Wood (木 \"mù\"), Fire (火 \"huǒ\"), Earth (土 \"tǔ\"), Metal (金 \"jīn\"), and Water (水 \"shuǐ\"), though these are described more as energies or transitions than as types of material.\n\nThese different cultures and even individual philosophers had widely varying explanations concerning their attributes and how they related to observable phenomena as well as cosmology. Sometimes these theories overlapped with mythology and were personified in deities. Some of these interpretations included atomism (the idea of very small, indivisible portions of matter) but other interpretations considered the elements to be divisible into infinitely small pieces without changing their nature.\n\nWhile the classification of the material world by the ancient Indians and Greeks into Air, Earth, Fire and Water was more philosophical, during the Islamic Golden Age medieval middle eastern scientists used practical, experimental observation to classify materials. In Europe, the Ancient Greek system of Aristotle evolved slightly into the medieval system, which for the first time in Europe became subject to experimental verification in the 1600s, during the Scientific Revolution.\n\nCenturies of empirical investigation have proven that all the ancient systems were incorrect explanations of the physical world. It is now known that atomic theory is a correct explanation, and that atoms can be classified into more than a hundred chemical elements such as oxygen, iron, and mercury. These elements form chemical compounds and mixtures, and under different temperatures and pressures, these substances can adopt different states of matter. The most commonly observed states of solid, liquid, gas, and plasma share many attributes with the classical elements of earth, water, air, and fire, respectively, but it is now known that these states are due to similar behavior of different types of atoms at similar energy levels, and not due to containing a certain type of atom or a certain type of infinitely divisible substance or energy.\n\nIn classical thought, the four elements earth, water, air, and fire as proposed by Empedocles frequently occur; Aristotle added a fifth element, aether; it has been called akasha in India and quintessence in Europe.\n\nThe concept of the five elements formed a basis of analysis in both Hinduism and Buddhism. In Hinduism, particularly in an esoteric context, the four states-of-matter describe matter, and a fifth element describes that which was beyond the material world. Similar lists existed in ancient China, Korea and Japan. In Buddhism the four great elements, to which two others are sometimes added, are not viewed as substances, but as categories of sensory experience.\n\nA Greek text called the \"Kore Kosmou\" (\"Virgin of the World\") ascribed to Hermes Trismegistus (associated with the Egyptian god Thoth), names the four elements fire, water, air, and earth. As described in this book:\n\nAnd Isis answer made: Of living things, my son, some are made friends with \"fire\", and some with \"water\", some with \"air\", and some with \"earth\", and some with two or three of these, and some with all. And, on the contrary, again some are made enemies of fire, and some of water, some of earth, and some of air, and some of two of them, and some of three, and some of all. For instance, son, the locust and all flies flee fire; the eagle and the hawk and all high-flying birds flee water; fish, air and earth; the snake avoids the open air. Whereas snakes and all creeping things love earth; all swimming things love water; winged things, air, of which they are the citizens; while those that fly still higher love the fire and have the habitat near it. Not that some of the animals as well do not love fire; for instance salamanders, for they even have their homes in it. It is because one or another of the elements doth form their bodies' outer envelope. Each soul, accordingly, while it is in its body is weighted and constricted by these four.\n\nAccording to Galen, these elements were used by Hippocrates in describing the human body with an association with the four humours: yellow bile (fire), black bile (earth), blood (air), and phlegm (water). Medical care was flexible and primarily about helping the patient stay in or return to his/her own personal natural balanced state.\n\nIn Babylonian mythology, the cosmogony called \"Enûma Eliš\", a text written between the 18th and 16th centuries BC, involves four gods that we might see as personified cosmic elements: sea, earth, sky, wind. In other Babylonian texts these phenomena are considered independent of their association with deities, though they are not treated as the component elements of the universe, as later in Empedocles.\n\nThe system of five elements are found in Vedas, especially Ayurveda, the \"pancha mahabhuta\", or \"five great elements\", of Hinduism are \"bhūmi\" (earth), \"ap\" or \"jala\" (water), \"tejas\" or \"agni\" (fire), \"marut\", \"vayu\" or \"pavan\" (air or wind) and \"vyom\" or \"shunya\" (space or zero) or \"akash\" (aether or void). They further suggest that all of creation, including the human body, is made up of these five essential elements and that upon death, the human body dissolves into these five elements of nature, thereby balancing the cycle of nature.\n\nThe five elements are associated with the five senses, and act as the gross medium for the experience of sensations. The basest element, earth, created using all the other elements, can be perceived by all five senses – (i) hearing, (ii) touch, (iii) sight, (iv) taste, and (v) smell. The next higher element, water, has no odor but can be heard, felt, seen and tasted. Next comes fire, which can be heard, felt and seen. Air can be heard and felt. \"Akasha\" (aether) is beyond the senses of smell, taste, sight, and touch; it being accessible to the sense of hearing alone.\n\nIn the Pali literature, the \"mahabhuta\" (\"great elements\") or \"catudhatu\" (\"four elements\") are earth, water, fire and air. In early Buddhism, the four elements are a basis for understanding suffering and for liberating oneself from suffering. The earliest Buddhist texts explain that the four primary material elements are the sensory qualities solidity, fluidity, temperature, and mobility; their characterization as earth, water, fire, and air, respectively, is declared an abstraction – instead of concentrating on the fact of material existence, one observes how a physical thing is sensed, felt, perceived.\n\nThe Buddha's teaching regarding the four elements is to be understood as the base of all observation of real sensations rather than as a philosophy. The four properties are cohesion (water), solidity or inertia (earth), expansion or vibration (air) and heat or energy content (fire). He promulgated a categorization of mind and matter as composed of eight types of \"kalapas\" of which the four elements are primary and a secondary group of four are color, smell, taste, and nutriment which are derivative from the four primaries.\n\nThanissaro Bhikkhu (1997) renders an extract of Shakyamuni Buddha's from Pali into English thus:\nTibetan Buddhist medical literature speaks of the Panch Mahābhūta (five elements).\n\nThe Chinese had a somewhat different series of elements, namely Fire, Earth, Metal (literally gold), Water and Wood, which were understood as different types of energy in a state of constant interaction and flux with one another, rather than the Western notion of different kinds of material.\n\nAlthough it is usually translated as \"element\", the Chinese word \"xing\" literally means something like \"changing states of being\", \"permutations\" or \"metamorphoses of being\". In fact Sinologists cannot agree on any single translation. The Chinese elements were seen as ever changing and movingone translation of \"wu xing\" is simply \"the five changes\".\n\nThe Wu Xing are chiefly an ancient mnemonic device for systems with five stages; hence the preferred translation of \"movements\", \"phases\" or \"steps\" over \"elements.\"\n\nIn the bagua, metal is associated with the divination figure 兌 \"Duì\" (☱, the lake or marsh: 澤/泽 \"zé\") and with 乾 \"Qián\" (☰, the sky or heavens: 天 \"tiān\"). Wood is associated with 巽 \"Xùn\" (☴, the wind: 風/风 \"fēng\") and with 震 \"Zhèn\" (☳, the arousing/thunder: 雷 \"léi\"). In view of the durability of meteoric iron, metal came to be associated with the aether, which is sometimes conflated with Stoic pneuma, as both terms originally referred to air (the former being higher, brighter, more fiery or celestial and the latter being merely warmer, and thus vital or biogenetic). In Taoism, \"qi\" functions similarly to pneuma in a prime matter (a basic principle of energetic transformation) that accounts for both biological and inanimate phenomena.\n\nIn Chinese philosophy the universe consists of heaven and earth. The five major planets are associated with and even named after the elements: Jupiter 木星 is Wood (木), Mars 火星 is Fire (火), Saturn 土星 is Earth (土), Venus 金星 is Metal (金), and Mercury 水星 is Water (水). Also, the Moon represents Yin (陰), and the Sun 太陽 represents Yang (陽). Yin, Yang, and the five elements are associated with themes in the I Ching, the oldest of Chinese classical texts which describes an ancient system of cosmology and philosophy. The five elements also play an important part in Chinese astrology and the Chinese form of geomancy known as Feng shui.\n\nThe doctrine of five phases describes two cycles of balance, a generating or creation (生, shēng) cycle and an overcoming or destruction (克/剋, kè) cycle of interactions between the phases.\n\n\"Generating\"\n\n\"Overcoming\"\n\nThere are also two cycles of imbalance, an overacting cycle (cheng) and an insulting cycle (wu).\n\nThe ancient Greek belief in five basic elements, these being earth (γῆ \"ge\"), water (ὕδωρ \"hudor\"), air (ἀήρ \"aer\"), fire (πῦρ \"pur\") and aether (αἰθήρ \"aither\"), dates from pre-Socratic times and persisted throughout the Middle Ages and into the Renaissance, deeply influencing European thought and culture. These five elements are sometimes associated with the five platonic solids.\n\nSicilian philosopher Empedocles (ca. 450 BC) proved (at least to his satisfaction) that air was a separate substance by observing that a bucket inverted in water did not become filled with water, a pocket of air remaining trapped inside. Prior to Empedocles, Greek philosophers had debated which substance was the primordial element from which everything else was made; Heraclitus championed fire, Thales supported water, and Anaximenes plumped for air. Anaximander argued that the primordial substance was not any of the known substances, but could be transformed into them, and they into each other. Empedocles was the first to propose four elements, fire, earth, air, and water. He called them the four \"roots\" (ῥιζὤματα, rhizōmata).\n\nPlato seems to have been the first to use the term \"element (στοιχεῖον, \"stoicheion\")\" in reference to air, fire, earth, and water. The ancient Greek word for element, \"stoicheion\" (from \"stoicheo\", \"to line up\") meant \"smallest division (of a sun-dial), a syllable\", as the composing unit of an alphabet it could denote a letter and the smallest unit from which a word is formed. A similar alphabetic metaphor may be the origin of the equivalent Latin word \"elementum\" (from which the English word comes), possibly based on the names of the letters 'l', 'm', and 'n', though the validity of this idea is debated.\n\nIn his \"On Generation and Corruption\", Aristotle related each of the four elements to two of the four sensible qualities:\n\nA classic diagram has one square inscribed in the other, with the corners of one being the classical elements, and the corners of the other being the properties. The opposite corner is the opposite of these properties, \"hot – cold\" and \"dry – wet\".\n\nAristotle added a fifth element, aether, as the quintessence, reasoning that whereas fire, earth, air, and water were earthly and corruptible, since no changes had been perceived in the heavenly regions, the stars cannot be made out of any of the four elements but must be made of a different, unchangeable, heavenly substance.\n\nThe Neoplatonic philosopher, Proclus, rejected Aristotle's theory relating the elements to the sensible qualities hot, cold, wet, and dry. He maintained that each of the elements has three properties. Fire is sharp, subtle, and mobile while its opposite, earth, is blunt, dense, and immobile; they are joined by the intermediate elements, air and water, in the following fashion:\n\nIn Bön or ancient Tibetan philosophy, the five elemental processes of earth, water, fire, air and space are the essential materials of all existent phenomena or aggregates. The elemental processes form the basis of the calendar, astrology, medicine, psychology and are the foundation of the spiritual traditions of shamanism, tantra and Dzogchen.\n\nTenzin Wangyal Rinpoche states that\nThe names of the elements are analogous to categorised experiential sensations of the natural world. The names are symbolic and key to their inherent qualities and/or modes of action by analogy. In Bön the elemental processes are fundamental metaphors for working with external, internal and secret energetic forces. All five elemental processes in their essential purity are inherent in the mindstream and link the trikaya and are aspects of primordial energy. As Herbert V. Günther states:\nIn the above block quote the trikaya is encoded as: dharmakaya \"god\"; sambhogakaya \"temple\" and nirmanakaya \"house\".\n\nThe elemental system used in Medieval alchemy was developed primarily by the Persian alchemist Jābir ibn Hayyān (Geber). His system consisted of the four classical elements of air, earth, fire, and water, in addition to two philosophical elements: sulphur, characterizing the principle of combustibility, \"the stone which burns\"; and mercury, characterizing the principle of metallic properties. They were seen by early alchemists as idealized expressions of irreducibile components of the universe and are of larger consideration within philosophical alchemy.\n\nThe three metallic principles—sulphur to flammability or combustion, mercury to volatility and stability, and salt to solidity—became the \"tria prima\" of the Swiss alchemist Paracelsus. He reasoned that Aristotle’s four element theory appeared in bodies as three principles. Paracelsus saw these principles as fundamental and justified them by recourse to the description of how wood burns in fire. Mercury included the cohesive principle, so that when it left in smoke the wood fell apart. Smoke described the volatility (the mercurial principle), the heat-giving flames described flammability (sulphur), and the remnant ash described solidity (salt).\n\nThe Islamic philosophers al-Kindi, Avicenna and Fakhr al-Din al-Razi connected the four elements with the four natures heat and cold (the active force), and dryness and moisture (the recipients).\n\nJapanese traditions use a set of elements called the (\"godai\", literally \"five great\"). These five are earth, water, fire, wind/air, and void. These came from Indian Vastu shastra philosophy and Buddhist beliefs; in addition, the classical Chinese elements (, \"wu xing\") are also prominent in Japanese culture, especially to the influential Neo-Confucianists during the medieval Edo period.\n\n\nWestern astrology uses the four classical elements in connection with astrological charts and horoscopes. The twelve signs of the zodiac are divided into the four elements: Fire signs are Aries, Leo and Sagittarius, Earth signs are Taurus, Virgo and Capricorn, Air signs are Gemini, Libra and Aquarius, and Water signs are Cancer, Scorpio, and Pisces.\n\nThe Aristotelian tradition and medieval Alchemy eventually gave rise to modern scientific theories and new taxonomies. By the time of Antoine Lavoisier, for example, a list of elements would no longer refer to classical elements. Some modern scientists see a parallel between the classical elements and the four states of matter: solid, liquid, gas and weakly ionized plasma.\n\nModern science recognizes classes of elementary particles which have no substructure (or rather, particles that are not made of other particles) and composite particles having substructure (particles made of other particles).\n\n\n\n"}
{"id": "6314", "url": "https://en.wikipedia.org/wiki?curid=6314", "title": "Fire (classical element)", "text": "Fire (classical element)\n\nFire has been an important part of all cultures and religions from pre-history to modern day and was vital to the development of civilization. It has been regarded in many different contexts throughout history, but especially as a metaphysical constant of the world.\n\nFire is one of the four classical elements in ancient Greek philosophy and science. It was commonly associated with the qualities of energy, assertiveness, and passion. In one Greek myth, Prometheus stole \"fire\" from the gods to protect the otherwise helpless humans, but was punished for this charity.\n\nFire was one of many \"archai\" proposed by the Pre-socratics, most of whom sought to reduce the cosmos, or its creation, to a single substance. Heraclitus considered \"fire\" to be the most fundamental of all elements. He believed fire gave rise to the other three elements: \"All things are an interchange for fire, and fire for all things, just like goods for gold and gold for goods.\" He had a reputation for obscure philosophical principles and for speaking in riddles. He described how fire gave rise to the other elements as the: \"upward-downward path\", (), a \"hidden harmony\"  or series of transformations he called the \"turnings of fire\", (), first into \"sea\", and half that \"sea\" into \"earth\", and half that \"earth\" into rarefied \"air\". This is a concept that anticipates both the four classical elements of Empedocles and Aristotle's transmutation of the four elements into one another.\nThis world, which is the same for all, no one of gods or men has made. But it always was and will be: an ever-living fire, with measures of it kindling, and measures going out. \nHeraclitus regarded the soul as being a mixture of fire and water, with fire being the more noble part and water the ignoble aspect. He believed the goal of the soul is to be rid of water and become pure fire: the dry soul is the best and it is worldly pleasures that make the soul \"moist\". He was known as the \"weeping philosopher\" and died of hydropsy, a swelling due to abnormal accumulation of fluid beneath the skin.\n\nHowever, Empedocles of Acragas , is best known for having selected all elements as his \"archai\" and by the time of Plato , the four Empedoclian elements were well established. In the \"Timaeus\", Plato's major cosmological dialogue, the Platonic solid he associated with fire was the tetrahedron which is formed from four triangles and contains the least volume with the greatest surface area. This also makes fire the element with the smallest number of sides, and Plato regarded it as appropriate for the heat of fire, which he felt is sharp and stabbing, (like one of the points of a tetrahedron).\n\nPlato’s student Aristotle did not maintain his former teacher's geometric view of the elements, but rather preferred a somewhat more naturalistic explanation for the elements based on their traditional qualities. Fire the hot and dry element, like the other elements, was an abstract principle and not identical with the normal solids, liquids and combustion phenomena we experience:\n\nAccording to Aristotle, the four elements rise or fall toward their natural place in concentric layers surrounding the center of the earth and form the terrestrial or sublunary spheres.\n\nIn ancient Greek medicine, each of the four humours became associated with an element. Yellow bile was the humor identified with fire, since both were hot and dry. Other things associated with fire and yellow bile in ancient and medieval medicine included the season of summer, since it increased the qualities of heat and aridity; the choleric temperament (of a person dominated by the yellow bile humour); the masculine; and the eastern point of the compass.\n\nIn alchemy the chemical element of sulfur was often associated with fire and its alchemical symbol and its symbol was an upward-pointing triangle. In alchemic tradition, metals are incubated by fire in the womb of the Earth and alchemists only accelerate their development.\n\nAgni is a Hindu and Vedic deity. The word \"agni\" is Sanskrit for fire (noun), cognate with Latin \"ignis\" (the root of English \"ignite\"), Russian \"огонь\" (fire), pronounced \"agon\". Agni has three forms: fire, lightning and the sun.\n\nAgni is one of the most important of the Vedic gods. He is the god of fire and the acceptor of sacrifices. The sacrifices made to Agni go to the deities because Agni is a messenger from and to the other gods. He is ever-young, because the fire is re-lit every day, yet he is also immortal. In Indian tradition Fire is also linked to Surya or the Sun and Mangala or Mars, and with the south-east direction.\n\nFire and the other Greek classical elements were incorporated into the Golden Dawn system. Philosophus (4=7) is the elemental grade attributed to fire; this grade is also attributed to the Qabalistic Sephirah Netzach and the planet Venus. The elemental weapon of fire is the Wand. Each of the elements has several associated spiritual beings. The archangel of fire is Michael, the angel is Aral, the ruler is Seraph, the king is Djin, and the fire elementals (following Paracelsus) are called salamanders. Fire is considered to be active; it is represented by the symbol for Leo and it is referred to the lower right point of the pentacle in the Supreme Invoking Ritual of the Pentacle. Many of these associations have since spread throughout the occult community.\n\nFire in Tarot symbolizes conversion or passion. Many references to fire in tarot are related to the usage of fire in the practice of alchemy, in which the application of fire is a prime method of conversion, and everything that touches fire is changed, often beyond recognition. The symbol of fire was a cue pointing towards transformation, the chemical variant being the symbol delta, which is also the classical symbol for fire. Conversion symbolized can be good, for example, refining raw crudities to gold, as seen in The Devil. Conversion can also be bad, as in The Tower, symbolizing a downfall due to anger. Fire is associated with the suit of rods/wands, and as such, represents passion from inspiration. As an element, fire has mixed symbolism because it represents energy, which can be helpful when controlled, but volatile if left unchecked.\n\nFire is one of the five elements that appear in most Wiccan traditions influenced by the Golden Dawn system of magic, and Aleister Crowley's mysticism, which was in turn inspired by the Golden Dawn.\n\nThe element of fire shows up in mythological stories all across the world, often in stories related to the sun.\n\nIn East Asia fire is represented by the Vermilion Bird, known as 朱雀 (\"Zhū Què\") in Chinese, \"Suzaku\" in Japanese and Ju-jak (주작, Hanja:朱雀) in Korean. \"Fire\" is represented in the Aztec religion by a flint; to the Native Americans, a mouse; to the Hindu and Islamic faiths, a lightning bolt; to the Scythians, an axe, to the Greeks, an apple-bough; and in Christian iconography, lions and ravens.\n\nIn freemasonry, fire is present, for example, during the ceremony of winter solstice, a symbol also of renaissance and energy. Freemasonry takes the ancient symbolic meaning of fire and recognizes its double nature: creation, light, on the one hand, and destruction and purification, on the other.\n\n\n\n"}
{"id": "6315", "url": "https://en.wikipedia.org/wiki?curid=6315", "title": "Air (classical element)", "text": "Air (classical element)\n\nAir is one of the four classical elements in ancient Greek philosophy and in Western alchemy.\n\nAccording to Plato, it is associated with the octahedron; air is considered to be both hot and wet. The ancient Greeks used two words for air: \"aer\" meant the dim lower atmosphere, and \"aether\" meant the bright upper atmosphere above the clouds. Plato, for instance writes that \"So it is with air: there is the brightest variety which we call \"aether\", the muddiest which we call mist and darkness, and other kinds for which we have no name...\" Among the early Greek Pre-Socratic philosophers, Anaximenes (mid-6th century BCE) named air as the \"arche\". A similar belief was attributed by some ancient sources to Diogenes Apolloniates (late 5th century BCE), who also linked air with intelligence and soul (\"psyche\"), but other sources claim that his \"arche\" was a substance between air and fire. Aristophanes parodied such teachings in his play \"The Clouds\" by putting a prayer to air in the mouth of Socrates.\n\nAir was one of many \"archai\" proposed by the Pre-socratics, most of whom tried to reduce all things to a single substance. However, Empedocles of Acragas (c. 495-c. 435 BCE) selected four \"archai\" for his four roots: Air, fire, water, and earth. Ancient and modern opinions differ as to whether he identified air by the divine name Hera, Aidoneus or even Zeus. Empedocles’ roots became the four classical elements of Greek philosophy. Plato (427–347 BCE) took over the four elements of Empedocles. In the \"Timaeus\", his major cosmological dialogue, the Platonic solid associated with air is the octahedron which is formed from eight equilateral triangles. This places air between fire and water which Plato regarded as appropriate because it is intermediate in its mobility, sharpness, and ability to penetrate. He also said of air that its minuscule components are so smooth that one can barely feel them.\n\nPlato's student Aristotle (384–322 BCE) developed a different explanation for the elements based on pairs of qualities. The four elements were arranged concentrically around the center of the universe to form the sublunary sphere. According to Aristotle, air is both hot and wet and occupies a place between fire and water among the elemental spheres. Aristotle definitively separated air from aether. For him, aether was an unchanging, almost divine substance that was found only in the heavens, where it formed celestial spheres.\n\nIn ancient Greek medicine, each of the four humours became associated with an element. Blood was the humor identified with air, since both were hot and wet. Other things associated with air and blood in ancient and medieval medicine included the season of spring, since it increased the qualities of heat and moisture; the sanguine temperament (of a person dominated by the blood humour); hermaphrodite (combining the masculine quality of heat with the feminine quality of moisture); and the northern point of the compass.\n\nThe alchemical symbol for air is an upward-pointing triangle, bisected by a horizontal line.\n\nThe Hermetic Order of the Golden Dawn, founded in 1888, incorporates air and the other Greek classical elements into its teachings. The elemental weapon of air is the dagger which must be painted yellow with magical names and sigils written upon it in violet. Each of the elements has several associated spiritual beings. The archangel of air is Raphael, the angel is Chassan, the ruler is Aral, the king is Paralda, and the air elementals (following Paracelsus) are called sylphs. Air is considerable and it is referred to the upper left point of the pentagram in the Supreme Invoking Ritual of the Pentagram. Many of these associations have since spread throughout the occult community.\n\nIn the Golden Dawn and many other magical systems, each element is associated with one of the cardinal points and is placed under the care of guardian Watchtowers. The Watchtowers derive from the Enochian system of magic founded by Dee. In the Golden Dawn, they are represented by the Enochian elemental tablets. Air is associated with the east, which is guarded by the First Watchtower.\nAir is one of the five elements that appear in most Wiccan and Pagan traditions. Wicca in particular was influenced by the Golden Dawn system of magic and Aleister Crowley's mysticism.\n\nIn Hinduism, \"Vayu\" (Sanskrit वायु ), also known as \"Vāta\" वात, \"Pavana\" पवन (meaning the Purifier), or \"Prāna,\" is a primary deity, who is the father of Bhima and the spiritual father of Lord Hanuman. As the words for air (Vāyu) or wind (Pavana) it is one of the \"Panchamahābhuta\" the \"five great elements\" in Hinduism. \n\nAir is not one of the traditional five Chinese classical elements. Nevertheless, the ancient Chinese concept of \"Qi\" or \"chi\" is believed to be close to that of air. Qi (; spelled \"qì\" in Pinyin romanization and \"ch'i4\" in Wade-Giles) or ki (in Japanese romanization), is a fundamental concept of traditional Chinese culture. Qi is believed to be part of every living thing that exists, as a kind of \"life force\" or \"spiritual energy\". It is frequently translated as \"energy flow\", or literally as \"air\" or \"breath\". (For example, \"tiānqì\", literally \"sky breath\", is the ordinary Chinese word for \"weather\"). In Mandarin Chinese it is pronounced something like \"chee\" in English, but the tongue position is different. (See .) The concept of qi is often reified, however no scientific evidence supports its existence.\n\nThe element air also appears as a concept in the Buddhist philosophy which has an ancient history in China.\n\nSome Western modern occultists equate the Chinese classical element of metal with \"air\", others with wood due to the elemental association of wind and wood in the bagua.\n\nEnlil was the god of air in ancient Sumer. Shu was the ancient Egyptian deity of air and the husband of Tefnut, goddess of moisture. He became an emblem of strength by virtue of his role in separating Nut from Geb. Shu played a primary role in the Coffin Texts, which were spells intended to help the deceased reach the realm of the afterlife safely. On the way to the sky, the spirit had to travel through the air as one spell indicates: \"I have gone up in Shu, I have climbed on the sunbeams.\"\n\n\n\n"}
{"id": "6316", "url": "https://en.wikipedia.org/wiki?curid=6316", "title": "Water (classical element)", "text": "Water (classical element)\n\nWater is one of the elements in ancient Greek philosophy, in the Asian Indian system \"Panchamahabhuta\", and in the Chinese cosmological and physiological system \"Wu Xing\". In contemporary esoteric traditions, it is commonly associated with the qualities of emotion and intuition.\n\nWater was one of many \"archai\" proposed by the Pre-socratics, most of whom tried to reduce all things to a single substance. However, Empedocles of Acragas (c. 495 – c. 435 BC) selected four archai for his four roots: air, fire, water and earth. Empedocles roots became the four classical elements of Greek philosophy. Plato (427–347 BC) took over the four elements of Empedocles. In the Timaeus, his major cosmological dialogue, the Platonic solid associated with water is the icosahedron which is formed from twenty equilateral triangles. This makes water the element with the greatest number of sides, which Plato regarded as appropriate because water flows out of one's hand when picked up, as if it is made of tiny little balls.\n\nPlato’s student Aristotle (384–322 BC) developed a different explanation for the elements based on pairs of qualities. The four elements were arranged concentrically around the center of the Universe to form the sublunary sphere. According to Aristotle, water is both cold and wet and occupies a place between air and earth among the elemental spheres.\n\nIn ancient Greek medicine, each of the four humours became associated with an element. Phlegm was the humor identified with water, since both were cold and wet. Other things associated with water and phlegm in ancient and medieval medicine included the season of Winter, since it increased the qualities of cold and moisture; the phlegmatic temperament, the feminine, the brain and the western point of the compass.\n\nIn alchemy, the chemical element of mercury was often associated with water and its alchemical symbol was a downward-pointing triangle.\n\nAp (') is the Vedic Sanskrit term for water, in Classical Sanskrit occurring only in the plural is not an element.v, ' (sometimes re-analysed as a thematic singular, '), whence Hindi '. The term is from PIE \"hap\" water.\n\nIn Hindu philosophy, the term refers to \nwater as an element, one of the \"Panchamahabhuta,\" or \"five great elements\". In Hinduism, it is also the name of the deva, a personification of water, (one of the Vasus in most later Puranic lists). The element water is also associated with Chandra or the moon and Shukra, who represent feelings, intuition and imagination.\n\nWater and the other Greek classical elements were incorporated into the Golden Dawn system. The elemental weapon of water is the cup. Each of the elements has several associated spiritual beings. The archangel of water is Gabriel, the angel is Taliahad, the ruler is Tharsis, the king is Nichsa and the water elementals are called Ondines. It is referred to the upper right point of the pentagram in the Supreme Invoking Ritual of the Pentagram. Many of these associations have since spread throughout the occult community.\n\nWater is one of the five elements that appear in most Wiccan traditions. Wicca in particular was influenced by the Golden Dawn system of magic and Aleister Crowley's mysticism, which was in turn inspired by the Golden Dawn.\n\n\n"}
{"id": "6317", "url": "https://en.wikipedia.org/wiki?curid=6317", "title": "Earth (classical element)", "text": "Earth (classical element)\n\nEarth is one of the classical elements, in some systems numbering four along with air, fire, and water.\n\nEarth is one of the four classical elements in ancient Greek philosophy and science. It was commonly associated with qualities of heaviness, matter and the terrestrial world. Due to the hero cults, and chthonic underworld deities, the element of \"earth\" is also associated with the sensual aspects of both life and death in later occultism.\n\nEmpedocles of Acragas proposed four \"archai\" by which to understand the cosmos: \"fire\",\" air\", \"water\", and \"earth\". Plato believed the elements were geometric forms (the platonic solids) and he assigned the cube to the element of \"earth\" in his dialogue \"Timaeus\". Aristotle, (384–322 BCE), believed \"earth\" was the heaviest element, and his theory of \"natural place\" suggested that any \"earth–laden\" substances, would fall quickly, straight down, toward the center of the \"cosmos\".\n\nIn Classical Greek and Roman myth, various goddesses represented the Earth, seasons, crops and fertility, including Demeter and Persephone; Ceres; the Horae (goddesses of the seasons), and Proserpina; and Hades (Pluto) who ruled the souls of dead in the Underworld.\n\nIn ancient Greek medicine, each of the four humours became associated with an element. Black bile was the humor identified with earth, since both were cold and dry. Other things associated with earth and black bile in ancient and medieval medicine included the season of fall, since it increased the qualities of cold and aridity; the melancholic temperament (of a person dominated by the black bile humour); the feminine; and the southern point of the compass.\n\nIn alchemy, earth was believed to be primarily dry, and secondarily cold, (as per Aristotle). Beyond those classical attributes, the chemical substance salt, was associated with earth and its alchemical symbol was a downward-pointing triangle, bisected by a horizontal line.\n\nPrithvi (Sanskrit: ', also ') is the Hindu \"earth\" and mother goddess. According to one such tradition, she is the personification of the Earth itself; according to another, its actual mother, being \"Prithvi Tattwa\", the essence of the element earth.\n\nAs \"Prithvi Mata\", or \"Mother Earth\", she contrasts with \"Dyaus Pita\", \"father sky\". In the Rigveda, \"earth\" and sky are frequently addressed as a duality, often indicated by the idea of two complementary \"half-shells.\" In addition, the element Earth is associated with Budha or Mercury who represents communication, business, mathematics and other practical matters.\n\nEarth and the other Greek classical elements were incorporated into the Golden Dawn system. Zelator is the elemental grade attributed to earth; this grade is also attributed to the Qabbalistic sphere Malkuth. The elemental weapon of earth is the Pentacle. Each of the elements has several associated spiritual beings. The archangel of earth is Uriel, the angel is Phorlakh, the ruler is Kerub, the king is Ghob, and the earth elementals (following Paracelsus) are called gnomes. Earth is considered to be passive; it is represented by the symbol for Taurus, and it is referred to the lower left point of the pentagram in the Supreme Invoking Ritual of the Pentagram. Many of these associations have since spread throughout the occult community.\n\nIt is sometimes represented by its Tattva or by a downward pointing triangle with a horizontal line through it.\n\nEarth is one of the five elements that appear in most Wiccan and Pagan traditions. Wicca in particular was influenced by the Golden Dawn system of magic, and Aleister Crowley's mysticism which was in turn inspired by the Golden Dawn.\n\nIn East Asia, metal is sometimes seen as the equivalent of \"earth\" and is represented by the White Tiger (Chinese constellation), known as 白虎 (\"Bái Hǔ\") in Chinese, \"Byakko\" in Japanese, \"Bạch Hổ\" in Vietnamese and \"Baekho\" (백호, Hanja:白虎) in Korean. \"Earth\" is represented in the Aztec religion by a house; to the Hindus, a lotus; to the Scythians, a plough; to the Greeks, a wheel; and in Christian iconography; bulls and birds.\n\n\n"}
{"id": "6319", "url": "https://en.wikipedia.org/wiki?curid=6319", "title": "Blue Jam", "text": "Blue Jam\n\nBlue Jam was an ambient radio comedy programme created and directed by Chris Morris. It aired on BBC Radio 1 in the early hours of the morning from 1997 to 1999.\n\nThe programme gained cult status due to its unique mix of surreal monologue, music, synthesised voices, heavily edited broadcasts and recurring sketches. It featured vocal performances of Kevin Eldon, Julia Davis, Mark Heap, David Cann and Amelia Bullmore, with Morris himself delivering disturbing monologues, one of which was revamped and made into the BAFTA-winning short film \"My Wrongs #8245–8249 & 117\". Writers who contributed to the programme included Graham Linehan, Arthur Mathews, Peter Baynham, David Quantick, Jane Bussmann, Robert Katz and the cast.\n\nThe programme was adapted into the TV series \"Jam\", which aired in 2000. All episodes of \"Blue Jam\" are currently available for streaming and download on the Internet Archive.\n\nOn his inspiration for making the show, Morris commented: \"It was so singular, and it came from a mood, quite a desolate mood. I had this misty, autumnal, boggy mood anyway, so I just went with that\".\n\nEach episode opened (and closed) with a short spoken introduction (delivered by Morris) describing, in surreal, broken language, various bizarre feelings and situations (for example: \"when you sick so sad you cry, and in crying cry a whole leopard from your eye\"), set to ambient music interspersed with short clips of other songs and sounds. The introduction would always end with \"welcome in Blue Jam\", inviting the listener, who is presumably experiencing such feelings, to get lost in the program. (This format was replicated in the television adaptation \"Jam\", often reusing opening monologues from series 3 of the radio series.) The sketches within dealt with heavy and taboo topics, such as murder, suicide, missing or dead children, and rape.\n\n\nThe sketches not listed are typically in the style of a documentary; characters speak as if being interviewed about a recent event. In one sketch, a character voiced by Morris describes a man attempting to commit suicide by jumping off a second-story balcony repeatedly; in another, an angry man shouts about how his car, after being picked up from the garage, is only four feet long.\n\nMorris included a series of 'radio stings', bizarre sequences of sounds and prose as a parody of modern DJs' own soundbites and self-advertising pieces. Each one revolves around a contemporary DJ, such as Chris Moyles, Jo Whiley and Mark Goodier, typically involving each DJ dying in a graphic way or going mad in some form – for example, Chris Moyles covering himself in jam and hanging himself from the top of a building.\n\nThree series were produced, with a total of eighteen episodes. All episodes were originally broadcast weekly on BBC Radio 1. Series 1 was broadcast from 14 November to 19 December 1997; series 2 was broadcast from 27 March to 1 May 1998; and series 3 broadcast from 21 January to 25 February 1999.\n\n\nThe first five episodes of series 1 of \"Blue Jam\" were repeated by BBC Radio 4 Extra in February and March 2014, and series 2 was rebroadcast in December.\n\n\"Blue Jam\" features songs, generally of a downtempo nature, interspersed between (and sometimes during) sketches. Artists featured includes Massive Attack, Air, Morcheeba, The Chemical Brothers, Björk, Aphex Twin, Everything But the Girl and Dimitri from Paris, as well as various non-electronic artists including Sly and the Family Stone, Serge Gainsbourg, The Cardigans and Eels.\n\n\"Blue Jam\" was favourably reviewed on several occasions by \"The Guardian\" and also received a positive review by \"The Independent\".\n\nDigital Spy wrote in 2014: \"It's a heady cocktail that provokes an odd, unsettling reaction in the listener, yet \"Blue Jam\" is still thumpingly and frequently laugh-out-loud hilarious.\" \"Hot Press\" called it \"as odd as comedy gets\".\n\nA CD of a number of \"Blue Jam\" sketches was released on 23 October 2000 by record label Warp. Although the CD claims to have 22 tracks, the last one, \"www.bishopslips.com\", is not a track, but rather a reference to the \"Bishopslips\" sketch, which was cut in the middle of a broadcast. Most of the sketches on the CD were remade for \"Jam\".\n\n\n\n\"Blue Jam\" was later made for television and broadcast on Channel 4 as \"Jam\". It utilised unusual editing techniques to achieve an unnerving ambience in keeping with the radio show. Many of the sketches were lifted from the radio version, even to the extent of simply setting images to the radio soundtrack. A subsequent \"re-mixed\" airing, called \"Jaaaaam\" was even more extreme in its use of post-production gadgetry, often heavily distorting the footage.\n\n\"Blue Jam\" shares parallels with early editions of a US public radio show \"Work in Progress\" from the mid-1980s, that Joe Frank did on the NPR affiliate station, KCRW, in Santa Monica, California.\n\n"}
{"id": "6321", "url": "https://en.wikipedia.org/wiki?curid=6321", "title": "Channel 4", "text": "Channel 4\n\nChannel 4 is a British public-service television broadcaster that began transmission on 2 November 1982. Although largely commercially self-funded, it is ultimately publicly owned; originally a subsidiary of the Independent Broadcasting Authority (IBA), the station is now owned and operated by Channel Four Television Corporation, a public corporation of the Department for Culture, Media & Sport, which was established in 1990 and came into operation in 1993. With the conversion of the Wenvoe transmitter group in Wales to digital on 31 March 2010, Channel 4 became a UK-wide TV channel for the first time.\n\nThe channel was established to provide a fourth television service to the United Kingdom in addition to the licence-funded BBC's two services, BBC One and BBC Two and the single commercial broadcasting network, ITV.\n\nBefore Channel 4 and S4C, Britain had three terrestrial television services: BBC1, BBC2, and ITV. The Broadcasting Act 1980 began the process of adding a fourth, and Channel 4, along with its Welsh counterpart, was formally created by an Act of Parliament in 1982. After some months of test broadcasts, it began scheduled transmissions on 2 November 1982.\n\nThe notion of a second commercial broadcaster in the United Kingdom had been around since the inception of ITV in 1954 and its subsequent launch in 1955; the idea of an \"ITV2\" was long expected and pushed for. Indeed, television sets sold throughout the 1970s and early 1980s had a spare tuning button labelled \"ITV/IBA 2\". Throughout ITV's history and until Channel 4 finally became a reality, a perennial dialogue existed between the GPO, the government, the ITV companies and other interested parties, concerning the form such an expansion of commercial broadcasting would take. It was most likely politics which had the biggest impact in leading to a delay of almost three decades before the second commercial channel became a reality. \n\nOne clear benefit of the \"late arrival\" of the channel was that its frequency allocations at each transmitter had already been arranged in the early 1960s, when the launch of an ITV2 was highly anticipated. This led to very good coverage across most of the country and few problems of interference with other UK-based transmissions; a stark contrast to the problems associated with Channel 5's launch almost 15 years later.\n\nAt the time the fourth service was being considered, a movement in Wales lobbied for the creation of dedicated service that would air Welsh-language programmes, then only catered for at 'off peak' times on BBC Wales and HTV. The campaign was taken so seriously by Gwynfor Evans, former president of Plaid Cymru, that he threatened the government with a hunger strike were it not to honour the plans.\n\nThe result was that Channel 4 as seen by the rest of the United Kingdom would be replaced in Wales by \"Sianel Pedwar Cymru (S4C)\" (\"Channel Four Wales\"). Operated by a specially created authority, S4C would air programmes in Welsh made by HTV, the BBC and independent companies. Initially limited frequency space meant that Channel 4 could not be broadcast alongside S4C, though some Channel 4 programmes would be aired at less popular times on the Welsh variant, a practice that carried on up until the closure of S4C's analogue transmissions in 2010 when S4C became a fully Welsh channel.\n\nSince then, carriage on digital cable, satellite and digital terrestrial has introduced Channel 4 to Welsh homes where it is now universally available.\n\nThe first voice heard on Channel 4's opening day of Tuesday 2 November 1982 was that of continuity announcer Paul Coia who said:\n\nFollowing the announcement, the channel headed into a montage of clips from its programmes set to the station's signature tune, \"Fourscore\", written by David Dundas, which would form the basis of the station's jingles for its first decade. The first programme to air on the channel was the teatime game show \"Countdown\", at 16:45 produced by Yorkshire Television. The first person to be seen on Channel 4 was Richard Whiteley with Ted Moult being the second. The first woman on the channel, contrary to popular belief, was not Whiteley's \"Countdown\" co-host Carol Vorderman but a lexicographer only ever identified as Mary. Whiteley opened the show with the words:\n\nOn its first day, Channel 4 also broadcast controversial soap opera \"Brookside\", which ran until 2003.\n\nOn its launch, Channel 4 committed itself to providing an alternative to the existing channels, an agenda in part set out by its remit which required the provision of programming to minority groups.\n\nIn step with its remit, the channel became well received both by minority groups and the arts and cultural worlds during this period, especially under founding chief executive Jeremy Isaacs, where the channel gained a reputation for programmes on the contemporary arts. Channel 4 co-commissioned Robert Ashley's ground-breaking television opera \"Perfect Lives\", which it premiered over several episodes in 1984. The channel often did not receive mass audiences for much of this period, however, as might be expected for a station focusing on minority interest.\n\nChannel 4 also began the funding of independent films, such as the Merchant-Ivory docudrama \"The Courtesans of Bombay\", during this time.\n\nIn 1992, Channel 4 also faced its first libel case by Jani Allan, a South African journalist, who objected to her representation in Nick Broomfield's documentary \"The Leader, His Driver and the Driver's Wife\".\n\nIn September 1993, the channel broadcast the direct-to-TV documentary film \"Beyond Citizen Kane\", in which it displayed the dominant position of the Rede Globo television network, and discussed its influence, power and political connections in Brazil.\n\nAfter control of the station passed from the Channel Four Television Company to the Channel Four Television Corporation in 1993, a shift in broadcasting style took place. Instead of aiming for the fringes of society, it began to focus on the edges of the mainstream, and the centre of the mass market itself. It began to show many US programmes in peak viewing time, far more than it had previously done. It gave such shows as \"Friends\" and \"ER\" their UK premières.\n\nIn the early 2000s, Channel 4 began broadcasting reality formats such as \"Big Brother\" and obtained the rights to broadcast mass appeal sporting events like cricket and horse racing. This new direction increased ratings and revenues.\n\nIn addition, the corporation launched a number of new television channels through its new 4Ventures offshoot, including Film4, At the Races, E4 and More4.\n\nPartially in reaction to its new 'populist' direction, the Communications Act 2003 directed the channel to demonstrate innovation, experimentation and creativity, appeal to the tastes and interests of a culturally diverse society and to include programmes of an educational nature which exhibit a distinctive character.\n\nOn 31 December 2004, Channel 4 launched a new look and new idents in which the logo is disguised as different objects and the 4 can be seen in an angle.\n\nUnder the leadership of Freeview founder Andy Duncan, 2005 saw a change of direction for Channel 4's digital channels. Channel 4 made E4 free-to-air on digital terrestrial television, and launched a new free-to-air digital channel called More4. By October, Channel 4 had joined the Freeview consortium. By July 2006, Film4 had also become a 'free to air' and restarted broadcasting on digital terrestrial.\n\nVenturing into radio broadcasting, 2005 saw Channel 4 purchase 51% of shares in the now defunct Oneword radio station with UBC Media holding on to the remaining shares. New programmes such as the weekly, half-hour \"The Morning Report\" news programme were among some of the new content Channel 4 provided for the station, with the name 4Radio being used. As of early 2009, however, Channel 4's future involvement in radio remained uncertain.\n\nOn 2 November 2007, the station celebrated its 25th birthday. It showed the first episode of \"Countdown\", an anniversary \"Countdown\" special, as well as a special edition of \"The Big Fat Quiz\" and using the original multicoloured 1982–1996 blocks logo on presentation and idents using the Fourscore jingle throughout the day.\n\nIn November 2009, Channel 4 launched a week of 3D television, broadcasting selected programmes each night using stereoscopic ColorCode 3D technology. The accompanying 3D glasses were distributed through Sainsbury's supermarkets.\n\nOn 29 September 2015, Channel 4 revamped its presentation for a fifth time; the new branding downplays the \"4\" logo from most on-air usage, and instead utilises various variations of the shapes which comprise the logo. The full logo is still occasionally used, but primarily for print advertising. Four new idents, filmed by Jonathan Glazer, are used to introduce programmes and feature various elements of the blocks within them. A fifth ident, in which hundreds of the blocks form what appears to be a clock face, is used to introduce the Channel 4 News. On June 2017, the presentation bumpers were updated slightly to re-introduce the \"4\" logo.\n\nChannel 4 has raised concerns over how it might finance its public service obligations after digital switch-over. However, some certainty came with the announcement in April 2006 that Channel 4's digital switch-over costs would be paid for by licence fee revenues.\n\nOn 28 March 2007, Channel 4 announced plans to launch a music channel \"4Music\" as a joint venture with British media company EMAP which would include carriage on the Freeview platform. On 15 August 2008, 4Music was launched across the UK. Channel 4 has announced interest in launching a high-definition version of Film4 on Freeview, to coincide with the launch of Channel 4 HD, however the fourth HD slot was given to Channel 5 instead. Channel 4 has since acquired a 50% stake in EMAP's TV business for a reported £28 million.\n\nChannel 4 was considered for privatisation by the governments of Margaret Thatcher, John Major and Tony Blair. As of 2016 the future of the channel was again being looked into by the government, with analysts suggesting several options for the channel's future.\n\nIn June 2017, it was announced that Alex Mahon would be the next chief executive, and would take over from David Abraham, when he leaves in October or November 2017.\n\nChannel 4 was established with, and continues to hold, a remit of public service obligations which it must fulfil. The remit changes periodically, as dictated by various broadcasting and communications acts, and is regulated by the various authorities Channel 4 has been answerable to; originally the IBA, then the ITC and now Ofcom.\n\nThe preamble of the remit as per the Communications Act 2003 states that:\n\"The public service remit for Channel 4 is the provision of a broad range of high quality and diverse programming which, in particular:\nThe remit also involves an obligation to provide programming for schools, and a substantial amount of programming produced outside of Greater London.\n\nChannel 4 was carried from its beginning on analogue terrestrial, which was practically the only means of television broadcast in the United Kingdom at the time. It continued to be broadcast through these means until the changeover to digital terrestrial television in the United Kingdom was complete. Since 1998, it has been universally available on digital terrestrial, and the Sky platform (initially encrypted, though encryption was dropped on 14 April 2008 and is now free of charge and available on the Freesat platform) as well as having been available from various times in various areas, on analogue and digital cable networks.\n\nDue to its special status as a public service broadcaster with a specific remit, it is afforded free carriage on the terrestrial platforms, in contrast with other broadcasters such as ITV.\n\nChannel 4 is also seen outside the United Kingdom where it is widely available in Ireland, Switzerland and Belgium. Here viewers receive the channel either on basic cable subscription services or premium services.\n\nChannel 4 Ulster has been available in large parts of Ireland, especially border counties which have been able to receive terrestrial transmissions from Northern Ireland. Channel 4 Ulster has been carried on Irish cable networks since the station went on the air in 1982. From 4 December 2006 Channel 4 was officially available to Sky viewers in Ireland; some programmes, mainly imports, are not aired on this channel variant, due to Channel 4 not owning the relevant broadcast rights within the country. As of the 2015, Channel 4 has an opt-out for the Republic of Ireland featuring Irish advertising, the schedule is the same as the UK channel. \n\nChannel 4 allowed Internet users in the United Kingdom to watch Channel 4 live on the Internet. However some programmes (mostly international imports) were not shown and this service no longer exists. Channel 4 is also provided by Virgin Mobile's DAB mobile TV service which has the same restrictions as the Internet live stream had. Channel 4 is also carried by the Internet TV service TVCatchup and was previously carried by Zattoo until the operator removed the channel from its platform.\n\nChannel 4 also makes some of its programming available \"on demand\" via cable and the Internet through All 4.\n\nDuring the station's formative years, funding came from the ITV companies in return for their right to sell advertisements in their region on the fourth channel.\n\nNowadays it pays for itself in much the same way as most privately run commercial stations, i.e. through the sale of on-air advertising, programme sponsorship, and the sale of any programme content and merchandising rights it owns, such as overseas sales and video sales. For example, as of 2012 its total revenues were £925 million with 91% derived from sale of advertising. It also has the ability to subsidise the main network through any profits made on the corporation's other endeavours, which have in the past included subscription fees from stations such as E4 and Film4 (now no longer subscription services) and its 'video-on-demand' sales. In practice, however, these other activities are loss-making, and are subsidised by the main network. According to Channel 4's last published accounts, for 2005, the extent of this cross-subsidy was some £30 million.\n\nThe change in funding came about under the Broadcasting Act 1990 when the new corporation was afforded the ability to fund itself. Originally this arrangement left a 'safety net' guaranteed minimum income should the revenue fall too low, funded by large insurance payments made to the ITV companies. Such a subsidy was never required, however, and these premiums were phased out by the government in 1998. After the link with ITV was cut, the cross-promotion which had existed between ITV and Channel 4 also ended.\n\nIn 2007 due to severe funding difficulties, the channel sought government help and was granted a payment of £14 million over a six-year period. The money would have come from the television licence fee and would have been the first time that money from the licence fee had been given to any broadcaster other than the BBC. The plan was scrapped by The Secretary of State for Culture, Media and Sport, Andy Burnham, ahead of \"broader decisions about the future framework of public service broadcasting\".\nThe broadcasting regulator Ofcom released its review in January 2009 in which it suggested that Channel 4 would preferably be funded by \"partnerships, joint ventures or mergers\".\n\nChannel 4 is a \"publisher-broadcaster\", meaning that it commissions or \"buys\" all of its programming from companies independent of itself, and was the first broadcaster in the United Kingdom to do so on any significant scale; such commissioning is a stipulation which is included in its licence to broadcast. This had the consequence of starting an industry of production companies that did not have to rely on owning an ITV licence to see their programmes air, though since Channel 4, external commissioning has become regular practice on the numerous stations that have launched since, as well as on the BBC and in ITV (where a quota of 25% minimum of total output has been imposed since the 1990 Broadcasting Act came into force). Although it was the first British broadcaster to commission all of its programmes from third parties, Channel 4 was the last terrestrial broadcaster to outsource its transmission and playout operations (to Red Bee Media), after 25 years in-house.\n\nThe requirement to obtain all content externally is stipulated in its licence. Additionally, Channel 4 also began a trend of owning the copyright and distribution rights of the programmes it aired, in a manner that is similar to the major Hollywood studios' ownership of television programmes that they did not directly produce. Thus, although Channel 4 does not produce programmes, many are seen as belonging to it.\n\nIt was established with a specific intention of providing programming to groups of minority interests, not catered for by its competitors, which at the time were only the BBC and ITV.\n\nChannel 4 also pioneered the concept of \"stranded programming\", where seasons of programmes following a common theme would be aired and promoted together. Some would be very specific, and run for a fixed period of time; the \"4 Mation\" season, for example, showed innovative animation. Other, less specific strands, were (and still are) run regularly, such as \"T4\", a strand of programming aimed at teenagers, on weekend mornings (and weekdays during school/college holidays); \"Friday Night Comedy\", a slot where the channel would pioneer its style of comedy commissions, \"4Music\" (now a separate channel) and \"4Later\", an eclectic collection of offbeat programmes transmitted in the early hours of the morning.\n\nIn its earlier years, \"Red Triangle\" was the name given to the airing of certain risqué art-house films due to the use of a red triangle DOG in the upper right of the screen, dubbed as being pornographic by many of Channel 4's critics, while general broadcasting of films on the station for many years came under the banner of \"Film on Four\" prior to the launch of the \"FilmFour\" brand and station in the late 1990s.\n\nThe following is a list of the 10 most watched shows on Channel 4 since launch, based on Live +7 data supplied by BARB, and archival data published by Channel 4.\nDuring the station's early days, the screenings of innovative short one-off comedy films produced by a rotating line-up of alternative comedians went under the title of \"The Comic Strip Presents\". \"The Tube\" and \"Saturday Live/Friday Night Live\" also launched the careers of a number of comedians and writers. Channel 4 broadcast a number of popular American imports including \"Roseanne\", \"Friends\", \"Sex and the City\", \"South Park\" and \"Will & Grace\". Other significant US acquisitions include \"The Simpsons\", for which the station was reported to have paid £700,000 per episode for the terrestrial television rights.\n\nIn April 2010, Channel 4 became the first UK broadcaster to adapt the American comedy institution of roasting to British television, with \"A Comedy Roast\".\n\nIn 2010, Channel 4 organised \"Channel 4's Comedy Gala\", a comedy benefit show in aid of Great Ormond Street Children's Hospital. With over 25 comedians appearing, it billed it as \"the biggest live stand up show in United Kingdom history\". Filmed live on 30 March in front of 14,000 at The O2 Arena in London, it was broadcast on 5 April. This has continued to 2016.\n\nChannel 4 has a strong reputation for history programmes and real-life documentaries. It has also courted controversy, for example by broadcasting live the first public autopsy in the UK for 170 years, carried out by Gunther von Hagens in 2002, or the 2003 one-off stunt \"Derren Brown Plays Russian Roulette Live\".\n\nIts news service, \"Channel 4 News\", is supplied by ITN whilst its long-standing investigative documentary series, \"Dispatches\", attracts perennial media attention.\n\nFourDocs is an online documentary site provided by Channel 4. It allows viewers to upload their own documentaries to the site for others to view. It focuses on documentaries of between 3 and 5 minutes. The website also includes an archive of classic documentaries, interviews with documentary filmmakers and short educational guides to documentary-making. It won a Peabody Award in 2006. The site also includes a strand for documentaries of under 59 seconds, called 'Microdocs'.\n\nChannel 4 is obliged to carry schools programming as part of its remit and licence.\n\nSince 1957 ITV had produced schools programming, which became an obligation. In 1987, five years after the station was launched, the IBA afforded ITV free carriage of these programmes during Channel 4's then-unused weekday morning hours. This arrangement allowed the ITV companies to fulfil their obligation to provide schools programming, whilst allowing ITV itself to broadcast regular programmes complete with advertisements. During the times in which schools programmes were aired Central Television provided most of the continuity with play-out originating from Birmingham.\n\nAfter the restructuring of the station in 1993, ITV's obligations to provide such programming on Channel 4's airtime passed to Channel 4 itself, and the new service became Channel 4 Schools, with the new corporation administering the service and commissioning its programmes, some still from ITV, others from independent producers.\n\nIn March 2008, the 4Leaning interactive new media commission slabovia.tv was launched. The Slabplayer online media player showing TV shows for teenagers was launched on 26 May 2008.\n\nThe schools programming has always had elements different to its normal presentational package. In 1993, the Channel 4 Schools idents featured famous people in one category, with light shining on them in front of an industrial looking setting supplemented by instrumental calming music. This changed in 1996 with the circles look to numerous children touching the screen, forming circles of information then picked up by other children. The last child would produce the channel 4 logo in the form of three vertical circles, with another in the middle and to the left containing the Channel 4 logo.\n\nA present feature of presentation was a countdown sequence featuring, in 1993 a slide with the programme name, and afterwards an extended sequence matching the channel branding. In 1996, this was an extended ident with timer in top left corner, and in 1999 following the adoption of the squares look, featured a square with timer slowly make its way across the right of the screen with people learning and having fun while doing so passing across the screen. It finished with the Channel 4 logo box on the right of the screen and the name 'Channel 4 Schools' being shown. This was adapted in 2000 when the services name was changed to '4Learning'. In 2001, this was altered to various scenes from classrooms around the world and different parts of school life. The countdown now flips over from the top, right, bottom and left with each second, and ends with four coloured squares, three of which are aligned vertically to the left of the Channel 4 logo, with is contained inside the fourth box. The tag 'Learning' is located directly beneath the logo. The final countdown sequence lasted between 2004 and 2005 and featured a background video of current controversial issues, overlaid with upcoming programming information. the video features people in the style of graffiti enacting the overuse of CCTV cameras, fox hunting, computer viruses and pirate videos, relationships, pollution of the seas and violent lifestyles. Following 2005, no branded section has been used for Schools programmes.\n\nNumerous genres of film-making – such as comedy, drama, documentary, adventure/action, romance and horror/thriller – are represented in the channel's schedule. From the launch of Channel 4 until 1998, film presentations on C4 would often be broadcast under the \"Film on Four\" banner.\n\nIn March 2005, Channel 4 screened the uncut Lars von Trier film \"The Idiots\", which includes unsimulated sexual intercourse, making it the first UK terrestrial channel to do so. The channel had previously screened other films with similar material but censored and with warnings.\n\nSince 1 November 1998, Channel 4 has had a digital subsidiary channel dedicated to the screening of films. This channel launched as a paid subscription channel under the name \"FilmFour\", and was relaunched in July 2006 as a free-to-air channel under the current name of \"Film4\". The Film4 channel carries a wide range of film productions, including acquired and Film4-produced projects. Channel 4's general entertainment channels E4 and More4 also screen feature films at certain points in the schedule as part of their content mix.\n\nA season of television programmes about masturbation, called \"Wank Week\", was to be broadcast in the United Kingdom by Channel 4 in March 2007. The first show was about a Masturbate-a-thon, a public mass masturbation event, organised to raise money for the sexual health charity Marie Stopes International. Another film would have focused on compulsive male masturbators and a third was to feature the sex educator Dr Betty Dodson.\n\nThe series came under public attack from senior television figures, and was pulled amid claims of declining editorial standards and controversy over the channel's public service broadcasting credentials.\n\nOn 8 March 2007 Channel 4 screened the highly controversial documentary \"The Great Global Warming Swindle\". The programme states that global warming is \"a lie\" and \"the biggest scam of modern times\". The programme's accuracy has been disputed on multiple points and several commentators have criticised it for being one-sided, noting that the mainstream position on global warming is supported by the scientific academies of the major industrialised nations\nThere were 246 complaints to Ofcom as of 25 April 2007, including the complaints that the programme falsified data. The programme has been criticised by scientists and scientific organisations and various scientists which participated in the documentary claimed their views had been distorted.\n\n\"Against Nature\": An earlier controversial Channel 4 programme made by Martin Durkin which was also critical of the environmental movement and was charged by the Independent Television Commission of the UK for misrepresenting and distorting the views of interviewees by selective editing.\n\n\"The Greenhouse Conspiracy\": An earlier Channel 4 documentary broadcast on 12 August 1990, as part of the \"Equinox\" series, in which similar claims were made. Three of the people interviewed (Lindzen, Michaels and Spencer) were also interviewed in \"The Great Global Warming Swindle\".\n\nIn the Christmas address of 2008, a Channel 4 tradition since 1993, Iranian President Mahmoud Ahmadinejad made a thinly veiled attack on the United States by claiming that Christ would have been against \"bullying, ill-tempered and expansionist powers\".\n\nA spokeswoman for the FCO said: “President Ahmadinejad has, during his time in office, made a series of appalling anti-Semitic statements. The British media are rightly free to make their own editorial choices, but this invitation will cause offence and bemusement not just at home but among friendly countries abroad.”\n\nOn 15 August 2013, Channel 4 aired a 45-minute documentary on One Direction and their fans dubbed as \"Directioners\". Following the airing, fans across the world complained on social media about the documentary, arguing that this was not a reflection of them.\n\n4Talent is an editorial branch of Channel 4's commissioning wing, which co-ordinates Channel 4's various talent development schemes for film, television, radio, new media and other platforms and provides a showcasing platform for new talent.\n\nThere are bases in London, Birmingham, Glasgow and Belfast, serving editorial hubs known respectively as 4Talent National, 4Talent Central England, 4Talent Scotland and 4Talent Northern Ireland. These four sites include features, profiles and interviews in text, audio and video formats, divided into five zones: TV, Film, Radio, New Media and Extras, which covers other arts such as theatre, music and design. 4Talent also collates networking, showcasing and professional development opportunities, and runs workshops, masterclasses, seminars and showcasing events across the UK.\n\n\"4Talent magazine\" is the creative industries magazine from 4Talent, which launched in 2005 (originally titled TEN4 magazine) under the editorship of Dan Jones. \"4Talent Magazine\" is currently edited by Nick Carson. Other staff include deputy editor Catherine Bray and production editor Helen Byrne. The magazine covers rising and established figures of interest in the creative industries, a remit including film, radio, TV, comedy, music, new media and design.\n\nSubjects are usually UK-based, with contributing editors based in Northern Ireland, Scotland, London and Birmingham, but the publication has been known to source international content from Australia, America, continental Europe and the Middle East. The magazine is frequently organised around a theme for the issue, for instance giving half of November 2007's pages over to profiling winners of the annual 4Talent Awards.\n\nAn unusual feature of the magazine's credits is the equal prominence given to the names of writers, photographers, designers and illustrators, contradicting standard industry practice of more prominent writer bylines. It is also recognisable for its 'wraparound' covers, which use the front and back as a continuous canvas – often produced by guest artists.\n\nAlthough \"4Talent Magazine\" is technically a newsstand title, a significant proportion of its readers are subscribers. It started life as a quarterly 100-page title, but has since doubled in size and is now published bi-annually.\n\nChannel 4 has, since its inception, broadcast identical programmes and continuity throughout the United Kingdom (excluding Wales where it did not operate on analogue transmitters). At launch this made it unique, as both the BBC and ITV had long established traditions of providing regional variations in their programming in different areas of the country. Since the launch of subsequent British television channels, Channel 4 has become typical in its lack of regional programming variations.\n\nA few exceptions exist to this rule for programming and continuity:\n\nSome of Channel 4's schools' programming (1980s/early '90s) were regionalised due to differences in curricula between different regions.\n\nPart of Channel 4's remit covers the commissioning of programmes from outside London. Channel 4 has a dedicated director of nations and regions, Stuart Cosgrove, who is based in a regional office in Glasgow. As his job title suggests, it is his responsibility to foster relations with independent producers based in areas of the United Kingdom (including Wales) outside London.\n\nAdvertising on Channel 4 does contain regular variation: prior to 1993, when ITV was responsible for selling Channel 4's advertising, each regional ITV company would provide the content of advertising breaks, covering the same transmitter area as themselves, and these breaks were often unique to that area. After Channel 4 became responsible for its own advertising, it continued to offer advertisers the ability to target particular audiences and divided its coverage area into six regions: London, South, Midlands, North, Northern Ireland and Scotland.\n\nAt present, Wales does not have its own advertising region, instead its viewers receive the southern region on digital platforms intentionally broadcast to the area, or the neighbouring region where terrestrial transmissions spill over into Wales. The Republic of Ireland shares its advertising region with Northern Ireland (referred to by Channel 4 as the 'Ulster Macro') with many advertisers selling products for Ireland here. E4 has an advertising variant for Ireland, although Northern Ireland receives the UK version of E4. The six regions are also carried on satellite, cable and Digital Terrestrial.\n\nChannel 5 and ITV Breakfast use a similar model to Channel 4 for providing their own advertising regions, despite also having a single national output of programming.\n\nDespite the Republic of Ireland not being in the UK, Channel 4 has a dedicated variant broadcast on Sky Ireland which omits programmes for which broadcast rights are not held in Ireland. For example, the series \"Glee\" is not available on Channel 4 on Sky in Ireland. In recent years a Republic of Ireland advertising opt-out has been added to this version.\n\nWith ITV plc pushing for much looser requirements on the amount of regional news and other programming it is obliged to broadcast in its ITV regions, the idea of Channel 4 taking on a regional news commitment has been considered, with the corporation in talks with Ofcom and ITV over the matter. Channel 4 believe that a scaling-back of such operations on ITV's part would be detrimental to Channel 4's national news operation, which shares much of its resources with ITV through their shared news contractor ITN. At the same time, Channel 4 also believe that such an additional public service commitment would bode well in on-going negotiations with Ofcom in securing additional funding for its other public service commitments.\n\nIn mid-2006 Channel 4 ran a six-month closed trial of HDTV, as part of the wider Freeview HD experiment via the Crystal Palace transmitter to London and parts of the home counties, including the use of \"Lost\" and \"Desperate Housewives\" as part of the experiment, as US broadcasters such as ABC already have an HDTV back catalogue.\n\nOn 10 December 2007, Channel 4 launched a high definition television simulcast of Channel 4 on Sky's digital satellite platform, after Sky agreed to contribute toward the channel's satellite distribution costs. It was the first full-time high definition channel from a terrestrial UK broadcaster.\n\nOn 31 July 2009, Virgin Media added Channel 4 HD on channel 146 (later on channel 142, now on channel 141) as a part of the M pack. On 25 March 2010 Channel 4 HD appeared on Freeview channel 52 with a placeholding caption, ahead of a commercial launch on 30 March 2010, coinciding with the commercial launch of Freeview HD. On 19 April 2011, Channel 4 HD was added to Freesat on channel 126. As a consequence, the channel moved from being free-to-view to free-to-air on satellite during March 2011. With the closure of S4C Clirlun in Wales on 1 December 2012, on Freeview, Channel 4 HD launched in Wales on 2 December 2012.\n\nThe channel carries the same schedule as Channel 4, broadcasting programmes in HD when available, acting as a simulcast. Therefore, SD programming is broadcast upscaled to HD. The first true HD programme to be shown was the 1996 Adam Sandler film Happy Gilmore. From launch until 2016 the presence of the 4HD logo on screen denoted true HD content.\n\nOn 1 July 2014, Channel 4 +1 HD, a timeshift of Channel 4 HD, launched on Freeview channel 110.\n\nAll 4 is a video on demand service from Channel 4, launched in November 2006 as 4oD. The service offers a variety of programmes recently shown on Channel 4, E4, More4 or from their archives, though some programmes and movies are not available due to rights issues.\n\nChannel 4 originally licensed an ancillary teletext service to provide schedules, programme information and features. The original service was called 4-Tel, and was produced by Intelfax, a company set up especially for the purpose. It was carried in the 400s on Oracle. In 1993, with Oracle losing its franchise to Teletext Ltd, 4-Tel found a new home in the 300s, and had its name shown in the header row. Intelfax continued to produce the service and in 2002 it was renamed FourText.\n\nIn 2003, Channel 4 awarded Teletext Ltd a ten-year contract to run the channel's ancillary teletext service, named Teletext on 4. This has now ceased and Teletext is no longer available on Channel 4, ITV and Channel 5.\n\n"}
{"id": "6322", "url": "https://en.wikipedia.org/wiki?curid=6322", "title": "Carolina parakeet", "text": "Carolina parakeet\n\nThe Carolina parakeet (\"Conuropsis carolinensis\") or Carolina conure was a small green neotropical parrot with a bright yellow head, reddish orange face and pale beak native to the eastern, midwest and plains states of the United States and was the only indigenous parrot within its range, as well as one of only two parrots native to the United States (the other being the thick-billed parrot). It was found from southern New York and Wisconsin to Kentucky, Tennessee and the Gulf of Mexico, from the Atlantic seaboard to as far west as eastern Colorado. It lived in old-growth forests along rivers and in swamps. It was called \"puzzi la née\" (\"head of yellow\") or \"pot pot chee\" by the Seminole and \"kelinky\" in Chickasaw. Though formerly prevalent within its range, the bird had become rare by the middle of the 19th century. The last confirmed sighting in the wild was of the \"ludovicianus\" subspecies in 1910. The last known specimen perished in captivity at the Cincinnati Zoo in 1918 and the species was declared extinct in 1939.\n\nThe earliest reference to these parrots was in 1583 in Florida reported by Sir George Peckham in \"A True Report of the Late Discoveries of the Newfound Lands\" of expeditions conducted by English explorer Sir Humphrey Gilbert who notes that explorers in North America \"doe testifie that they have found in those countryes; ... parrots.\" They were first scientifically described in English naturalist Mark Catesby's two volume \"Natural History of Carolina, Florida and the Bahama Islands\" published in London in 1731 and 1743.\n\nCarolina parakeets were probably poisonous—American naturalist and painter John J. Audubon noted that cats apparently died from eating them, and they are known to have eaten the toxic seeds of cockleburs.\n\n\"Carolinensis\" is a species of the genus \"Conuropsis\", one of numerous genera of New World long-tailed parrots in tribe Arini, which also includes the Central and South American macaws. Tribe Arini together with the Amazonian parrots and a few miscellaneous genera make up subfamily Arinae of Neotropical parrots in family Psittacidae of true parrots.\n\nThe specific name \"Psittacus carolinensis\" was assigned by Swedish zoologist Carl Linnaeus in the 10th edition of Systema Naturae published in 1758. The species was given its own genus \"Conuropsis\" by Italian zoologist and ornithologist Tommaso Salvadori in 1891 in his \"Catalogue of the Birds in the British Museum\", volume 20. The name is derived from the Greek-ified \"conure\" (\"parrot of the genus \"Conurus\"\" an obsolete name of genus \"Aratinga\") + \"-opsis\" (\"likeness of\") and Latinized \"Carolina\" (from Carolana, an English colonial province) + \"-ensis\" (of or \"from a place\"), therefore a bird \"like a conure from Carolina\".\n\nThere are two recognized subspecies. The Louisiana subspecies of the Carolina parakeet, \"C. c. ludovicianus\", was slightly different in color than the nominate subspecies, being more bluish-green and generally of a somewhat subdued coloration, and became extinct in much the same way, but at a somewhat earlier date (early 1910s). The Appalachian Mountains separated these birds from the eastern \"C. c. carolinensis\".\n\nAccording to a study of mitochondrial DNA recovered from museum specimens, their closest living relatives include some of the South American \"Aratinga\" parakeets: the Nanday parakeet, the sun parakeet, and the golden-capped parakeet. The authors note the bright yellow and orange plumage and blue wing feathers found in \"Conuropsis carolinensis\" are traits shared by another species, the jandaya parakeet (\"A. jandaya\"), that was not sampled in the study but is generally thought to be closely related. Carolinensis is in a sister clade to that of Spix's macaw. The Carolina parakeet colonized North America about 5.5 million years ago. This was well before North America and South America were joined together by the formation of the Panama land bridge about 3.5 mya. Since the Carolina parakeets' more distant relations are geographically closer to its own historic range whilst its closest relatives are more geographically distant to it, these data are consistent with the generally accepted hypothesis that Central and North America were colonized at different times by distinct lineages of parrots – parrots that originally invaded South America from Antarctica some time after the breakup of Gondwana, where Neotropical parrots originated approximately 50 mya.\n\nThe following cladogram shows the placement of the Carolina parakeet among its closest relatives, after a DNA study by Kirchman et al., 2012:\nA fossil parrot, designated \"Conuropsis fratercula\", was described based on a single humerus from the Miocene Sheep Creek Formation (possibly late Hemingfordian, c. 16 mya, possibly later) of Snake River, Nebraska. This was a smaller bird, three-quarters the size of the Carolina parakeet. \"The present \"species\" is of peculiar interest as it represents the first known parrotlike bird to be described as a fossil from North America.\" (Wetmore 1926; italics added) However, it is not altogether certain that this species is correctly assigned to \"Conuropsis\", but some authors consider it a paleosubspecies of the Carolina parakeet.\n\nThe Carolina parakeet was a small green parrot very similar in size and coloration to the extant jenday and sun conures. The majority of the plumage was green with lighter green underparts, a bright yellow head and orange forehead and face extending to behind the eyes and upper cheeks (lores). The shoulders were yellow, continuing down the outer edge of the wings. The primary feathers were mostly green, but with yellow edges on the outer primaries. Thighs were green towards the top and yellow towards the feet. Male and female adults were identical in plumage, however males were slightly larger than females (sexually dimorphic). The legs and feet were light brown. They share the zygodactyl feet of the parrot family. The skin around the eyes was white and the beak was pale flesh colored. These birds weigh about 3.5 oz., are 13 in. long, and have wingspans of 2123 in.\n\nYoung Carolina parakeets differed slightly in coloration from adults. The face and entire body was green, with paler underparts. They lacked yellow or orange plumage on the face, wings, and thighs. Hatchlings were covered in mouse-gray down, until about 39–40 days when green wings and tails appear. Fledglings had full adult plumage at around 1 year of age. (\"Nature Serve, Conuropsis carolinensis\", 2005; Fuller, 2001; Mauler, 2001; Rising, 2004; Snyder and Russell, 2002)\n\nThese birds were fairly long lived, at least in captivity - a pair was kept at the Cincinnati Zoo for over 35 years.\n\nThe Carolina parakeet had the northern-most range of any known parrot. It was found from southern New England and New York and Wisconsin to Kentucky, Tennessee and the Gulf of Mexico. It has also had a wide distribution west of the Mississippi River, as far west as eastern Colorado. Its range was described by early explorers thus: the 43rd parallel as the northern limit, the 26th as the most southern, the 73rd and 106th meridians as the eastern and western boundaries respectively, the range included all or portions of at least 28 states. Its habitats were old-growth wetland forests along rivers and in swamps especially in the Mississippi-Missouri drainage basin with large hollow trees including cypress and sycamore to use as roosting and nesting sites.\n\nOnly very rough estimates of the birds' former prevalence can be made: with an estimated range of 20,000 to 2.5 million km, and population density of 0.5 to 2.0 parrots per km, population estimates range from tens of thousands to a few million birds (though the densest populations occurred in Florida covering 170,000 km, so there may have been hundreds of thousands of the birds in that state alone).\n\nThe species may have appeared as a very rare vagrant in places as far north as Southern Ontario. A few bones, including a pygostyle found at the Calvert Site in Southern Ontario, came from the Carolina parakeet. The possibility remains open that this specimen was taken to Southern Ontario for ceremonial purposes.\n\nThe bird lived in huge, noisy flocks of as many as 200–300 birds. It built its nest in a hollow tree, laying two to five (most accounts say two) round white eggs.\n\nIt mostly ate the seeds of forest trees and shrubs including those of cypress, hackberry, beech, sycamore, elm, pine, maple, oak, and other plants such as thistles and sandspurs (\"Cenchrus\" species). It also ate fruits, including apples, grapes and figs (often from orchards by the time of its decline). It was especially noted for its predilection for cockleburs (\"Xanthium strumarium\"), a plant which contains a toxic glucoside, and it was considered to be an agricultural pest of grain crops.\n\nIncas, the last captive Carolina parakeet died at the Cincinnati Zoo on February 21, 1918 in the same cage as Martha, the last passenger pigeon, which died in 1914. There are no scientific studies or surveys of this bird by American naturalists; most information about it is from anecdotal accounts and museum specimens. Therefore, details of its prevalence and decline are unverified or speculative.\n\nThere are extensive accounts of the pre-colonial and early colonial prevalence of this bird. The existence of flocks of gregarious, very colorful and raucous parrots could hardly have gone unnoted by European explorers, as parrots were virtually unknown in seafaring European nations in the 16th and 17th centuries. Later accounts in the latter half of the 19th century onward noted the birds' sparseness and absence.\n\nThe birds' range collapsed from east to west with settlement and clearing of the eastern and southern deciduous forests. John J. Audubon commented as early as 1832 on the decline of the birds. The bird was rarely reported outside Florida after 1860. The last reported sighting east of the Mississippi River (except Florida) was in 1878 in Kentucky. By the turn of the century it was restricted to the swamps of central Florida. The last known wild specimen was killed in Okeechobee County, Florida, in 1904, and the last captive bird died at the Cincinnati Zoo on February 21, 1918. This was the male specimen, called \"Incas\", who died within a year of his mate, \"Lady Jane\". Additional reports of the bird were made in Okeechobee County, Florida, until the late 1920s, but these are not supported by specimens. It was not until 1939, however, that the American Ornithologists' Union declared that the Carolina parakeet had become extinct. The IUCN has listed the species as extinct since 1920.\nIn 1937, three parakeets resembling this species were sighted and filmed in the Okefenokee Swamp of Georgia. However, the American Ornithologists' Union analyzed the film and concluded that they had probably filmed feral parakeets. A year later, in 1938, a flock of parakeets was apparently sighted by a group of experienced ornithologists in the swamps of the Santee River basin in South Carolina. However, this sighting was doubted by most other ornithologists. The birds were never seen again after this sighting, and shortly after a portion of the area was destroyed to make way for power lines, making the species' continued existence unlikely.\n\nAbout 720 skins and 16 skeletons are housed in museums around the world and analyzable DNA has been extracted from them.\n\nThe evidence is conclusive that humans caused the extinction of the Carolina parakeet, through a variety of means. Chief was deforestation in the 18th and 19th centuries. Hunting played a significant role, both for decorative use of their colorful feathers, for example adornment of women's hats, and for reduction of crop predation. This was partially offset by recognition of their value in controlling invasive cockleburs. Minor roles were played by capture for the pet trade and, it was hypothesized, by the introduction for crop pollination of European honeybees that competed for nest sites.\n\nA factor that exacerbated their decline to extinction was the flocking behavior that led them to return to the vicinity of dead and dying birds (e.g., birds downed by hunting), enabling wholesale slaughter.\n\nThe final extinction of the species in the early years of the 20th century is somewhat of a mystery, as it happened so rapidly. Vigorous flocks with many juveniles and reproducing pairs were noted as late as 1896, and the birds were long-lived in captivity, but they had virtually disappeared by 1904. Sufficient nest sites remained intact, so deforestation was not the final cause. American ornithologist Noel F. Snyder speculates that the most likely cause seems to be that the birds succumbed to poultry disease, although no recent or historical records exist of New World parrot populations being afflicted by domestic poultry diseases. The modern poultry scourge Newcastle disease was not detected until 1926 in Indonesia, and only a subacute form of it was reported in the United States in 1938.\n\n\n\n"}
{"id": "6324", "url": "https://en.wikipedia.org/wiki?curid=6324", "title": "Collective trauma", "text": "Collective trauma\n\nA collective trauma is a traumatic psychological effect shared by a group of people of any size, up to and including an entire society. Traumatic events witnessed by an entire society can stir up collective sentiment, often resulting in a shift in that society's culture and mass actions.\n\nWell known collective traumas include: The Holocaust, the Armenian Genocide, Slavery in the United States, the Atomic bombings of Hiroshima and Nagasaki, the Trail of Tears, the Assassination of John F. Kennedy in the United States, the MS Estonia in Sweden, the September 11, 2001 attacks in the United States, and various others.\n\nCollective traumas have been shown to play a key role in group identity formation (see: Law of Common Fate). During World War II, a US submarine, the USS Puffer (SS-268), came under several hours of depth charge attack by a Japanese surface vessel until the ship became convinced the submarine had somehow escaped. Psychological studies later showed that crewmen transferred to the submarine after the event were never accepted as part of the team. Later, US naval policy was changed so that after events of such psychological trauma, the crew would be dispersed to new assignments.\n\nRehabilitation of survivors becomes extremely difficult when entire nation has experienced such severe traumas as war, genocide, torture, massacre, etc. Treatment is hardly effective when everybody is traumatized. Trauma remains chronic and would reproduce itself as long as social causes are not addressed and perpetrators continue to enjoy impunity. The whole society may suffer from an everlasting culture of pain. (1)\n\nDuring liberation war in Algeria, the Algerian Psychiatrist Frantz Omar Fanon found his practice of treatment of native Algerians ineffective due to the continuation of the horror of a colonial war. He emphasized about the social origin of traumas, joined the liberation movement and urged oppressed people to purge themselves of their degrading traumas through their collective liberation struggle. He made the following remarks in his letter of resignation, as the Head of the Psychiatry Department at the Blida-Joinville Hospital in Algeria:\"If psychiatry is the medical technique that aims to enable man no longer to be a stranger to his environment, I owe it to myself to affirm that the Arab, permanently an alien in his own country, lives in a state of absolute depersonalization.\" (2) Inculcation of horror and anxiety, through widespread torture, massacre, genocide and similar coercive measures has happened frequently in human history. There are plenty of examples in our modern history. Tyrants have always used their technique of \"psychological artillery\" in an attempt to cause havoc and confusion in the minds of people and hypnotize them with intimidation and cynicism. The result is a collective trauma that will pass through generations. There is no magic formula of rehabilitation. Collective trauma can be alleviated through cohesive and collective efforts such as recognition, remembrance, solidarity, communal therapy and massive cooperation.\n\n\n"}
{"id": "6325", "url": "https://en.wikipedia.org/wiki?curid=6325", "title": "Church (building)", "text": "Church (building)\n\nA church building, often simply called a church, is a building used for Christian religious activities, particularly for worship services. The term in its architectural sense is most often used by Christians to refer to their religious buildings, but it is sometimes used (by analogy) to refer to buildings of other religions. In traditional Christian architecture, the church is often arranged in the shape of a Christian cross. When viewed from plan view the longest part of a cross is represented by the aisle and the junction of the cross is located at the altar area.\n\nTowers or domes are often added with the intention of directing the eye of the viewer towards the heavens and inspiring church visitors. Modern church buildings have a variety of architectural styles and layouts; many buildings that were designed for other purposes have now been converted for church use; and, similarly, many original church buildings have been put to other uses.\n\nThe earliest identified Christian church was a house church founded between 233 and 256. From the 11th through the 14th centuries, a wave of building of cathedrals and smaller parish churches occurred across Western Europe. A cathedral is a church, usually Roman Catholic, Anglican, Oriental Orthodox or Eastern Orthodox, housing the seat of a bishop.\n\nIn Greek, the adjective \"kyriak-ós/-ē/-ón\" means \"belonging, or pertaining, to a \"Kyrios\"\" (\"Lord\"), and the usage was adopted by early Christians of the Eastern Mediterranean with regard to anything pertaining to the Lord Jesus Christ: hence \"\"Kyriakós oíkos\"\" (\"house of the Lord\", church), \"\"Kyriakē\"\" (\"[the day] of the Lord\", i.e. Sunday), or \"\"Kyriakē proseukhē\"\" (the \"Lord's Prayer\").\n\nIn standard Greek usage, the older word \"ecclesia\" (ἐκκλησία, \"ekklesía\", literally \"assembly\", \"congregation\", or the place where such a gathering occurs) was retained to signify both a specific edifice of Christian worship (a \"church\"), and the overall community of the faithful (the \"Church\"). This usage was also retained in Latin and the languages derived from Latin (e.g. French \"église\", Italian \"chiesa\", Spanish \"iglesia\", Portuguese \"igreja\", etc.), as well as in the Celtic languages (Welsh \"eglwys\", Irish \"eaglais\", Breton \"iliz\", etc.) and in Turkish (\"kilise\").\n\nIn the Germanic and some Slavic languages, the word \"kyriak-ós/-ē/-ón\" was adopted instead and derivatives formed thereof. In Old English the sequence of derivation started as \"cirice\" (Ki-ri-keh), then \"churche\" (kerke), and eventually \"church\" in its current pronunciation. German \"Kirche\", Scots \"kirk\", Russian церковь (\"tserkov\"), etc., are all similarly derived.\n\nAccording to the New Testament, the earliest Christians did not build church buildings. Instead, they gathered in homes (Acts 17:5, 20:20, 1 Corinthians 16:19) or in Jewish worship places like the Second Temple or synagogues (Acts 2:46, 19:8). The earliest archeologically identified Christian church is a house church (\"domus ecclesiae\"), the Dura-Europos church, founded between 233 and 256. In the second half of the 3rd century CE, the first purpose-built halls for Christian worship (\"aula ecclesiae\") began to be constructed. Although many of these were destroyed early in the next century during the Diocletianic Persecution, even larger and more elaborate church buildings began to appear during the reign of the Emperor Constantine the Great.\n\nFrom the 11th through the 14th centuries, a wave of building of cathedrals and smaller parish churches occurred across Western Europe. In addition to being a place of worship, the cathedral or the parish church was used by the community in other ways. It could serve as a meeting place for guilds or a hall for banquets. Mystery plays were sometimes performed in cathedrals, and cathedrals might also be used for fairs. The church could be used as a place to thresh and store grain.\n\nBetween 1000 and 1200 AD the romanesque style became popular across Europe. While the name of the romanesque era refers to the tradition of roman architecture, it was actually a West- and Central European trend. Romanesque buildings appear rather bulky and compact. Typical features are circular arches, round or octagonal towers and cushion capitals on the pillars. In the early romanesque era, coffering on the ceiling was fashionable, while later in the same era, groined vault was more popular. The rooms became wider and the motivs of sculptures became more epic.\n\nThe Gothic style emerged around 1140 in Île-de-France and spread through all of Europe. The gothic buildings were less compact than they had been in the romanesque era and often contained symbolic and allegoric features. For the first time, pointed arches, rib vaults and and buttresses were used, with the result that massive walls were not longer needed to stabilise the building. Due to that advantage, the area of the windows became bigger, which resulted in a brighter and more friendly atmosphere inside the church. The nave became higher and so did the pillars and the church steeple. The amibition to test out the limits of the architectural possibilities resulted in the collapse of several towers. In Germany and the Netherlands, but also in Spain, it became popular to build hall churches, in which every vault has the same height. \n\nCathedrals were build in a very lavish way, as in the romanesque era. Examples for that are the Notre-Dame de Paris and the Notre-Dame de Reims in France, but also the San Francesco d’Assisi in Palermo, the Salisbury Cathedral and the Wool Church in Lavenham, England. \n\nMany gothic churches contain features from the romanesque era. Some of the most well-known gothic churches stayed unfinished for hundreds of years, after the gothic style was not popular anymore. About half of the Cologne Cathedral was for example build in the 19th century. \n\nIn the 15th and 16th century, the change in ethics and society due to the renaissance and the reformation also influenced the building of churches. The common style was much like the gothic style, but in a simplified way. The basilica was not the most popular type of church anymore, but instead hall churches were build. Typical features are columns and classical capitals. \n\nIn protestant churches, were the proclamation of God's Word is of special importance, the visitor's line of view is directed towards the pulpit. \n\nThe baroque style was first used in Italy around 1575. From there it spread to the rest of Europe and to the european colonies. During the baroque era, the building industry increased heavily. Buildings, even churches, were used as indicators for wealth, authority and influence.The use of forms known from the renaissance were extremely exaggerated. Domes and capitals were decorated with moulding and the former stucco-sculptures were replaced by fresco paintings on the ceilings. For the first time, churches were seen as one connected work of art and consistent artistic concepts were developed. Instead of long buildings, more central-plan buildings were created. The sprawling decoration with floral ornamentation and mythological motives raised until about 1720 to the rococo era. \n\nThe protestant parishes perfered lateral churches, in which all the visitors coud be as close as possible to the pulpit and the altar.\n\nA common architecture for churches is the shape of a cross (a long central rectangle, with side rectangles, and a rectangle in front for the altar space or sanctuary). These churches also often have a dome or other large vaulted space in the interior to represent or draw attention to the heavens. Other common shapes for churches include a circle, to represent eternity, or an octagon or similar star shape, to represent the church's bringing light to the world. Another common feature is the spire, a tall tower on the \"west\" end of the church or over the crossing.\n\nAnother common feature of many christian churches is the eastwards orientation of the front altar. Unlike the jewish synagogues, which usually point in direction of the city Jerusalem, christian churches point in the direction of the rising sun. The sun is used as a symbol for Jesus Christ, who describes himself as \"light of the world\" in the New Testament of the Bible, .\n\n\"I am the light of the world. Whoever follows me will never walk in darkness, but will have the light of life.\"\n\nThe Latin word basilica (derived from Greek, \"Basiliké Stoà\", Royal \"Stoa\") was originally used to describe a Roman public building (as in Greece, mainly a tribunal), usually located in the forum of a Roman town.\n\nAfter the Roman Empire became officially Christian, the term came by extension to refer to a large and important church that has been given special ceremonial rights by the Pope. Thus the word retains two senses today, one architectural and the other ecclesiastical.\n\nA cathedral is a church, usually Roman Catholic, Anglican, Oriental Orthodox or Eastern Orthodox, housing the seat of a bishop. The word cathedral takes its name from \"cathedra\", or Bishop's Throne (In ). The term is sometimes (improperly) used to refer to any church of great size.\n\nA church that has the function of cathedral is not necessarily a large building. It might be as small as Christ Church Cathedral in Oxford, England, Sacred Heart Cathedral in Raleigh, United States, or Chur Cathedral in Switzerland. However, frequently, the cathedral along with some of the abbey churches, was the largest building in any region.\n\nOld and disused church buildings can be seen as an interesting proposition for developers as the architecture and location often provide for attractive homes or city centre entertainment venues On the other hand, many newer churches have decided to host meetings in public buildings such as schools, universities, cinemas or theatres.\n\nThere is another trend to convert old buildings for worship rather than face the construction costs and planning difficulties of a new build. Unusual venues in the UK include an old Tram power station, a former bus garage, an old cinema and bingo hall, a former Territorial Army Drill Hall, and a former synagogue. A windmill has also been converted into a church at Reigate Heath.\nThere has been an increase in partnerships between church management and private real estate companies to redevelop church properties into mixed uses. While it has garnered criticism from some, the partnership offers congregations the opportunity to increase revenue while preserving the property.\n\n\n\n"}
{"id": "6326", "url": "https://en.wikipedia.org/wiki?curid=6326", "title": "Childe's Tomb", "text": "Childe's Tomb\n\nChilde's Tomb is a granite cross on Dartmoor, Devon, England. Although not in its original form, it is more elaborate than most of the crosses on Dartmoor, being raised upon a constructed base, and it is known that a kistvaen is underneath.\n\nA well-known legend attached to the site, first recorded in 1630 by Tristram Risdon, concerns a wealthy hunter, Childe, who became lost in a snow storm and supposedly died there despite disembowelling his horse and climbing into its body for protection. The legend relates that Childe left a note of some sort saying that whoever found and buried his body would inherit his lands at Plymstock. After a race between the monks of Tavistock Abbey and the men of Plymstock, the Abbey won.\n\nThe tomb was virtually destroyed in 1812 by a man who stole most of the stones to build a house nearby, but it was partly reconstructed in 1890.\n\nChilde's Tomb is a reconstructed granite cross on the south-east edge of Foxtor Mires, about 500 metres north of Fox Tor on Dartmoor, Devon, England at . According to William Burt, in his notes to \"Dartmoor, a Descriptive Poem\" by N. T. Carrington (1826), the original tomb consisted of a pedestal of three steps, the lowest of which was built of four stones each six feet long and twelve inches square. The two upper steps were made of eight shorter but similarly shaped stones, and on top was an octagonal block about three feet high with a cross fixed upon it.\n\nThe tomb lies on the line of several cairns that marked the east-west route of the ancient Monks' Path between Buckfast Abbey and Tavistock Abbey and it was no doubt erected here as part of that route: it would have been particularly useful in this part of the moor with few landmarks where a traveller straying from the path could easily end up in Foxtor Mires. Tristram Risdon, writing in about 1630, said that Childe's Tomb was one of three remarkable things in the Forest of Dartmoor (the others being Crockern Tor and Wistman's Wood). Risdon also stated that the original tomb bore an inscription: \"They fyrste that fyndes and bringes mee to my grave, The priorie of Plimstoke they shall have\", but no sign of this has ever been found.\n\nToday the cross, which is a replacement, is about tall and across at the crosspiece, and it has its base in a socket stone which rests on a pedestal of granite blocks that raises the total height of the cross to . The original, now broken, socket stone for the cross lies nearby. The whole is surrounded by a circle of granite stones set on their edge which once surrounded the cairn—the rocks of which are now scattered around—that was originally built over a large kistvaen that still exists beneath the pedestal.\n\nIn the early 19th century there was much interest in enclosing and \"improving\" the open moorland on Dartmoor, encouraged by Sir Thomas Tyrwhitt's early successes at Tor Royal near Princetown. Enclosure was aided by the greatly enhanced access provided by the construction of the first turnpike roads over the moor: the road between Ashburton and Two Bridges opened in around 1800, for instance. In February 1809 one Thomas Windeatt, from Bridgetown, Totnes, took over the lease of a plot of land (a \"newtake\") of about 582 acres in the valley of the River Swincombe. In 1812 Windeatt started to build a farmhouse, Fox Tor Farm, on his land and his workmen robbed the nearby Childe's Tomb of most of its stones for the building and its doorsteps.\n\nIn 1902 William Crossing wrote that he had been told by an old moorman that some of the granite blocks from the tomb's pedestal had also been used to make a clapper bridge across a stream flowing into the River Swincombe near the farm. The moorman also said that they had lettering on their undersides. This encouraged Crossing to arrange to lift the clapper bridge, but no inscription was found. However, he did locate nine out of the twelve stones that had made up the pedestal, as well as the broken socket stone for the cross.\n\nCrossing rediscovered the original site of the tomb in 1882 and said that all that remained was a small mound and some half buried stones. He cleared out the kistvaen, reporting that it was long by wide and that unlike most kistvaens found on the moor, the stones lining it had apparently been shaped by man, which led him to suggest that it was less old than most. Having located most of the stones of the original tomb, Crossing thought that it could be rebuilt in its original form with little effort, but it was not to be.\n\nJ. Brooking Rowe, writing in 1895, states that the tomb was re-erected in 1890 under the direction of Mr. E. Fearnley Tanner, who said that he was dissatisfied with the result because several stones were missing and it was difficult to recreate the original character of the monument. Tanner was the honourable secretary of the Dartmoor Preservation Association, and this reconstruction was one of the first acts of that organisation. The replacement base and cross were made in Holne in 1885.\n\nAccording to legend, the cross was erected over the kistvaen (burial chamber) of Childe the Hunter, who was Ordulf, son of Ordgar, an Anglo-Saxon Earl of Devon in the 11th century. The name \"Childe\" is probably derived from the Old English word \"cild\" which was used as a title of honour.\n\nLegend has it that Childe was in a party hunting on the moor when they were caught in some changeable weather. Childe became separated from the main party and was lost. In order to save himself from dying of exposure, he killed his horse, disembowelled it and crept inside the warm carcass for shelter. He nevertheless froze to death, but before he died, he wrote a note to the effect that whoever should find him and bury him in their church should inherit his Plymstock estate.\n\nHis body was found by the monks of Tavistock Abbey, who started to carry it back. However, they heard of a plot to ambush them by the people of Plymstock, at a bridge over the River Tavy. They took a detour and built a new bridge over the river, just outside Tavistock. They were successful in burying the body in the grounds of the Abbey and inherited the Plymstock estate.\n\nThe first account of this story is to be found in Risdon's \"Survey of Devon\" which was completed in around 1632:\n\nFinberg pointed out, however, that a document of 1651 refers to Tavistock's guildhall as \"Guilehall\", so \"Guilebridge\" is more likely to be \"guild bridge\", probably because it was built or maintained by one of the town guilds.\n\nDevon folk singer Seth Lakeman sang about Childe the Hunter on his 2006 album \"Freedom Fields\".\n\n"}
{"id": "6328", "url": "https://en.wikipedia.org/wiki?curid=6328", "title": "Cognate", "text": "Cognate\n\nIn linguistics, cognates are words that have a common etymological origin. In etymology, the \"cognate\" category excludes doublets and loanwords. The word \"cognate\" derives from the Latin noun \"cognatus\", which means \"blood relative\".\n\nCognates do not need to have the same meaning, which may have changed as the languages developed separately. For example English \"starve\" and Dutch \"sterven\" or German \"sterben\" (\"to die\") all derive from the same Proto-Germanic root, \"*sterbaną\" (\"die\"). English \"dish\" and German \"Tisch\" (\"table\"), with their flat surfaces, both come from Latin \"discus\", but their later meanings are different. \"Discus\" is from Greek \"δίσκος\" (from the verb \"δικεῖν\" \"to throw\"). A later and separate English reflex of \"discus\", probably through medieval Latin \"desca\", is \"desk\" (see OED s.v. \"desk\").\n\nCognates also do not need to have similar forms: English \"father\", French \"père\", and Armenian հայր (\"hayr\") all descend directly from Proto-Indo-European \"*ph₂tḗr\".\n\nExamples of cognates in Indo-European languages are the words \"night\" (English), \"nuit\" (French), \"noche\" (Spanish), \"Nacht\" (German), \"nacht\" (Dutch), \"nag\" (Afrikaans), \"nicht\" (Scots), \"natt\" (Swedish, Norwegian), \"nat\" (Danish), \"nátt\" (Faroese), \"nótt\" (Icelandic), \"noc\" (Czech, Slovak, Polish), ночь, \"noch\" (Russian), ноќ, \"noć\" (Macedonian), нощ, \"nosht\" (Bulgarian), \"ніч\", \"nich\" (Ukrainian), \"ноч\", \"noch\"/\"noč\" (Belarusian), \"noč\" (Slovene), \"noć\" (Bosnian, Serbian, Croatian), νύξ, \"nyx\" (Ancient Greek, \"νύχτα\"/\"nychta\" in Modern Greek), \"nox/nocte\" (Latin), \"nakt-\" (Sanskrit), \"natë\" (Albanian), \"nos\" (Welsh), \"nueche\" (Asturian), \"noite\" (Portuguese and Galician), \"notte\" (Italian), \"nit\" (Catalan), \"nuèch/nuèit\" (Occitan), \"noapte\" (Romanian), \"nakts\" (Latvian), \"naktis\" (Lithuanian) and \"Naach\" (Colognian), all meaning \"night\" and being derived from the Proto-Indo-European \"night\".\n\nAnother Indo-European example is \"star\" (English), \"str-\" (Sanskrit), \"tara\" (Hindustani and Bengali), \"tora\" (Assamese), \"astre\"/\"étoile\" (French), \"ἀστήρ (astēr)\" (Greek or \"ἀστέρι\"/\"ἄστρο\", \"asteri\"/\"astro\" in Modern Greek), \"astro/stella\" (Italian), \"aster\" (Latin) \"stea\" (Romanian and Venetian), \"stairno\" (Gothic), \"astl\" (Armenian), \"Stern\" (German), \"ster\" (Dutch and Afrikaans), \"Schtähn\" (Colognian), \"starn\" (Scots), \"stjerne\" (Norwegian and Danish), \"stjarna\" (Icelandic), \"stjärna\" (Swedish), \"stjørna\" (Faroese), \"setāre\" (Persian), \"stoorei\" (Pashto), \"seren\" (Welsh), \"steren\" (Cornish), \"estel\" (Catalan), \"estela\" (Occitan) \"estrella\" and \"astro\" Spanish, \"estrella\" Asturian and Leonese, \"estrela\" and \"astro\" (Portuguese and Galician) and \"estêre\" or \"stêrk\" (Kurdish), from the Proto-Indo-European \"star\".\n\nThe Hebrew \"shalom\", the Arabic \"salām\", the Assyrian Neo-Aramaic \"shlama\" and the Amharic \"selam\" (\"peace\") are also cognates, derived from the Proto-Semitic *šalām- \"peace\".\n\nCognates may often be less easily recognised than the above examples, and authorities sometimes differ in their interpretations of the evidence. The English word \"milk\" is clearly a cognate of German \"Milch\", Dutch \"melk\", Russian \"молоко (moloko)\" and (Bosnian, Serbian, Slovenian \"mleko\" Croatian) also Montenegrin \"mlijeko\". On the other hand, French \"lait\", Catalan \"llet\", Italian \"latte\", Romanian \"lapte\", Spanish \"leche\" and \"leite\" (Portuguese and Galician) (all meaning \"milk\") are less-obvious cognates of Ancient Greek \"\" \"gálaktos\" (genitive singular of \"gála\", \"milk\"), a relationship that is more evidently seen through the intermediate Latin \"lac\" \"milk\" as well as the English word \"lactic\" and other terms borrowed from Latin. All of them come from Proto-Indo-European \"milk\".\n\nSome cognates are semantic opposites. For instance, while the Hebrew word \"chutzpah\" means \"impudence,\" its Classical Arabic cognate \"ḥaṣāfah\" means \"sound judgment.\" English \"black\" and Polish \"biały\", meaning white, are cognates with opposite meanings, both deriving from the Proto-Indo-European \"to burn or shine\".\n\nCognates within a single language, or \"doublets\", may have meanings that are slightly or even totally different. For example, English \"ward\" and \"guard\" (<PIE \"*wer-\", \"to perceive, watch out for\") are cognates, as are \"shirt\" (garment on top) and \"skirt\" (garment on bottom) (<PIE \"*sker-\", \"to cut\"). In some cases, including this one, one cognate (\"skirt\") has an ultimate source in another language related to English, but the other one (\"shirt\") is native. That happened with many loanwords, such as \"skirt\" in this example, which was borrowed from Old Norse during the Danelaw.\n\nSometimes both doublets come from other languages, often the same one but at different times. For example, the word \"chief\" (meaning the leader of any group) comes from the Middle French \"chef\" (\"head\"), and its modern pronunciation preserves the Middle French consonant sound; the word \"chef\" (the leader of the cooks) was borrowed from the same source centuries later, but by then, the consonant had changed to a \"sh\" sound in French. Such word sets can also be called etymological twins, and of course, they may come in groups of higher numbers, as with, for example, the words \"wain\" (native), \"waggon/wagon\" (Dutch), and \"vehicle\" (Latin) in English.\n\nA word may also enter another language, develop a new form or meaning there, and be re-borrowed into the original language; that is called reborrowing. For example, the Greek word \"κίνημα\" (\"kinima\", \"movement\") became French \"cinéma\" (compare American English \"movie\") and then later returned to Greece as \"σινεμά\" (\"sinema\", \"the art of film\", \"movie theater\"). In Greece, \"κίνημα\" (\"kinima\", \"movement\") and \"σινεμά\" (\"sinema\", \"filmmaking, cinema\") are now doublets.\n\nLess-obvious English-language doublets are \"grammar\" and \"glamour\".\n\nFalse cognates are words that people commonly believe are related (have a common origin), but that linguistic examination reveals are unrelated. For example, on the basis of superficial similarities, the Latin verb \"habēre\" and German \"haben\", both meaning 'to have', appear to be cognates. However, because of the way words in the two languages evolved from Proto-Indo-European (PIE) roots, they cannot be cognate (see for example Grimm's law). German \"haben\", like English \"have\", comes from PIE \"*kh₂pyé-\" 'to grasp', and its real cognate in Latin is \"capere\", 'to seize, grasp, capture'. Latin \"habēre\", on the other hand, is from PIE \"*gʰabʰ\", 'to give, to receive', and hence cognate with English \"give\" and German \"geben\".\n\nLikewise, English \"much\" and Spanish \"mucho\" look similar and have a similar meaning but are not cognates, as they evolved from different roots: \"much\" from Proto-Germanic \"*mikilaz\" < PIE \"*meǵ-\" and \"mucho\" from Latin \"multum\" < PIE \"*mel-\".\n\n\n\n"}
{"id": "6329", "url": "https://en.wikipedia.org/wiki?curid=6329", "title": "Chromatography", "text": "Chromatography\n\nChromatography is a laboratory technique for the separation of a mixture.\nThe mixture is dissolved in a fluid called the \"mobile phase,\" which carries it through a structure holding another material called the \"stationary phase.\" The various constituents of the mixture travel at different speeds, causing them to separate. The separation is based on differential partitioning between the mobile and stationary phases. Subtle differences in a compound's partition coefficient result in differential retention on the stationary phase and thus affect the separation.\n\nChromatography may be preparative or analytical. The purpose of preparative chromatography is to separate the components of a mixture for later use, and is thus a form of purification. Analytical chromatography is done normally with smaller amounts of material and is for establishing the presence or measuring the relative proportions of analytes in a mixture. The two are not mutually exclusive.\n\nChromatography, pronounced , is derived from Greek χρῶμα \"chroma\" which means \"color\" and γράφειν \"graphein\" \"to write\")\n\nChromatography was first employed in Russia by the Italian-born scientist Mikhail Tsvet in 1900. He continued to work with chromatography in the first decade of the 20th century, primarily for the separation of plant pigments such as chlorophyll, carotenes, and xanthophylls. Since these components have different colors (green, orange, and yellow, respectively) they gave the technique its name. New types of chromatography developed during the 1930s and 1940s made the technique useful for many separation processes.\n\nChromatography technique developed substantially as a result of the work of Archer John Porter Martin and Richard Laurence Millington Synge during the 1940s and 1950s, for which they won a Nobel prize. They established the principles and basic techniques of partition chromatography, and their work encouraged the rapid development of several chromatographic methods: paper chromatography, gas chromatography, and what would become known as high performance liquid chromatography. Since then, the technology has advanced rapidly. Researchers found that the main principles of Tsvet's chromatography could be applied in many different ways, resulting in the different varieties of chromatography described below. Advances are continually improving the technical performance of chromatography, allowing the separation of increasingly similar molecules.\n\nChromatography is based on the concept of partition coefficient. Any solute partitions between two immiscible solvents. When we make one solvent immobile (by adsorption on a solid support matrix) and another mobile it results in most common applications of chromatography. If the matrix support, or stationary phase, is polar (e.g. paper, silica etc.) it is forward phase chromatography, and if it is non-polar (C-18) it is reverse phase.\n\nColumn chromatography is a separation technique in which the stationary bed is within a tube. The particles of the solid stationary phase or the support coated with a liquid stationary phase may fill the whole inside volume of the tube (packed column) or be concentrated on or along the inside tube wall leaving an open, unrestricted path for the mobile phase in the middle part of the tube (open tubular column). Differences in rates of movement through the medium are calculated to different retention times of the sample.\n\nIn 1978, W. Clark Still introduced a modified version of column chromatography called flash column chromatography (flash). The technique is very similar to the traditional column chromatography, except for that the solvent is driven through the column by applying positive pressure. This allowed most separations to be performed in less than 20 minutes, with improved separations compared to the old method. Modern flash chromatography systems are sold as pre-packed plastic cartridges, and the solvent is pumped through the cartridge. Systems may also be linked with detectors and fraction collectors providing automation. The introduction of gradient pumps resulted in quicker separations and less solvent usage.\n\nIn expanded bed adsorption, a fluidized bed is used, rather than a solid phase made by a packed bed. This allows omission of initial clearing steps such as centrifugation and filtration, for culture broths or slurries of broken cells.\n\nPhosphocellulose chromatography utilizes the binding affinity of many DNA-binding proteins for phosphocellulose. The stronger a protein's interaction with DNA, the higher the salt concentration needed to elute that protein.\n\nPlanar chromatography is a separation technique in which the stationary phase is present as or on a plane. The plane can be a paper, serving as such or impregnated by a substance as the stationary bed (paper chromatography) or a layer of solid particles spread on a support such as a glass plate (thin layer chromatography). Different compounds in the sample mixture travel different distances according to how strongly they interact with the stationary phase as compared to the mobile phase. The specific Retention factor (R) of each chemical can be used to aid in the identification of an unknown substance.\n\nPaper chromatography is a technique that involves placing a small dot or line of sample solution onto a strip of \"chromatography paper\". The paper is placed in a container with a shallow layer of solvent and sealed. As the solvent rises through the paper, it meets the sample mixture, which starts to travel up the paper with the solvent. This paper is made of cellulose, a polar substance, and the compounds within the mixture travel farther if they are non-polar. More polar substances bond with the cellulose paper more quickly, and therefore do not travel as far.\n\nThin layer chromatography (TLC) is a widely employed laboratory technique used to separate different biochemicals on the basis of their size and is similar to paper chromatography. However, instead of using a stationary phase of paper, it involves a stationary phase of a thin layer of adsorbent like silica gel, alumina, or cellulose on a flat, inert substrate. TLC is very versatile; multiple samples can be separated simultaneously on the same layer, making it very useful for screening applications such as testing drug levels and water purity. Possibility of cross-contamination is low since each separation is performed on a new layer. Compared to paper, it has the advantage of faster runs, better separations, better quantitative analysis, and the choice between different adsorbents. For even better resolution and faster separation that utilizes less solvent, high-performance TLC can be used. An older popular use had been to differentiate chromosomes by observing distance in gel (separation of was a separate step).\n\nThe basic principle of displacement chromatography is:\nA molecule with a high affinity for the chromatography matrix (the displacer) competes effectively for binding sites, and thus displace all molecules with lesser affinities.\nThere are distinct differences between displacement and elution chromatography. In elution mode, substances typically emerge from a column in narrow, Gaussian peaks. Wide separation of peaks, preferably to baseline, is desired for maximum purification. The speed at which any component of a mixture travels down the column in elution mode depends on many factors. But for two substances to travel at different speeds, and thereby be resolved, there must be substantial differences in some interaction between the biomolecules and the chromatography matrix. Operating parameters are adjusted to maximize the effect of this difference. In many cases, baseline separation of the peaks can be achieved only with gradient elution and low column loadings. Thus, two drawbacks to elution mode chromatography, especially at the preparative scale, are operational complexity, due to gradient solvent pumping, and low throughput, due to low column loadings. Displacement chromatography has advantages over elution chromatography in that components are resolved into consecutive zones of pure substances rather than “peaks”. Because the process takes advantage of the nonlinearity of the isotherms, a larger column feed can be separated on a given column with the purified components recovered at significantly higher concentrations.\n\nGas chromatography (GC), also sometimes known as gas-liquid chromatography, (GLC), is a separation technique in which the mobile phase is a gas. Gas chromatographic separation is always carried out in a column, which is typically \"packed\" or \"capillary\". Packed columns are the routine work horses of gas chromatography, being cheaper and easier to use and often giving adequate performance. Capillary columns generally give far superior resolution and although more expensive are becoming widely used, especially for complex mixtures. Both types of column are made from non-adsorbent and chemically inert materials. Stainless steel and glass are the usual materials for packed columns and quartz or fused silica for capillary columns.\n\nGas chromatography is based on a partition equilibrium of analyte between a solid or viscous liquid stationary phase (often a liquid silicone-based material) and a mobile gas (most often helium). The stationary phase is adhered to the inside of a small-diameter (commonly 0.53 – 0.18mm inside diameter) glass or fused-silica tube (a capillary column) or a solid matrix inside a larger metal tube (a packed column). It is widely used in analytical chemistry; though the high temperatures used in GC make it unsuitable for high molecular weight biopolymers or proteins (heat denatures them), frequently encountered in biochemistry, it is well suited for use in the petrochemical, environmental monitoring and remediation, and industrial chemical fields. It is also used extensively in chemistry research.\n\nLiquid chromatography (LC) is a separation technique in which the mobile phase is a liquid. It can be carried out either in a column or a plane. Present day liquid chromatography that generally utilizes very small packing particles and a relatively high pressure is referred to as high performance liquid chromatography (HPLC).\n\nIn HPLC the sample is forced by a liquid at high pressure (the mobile phase) through a column that is packed with a stationary phase composed of irregularly or spherically shaped particles, a porous monolithic layer, or a porous membrane. HPLC is historically divided into two different sub-classes based on the polarity of the mobile and stationary phases. Methods in which the stationary phase is more polar than the mobile phase (e.g., toluene as the mobile phase, silica as the stationary phase) are termed normal phase liquid chromatography (NPLC) and the opposite (e.g., water-methanol mixture as the mobile phase and C18 (octadecylsilyl) as the stationary phase) is termed reversed phase liquid chromatography (RPLC).\n\nSpecific techniques under this broad heading are listed below.\n\nAffinity chromatography is based on selective non-covalent interaction between an analyte and specific molecules. It is very specific, but not very robust. It is often used in biochemistry in the purification of proteins bound to tags. These fusion proteins are labeled with compounds such as His-tags, biotin or antigens, which bind to the stationary phase specifically. After purification, some of these tags are usually removed and the pure protein is obtained.\n\nAffinity chromatography often utilizes a biomolecule's affinity for a metal (Zn, Cu, Fe, etc.). Columns are often manually prepared. Traditional affinity columns are used as a preparative step to flush out unwanted biomolecules.\n\nHowever, HPLC techniques exist that do utilize affinity chromatography properties. Immobilized Metal Affinity Chromatography (IMAC) is useful to separate aforementioned molecules based on the relative affinity for the metal (I.e. Dionex IMAC). Often these columns can be loaded with different metals to create a column with a targeted affinity.\n\nSupercritical fluid chromatography is a separation technique in which the mobile phase is a fluid above and relatively close to its critical temperature and pressure.\n\nIon exchange chromatography (usually referred to as ion chromatography) uses an ion exchange mechanism to separate analytes based on their respective charges. It is usually performed in columns but can also be useful in planar mode. Ion exchange chromatography uses a charged stationary phase to separate charged compounds including anions, cations, amino acids, peptides, and proteins. In conventional methods the stationary phase is an ion exchange resin that carries charged functional groups that interact with oppositely charged groups of the compound to retain. There are two types of ion exchange chromatography: Cation-Exchange and Anion-Exchange. In the Cation-Exchange Chromatography the stationary phase has negative charge and the exchangeable ion is a cation, whereas, in the Anion-Exchange Chromatography the stationary phase has positive charge and the exchangeable ion is an anion. Ion exchange chromatography is commonly used to purify proteins using FPLC.\n\nSize-exclusion chromatography (SEC) is also known as gel permeation chromatography (GPC) or gel filtration chromatography and separates molecules according to their size (or more accurately according to their hydrodynamic diameter or hydrodynamic volume).\nSmaller molecules are able to enter the pores of the media and, therefore, molecules are trapped and removed from the flow of the mobile phase. The average residence time in the pores depends upon the effective size of the analyte molecules. However, molecules that are larger than the average pore size of the packing are excluded and thus suffer essentially no retention; such species are the first to be eluted. It is generally a low-resolution chromatography technique and thus it is often reserved for the final, \"polishing\" step of a purification. It is also useful for determining the tertiary structure and quaternary structure of purified proteins, especially since it can be carried out under native solution conditions.\n\nAn expanded bed chromatographic adsorption (EBA) column for a biochemical separation process comprises a pressure equalization liquid distributor having a self-cleaning function below a porous blocking sieve plate at the bottom of the expanded bed, an upper part nozzle assembly having a backflush cleaning function at the top of the expanded bed, a better distribution of the feedstock liquor added into the expanded bed ensuring that the fluid passed through the expanded bed layer displays a state of piston flow. The expanded bed layer displays a state of piston flow. The expanded bed chromatographic separation column has advantages of increasing the separation efficiency of the expanded bed.\n\nExpanded-bed adsorption (EBA) chromatography is a convenient and effective technique for the capture of proteins directly from unclarified crude sample. In EBA chromatography, the settled bed is first expanded by upward flow of equilibration buffer. The crude feed, a mixture of soluble proteins, contaminants, cells, and cell debris, is then passed upward through the expanded bed. Target proteins are captured on the adsorbent, while particulates and contaminants pass through. A change to elution buffer while maintaining upward flow results in desorption of the target protein in expanded-bed mode. Alternatively, if the flow is reversed, the adsorbed particles will quickly settle and the proteins can be desorbed by an elution buffer. The mode used for elution (expanded-bed versus settled-bed) depends on the characteristics of the feed. After elution, the adsorbent is cleaned with a predefined cleaning-in-place (CIP) solution, with cleaning followed by either column regeneration (for further use) or storage.\n\nReversed-phase chromatography (RPC) is any liquid chromatography procedure in which the mobile phase is significantly more polar than the stationary phase. It is so named because in normal-phase liquid chromatography, the mobile phase is significantly less polar than the stationary phase. Hydrophobic molecules in the mobile phase tend to adsorb to the relatively hydrophobic stationary phase. Hydrophilic molecules in the mobile phase will tend to elute first. Separating columns typically comprise a C8 or C18 carbon-chain bonded to a silica particle substrate.\n\nHydrophobic interactions between proteins and the chromatographic matrix can be exploited to purify proteins. In hydrophobic interaction chromatography the matrix material is lightly substituted with hydrophobic groups. These groups can range from methyl, ethyl, propyl, octyl, or phenyl groups.[] At high salt concentrations, non-polar sidechains on the surface on proteins \"interact\" with the hydrophobic groups; that is, both types of groups are excluded by the polar solvent (hydrophobic effects are augmented by increased ionic strength). Thus, the sample is applied to the column in a buffer which is highly polar. The eluant is typically an aqueous buffer with decreasing salt concentrations, increasing concentrations of detergent (which disrupts hydrophobic interactions), or changes in pH.\n\nIn general, Hydrophobic Interaction Chromatography (HIC) is advantageous if the sample is sensitive to pH change or harsh solvents typically used in other types of chromatography but not high salt concentrations. Commonly, it is the amount of salt in the buffer which is varied. In 2012, Müller and Franzreb described the effects of temperature on HIC using Bovine Serum Albumin (BSA) with four different types of hydrophobic resin. The study altered temperature as to effect the binding affinity of BSA onto the matrix. It was concluded that cycling temperature from 50 degrees to 10 degrees would not be adequate to effectively wash all BSA from the matrix but could be very effective if the column would only be used a few times. Using temperature to effect change allows labs to cut costs on buying salt and saves money.\n\nIf high salt concentrations along with temperature fluctuations want to be avoided you can use a more hydrophobic to compete with your sample to elute it. [source] This so-called salt independent method of HIC showed a direct isolation of Human Immunoglobulin G (IgG) from serum with satisfactory yield and used Beta-cyclodextrin as a competitor to displace IgG from the matrix. This largely opens up the possibility of using HIC with samples which are salt sensitive as we know high salt concentrations precipitate proteins.\n\nIn some cases, the chemistry within a given column can be insufficient to separate some analytes. It is possible to direct a series of unresolved peaks onto a second column with different physico-chemical (Chemical classification) properties. Since the mechanism of retention on this new solid support is different from the first dimensional separation, it can be possible to separate compounds that are indistinguishable by one-dimensional chromatography.\nThe sample is spotted at one corner of a square plate, developed, air-dried, then rotated by 90° and usually redeveloped in a second solvent system.\n\nThe simulated moving bed (SMB) technique is a variant of high performance liquid chromatography; it is used to separate particles and/or chemical compounds that would be difficult or impossible to resolve otherwise. This increased separation is brought about by a valve-and-column arrangement that is used to lengthen the stationary phase indefinitely.\nIn the moving bed technique of preparative chromatography the feed entry and the analyte recovery are simultaneous and continuous, but because of practical difficulties with a continuously moving bed, simulated moving bed technique was proposed. In the simulated moving bed technique instead of moving the bed, the sample inlet and the analyte exit positions are moved continuously, giving the impression of a moving bed.\nTrue moving bed chromatography (TMBC) is only a theoretical concept. Its simulation, SMBC is achieved by the use of a multiplicity of columns in series and a complex valve arrangement, which provides for sample and solvent feed, and also analyte and waste takeoff at appropriate locations of any column, whereby it allows switching at regular intervals the sample entry in one direction, the solvent entry in the opposite direction, whilst changing the analyte and waste takeoff positions appropriately as well.\n\nPyrolysis gas chromatography mass spectrometry is a method of chemical analysis in which the sample is heated to decomposition to produce smaller molecules that are separated by gas chromatography and detected using mass spectrometry.\n\nPyrolysis is the thermal decomposition of materials in an inert atmosphere or a vacuum. The sample is put into direct contact with a platinum wire, or placed in a quartz sample tube, and rapidly heated to 600–1000 °C. Depending on the application even higher temperatures are used. Three different heating techniques are used in actual pyrolyzers: Isothermal furnace, inductive heating (Curie Point filament), and resistive heating using platinum filaments. Large molecules cleave at their weakest points and produce smaller, more volatile fragments. These fragments can be separated by gas chromatography. Pyrolysis GC chromatograms are typically complex because a wide range of different decomposition products is formed. The data can either be used as fingerprint to prove material identity or the GC/MS data is used to identify individual fragments to obtain structural information. To increase the volatility of polar fragments, various methylating reagents can be added to a sample before pyrolysis.\n\nBesides the usage of dedicated pyrolyzers, pyrolysis GC of solid and liquid samples can be performed directly inside Programmable Temperature Vaporizer (PTV) injectors that provide quick heating (up to 30 °C/s) and high maximum temperatures of 600–650 °C. This is sufficient for some pyrolysis applications. The main advantage is that no dedicated instrument has to be purchased and pyrolysis can be performed as part of routine GC analysis. In this case quartz GC inlet liners have to be used. Quantitative data can be acquired, and good results of derivatization inside the PTV injector are published as well.\n\nFast protein liquid chromatography (FPLC), is a form of liquid chromatography that is often used to analyze or purify mixtures of proteins. As in other forms of chromatography, separation is possible because the different components of a mixture have different affinities for two materials, a moving fluid (the \"mobile phase\") and a porous solid (the stationary phase). In FPLC the mobile phase is an aqueous solution, or \"buffer\". The buffer flow rate is controlled by a positive-displacement pump and is normally kept constant, while the composition of the buffer can be varied by drawing fluids in different proportions from two or more external reservoirs. The stationary phase is a resin composed of beads, usually of cross-linked agarose, packed into a cylindrical glass or plastic column. FPLC resins are available in a wide range of bead sizes and surface ligands depending on the application.\n\nCountercurrent chromatography (CCC) is a type of liquid-liquid chromatography, where both the stationary and mobile phases are liquids. \nThe operating principle of CCC equipment requires a column consisting of an open tube coiled around a bobbin. The bobbin is rotated in a double-axis gyratory motion (a cardioid), which causes a variable gravity (G) field to act on the column during each rotation. This motion causes the column to see one partitioning step per revolution and components of the sample separate in the column due to their partitioning coefficient between the two immiscible liquid phases used. There are many types of CCC available today. These include HSCCC (High Speed CCC) and HPCCC (High Performance CCC). HPCCC is the latest and best performing version of the instrumentation available currently.\n\nChiral chromatography involves the separation of stereoisomers. In the case of enantiomers, these have no chemical or physical differences apart from being three-dimensional mirror images. Conventional chromatography or other separation processes are incapable of separating them. To enable chiral separations to take place, either the mobile phase or the stationary phase must themselves be made chiral, giving differing affinities between the analytes. Chiral chromatography HPLC columns (with a chiral stationary phase) in both normal and reversed phase are commercially available.\n\n"}
{"id": "6330", "url": "https://en.wikipedia.org/wiki?curid=6330", "title": "Clement Martyn Doke", "text": "Clement Martyn Doke\n\nClement Martyn Doke (16 May 1893 in Bristol, United Kingdom – 24 February 1980 in East London, South Africa) was a South African linguist working mainly on African languages. Realizing that the grammatical structures of Bantu languages are quite different from those of European languages, he was one of the first African linguists of his time to abandon the Euro-centric approach to language description for a more locally grounded one. A most prolific writer, he published a string of grammars, several dictionaries, comparative work, and a history of Bantu linguistics.\n\nThe Doke family had been engaged in missionary activity for the Baptist Church for some generations. His father Reverend Joseph J. Doke left England and travelled to South Africa in 1882, where he met and married Agnes Biggs. They returned to England, where Clement was born as the third of four children. The family moved to New Zealand and eventually returned to South Africa in 1903, where they later on settled in Johannesburg.\n\nAt the age of 18, Clement received a bachelor's degree from Transvaal University College in Pretoria (now the University of Pretoria). He decided to devote his life to missionary activity. In 1913, he accompanied his father on a tour of north-western Rhodesia, to an area called Lambaland, now known as Ilamba. It is situated at the watershed of the Congo and Zambesi rivers, part of the district lay in Northern Rhodesia and part in the Belgian Congo State. The Cape-Cairo Railway threaded through its eastern portion; otherwise, travelling mostly had to be done on foot.\n\nThe Reverend William Arthur Phillips of the Nyasa Industrial Mission in Blantyre had established a Baptist mission there in 1905, serving an area of and 50,000 souls. The Dokes were supposed to investigate, whether the mission in Lambaland could be taken over by the Baptist Union of South Africa. It was on this trip that Doke's father contracted enteric fever and died soon afterwards (Gandhi attended the memorial service and addressed the congregation). Clement assumed his father's role.\n\nThe South African Baptists decided to take over Kafulafuta Mission, while its founder Reverend Phillips remained as superintendent. Clement Doke returned to Kafulafuta as missionary in 1914, followed by his sister Olive two years later.\n\nAt first, Clement Doke was frustrated by his inability to communicate with the Lamba. The only written material available at the time was a translation of Jonah and a collection of 47 hymns. Soon he mastered the language and published his first book \"Ifintu Fyakwe Lesa\" (The Things of God, a Primer of Scripture Knowledge) in 1917. He enrolled in Johannesburg as the extension of Transvaal University College for an MA degree. His thesis was published as \"The Grammar of the Lamba language\". The book is couched in traditional grammatical terms as Doke had not yet established his innovative method of analysis and description for the Bantu languages. His later \"Textbook of Lamba Grammar\" is far superior in this respect. \n\nClement Doke was also interested in ethnology. In 1931 he compiled \"The Lambas of Northern Rhodesia\", which remains one of the outstanding ethnographic descriptions of the peoples of Central Africa. For Doke, literacy was part of the evangelisation since people had to be able to read to appreciate the message of the Bible, but it was only after his retirement that he completed the translation of the Bible into Lamba. It was published under the title of \"Amasiwi AwaLesa\" (The Words of God) in 1959.\n\nIn 1919 Doke married Hilda Lehmann, who accompanied him back to Lambaland. They both contracted malaria during their work and she was forbidden to return to Lambaland. Clement Doke also realised that his field work couldn't continue much longer and left in 1921. He was recruited by the newly founded University of the Witwatersrand. In order to secure a qualification as a lecturer, the family moved to England, where he registered at the School of Oriental and African Studies. His major languages were Lamba and Luba, but as no suitable examiner was available, he eventually had to change his language to Zulu.\n\nDoke took up his appointment in the new Department of Bantu Studies at the University of Witwatersrand in 1923. In 1925 he received his D. Litt. for his doctoral thesis \"The Phonetics of the Zulu Language\" and was promoted to Senior Lecturer. In 1931 he was appointed to the Chair of Bantu Studies and thus headed the Department of Bantu Studies. The Department acted as a catalyst for the admission of Africans to the University: as early as 1925 a limited number were admitted to the vacation course in African Studies. Doke supported the appointment of Benedict Wallet Vilakazi as member of the staff, as he believed a native speaker was essential for acquiring a language. This provoked a storm of criticism and controversy from the public. They both collaborated on the \"Zulu-English Dictionary\", first published in 1948. It is still one of the best examples of lexicography for any of the Bantu languages. \n\nAt the request of the government of Southern Rhodesia, Doke investigated the range of dialect diversity among the languages of the country and made recommendations for \"Unified Shona\". This formed the basis for Standard Shona. He devised a unified orthography based on the Zezuru, Karanga and Manyika dialects. However, Doke's orthography was never fully accepted and the South African government introduced an alternative, leaving Shona with two competing orthographies between 1935 and 1955.\n\nDuring his tenure Doke developed and promoted a method of linguistic analysis and description of the Bantu languages that was based upon the structure of these languages. The \"Dokean model\" continues to be one of the dominant models of linguistic description in Southern and Central Africa. His classification of the Bantu languages was for many years the dominant view of the interrelations among the African languages. He was also an early describer of Khoisan and Bantu click consonants, devising phonetic symbols for a number of them. \n\nDoke served the University of the Witwatersrand until his retirement in 1953. He was awarded the honorary degree of Doctor of letters by Rhodes University and the honorary degree of Doctor of Laws by the University of the Witwatersrand in 1972.\n\nThe former missionary always remained devoted to the Baptist Church. He was elected President of the South African Baptist Union in 1949 and spent a year visiting churches and mission stations. He used his presidential address in condemning the recently established apartheid policy: \"I solemnly warn the Government that the spirit behind their apartheid legislation, and the way in which they are introducing discriminatory measures of all types today, will bring disaster upon this fair land of ours.\"\n\n"}
{"id": "6331", "url": "https://en.wikipedia.org/wiki?curid=6331", "title": "Carl Meinhof", "text": "Carl Meinhof\n\nCarl Friedrich Michael Meinhof (July 23, 1857 – February 11, 1944) was a German linguist and one of the first linguists to study African languages.\n\nMeinhof was born in Barzwitz near Rügenwalde in the Province of Pomerania. He studied at the University of Tübingen and at the University of Greifswald. In 1905 he became professor at the School of Oriental Studies in Berlin. On 5 May 1933 he became a member of the Nazi Party.\n\nHis most notable work was developing comparative grammar studies of the Bantu languages, building on the pioneering work of Wilhelm Bleek. In his work, Meinhof looked at the common Bantu languages such as Swahili and Zulu to determine similarities and differences.\n\nIn his work, Meinhof looked at noun classes with all Bantu languages having at least 10 classes and with 22 classes of nouns existing throughout the Bantu languages, though his definition of noun class differs slightly from the accepted one, considering the plural form of a word as belonging to a different class from the singular form (thus leading, for example, to consider a language like French as having four classes instead of two). While no language has all 22 (later: 23) classes active, Venda has 20, Lozi has 18, and Ganda has 16 or 17 (depending on whether the locative class 23 \"e-\" is included). All Bantu languages have a noun class specifically for humans (sometimes including other animate beings).\n\nMeinhof also examined other African languages, including groups classified at the time as Kordofanian, Bushman, Khoikhoi, and Hamitic.\n\nMeinhof developed a comprehensive classification scheme for African languages. His classification was the standard one for many years (Greenberg 1955:3). It was replaced by those of Joseph Greenberg in 1955 and in 1963.\n\nIn 1902, Meinhof made recordings of East African music. These are among the first recordings made of traditional African music.\n\nIn 1912, Carl Meinhof published \"Die Sprachen Der Hamiten\" (The Languages of the Hamites). He used the term Hamitic. Meinhof's system of classification of the Hamitic languages was based on a belief that \"speakers of Hamitic became largely coterminous with cattle herding peoples with essentially Caucasian origins, intrinsically different from and superior to the 'Negroes of Africa'.\" However, in the case of the so-called Nilo-Hamitic languages (a concept he introduced), it was based on the typological feature of gender and a \"fallacious theory of language mixture.\" Meinhof did this in spite of earlier work by scholars such as Lepsius and Johnston demonstrating that the languages which he would later dub \"Nilo-Hamitic\" were in fact Nilotic languages with numerous similarities in vocabulary with other Nilotic languages.\n\nCarl Meinhof was the great-uncle (the brother of the grandfather) of Ulrike Meinhof, a founding member of the German Red Army Faction (RAF), a left-wing militant group, which operated in West Germany in the 1970s and 1980s.\n\n"}
{"id": "6335", "url": "https://en.wikipedia.org/wiki?curid=6335", "title": "Cucurbitaceae", "text": "Cucurbitaceae\n\nThe Cucurbitaceae, also called cucurbits and the gourd family, are a plant family consisting of about 965 species in around 95 genera, the most important of which are:\n\nThe plants in this family are grown around the tropics and in temperate areas, where those with edible fruits were among the earliest cultivated plants both in the Old and New Worlds. The Cucurbitaceae family ranks among the highest of plant families for number and percentage of species used as human food.\n\nThe Cucurbitaceae consist of 98 proposed genera with 975 species, mainly in regions tropical and subtropical. All species are sensitive to frost. Most of the plants in this family are annual vines, but some are woody lianas, thorny shrubs, or trees (\"Dendrosicyos\"). Many species have large, yellow or white flowers. The stems are hairy and pentangular. Tendrils are present at 90° to the leaf petioles at nodes. Leaves are exstipulate alternate simple palmately lobed or palmately compound. The flowers are unisexual, with male and female flowers on different plants (dioecious) or on the same plant (monoecious). The female flowers have inferior ovaries. The fruit is often a kind of modified berry called a pepo.\n\nOne of the oldest fossil records so far is \"Cucurbitaciphyllum lobatum\" from the Paleocene epoch, found at Shirley Canal, Montana. It was described for the first time in 1924 by Knowlton. The fossil leaf is palmate, trilobed with rounded lobal sinuses and an entire or serrate margin. It has a leaf pattern similar to the members of the genera \"Kedrostis\", \"Melothria\" and \"Zehneria\".\n\nThe most recent classification of Cucurbitaceae delineates 15 tribes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"Abobra, Acanthosicyos, Actinostemma, Alsomitra, Ampelosicyos, Anisosperma, Apodanthera, Austrobryonia, Baijiania, Bambekea, Bayabusua, Benincasa, Borneosicyos, Bryonia, Calycophysum, Cayaponia, Cephalopentandra, Ceratosanthes, Cionosicys, Citrullus, Coccinia, Cogniauxia, Corallocarpus, Ctenolepis, Cucumis, Cucurbita, Cucurbitella, Cyclanthera, Cyclantheropsis, Dactyliandra, Dendrosicyos, Diplocyclos, Doyerea, Ecballium, Echinocystis, Echinopepon, Eureiandra, Fevillea, Frantzia, Gerrardanthus, Gomphogyne, Gurania, Gynostemma, Halosicyos, Hanburia, Helmontia, Hemsleya, Herpetospermum, Hodgsonia, Ibervillea, Indofevillea, Indomelothria, Kedrostis, Khmeriosicyos, Lagenaria, Lemurosicyos, Linnaeosicyos, Luffa, Marah, Melothria, Melotrianthus, Momordica, Muellerargia, Neoalsomitra, Nothoalsomitra, Papuasicyos, Penelopeia, Peponium, Peponopsis, Polyclathra, Psiguria, Pteropepon, Raphidiocystis, Ruthalicia, Schizocarpum, Schizopepon, Scopellaria, Seyrigia, Sicana, Sicydium, Sicyos, Siolmatra, Siraitia, Solena, Tecunumania, Telfairia, Thladiantha, Trichosanthes, Trochomeria, Trochomeriopsis, Tumamoca, Wilbrandia, Xerosicyos, Zanonia, Zehneria.\"\nModern molecular phylogenetics suggest the following relationships:\n\nSix cucurbit crops are represented in 23 Byzantine-era mosaics from Israel, these being round melons (\"Cucumis melo\"), watermelons (\"Citrullus lanatus\"), sponge gourds (\"Luffa aegyptiaca\"), snake melons (\"faqqous\", \"Cucumis melo\" flexuosus group), \"adzhur\" melons (\"C. melo\" adzhur group), and bottle gourds (\"Lagenaria siceraria\"). Cucurbits are represented in 23 of the 134 mosaics containing images of crop plants, a surprisingly high frequency of 17%. Several of the cucurbit images have not been found elsewhere, suggesting a diverse and highly developed local horticulture of cucurbits in Israel during the Byzantine era. Representations of mature sponge gourds are found in widespread localities, suggestive of the high value accorded to cleanliness and hygiene.\n\nThe name \"Cucurbitaceae\" () comes to international scientific vocabulary from New Latin, from \"Cucurbita\", the type genus, + \"-aceae\", a standardized suffix for plant family names in modern taxonomy. The genus name comes from the Classical Latin word \"cucurbita\", \"gourd\".\n\n\n"}
{"id": "6336", "url": "https://en.wikipedia.org/wiki?curid=6336", "title": "Chorded keyboard", "text": "Chorded keyboard\n\nA keyset or chorded keyboard (also called a chorded keyset, \"chord keyboard\" or \"chording keyboard\") is a computer input device that allows the user to enter characters or commands formed by pressing several keys together, like playing a \"chord\" on a piano. The large number of combinations available from a small number of keys allows text or commands to be entered with one hand, leaving the other hand free. A secondary advantage is that it can be built into a device (such as a pocket-sized computer or a bicycle handlebar) that is too small to contain a normal-sized keyboard.\n\nA chorded keyboard minus the board, typically designed to be used while held in the hand, is called a keyer. Douglas Engelbart introduced the chorded keyset as a computer interface in 1968 at what is often called \"The Mother of All Demos\".\n\nEach key is mapped to a number and then can be mapped to a corresponding letter or command. By pressing two or more keys together the user can generate many combinations. In Engelbart's original mapping, he used five keys: 1,2,4,8,16. The keys were mapped as follows: a = 1, b = 2, c = 3, d = 4, and so on. If the user pressed keys 1 + 2 = 3 simultaneously, and then released the keys, the letter \"c\" appeared. Unlike pressing a chord on a piano, the chord is recognized only after all the keys or mouse buttons are released. Since Engelbart introduced the keyset, several different designs have been developed based on similar concepts.\n\nAs a crude example, each finger might control one key which corresponds to one bit in a byte, so that using seven keys and seven fingers, one could enter any character in the ASCII set—if the user could remember the binary codes. Due to the small number of keys required, chording is easily adapted from a desktop to mobile environment.\n\nPractical devices generally use simpler chords for common characters (\"e.g.,\" Baudot), or may have ways to make it easier to remember the chords (\"e.g.,\" Microwriter), but the same principles apply. These portable devices first became popular with the wearable computer movement in the 1980s.\n\nTipTap.mobi released a chording app for the iPhone based on Douglas Engelbart's design in 2010.\n\nThad Starner from Georgia Institute of Technology and others published numerous studies showing that two handed chorded text entry was faster and yielded fewer errors than on a QWERTY keyboard. Currently stenotype machines hold the record for fastest word entry. Many stenotype users can reach 300 words per minute. However, stenographers typically train for three years before reaching professional levels of speed and accuracy.\n\nThe earliest known chord keyboard was part of the \"five-needle\" telegraph operator station, designed by Wheatstone and Cooke in 1836, in which any two of the five needles could point left or right to indicate letters on a grid. It was designed to be used by untrained operators (who would determine which keys to press by looking at the grid), and was not used where trained telegraph operators were available.\n\nThe first widespread use of a chord keyboard was in the stenotype machine used by court reporters, which was invented in 1868 and is still in use. The output of the stenotype was originally a phonetic code that had to be transcribed later (usually by the same operator who produced the original output), rather than arbitrary text—automatic conversion software is now commonplace.\n\nIn 1874, the five-bit Baudot telegraph code and a matching 5-key chord keyboard was designed to be used with the operator forming the codes manually. The code is optimized for speed and low wear: chords were chosen so that the most common characters used the simplest chords. But telegraph operators were already using typewriters with QWERTY keyboards to \"copy\" received messages, and at the time it made more sense to build a typewriter that could generate the codes automatically, rather than making them learn to use a new input device.\n\nSome early keypunch machines used a keyboard with 12 labeled keys to punch the correct holes in paper cards. The numbers 0 through 9 were represented by one punch; 26 letters were represented by combinations of two punches, and symbols were represented by combinations of two or three punches.\nBraille (a writing system for the blind) uses either 6 or 8 tactile 'points' from which all letters and numbers are formed. When Louis Braille invented it, it was produced with a needle holing successively all needed points in a cardboard sheet. In 1892, Frank Haven Hall, superintendent of the Illinois Institute for the Education of the Blind, created the Hall Braille Writer, which was like a typewriter with 6 keys, one for each dot in a braille cell. The Perkins Brailler, first manufactured in 1951, uses a 6-key chord keyboard (plus a spacebar) to produce braille output, and has been very successful as a mass market affordable product. Braille, like Baudot, uses a number symbol and a shift symbol, which may be repeated for shift lock, to fit numbers and upper case into the 63 codes that 6 bits offer.\n\nAfter World War II, with the arrival of electronics for reading chords and looking in tables of \"codes\", the postal sorting offices started to research chordic solutions to be able to employ people other than trained and expensive typists. In 1954, an important concept was discovered: chordic production is easier to master when the production is done at the release of the keys instead of when they are pressed.\n\nResearchers at IBM investigated chord keyboards for both typewriters and computer data entry as early as 1959, with the idea that it might be faster than touch-typing if some chords were used to enter whole words or parts of words. A 1975 design by IBM Fellow Nat Rochester had 14 keys that were dimpled on the edges as well as the top, so one finger could press two adjacent keys for additional combinations. Their results were inconclusive, but research continued until at least 1978.\n\nDoug Engelbart began experimenting with keysets to use with the mouse in the mid 1960s. In a famous 1968 demonstration, Engelbart introduced a computer human interface that included the QWERTY keyboard, a three button mouse, and a five key keyset. Engelbart used the keyset with his left hand and the mouse with his right to type text and enter commands. The mouse buttons marked selections and confirmed or aborted commands.\n\nUsers in Engelbart's Augmentation Research Center at SRI became proficient with the mouse and keyset. In the 1970s the funding Engelbart's group received from the Advanced Research Projects Agency (ARPA) was cut and many key members of Engelbart's team went to work for Xerox PARC where they continued to experiment with the mouse and keyset. Keychord sets were used at Xerox PARC in the early 1980s, along with mice, GUIs, on the Xerox Star and Alto workstations. A one button version of the mouse was incorporated into the Apple Macintosh but Steve Jobs decided against incorporating the chorded keyset.\n\nIn the early 1980s, Philips Research labs at Redhill, Surrey did a brief study into small, cheap keyboards for entering text on a telephone. One solution used a grid of hexagonal keys with symbols inscribed into dimples in the keys that were either in the center of a key, across the boundary of two keys, or at the joining of three keys. Pressing down on one of the dimples would cause either one, two or three of the hexagonal buttons to be depressed at the same time, forming a chord that would be unique to that symbol. With this arrangement, a nine button keyboard with three rows of three hexagonal buttons could be fitted onto a telephone and could produce up to 33 different symbols. By choosing widely separated keys, one could employ one dimple as a 'shift' key to allow both letters and numbers to be produced. With eleven keys in a 3/4/4 arrangement, 43 symbols could be arranged allowing for lowercase text, numbers and a modest number of punctuation symbols to be represented along with a 'shift' function for accessing uppercase letters. While this had the advantage of being usable by untrained users via 'hunt and peck' typing and requiring one less key switch than a conventional 12 button keypad, it had the disadvantage that some symbols required three times as much force to depress them as others which made it hard to achieve any speed with the device. That solution is still alive and proposed by Fastap and Unitap among others, and a commercial phone has been produced and promoted in Canada during 2006.\n\nHistorically, the baudot and braille keyboards were standardized to some extent, but they are unable to replicate the full character set of a modern keyboard. Braille comes closest, as it has been extended to eight bits.\n\nThe only proposed modern standard, GKOS (or Global Keyboard Open Standard) can support most characters and functions found on a computer keyboard but has had little commercial development. There is, however, a GKOS keyboard application available for iPhone since May 8, 2010, for Android since October 3, 2010 and for MeeGo Harmattan since October 27, 2011.\n\nFour open-source keyer/keyset designs are available: The pickey, a PS/2 device based on the PIC microcontroller; the spiffchorder, a USB device based on the Atmel AVR family of microcontrollers; the FeatherChorder, a BLE chorder based on the Adafruit Feather, an all in one board incorporating an Arduino-compatible microcontroller; and the GKOS keypad driver for Linux as well as the Gkos library for the Atmel/Arduino open source board.\n\nPlover is a free, open-source, cross-platform program intended to bring realtime stenographic technology not just to stenographers, but also to hobbyists using anything from professional Stenotype machines to low-cost NKRO gaming keyboards. It is available for GNU/Linux, Microsoft Windows, and Apple Mac macOS.\n\nJoy2chord is a chorded keyboard driver for GNU/Linux. With a configuration file, any joystick or gamepad can be turned into a chorded keyboard. This design philosophy was decided on to lower the cost of building devices, and in turn lower the entry barrier to becoming familiar with chorded keyboards. Macro keys, and multiple modes are also easily implemented with a user space driver.\n\nOne minimal chordic keyboard example is Edgar Matias' Half-Qwerty keyboard described in patent circa 1992 that produces the letters of the missing half when the user simultaneously presses the space bar along with the mirror key. INTERCHI '93 published a study by Matias, MacKenzie and Buxton showing that people who have already learned to touch-type can quickly recover 50 to 70% of their two-handed typing speed. The loss contributes to the speed discussion above. It is implemented on two popular mobile phones, each provided with software disambiguation, which allows users to avoid using the space-bar.\n\n\"Multiambic\" keyers for use with wearable computers were invented in Canada in the 1970s. Multiambic keyers are similar to chording keyboards but without the board, in that the keys are grouped in a cluster for being handheld, rather than for sitting on a flat surface.\n\nChording keyboards are also used as portable but two handed input devices for the visually impaired (either combined with a refreshable braille display or vocal synthesis). Such keyboards use a minimum of seven keys, where each key corresponds to an individual braille point, except one key which is used as a spacebar. In some applications, the spacebar is used to produce additional chords which enable the user to issue editing commands, such as moving the cursor, or deleting words. Note that the number of points used in braille computing is not 6, but 8, as this allows the user, among other things, to distinguish between small and capital letters, as well as identify the position of the cursor. As a result, most newer chorded keyboards for braille input include at least nine keys.\n\nTouch screen chordic keyboards are available to smartphone users as an optional way of entering text. As the number of keys is low the button areas can be made bigger and easier to hit on the small screen. The most common letters do not necessarily require chording as is the case with the GKOS keyboard optimised layouts (Android app) where the twelve most frequent characters only require single keys.\n\nThe WriteHander, a 12-key chord keyboard from NewO Company, appeared in 1978 issues of ROM Magazine, an early microcomputer applications magazine.\n\nAnother early commercial model was the six-button Microwriter, designed by Cy Endfield and Chris Rainey, and first sold in 1980. Microwriting is the system of chord keying and is based on a set of mnemonics. It was designed only for right-handed use.\n\nThe BAT is a 7-key hand-sized device from Infogrip, and has been sold since 1985. It provides one key for each finger and three for the thumb. It is proposed for the hand which does not hold the mouse, in an exact continuation of Engelbart's vision.\n\nModern examples of chorded keyboards include TipTapSpeech (using Engelbart's original mapping), the GKOS keyboard, the FrogPad, the In10did method, the EkaPad, TextFaster and HotTyper. Some of them are intended for tiny tablet computers and wireless mobile terminals, many of them are additionally available as apps on Apple's iOS devices. See also the on-screen virtual keyset at Teague Labs.\nChris Rainey, the co-inventor of Microwriter, re-introduced Microwriting for PC and Palm PDAs with a standalone miniature chording keyboard called CyKey which caters to both left and right-handed users, being 9-keys. CyKey (pronounced sai-ki) is named after the Microwriter chord system's co-inventor Cy Endfield, who died in 1995 but the name also reflects its intuitive nature.\nThe GKOS is a 6-key keyboard with a different signs and commands allocation of the 63 different chords in order to provide all PC keyboard functions and to make entering letters and numbers lighter by having to press fewer keys simultaneously. The 6 physical keys are intended to be on the back of the device and to be operated with the six free fingers of two hands holding the device. Another option is to have virtual GKOS keys positioned towards the sides of a touch sensitive screen. This so-called GKOS Thumbs has additional keys to enable all combos by only one keypress per hand. GKOS iPhone, Android phone/tablet and MeeGo Harmattan applications use this principle.\nThe EkaPad is a 12-key chorded keyboard operated with the four fingers of one hand. It is supported on the thumb. With the 9 main keys, (operated by the index, middle, and ring fingers), 2 prefix keys and one delete key, the EkaPad can produce all the inputs of a standard qwerty keyboard with one, two, and a few three finger chords. For some characters one or two prefix chords are required. 9 main keys (3×3 matrix) can produce a total of 511 chords. With each of the three fingers limited to its own row, 229 chords are possible with 3 fingers. EkaPad uses 66 of these accessible chords. One and two finger chords produce about 85% of American English; with an additional prefix chord about 97%. In addition, the EkaPad can store 100 text strings and 100 keyboard shortcuts. Like many other chorded keyboards, it can be used with one hand.\n\nEkaPads are no longer manufactured at this writing.\n\nThe FrogPad is a 20-key chorded keyboard about the size of a numeric keypad that can be used with one hand, and is optimized by character frequency. 85% of average keystrokes in English text can be typed without chording, and chords are limited to 2 fingers.\n\nThe Decatxt keyboard uses the IN10DID 10 key chording method[29] (pronounced \"intended\"), and is currently on Amazon. It is a wireless one-handed chord keyboard that places two keys under each finger in order to utilize one hand for typing. Typically only two fingers are needed for any operations. Each key is essentially a shift key so that with ten keys, there are ten single keystrokes and dozens of two and three key combinations. The alphabet is produced with a single press for ten letters or by shifting with either thumb for sixteen more. Changing modes, such as number lock, can make other input such as numbers, provided with a single keystroke. This avoids complex chords while providing enough keystrokes for efficient typing and allows for some unique implementations such as typing with gloves or on a steering wheel. A video game controller called the X-SKIN, using this system, was expected to be commercially available by 2010[citation needed] to help make Morphs popular on console systems and ease entry of common data such as a username and password, but the USB device was never made commercially available. The IN10DID chording system can be applied in single hand configurations, two handed or with one key at a time if desired. Claimed advantages of the IN10DID method are the diversity of devices, limited motion and simple chords.\n\nThe Twiddler is a fully featured 16-key keyboard (plus mouse) designed to fit in the palm of one hand. It was originally introduced in the early 1990s by Handykey and is currently being produced by Tek Gear (Tek Gear acquired Handykey on April 30, 2008). It is popular among wearable computer researchers and hobbyists due to its ease of use, large community of users, and active support by the manufacturer. Every single and multi-key chord on the Twiddler can be customized by the end user. The Twiddler comes standard with an \"A, B, C, D\" chord set, with TabSpace and other chord sets available. Chords are not limited to single keystrokes - multiple keystrokes can be sent with a single chord press. An example of this is an email address or address block can be typed by pressing just one chord. The efficiency gained by using multi-character chords have novice Twiddler users typing at 47 WPM while experts can burst up to 130 WPM.\nASETNIOP is a virtual keyboard based on chords that appeared in 2012. The alphabet uses the 8 keys of the home row as ASET and NIOP (the eight most common letters in the English language), plus 18 chorded combinations. The layout also makes a less-cluttered 10-button keypad for tablet computers, touchscreens, touchpads, and can be used in wired gloves.\n\n\n"}
{"id": "6337", "url": "https://en.wikipedia.org/wiki?curid=6337", "title": "Carolyn Beug", "text": "Carolyn Beug\n\nCarolyn Ann Mayer-Beug (December 11, 1952 – September 11, 2001) was a filmmaker and video producer from Santa Monica, California. She died in the September 11 attacks.\n\nIn addition to her work as video producer, Beug also directed three music videos for country singer Dwight Yoakam: \"Ain't That Lonely Yet\", \"A Thousand Miles from Nowhere\" and \"Fast as You.\" Beug co-directed the former two videos with Yoakam and was the sole director of the latter video. She won an MTV Video Music award for the Van Halen music video of the song \"Right Now\", which she produced. She also served as senior vice president of Walt Disney Records.\n\nBeug lived in a Tudor-style home in the North 25th Street neighborhood. She hosted an annual backyard barbecue for the Santa Monica High School cross country and track team, which her daughters captained. Beug was a Latter Day Saint.\n\nBeug was killed at the age of 48 in the crash of American Airlines Flight 11 in the September 11, 2001 attacks. At the time of her death, Carolyn Beug was working on a children's book about Noah's Ark which was to be told from Noah's wife's point of view. On the plane with her was her mother, Mary Alice Wahlstrom. Beug was survived by her twin eighteen-year-old daughters Lauren and Lindsey Mayer-Beug, her 13-year-old son, Nick, and her husband, John Beug, a senior vice president in charge of filmed production for Warner Brothers' record division. She was returning home from taking her daughters to college at the Rhode Island School of Design.\n\nAt the National 9/11 Memorial, Beug is memorialized at the North Pool, on Panel N-1.\n\n"}
{"id": "6339", "url": "https://en.wikipedia.org/wiki?curid=6339", "title": "Cell biology", "text": "Cell biology\n\nCell biology (formerly called cytology, from the Greek κυτος, \"kytos\", \"vessel\") is a branch of biology that studies the different structures and functions of the cell and focuses mainly on the idea of the cell as the basic unit of life. Cell biology explains the structure, organization of the organelles they contain, their physiological properties, metabolic processes, Signaling pathways, life cycle, and interactions with their environment. This is done both on a microscopic and molecular level as it encompasses prokaryotic cells and eukaryotic cells. Knowing the components of cells and how cells work is fundamental to all biological sciences; it is also essential for research in bio-medical fields such as cancer, and other diseases. Research in cell biology is closely related to genetics, biochemistry, molecular biology, immunology, and developmental biology.\n\nThe study of the cell is done on a molecular level; however, most of the processes within the cell are made up of a mixture of small organic molecules, inorganic ions, hormones, and water. Approximately 75–85% of the cell's volume is due to water making it an indispensable solvent as a result of its polarity and structure. These molecules within the cell, which operate as substrates, provide a suitable environment for the cell to carry out metabolic reactions and signalling. The cell shape varies among the different types of organisms, and are thus then classified into two categories: eukaryotes and prokaryotes. In the case of eukaryotic cells – which are made up of animal, plant, fungi, and protozoa cells – the shapes are generally round and spherical, while for prokaryotic cells – which are composed of bacteria and archaea – the shapes are: spherical (cocci), rods (bacillus), curved (vibrio), and spirals (\"spirochetes)\". \n\nCell biology focuses more on the study of eukaryotic cells, and their signalling pathways, rather than on prokaryotes which is covered under microbiology. The main constituents of the general molecular composition of the cell includes: proteins and lipids which are either free-flowing or membrane-bound, along with different internal compartments known as organelles. This environment of the cell is made up of hydrophilic and hydrophobic regions which allows for the exchange of the above-mentioned molecules and ions. The hydrophilic regions of the cell are mainly on the inside and outside of the cell, while the hydrophobic regions are within the phospholipid bilayer of the cell membrane. The cell membrane consists of lipids and proteins, which accounts for its hydrophobicity as a result of being non-polar substances. Therefore, in order for these molecules to participate in reactions, within the cell, they need to be able to cross this membrane layer to get into the cell. They accomplish this process of gaining access to the cell via: osmotic pressure, diffusion, concentration gradients, and membrane channels. Inside of the cell are extensive internal sub-cellular membrane-bounded compartments called organelles.\n\n\nThe growth process of the cell does not refer to the size of the cell, but instead the density of the number of cells present in the organism at a given time. Cell growth pertains to the increase in the number of cells present in an organism as it grows and develops; as the organism gets larger so too does the number of cells present. Cells are the foundation of all organisms, they are the fundamental unit of life. The growth and development of the cell are essential for the maintenance of the host, and survival of the organisms. For this process the cell goes through the steps of the cell cycle and development which involves cell growth, DNA replication, cell division, regeneration, specialization, and cell death. The cell cycle is divided into four distinct phases, G1, S, G2, and M. The G phases – which is the cell growth phase – makes up approximately 95% of the cycle. The proliferation of cells is instigated by progenitors, the cells then differentiate to become specialized, where specialized cells of the same type aggregate to form tissues, then organs and ultimately systems. The G phases along with the S phase – DNA replication, damage and repair – are considered to be the interphase portion of the cycle. While the M phase (mitosis and cytokinesis) is the cell division portion of the cycle. The cell cycle is regulated by a series of signalling factors and complexes such as CDK's, kinases, and p53. to name a few. When the cell has completed its growth process, and if it is found to be damaged or altered it undergoes cell death, either by apoptosis or necrosis, to eliminate the threat it causes to the organism's survival.\n\n\nCells may be observed under the microscope, using several different techniques; these include optical microscopy, transmission electron microscopy, scanning electron microscopy, fluorescence microscopy, correlative light-electron microscopy, and confocal microscopy.\n\nThere are several different methods used in the study of cells:\n\nPurification of cells and their parts\nPurification may be performed using the following methods:\n\nPractical job applications for a degree in Cell Molecular Biology includes the following.\n\n\n\n\n\n\n\n\n"}
{"id": "6340", "url": "https://en.wikipedia.org/wiki?curid=6340", "title": "Canadian English", "text": "Canadian English\n\nCanadian English (CanE, CE, en-CA) is the set of varieties of the English language native to Canada. According to the 2011 census, English was the first language of approximately 19 million Canadians, or 57% of the population; the remainder of the population were native speakers of Canadian French (22%) or other languages (allophones, 21%). A larger number, 28 million people, reported using English as their dominant language. 82% of Canadians outside the province of Quebec reported speaking English natively, but within Quebec the figure was just 7.7% as most of its residents are native speakers of Quebec French.\n\nCanadian English contains elements of British English and American English, as well as many Canadianisms (meaning 2): elements \"distinctively characteristic of Canadian usage\". While, broadly speaking, Canadian English varieties tend to be close to American English varieties in terms of linguistic distance, the precise influence of American English, British English and other unique sources on Canadian English varieties has been the ongoing focus of systematic studies since the 1950s.\n\nCanadian and American English are phonologically classified together as North American English, emphasizing the fact that the vast majority of outsiders, even other native English speakers, cannot distinguish the typical accents of Canadian English from American English by sound. There are minor disagreements over the degree to which even Canadians and Americans themselves can differentiate their own two accents, and there is even evidence that some Western American English (Pacific Northwest and California English, for example) is undergoing the Canadian Vowel Shift that was first reported in mainland Canadian English in the early 1990s. The construction of identities and English-language varieties across political borders is a complex social phenomenon.\n\nThe term \"Canadian English\" is first attested in a speech by the Reverend A. Constable Geikie in an address to the Canadian Institute in 1857 (see DCHP-1 Online, s.v \"Canadian English\", Avis et al. 1967). Geikie, a Scottish-born Canadian, reflected the Anglocentric attitude that would be prevalent in Canada for the next hundred years when he referred to the language as \"a corrupt dialect\", in comparison with what he considered the proper English spoken by immigrants from Britain.\n\nCanadian English is the product of five waves of immigration and settlement over a period of more than two centuries. The first large wave of permanent English-speaking settlement in Canada, and linguistically the most important, was the influx of Loyalists fleeing the American Revolution, chiefly from the Mid-Atlantic States – as such, Canadian English is believed by some scholars to have derived from northern American English. The historical development of Canadian English is underexplored, but recent studies suggest that Canadian English has been developing features of its own since the early 19th century, while recent studies have shown the emergence of Canadian English features. The second wave from Britain and Ireland was encouraged to settle in Canada after the War of 1812 by the governors of Canada, who were worried about American dominance and influence among its citizens. Further waves of immigration from around the globe peaked in 1910, 1960 and at the present time had a lesser influence, but they did make Canada a multicultural country, ready to accept linguistic change from around the world during the current period of globalization.\n\nThe languages of Aboriginal peoples in Canada started to influence European languages used in Canada even before widespread settlement took place, and the French of Lower Canada provided vocabulary to the English of Upper Canada.\n\nStudies on earlier forms of English in Canada are rare, yet connections with other work to historical linguistics can be forged. An overview of diachronic work on Canadian English, or diachronically-relevant work, is Dollinger (2012). Until the 2000s, basically all commentators on the history of CanE have argued from the \"language-external\" history, i.e. social and political history (e.g.,). An exception has been in the area of lexis, where Avis et al's (1967) \"Dictionary of Canadianisms on Historical Principles\", offered real-time historical data though its quotations. Recently, historical linguists have started to study earlier Canadian English on historical linguistic data. DCHP-1 is now available in open access.) Most notably, Dollinger (2008) pioneered the historical corpus linguistic approach for English in Canada with CONTE (Corpus of Early Ontario English, 1776-1849) and offers a developmental scenario for 18th and 19th century Ontario. Recently, Reuter (2015), with a 19th-century newspaper corpus from Ontario, has confirmed the scenario laid out in Dollinger (2008).\n\nHistorically, Canadian English included a class-based sociolect known as Canadian dainty. Treated as a marker of upper-class prestige in the 19th century and the early part of the 20th, Canadian dainty was marked by the use of some features of British English pronunciation, resulting in an accent similar to the Mid-Atlantic accent known in the United States. This accent faded in prominence following World War II, when it became stigmatized as pretentious, and is now almost never heard in contemporary Canadian life outside of archival recordings used in film, television or radio documentaries.\n\nCanadian spelling of the English language combines British and American conventions.\n\n\nCanadian spelling conventions can be partly explained by Canada's trade history. For instance, the British spelling of the word \"cheque\" probably relates to Canada's once-important ties to British financial institutions. Canada's automobile industry, on the other hand, has been dominated by American firms from its inception, explaining why Canadians use the American spelling of \"tire\" (hence, \"Canadian Tire\") and American terminology for automobiles and their parts (for example, \"truck\" instead of \"lorry\", \"gasoline\" instead of \"petrol\", \"trunk\" instead of \"boot\").\n\nCanada's political history has also had an influence on Canadian spelling. Canada's first prime minister, John A. Macdonald, once directed the Governor General of Canada to issue an order-in-council directing that government papers be written in the British style.\n\nA contemporary reference for formal Canadian spelling is the spelling used for Hansard transcripts of the Parliament of Canada . Many Canadian editors, though, use the \"Canadian Oxford Dictionary\", often along with the chapter on spelling in \"Editing Canadian English\", and, where necessary (depending on context), one or more other references. \n\nThroughout part of the 20th century, some Canadian newspapers adopted American spellings, for example, \"color\" as opposed to the British-based \"colour\". Some of the most substantial historical spelling data can be found in Dollinger (2010) and Grue (2013). The use of such spellings was the long-standing practice of the Canadian Press perhaps since that news agency's inception, but visibly the norm prior to World War II. The practice of dropping the letter \"u\" in such words was also considered a labour-saving technique during the early days of printing in which movable type was set manually. Canadian newspapers also received much of their international content from American press agencies, therefore it was much easier for editorial staff to leave the spellings from the wire services as provided.\n\nIn the 1990s, Canadian newspapers began to adopt the British spelling variants such as \"-our\" endings, notably with \"The Globe and Mail\" changing its spelling policy in October 1990. Other Canadian newspapers adopted similar changes later that decade, such as the Southam newspaper chain's conversion in September 1998. The \"Toronto Star\" adopted this new spelling policy in September 1997 after that publication's ombudsman discounted the issue earlier in 1997. The \"Star\" had always avoided using recognized Canadian spelling, citing the \"Gage Canadian Dictionary\" in their defence. Controversy around this issue was frequent. When the \"Gage Dictionary\" finally adopted standard Canadian spelling, the \"Star\" followed suit. Some publishers, e.g. \"Maclean's\", continue to prefer American spellings.\n\nThe first Canadian dictionaries of Canadian English were edited by Walter Spencer Avis and published by Gage Ltd. The \"Beginner's Dictionary\" (1962), the \"Intermediate Dictionary\" (1964) and, finally, the \"Senior Dictionary\" (1967) were milestones in Canadian English lexicography. In November 1967, the flagship dictionary, the scholarly A Dictionary of Canadianisms on Historical Principles (Avis et al. 1967) was published and completed the first edition of Gage Publishing's Dictionary of Canadian English Series (online version accessible here, as (Dollinger, Brinton and Fee 2013). Many secondary schools in Canada use the graded dictionaries. The dictionaries have regularly been updated since: the \"Senior Dictionary\" was renamed \"Gage Canadian Dictionary\". Its fifth edition was printed beginning in 1997. Gage was acquired by Thomson Nelson around 2003. The latest editions were published in 2009 by HarperCollins. On 17 March 2017, coinciding with the 57th anniversary of founding editor Charles J. Lovell's passing, a second edition of DCHP was presented on the occasion of Canada's sesquicentennial as DCHP-2, which can be accessed here (as Dollinger & Fee 2017). DCHP-2 incorporates the c. 10,000 lexemes from DCHP-1 and adds c. 1300 novel meanings or 1002 lexemes to the documented lexicon of Canadian English, with a number of methodological innovations (see Dollinger In press [as of June 2017] for a summary of these features).\n\nIn 1997, the \"ITP Nelson Dictionary of the Canadian English Language\" was another product, but has not been updated since.\n\nIn 1998, Oxford University Press produced a Canadian English dictionary, after five years of lexicographical research, entitled \"The Oxford Canadian Dictionary\". A second edition, retitled \"The Canadian Oxford Dictionary\", was published in 2004. Just as the older dictionaries it includes uniquely Canadian words and words borrowed from other languages, and surveyed spellings, such as whether \"colour\" or \"color\" was the more popular choice in common use. Paperback and concise versions (2005, 2006), with minor updates, are available.\n\nThe scholarly \"Dictionary of Canadianisms on Historical Principles\" (\"DCHP\") was first published in 1967 by Gage Ltd. It was a partner project of the \"Senior Dictionary\" (and appeared only a few weeks apart from it). The \"DCHP\" can be considered the \"Canadian OED\", as it documents the historical development of Canadian English words that can be classified as \"Canadianisms\". It therefore includes words such as \"mukluk, Canuck, bluff\" and \"grow op\", but does not list common core words such as \"desk\", \"table\" or \"car\". It is a specialist, scholarly dictionary, but is not without interest to the general public. A digital edition in open access is now available. In 2006, a second edition (DCHP-2)\nwas commenced at UBC in Vancouver, and is/was launched on 17 March 2017 on www.dchp.ca/dchp2. The principles of DCHP-2, which includes frequency information on items and rationales for each term's assessment as a Canadianism (or not) are explained in the chief editor's account. An example of the Frequency charts, which are normalized internet domain searches, is shown on the right-hand side for the example \"chuck wagon\" 'food cart for mobile lunches etc.']).\nIn terms of the major sound systems (phonologies) of English around the world, Canadian English aligns most closely to U.S. English, both being grouped together under a common North American English sound system; the mainstream Canadian accent (\"Standard Canadian\") is often compared to the very similar and largely overlapping \"General American\" accent, an accent widely spoken throughout the United States and perceived there as being relatively lacking in any noticeable regional features.\n\nThe provinces east of Ontario show the largest dialect diversity. Northern Canada is, according to William Labov, a dialect region in formation, and a homogeneous dialect has not yet formed. A very homogeneous dialect exists in Western and Central Canada, a situation that is similar to that of the Western United States. Labov identifies an inland region that concentrates all of the defining features of the dialect centred on the Prairies, with periphery areas with more variable patterns including the metropolitan areas of Vancouver and Toronto. This dialect forms a dialect continuum with the far Western U.S. English, however it is sharply differentiated from the Inland Northern U.S. English of the central and eastern Great Lakes region.\n\nThroughout all of Canada, English mostly has a uniform phonology and very little diversity of dialects compared with the neighbouring English of the United States; the greatly homogeneous single variety spoken particularly in western and central (inland) Canada, as well as across all of Canada among urban middle-class speakers from Anglophone family backgrounds, is commonly referred to as Standard Canadian English. The Standard Canadian English dialect region is defined by the cot–caught merger to or and an accompanying chain shift of vowel sounds, called the Canadian Shift. A subset of this dialect geographically at its central core, excluding British Columbia to the west as well as Montreal and everything else to the east, has been called Inland Canadian English, and is further defined by both of the phenomena known as \"Canadian raising\", the production of and with back starting points in the mouth, and the production of with a front starting point and very little glide (almost ).\n\nAlthough Canadian English phonology is part of the greater North American sound system, and therefore similar to U.S. English phonology, the pronunciation of particular words may have British influence, while other pronunciations are uniquely Canadian. According to the Cambridge History of the English Language, \"[w]hat perhaps most characterizes Canadian speakers, however, is their use of several possible variant pronunciations for the same word, sometimes even in the same sentence.\"\n\nLike most other North American English dialects, Canadian English is almost always spoken with a rhotic accent, meaning that the \"r\" sound is preserved in any environment and not \"dropped\" after vowels, as commonly done by, for example, speakers in central and southern England where it is only pronounced when preceding a vowel.\n\nLike General American, Canadian English possesses a wide range of phonological mergers not found in other major varieties of English: the Mary–marry–merry merger which makes word pairs like \"Barry/berry\", \"Carrie/Kerry\", \"hairy/Harry\", \"perish/parish\", etc. as well as trios like \"airable/errable/arable\" and \"Mary/merry/marry\" have identical pronunciations (however, a distinction between the \"marry\" and \"merry\" sets remains in Montreal); the father–bother merger that makes \"lager/logger\", \"con/Kahn\", etc. sound identical; the very common horse–hoarse merger making pairs like \"for/four\", \"horse/hoarse\", \"morning/mourning\", \"war/wore\" etc. perfect homophones; and the prevalent wine–whine merger which produces homophone pairs like \"Wales/whales\", \"wear/where\", \"wine/whine\" etc. by, in most cases, eliminating (ʍ), except in some older speakers.\n\nIn addition to that, flapping of intervocalic and to alveolar tap before reduced vowels is ubiquitous, so the words \"ladder\" and \"latter\", for example, are mostly or entirely pronounced the same. Therefore, the pronunciation of the word \"British\" in Canada and the U.S. is most often , while in England it is commonly . For some speakers, the merger is incomplete and 't' before a reduced vowel is sometimes not tapped following or when it represents underlying 't'; thus \"greater\" and \"grader\", and \"unbitten\" and \"unbidden\" are distinguished.\n\nMany Canadian speakers have the typical American dropping of after alveolar consonants, so that \"new\", \"duke\", \"Tuesday\", \"suit\", \"resume\", \"lute\", for instance, are pronounced , , , , , .\n\nPerhaps the most recognizable feature of Canadian English is \"Canadian raising,\" which is found most prominently throughout central and west-central Canada, as well as in parts of the Atlantic Provinces. For the beginning points of the diphthongs (gliding vowels) (as in the words \"height\" and \"mice\") and (as in \"shout\" and \"house\"), the tongue is often more \"raised\" in the mouth when these diphthongs come before voiceless consonants, namely , , , , and , in comparison with other varieties of English.\n\nBefore voiceless consonants, becomes . One of the few phonetic variables that divides Canadians regionally is the articulation of the raised allophone of this as well as of ; in Ontario, it tends to have a mid-central or even mid-front articulation, sometimes approaching , while in the West and Maritimes a more retracted sound is heard, closer to . Among some speakers in the Prairies and in Nova Scotia, the retraction is strong enough to cause some tokens of raised to merge with , so that \"couch\" merges with \"coach\", meaning the two sound the same, and \"about\" sounds like \"a boat\"; this is often inaccurately represented as sounding like \"a boot\" for comic effect in American popular culture.\n\nIn General American, \"out\" is typically , but, with slight Canadian raising, it may sound more like , or, with the strong Canadian raising of the Prairies and Nova Scotia, more like . Due to Canadian raising, words like \"height\" and \"hide\" have two different vowel qualities; also, for example, \"house\" as a noun (\"I saw a house\") and \"house\" as a verb (\"Where will you house them tonight?\") have two different vowel qualities: potentially, versus .\n\nEspecially in parts of the Atlantic provinces, some Canadians do not possess the phenomenon of Canadian raising. On the other hand, certain non-Canadian accents demonstrate Canadian raising. In the U.S., this feature can be found in areas near the border and thus in the Upper Midwest, Pacific Northwest, and northeastern New England (e.g. Boston) dialects, though it is much less common than in Canada. The raising of alone, is actually increasing throughout the U.S., and unlike raising of , and is generally not perceived as unusual by people who do not have the raising.\n\nBecause of Canadian raising, many speakers are able to distinguish between words such as \"writer\" and \"rider\" –which can otherwise be impossible, since North American dialects typically turn both intervocalic and into an alveolar flap. Thus \"writer\" and \"rider\" are distinguished solely by their vowel characteristics as determined by Canadian raising: thus, a split between \"rider\" as and \"writer\" possibly as ().\n\nWhen not in a raised position (before voiceless consonants), is fronted to before nasals, and low-central elsewhere.\n\nAlmost all Canadians have the cot–caught merger, which also occurs primarily in the Western U.S., but often elsewhere in the U.S., especially recently. Speakers do not distinguish the vowels (as in \"caught\") and (as in \"cot\"), which merge as either (more common in Western and Maritime Canada) or (more common in Central and Eastern Canada, where it might even be fronted). Speakers with this merger produce these vowels identically, and often fail to hear the difference when speakers who preserve the distinction (for example, speakers of General American and Inland Northern American English) pronounce these vowels. This merger has existed in Canada for several generations.\n\nThis merger creates a hole in the short vowel sub-system and triggers a sound change known as the Canadian Shift, which involves the front lax vowels . The of \"bat\" is lowered and retracted in the direction of (except in some environments, see below). Indeed, is further back in this variety than almost all other North American dialects; the retraction of was independently observed in Vancouver and is more advanced for Ontarians and women than for people from the Prairies or Atlantic Canada and men. Then, and may be lowered (in the direction of and ) and/or retracted; studies actually disagree on the trajectory of the shift. For example, Labov and others (2006) noted a backward and downward movement of in apparent time in all of Canada except the Atlantic Provinces, but no movement of was detected.\n\nTherefore, in Canadian English, the short-\"a\" and the short-\"o\" are shifted in opposite directions to that of the Northern Cities shift, found across the border in the Inland Northern U.S., which is causing these two dialects to diverge: the Canadian short-\"a\" is very similar in quality to the Inland Northern short-\"o\"; for example, the production would be recognized as \"map\" in Canada, but \"mop\" in the Inland North dialect of the U.S.\n\nA notable exception to the merger occurs, in which some speakers over the age of 60, especially in rural areas in the Prairies, may not exhibit the merger.\n\nUnlike in many American English dialects, remains a low-front vowel in most environments in Canadian English. Raising along the front periphery of the vowel space is restricted to two environments – before nasal and voiced velar consonants – and varies regionally even in these. Ontario and Maritime Canadian English commonly show some raising before nasals, though not as extreme as in many U.S. varieties. Much less raising is heard on the Prairies, and some ethnic groups in Montreal show no pre-nasal raising at all. On the other hand, some Prairie speech exhibits raising of before voiced velars ( and ), with an up-glide rather than an in-glide, so that \"bag\" can almost rhyme with \"vague\". For most Canadian speakers, is also realized higher as before /g/.\n\nThe first element of (as in \"start\") tends to be raised. As with Canadian raising, the relative advancement of the raised nucleus is a regional indicator. A striking feature of Atlantic Canadian speech (the Maritimes and Newfoundland) is a nucleus that approaches the front region of the vowel space, accompanied by strong rhoticity, ranging from to . Western Canadian speech has a much more retracted articulation with a longer non-rhotic portion, approaching a mid-back quality, (though there is no tendency toward a merger with ). Articulation of in Ontario is in a position midway between the Atlantic and Western values.\n\nThe vowels , and before may be lowered and laxed to , and , causing pronunciations like , and for \"pair\", \"peer\" and \"pure\" for many speakers.\n\nThe words \"origin, Florida, horrible, quarrel, warren,\" as well as \"tomorrow, sorry, sorrow,\", etc. are all generally use the sound sequence (as in \"gory\"), rather than (as in \"starry\") or . The latter set of words often distinguishes Canadian pronunciation from U.S. pronunciation.\n\nAnother change in progress in Canadian English, part of a continental trend affecting many North American varieties, is the fronting of , whereby the nucleus of moves forward to high-central or even high-front position, directly behind . There is a wide range of allophonic dispersion in the set of words containing (i.e., the GOOSE set), extending over most of the high region of the vowel space. Most advanced are tokens of in free position after coronals (\"do\", \"too\"); behind these are tokens in syllables closed with coronals (\"boots\", \"food\", \"soon\"), then tokens before non-coronals (\"goof\", \"soup\"); remaining in back position are tokens of before (\"cool\", \"pool\", \"tool\"). Unlike in some British speech, Canadian English does not show any fronting or unrounding of the glide of , and most Canadians show no parallel centralization of , which generally remains in back position, except in Cape Breton Island and Newfoundland.\n\nThe word \"milk\" is realized as (to rhyme with \"elk\") by some speakers, (to rhyme with \"ilk\") by others.\n\nTraditionally diphthongal vowels such as (as in \"boat\") and (as in \"bait\") have qualities much closer to pure vowel (monophthongs) in some speakers especially in the inland region.\n\nThe map to the left shows the major regional dialects of Canadian English (each designated in all capital letters), as demarcated primarily by Labov et al.'s \"Atlas of North American English\", as well as the related Telsur Project's regional maps. The broadest regional dialects include:\n\nA study shows that people from Vancouver exhibit more vowel retraction of before nasals than people from Toronto, and this retraction may become a regional marker of West Coast English. Canadian raising (found in words such as \"about\" and \"writer\") is less prominent in BC than other parts of the country and is on the decline further, with many speakers not raising before voiceless consonants. Younger speakers in the Greater Vancouver area do not even raise , causing \"about\" to sound somewhat like \"a boat\". The \"o\" in such words as \"holy, goal, load, know,\" etc. is pronounced as a back and rounded , but not as rounded as in the Prairies where there is a strong Scandinavian, Slavic and German influence.\n\nCanadian raising is quite strong throughout the province of Ontario, except within the Ottawa Valley. The Canadian Shift is also a common vowel shift found in Ontario. The retraction of was found to be more advanced for women in Ontarians than for people from the Prairies or Atlantic Canada and men.\n\nIn Southwestern Ontario (roughly in the line south from Sarnia to St. Catharines), despite the existence of the many characteristics of West/Central Canadian English, many speakers, especially those under 30 speak a dialect which is influenced by the Inland Northern American English dialect found on much of the American regions adjacent to the Great Lakes, though there are minor differences such as Canadian raising (listen to \"ice\" vs \"my\"). Additionally, there is a tendency to round the mouth after pronouncing the vowel \"o\" which is distinct from the General American Accent. Also, the vowel of \"bag\" sounds closer to \"vague\" or \"egg\"; \"right\" sounds like \"rate\"; and the \"ah\" vowel in \"can't\" is drawn out, sounding like \"kee-ant\".\n\nThe subregion of Midwestern Ontario consists of the Counties of Huron, Bruce, Grey, and Perth. The \"Queen's Bush\" as the area was called, did not experience communication with Southwestern and Central dialects until the early 20th century. Thus, a strong accent similar to Central Ontarian is heard, yet many different phrasings exist. It is typical in the area to drop phonetic sounds to make shorter contractions, such as: Prolly (Probably), Goin' (Going), and \"Wuts goin' on tonight? D'ya wanna do sumthin'?\" It is particularly strong in the County of Bruce, so much that it is commonly referred to as being the Bruce Cownian (Bruce Countian) accent. Also 'er' sounds are often pronounced 'air', with \"were\" sounding more like \"wear\".\n\nResidents of the Golden Horseshoe (including the Greater Toronto Area) are known to merge the second with the in \"Toronto\", pronouncing the name variously as , or even or . This, however, is not unique to Toronto as Atlanta is often pronounced \"Atlanna\" by residents. In Toronto and the other areas within the Greater Toronto Area, the \"th\" sound is often pronounced . Sometimes is elided altogether, resulting in \"Do you want this one er'iss one?\" The word \"southern\" is often pronounced with . In the area north of the Regional Municipality of York and south of Parry Sound, notably among those who were born in the surrounding communities, the cutting down of syllables and consonants often heard, e.g. \"probably\" is reduced to \"prolly\", or \"probly\" when used as a response. In Greater Toronto, the diphthong tends to be fronted (as a result the word \"about\" is pronounced as or ‘a-beh-oot’).\n\nThe Greater Toronto Area is diverse linguistically with 44 percent of its people holding a mother tongue other than English. As a result Toronto has a distinct variation from other regions. In Toronto's ethnic communities there are many words that are distinct; many of which come from the city's large Caribbean community. Although only 1.5% of Torontonians speak French, a relatively low proportion of them (56.2%) are native speakers of English, according to the 2006 Census. As a result Toronto shows a more variable speech pattern.\n\nIn Eastern Ontario, Canadian raising is not as strong as it is in the rest of the province. In Prescott and Russell, parts of Stormont-Dundas-Glengarry and Eastern Ottawa, French accents are often mixed with English ones due to the high Franco-Ontarian population there. In Lanark County, Western Ottawa and Leeds-Grenville and the rest of Stormont-Dundas-Glengarry, the accent spoken is nearly identical to that spoken in Central Ontario and the Quinte area. Phrases such as \"got it\" is often pronounced as . Okay is often pronounced as , while \"hello\" is often pronounced as .\n\nA linguistic exclave has also formed in the Ottawa Valley, heavily influenced by original Scottish, Irish, and German settlers, and existing along the Ontario-Quebec boundary, has its own distinct accent known as the Ottawa Valley twang or brogue). Phonetically, the Ottawa Valley twang is characterized by the lack of Canadian raising as well as the cot–caught merger, two common elements of mainstream Canadian English. However, this accent is quite rare in the region today.\n\nEnglish is a minority language in Quebec (with French in the majority), but has many speakers in Montreal, the Eastern Townships and in the Gatineau-Ottawa region. Uniquely, many people in Montreal distinguish between words like \"marry\" versus \"merry\" and \"parish\" versus \"perish\", which are homophones to most other speakers of Canadian English. Quebec also has French influence. A person with English mother tongue and still speaking English as the first language is called an \"Anglophone\" versus a French speaker, or \"Francophone\". Quebec Anglophones generally pronounce French street names in Montreal as French words. \"Pie IX\" Boulevard is pronounced as in French, not as \"pie nine\", but as \"pee-nuff\". On the other hand, Anglophones do pronounce final d's as in \"Bernard\" and \"Bouchard\"; the word \"Montreal\" is pronounced as an English word and \"Rue Lambert-Closse\" is known as \"Clossy Street\".\n\nIn the city of Montreal, especially in some of the western suburbs like Côte-St-Luc and Hampstead, there is a strong Jewish influence in the English spoken in these areas. A large wave of Jewish immigration from Eastern Europe and the former Soviet Union before and after World War II is also evident today. Their English has a strong Yiddish influence; there are some similarities to English spoken in New York. Words used mainly in Quebec and especially in Montreal are: \"stage\" for \"apprenticeship\" or \"internship\", \"copybook\" for a notebook, \"dépanneur\" or \"dep\" for a convenience store, and \"guichet\" for an ABM/ATM. It is also common for Anglophones, particularly of Greek or Italian descent, to use translated French words instead of common English equivalents such as \"open\" and \"close\" for \"on\" and \"off\" or \"Open the lights, please\" for \"Turn on the lights, please\".\n\nMany in the Maritime provinces – Nova Scotia, New Brunswick and Prince Edward Island – have an accent that sounds more like Scottish English and, in some places, Irish English than General American. Outside of major communities, dialects can vary markedly from community to community, as well as from province to province, reflecting ethnic origin as well as a past in which there were few roads and many communities, with some villages very isolated. Into the 1980s, residents of villages in northern Nova Scotia could identify themselves by dialects and accents distinctive to their village. The dialects of Prince Edward Island are often considered the most distinct grouping.\n\nThe phonology of Maritimer English has some unique features:\n\nThe dialect spoken in the province of Newfoundland and Labrador, an autonomous dominion until March 31, 1949, is often considered the most distinctive Canadian English dialect. Some Newfoundland English differs in vowel pronunciation, morphology, syntax, and preservation of archaic adverbial-intensifiers. The dialect can vary markedly from community to community, as well as from region to region, reflecting ethnic origin as well as a past in which there were few roads and many communities, and fishing villages in particular remained very isolated. A few speakers have a transitional pin–pen merger.\n\nFirst Nations and Inuit people from Northern Canada speak a version of Canadian English influenced by the phonology of their first languages. European Canadians in these regions are relatively recent arrivals, and have not produced a dialect that is distinct from southern Canadian English.\n\n\nWhere Canadian English shares vocabulary with other English dialects, it tends to share most with American English, but also has many non-American terms distinctively shared instead with Britain. British and American terms also can coexist in Canadian English to various extents, sometimes with new nuances in meaning; a classic example is \"holiday\" (British) often used interchangeably with \"vacation\" (American), though, in Canadian speech, the latter can more narrowly mean a trip elsewhere and the former can mean general time off work. In addition, the vocabulary of Canadian English also features some words that are seldom (if ever) found elsewhere. A good resource for these and other words is the Dictionary of Canadianisms on Historical Principles, which is currently being revised at the University of British Columbia in Vancouver, British Columbia. The Canadian public appears to take interest in unique \"Canadianisms\": words that are distinctively characteristic of Canadian English—though perhaps not exclusive to Canada; there is some disagreement about the extent to which \"Canadianism\" means a term actually unique to Canada, with such an understanding possibly overstated by the popular media.\nAs a member of the Commonwealth of Nations, Canada shares many items of institutional terminology and professional designations with the countries of the former British Empire – for example, \"constable\", for a police officer of the lowest rank, and \"chartered accountant\".\n\nThe term \"college\", which refers to post-secondary education in general in the U.S., refers in Canada to either a post-secondary technical or vocational institution, or to one of the colleges that exist as federated schools within some Canadian universities. Most often, a \"college\" is a community college, not a university. It may also refer to a CEGEP in Quebec. In Canada, \"college student\" might denote someone obtaining a diploma in business management while \"university student\" is the term for someone earning a bachelor's degree. For that reason, \"going to college\" does not have the same meaning as \"going to university\", unless the speaker or context clarifies the specific level of post-secondary education that is meant.\n\nWithin the public school system the chief administrator of a school is generally \"the principal\", as in the United States, but the term is not used preceding his or her name, i.e. \"Principal Smith\". The assistant to the principal is not titled as \"assistant principal\", but rather as \"vice-principal\", although the former is not unknown. This usage is identical to that in Northern Ireland.\n\nCanadian universities publish \"calendars\" or \"schedules\", not \"catalogs\" as in the U.S.. Canadian students \"write\" or \"take\" exams (in the U.S., students generally \"take\" exams while teachers \"write\" them); they rarely \"sit\" them (standard British usage). Those who supervise students during an exam are sometimes called \"invigilators\" as in Britain, or sometimes \"proctors\" as in the U.S; usage may depend on the region or even the individual institution.\n\nSuccessive years of school are usually referred to as \"grade one\", \"grade two\", and so on. In Quebec, the speaker (if Francophone) will often say \"primary one\", \"primary two\" (a direct translation from the French), and so on; while Anglophones will say \"grade one\", \"grade two\". (Compare American \"first grade, second grade\" (sporadically found in Canada), and English/Welsh \"Year 1, Year 2\", Scottish/Nth.Irish \"Primary 1, Primary 2\" or \"P1, P2\", and Sth.Irish \"First Class, Second Class\" and so on.). The year of school before grade 1 is usually called \"Kindergarten\", with the exception of Nova Scotia, where it is called \"grade primary\".\n\nIn the U.S., the four years of high school are termed the freshman, sophomore, junior, and senior years (terms also used for college years); in Canada, the specific levels are used instead (i.e., \"grade nine\"). As for higher education, only the term \"freshman\" (often reduced to \"frosh\") has some currency in Canada. The American usages \"sophomore\", \"junior\" and \"senior\" are not used in Canadian university terminology, or in speech. The specific high-school grades and university years are therefore stated and individualized; for example, \"the grade 12s failed to graduate\"; \"John is in his second year at McMaster\". The \"first year\", \"third year\" designation also applies to Canadian law school students, as opposed to the common American usage of \"1L\", \"2L\" and \"3L\".\n\nCanadian students use the term \"marks\" (more common in England) or \"grades\" (more common in the US) to refer to their results; usage is very mixed.\n\nUnlike in the United States, use of metric units within a majority of industries (but not all) is standard in Canada, as a result of the national adoption of the metric system during the mid-to-late 1970s; this has spawned some colloquial usages such as \"klick\" for kilometre (as also heard in the U.S. military). Nonetheless, Imperial units are still used in many situations. For example, English Canadians state their weight and height in pounds and feet/inches, respectively. Distances while playing golf are always marked and discussed in yards, though official scorecards may also show metres. Temperatures for cooking are often given in Fahrenheit, while the weather is given in Celsius. Directions in the Prairie provinces are sometimes given using miles, because the country roads generally follow the mile-based grid of the Dominion Land Survey. Canadians measure property, both residential and commercial, in square feet exclusively. Fuel efficiency is less frequently discussed in miles per gallon, more often the metric L/100 km. The Letter paper size of 8.5 inches × 11 inches is used instead of the international and metric A4 size of 210 mm × 297 mm.\n\n\nHowever, \"expressway\" may also refer to a limited-access road that has control of access but has at-grade junctions, railway crossings (for example, the Harbour Expressway in Thunder Bay.) Sometimes the term \"Parkway\" is also used (for example, the Hanlon Parkway in Guelph). In Saskatchewan, the term 'grid road' is used to refer to minor highways or rural roads, usually gravel, referring to the 'grid' upon which they were originally designed. In Quebec, freeways and expressways are called autoroutes.\n\nIn Alberta, the generic \"Trail\" is often used to describe a freeway, expressway or major urban street (for example, Deerfoot Trail, Macleod Trail or Crowchild Trail in Calgary, Yellowhead Trail in Edmonton). The British term \"motorway\" is not used. The American terms \"turnpike\" and \"tollway\" for a toll road are not common. The term \"throughway\" or \"thruway\" was used for first tolled limited-access highways (for example, the Deas Island Throughway, now Highway 99, from Vancouver, BC, to Blaine, Washington, USA or the Saint John Throughway (Highway 1) in Saint John, NB), but this term is not common anymore. In everyday speech, when a particular roadway is not being specified, the term \"highway\" is generally or exclusively used.\n\n\nLawyers in all parts of Canada, except Quebec, which has its own civil law system, are called \"barristers and solicitors\" because any lawyer licensed in any of the common law provinces and territories must pass bar exams for, and is permitted to engage in, both types of legal practice in contrast to other common-law jurisdictions such as England, Wales and Ireland where the two are traditionally separated (i.e., Canada has a fused legal profession). The words \"lawyer\" and \"counsel\" (not \"counsellor\") predominate in everyday contexts; the word \"attorney\" refers to any personal representative. Canadian lawyers generally do not refer to themselves as \"attorneys\", a term which is common in the United States.\n\nThe equivalent of an American \"district attorney\", meaning the barrister representing the state in criminal proceedings, is called a \"crown attorney\" (in Ontario), \"crown counsel\" (in British Columbia), \"crown prosecutor\" or \"the crown\", on account of Canada's status as a constitutional monarchy in which the Crown is the locus of state power.\n\nThe words \"advocate\" and \"notary\" – two distinct professions in Quebec civil law – are used to refer to that province's equivalent of barrister and solicitor, respectively. In Canada's common law provinces and territories, the word \"notary\" means strictly a notary public.\n\nWithin the Canadian legal community itself, the word \"solicitor\" is often used to refer to any Canadian lawyer in general (much like the way the word \"attorney\" is used in the United States to refer to any American lawyer in general). Despite the conceptual distinction between \"barrister\" and \"solicitor\", Canadian court documents would contain a phrase such as \"\"John Smith, \"solicitor\" for the Plaintiff\"\" even though \"John Smith\" may well himself be the barrister who argues the case in court. In a letter introducing him/herself to an opposing lawyer, a Canadian lawyer normally writes something like \"\"I am the \"solicitor\" for Mr. Tom Jones.\"\n\nThe word \"litigator\" is also used by lawyers to refer to a fellow lawyer who specializes in lawsuits even though the more traditional word \"barrister\" is still employed to denote the same specialization.\n\nJudges of Canada's superior courts (which exist at the provincial and territorial levels) are traditionally addressed as \"\"My Lord\"\" or \"\"My Lady\"\", however there are some variances across certain jurisdictions, with some superior court judges preferring the titles \"\"Mister Justice\"\" or \"\"Madam Justice\"\" to \"\"Lordship\"\".\n\nMasters are addressed as \"\"Mr. Master\"\" or simply \"\"Sir.\"\" In British Columbia, masters are addressed as \"\"Your Honour.\"\"\n\nJudges of provincial or inferior courts are traditionally referred to in person as \"\"Your Honour\"\". Judges of the Supreme Court of Canada and of the federal-level courts prefer the use of \"\"Mister/Madam (Chief) Justice\"\". Justices of The Peace are addressed as \"\"Your Worship\"\". \"\"Your Honour\"\" is also the correct form of address for a Lieutenant Governor.\n\nA serious crime is called an indictable offence, while a less-serious crime is called a summary offence. The older words felony and misdemeanour, which are still used in the United States, are not used in Canada's current \"Criminal Code\" (R.S.C. 1985, c. C-46) or by today's Canadian legal system. As noted throughout the \"Criminal Code\", a person accused of a crime is called \"the accused\" and not \"the defendant\", a term used instead in civil lawsuits.\n\nIn Canada, \"visible minority\" refers to a non-aboriginal person or group visibly not one of the majority race in a given population. The term comes from the Canadian Employment Equity Act, which defines such people as \"persons, other than Aboriginal people, who are non-Caucasian in race or non-white in colour.\" The term is used as a demographic category by Statistics Canada. The qualifier \"visible\" is used to distinguish such minorities from the \"invisible\" minorities determined by language (English vs. French) and certain distinctions in religion (Catholics vs. Protestants).\n\nA county in British Columbia means only a regional jurisdiction of the courts and justice system and is not otherwise connected to governance as with counties in other provinces and in the United States. The rough equivalent to \"county\" as used elsewhere is a \"Regional District\".\n\nDistinctive Canadianisms are:\n\nTerms common in Canada, Britain and Ireland but less frequent or nonexistent in the United States are:\n\nThe following are more or less distinctively Canadian:\n\nThe following are common in Canada, but not in the United States or the United Kingdom.\n\n\nA strong Canadian raising exists in the prairie regions together with certain older usages such as \"chesterfield\" and \"front room\" also associated with the Maritimes. Aboriginal Canadians are a larger and more conspicuous population in prairie cities than elsewhere in the country and certain elements of aboriginal speech in English are sometimes to be heard. Similarly, the linguistic legacy, mostly intonation but also speech patterns and syntax, of the Scandinavian, Slavic and German settlers – who are far more numerous and historically important in the Prairies than in Ontario or the Maritimes – can be heard in the general milieu. Again, the large Métis population in Saskatchewan and Manitoba also carries with it certain linguistic traits inherited from French, Aboriginal and Celtic forebears.\nSome terms are derived from immigrant groups or are just local inventions:\n\nIn farming communities with substantial Ukrainian, German or Mennonite populations, accents, sentence structure and vocabulary influenced by these languages is common. These communities are most common in the Saskatchewan Valley region of Saskatchewan and Red River Valley region of Manitoba.\n\nBritish Columbian English has several words still in current use borrowed from the Chinook Jargon although the use of such vocabulary is observably decreasing. The most famous and widely used of these terms are \"skookum\" and \"saltchuck\". However, among young British Columbians, almost no one uses this vocabulary, and only a small percentage is even familiar with the meaning of such words. In the Yukon, \"cheechako\" is used for newcomers or greenhorns.\n\nNorthern Ontario English has several distinct qualities stemming from its large Franco-Ontarian population. As a result several French and English words are used interchangeably. A number of phrases and expressions may also be found in Northern Ontario that are not present in the rest of the province, such as the use of \"camp\" for a summer home where Southern Ontario speakers would idiomatically use cottage.\n\nA \"rubber\" in the U.S. and Canada is slang for a condom; however, in Canada it is sometimes (rarely except for Newfoundland and South Western Ontario) another term for an eraser (as it is in the United Kingdom and Ireland).\n\nThe word \"bum\" can refer either to the buttocks (as in Britain), or, derogatorily, to a homeless person (as in the U.S.). However, the \"buttocks\" sense does not have the indecent character it retains in British use, as it and \"butt\" are commonly used as a polite or childish euphemism for ruder words such as \"arse\" (commonly used in Atlantic Canada and among older people in Ontario and to the west) or \"ass\", or \"mitiss\" (used in the Prairie Provinces, especially in northern and central Saskatchewan; probably originally a Cree loanword). Older Canadians may see \"bum\" as more polite than \"butt\", which before the 1980s was often considered rude.\n\nSimilarly the word \"pissed\" can refer either to being drunk (as in Britain), or being angry (as in the U.S.), though anger is more often said as \"pissed off\", while \"piss drunk\" or \"pissed up\" is said to describe inebriation (though \"piss drunk\" is sometimes also used in the US, especially in the northern states).\n\nOne of the most distinctive Canadian phrases is the spoken interrogation or tag \"eh\". The only usage of \"eh\" exclusive to Canada, according to the \"Canadian Oxford Dictionary\", is for \"ascertaining the comprehension, continued interest, agreement, etc., of the person or persons addressed\" as in, \"It's four kilometres away, eh, so I have to go by bike.\" In that case, \"eh?\" is used to confirm the attention of the listener and to invite a supportive noise such as \"mm\" or \"oh\" or \"okay\". This usage is also common in Queensland, Australia and New Zealand. Other uses of \"eh\" – for instance, in place of \"huh?\" or \"what?\" meaning \"please repeat or say again\" – are also found in parts of the British Isles and Australia. It is common in Northern/Central Ontario, the Maritimes and the Prairie provinces. The word \"eh\" is used quite frequently in the North Central dialect, so a Canadian accent is often perceived in people from North Dakota, Michigan, Minnesota, and Wisconsin.\nThe term \"Canuck\" simply means \"Canadian\" in its demonymic form, and, as a term used even by Canadians themselves, it is not considered derogatory. In the 19th century and early 20th century it tended to refer to French-Canadians, while the only Canadian-built version of the popular World War I-era American Curtiss JN-4 \"Jenny\" training biplane aircraft, the JN-4C, got the \"Canuck\" nickname, 1,260 of which were built. The nickname Janey Canuck was used by Anglophone women's rights writer Emily Murphy in the 1920s and the \"Johnny Canuck\" comic book character of the 1940s. Throughout the 1970s, Canada's winning World Cup men's downhill ski team was called the \"Crazy Canucks\" for their fearlessness on the slopes. It is also the name of the Vancouver Canucks, the National Hockey League team of Vancouver.\n\nThe term \"hoser\", popularized by Bob & Doug McKenzie, typically refers to an uncouth, beer-swilling male and is a euphemism for \"loser\" coming from the earlier days of hockey played on an outdoor rink and the losing team would have to hose down the ice after the game so it refreezes smooth. Bob & Doug also popularized the use of \"Beauty, eh\", another western slang term which may be used in variety of ways. This describes something as being of interest, of note, signals approval or simply draws attention to it. \n\nA \"Newf\" or \"Newfie\" is someone from Newfoundland and Labrador; sometimes considered derogatory. In Newfoundland, the term \"Mainlander\" refers to any Canadian (sometimes American, occasionally Labradorian) not from the island of Newfoundland. \"Mainlander\" is also occasionally used derogatorily.\n\nIn the Maritimes, a \"Caper\" or \"Cape Bretoner\" is someone from Cape Breton Island, a \"Bluenoser\" is someone with a thick, usually southern Nova Scotia accent or as a general term for a Nova Scotian (Including Cape Bretoners), while an \"Islander\" is someone from Prince Edward Island (the same term is used in British Columbia for people from Vancouver Island, or the numerous islands along it). A \"Haligonian\" refers to someone from the city of Halifax.\n\n\nIn 2011, just under 21.5 million Canadians, representing 65% of the population, spoke English most of the time at home, while 58% declared it their mother language. English is the major language everywhere in Canada except Quebec, and most Canadians (85%) can speak English. While English is not the preferred language in Quebec, 36.1% of Québécois can speak English. Nationally, Francophones are five times more likely to speak English than Anglophones are to speak French – 44% and 9% respectively. Only 3.2% of Canada's English-speaking population resides in Quebec—mostly in Montreal.\n\nMore Canadians know how to speak English than speak it at home.\n\nAttitude studies on Canadian English are somewhat rare. A perceptual study on Albertan and Ontarians exists in combination with older literature from the 1970/80s. Sporadic reports can be found in the literature, e.g. on Vancouver English, in which more than 80% believe in a \"Canadian way of speaking\", with those with a university education reporting higher than those without.\n\n\nDollinger, Stefan (2015). The Written Questionnaire in Social Dialectology: History, Theory, Practice. Amsterdam/Philadelphia: Benjamins. The book's examples are exclusive taken from Canadian English and represent one of the more extensive collections of variables for Canadian English.\n\n"}
{"id": "6343", "url": "https://en.wikipedia.org/wiki?curid=6343", "title": "Czech language", "text": "Czech language\n\nCzech (; \"čeština\" ), historically also Bohemian (; \"lingua Bohemica\" in Latin), is a West Slavic language of the Czech–Slovak group, that is extensively influenced by Latin and German. Spoken by over 10 million people, it serves as the official language of the Czech Republic. Czech is closely related to Slovak, to the point of mutual intelligibility to a very high degree.\n\nThe Czecho-Slovak group developed within West Slavic in the high medieval period, and the standardisation of Czech and Slovak within the Czech–Slovak dialect continuum emerges in the early modern period. In the later 18th to mid-19th century, the modern written standard became codified in the context of the Czech National Revival. The main vernacular, known as Common Czech, is based on the vernacular of Prague, but is now spoken throughout most of the Czech Republic. The Moravian dialects spoken in the eastern part of the country are also classified as Czech, although some of their eastern variants are closer to Slovak.\n\nCzech has a moderately-sized phoneme inventory, comprising five vowels (each short or long) and twenty-five consonants (divided into \"hard\", \"neutral\" and \"soft\" categories). Words may contain uncommon (or complicated) consonant clusters, including one consonant represented by the grapheme \"ř\", or lack vowels altogether. Czech uses a simple orthography which phonologists have used as a model.\n\nCzech is a member of the West Slavic sub-branch of the Slavic branch of the Indo-European language family. This branch includes Polish, Kashubian, Upper and Lower Sorbian and Slovak. Slovak is by the closest language genetic neighbor of Czech, followed by Polish and Silesian.\n\nThe West Slavic languages are spoken in Central Europe. Czech is distinguished from other West Slavic languages by a more-restricted distinction between \"hard\" and \"soft\" consonants (see Phonology below).\n\nThe term \"Old Czech\" is applied to the period predating the 16th century, with the earliest records of the high medieval period also classified as \"early Old Czech\", but it is also possible to speak simply about \"Medieval Czech\".\n\nAround the 7th century, the Slavic expansion reached Central Europe, settling on the eastern fringes of the Frankish Empire. The West Slavic polity of Great Moravia formed by the 9th century. The Christianization of Bohemia took place during the 9th and 10th centuries. The diversification of the Czech-Slovak group within West Slavic began around that time, marked among other things by its ephemeral use of the voiced velar fricative consonant (/ɣ/) and consistent stress on the first syllable.\n\nThe Bohemian (Czech) language is first recorded in writing in glosses and short notes during the 12th to 13th centuries. Literary works written in Czech appear in the early 14th century and administrative documents first appear towards the late 14th century. The first complete Bible translation also dates to this period. Old Czech texts, including poetry and cookbooks, were produced outside the university as well.\n\nLiterary activity becomes widespread in the early 15th century in the context of the Bohemian Reformation. Jan Hus contributed significantly to the standardization of Czech orthography, advocated for widespread literacy among Czech commoners (particularly in religion) and made early efforts to model written Czech after the spoken language.\n\nThere was no standardization distinguishing between Czech and Slovak prior to the 15th century. In the 16th century, the division between Czech and Slovak becomes apparent, marking the confessional division between Lutheran Protestants in Slovakia using Czech orthography and Catholics, especially Slovak Jesuits, beginning to use a separate Slovak orthography based on the language of the Trnava region.\n\nThe publication of the Kralice Bible between 1579 and 1593 (the first complete Czech translation of the Bible from the original languages) became very important for standardization of the Czech language in the following centuries.\n\nIn 1615, the Bohemian diet tried to declare Czech to be the only official language of the kingdom. After the Bohemian Revolt (of predominantly Protestant aristocracy) which was defeated by the Habsburgs in 1620, the Protestant intellectuals had to leave the country. This emigration together with other consequences of the Thirty Years' War had a negative impact on the further use of the Czech language. In 1627, Czech and German became official languages of the Kingdom of Bohemia and in the 18th century German became dominant in Bohemia and Moravia, especially among the upper classes.\n\nThe modern standard Czech language originates in standardization efforts of the 18th century. By then the language had developed a literary tradition, and since then it has changed little; journals from that period have no substantial differences from modern standard Czech, and contemporary Czechs can understand them with little difficulty. Changes include the morphological shift of \"í\" to \"ej\" and \"é\" to \"í\" (although \"é\" survives for some uses) and the merging of \"í\" and the former \"ejí\". Sometime before the 18th century, the Czech language abandoned a distinction between phonemic /l/ and /ʎ/ which survives in Slovak.\n\nWith the beginning of the national revival of the mid-18th century, Czech historians began to emphasize their people's accomplishments from the 15th through the 17th centuries, rebelling against the Counter-Reformation (the Habsburg re-catholization efforts which had denigrated Czech and other non-Latin languages). Czech philologists studied sixteenth-century texts, advocating the return of the language to high culture. This period is known as the Czech National Revival (or Renaissance).\n\nDuring the national revival, in 1809 linguist and historian Josef Dobrovský released a German-language grammar of Old Czech entitled \"Ausführliches Lehrgebäude der böhmischen Sprache\" (\"Comprehensive Doctrine of the Bohemian Language\"). Dobrovský had intended his book to be descriptive, and did not think Czech had a realistic chance of returning as a major language. However, Josef Jungmann and other revivalists used Dobrovský's book to advocate for a Czech linguistic revival. Changes during this time included spelling reform (notably, \"í\" in place of the former \"j\" and \"j\" in place of \"g\"), the use of \"t\" (rather than \"ti\") to end infinitive verbs and the non-capitalization of nouns (which had been a late borrowing from German). These changes differentiated Czech from Slovak. Modern scholars disagree about whether the conservative revivalists were motivated by nationalism or considered contemporary spoken Czech unsuitable for formal, widespread use.\n\nAdherence to historical patterns was later relaxed and standard Czech adopted a number of features from Common Czech (a widespread, informal register), such as leaving some proper nouns undeclined. This has resulted in a relatively high level of homogeneity among all varieties of the language.\n\nIn 2005 and 2007, Czech was spoken by about 10 million residents of the Czech Republic. A Eurobarometer survey conducted from January to March 2012 found that the first language of 98 percent of Czech citizens was Czech, the third-highest in the European Union (behind Greece and Hungary).\n\nCzech, the official language of the Czech Republic (a member of the European Union since 2004), is one of the EU's official languages and the 2012 Eurobarometer survey found that Czech was the foreign language most often used in Slovakia. Economist Jonathan van Parys collected data on language knowledge in Europe for the 2012 European Day of Languages. The five countries with the greatest use of Czech were the Czech Republic (98.77 percent), Slovakia (24.86 percent), Portugal (1.93 percent), Poland (0.98 percent) and Germany (0.47 percent).\n\nCzech speakers in Slovakia primarily live in cities. Since it is a recognised minority language in Slovakia, Slovak citizens who speak only Czech may communicate with the government in their language to the extent that Slovak speakers in the Czech Republic may do so.\n\nImmigration of Czechs from Europe to the United States occurred primarily from 1848 to 1914. Czech is a Less Commonly Taught Language in U.S. schools, and is taught at Czech heritage centers. Large communities of Czech Americans live in the states of Texas, Nebraska and Wisconsin. In the 2000 United States Census, Czech was reported as the most-common language spoken at home (besides English) in Valley, Butler and Saunders Counties, Nebraska and Republic County, Kansas. With the exception of Spanish (the non-English language most commonly spoken at home nationwide), Czech was the most-common home language in over a dozen additional counties in Nebraska, Kansas, Texas, North Dakota and Minnesota. As of 2009, 70,500 Americans spoke Czech as their first language (49th place nationwide, behind Turkish and ahead of Swedish).\n\nThe main vernacular is \"Common Czech\", based on the dialect of the Prague region.\nOther Bohemian dialects have become marginalized, while Moravian dialects remain more widespread, with a political movement for Moravian linguistic revival active since the 1990s.\n\nThe main Czech vernacular, spoken primarily near Prague but also throughout the country, is known as Common Czech (\"obecná čeština\"). This is an academic distinction; most Czechs are unaware of the term or associate it with vernacular (or incorrect) Czech. Compared to standard Czech, Common Czech is characterized by simpler inflection patterns and differences in sound distribution.\n\nCommon Czech has become ubiquitous in most parts of the Czech Republic since the later 20th century. It is usually defined as an interdialect used in common speech in Bohemia and western parts of Moravia (by about two thirds of all inhabitants of the Czech Republic). Common Czech is not codified, but some of its elements have become adopted in the written standard. \nSince the second half of the 20th century, Common Czech elements have also been spreading to regions previously unaffected, as a consequence of media influence.\nStandard Czech is still the norm for politicians, businesspeople and other Czechs in formal situations, but Common Czech is gaining ground in journalism and the mass media.\n\nCommon Czech is characterized by quite regular differences from the standard morphology and phonology. These variations are more or less common to all Common Czech dialects:\n\nExample of declension (with the comparison with the standard Czech):\n\n\"mladý člověk – young man/person, mladí lidé – young people, mladý stát – young state, mladá žena – young woman, mladé zvíře – young animal\"\n\nApart from the Common Czech vernacular, there remain a variety of other Bohemian dialect, mostly in marginal rural areas. Dialect use began to weaken in the second half of the 20th century, and by \nthe early 1990s dialect use was stigmatized, associated with the shrinking lower class and used in literature or other media for comedic effect. \nIncreased travel and media availability to dialect-speaking populations has encouraged them to shift to (or add to their own dialect) standard Czech. \nAlthough Czech has received considerable scholarly interest for a Slavic language, this interest has focused primarily on modern standard Czech and historical texts rather than dialects.\n\nThe Czech Statistical Office in 2003 recognized the following Bohemian dialects:\n\nThe Czech dialects spoken in Moravia and Silesia are known as Moravian (\"moravština\"). In the Austro-Hungarian Empire, \"Bohemian-Moravian-Slovak\" was a language citizens could register as speaking (with German, Polish and several others). Of the Czech dialects, only Moravian is distinguished in nationwide surveys by the Czech Statistical Office. As of 2011, 62,908 Czech citizens spoke Moravian as their first language and 45,561 were diglossal (speaking Moravian and standard Czech as first languages).\n\nBeginning in the sixteenth century, some varieties of Czech resembled Slovak; the southeastern Moravian dialects, in particular, are sometimes considered dialects of Slovak rather than Czech. These dialects form a continuum between the Czech and Slovak languages, using the same declension patterns for nouns and pronouns and the same verb conjugations as Slovak.\n\nThe Czech Statistical Office in 2003 recognized the following Moravian dialects:\n\nIn a 1964 textbook on Czech dialectology, Břetislav Koudela used the following sentence to highlight phonetic differences between dialects:\n\nCzech and Slovak have been considered mutually intelligible; speakers of either language can communicate with greater ease than those of any other pair of West Slavic languages. Since the 1993 dissolution of Czechoslovakia, mutual intelligibility has declined for younger speakers, probably because Czech speakers now experience less exposure to Slovak and vice versa.\n\nIn phonetic differences, Czech is characterized by a glottal stop before initial vowels and Slovak by its less-frequent use of long vowels than Czech; however, Slovak has long forms of the consonants \"r\" and \"l\" when they function as vowels. Phonemic differences between the two languages are generally consistent, typical of two dialects of a language. Grammatically, although Czech (unlike Slovak) has a vocative case, both languages share a common syntax.\n\nOne study showed that Czech and Slovak lexicons differed by 80 percent, but this high percentage was found to stem primarily from differing orthographies and slight inconsistencies in morphological formation; Slovak morphology is more regular (when changing from the nominative to the locative case, \"Praha\" becomes \"Praze\" in Czech and \"Prahe\" in Slovak). The two lexicons are generally considered similar, with most differences found in colloquial vocabulary and some scientific terminology. Slovak has slightly more borrowed words than Czech.\n\nThe similarities between Czech and Slovak led to the languages being considered a single language by a group of 19th-century scholars who called themselves \"Czechoslavs\" (\"Čechoslované\"), believing that the peoples were connected in a way which excluded German Bohemians and (to a lesser extent) Hungarians and other Slavs. During the First Czechoslovak Republic (1918–1938), although \"Czechoslovak\" was designated as the republic's official language, both Czech and Slovak written standards were used. Standard written Slovak was partially modeled on literary Czech, and Czech was preferred for some official functions in the Slovak half of the republic. Czech influence on Slovak was protested by Slovak scholars, and when Slovakia broke off from Czechoslovakia in 1938 as the Slovak State (which then aligned with Nazi Germany in World War II), literary Slovak was deliberately distanced from Czech. When the Axis powers lost the war and Czechoslovakia reformed, Slovak developed somewhat on its own (with Czech influence); during the Prague Spring of 1968, Slovak gained independence from (and equality with) Czech, due to the transformation of Czechoslovakia from a unitary state to a federation. Since the dissolution of Czechoslovakia in 1993, \"Czechoslovak\" has referred to improvised pidgins of the languages which have arisen from the decrease in mutual intelligibility.\n\nCzech vocabulary derives primarily from Slavic, Baltic and other Indo-European roots. Although most verbs have Balto-Slavic origins, pronouns, prepositions and some verbs have wider, Indo-European roots. Some loanwords have been restructured by folk etymology to resemble native Czech words (\"hřbitov\", \"graveyard\" and \"listina\", \"list\").\n\nMost Czech loanwords originated in one of two time periods. Earlier loanwords, primarily from German, Greek and Latin, arrived before the Czech National Revival. More recent loanwords derive primarily from English and French, and also from Hebrew, Arabic and Persian. Many Russian loanwords, principally animal names and naval terms, also exist in Czech.\n\nAlthough older German loanwords were colloquial, recent borrowings from other languages are associated with high culture. During the nineteenth century, words with Greek and Latin roots were rejected in favor of those based on older Czech words and common Slavic roots; \"music\" is \"muzyka\" in Polish and \"музыка\" (\"muzyka\") in Russian, but in Czech it is \"hudba\". Some Czech words have been borrowed as loanwords into English and other languages—for example, \"robot\" (from \"robota\", \"labor\") and \"polka\" (from \"polka\", \"Polish woman\" or from \"půlka\" \"half\").\n\nThe modern written standard is directly based on the standardisation during the Czech National Revival in the 1830s, significantly influenced by Josef Jungmann's Czech-German dictionary published during 1834–1839. Jungmann used vocabulary of the Bible of Kralice (1579–1613) period and of the language used by his contemporaries. He borrowed words not present in Czech from other Slavic languages or created neologisms.\n\nCzech contains ten basic vowel phonemes, and three more found only in loanwords. They are , their long counterparts , and three diphthongs, . The latter two diphthongs and the long are exclusive to loanwords. Vowels are never reduced to schwa sounds when unstressed. Each word usually has primary stress on its first syllable, except for enclitics (minor, monosyllabic, unstressed syllables). In all words of more than two syllables, every odd-numbered syllable receives secondary stress. Stress is unrelated to vowel length, and the possibility of stressed short vowels and unstressed long vowels can be confusing to students whose native language combines the features (such as English).\n\nVoiced consonants with unvoiced counterparts are unvoiced at the end of a word, or when they are followed by unvoiced consonants. Czech consonants are categorized as \"hard\", \"neutral\" or \"soft\":\nThis distinction describes the declension patterns of nouns, which is based on the category of a noun's ending consonant. Hard consonants may not be followed by \"i\" or \"í\" in writing, or soft ones by \"y\" or \"ý\" (except in loanwords such as \"kilogram\"). Neutral consonants may take either character. Hard consonants are sometimes known as \"strong\", and soft ones as \"weak\".\n\nThe phoneme represented by the letter \"ř\" (capital \"Ř\") is considered unique to Czech. It represents the raised alveolar non-sonorant trill (IPA: ), a sound somewhere between Czech's \"r\" and \"ž\" (example: ), and is present in \"Dvořák\".\n\nThe consonants can be syllabic, acting as syllable nuclei in place of a vowel. This can be difficult for non-native speakers to pronounce, and \"Strč prst skrz krk\" (\"Stick [your] finger down [your] throat\") is a Czech tongue twister.\n\nConsonants<br>\nVowels<br>\n\nSlavic grammar is fusional; its nouns, verbs, and adjectives are inflected by phonological processes to modify their meanings and grammatical functions, and the easily separable affixes characteristic of agglutinative languages are limited. \nSlavic-language inflection is complex and pervasive, inflecting for case, gender and number in nouns and tense, aspect, mood, person and subject number and gender in verbs.\n\nParts of speech include adjectives, adverbs, numbers, interrogative words, prepositions, conjunctions and interjections. Adverbs are primarily formed by taking the final \"ý\" or \"í\" of an adjective and replacing it with \"e\", \"ě\", or \"o\". Negative statements are formed by adding the affix \"ne-\" to the verb of a clause, with one exception: \"je\" (he, she or it is) becomes \"není\".\n\nBecause Czech uses grammatical case to convey word function in a sentence (instead of relying on word order, as English does), its word order is flexible. As a pro-drop language, in Czech an intransitive sentence can consist of only a verb; information about its subject is encoded in the verb. Enclitics (primarily auxiliary verbs and pronouns) must appear in the second syntactic slot of a sentence, after the first stressed unit. The first slot must contain a subject and object, a main form of a verb, an adverb or a conjunction (except for the light conjunctions \"a\", \"and\", \"i\", \"and even\" or \"ale\", \"but\").\n\nCzech syntax has a subject–verb–object sentence structure. In practice, however, word order is flexible and used for topicalization and focus. Although Czech has a periphrastic passive construction (like English), colloquial word-order changes frequently produce the passive voice. For example, to change \"Peter killed\nPaul\" to \"Paul was killed by Peter\" the order of subject and object is inverted: \"Petr zabil Pavla\" (\"Peter killed Paul\") becomes \"Paul, Peter killed\" (\"Pavla zabil Petr\"). \"Pavla\" is in the accusative case, the grammatical object (in this case, the victim) of the verb.\nA word at the end of a clause is typically emphasized, unless an upward intonation indicates that the sentence is a question:\nIn portions of Bohemia (including Prague), questions such as \"Jí pes bagetu?\" without an interrogative word (such as \"co\", \"what\" or \"kdo\", \"who\") are intoned in a slow rise from low to high, quickly dropping to low on the last word or phrase.\n\nIn Czech syntax, adjectives precede nouns. Relative clauses are introduced by relativizers such as the adjective \"který\", analogous to the English relative pronouns \"which\", \"that\", \"who\" and \"whom\". As with other adjectives, it is declined into the appropriate case (see Declension below) to match its associated noun, person and number. Relative clauses follow the noun they modify, and the following is a glossed example:\nEnglish: I want to visit the university that John attends.\n\nIn Czech, nouns and adjectives are declined into one of seven grammatical cases. Nouns are inflected to indicate their use in a sentence. A nominative–accusative language, Czech marks subject nouns with nominative case and object nouns with accusative case. The genitive case marks possessive nouns and some types of movement. The remaining cases (instrumental, locative, vocative and dative) indicate semantic relationships, such as secondary objects, movement or position (dative case) and accompaniment (instrumental case). An adjective's case agrees with that of the noun it describes. When Czech children learn their language's declension patterns, the cases are referred to by number: \n\nSome Czech grammatical texts order the cases differently, grouping the nominative and accusative (and the dative and locative) together because those declension patterns are often identical; this order accommodates learners with experience in other inflected languages, such as Latin or Russian. This order is nominative, accusative, genitive, dative, locative, instrumental and vocative.\n\nSome prepositions require the nouns they modify to take a particular case. The cases assigned by each preposition are based on the physical (or metaphorical) direction, or location, conveyed by it. For example, \"od\" (from, away from) and \"z\" (out of, off) assign the genitive case. Other prepositions take one of several cases, with their meaning dependent on the case; \"na\" means \"onto\" or \"for\" with the accusative case, but \"on\" with the locative.\n\nExamples of declension patterns (using prepositions) for a few nouns with adjectives follow. Only one plural example is given, since plural declension patterns are similar across genders.\n\nThis is a glossed example of a sentence using several cases:\n\n\"English:\" I carried the box into the house with my friend.\n\nCzech distinguishes three genders—masculine, feminine, and neuter—and the masculine gender is subdivided into animate and inanimate. With few exceptions, feminine nouns in the nominative case end in \"-a\", \"-e\", or \"-ost\"; neuter nouns in \"-o\", \"-e\", or \"-í\", and masculine nouns in a consonant. Adjectives agree in gender and animacy (for masculine nouns in the accusative or genitive singular and the nominative plural) with the nouns they modify. The main effect of gender in Czech is the difference in noun and adjective declension, but other effects include past-tense verb endings: for example, \"dělal\" (he did, or made); \"dělala\" (she did, or made) and \"dělalo\" (it did, or made).\n\nNouns are also inflected for number, distinguishing between singular and plural. Typical of a Slavic language, Czech cardinal numbers one through four allow the nouns and adjectives they modify to take any case, but numbers over five place these nouns and adjectives in the genitive case when the entire expression is in nominative or accusative case. The Czech koruna is an example of this feature; it is shown here as the subject of a hypothetical sentence, and declined as genitive for numbers five and up.\nNumerical words decline for case and, for numbers one and two, for gender. Numbers one through five are shown below as examples, and have some of the most exceptions among Czech numbers. The number one has declension patterns identical to those of the demonstrative pronoun, \"to\".\nAlthough Czech's grammatical numbers are singular and plural, several residuals of dual forms remain. Some nouns for paired body parts use a historical dual form to express plural in some cases: \"ruka\" (hand)—\"ruce\" (nominative); \"noha\" (leg)—\"nohama\" (instrumental), \"nohou\" (genitive/locative); \"oko\" (eye)—\"oči\", and \"ucho\" (ear)—\"uši\". While two of these nouns are neuter in their singular forms, all plural forms are considered feminine; their gender is relevant to their associated adjectives and verbs. These forms are plural semantically, used for any non-singular count, as in \"mezi čtyřma očima\" (face to face, lit. \"among four eyes\"). The plural number paradigms of these nouns are actually a mixture of historical dual and plural forms. For example, \"nohy\" (legs; nominative/accusative) is a standard plural form of this type of noun.\n\nCzech verb conjugation is less complex than noun and adjective declension because it codes for fewer categories. Verbs agree with their subjects in person (first, second or third) and number (singular or plural), and are conjugated for tense (past, present or future). For example, the conjugated verb \"mluvíme\" (we speak) is in the present tense and first-person plural; it is distinguished from other conjugations of the infinitive \"mluvit\" by its ending, \"me\".\nTypical of Slavic languages, Czech marks its verbs for one of two grammatical aspects: perfective and imperfective. Most verbs are part of inflected aspect pairs—for example, \"koupit\" (perfective) and \"kupovat\" (imperfective). Although the verbs' meaning is similar, in perfective verbs the action is completed and in imperfective verbs it is ongoing. This is distinct from past and present tense, and any Czech verb of either aspect can be conjugated into any of its three tenses. Aspect describes the state of the action at the time specified by the tense.\n\nThe verbs of most aspect pairs differ in one of two ways: by prefix or by suffix. In prefix pairs, the perfective verb has an added prefix—for example, the imperfective \"psát\" (to write, to be writing) compared with the perfective \"napsat\" (to write down, to finish writing). The most common prefixes are \"na-\", \"o-\", \"po-\", \"s-\", \"u-\", \"vy-\", \"z-\" and \"za-\". In suffix pairs, a different infinitive ending is added to the perfective stem; for example, the perfective verbs \"koupit\" (to buy) and \"prodat\" (to sell) have the imperfective forms \"kupovat\" and \"prodávat\". Imperfective verbs may undergo further morphology to make other imperfective verbs (iterative and frequentative forms), denoting repeated or regular action. The verb \"jít\" (to go) has the iterative form \"chodit\" (to go repeatedly) and the frequentative form \"chodívat\" (to go regularly).\n\nMany verbs have only one aspect, and verbs describing continual states of being—\"být\" (to be), \"chtít\" (to want), \"moct\" (to be able to), \"ležet\" (to lie down, to be lying down)—have no perfective form. Conversely, verbs describing immediate states of change—for example, \"otěhotnět\" (to become pregnant) and \"nadchnout se\" (to become enthusiastic)—have no imperfective aspect.\n\nAlthough Czech's use of present and future tense is largely similar to that of English, the language uses past tense to represent the English present perfect and past perfect; \"ona běžela\" could mean \"she ran\", \"she has run\" or \"she had run\".\n\nIn some contexts, Czech's perfective present (which differs from the English present perfect) implies future action; in others, it connotes habitual action. As a result, the language has a proper future tense to minimize ambiguity. The future tense does not involve conjugating the verb describing an action to be undertaken in the future; instead, the future form of \"být\" (as shown in the table at left) is placed before the infinitive (for example, \"budu jíst\"—\"I will eat\").\n\nThis conjugation is not followed by \"být\" itself, so future-oriented expressions involving nouns, adjectives, or prepositions (rather than verbs) omit \"být\". \"I will be happy\" is translated as \"Budu šťastný\" (not \"Budu být šťastný\").\n\nThe infinitive form ends in \"t\" (archaically, \"ti\"). It is the form found in dictionaries and the form that follows auxiliary verbs (for example, \"můžu tě slyšet\"—\"I can \"hear\" you\"). Czech verbs have three grammatical moods: indicative, imperative and conditional. The imperative mood adds specific endings for each of three person (or number) categories: \"-Ø/-i/-ej\" for second-person singular, \"-te/-ete/-ejte\" for second-person plural and \"-me/-eme/-ejme\" for first-person plural. The conditional mood is formed with a particle after the past-tense verb. This mood indicates possible events, expressed in English as \"I would\" or \"I wish\".\nMost Czech verbs fall into one of five classes, which determine their conjugation patterns. The future tense of \"být\" would be classified as a Class I verb because of its endings. Examples of the present tense of each class and some common irregular verbs follow in the tables below:\n\nCzech has one of the most phonemic orthographies of all European languages. Its thirty-one graphemes represent thirty sounds (in most dialects, \"i\" and \"y\" have the same sound), and it contains only one digraph: \"ch\", which follows \"h\" in the alphabet. As a result, some of its characters have been used by phonologists to denote corresponding sounds in other languages. The characters \"q\", \"w\" and \"x\" appear only in foreign words. The háček (ˇ) is used with certain letters to form new characters: \"š\", \"ž\", and \"č\", as well as \"ň\", \"ě\", \"ř\", \"ť\", and \"ď\" (the latter five uncommon outside Czech). The last two letters are sometimes written with a comma above (ʼ, an abbreviated háček) because of their height. The character \"ó\" exists only in loanwords and onomatopoeia.\n\nUnlike most European languages, Czech distinguishes vowel length; long vowels are indicated by an acute accent or, occasionally with \"ů\", a ring. Long \"u\" is usually written \"ú\" at the beginning of a word or morpheme (\"úroda\", \"neúrodný\") and \"ů\" elsewhere, except for loanwords (\"skútr\") or onomatopoeia (\"bú\"). Long vowels and \"ě\" are not considered separate letters.\n\nCzech typographical features not associated with phonetics generally resemble those of most Latin European languages, including English. Proper nouns, honorifics, and the first letters of quotations are capitalized, and punctuation is typical of other Latin European languages. Writing of ordinal numerals is similar to most European languages. The Czech language uses a decimal comma instead of a decimal point. When writing a long number, spaces between every three numbers (e.g. between hundreds and thousands) may be used for better orientation in handwritten texts, but not in decimal places, like in English. The number 1,234,567.8910 may be written as 1234567,8910 or 1 234 567,8910. Ordinal numbers (1st) use a point as in German (1.). In proper noun phrases (except personal names), only the first word is capitalized (\"Pražský hrad\", Prague Castle).\n\nAccording to Article 1 of the United Nations Universal Declaration of Human Rights:\n\nCzech: \"Všichni lidé se rodí svobodní a sobě rovní co do důstojnosti a práv. Jsou nadáni rozumem a svědomím a mají spolu jednat v duchu bratrství.\"\n\nEnglish: \"All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood.\"\n\n\n\n"}
{"id": "6344", "url": "https://en.wikipedia.org/wiki?curid=6344", "title": "Capsid", "text": "Capsid\n\nA capsid is the protein shell of a virus. It consists of several oligomeric structural subunits made of protein called protomers. The observable 3-dimensional morphological subunits, which may or may not correspond to individual proteins, are called capsomeres. The capsid encloses the genetic material of the virus.\n\nCapsids are broadly classified according to their structure. The majority of viruses have capsids with either helical or icosahedral structure. Some viruses, such as bacteriophages, have developed more complicated structures due to constraints of elasticity and electrostatics. The icosahedral shape, which has 20 equilateral triangular faces, approximates a sphere, while the helical shape resembles the shape of a spring, taking the space of a cylinder but not being a cylinder itself. The capsid faces may consist of one or more proteins. For example, the foot-and-mouth disease virus capsid has faces consisting of three proteins named VP1–3.\n\nSome viruses are \"enveloped\", meaning that the capsid is coated with a lipid membrane known as the \"viral envelope\". The envelope is acquired by the capsid from an intracellular membrane in the virus' host; examples include the inner nuclear membrane, the golgi membrane, and the cell's outer membrane.\n\nOnce the virus has infected a cell and begins replicating itself, new capsid subunits are synthesized according to the genetic material of the virus, using the protein biosynthesis mechanism of the cell. During the assembly process, a portal subunit is assembled at one vertex of the capsid. Through this portal, viral DNA or RNA is transported into the capsid.\n\nStructural analyses of major capsid protein (MCP) architectures have been used to categorise viruses into families. For example, the bacteriophage PRD1, Paramecium bursaria Chlorella algal virus, and mammalian adenovirus have been placed in the same family.\n\nAlthough the icosahedral structure is extremely common among viruses, size differences and slight variations exist between virions. Given an asymmetric subunit on a triangular face of a regular icosahedron, with three subunits per face 60 such subunits can be placed in an equivalent manner. Most virions, because of their size, have more than 60 subunits. These variations have been classified on the basis of the quasi-equivalence principle proposed by Donald Caspar and Aaron Klug.\n\nAn icosahedral structure can be regarded as being constructed from 12 pentamers. The number of pentamers is fixed but the number of hexamers can vary. These shells can be constructed from pentamers and hexamers by minimizing the number T (triangulation number) of nonequivalent locations that subunits occupy, with the T-number adopting the particular integer values 1, 3, 4, 7, 12, 13...(T = h + k + hk, with h, k equal to nonnegative integers). These shells always contain 12 pentamers plus 10 (T-1) hexamers. Although this classification can be applied to the majority of known viruses exceptions are known including the retroviruses where point mutations disrupt the symmetry.\n\nThis is an icosahedron elongated along the fivefold axis and is a common arrangement of the heads of bacteriophages. Such a structure is composed of a cylinder with a cap at either end. The cylinder is composed of 10 triangles. The Q number, which can be any positive integer, specifies the number of triangles, composed of asymmetric subunits, that make up the 10 triangles of the cylinder. The caps are classified by the T number. An elongated, stretched-out, shperical capsid is characterisitc of the prolate shape.\n\nMany rod-shaped and filamentous plant viruses have capsids with helical symmetry. The helical structure can be described as a set of n 1-D molecular helices related by an n-fold axial symmetry. The helical transformation are classified into two categories: one-dimensional and two-dimensional helical systems. Creating an entire helical structure relies on a set of translational and rotational matrices which are coded in the protein data bank. Helical symmetry is given by the formula P=μ x ρ, where μ is the number of structural units per turn of the helix, ρ is the axial rise per unit and P is the pitch of the helix. The structure is said to be open due to the characteristic that any volume can be enclosed by varying the length of the helix. The most understood helical virus is the tobacco mosaic virus. The virus is a single molecule of (+) strand RNA. Each coat protein on the interior of the helix bind three nucleotides of the RNA genome. Influenza A viruses differ by comprising multiple ribonucleoproteins, the viral NP protein organizes the RNA into a helical structure. The size is also different the tobacco mosaic virus has a 16.33 protein subunits per helical turn, while the influenza A virus has a 28 amino acid tail loop.\n\nIcosahedral virus capsids are typically assigned a triangulation number (T-number) to describe the relation between the number of pentagons and hexagons \"i.e.\" their quasi-symmetry in the capsid shell. The T-number idea was originally developed to explain the quasi-symmetry by Caspar and Klug in 1962.\n\nFor example, a purely dodecahedral virus has a T-number of 1 (usually written, T=1) and a truncated icosahedron is assigned T=3. The T-number is calculated by (1) applying a grid to the surface of the virus with coordinates \"h\" and \"k\", (2) counting the number of steps between successive pentagons on the virus surface, (3) applying the formula:\n\nwhere formula_3 and \"h\" and \"k\" are the distances between the successive pentagons on the virus surface for each axis (see figure on right). The larger the T-number the more hexagons are present relative to the pentagons.\n\nFor the hexagonal system, the polyhedra have 20\"T\" vertices, 30\"T\" edges, 10\"T\"+2 faces (12 pentagons and 10(\"T\"-1) hexagons). For the dual triangular, the vertex and face counts are flipped.\n\nT-numbers can be represented in different ways, for example T=1 can only be represented as an icosahedron or a dodecahedron and, depending on the type of quasi-symmetry, T= 3 can be presented as a truncated dodecahedron, an icosidodecahedron, or a truncated icosahedron and their respective duals a triakis icosahedron, a rhombic triacontahedron, or a pentakis dodecahedron.\n\nThe functions of the virion are to protect the genome, deliver the genome and interact with the host. The virion must assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents. These include forms of natural radiation, extremes of pH or temperature and proteolytic and nucleolytic enzymes. Delivery of the genome is also important by specific binding to external receptors of the host cell, transmission of specific signals that induce uncoating of the genome, and induction of fusion with host cell membranes.\n\nThe viral particle must be metastable so that interactions can be reversed readily when entering and uncoating a new host cell. If it attains the minimum free energy state conformation will be irreversible associated with attachment and entry. Each subunit of the capsid has identical bonding contacts with its neighbors, and the two binding contacts are usually noncovalent. The non-covalent bonding holds the structural unit together. The reversible formation of non-covalent bonds between properly folded subunits leads to error-free assembly and minimizes free energy.\n\nIt has been suggested that many viral capsid proteins have evolved on multiple occasions from functionally diverse cellular proteins. The recruitment of cellular proteins has apparently occurred at different stages of evolution, so that some cellular proteins were captured and refunctionalized prior to the divergence of cellular organisms into the three contemporary domains of life, whereas others were hijacked relatively recently. As a result, some capsid proteins are widespread in viruses infecting distantly related organisms (e.g., capsid proteins with the jelly-roll fold), whereas others are restricted to a particular group of viruses (e.g., capsid proteins of alphaviruses).\n\n\n"}
{"id": "6346", "url": "https://en.wikipedia.org/wiki?curid=6346", "title": "Chloramphenicol", "text": "Chloramphenicol\n\nChloramphenicol is an antibiotic useful for the treatment of a number of bacterial infections. This includes as an eye ointment to treat conjunctivitis. By mouth or by injection into a vein, it is used to treat meningitis, plague, cholera, and typhoid fever. Its use by mouth or by injection is only recommended when safer antibiotics cannot be used and if used, monitoring both blood levels of the medication and blood cell levels every two days is recommended during treatment.\nCommon side effects include bone marrow suppression, nausea, and diarrhea. The bone marrow suppression may result in death. To reduce the risk of side effects treatment duration should be as short as possible. People with liver or kidney problems may need lower doses. In young children a condition known as gray baby syndrome may occur which results in a swollen stomach and low blood pressure. Its use near the end of pregnancy and during breastfeeding is typically not recommended. Chloramphenicol is a broad-spectrum antibiotic that typically stops bacterial growth by stopping the production of proteins.\nChloramphenicol was discovered in 1947. It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. It is available as a generic medication. The wholesale cost in the developing world of an intravenous dose is about 0.40 to 1.90 USD. In the United States an intravenous dose costs about 41.47 USD. Global issues relating to bacterial resistance have revived interest in its use.\n\nThe original indication of chloramphenicol was in the treatment of typhoid, but the now almost universal presence of multiple drug-resistant \"Salmonella typhi\" has meant it is seldom used for this indication except when the organism is known to be sensitive. Chloramphenicol may be used as a second-line agent in the treatment of tetracycline-resistant cholera.\n\nBecause of its excellent blood-brain barrier penetration (far superior to any of the cephalosporins), chloramphenicol remains the first-choice treatment for staphylococcal brain abscesses. It is also useful in the treatment of brain abscesses due to mixed organisms or when the causative organism is not known.\n\nChloramphenicol is active against the three main bacterial causes of meningitis: \"Neisseria meningitidis\", \"Streptococcus pneumoniae\", and \"Haemophilus influenzae\". In the West, chloramphenicol remains the drug of choice in the treatment of meningitis in patients with severe penicillin or cephalosporin allergy and general practitioners are recommended to carry intravenous chloramphenicol in their bag. In low-income countries, the WHO no longer recommends oily chloramphenicol as first-line to treat meningitis, but recognises it may be used with caution if there are no available alternatives.\n\nChloramphenicol has been used in the U.S. in the initial empirical treatment of children with fever and a petechial rash, when the differential diagnosis includes both \"Neisseria meningitidis\" sepsis and Rocky Mountain spotted fever, pending the results of diagnostic investigations.\n\nChloramphenicol is also effective against \"Enterococcus faecium\", which has led to its being considered for treatment of vancomycin-resistant enterococcus.\n\nIn the context of preventing endophthalmitis, a complication of cataract surgery, a 2017 systematic review found moderate evidence that using chloramphenicol eye drops in addition to an antibiotic injection (cefuroxime or penicillin) will likely lower the risk of endophthalmitis, compared to eye drops or antibiotic injections alone.\n\nChloramphenicol has a broad spectrum of activity and has been effective in treating ocular infections caused by a number of bacteria including \"Staphylococcus aureus, Streptococcus pneumoniae\", and \"Escherichia coli\". It is not effective against \"Pseudomonas aeruginosa\". The following susceptibility data represent the minimum inhibitory concentration for a few medically significant organisms.\nEach of these concentrations is dependent upon the bacterial strain being targeted. Some strains of \"E. coli\", for example, show spontaneous emergence of chloramphenicol resistance.\n\nThree mechanisms of resistance to chloramphenicol are known: reduced membrane permeability, mutation of the 50S ribosomal subunit, and elaboration of chloramphenicol acetyltransferase. It is easy to select for reduced membrane permeability to chloramphenicol \"in vitro\" by serial passage of bacteria, and this is the most common mechanism of low-level chloramphenicol resistance. High-level resistance is conferred by the \"cat\"-gene; this gene codes for an enzyme called chloramphenicol acetyltransferase, which inactivates chloramphenicol by covalently linking one or two acetyl groups, derived from acetyl-S-coenzyme A, to the hydroxyl groups on the chloramphenicol molecule. The acetylation prevents chloramphenicol from binding to the ribosome. Resistance-conferring mutations of the 50S ribosomal subunit are rare.\n\nChloramphenicol resistance may be carried on a plasmid that also codes for resistance to other drugs. One example is the ACCoT plasmid (A=ampicillin, C=chloramphenicol, Co=co-trimoxazole, T=tetracycline), which mediates multiple-drug resistance in typhoid (also called R factors).\n\nCurrently, some \"Enterococcus faecium\" and\" Pseudomonas aeruginosa\" strains are resistant to chloramphenicol. Some \"Veillonella\" spp. and \"Staphylococcus capitis\" strains have also developed resistance to chloramphenicol to varying degrees.\n\nThe most serious side effect of chloramphenicol treatment is aplastic anaemia. This effect is rare and sometimes fatal. The risk of AA is high enough that alternatives should be strongly considered. Treatments are available but expensive. No way exists to predict who may or may not get this side effect. The effect usually occurs weeks or months after treatment has been stopped, and a genetic predisposition may be involved. It is not known whether monitoring the blood counts of patients can prevent the development of aplastic anaemia, but patients are recommended to have a baseline blood count with a repeat blood count every few days while on treatment. Chloramphenicol should be discontinued if the complete blood count drops below 2.5 x 10 cells/l. The highest risk is with oral chloramphenicol (affecting 1 in 24,000–40,000) and the lowest risk occurs with eye drops (affecting less than one in 224,716 prescriptions).\n\nThiamphenicol, a related compound with a similar spectrum of activity, is available in Italy and China for human use, and has never been associated with aplastic anaemia. Thiamphenicol is available in the U.S. and Europe as a veterinary antibiotic, but is not approved for use in humans.\n\nChloramphenicol may cause bone marrow suppression during treatment; this is a direct toxic effect of the drug on human mitochondria. This effect manifests first as a fall in hemoglobin levels, which occurs quite predictably once a cumulative dose of 20 g has been given. The anaemia is fully reversible once the drug is stopped and does not predict future development of aplastic anaemia. Studies in mice have suggested existing marrow damage may compound any marrow damage resulting from the toxic effects of chloramphenicol.\n\nLeukemia, a cancer of the blood or bone marrow, is characterized by an abnormal increase of immature white blood cells. The risk of childhood leukemia is increased, as demonstrated in a Chinese case-controlled study, and the risk increases with length of treatment.\n\nIntravenous chloramphenicol use has been associated with the so-called gray baby syndrome.\nThis phenomenon occurs in newborn infants because they do not yet have fully functional liver enzymes (i.e. UDP-glucuronyl transferase), so chloramphenicol remains unmetabolized in the body.\nThis causes several adverse effects, including hypotension and cyanosis. The condition can be prevented by using the drug at the recommended doses, and monitoring blood levels.\n\nFever, macular and vesicular rashes, angioedema, urticaria, and anaphylaxis may occur. Herxheimer’s reactions have occurred during therapy for typhoid fever.\n\nHeadache, mild depression, mental confusion, and delirium have been described in patients receiving chloramphenicol. Optic and peripheral neuritis have been reported, usually following long-term therapy. If this occurs, the drug should be promptly withdrawn.\n\nChloramphenicol is extremely lipid-soluble; it remains relatively unbound to protein and is a small molecule. It has a large apparent volume of distribution and penetrates effectively into all tissues of the body, including the brain. Distribution is not uniform, with highest concentrations found in the liver and kidney, with lowest in the brain and cerebrospinal fluid. The concentration achieved in brain and cerebrospinal fluid is around 30 to 50% of the overall average body concentration, even when the meninges are not inflamed; this increases to as high as 89% when the meninges are inflamed.\n\nChloramphenicol increases the absorption of iron.\n\nChloramphenicol is metabolized by the liver to chloramphenicol glucuronate (which is inactive). In liver impairment, the dose of chloramphenicol must therefore be reduced. No standard dose reduction exists for chloramphenicol in liver impairment, and the dose should be adjusted according to measured plasma concentrations.\n\nThe majority of the chloramphenicol dose is excreted by the kidneys as the inactive metabolite, chloramphenicol glucuronate. Only a tiny fraction of the chloramphenicol is excreted by the kidneys unchanged. Plasma levels should be monitored in patients with renal impairment, but this is not mandatory. Chloramphenicol succinate ester (an intravenous prodrug form) is readily excreted unchanged by the kidneys, more so than chloramphenicol base, and this is the major reason why levels of chloramphenicol in the blood are much lower when given intravenously than orally.\n\nChloramphenicol passes into breast milk, so should therefore be avoided during breast feeding, if possible.\n\nPlasma levels of chloramphenicol must be monitored in neonates and patients with abnormal liver function. Plasma levels should be monitored in all children under the age of four, the elderly, and patients with renal failure.\nBecause efficacy and toxicity of chloramphenicol are associated with a maximum serum concentration, peak levels (one hour after the intravenous dose is given) should be 10-20 µg/ml with toxicity ; trough levels (taken immediately before a dose) should be 5-10 µg/ml.\n\nAdministration of chloramphenicol concomitantly with bone marrow depressant drugs is contraindicated, although concerns over aplastic anaemia associated with ocular chloramphenicol have largely been discounted.\n\nChloramphenicol is a potent inhibitor of the cytochrome P450 isoforms CYP2C19 and CYP3A4 in the liver. Inhibition of CYP2C19 causes decreased metabolism and therefore increased levels of, for example, antidepressants, antiepileptics, proton pump inhibitors, and anticoagulants if they are given concomitantly. Inhibition of CYP3A4 causes increased levels of, for example, calcium channel blockers, immunosuppressants, chemotherapeutic drugs, benzodiazepines, azole antifungals, tricyclic antidepressants, macrolide antibiotics, SSRIs, statins, cardiac antiarrhythmics, antivirals, anticoagulants, and PDE5 inhibitors.\n\nChloramphenicol is antagonistic with most cephalosporins and using both together should be avoided in the treatment of infections.\n\nChloramphenicol is a bacteriostatic by inhibiting protein synthesis. It prevents protein chain elongation by inhibiting the peptidyl transferase activity of the bacterial ribosome. It specifically binds to A2451 and A2452 residues in the 23S rRNA of the 50S ribosomal subunit, preventing peptide bond formation. While chloramphenicol and the macrolide class of antibiotics both interact with ribosomes, chloramphenicol is not a macrolide. It directly interferes with substrate binding, whereas macrolides sterically block the progression of the growing peptide.\n\nIn 2007, the accumulation of reports associating aplastic anemia and blood dyscrasia with chloramphenicol eye drops lead to the classification of “probable human carcinogen” according to World Health Organization criteria, based on the known published case reports and the spontaneous reports submitted to the National Registry of Drug-Induced Ocular Side Effects.\n\nIn many areas of the world an intravenous dose is about 0.40 to 1.90 USD. In the United States it costs about 3.60 USD per dose in oral tablet form at wholesale.\n\nChloramphenicol is available as a generic worldwide under many brandnames and also under various generic names in eastern Europe and Russia, including chlornitromycin, levomycetin, and chloromycetin; the racemate is known as synthomycetin.\n\nChloramphenicol is available as a capsule or as a liquid. In some countries, it is sold as chloramphenicol palmitate ester (CPE). CPE is inactive, and is hydrolysed to active chloramphenicol in the small intestine. No difference in bioavailability is noted between chloramphenicol and CPE.\n\nManufacture of oral chloramphenicol in the U.S. stopped in 1991, because the vast majority of chloramphenicol-associated cases of aplastic anaemia are associated with the oral preparation. No oral formulation of chloramphenicol is now available in the U.S.\n\nIn molecular biology, chloramphenicol is prepared in ethanol.\n\nThe intravenous (IV) preparation of chloramphenicol is the succinate ester, because pure chloramphenicol does not dissolve in water. This creates a problem: Chloramphenicol succinate ester is an inactive prodrug and must first be hydrolysed to chloramphenicol; however, the hydrolysis process is often incomplete, and 30% of the dose is lost and removed in the urine. Serum concentrations of IV chloramphenicol are only 70% of those achieved when chloramphenicol is given orally. For this reason, the dose needs to be increased to 75 mg/kg/day when administered IV to achieve levels equivalent to the oral dose.\n\nOily chloramphenicol (or chloramphenicol oil suspension) is a long-acting preparation of chloramphenicol first introduced by Roussel in 1954; marketed as Tifomycine, it was originally used as a treatment for typhoid. Roussel stopped production of oily chloramphenicol in 1995; the International Dispensary Association has manufactured it since 1998, first in Malta and then in India from December 2004.\n\nOily chloramphenicol is recommended by the World Health Organization as the first-line treatment of meningitis in low-income countries, and appears on the WHO essential drugs list. It was first used to treat meningitis in 1975 and numerous studies since have demonstrated its efficacy. It is the cheapest treatment available for meningitis (US$5 per treatment course, compared to US$30 for ampicillin and US$15 for five days of ceftriaxone). It has the great advantage of requiring only a single injection, whereas ceftriaxone is traditionally given daily for five days. This recommendation may yet change, now that a single dose of ceftriaxone (cost US$3) has been shown to be equivalent to one dose of oily chloramphenicol.\n\nChloramphenicol is still used occasionally in topical preparations (ointments and eye drops) for the treatment of bacterial conjunctivitis. Isolated case reports of aplastic anaemia following use of chloramphenicol eyedrops exist, but the risk is estimated to be less than one in 224,716 prescriptions. In Mexico, this is the treatment used prophylactically in newborns.\n\nAlthough its use in veterinary medicine is highly restricted, chloramphenicol still has some important veterinary uses. It is currently considered the most useful treatment of chlamydial disease in koalas. The pharmacokinetics of chloramphenicol have been investigated in koalas.\n\nAlthough unpublished, recent research suggests chloramphenicol could also be applied to frogs to prevent their widespread destruction from fungal infections. It has recently been discovered to be a life-saving cure for chytridiomycosis in amphibians. Chytridiomycosis is a fungal disease, blamed for the extinction of one-third of the 120 frog species lost since 1980.\n\n"}
{"id": "6347", "url": "https://en.wikipedia.org/wiki?curid=6347", "title": "Cut-up technique", "text": "Cut-up technique\n\nThe cut-up technique (or \"découpé\" in French) is an aleatory literary technique in which a text is cut up and rearranged to create a new text. The concept can be traced to at least the Dadaists of the 1920s, but was popularized in the late 1950s and early 1960s by writer William S. Burroughs, and has since been used in a wide variety of contexts.\n\nThe cut-up and the closely associated fold-in are the two main techniques:\n\n\nA precedent of the technique occurred during a Dadaist rally in the 1920s in which Tristan Tzara offered to create a poem on the spot by pulling words at random from a hat. Collage, which was popularized roughly contemporaneously with the Surrealist movement, sometimes incorporated texts such as newspapers or brochures. Prior to this event, the technique had been published in an issue of 391 in the poem by Tzara, \"dada manifesto on feeble love and bitter love\" under the sub-title, \"TO MAKE A DADAIST POEM\".\n\nWilliam Burroughs cited T. S. Eliot's poem, \"The Waste Land\" (1922) and John Dos Passos' U.S.A. trilogy, which incorporated newspaper clippings, as early examples of the cut ups he popularized.\n\nGil J. Wolman developed cut-up techniques as part of his lettrist practice in the early 1950s.\n\nAlso in the 1950s, painter and writer Brion Gysin more fully developed the cut-up method after accidentally re-discovering it. He had placed layers of newspapers as a mat to protect a tabletop from being scratched while he cut papers with a razor blade. Upon cutting through the newspapers, Gysin noticed that the sliced layers offered interesting juxtapositions of text and image. He began deliberately cutting newspaper articles into sections, which he randomly rearranged. The book \"Minutes to Go\" resulted from his initial cut-up experiment: unedited and unchanged cut-ups which emerged as coherent and meaningful prose. South African poet Sinclair Beiles also used this technique and co-authored \"Minutes To Go\".\n\nGysin introduced Burroughs to the technique at the Beat Hotel. The pair later applied the technique to printed media and audio recordings in an effort to decode the material's implicit content, hypothesizing that such a technique could be used to discover the true meaning of a given text. Burroughs also suggested cut-ups may be effective as a form of divination saying, \"When you cut into the present the future leaks out.\" Burroughs also further developed the \"fold-in\" technique. In 1977, Burroughs and Gysin published \"The Third Mind\", a collection of cut-up writings and essays on the form. Jeff Nuttall's publication \"My Own Mag\" was another important outlet for the then-radical technique.\n\nIn an interview, Alan Burns noted that for \"Europe After The Rain\" (1965) and subsequent novels he used a version of cut-ups: \"I did not actually use scissors, but I folded pages, read across columns, and so on, discovering for myself many of the techniques Burroughs and Gysin describe\".\n\nArgentine writer Julio Cortázar often used cut ups in his 1963 novel \"Hopscotch\".\n\nIn 1969, poets Howard W. Bergerson and J. A. Lindon developed a cut-up technique known as vocabularyclept poetry, in which a poem is formed by taking all the words of an existing poem and rearranging them, often preserving the metre and stanza lengths.\n\nA drama scripted for five voices by performance poet Hedwig Gorski in 1977 originated the idea of creating poetry only for performance instead of for print publication. The “neo-verse drama” titled \"Booby, Mama!\" written for \"guerilla theater\" performances in public places used a combination of newspaper cut-ups that were edited and choreographed for a troupe of non-professional street actors.\n\nKathy Acker, a literary and intermedia artist, sampled external sources and reconfigured them into the creation of shifting versions of her own constructed identity. In her novel \"Blood and Guts in High School\", Acker explored literary cut-up and appropriation as an integral part of her method.\n\nAntony Balch and Burroughs created a collaboration film, \"The Cut-Ups\" that opened in London in 1967. This was part of an abandoned project called \"Guerrilla Conditions\" meant as a documentary on Burroughs and filmed throughout 1961-1965. Inspired by Burroughs' and Gysin's technique of cutting up text and rearranging it in random order, Balch had an editor cut his footage for the documentary into little pieces and impose no control over its reassembly. The film opened at Oxford Street’s Cinephone cinema and had a disturbing reaction. Many audience members claimed the film made them ill, others demanded their money back, while some just stumbled out of the cinema ranting \"it's disgusting\". Other cut-up films include \"Ghost at n°9 (Paris)\" (1963–72), a posthumously released short film compiled from reels found at Balch’s office after his death, and \"William Buys a Parrott\" (1982), \"Bill and Tony\" (1972), \"Towers Open Fire\" (1963) and \"The Junky's Christmas\" (1966).\n\nFrom the early 1970s, David Bowie used cut-ups to create some of his lyrics. Thom Yorke applied a similar method in Radiohead's \"Kid A\" (2000) album, writing single lines, putting them into a hat, and drawing them out at random while the band rehearsed the songs. Perhaps indicative of Thom Yorke's influences, instructions for \"How to make a Dada poem\" appeared on Radiohead's website at this time.\n\nStephen Mallinder of Cabaret Voltaire reported to \"Inpress\" magazine's Andrez Bergen that \"I do think the manipulation of sound in our early days – the physical act of cutting up tapes, creating tape loops and all that – has a strong reference to Burroughs and Gysin.\"\n\n\n"}
{"id": "6352", "url": "https://en.wikipedia.org/wiki?curid=6352", "title": "Cretinism", "text": "Cretinism\n\nCretinism is a condition of severely stunted physical and mental growth owing to untreated congenital deficiency of thyroid hormone (congenital hypothyroidism) usually owing to maternal hypothyroidism.\n\nAround the world, the most common cause of congenital hypothyroidism is iodine deficiency. Cretinism is therefore most probably due to a diet deficient in iodine. It has affected many people worldwide and continues to be a major public health problem in many countries. Iodine is an essential trace element, necessary primarily for the synthesis of thyroid hormones. Iodine deficiency is the most common preventable cause of brain damage worldwide. Although iodine is found in many foods, it is not universally present in all soils in adequate amounts. Most iodine, in iodide form, is in the oceans where the iodide ions oxidize to elemental iodine, which then enters the atmosphere and falls to earth as rain, introducing iodine to soils. Earth deficient in iodine is most common inland and in mountainous areas and areas of frequent flooding, but can also occur in coastal regions owing to past glaciation, and leaching by snow, water and heavy rainfall, which removes iodine from the soil. Plants and animals grown in iodine deficient soils are correspondingly deficient. Populations living in those areas without outside food sources are most at risk of iodine deficiency diseases.\n\nIodine deficiency results in the impairments in varying degrees of physical and mental development. It also causes gradual enlargement of the thyroid gland, referred to as a goitre. It is being combated in many countries by public health campaigns of iodine administration.\n\nCongenital hypothyroidism can be endemic, genetic, or sporadic. If untreated, it results in mild to severe impairment of both physical and mental growth and development.\n\nPoor length growth is apparent as early as the first year of life. Adult stature without treatment ranges from , depending on severity, sex, and other genetic factors. In adults, Cretinism results in mental deterioration, swelling of the skin, loss of water and hair. \nBone maturation and puberty are severely delayed. Ovulation is impeded, and infertility is common.\n\nNeurological impairment may be mild, with reduced muscle tone and coordination, or so severe that the person cannot stand or walk. Cognitive impairment may also range from mild to so severe that the person is nonverbal and dependent on others for basic care. Thought and reflexes are slower.\n\nOther signs may include thickened skin, enlarged tongue, or a protruding abdomen.\n\nDwarfism may also be caused by malnutrition or other hormonal deficiencies, such as insufficient growth hormone secretion, hypopituitarism, decreased secretion of growth hormone-releasing hormone, deficient growth hormone receptor activity and downstream causes, such as insulin-like growth factor 1 (IGF-1) deficiency.\n\nSporadic and genetic cretinism results from abnormal development or growth of the foetal thyroid gland. This type of cretinism has been almost completely eliminated in developed countries by early diagnosis by newborn screening schemes followed by lifelong treatment with thyroxine (T4).\n\nThyroxine must be dosed as tablets only, even to newborns, as the liquid oral suspensions and compounded forms cannot be depended on for reliable dosing. In the case of dosing infants, the T4 tablets are generally crushed and mixed with breast milk, formula milk or water. If the medication is mixed with formulas containing iron or soya products, larger doses may be required, as these substances may alter the absorption of thyroid hormone from the gut. Frequent monitoring (every 2–3 weeks during the first months of life) is recommended to ensure that infants with congenital hypothyroidism remain within the high end of normal range, or euthyroid.\n\nGoiter is the most specific clinical marker of either the direct or indirect insufficient intake of iodine in the human body. There is evidence of goiter, and its medical treatment with iodine-rich algae and burnt sponges, in Chinese, Egyptian, and Roman ancient medical texts. In 1848, the Italian King Carlo Alberto of Sardinia commissioned the first epidemiological study of goiter and cretinism in his Haute-Savoie territories, where hideous cases of goiters and cretinism frequently occurred in the population. In past centuries, the well reported social diseases prevalent among the poorer social classes and farmers, caused by dietary and agricultural monocultures, were: pellagra, rickets, beriberi, scurvy in long-term sailors, and the endemic goiter caused by Iodine deficiency. However, this disease was less mentioned in medical books because it was erroneously considered to be an aesthetic rather than a clinical disorder.\nEndemic cretinism was especially common in areas of southern Europe around the Alps and was often described by ancient Roman writers and depicted by artists. The earliest Alpine mountain climbers sometimes came upon whole villages of cretins. Alpine cretinism was described from a medical perspective by several travellers and physicians in the late 18th and early 19th centuries. At that time the cause was not known and it was often attributed to \"stagnant air\" in mountain valleys or \"bad water\". The proportion of people affected varied markedly throughout southern Europe and even within very small areas it might be common in one valley and not another. The number of severely affected persons was always a minority, and most persons were only affected to the extent of having a goitre and some degree of reduced cognition and growth. The majority of such cases were still socially functional in their pastoral villages.\n\nMore mildly affected areas of Europe and North America in the 19th century were referred to as \"goitre belts\". The degree of iodine deficiency was milder and manifested primarily as thyroid enlargement rather than severe mental and physical impairment. In Switzerland, for example, where soil does not contain a large amount of iodine, cases of cretinism were very abundant and even considered genetically caused. As the variety of food sources dramatically increased in Europe and North America and the populations became less completely dependent on locally grown food, the prevalence of endemic goitre diminished.\n\nThe early 20th century saw the discovery of the relationships of sporadic cretinism with congenital hypothyroidism, and of endemic cretinism with hypothyroidism due to iodine deficiency. Both have been largely eliminated in the developed world.\n\nThe term \"cretin\" was once used to describe a person affected by cretinism, but, as with words such as \"spastic\", and \"lunatic\", it is now considered derogatory and inappropriate. \"Cretin\" became a medical term in the 18th century, from an Occitan and an Alpine French expression, prevalent in a region where persons with such a condition were especially common (see below); it saw wide medical use in the 19th and early 20th centuries, and was actually a \"tick box\" category on Victorian-era census forms in the UK. The term spread more widely in popular English as a markedly derogatory term for a person who behaves stupidly. Because of its pejorative connotations in popular speech, health-care workers have mostly abandoned the term \"cretin\".\n\nThe etymology of \"cretin\" is uncertain. Several hypotheses exist. The most common derivation provided in English dictionaries is from the Alpine French dialect pronunciation of the word \"Chrétien\" (\"(a) Christian\"), which was a greeting there. According to the \"Oxford English Dictionary\", the translation of the French term into \"human creature\" implies that the label \"Christian\" is a reminder of the humanity of the afflicted, in contrast to brute beasts. Other sources suggest that \"Christian\" describes the person's \"Christ-like\" inability to sin, stemming, in such cases, from an incapacity to distinguish right from wrong.\n\nOther speculative etymologies have been offered:\n\n"}
{"id": "6353", "url": "https://en.wikipedia.org/wiki?curid=6353", "title": "Cretin", "text": "Cretin\n\nCretin may refer to:\n\n\n"}
{"id": "6354", "url": "https://en.wikipedia.org/wiki?curid=6354", "title": "Council of Trent", "text": "Council of Trent\n\nThe Council of Trent (), held between 1545 and 1563 in Trento (Trent) and Bologna, northern Italy, was one of the Roman Catholic Church's most important ecumenical councils. Prompted by the Protestant Reformation, it has been described as the embodiment of the Counter-Reformation. Four hundred years later, when Pope John XXIII initiated preparations for the Second Vatican Council (Vatican II), he affirmed the decrees it had issued: \"What was, still is.\"\n\nAs well as decrees, the Council issued condemnations of what it defined to be heresies committed by Protestantism and, in response to them, key statements and clarifications of the Church's doctrine and teachings. These addressed a wide range of subjects, including scripture, the Biblical canon, sacred tradition, original sin, justification, salvation, the sacraments, the Mass and the veneration of saints. The Council met for twenty-five sessions between 13 December 1545 and 4 December 1563, all in Trento (then the capital of the Prince-Bishopric of Trent in the Holy Roman Empire), apart from the ninth to eleventh sessions held in Bologna during 1547. Pope Paul III, who convoked the Council, presided over these and the first eight sessions (1545–47), while the twelfth to sixteenth sessions (1551–52) were overseen by Pope Julius III and the seventeenth to twenty-fifth sessions (1562–63) by Pope Pius IV.\n\nThe consequences of the Council were also significant as regards the Church's liturgy and practices. During its deliberations, the Council made the Vulgate the official example of the Biblical canon and commissioned the creation of a standard version, although this was not achieved until the 1590s. In 1565, however, a year or so after the Council finished its work, Pius IV issued the Tridentine Creed (after \"Tridentum\", Trento's Latin name) and his successor Pius V then issued the Roman Catechism and revisions of the Breviary and Missal in, respectively, 1566, 1568 and 1570. These, in turn, led to the codification of the Tridentine Mass, which remained the Church's primary form of the Mass for the next four hundred years.\n\nMore than three hundred years passed until the next ecumenical council, the First Vatican Council, was convened in 1869.\n\nOn 15 March 1517, the Fifth Council of the Lateran closed its activities with a number of reform proposals (on the selection of bishops, taxation, censorship and preaching) but not on the major problems that confronted the Church in Germany and other parts of Europe. A few months later, on 31 October 1517, Martin Luther issued his 95 Theses in Wittenberg.\n\nLuther's position on ecumenical councils shifted over time, but in 1520 he appealed to the German princes to oppose the papal Church, if necessary with a council in Germany, open and free of the Papacy. After the Pope condemned in \"Exsurge Domine\" fifty-two of Luther's theses as heresy, German opinion considered a council the best method to reconcile existing differences. German Catholics, diminished in number, hoped for a council to clarify matters.\n\nIt took a generation for the council to materialise, partly because of papal reluctance, given that a Lutheran demand was the exclusion of the papacy from the Council, and partly because of ongoing political rivalries between France and Germany and the Turkish dangers in the Mediterranean. Under Pope Clement VII (1523–34), troops of the Catholic Holy Roman Emperor Charles V sacked Papal Rome in 1527, \"raping, killing, burning, stealing, the like had not been seen since the Vandals\". Saint Peter's Basilica and the Sistine Chapel were used for horses. This, together with the Pontiff's ambivalence between France and Germany, led to his hesitation.\n\nCharles V strongly favoured a council, but needed the support of King Francis I of France, who attacked him militarily. Francis I generally opposed a general council due to partial support of the Protestant cause within France, and in 1533 he further complicated matters when suggesting a general council to include both Catholic and Protestant rulers of Europe that would devise a compromise between the two theological systems. This proposal met the opposition of the Pope for it gave recognition to Protestants and also elevated the secular Princes of Europe above the clergy on church matters. Faced with a Turkish attack, Charles held the support of the Protestant German rulers, all of whom delayed the opening of the Council of Trent.\n\nIn reply to the Papal bull \"Exsurge Domine\" of Pope Leo X (1520), Martin Luther burned the document and appealed for a general council. In 1522 German diets joined in the appeal, with Charles V seconding and pressing for a council as a means of reunifying the Church and settling the Reformation controversies. Pope Clement VII (1523–34) was vehemently against the idea of a council, agreeing with Francis I of France. After Pope Pius II, in his bull \"Execrabilis\" (1460) and his reply to the University of Cologne (1463), set aside the theory of the supremacy of general councils laid down by the Council of Constance.\n\nPope Paul III (1534–49), seeing that the Protestant Reformation was no longer confined to a few preachers, but had won over various princes, particularly in Germany, to its ideas, desired a council. Yet when he proposed the idea to his cardinals, it was almost unanimously opposed. Nonetheless, he sent nuncios throughout Europe to propose the idea. Paul III issued a decree for a general council to be held in Mantua, Italy, to begin on 23 May 1537. Martin Luther wrote the Smalcald Articles in preparation for the general council. The Smalcald Articles were designed to sharply define where the Lutherans could and could not compromise. The council was ordered by the Emperor and Pope Paul III to convene in Mantua on 23 May 1537. It failed to convene after another war broke out between France and Charles V, resulting in a non-attendance of French prelates. Protestants, just defeated by Charles V, refused to attend as well. Financial difficulties in Mantua led the Pope in the autumn of 1537 to move the council to Vicenza, where participation was poor. The Council was postponed indefinitely on 21 May 1539. Pope Paul III then initiated several internal Church reforms while Emperor Charles V convened with Protestants at an imperial diet in Regensburg, to reconcile differences. Unity failed between Catholic and Protestant representatives \"because of different concepts of \"Church\" and \"justification\"\".\n\nHowever, the council was delayed until 1545 and, as it happened, convened right before Luther's death. Unable, however, to resist the urging of Charles V, the pope, after proposing Mantua as the place of meeting, convened the council at Trento (at that time a free city of the Holy Roman Empire under a prince-bishop), on 13 December 1545; the Pope's decision to transfer it to Bologna in March 1547 on the pretext of avoiding a plague failed to take effect and the Council was indefinitely prorogued on 17 September 1549. None of the three popes reigning over the duration of the council ever attended, which had been a condition of Charles V. Papal legates were appointed to represent the Papacy.\n\nReopened at Trento on 1 May 1551 by convocation of Pope Julius III (1550–5), it was broken up by the sudden victory of Maurice, Elector of Saxony over the Emperor Charles V and his march into surrounding state of Tirol on 28 April 1552. There was no hope of reassembling the council while the very anti-Protestant Paul IV was Pope. The council was reconvened by Pope Pius IV (1559–65) for the last time, meeting from 18 January 1562 at Santa Maria Maggiore, Trento, and continued until its final adjournment on 4 December 1563. It closed with a series of ritual acclamations honouring the reigning Pope, the Popes who had convoked the Council, the emperor and the kings who had supported it, the papal legates, the cardinals, the ambassadors present, and the bishops, followed by acclamations of acceptance of the faith of the Council and its decrees, and of anathema for all heretics.\n\nThe history of the council is thus divided into three distinct periods: 1545–49, 1551–52 and 1562–63. During the second period, the Protestants present asked for renewed discussion on points already defined and for bishops to be released from their oaths of allegiance to the Pope. When the last period began, all hope of conciliating the Protestants was gone and the Jesuits had become a strong force.\n\nThe number of attending members in the three periods varied considerably. The council was small to begin with, opening with only about 30 bishops. It increased toward the close, but never reached the number of the First Council of Nicaea (which had 318 members) nor of the First Vatican Council (which numbered 744). The decrees were signed in 1563 by 255 members, the highest attendance of the whole council, including four papal legates, two cardinals, three patriarchs, twenty-five archbishops, and 168 bishops, two-thirds of whom were Italians. The Italian and Spanish prelates were vastly preponderant in power and numbers. At the passage of the most important decrees, not more than sixty prelates were present.\n\nThe French monarchy boycotted the entire council until the last minute; a delegation led by Charles de Guise, Cardinal of Lorraine finally arrived in November 1562. The first outbreak of the French Wars of Religion had been earlier in the year, and the French had experience of a significant and powerful Protestant minority, iconoclasm and tensions leading to violence in a way the Italians and Iberians did not. Among other influences, the last minute inclusion of a decree on sacred images was a French initiative, and the text, never discussed on the floor of the council or referred to council theologians, was based on a French draft.\n\nThe main objectives of the council were twofold, although there were other issues that were also discussed:\n\n\nThe doctrinal decisions of the council are divided into decrees (\"decreta\"), which contain the positive statement of the conciliar dogmas, and into short canons (\"canones\"), which condemn the dissenting Protestant views with the concluding \"\"anathema sit\"\" (\"let him be anathema\").\n\nThe doctrinal acts are as follows: after reaffirming the Niceno-Constantinopolitan Creed (third session), the decree was passed (fourth session) confirming that the deuterocanonical books were on a par with the other books of the canon (against Luther's placement of these books in the Apocrypha of his edition) and coordinating church tradition with the Scriptures as a rule of faith. The Vulgate translation was affirmed to be authoritative for the text of Scripture.\n\nJustification (sixth session) was declared to be offered upon the basis of human cooperation with divine grace as opposed to the Protestant doctrine of passive reception of grace. Understanding the Protestant \"faith alone\" doctrine to be one of simple human confidence in divine mercy, the Council rejected the \"vain confidence\" of the Protestants, stating that no one can know who has received the grace of God. Furthermore, the Council affirmed—against Protestant doctrine—that the grace of God can be forfeited through mortal sin.\n\nThe greatest weight in the Council's decrees is given to the sacraments. The seven sacraments were reaffirmed and the Eucharist pronounced to be a true propitiatory sacrifice as well as a sacrament, in which the bread and wine were consecrated into the Eucharist (thirteenth and twenty-second sessions). The term transubstantiation was used by the Council, but the specific Aristotelian explanation given by Scholasticism was not cited as dogmatic. Instead, the decree states that Christ is \"really, truly, substantially present\" in the consecrated forms. The sacrifice of the Mass was to be offered for dead and living alike and in giving to the apostles the command \"do this in remembrance of me,\" Christ conferred upon them a sacerdotal power. The practice of withholding the cup from the laity was confirmed (twenty-first session) as one which the Church Fathers had commanded for good and sufficient reasons; yet in certain cases the Pope was made the supreme arbiter as to whether the rule should be strictly maintained. On the language of the Mass, \"contrary to what is often said\", the council condemned the belief that only vernacular languages should be used, while insisting on the use of Latin.\n\nOrdination (twenty-third session) was defined to imprint an indelible character on the soul. The priesthood of the New Testament takes the place of the Levitical priesthood. To the performance of its functions, the consent of the people is not necessary.\n\nIn the decrees on marriage (twenty-fourth session) the excellence of the celibate state was reaffirmed, concubinage condemned and the validity of marriage made dependent upon the wedding taking place before a priest and two witnesses, although the lack of a requirement for parental consent ended a debate that had proceeded from the 12th century. In the case of a divorce, the right of the innocent party to marry again was denied so long as the other party was alive, even if the other party had committed adultery. However the council \"refused … to assert the necessity of usefulness of clerical celibacy.\n\nIn the twenty-fifth and last session, the doctrines of purgatory, the invocation of saints and the veneration of relics were reaffirmed, as was also the efficacy of indulgences as dispensed by the Church according to the power given her, but with some cautionary recommendations, and a ban on the sale of indulgences. Short and rather inexplicit passages concerning religious images, were to have great impact on the development of Roman Catholic art. Much more than the Second Council of Nicaea (787) the Council fathers of Trent stressed the pedagogical purpose of Christian images.\n\nThe council appointed, in 1562 (eighteenth session), a commission to prepare a list of forbidden books (\"Index Librorum Prohibitorum\"), but it later left the matter to the Pope. The preparation of a catechism and the revision of the Breviary and Missal were also left to the pope. The catechism embodied the council's far-reaching results, including reforms and definitions of the sacraments, the Scriptures, church dogma, and duties of the clergy.\n\nOn adjourning, the Council asked the supreme pontiff to ratify all its decrees and definitions. This petition was complied with by Pope Pius IV, on 26 January 1564, in the papal bull, \"Benedictus Deus\", which enjoins strict obedience upon all Roman Catholics and forbids, under pain of excommunication, all unauthorised interpretation, reserving this to the Pope alone and threatens the disobedient with \"the indignation of Almighty God and of his blessed apostles, Peter and Paul.\" Pope Pius appointed a commission of cardinals to assist him in interpreting and enforcing the decrees.\n\nThe \"Index librorum prohibitorum\" was announced in 1564 and the following books were issued with the papal imprimatur: the Profession of the Tridentine Faith and the Tridentine Catechism (1566), the Breviary (1568), the Missal (1570) and the Vulgate (1590 and then 1592).\n\nThe decrees of the council were acknowledged in Italy, Portugal, Poland and by the Catholic princes of Germany at the Diet of Augsburg in 1566. Philip II of Spain accepted them for Spain, the Netherlands and Sicily inasmuch as they did not infringe the royal prerogative. In France they were officially recognised by the king only in their doctrinal parts. The disciplinary sections received official recognition at provincial synods and were enforced by the bishops. No attempt was made to introduce it into England. Pius IV sent the decrees to Mary, Queen of Scots, with a letter dated 13 June 1564, requesting her to publish them in Scotland, but she dared not do it in the face of John Knox and the Reformation.\n\nThese decrees were later supplemented by the First Vatican Council of 1870.\n\nThe most comprehensive history is still Hubert Jedin's \"The History of the Council of Trent (Geschichte des Konzils von Trient)\" with about 2500 pages in four volumes: \"The History of the Council of Trent: The fight for a Council\" (Vol I, 1951); \"The History of the Council of Trent: The first Sessions in Trent (1545–1547)\" (Vol II, 1957); \"The History of the Council of Trent: Sessions in Bologna 1547–1548 and Trento 1551–1552\" (Vol III, 1970, 1998); \"The History of the Council of Trent: Third Period and Conclusion\" (Vol IV, 1976).\n\nThe canons and decrees of the council have been published very often and in many languages (for a large list consult \"British Museum Catalogue\", under \"Trent, Council of\"). The first issue was by Paulus Manutius (Rome, 1564). The best Latin editions are by Judocus Le Plat (Antwerp, 1779) and by Johann Friedrich von Schulte and Aemilius Ludwig Richter (Leipzig, 1853). Other good editions are in vol. vii. of the \"Acta et decreta conciliorum recentiorum. Collectio Lacensis\" (7 vols., Freiburg, 1870–90), reissued as independent volume (1892); \"Concilium Tridentinum: Diariorum, actorum, epistularum, … collectio\", ed. Sebastianus Merkle (4 vols., Freiburg, 1901 sqq.); not to overlook Mansi, \"Concilia\", xxxv. 345 sqq. Note also Carl Mirbt, \"Quellen\", 2d ed, pp. 202–255. The best English edition is by James Waterworth (London, 1848; \"With Essays on the External and Internal History of the Council\").\n\nThe original acts and debates of the council, as prepared by its general secretary, Bishop Angelo Massarelli, in six large folio volumes, are deposited in the Vatican Library and remained there unpublished for more than 300 years and were brought to light, though only in part, by Augustin Theiner, priest of the oratory (d. 1874), in \"Acta genuina sancti et oecumenici Concilii Tridentini nunc primum integre edita\" (2 vols., Leipzig, 1874).\n\nMost of the official documents and private reports, however, which bear upon the council, were made known in the 16th century and since. The most complete collection of them is that of J. Le Plat, \"Monumentorum ad historicam Concilii Tridentini collectio\" (7 vols., Leuven, 1781–87). New materials(Vienna, 1872); by JJI von Döllinger \"(Ungedruckte Berichte und Tagebücher zur Geschichte des Concilii von Trient)\" (2 parts, Nördlingen, 1876); and August von Druffel, \"Monumenta Tridentina\" (Munich, 1884–97).\n\n\n\n\n"}
{"id": "6355", "url": "https://en.wikipedia.org/wiki?curid=6355", "title": "Chloroplast", "text": "Chloroplast\n\nChloroplasts are organelles, specialized subunits, in plant and algal cells. Their discovery inside plant cells is usually credited to Julius von Sachs (1832–1897), an influential botanist and author of standard botanical textbooks – sometimes called \"The Father of Plant Physiology\".\n\nThe main role of chloroplasts is to conduct photosynthesis, where the photosynthetic pigment chlorophyll captures the energy from sunlight and converts it and stores it in the energy-storage molecules ATP and NADPH while freeing oxygen from water. They then use the ATP and NADPH to make organic molecules from carbon dioxide in a process known as the Calvin cycle. Chloroplasts carry out a number of other functions, including fatty acid synthesis, much amino acid synthesis, and the immune response in plants. The number of chloroplasts per cell varies from one, in unicellular algae, up to 100 in plants like \"Arabidopsis\" and wheat.\n\nA chloroplast is a type of organelle known as a plastid, characterized by its high concentration of chlorophyll. Other plastid types, such as the leucoplast and the chromoplast, contain little chlorophyll and do not carry out photosynthesis.\n\nChloroplasts are highly dynamic—they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce. Their behavior is strongly influenced by environmental factors like light color and intensity. Chloroplasts, like mitochondria, contain their own DNA, which is thought to be inherited from their ancestor—a photosynthetic cyanobacterium that was engulfed by an early eukaryotic cell. Chloroplasts cannot be made by the plant cell and must be inherited by each daughter cell during cell division.\n\nWith one exception (the amoeboid \"Paulinella chromatophora\"), all chloroplasts can probably be traced back to a single endosymbiotic event, when a cyanobacterium was engulfed by the eukaryote. Despite this, chloroplasts can be found in an extremely wide set of organisms, some not even directly related to each other—a consequence of many secondary and even tertiary endosymbiotic events.\n\nThe word \"chloroplast\" is derived from the Greek words \"chloros\" (χλωρός), which means green, and \"plastes\" (πλάστης), which means \"the one who forms\".\n\nThe first definitive description of a chloroplast (\"Chlorophyllkörnen\", \"grain of chlorophyll\") was given by Hugo von Mohl in 1837 as discrete bodies within the green plant cell. In 1883, A. F. W. Schimper would name these bodies as \"chloroplastids\" (\"Chloroplastiden\"). In 1884, Eduard Strasburger adopted the term \"chloroplasts\" (\"Chloroplasten\").\n\nChloroplasts are one of many types of organelles in the plant cell. They are considered to have originated from cyanobacteria through endosymbiosis—when a eukaryotic cell engulfed a photosynthesizing cyanobacterium that became a permanent resident in the cell. Mitochondria are thought to have come from a similar event, where an aerobic prokaryote was engulfed. This origin of chloroplasts was first suggested by the Russian biologist Konstantin Mereschkowski in 1905 after Andreas Schimper observed in 1883 that chloroplasts closely resemble cyanobacteria. Chloroplasts are only found in plants, algae, and the amoeboid \"Paulinella chromatophora\".\n\nCyanobacteria are considered the ancestors of chloroplasts. They are sometimes called blue-green algae even though they are prokaryotes. They are a diverse phylum of bacteria capable of carrying out photosynthesis, and are gram-negative, meaning that they have two cell membranes. Cyanobacteria also contain a peptidoglycan cell wall, which is thicker than in other gram-negative bacteria, and which is located between their two cell membranes. Like chloroplasts, they have thylakoids within. On the thylakoid membranes are photosynthetic pigments, including chlorophyll \"a\". Phycobilins are also common cyanobacterial pigments, usually organized into hemispherical phycobilisomes attached to the outside of the thylakoid membranes (phycobilins are not shared with all chloroplasts though).\n\nSomewhere around a billion years ago, a free-living cyanobacterium entered an early eukaryotic cell, either as food or as an internal parasite, but managed to escape the phagocytic vacuole it was contained in. The two innermost lipid-bilayer membranes that surround all chloroplasts correspond to the outer and inner membranes of the ancestral cyanobacterium's gram negative cell wall, and not the phagosomal membrane from the host, which was probably lost.\nThe new cellular resident quickly became an advantage, providing food for the eukaryotic host, which allowed it to live within it. Over time, the cyanobacterium was assimilated, and many of its genes were lost or transferred to the nucleus of the host. From genomes that probably originally contained over 3000 genes only about 130 genes remain in the chloroplasts of contemporary plants. Some of its proteins were then synthesized in the cytoplasm of the host cell, and imported back into the chloroplast (formerly the cyanobacterium). Separately, somewhere around 100 million years ago, it happened again and led to the amoeboid \"Paulinella chromatophora\".\n\nThis event is called \"endosymbiosis\", or \"cell living inside another cell\". The cell living inside the other cell is called the \"endosymbiont\"; the endosymbiont is found inside the \"host cell\".\n\nChloroplasts are believed to have arisen after mitochondria, since all eukaryotes contain mitochondria, but not all have chloroplasts. This is called \"serial endosymbiosis\"—an early eukaryote engulfing the mitochondrion ancestor, and some descendants of it then engulfing the chloroplast ancestor, creating a cell with both chloroplasts and mitochondria.\n\nWhether or not chloroplasts came from a single endosymbiotic event, or many independent engulfments across various eukaryotic lineages, has been long debated. It is now generally held that most organisms with chloroplasts either share a single ancestor or obtained their chloroplast from organisms that share a common ancestor that took in a cyanobacterium 600–1600 million years ago. The exception is the amoeboid \"Paulinella chromatophora\", which descends from an ancestor that took in a cyanobacterium 90–140 million years ago.\n\nThese chloroplasts, which can be traced back directly to a cyanobacterial ancestor, are known as \"primary plastids\" (\"\"plastid\"\" in this context means almost the same thing as chloroplast). All primary chloroplasts belong to one of four chloroplast lineages—the glaucophyte chloroplast lineage, the amoeboid \"Paulinella chromatophora\" lineage, the rhodophyte (red algal) chloroplast lineage, or the chloroplastidan (green) chloroplast lineage. The rhodophyte and chloroplastidan lineages are the largest, with chloroplastidan (green) being the one that contains the land plants.\n\nThe alga \"Cyanophora\", a glaucophyte, is thought to be one of the first organisms to contain a chloroplast. The glaucophyte chloroplast group is the smallest of the three primary chloroplast lineages, being found in only 13 species, and is thought to be the one that branched off the earliest. Glaucophytes have chloroplasts that retain a peptidoglycan wall between their double membranes, like their cyanobacterial parent. For this reason, glaucophyte chloroplasts are also known as \"muroplasts\". Glaucophyte chloroplasts also contain concentric unstacked thylakoids, which surround a carboxysome – an icosahedral structure that glaucophyte chloroplasts and cyanobacteria keep their carbon fixation enzyme rubisco in. The starch that they synthesize collects outside the chloroplast. Like cyanobacteria, glaucophyte chloroplast thylakoids are studded with light collecting structures called phycobilisomes. For these reasons, glaucophyte chloroplasts are considered a primitive intermediate between cyanobacteria and the more evolved chloroplasts in red algae and plants.\n\nThe rhodophyte, or red algal chloroplast group is another large and diverse chloroplast lineage. Rhodophyte chloroplasts are also called \"rhodoplasts\", literally \"red chloroplasts\".\n\nRhodoplasts have a double membrane with an intermembrane space and phycobilin pigments organized into phycobilisomes on the thylakoid membranes, preventing their thylakoids from stacking. Some contain pyrenoids. Rhodoplasts have chlorophyll \"a\" and phycobilins for photosynthetic pigments; the phycobilin phycoerytherin is responsible for giving many red algae their distinctive red color. However, since they also contain the blue-green chlorophyll \"a\" and other pigments, many are reddish to purple from the combination. The red phycoerytherin pigment is an adaptation to help red algae catch more sunlight in deep water—as such, some red algae that live in shallow water have less phycoerytherin in their rhodoplasts, and can appear more greenish. Rhodoplasts synthesize a form of starch called floridean starch, which collects into granules outside the rhodoplast, in the cytoplasm of the red alga.\n\nThe chloroplastidan chloroplasts, or green chloroplasts, are another large, highly diverse primary chloroplast lineage. Their host organisms are commonly known as the green algae and land plants. They differ from glaucophyte and red algal chloroplasts in that they have lost their phycobilisomes, and contain chlorophyll \"b\" instead. Most green chloroplasts are (obviously) green, though some aren't, like some forms of \"Hæmatococcus pluvialis\", due to accessory pigments that override the chlorophylls' green colors. Chloroplastidan chloroplasts have lost the peptidoglycan wall between their double membrane, leaving an intermembrane space. Some plants seem to have kept the genes for the synthesis of the peptidoglycan layer, though they've been repurposed for use in chloroplast division instead.\n\nMost of the chloroplasts depicted in this article are green chloroplasts.\n\nGreen algae and plants keep their starch \"inside\" their chloroplasts, and in plants and some algae, the chloroplast thylakoids are arranged in grana stacks. Some green algal chloroplasts contain a structure called a pyrenoid, which is functionally similar to the glaucophyte carboxysome in that it is where rubisco and CO are concentrated in the chloroplast.\n\nHelicosporidium is a genus of nonphotosynthetic parasitic green algae that is thought to contain a vestigial chloroplast. Genes from a chloroplast and nuclear genes indicating the presence of a chloroplast have been found in Helicosporidium even if nobody's seen the chloroplast itself.\n\nWhile most chloroplasts originate from that first set of endosymbiotic events, \"Paulinella chromatophora\" is an exception that acquired a photosynthetic cyanobacterial endosymbiont more recently. It is not clear whether that symbiont is closely related to the ancestral chloroplast of other eukaryotes. Being in the early stages of endosymbiosis, \"Paulinella chromatophora\" can offer some insights into how chloroplasts evolved. \"Paulinella\" cells contain one or two sausage shaped blue-green photosynthesizing structures called chromatophores, descended from the cyanobacterium \"Synechococcus\". Chromatophores cannot survive outside their host. Chromatophore DNA is about a million base pairs long, containing around 850 protein encoding genes—far less than the three million base pair \"Synechococcus\" genome, but much larger than the approximately 150,000 base pair genome of the more assimilated chloroplast. Chromatophores have transferred much less of their DNA to the nucleus of their host. About 0.3–0.8% of the nuclear DNA in \"Paulinella\" is from the chromatophore, compared with 11–14% from the chloroplast in plants.\n\nMany other organisms obtained chloroplasts from the primary chloroplast lineages through secondary endosymbiosis—engulfing a red or green alga that contained a chloroplast. These chloroplasts are known as secondary plastids.\n\nWhile primary chloroplasts have a double membrane from their cyanobacterial ancestor, secondary chloroplasts have additional membranes outside of the original two, as a result of the secondary endosymbiotic event, when a nonphotosynthetic eukaryote engulfed a chloroplast-containing alga but failed to digest it—much like the cyanobacterium at the beginning of this story. The engulfed alga was broken down, leaving only its chloroplast, and sometimes its cell membrane and nucleus, forming a chloroplast with three or four membranes—the two cyanobacterial membranes, sometimes the eaten alga's cell membrane, and the phagosomal vacuole from the host's cell membrane.\n\nThe genes in the phagocytosed eukaryote's nucleus are often transferred to the secondary host's nucleus.\nCryptomonads and chlorarachniophytes retain the phagocytosed eukaryote's nucleus, an object called a nucleomorph, located between the second and third membranes of the chloroplast.\n\nAll secondary chloroplasts come from green and red algae—no secondary chloroplasts from glaucophytes have been observed, probably because glaucophytes are relatively rare in nature, making them less likely to have been taken up by another eukaryote.\n\nGreen algae have been taken up by the euglenids, chlorarachniophytes, a lineage of dinoflagellates, and possibly the ancestor of the CASH lineage (cryptomonads, alveolates , stramenopiles and haptophytes) in three or four separate engulfments. Many green algal derived chloroplasts contain pyrenoids, but unlike chloroplasts in their green algal ancestors, storgae product collects in granules outside the chloroplast.\n\nEuglenophytes are a group of common flagellated protists that contain chloroplasts derived from a green alga. Euglenophyte chloroplasts have three membranes—it is thought that the membrane of the primary endosymbiont was lost, leaving the cyanobacterial membranes, and the secondary host's phagosomal membrane. Euglenophyte chloroplasts have a pyrenoid and thylakoids stacked in groups of three. Photosynthetic product is stored in the form of paramylon, which is contained in membrane-bound granules in the cytoplasm of the euglenophyte.\n\nChlorarachniophytes are a rare group of organisms that also contain chloroplasts derived from green algae, though their story is more complicated than that of the euglenophytes. The ancestor of chlorarachniophytes is thought to have been a eukaryote with a \"red\" algal derived chloroplast. It is then thought to have lost its first red algal chloroplast, and later engulfed a green alga, giving it its second, green algal derived chloroplast.\n\nChlorarachniophyte chloroplasts are bounded by four membranes, except near the cell membrane, where the chloroplast membranes fuse into a double membrane. Their thylakoids are arranged in loose stacks of three. Chlorarachniophytes have a form of polysaccharide called chrysolaminarin, which they store in the cytoplasm, often collected around the chloroplast pyrenoid, which bulges into the cytoplasm.\n\nChlorarachniophyte chloroplasts are notable because the green alga they are derived from has not been completely broken down—its nucleus still persists as a nucleomorph found between the second and third chloroplast membranes—the periplastid space, which corresponds to the green alga's cytoplasm.\n\n\"Lepidodinium viride\" and its close relatives are dinophytes (see below) that lost their original peridinin chloroplast and replaced it with a green algal derived chloroplast (more specifically, a prasinophyte). \"Lepidodinium\" is the only dinophyte that has a chloroplast that's not from the rhodoplast lineage. The chloroplast is surrounded by two membranes and has no nucleomorph—all the nucleomorph genes have been transferred to the dinophyte nucleus. The endosymbiotic event that led to this chloroplast was serial secondary endosymbiosis rather than tertiary endosymbiosis—the endosymbiont was a green alga containing a primary chloroplast (making a secondary chloroplast).\n\nCryptophytes, or cryptomonads are a group of algae that contain a red-algal derived chloroplast. Cryptophyte chloroplasts contain a nucleomorph that superficially resembles that of the chlorarachniophytes. Cryptophyte chloroplasts have four membranes, the outermost of which is continuous with the rough endoplasmic reticulum. They synthesize ordinary starch, which is stored in granules found in the periplastid space—outside the original double membrane, in the place that corresponds to the red alga's cytoplasm. Inside cryptophyte chloroplasts is a pyrenoid and thylakoids in stacks of two.\n\nTheir chloroplasts do not have phycobilisomes, but they do have phycobilin pigments which they keep in their thylakoid space, rather than anchored on the outside of their thylakoid membranes.\n\nHaptophytes are similar and closely related to cryptophytes or heterokontophytes. Their chloroplasts lack a nucleomorph, their thylakoids are in stacks of three, and they synthesize chrysolaminarin sugar, which they store completely outside of the chloroplast, in the cytoplasm of the haptophyte.\n\nThe heterokontophytes, also known as the stramenopiles, are a very large and diverse group of eukaryotes. The photoautotrophic lineage, Ochrophyta, including the diatoms and the brown algae, golden algae, and yellow-green algae, also contains red algal derived chloroplasts. \n\nHeterokont chloroplasts are very similar to haptophyte chloroplasts, containing a pyrenoid, triplet thylakoids, and with some exceptions, having four layer plastidic envelope, the outermost epiplastid membrane connected to the endoplasmic reticulum. Like haptophytes, heterokontophytes store sugar in chrysolaminarin granules in the cytoplasm. Heterokontophyte chloroplasts contain chlorophyll \"a\" and with a few exceptions chlorophyll \"c\", but also have carotenoids which give them their many colors.\n\nThe alveolates are a major clade of unicellular eukaryotes of both autotrophic and heterotrophic members. The most notable shared characteristic is the presence of cortical (outer-region) alveoli (sacs). These are flattened vesicles (sacs) packed into a continuous layer just under the membrane and supporting it, typically forming a flexible pellicle (thin skin). In dinoflagellates they often form armor plates. Many members contain a red-algal derived plastid. One notable characteristic of this diverse group is the frequent loss of photosynthesis. However, a majority of these heterotrophs continue to process a non-photosynthetic plastid.\n\nApicomplexans are a group of alveolates. Like the helicosproidia, they're parasitic, and have a nonphotosynthetic chloroplast. They were once thought to be related to the helicosproidia, but it is now known that the helicosproida are green algae rather than part of the CASH lineage. The apicomplexans include \"Plasmodium\", the malaria parasite. Many apicomplexans keep a vestigial red algal derived chloroplast called an apicoplast, which they inherited from their ancestors. Other apicomplexans like \"Cryptosporidium\" have lost the chloroplast completely. Apicomplexans store their energy in amylopectin granules that are located in their cytoplasm, even though they are nonphotosynthetic.\n\nApicoplasts have lost all photosynthetic function, and contain no photosynthetic pigments or true thylakoids. They are bounded by four membranes, but the membranes are not connected to the endoplasmic reticulum. The fact that apicomplexans still keep their nonphotosynthetic chloroplast around demonstrates how the chloroplast carries out important functions other than photosynthesis. Plant chloroplasts provide plant cells with many important things besides sugar, and apicoplasts are no different—they synthesize fatty acids, isopentenyl pyrophosphate, iron-sulfur clusters, and carry out part of the heme pathway. This makes the apicoplast an attractive target for drugs to cure apicomplexan-related diseases. The most important apicoplast function is isopentenyl pyrophosphate synthesis—in fact, apicomplexans die when something interferes with this apicoplast function, and when apicomplexans are grown in an isopentenyl pyrophosphate-rich medium, they dump the organelle.\n\nThe Chromerida is a newly discovered group of algae from Australian corals which comprises some close photosynthetic relatives of the apicomplexans. The first member, \"Chromera velia\", was discovered and first isolated in 2001. The discovery of \"Chromera velia\" with similar structure to the apicomplexanss, provides an important link in the evolutionary history of the apicomplexans and dinophytes. Their plastids have four membranes, lack chlorophyll c and use the type II form of RuBisCO obtained from a horizontal transfer event.\n\nThe dinoflagellates are yet another very large and diverse group of protists, around half of which are (at least partially) photosynthetic.\n\nMost dinophyte chloroplasts are secondary red algal derived chloroplasts. Many other dinophytes have lost the chloroplast (becoming the nonphotosynthetic kind of dinoflagellate), or replaced it though \"tertiary\" endosymbiosis—the engulfment of another eukaryotic algae containing a red algal derived chloroplast. Others replaced their original chloroplast with a green algal derived one.\n\nMost dinophyte chloroplasts contain form II RuBisCO, at least the photosynthetic pigments chlorophyll \"a\", chlorophyll \"c\", \"beta\"-carotene, and at least one dinophyte-unique xanthophyll (peridinin, dinoxanthin, or diadinoxanthin), giving many a golden-brown color. All dinophytes store starch in their cytoplasm, and most have chloroplasts with thylakoids arranged in stacks of three.\n\nThe most common dinophyte chloroplast is the peridinin-type chloroplast, characterized by the carotenoid pigment peridinin in their chloroplasts, along with chlorophyll \"a\" and chlorophyll \"c\". Peridinin is not found in any other group of chloroplasts. The peridinin chloroplast is bounded by three membranes (occasionally two), having lost the red algal endosymbiont's original cell membrane. The outermost membrane is not connected to the endoplasmic reticulum. They contain a pyrenoid, and have triplet-stacked thylakoids. Starch is found outside the chloroplast. An important feature of these chloroplasts is that their chloroplast DNA is highly reduced and fragmented into many small circles. Most of the genome has migrated to the nucleus, and only critical photosynthesis-related genes remain in the chloroplast.\n\nThe peridinin chloroplast is thought to be the dinophytes' \"original\" chloroplast, which has been lost, reduced, replaced, or has company in several other dinophyte lineages.\n\nThe fucoxanthin dinophyte lineages (including \"Karlodinium\" and \"Karenia\") lost their original red algal derived chloroplast, and replaced it with a new chloroplast derived from a haptophyte endosymbiont. \"Karlodinium\" and \"Karenia\" probably took up different heterokontophytes. Because the haptophyte chloroplast has four membranes, tertiary endosymbiosis would be expected to create a six membraned chloroplast, adding the haptophyte's cell membrane and the dinophyte's phagosomal vacuole. However, the haptophyte was heavily reduced, stripped of a few membranes and its nucleus, leaving only its chloroplast (with its original double membrane), and possibly one or two additional membranes around it.\n\nFucoxanthin-containing chloroplasts are characterized by having the pigment fucoxanthin (actually 19′-hexanoyloxy-fucoxanthin and/or 19′-butanoyloxy-fucoxanthin) and no peridinin. Fucoxanthin is also found in haptophyte chloroplasts, providing evidence of ancestry.\n\nSome dinophytes, like \"Kryptoperidinium\" and \"Durinskia\" have a diatom (heterokontophyte) derived chloroplast. These chloroplasts are bounded by up to \"five\" membranes, (depending on whether you count the entire diatom endosymbiont as the chloroplast, or just the red algal derived chloroplast inside it). The diatom endosymbiont has been reduced relatively little—it still retains its original mitochondria, and has endoplasmic reticulum, ribosomes, a nucleus, and of course, red algal derived chloroplasts—practically a complete cell, all inside the host's endoplasmic reticulum lumen. However the diatom endosymbiont can't store its own food—its storage polysaccharide is found in granules in the dinophyte host's cytoplasm instead. The diatom endosymbiont's nucleus is present, but it probably can't be called a nucleomorph because it shows no sign of genome reduction, and might have even been \"expanded\". Diatoms have been engulfed by dinoflagellates at least three times.\n\nThe diatom endosymbiont is bounded by a single membrane, inside it are chloroplasts with four membranes. Like the diatom endosymbiont's diatom ancestor, the chloroplasts have triplet thylakoids and pyrenoids.\n\nIn some of these genera, the diatom endosymbiont's chloroplasts aren't the only chloroplasts in the dinophyte. The original three-membraned peridinin chloroplast is still around, converted to an eyespot.\n\nIn some groups of mixotrophic protists, like some dinoflagellates (e.g. \"Dinophysis\"), chloroplasts are separated from a captured alga and used temporarily. These klepto chloroplasts may only have a lifetime of a few days and are then replaced.\n\nMembers of the genus \"Dinophysis\" have a phycobilin-containing chloroplast taken from a cryptophyte. However, the cryptophyte is not an endosymbiont—only the chloroplast seems to have been taken, and the chloroplast has been stripped of its nucleomorph and outermost two membranes, leaving just a two-membraned chloroplast. Cryptophyte chloroplasts require their nucleomorph to maintain themselves, and \"Dinophysis\" species grown in cell culture alone cannot survive, so it is possible (but not confirmed) that the \"Dinophysis\" chloroplast is a kleptoplast—if so, \"Dinophysis\" chloroplasts wear out and \"Dinophysis\" species must continually engulf cryptophytes to obtain new chloroplasts to replace the old ones.\n\nChloroplasts have their own DNA, often abbreviated as ctDNA, or cpDNA. It is also known as the plastome. Its existence was first proved in 1962, and first sequenced in 1986—when two Japanese research teams sequenced the chloroplast DNA of liverwort and tobacco. Since then, hundreds of chloroplast DNAs from various species have been sequenced, but they're mostly those of land plants and green algae—glaucophytes, red algae, and other algal groups are extremely underrepresented, potentially introducing some bias in views of \"typical\" chloroplast DNA structure and content.\n\nWith few exceptions, most chloroplasts have their entire chloroplast genome combined into a single large circular DNA molecule, typically 120,000–170,000 base pairs long. They can have a contour length of around 30–60 micrometers, and have a mass of about 80–130 million daltons.\n\nWhile usually thought of as a circular molecule, there is some evidence that chloroplast DNA molecules more often take on a linear shape.\n\nMany chloroplast DNAs contain two \"inverted repeats\", which separate a long single copy section (LSC) from a short single copy section (SSC).\nWhile a given pair of inverted repeats are rarely completely identical, they are always very similar to each other, apparently resulting from concerted evolution.\n\nThe inverted repeats vary wildly in length, ranging from 4,000 to 25,000 base pairs long each and containing as few as four or as many as over 150 genes. Inverted repeats in plants tend to be at the upper end of this range, each being 20,000–25,000 base pairs long.\n\nThe inverted repeat regions are highly conserved among land plants, and accumulate few mutations. Similar inverted repeats exist in the genomes of cyanobacteria and the other two chloroplast lineages (glaucophyta and rhodophyceae), suggesting that they predate the chloroplast, though some chloroplast DNAs have since lost or flipped the inverted repeats (making them direct repeats). It is possible that the inverted repeats help stabilize the rest of the chloroplast genome, as chloroplast DNAs which have lost some of the inverted repeat segments tend to get rearranged more.\n\nNew chloroplasts may contain up to 100 copies of their DNA, though the number of chloroplast DNA copies decreases to about 15–20 as the chloroplasts age. They are usually packed into nucleoids, which can contain several identical chloroplast DNA rings. Many nucleoids can be found in each chloroplast.\nIn primitive red algae, the chloroplast DNA nucleoids are clustered in the center of the chloroplast, while in green plants and green algae, the nucleoids are dispersed throughout the stroma.\n\nThough chloroplast DNA is not associated with true histones, in red algae, similar proteins that tightly pack each chloroplast DNA ring into a nucleoid have been found.\n\nThe mechanism for chloroplast DNA (cpDNA) replication has not been conclusively determined, but two main models have been proposed. Scientists have attempted to observe chloroplast replication via electron microscopy since the 1970s. The results of the microscopy experiments led to the idea that chloroplast DNA replicates using a double displacement loop (D-loop). As the D-loop moves through the circular DNA, it adopts a theta intermediary form, also known as a Cairns replication intermediate, and completes replication with a rolling circle mechanism. Transcription starts at specific points of origin. Multiple replication forks open up, allowing replication machinery to transcribe the DNA. As replication continues, the forks grow and eventually converge. The new cpDNA structures separate, creating daughter cpDNA chromosomes.\n\nIn addition to the early microscopy experiments, this model is also supported by the amounts of deamination seen in cpDNA. Deamination occurs when an amino group is lost and is a mutation that often results in base changes. When adenine is deaminated, it becomes hypoxanthine. Hypoxanthine can bind to cytosine, and when the XC base pair is replicated, it becomes a GC (thus, an A → G base change). \nIn cpDNA, there are several A → G deamination gradients. DNA becomes susceptible to deamination events when it is single stranded. When replication forks form, the strand not being copied is single stranded, and thus at risk for A → G deamination. Therefore, gradients in deamination indicate that replication forks were most likely present and the direction that they initially opened (the highest gradient is most likely nearest the start site because it was single stranded for the longest amount of time). This mechanism is still the leading theory today; however, a second theory suggests that most cpDNA is actually linear and replicates through homologous recombination. It further contends that only a minority of the genetic material is kept in circular chromosomes while the rest is in branched, linear, or other complex structures.\n\nOne of competing model for cpDNA replication asserts that most cpDNA is linear and participates in homologous recombination and replication structures similar to bacteriophage T4. It has been established that some plants have linear cpDNA, such as maize, and that more species still contain complex structures that scientists do not yet understand. When the original experiments on cpDNA were performed, scientists did notice linear structures; however, they attributed these linear forms to broken circles. If the branched and complex structures seen in cpDNA experiments are real and not artifacts of concatenated circular DNA or broken circles, then a D-loop mechanism of replication is insufficient to explain how those structures would replicate. At the same time, homologous recombination does not expand the multiple A --> G gradients seen in plastomes. Because of the failure to explain the deamination gradient as well as the numerous plant species that have been shown to have circular cpDNA, the predominant theory continues to hold that most cpDNA is circular and most likely replicates via a D loop mechanism.\n\nThe chloroplast genome most commonly includes around 100 genes that code for a variety of things, mostly to do with the protein pipeline and photosynthesis. As in prokaryotes, genes in chloroplast DNA are organized into operons. Interestingly though, unlike prokaryotic DNA molecules, chloroplast DNA molecules contain introns (plant mitochondrial DNAs do too, but not human mtDNAs).\n\nAmong land plants, the contents of the chloroplast genome are fairly similar.\n\nOver time, many parts of the chloroplast genome were transferred to the nuclear genome of the host, a process called \"endosymbiotic gene transfer\". As a result, the chloroplast genome is heavily reduced compared to that of free-living cyanobacteria. Chloroplasts may contain 60–100 genes whereas cyanobacteria often have more than 1500 genes in their genome. Recently, a plastid without a genome was found, demonstrating chloroplasts can lose their genome during endosymbiotic the gene transfer process.\n\nEndosymbiotic gene transfer is how we know about the lost chloroplasts in many CASH lineages. Even if a chloroplast is eventually lost, the genes it donated to the former host's nucleus persist, providing evidence for the lost chloroplast's existence. For example, while diatoms (a heterokontophyte) now have a red algal derived chloroplast, the presence of many green algal genes in the diatom nucleus provide evidence that the diatom ancestor had a green algal derived chloroplast at some point, which was subsequently replaced by the red chloroplast.\n\nIn land plants, some 11–14% of the DNA in their nuclei can be traced back to the chloroplast, up to 18% in \"Arabidopsis\", corresponding to about 4,500 protein-coding genes. There have been a few recent transfers of genes from the chloroplast DNA to the nuclear genome in land plants.\n\nOf the approximately 3000 proteins found in chloroplasts, some 95% of them are encoded by nuclear genes. Many of the chloroplast's protein complexes consist of subunits from both the chloroplast genome and the host's nuclear genome. As a result, protein synthesis must be coordinated between the chloroplast and the nucleus. The chloroplast is mostly under nuclear control, though chloroplasts can also give out signals regulating gene expression in the nucleus, called \"retrograde signaling\".\n\nProtein synthesis within chloroplasts relies on two RNA polymerases. One is coded by the chloroplast DNA, the other is of nuclear origin. The two RNA polymerases may recognize and bind to different kinds of promoters within the chloroplast genome. The ribosomes in chloroplasts are similar to bacterial ribosomes.\n\nBecause so many chloroplast genes have been moved to the nucleus, many proteins that would originally have been translated in the chloroplast are now synthesized in the cytoplasm of the plant cell. These proteins must be directed back to the chloroplast, and imported through at least two chloroplast membranes.\n\nCuriously, around half of the protein products of transferred genes aren't even targeted back to the chloroplast. Many became exaptations, taking on new functions like participating in cell division, protein routing, and even disease resistance. A few chloroplast genes found new homes in the mitochondrial genome—most became nonfunctional pseudogenes, though a few tRNA genes still work in the mitochondrion. Some transferred chloroplast DNA protein products get directed to the secretory pathway though it should be noted that many secondary plastids are bounded by an outermost membrane derived from the host's cell membrane, and therefore topologically outside of the cell, because to reach the chloroplast from the cytosol, you have to cross the cell membrane, just like if you were headed for the extracellular space. In those cases, chloroplast-targeted proteins do initially travel along the secretory pathway.\n\nBecause the cell acquiring a chloroplast already had mitochondria (and peroxisomes, and a cell membrane for secretion), the new chloroplast host had to develop a unique protein targeting system to avoid having chloroplast proteins being sent to the wrong organelle.\n\nIn most, but not all cases, nuclear-encoded chloroplast proteins are translated with a \"cleavable transit peptide\" that's added to the N-terminus of the protein precursor. Sometimes the transit sequence is found on the C-terminus of the protein, or within the functional part of the protein.\n\nAfter a chloroplast polypeptide is synthesized on a ribosome in the cytosol, an enzyme specific to chloroplast proteins phosphorylates, or adds a phosphate group to many (but not all) of them in their transit sequences.\nPhosphorylation helps many proteins bind the polypeptide, keeping it from folding prematurely. This is important because it prevents chloroplast proteins from assuming their active form and carrying out their chloroplast functions in the wrong place—the cytosol. At the same time, they have to keep just enough shape so that they can be recognized by the chloroplast. These proteins also help the polypeptide get imported into the chloroplast.\n\nFrom here, chloroplast proteins bound for the stroma must pass through two protein complexes—the TOC complex, or \"translocon on the outer chloroplast membrane\", and the TIC translocon, or \"translocon on the inner chloroplast membrane translocon\". Chloroplast polypeptide chains probably often travel through the two complexes at the same time, but the TIC complex can also retrieve preproteins lost in the intermembrane space.\n\nIn land plants, chloroplasts are generally lens-shaped, 3–10 μm in diameter and 1–3 μm thick. Corn seedling chloroplasts are ≈20 µm in volume. Greater diversity in chloroplast shapes exists among the algae, which often contain a single chloroplast that can be shaped like a net (e.g., \"Oedogonium\"), a cup (e.g., \"Chlamydomonas\"), a ribbon-like spiral around the edges of the cell (e.g., \"Spirogyra\"), or slightly twisted bands at the cell edges (e.g., \"Sirogonium\"). Some algae have two chloroplasts in each cell; they are star-shaped in \"Zygnema\", or may follow the shape of half the cell in order Desmidiales. In some algae, the chloroplast takes up most of the cell, with pockets for the nucleus and other organelles, for example, some species of \"Chlorella\" have a cup-shaped chloroplast that occupies much of the cell.\n\nAll chloroplasts have at least three membrane systems—the outer chloroplast membrane, the inner chloroplast membrane, and the thylakoid system. Chloroplasts that are the product of secondary endosymbiosis may have additional membranes surrounding these three. Inside the outer and inner chloroplast membranes is the chloroplast stroma, a semi-gel-like fluid that makes up much of a chloroplast's volume, and in which the thylakoid system floats.\n\nThere are some common misconceptions about the outer and inner chloroplast membranes. The fact that chloroplasts are surrounded by a double membrane is often cited as evidence that they are the descendants of endosymbiotic cyanobacteria. This is often interpreted as meaning the outer chloroplast membrane is the product of the host's cell membrane infolding to form a vesicle to surround the ancestral cyanobacterium—which is not true—both chloroplast membranes are homologous to the cyanobacterium's original double membranes.\n\nThe chloroplast double membrane is also often compared to the mitochondrial double membrane. This is not a valid comparison—the inner mitochondria membrane is used to run proton pumps and carry out oxidative phosphorylation across to generate ATP energy. The only chloroplast structure that can considered analogous to it is the internal thylakoid system. Even so, in terms of \"in-out\", the direction of chloroplast H ion flow is in the opposite direction compared to oxidative phosphorylation in mitochondria. In addition, in terms of function, the inner chloroplast membrane, which regulates metabolite passage and synthesizes some materials, has no counterpart in the mitochondrion.\n\nThe outer chloroplast membrane is a semi-porous membrane that small molecules and ions can easily diffuse across. However, it is not permeable to larger proteins, so chloroplast polypeptides being synthesized in the cell cytoplasm must be transported across the outer chloroplast membrane by the TOC complex, or \"translocon on the outer chloroplast\" membrane.\n\nThe chloroplast membranes sometimes protrude out into the cytoplasm, forming a stromule, or stroma-containing tubule. Stromules are very rare in chloroplasts, and are much more common in other plastids like chromoplasts and amyloplasts in petals and roots, respectively. They may exist to increase the chloroplast's surface area for cross-membrane transport, because they are often branched and tangled with the endoplasmic reticulum. When they were first observed in 1962, some plant biologists dismissed the structures as artifactual, claiming that stromules were just oddly shaped chloroplasts with constricted regions or dividing chloroplasts. However, there is a growing body of evidence that stromules are functional, integral features of plant cell plastids, not merely artifacts.\n\nUsually, a thin intermembrane space about 10–20 nanometers thick exists between the outer and inner chloroplast membranes.\n\nGlaucophyte algal chloroplasts have a peptidoglycan layer between the chloroplast membranes. It corresponds to the peptidoglycan cell wall of their cyanobacterial ancestors, which is located between their two cell membranes. These chloroplasts are called \"muroplasts\" (from Latin \"\"mura\"\", meaning \"wall\"). Other chloroplasts have lost the cyanobacterial wall, leaving an intermembrane space between the two chloroplast envelope membranes.\n\nThe inner chloroplast membrane borders the stroma and regulates passage of materials in and out of the chloroplast. After passing through the TOC complex in the outer chloroplast membrane, polypeptides must pass through the TIC complex \"(translocon on the inner chloroplast membrane)\" which is located in the inner chloroplast membrane.\n\nIn addition to regulating the passage of materials, the inner chloroplast membrane is where fatty acids, lipids, and carotenoids are synthesized.\n\nSome chloroplasts contain a structure called the chloroplast peripheral reticulum. It is often found in the chloroplasts of plants, though it has also been found in some angiosperms, and even some gymnosperms. The chloroplast peripheral reticulum consists of a maze of membranous tubes and vesicles continuous with the inner chloroplast membrane that extends into the internal stromal fluid of the chloroplast. Its purpose is thought to be to increase the chloroplast's surface area for cross-membrane transport between its stroma and the cell cytoplasm. The small vesicles sometimes observed may serve as transport vesicles to shuttle stuff between the thylakoids and intermembrane space.\n\nThe protein-rich, alkaline, aqueous fluid within the inner chloroplast membrane and outside of the thylakoid space is called the stroma, which corresponds to the cytosol of the original cyanobacterium. Nucleoids of chloroplast DNA, chloroplast ribosomes, the thylakoid system with plastoglobuli, starch granules, and many proteins can be found floating around in it. The Calvin cycle, which fixes CO into sugar takes place in the stroma.\n\nChloroplasts have their own ribosomes, which they use to synthesize a small fraction of their proteins. Chloroplast ribosomes are about two-thirds the size of cytoplasmic ribosomes (around 17 nm vs 25 nm). They take mRNAs transcribed from the chloroplast DNA and translate them into protein. While similar to bacterial ribosomes, chloroplast translation is more complex than in bacteria, so chloroplast ribosomes include some chloroplast-unique features.\nSmall subunit ribosomal RNAs in several Chlorophyta and euglenid chloroplasts lack motifs for shine-dalgarno sequence recognition, which is considered essential for translation initiation in most chloroplasts and prokaryotes. Such loss is also rarely observed in other plastids and prokaryotes.\n\nPlastoglobuli (singular \"plastoglobulus\", sometimes spelled \"plastoglobule(s)\"), are spherical bubbles of lipids and proteins about 45–60 nanometers across. They are surrounded by a lipid monolayer. Plastoglobuli are found in all chloroplasts, but become more common when the chloroplast is under oxidative stress, or when it ages and transitions into a gerontoplast. Plastoglobuli also exhibit a greater size variation under these conditions. They are also common in etioplasts, but decrease in number as the etioplasts mature into chloroplasts.\n\nPlastoglubuli contain both structural proteins and enzymes involved in lipid synthesis and metabolism. They contain many types of lipids including plastoquinone, vitamin E, carotenoids and chlorophylls.\n\nPlastoglobuli were once thought to be free-floating in the stroma, but it is now thought that they are permanently attached either to a thylakoid or to another plastoglobulus attached to a thylakoid, a configuration that allows a plastoglobulus to exchange its contents with the thylakoid network. In normal green chloroplasts, the vast majority of plastoglobuli occur singularly, attached directly to their parent thylakoid. In old or stressed chloroplasts, plastoglobuli tend to occur in linked groups or chains, still always anchored to a thylakoid.\n\nPlastoglobuli form when a bubble appears between the layers of the lipid bilayer of the thylakoid membrane, or bud from existing plastoglubuli—though they never detach and float off into the stroma. Practically all plastoglobuli form on or near the highly curved edges of the thylakoid disks or sheets. They are also more common on stromal thylakoids than on granal ones.\n\nStarch granules are very common in chloroplasts, typically taking up 15% of the organelle's volume, though in some other plastids like amyloplasts, they can be big enough to distort the shape of the organelle. Starch granules are simply accumulations of starch in the stroma, and are not bounded by a membrane.\n\nStarch granules appear and grow throughout the day, as the chloroplast synthesizes sugars, and are consumed at night to fuel respiration and continue sugar export into the phloem, though in mature chloroplasts, it is rare for a starch granule to be completely consumed or for a new granule to accumulate.\n\nStarch granules vary in composition and location across different chloroplast lineages. In red algae, starch granules are found in the cytoplasm rather than in the chloroplast. In plants, mesophyll chloroplasts, which do not synthesize sugars, lack starch granules.\n\nThe chloroplast stroma contains many proteins, though the most common and important is Rubisco, which is probably also the most abundant protein on the planet. Rubisco is the enzyme that fixes CO into sugar molecules. In plants, rubisco is abundant in all chloroplasts, though in plants, it is confined to the bundle sheath chloroplasts, where the Calvin cycle is carried out in plants.\n\nThe chloroplasts of some hornworts and algae contain structures called pyrenoids. They are not found in higher plants. Pyrenoids are roughly spherical and highly refractive bodies which are a site of starch accumulation in plants that contain them. They consist of a matrix opaque to electrons, surrounded by two hemispherical starch plates. The starch is accumulated as the pyrenoids mature. In algae with carbon concentrating mechanisms, the enzyme rubisco is found in the pyrenoids. Starch can also accumulate around the pyrenoids when CO is scarce. Pyrenoids can divide to form new pyrenoids, or be produced \"de novo\".\n\nSuspended within the chloroplast stroma is the thylakoid system, a highly dynamic collection of membranous sacks called thylakoids where chlorophyll is found and the light reactions of photosynthesis happen.\nIn most vascular plant chloroplasts, the thylakoids are arranged in stacks called grana, though in certain plant chloroplasts and some algal chloroplasts, the thylakoids are free floating.\n\nUsing a light microscope, it is just barely possible to see tiny green granules—which were named grana. With electron microscopy, it became possible to see the thylakoid system in more detail, revealing it to consist of stacks of flat thylakoids which made up the grana, and long interconnecting stromal thylakoids which linked different grana.\nIn the transmission electron microscope, thylakoid membranes appear as alternating light-and-dark bands, 8.5 nanometers thick.\n\nFor a long time, the three-dimensional structure of the thylakoid system has been unknown or disputed. One model has the granum as a stack of thylakoids linked by helical stromal thylakoids; the other has the granum as a single folded thylakoid connected in a \"hub and spoke\" way to other grana by stromal thylakoids. While the thylakoid system is still commonly depicted according to the folded thylakoid model, it was determined in 2011 that the stacked and helical thylakoids model is correct.\n\nIn the helical thylakoid model, grana consist of a stack of flattened circular granal thylakoids that resemble pancakes. Each granum can contain anywhere from two to a hundred thylakoids, though grana with 10–20 thylakoids are most common. Wrapped around the grana are helicoid stromal thylakoids, also known as frets or lamellar thylakoids. The helices ascend at an angle of 20–25°, connecting to each granal thylakoid at a bridge-like slit junction. The helicoids may extend as large sheets that link multiple grana, or narrow to tube-like bridges between grana. While different parts of the thylakoid system contain different membrane proteins, the thylakoid membranes are continuous and the thylakoid space they enclose form a single continuous labyrinth.\n\nThylakoids (sometimes spelled \"thylakoïds\"), are small interconnected sacks which contain the membranes that the light reactions of photosynthesis take place on. The word \"thylakoid\" comes from the Greek word \"thylakos\" which means \"sack\".\n\nEmbedded in the thylakoid membranes are important protein complexes which carry out the light reactions of photosynthesis. Photosystem II and photosystem I contain light-harvesting complexes with chlorophyll and carotenoids that absorb light energy and use it to energize electrons. Molecules in the thylakoid membrane use the energized electrons to pump hydrogen ions into the thylakoid space, decreasing the pH and turning it acidic. ATP synthase is a large protein complex that harnesses the concentration gradient of the hydrogen ions in the thylakoid space to generate ATP energy as the hydrogen ions flow back out into the stroma—much like a dam turbine.\n\nThere are two types of thylakoids—granal thylakoids, which are arranged in grana, and stromal thylakoids, which are in contact with the stroma. Granal thylakoids are pancake-shaped circular disks about 300–600 nanometers in diameter. Stromal thylakoids are helicoid sheets that spiral around grana. The flat tops and bottoms of granal thylakoids contain only the relatively flat photosystem II protein complex. This allows them to stack tightly, forming grana with many layers of tightly appressed membrane, called granal membrane, increasing stability and surface area for light capture.\n\nIn contrast, photosystem I and ATP synthase are large protein complexes which jut out into the stroma. They can't fit in the appressed granal membranes, and so are found in the stromal thylakoid membrane—the edges of the granal thylakoid disks and the stromal thylakoids. These large protein complexes may act as spacers between the sheets of stromal thylakoids.\n\nThe number of thylakoids and the total thylakoid area of a chloroplast is influenced by light exposure. Shaded chloroplasts contain larger and more grana with more thylakoid membrane area than chloroplasts exposed to bright light, which have smaller and fewer grana and less thylakoid area. Thylakoid extent can change within minutes of light exposure or removal.\n\nInside the photosystems embedded in chloroplast thylakoid membranes are various photosynthetic pigments, which absorb and transfer light energy. The types of pigments found are different in various groups of chloroplasts, and are responsible for a wide variety of chloroplast colorations.\n\nChlorophyll \"a\" is found in all chloroplasts, as well as their cyanobacterial ancestors. Chlorophyll \"a\" is a blue-green pigment partially responsible for giving most cyanobacteria and chloroplasts their color. Other forms of chlorophyll exist, such as the accessory pigments chlorophyll \"b\", chlorophyll \"c\", chlorophyll \"d\", and chlorophyll \"f\".\n\nChlorophyll \"b\" is an olive green pigment found only in the chloroplasts of plants, green algae, any secondary chloroplasts obtained through the secondary endosymbiosis of a green alga, and a few cyanobacteria. It is the chlorophylls \"a\" and \"b\" together that make most plant and green algal chloroplasts green.\n\nChlorophyll \"c\" is mainly found in secondary endosymbiotic chloroplasts that originated from a red alga, although it is not found in chloroplasts of red algae themselves. Chlorophyll \"c\" is also found in some green algae and cyanobacteria.\n\nChlorophylls \"d\" and \"f\" are pigments found only in some cyanobacteria.\n\nIn addition to chlorophylls, another group of yellow–orange pigments called carotenoids are also found in the photosystems. There are about thirty photosynthetic carotenoids. They help transfer and dissipate excess energy, and their bright colors sometimes override the chlorophyll green, like during the fall, when the leaves of some land plants change color. β-carotene is a bright red-orange carotenoid found in nearly all chloroplasts, like chlorophyll \"a\". Xanthophylls, especially the orange-red zeaxanthin, are also common. Many other forms of carotenoids exist that are only found in certain groups of chloroplasts.\n\nPhycobilins are a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts. Phycobilins come in all colors, though phycoerytherin is one of the pigments that makes many red algae red. Phycobilins often organize into relatively large protein complexes about 40 nanometers across called phycobilisomes. Like photosystem I and ATP synthase, phycobilisomes jut into the stroma, preventing thylakoid stacking in red algal chloroplasts. Cryptophyte chloroplasts and some cyanobacteria don't have their phycobilin pigments organized into phycobilisomes, and keep them in their thylakoid space instead.\n\nTo fix carbon dioxide into sugar molecules in the process of photosynthesis, chloroplasts use an enzyme called rubisco. Rubisco has a problem—it has trouble distinguishing between carbon dioxide and oxygen, so at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors. This has the end result of ATP energy being wasted and being released, all with no sugar being produced. This is a big problem, since O is produced by the initial light reactions of photosynthesis, causing issues down the line in the Calvin cycle which uses rubisco.\n\nplants evolved a way to solve this—by spatially separating the light reactions and the Calvin cycle. The light reactions, which store light energy in ATP and NADPH, are done in the mesophyll cells of a leaf. The Calvin cycle, which uses the stored energy to make sugar using rubisco, is done in the bundle sheath cells, a layer of cells surrounding a vein in a leaf.\n\nAs a result, chloroplasts in mesophyll cells and bundle sheath cells are specialized for each stage of photosynthesis. In mesophyll cells, chloroplasts are specialized for the light reactions, so they lack rubisco, and have normal grana and thylakoids, which they use to make ATP and NADPH, as well as oxygen. They store in a four-carbon compound, which is why the process is called \" photosynthesis\". The four-carbon compound is then transported to the bundle sheath chloroplasts, where it drops off and returns to the mesophyll. Bundle sheath chloroplasts do not carry out the light reactions, preventing oxygen from building up in them and disrupting rubisco activity. Because of this, they lack thylakoids organized into grana stacks—though bundle sheath chloroplasts still have free-floating thylakoids in the stroma where they still carry out cyclic electron flow, a light-driven method of synthesizing ATP to power the Calvin cycle without generating oxygen. They lack photosystem II, and only have photosystem I—the only protein complex needed for cyclic electron flow. Because the job of bundle sheath chloroplasts is to carry out the Calvin cycle and make sugar, they often contain large starch grains.\n\nBoth types of chloroplast contain large amounts of chloroplast peripheral reticulum, which they use to get more surface area to transport stuff in and out of them. Mesophyll chloroplasts have a little more peripheral reticulum than bundle sheath chloroplasts.\n\nNot all cells in a multicellular plant contain chloroplasts. All green parts of a plant contain chloroplasts—the chloroplasts, or more specifically, the chlorophyll in them are what make the photosynthetic parts of a plant green. The plant cells which contain chloroplasts are usually parenchyma cells, though chloroplasts can also be found in collenchyma tissue. A plant cell which contains chloroplasts is known as a chlorenchyma cell. A typical chlorenchyma cell of a land plant contains about 10 to 100 chloroplasts.\n\nIn some plants such as cacti, chloroplasts are found in the stems, though in most plants, chloroplasts are concentrated in the leaves. One square millimeter of leaf tissue can contain half a million chloroplasts. Within a leaf, chloroplasts are mainly found in the mesophyll layers of a leaf, and the guard cells of stomata. Palisade mesophyll cells can contain 30–70 chloroplasts per cell, while stomatal guard cells contain only around 8–15 per cell, as well as much less chlorophyll. Chloroplasts can also be found in the bundle sheath cells of a leaf, especially in C plants, which carry out the Calvin cycle in their bundle sheath cells. They are often absent from the epidermis of a leaf.\n\nThe chloroplasts of plant and algal cells can orient themselves to best suit the available light. In low-light conditions, they will spread out in a sheet—maximizing the surface area to absorb light. Under intense light, they will seek shelter by aligning in vertical columns along the plant cell's cell wall or turning sideways so that light strikes them edge-on. This reduces exposure and protects them from photooxidative damage. This ability to distribute chloroplasts so that they can take shelter behind each other or spread out may be the reason why land plants evolved to have many small chloroplasts instead of a few big ones.\nChloroplast movement is considered one of the most closely regulated stimulus-response systems that can be found in plants. Mitochondria have also been observed to follow chloroplasts as they move.\n\nIn higher plants, chloroplast movement is run by phototropins, blue light photoreceptors also responsible for plant phototropism. In some algae, mosses, ferns, and flowering plants, chloroplast movement is influenced by red light in addition to blue light, though very long red wavelengths inhibit movement rather than speeding it up. Blue light generally causes chloroplasts to seek shelter, while red light draws them out to maximize light absorption.\n\nStudies of \"Vallisneria gigantea\", an aquatic flowering plant, have shown that chloroplasts can get moving within five minutes of light exposure, though they don't initially show any net directionality. They may move along microfilament tracks, and the fact that the microfilament mesh changes shape to form a honeycomb structure surrounding the chloroplasts after they have moved suggests that microfilaments may help to anchor chloroplasts in place.\n\nUnlike most epidermal cells, the guard cells of plant stomata contain relatively well-developed chloroplasts. However, exactly what they do is controversial.\n\nPlants lack specialized immune cells—all plant cells participate in the plant immune response. Chloroplasts, along with the nucleus, cell membrane, and endoplasmic reticulum, are key players in pathogen defense. Due to its role in a plant cell's immune response, pathogens frequently target the chloroplast.\n\nPlants have two main immune responses—the hypersensitive response, in which infected cells seal themselves off and undergo programmed cell death, and systemic acquired resistance, where infected cells release signals warning the rest of the plant of a pathogen's presence.\nChloroplasts stimulate both responses by purposely damaging their photosynthetic system, producing reactive oxygen species. High levels of reactive oxygen species will cause the hypersensitive response. The reactive oxygen species also directly kill any pathogens within the cell. Lower levels of reactive oxygen species initiate systemic acquired resistance, triggering defense-molecule production in the rest of the plant.\n\nIn some plants, chloroplasts are known to move closer to the infection site and the nucleus during an infection.\n\nChloroplasts can serve as cellular sensors. After detecting stress in a cell, which might be due to a pathogen, chloroplasts begin producing molecules like salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species which can serve as defense-signals. As cellular signals, reactive oxygen species are unstable molecules, so they probably don't leave the chloroplast, but instead pass on their signal to an unknown second messenger molecule. All these molecules initiate retrograde signaling—signals from the chloroplast that regulate gene expression in the nucleus.\n\nIn addition to defense signaling, chloroplasts, with the help of the peroxisomes, help synthesize an important defense molecule, jasmonate. Chloroplasts synthesize all the fatty acids in a plant cell—linoleic acid, a fatty acid, is a precursor to jasmonate.\n\nOne of the main functions of the chloroplast is its role in photosynthesis, the process by which light is transformed into chemical energy, to subsequently produce food in the form of sugars. Water (HO) and carbon dioxide (CO) are used in photosynthesis, and sugar and oxygen (O) is made, using light energy. Photosynthesis is divided into two stages—the light reactions, where water is split to produce oxygen, and the dark reactions, or Calvin cycle, which builds sugar molecules from carbon dioxide. The two phases are linked by the energy carriers adenosine triphosphate (ATP) and nicotinamide adenine dinucleotide phosphate (NADP).\n\nThe light reactions take place on the thylakoid membranes. They take light energy and store it in NADPH, a form of NADP, and ATP to fuel the dark reactions.\n\nATP is the phosphorylated version of adenosine diphosphate (ADP), which stores energy in a cell and powers most cellular activities. ATP is the energized form, while ADP is the (partially) depleted form. NADP is an electron carrier which ferries high energy electrons. In the light reactions, it gets reduced, meaning it picks up electrons, becoming NADPH.\n\nLike mitochondria, chloroplasts use the potential energy stored in an H, or hydrogen ion gradient to generate ATP energy. The two photosystems capture light energy to energize electrons taken from water, and release them down an electron transport chain. The molecules between the photosystems harness the electrons' energy to pump hydrogen ions into the thylakoid space, creating a concentration gradient, with more hydrogen ions (up to a thousand times as many) inside the thylakoid system than in the stroma. The hydrogen ions in the thylakoid space then diffuse back down their concentration gradient, flowing back out into the stroma through ATP synthase. ATP synthase uses the energy from the flowing hydrogen ions to phosphorylate adenosine diphosphate into adenosine triphosphate, or ATP. Because chloroplast ATP synthase projects out into the stroma, the ATP is synthesized there, in position to be used in the dark reactions.\n\nElectrons are often removed from the electron transport chains to charge NADP with electrons, reducing it to NADPH. Like ATP synthase, ferredoxin-NADP reductase, the enzyme that reduces NADP, releases the NADPH it makes into the stroma, right where it is needed for the dark reactions.\n\nBecause NADP reduction removes electrons from the electron transport chains, they must be replaced—the job of photosystem II, which splits water molecules (HO) to obtain the electrons from its hydrogen atoms.\n\nWhile photosystem II photolyzes water to obtain and energize new electrons, photosystem I simply reenergizes depleted electrons at the end of an electron transport chain. Normally, the reenergized electrons are taken by NADP, though sometimes they can flow back down more H-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP. This is termed cyclic photophosphorylation because the electrons are recycled. Cyclic photophosphorylation is common in plants, which need more ATP than NADPH.\n\nThe Calvin cycle, also known as the dark reactions, is a series of biochemical reactions that fixes CO into G3P sugar molecules and uses the energy and electrons from the ATP and NADPH made in the light reactions. The Calvin cycle takes place in the stroma of the chloroplast.\n\nWhile named \"\"the dark reactions\"\", in most plants, they take place in the light, since the dark reactions are dependent on the products of the light reactions.\n\nThe Calvin cycle starts by using the enzyme Rubisco to fix CO into five-carbon Ribulose bisphosphate (RuBP) molecules. The result is unstable six-carbon molecules that immediately break down into three-carbon molecules called 3-phosphoglyceric acid, or 3-PGA.\nThe ATP and NADPH made in the light reactions is used to convert the 3-PGA into glyceraldehyde-3-phosphate, or G3P sugar molecules. Most of the G3P molecules are recycled back into RuBP using energy from more ATP, but one out of every six produced leaves the cycle—the end product of the dark reactions.\n\nGlyceraldehyde-3-phosphate can double up to form larger sugar molecules like glucose and fructose. These molecules are processed, and from them, the still larger sucrose, a disaccharide commonly known as table sugar, is made, though this process takes place outside of the chloroplast, in the cytoplasm.\n\nAlternatively, glucose monomers in the chloroplast can be linked together to make starch, which accumulates into the starch grains found in the chloroplast.\nUnder conditions such as high atmospheric CO concentrations, these starch grains may grow very large, distorting the grana and thylakoids. The starch granules displace the thylakoids, but leave them intact.\nWaterlogged roots can also cause starch buildup in the chloroplasts, possibly due to less sucrose being exported out of the chloroplast (or more accurately, the plant cell). This depletes a plant's free phosphate supply, which indirectly stimulates chloroplast starch synthesis.\nWhile linked to low photosynthesis rates, the starch grains themselves may not necessarily interfere significantly with the efficiency of photosynthesis, and might simply be a side effect of another photosynthesis-depressing factor.\n\nPhotorespiration can occur when the oxygen concentration is too high. Rubisco cannot distinguish between oxygen and carbon dioxide very well, so it can accidentally add O instead of CO to RuBP. This process reduces the efficiency of photosynthesis—it consumes ATP and oxygen, releases CO, and produces no sugar. It can waste up to half the carbon fixed by the Calvin cycle. Several mechanisms have evolved in different lineages that raise the carbon dioxide concentration relative to oxygen within the chloroplast, increasing the efficiency of photosynthesis. These mechanisms are called carbon dioxide concentrating mechanisms, or CCMs. These include Crassulacean acid metabolism, carbon fixation, and pyrenoids. Chloroplasts in plants are notable as they exhibit a distinct chloroplast dimorphism.\n\nBecause of the H gradient across the thylakoid membrane, the interior of the thylakoid is acidic, with a pH around 4, while the stroma is slightly basic, with a pH of around 8.\nThe optimal stroma pH for the Calvin cycle is 8.1, with the reaction nearly stopping when the pH falls below 7.3.\n\nCO in water can form carbonic acid, which can disturb the pH of isolated chloroplasts, interfering with photosynthesis, even though CO is used in photosynthesis. However, chloroplasts in living plant cells are not affected by this as much.\n\nChloroplasts can pump K and H ions in and out of themselves using a poorly understood light-driven transport system.\n\nIn the presence of light, the pH of the thylakoid lumen can drop up to 1.5 pH units, while the pH of the stroma can rise by nearly one pH unit.\n\nChloroplasts alone make almost all of a plant cell's amino acids in their stroma except the sulfur-containing ones like cysteine and methionine. Cysteine is made in the chloroplast (the proplastid too) but it is also synthesized in the cytosol and mitochondria, probably because it has trouble crossing membranes to get to where it is needed. The chloroplast is known to make the precursors to methionine but it is unclear whether the organelle carries out the last leg of the pathway or if it happens in the cytosol.\n\nChloroplasts make all of a cell's purines and pyrimidines—the nitrogenous bases found in DNA and RNA. They also convert nitrite (NO) into ammonia (NH) which supplies the plant with nitrogen to make its amino acids and nucleotides.\n\nChloroplasts are the site of complex lipid metabolism.\n\nChloroplasts are a special type of a plant cell organelle called a plastid, though the two terms are sometimes used interchangeably. There are many other types of plastids, which carry out various functions. All chloroplasts in a plant are descended from undifferentiated proplastids found in the zygote, or fertilized egg. Proplastids are commonly found in an adult plant's apical meristems. Chloroplasts do not normally develop from proplastids in root tip meristems—instead, the formation of starch-storing amyloplasts is more common.\n\nIn shoots, proplastids from shoot apical meristems can gradually develop into chloroplasts in photosynthetic leaf tissues as the leaf matures, if exposed to the required light. This process involves invaginations of the inner plastid membrane, forming sheets of membrane that project into the internal stroma. These membrane sheets then fold to form thylakoids and grana.\n\nIf angiosperm shoots are not exposed to the required light for chloroplast formation, proplastids may develop into an etioplast stage before becoming chloroplasts. An etioplast is a plastid that lacks chlorophyll, and has inner membrane invaginations that form a lattice of tubes in their stroma, called a prolamellar body. While etioplasts lack chlorophyll, they have a yellow chlorophyll precursor stocked. Within a few minutes of light exposure, the prolamellar body begins to reorganize into stacks of thylakoids, and chlorophyll starts to be produced. This process, where the etioplast becomes a chloroplast, takes several hours. Gymnosperms do not require light to form chloroplasts.\n\nLight, however, does not guarantee that a proplastid will develop into a chloroplast. Whether a proplastid develops into a chloroplast some other kind of plastid is mostly controlled by the nucleus and is largely influenced by the kind of cell it resides in.\n\nPlastid differentiation is not permanent, in fact many interconversions are possible. Chloroplasts may be converted to chromoplasts, which are pigment-filled plastids responsible for the bright colors seen in flowers and ripe fruit. Starch storing amyloplasts can also be converted to chromoplasts, and it is possible for proplastids to develop straight into chromoplasts. Chromoplasts and amyloplasts can also become chloroplasts, like what happens when a carrot or a potato is illuminated. If a plant is injured, or something else causes a plant cell to revert to a meristematic state, chloroplasts and other plastids can turn back into proplastids. Chloroplast, amyloplast, chromoplast, proplast, etc., are not absolute states—intermediate forms are common.\n\nMost chloroplasts in a photosynthetic cell do not develop directly from proplastids or etioplasts. In fact, a typical shoot meristematic plant cell contains only 7–20 proplastids. These proplastids differentiate into chloroplasts, which divide to create the 30–70 chloroplasts found in a mature photosynthetic plant cell. If the cell divides, chloroplast division provides the additional chloroplasts to partition between the two daughter cells.\n\nIn single-celled algae, chloroplast division is the only way new chloroplasts are formed. There is no proplastid differentiation—when an algal cell divides, its chloroplast divides along with it, and each daughter cell receives a mature chloroplast.\n\nAlmost all chloroplasts in a cell divide, rather than a small group of rapidly dividing chloroplasts. Chloroplasts have no definite S-phase—their DNA replication is not synchronized or limited to that of their host cells.\nMuch of what we know about chloroplast division comes from studying organisms like \"Arabidopsis\" and the red alga \"Cyanidioschyzon merolæ\".\n\nThe division process starts when the proteins FtsZ1 and FtsZ2 assemble into filaments, and with the help of a protein ARC6, form a structure called a Z-ring within the chloroplast's stroma. The Min system manages the placement of the Z-ring, ensuring that the chloroplast is cleaved more or less evenly. The protein MinD prevents FtsZ from linking up and forming filaments. Another protein ARC3 may also be involved, but it is not very well understood. These proteins are active at the poles of the chloroplast, preventing Z-ring formation there, but near the center of the chloroplast, MinE inhibits them, allowing the Z-ring to form.\n\nNext, the two plastid-dividing rings, or PD rings form. The inner plastid-dividing ring is located in the inner side of the chloroplast's inner membrane, and is formed first. The outer plastid-dividing ring is found wrapped around the outer chloroplast membrane. It consists of filaments about 5 nanometers across, arranged in rows 6.4 nanometers apart, and shrinks to squeeze the chloroplast. This is when chloroplast constriction begins. In a few species like \"Cyanidioschyzon merolæ\", chloroplasts have a third plastid-dividing ring located in the chloroplast's intermembrane space.\n\nLate into the constriction phase, dynamin proteins assemble around the outer plastid-dividing ring, helping provide force to squeeze the chloroplast. Meanwhile, the Z-ring and the inner plastid-dividing ring break down. During this stage, the many chloroplast DNA plasmids floating around in the stroma are partitioned and distributed to the two forming daughter chloroplasts.\n\nLater, the dynamins migrate under the outer plastid dividing ring, into direct contact with the chloroplast's outer membrane, to cleave the chloroplast in two daughter chloroplasts.\n\nA remnant of the outer plastid dividing ring remains floating between the two daughter chloroplasts, and a remnant of the dynamin ring remains attached to one of the daughter chloroplasts.\n\nOf the five or six rings involved in chloroplast division, only the outer plastid-dividing ring is present for the entire constriction and division phase—while the Z-ring forms first, constriction does not begin until the outer plastid-dividing ring forms.\n\nIn species of algae that contain a single chloroplast, regulation of chloroplast division is extremely important to ensure that each daughter cell receives a chloroplast—chloroplasts can't be made from scratch. In organisms like plants, whose cells contain multiple chloroplasts, coordination is looser and less important. It is likely that chloroplast and cell division are somewhat synchronized, though the mechanisms for it are mostly unknown.\n\nLight has been shown to be a requirement for chloroplast division. Chloroplasts can grow and progress through some of the constriction stages under poor quality green light, but are slow to complete division—they require exposure to bright white light to complete division. Spinach leaves grown under green light have been observed to contain many large dumbbell-shaped chloroplasts. Exposure to white light can stimulate these chloroplasts to divide and reduce the population of dumbbell-shaped chloroplasts.\n\nLike mitochondria, chloroplasts are usually inherited from a single parent. Biparental chloroplast inheritance—where plastid genes are inherited from both parent plants—occurs in very low levels in some flowering plants.\n\nMany mechanisms prevent biparental chloroplast DNA inheritance, including selective destruction of chloroplasts or their genes within the gamete or zygote, and chloroplasts from one parent being excluded from the embryo. Parental chloroplasts can be sorted so that only one type is present in each offspring.\n\nGymnosperms, such as pine trees, mostly pass on chloroplasts paternally, while flowering plants often inherit chloroplasts maternally. Flowering plants were once thought to only inherit chloroplasts maternally. However, there are now many documented cases of angiosperms inheriting chloroplasts paternally.\n\nAngiosperms, which pass on chloroplasts maternally, have many ways to prevent paternal inheritance. Most of them produce sperm cells that do not contain any plastids. There are many other documented mechanisms that prevent paternal inheritance in these flowering plants, such as different rates of chloroplast replication within the embryo.\n\nAmong angiosperms, paternal chloroplast inheritance is observed more often in hybrids than in offspring from parents of the same species. This suggests that incompatible hybrid genes might interfere with the mechanisms that prevent paternal inheritance.\n\nRecently, chloroplasts have caught attention by developers of genetically modified crops. Since, in most flowering plants, chloroplasts are not inherited from the male parent, transgenes in these plastids cannot be disseminated by pollen. This makes plastid transformation a valuable tool for the creation and cultivation of genetically modified plants that are biologically contained, thus posing significantly lower environmental risks. This biological containment strategy is therefore suitable for establishing the coexistence of conventional and organic agriculture. While the reliability of this mechanism has not yet been studied for all relevant crop species, recent results in tobacco plants are promising, showing a failed containment rate of transplastomic plants at 3 in 1,000,000.\n\n"}
{"id": "6357", "url": "https://en.wikipedia.org/wiki?curid=6357", "title": "Camp David", "text": "Camp David\n\nCamp David is the country retreat of the President of the United States. It is located in wooded hills of Catoctin Mountain Park near Thurmont, Maryland, about 62 miles (100 km) north-northwest of Washington, D.C.. It is officially known as the Naval Support Facility Thurmont, because it is technically a military installation, and staffing is primarily provided by the United States Navy and the United States Marine Corps.\n\nOriginally known as Hi-Catoctin, Camp David was built as a camp for federal government agents and their families by the WPA. Construction started in 1935 and was completed in 1938. In 1942, Franklin D. Roosevelt converted it to a presidential retreat and renamed it \"Shangri-La\" (for the fictional Himalayan paradise in the 1933 novel \"Lost Horizon\" by British author James Hilton, which he had jokingly referenced as the source of the Doolittle Raid earlier that year). Camp David received its present name from Dwight D. Eisenhower, in honor of his father and grandson, both named David.\n\nThe Catoctin Mountain Park does not indicate the location of Camp David on park maps due to privacy and security concerns, although it can be seen through the use of publically accessible satellite images.\n\nSince it was built, every president has made use of the retreat.\n\nOn July 2, 2011, an F-15 intercepted a small two-seat passenger plane flying near Camp David, when President Obama was in residence. The civilian aircraft, which was out of radio communication, was intercepted approximately from the presidential retreat. The F-15 escorted the aircraft out of the area, and it landed in nearby Hagerstown, Maryland, without incident. The civilian plane's occupants were flying between two Maryland towns and were released without charge.\n\nOn July 10, 2011, an F-15 intercepted another small two-seat passenger plane flying near Camp David when President Barack Obama was again in residence; a total of three planes were intercepted over that July 9 weekend.\n\n\n"}
{"id": "6359", "url": "https://en.wikipedia.org/wiki?curid=6359", "title": "Crux", "text": "Crux\n\nCrux is a constellation located in the southern sky in a bright portion of the Milky Way. It is among the most easily distinguished constellations, as all of its four main stars have an apparent visual magnitude of at least +2.8, even though it is the smallest of all 88 modern constellations. Its name is Latin for cross, and it is dominated by a cross-shaped or kite-like asterism that is sometimes known as the Southern Cross.\nPredominating is the first-magnitude blue-white star of Alpha Crucis or Acrux, being the constellation's brightest and most southerly member. Crux is followed by four dominate stars, descending in clockwise order by magnitude: Beta, Gamma (one of the closest red giants to Earth), Delta and Epsilon Crucis. Many of these brighter stars are members of the Scorpius–Centaurus Association, a large but loose group of hot blue-white stars that appear to share common origins and motion across the southern Milky Way. The constellation contains four Cepheid variables that are each visible to the naked eye under optimum conditions. Crux also contains the bright and colourful open cluster known as the Jewel Box (NGC 4755) and, to the southwest, partly includes the extensive dark nebula, known as the Coalsack Nebula.\n\nThe stars of the constellation Crux were known to the Ancient Greeks; Ptolemy regarded them as part of the constellation Centaurus. They were entirely visible as far north as Britain in the fourth millennium BC. However, the precession of the equinoxes gradually lowered the stars below the European horizon, and they were eventually forgotten by the inhabitants of northern latitudes. By AD 400, most of the stars in the constellation we now call Crux never rose above the horizon for Athenians.\n\nThe 15th-century Venetian navigator Alvise Cadamosto made note of what was probably the Southern Cross on exiting the Gambia River in 1455, calling it the \"carro dell'ostro\" (\"southern chariot\"). However, Cadamosto's accompanying diagram was inaccurate. Historians generally credit João Faras - astronomer and physician of King Manuel I of Portugal who accompanied Pedro Álvares Cabral in the discovery of Brazil in 1500 - for being the first European to depict it correctly. Faras sketched and described the constellation (calling it \"Las Guardas\") in a letter written on the beaches of Brazil on May 1, 1500, to the Portuguese monarch.\n\nExplorer Amerigo Vespucci seems to have observed not only the Southern Cross but also the neighboring Coalsack Nebula on his second voyage in 1501–02.\n\nAnother early modern description clearly describing Crux as a separate constellation is attributed to Andreas Corsali, an Italian navigator who from 1515 to 1517 sailed to China and the East Indies in an expedition sponsored by King Manuel I. In 1516, Corsali wrote a letter to the monarch describing his observations of the southern sky, which included a rather crude map of the stars around the south celestial pole including the Southern Cross and the two Magellanic Clouds seen in an external orientation, as on a globe.\n\nEmery Molyneux and Petrus Plancius have also been cited as the first uranographers to distinguish Crux as a separate constellation; their representations date from 1592, the former depicting it on his celestial globe and the latter in one of the small celestial maps on his large wall map. Both authors, however, depended on unreliable sources and placed Crux in the wrong position. Crux was first shown in its correct position on the celestial globes of Petrus Plancius and Jodocus Hondius in 1598 and 1600. Its stars were first catalogued separately from Centaurus by Frederick de Houtman in 1603. Later adopters of the constellation included Jakob Bartsch in 1624 and Augustin Royer in 1679. Royer is sometimes wrongly cited as initially distinguishing Crux.\n\nCrux is bordered by the constellations Centaurus (which surrounds it on three sides) on the east, north and west, and Musca to the south. Covering 68 square degrees and 0.165% of the night sky, it is the smallest of the 88 constellations. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Cru'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of four segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between −55.68° and −64.70°. The whole constellation is visible to observers south of latitude 25°N.\n\nIn tropical regions Crux can be seen in the sky from April to June. Crux is exactly opposite to Cassiopeia on the celestial sphere, and therefore it cannot appear in the sky with the latter at the same time. For locations south of 34°S, Crux is circumpolar and thus always visible in the night sky.\n\nCrux is sometimes confused with the nearby False Cross by stargazers. Crux is somewhat kite-shaped (a Latin cross), and it has a fifth star (ε Crucis). The False Cross is diamond-shaped (a Greek cross), somewhat dimmer on average, does not have a fifth star and lacks the two prominent nearby \"Pointer Stars.\"\n\nCrux is easily visible from the southern hemisphere at practically any time of year. It is also visible near the horizon from tropical latitudes of the northern hemisphere for a few hours every night during the northern winter and spring. For instance, it is visible from Cancun or any other place at latitude 25° N or less at around 10 pm at the end of April. There are 5 main stars.\nDue to precession, Crux will move closer to the South Pole in the next millennia, up to 67 degrees south declination for the middle of the constellation. But in AD 18000 or BC 8000 Crux will be/was less than 30 degrees south declination making it visible in Northern Europe. Even in AD 14000 it will be visible for most parts of Europe and the whole United States.\n\nIn the Southern Hemisphere, the Southern Cross is frequently used for navigation in much the same way that Polaris is used in the Northern Hemisphere. Alpha and Gamma (known as Acrux and Gacrux respectively) are commonly used to mark south. Tracing a line from Gacrux to Acrux leads to a point close to the Southern Celestial Pole. Alternatively, if a line is constructed perpendicularly between Alpha Centauri and Beta Centauri, the point where the above-mentioned line and this line intersect marks the Southern Celestial Pole. Another way to find south, strike line through Gacrux and Acrux, 4 1/2 times the distance between Gacrux and Acrux, directly below that point is south. The two stars of Alpha and Beta Centauri are often referred to as the \"Southern Pointers\" or just \"The Pointers\", allowing people to easily find the asterism of the Southern Cross or the constellation of Crux. Very few bright stars of importance lie between Crux and the pole itself, although the constellation Musca is fairly easily recognised immediately beneath Crux.\n\nA technique used in the field is to clench one's right fist and to view the cross, aligning the first knuckle with the axis of the cross. The tip of the thumb will indicate south.\n\nArgentine gauchos are well known for using it for night orientation in the vast Pampas and Patagonic regions.\n\nWithin the constellation's borders, there are 49 stars brighter than or equal to apparent magnitude 6.5. The four main stars that form the asterism are Alpha, Beta, Gamma, and Delta Crucis. Also known as Acrux, Alpha Crucis is a triple star 321 light-years from Earth. Blue-tinged and magnitude 0.8 to the unaided eye, it has two close components of magnitude 1.3 and 1.8, as well as a wide component of magnitude 5. The two close components are resolved in a small amateur telescope and the wide component is readily visible in a pair of binoculars. Beta Crucis, called Mimosa, is a blue-hued giant of magnitude 1.3, 353 light-years from Earth. It is a Beta Cephei-type Cepheid variable with a variation of less than 0.1 magnitudes. Gamma Crucis, called Gacrux, is an optical double star. The primary is a red-hued giant star of magnitude 1.6, 88 light-years from Earth. The secondary is of magnitude 6.5, 264 light-years from Earth. Delta Crucis is a blue-white hued star of magnitude 2.8, 364 light-years from Earth. It is the dimmest of the Southern Cross stars. Like Beta it is a Beta Cepheid.\n\nThere are several dimmer stars within the borders of Crux. Epsilon Crucis is an orange-hued giant star of magnitude 3.6, 228 light-years from Earth. Iota Crucis is a binary star 125 light-years from Earth. The primary is an orange-hued giant of magnitude 4.6 and the secondary is of magnitude 9.5. Mu Crucis is a double star where the unrelated components are about 370 light-years from Earth. The primary is a blue-white hued star of magnitude 4.0 and the secondary is a blue-white hued star of magnitude 5.1. Mu Crucis is divisible in small amateur telescopes or large binoculars.\n\n15 of the 23 brightest stars are blue-white B-type stars. Of the five main cross stars, Delta Crucis and probably Acrux and Mimosa are co-moving B-type members of the Scorpius–Centaurus Association, the nearest OB association to the Sun. They are among the highest-mass stellar members of the Lower Centaurus-Crux subgroup of the association, with ages of roughly 10 to 20 million years. Other members include the blue-white stars Zeta, Lambda, Mu and Mu.\n\nLambda Crucis and Theta Crucis are also Beta Cepheid stars.\n\nCrux boasts four Cepheid variables that reach naked eye visibility. BG Crucis ranges from magnitude 5.34 to 5.58 over 3.3428 days, T Crucis ranges from 6.32 to 6.83 over 6.73331 days, S Crucis ranges from 6.22 to 6.92 over 4.68997 days, and R Crucis ranges from 6.4 to 7.23 over 5.82575 days. BH Crucis, also known as Welch's Red Variable, is a Mira variable that ranges from magnitude 6.6 to 9.8 over 530 days. Discovered in October 1969, it has become redder and brighter (mean magnitude changing from 8.047 to 7.762) and its period lengthened by 25% in the first thirty years since its discovery.\n\nThe star HD 106906 has been found to have a planet—HD 106906 b—that has a larger orbit than any other exoplanet discovered to date.\n\nThe Coalsack Nebula is the most prominent dark nebula in the skies, easily visible to the naked eye as a prominent dark patch in the southern Milky Way. It is large, five degrees by seven degrees, and is 600 light-years from Earth. Not all of the nebula is in the borders of Crux; some of it is technically in Musca and Centaurus.\n\nThe open cluster NGC 4755, better known as the Jewel Box or Crucis Cluster, has an overall magnitude of 4.2—to the naked eye it appears to be a fuzzy star—and is about 7600 light-years from Earth. The cluster was given its name by John Herschel. About seven million years old, an age that makes it one of the youngest open clusters in the Milky Way, it appears to have the shape of a letter A. The Jewel Box Clusters is a Shapley class g and Trumpler class I 3 r cluster; it is a very rich, centrally-concentrated cluster detached from the surrounding star field. It has more than 100 stars that range significantly in brightness. The brightest stars are mostly blue supergiants, though the cluster contains a few bright red supergiants. Kappa Crucis is a true member of the cluster that bears its name, and is one of the brighter stars at magnitude 5.9.\n\nThe most prominent feature of Crux is the distinctive asterism known as the Southern Cross. It has great significance in the cultures of the southern hemisphere, particularly of Australia and New Zealand, whose pioneers were colloquially referred to as sons and daughters of the Southern Cross.\n\nBeginning in the colonial age, Crux became used as a national symbol by several southern nations. The brightest stars of Crux appear on the flags of Australia, Brazil, New Zealand, Papua New Guinea and Samoa. They also appear on the flags of the Australian state of Victoria, the Australian Capital Territory, the Northern Territory, as well as the flag of Magallanes Region of Chile, the flag of Londrina (Brazil) and several Argentine provincial flags and emblems (for example, \"Tierra del Fuego\" and \"Santa Cruz\"). The flag of the Mercosur trading zone displays the four brightest stars. Crux also appears on the Brazilian coat of arms and, as of July 2015, Brazilian Passports.\n\nIn Australia, the Southern Cross played a crucial role as symbol of the Eureka Stockade. In the Eureka Oath from Peter Lalor's famous speech in 1854 under the Eureka Flag he proclaimed \"We swear by the Southern Cross to stand truly by each other and fight to defend our rights and liberties.\" Of the Australian national flag, the Australian poet Banjo Paterson wrote in 1893: The English flag may flutter and wave,<br> where the world wide oceans toss,<br>but the flag the Australian dies to save,<br>is the flag of the Southern Cross.\"\nThe Southern Cross was written into the lyrics of \"Advance Australia Fair\" in 1901: \"Beneath our radiant Southern Cross\"; the song was adopted as the Australian National Anthem in 1984. The victory song of the Australian national cricket team is entitled \"Under the Southern Cross I Stand\".\n\nThe Southern Cross was included in the lyrics of the Brazilian National Anthem (1909): \"\"A imagem do Cruzeiro resplandece\"\" (\"the image of the Cross shines\"). The five stars are also in the logo of the Brazilian football team Cruzeiro Esporte Clube and in the Brazilian coat of arms, and has even been featured as name of currency (the \"cruzeiro\" from 1942 to 1986 and again from 1990 to 1994). The constellation is displayed in all coins of the current series of the Brazilian real.\n\nIt is referenced in several songs and literature, including the Martín Fierro. The Argentinian singer Charly García says that he is \"from the Southern Cross\" in the song \"No voy en tren\".\n\nThe Order of the Southern Cross is a Brazilian order of chivalry awarded to \"those who have rendered significant service to the Brazilian nation.\"\n\nIn O Sweet Saint Martin's Land, the lyrics for the Southern Cross are \"Thy Southern Cross the night\".\n\nA stylized version of Crux appears on the Australian Eureka Flag. The constellation was also used on the dark blue, shield-like patch worn by personnel of the U.S. Army's Americal Division, which was organized in the Southern Hemisphere, on the island of New Caledonia, and also the blue diamond of the U.S. 1st Marine Division, which fought on the Southern Hemisphere islands of Guadalcanal and New Britain.\n\nIn Australian Aboriginal astronomy, Crux and the Coalsack mark the head of the 'Emu in the Sky' in several Aboriginal cultures, while Crux itself is said to be a possum sitting in a tree (Boorong people of the Wimmera region of northwestern Victoria), a representation of the sky deity Mirrabooka (Quandamooka people of Stradbroke Island), a stingray (Yolngu people of Arnhem Land), or an eagle (Kaurna people of the Adelaide Plains). Two Pacific constellations also included Gamma Centauri. Torres Strait Islanders in modern-day Australia saw Gamma Centauri as the handle and the four stars as the trident of Tagai's Fishing Spear. The Aranda people of central Australia saw the four Cross stars as the talon of an eagle and Gamma Centauri as its leg.\n\nVarious peoples in the East Indies and Brazil viewed the four main stars as the body of a ray. In both Indonesia and Malaysia, it is known as \"Bintang Pari\" and \"Buruj Pari\" respectively (\"ray stars\")\n\nThe Javanese people of Indonesia called this constellation \"Gubug pèncèng\" (\"raking hut\") or \"lumbung\" (\"the granary\"), because the shape of the constellation was like that of a raking hut.\n\nThe Māori name for the Southern Cross is \"Te Punga\" (\"the anchor\"). It is thought of as the anchor of Tama-rereti's \"waka\" (the Milky Way), while the Pointers are its rope. In Tonga it is known as \"Toloa\" (\"duck\"); it is depicted as a duck flying south, with one of his wings (δ Crucis) wounded because \"Ongo tangata\" (\"two men\", α and β Centauri) threw a stone at it. The Coalsack is known as \"Humu\" (the \"triggerfish\"), because of its shape. In Samoa the constellation is called \"Sumu\" (\"triggerfish\") because of its rhomboid shape, while α and β Centauri are called \"Luatagata\" (Two Men), just as they are in Tonga. The peoples of the Solomon Islands saw several figures in the Southern Cross. These included a knee protector and a net used to catch Palolo worms. Neighboring peoples in the Marshall Islands saw these stars as a fish.\n\nIn Mapudungun, the language of Patagonian Mapuches, the name of the Southern Cross is \"Melipal\", which means \"four stars\". In Quechua, the language of the Inca civilization, Crux is known as \"Chakana\", which means literally \"stair\" (\"chaka\", bridge, link; \"hanan\", high, above), but carries a deep symbolism within Quechua mysticism. Acrux and Mimosa make up one foot of the Great Rhea, a constellation encompassing Centaurus and Circinus along with the two bright stars. The Great Rhea was a constellation of the Bororo of Brazil. The Mocoví people of Argentina also saw a rhea including the stars of Crux. Their rhea is attacked by two dogs, represented by bright stars in Centaurus and Circinus. The dogs' heads are marked by Alpha and Beta Centauri. The rhea's body is marked by the four main stars of Crux, while its head is Gamma Centauri and its feet are the bright stars of Musca. The Bakairi people of Brazil had a sprawling constellation representing a bird snare. It included the bright stars of Crux, the southern part of Centaurus, Circinus, at least one star in Lupus, the bright stars of Musca, Beta and Delta Chamaeleonis, Volans, and Mensa. The Kalapalo people of Mato Grosso state in Brazil saw the stars of Crux as \"Aganagi\" angry bees having emerged from the Coalsack, which they saw as the beehive.\n\nAmong Tuaregs, the four most visible stars of Crux are considered \"iggaren\", i.e. four \"Maerua crassifolia\" trees. The Tswana people of Botswana saw the constellation as \"Dithutlwa\", two giraffes - Acrux and Mimosa forming a male, and Gacrux and Delta Crucis forming the female.\n\nThe Argentine Air Force acrobatic display team is called \"Cruz del Sur\", the Spanish for \"Southern Cross\".\n\nIn the \"Victory At Sea\" suite, Richard Rodgers wrote \"Beneath The Southern Cross\" to depict the battleships in convoy and the loneliness of the sailors in the Southern Pacific during World War II. This tango melody is also \"No Other Love Have I\" in the musical \"Me and Juliet\" and a popular hit for Perry Como during the 1950s.\n\nMelbourne's Southern Cross Hotel was built and named in 1962 and was one of the city's foremost hotels during the decade. The hotel was demolished in 2005 and replaced by the similarly named office building known as Southern Cross Tower. There is a town in the Western Australian wheatbelt approx 300 km east of Perth called Southern Cross. Melbourne's Spencer Street Station was rebuilt and renamed \"Southern Cross Station\" in 2006.\n\nThe 1974 Australian America's Cup Challenger was named \"Southern Cross\" KA 4 representing the Royal Perth Yacht Club.\n\nPuranic Legend of King Trishanku is explained using the constellation. In Indian Mythology, the cross is imagined as inverted cone (शंकु). The group of three crosses (cones) is hence viewed as Trishanku (three cones). \n\n\"Southern Cross\" is also a 1982 song by the classic rock group Crosby, Stills and Nash, written by Rick Curtis, Michael Curtis, and Stephen Stills. This song was also covered by Jimmy Buffett and is commonly played at his concerts.\n\nAfter identifying a need for a church for Afrikaans speakers living in the Netherlands, a church was established in Leusden and is known as Suiderkruis Kerk. (Southern Cross Church) There is a town called Suiderkruis (Southern Cross) in the Western Cape province of South Africa. The opening lines of South African composer Koos du Plessis' Christmas carol, 'Somerkersfees' (Summer Christmas) are:\n\n\"\" claims that the Sun can be in the \"vicinity\" of Crux: this is seen through the northern hemisphere of the earth.\n\nMark Twain's travelogue \"Following the Equator\" features Twain remarking on the viewing the Southern Cross for the first time, \"It is ingeniously named, for it looks just as a cross would look if it looked like something else.\"\n\n\n\n\n"}
{"id": "6360", "url": "https://en.wikipedia.org/wiki?curid=6360", "title": "Cepheus", "text": "Cepheus\n\nCepheus (Ancient Greek: Κηφεύς \"Kepheús\") may refer to:\n\n\n\n\n\n"}
{"id": "6361", "url": "https://en.wikipedia.org/wiki?curid=6361", "title": "Cassiopeia", "text": "Cassiopeia\n\nCassiopeia () may refer to:\n\n\n\n\n\n"}
{"id": "6362", "url": "https://en.wikipedia.org/wiki?curid=6362", "title": "Cetus", "text": "Cetus\n\nCetus () is a constellation. Its name refers to Cetus, a sea monster in Greek mythology, although it is often called 'the whale' today. Cetus is located in the region of the sky that contains other water-related constellations such as Aquarius, Pisces, and Eridanus.\n\nAlthough Cetus is not generally considered part of the zodiac, the ecliptic passes less than a quarter of a degree from its constellation boundary, and thus the moon, planets, and even part of the sun may be in Cetus for brief periods of time. This is all the more true of asteroids, since their orbits usually have a greater inclination to the ecliptic than the moon and planets. For example, the asteroid 4 Vesta was discovered in this constellation in 1807.\n\nAs seen from Mars, the ecliptic passes into Cetus, with the sun appearing in Cetus for around six days shortly after the northern summer solstice. Mars's orbit is tilted by 1.85° with respect to Earth's.\n\nMira (\"the Wonderful\"), designated Omicron Ceti, was the first variable star to be discovered and the prototype of its class. Over a period of 332 days it reaches a maximum apparent magnitude of 3 - visible to the naked eye - and dips to a minimum magnitude of 10, invisible to the unaided eye. Its seeming appearance and disappearance gave it its common name, which means \"the amazing one\". Mira pulsates with a minimum size of 400 solar diameters and a maximum size of 500 solar diameters. 420 light-years from Earth, it was discovered by David Fabricius in 1596.\n\nα Ceti, traditionally called Menkar (\"the nose\"), is a red-hued giant star of magnitude 2.5, 220 light-years from Earth. It is a wide double star; the secondary is 93 Ceti, a blue-white hued star of magnitude 5.6, 440 light-years away. β Ceti, also called Deneb Kaitos and Diphda, is the brightest star in Cetus. It is an orange-hued giant star of magnitude 2.0, 96 light-years from Earth. The traditional name \"Deneb Kaitos\" means \"the whale's tail\". γ Ceti, Kaffaljidhma (\"head of the whale\") is a very close double star. The primary is a yellow-hued star of magnitude 3.5, 82 light-years from Earth, and the secondary is a blue-hued star of magnitude 6.6. Tau Ceti is noted for being the nearest Sun-like star at a distance of 11.9 light-years. It is a yellow-hued main-sequence star of magnitude 3.5.\n\nAA Ceti is a triple star system; the brightest member has a magnitude of 6.2. The primary and secondary are separated by 8.4 arcseconds at an angle of 304 degrees. The tertiary is not visible in telescopes. AA Ceti is an eclipsing variable star; the tertiary star passes in front of the primary and causes the system's apparent magnitude to decrease by 0.5 magnitudes. UV Ceti is an unusual binary variable star. 8.7 light-years from Earth, the system consists of two red dwarfs. both of magnitude 13. One of the stars is a flare star, which are prone to sudden, random outbursts that last several minutes; these increase the pair's apparent brightness significantly - as high as magnitude 7.\n\nCetus lies far from the galactic plane, so that many distant galaxies are visible, unobscured by dust from the Milky Way. Of these, the brightest is Messier 77 (NGC 1068), a 9th magnitude spiral galaxy near Delta Ceti. It appears face-on and has a clearly visible nucleus of magnitude 10. About 50 million light-years from Earth, M77 is also a Seyfert galaxy and thus a bright object in the radio spectrum. Recently, the galactic cluster JKCS 041 was confirmed to be the most distant cluster of galaxies yet discovered.\n\nThe massive cD galaxy Holmberg 15A is also found in Cetus.\n\nIC 1613 (Caldwell 51) is an irregular dwarf galaxy near the star 26 Ceti and is a member of the Local Group.\n\nNGC 246 (Caldwell 56), also called the Cetus Ring, is a planetary nebula with a magnitude of 8.0, 1600 light-years from Earth. Among some amateur astronomers, NGC 246 has garnered the nickname \"Pac-Man Nebula\" because of the arrangement of its central stars and the surrounding star field.\n\nCetus may have originally been associated with a whale, which would have had mythic status amongst Mesopotamian cultures. It is often now called the Whale, though it is most strongly associated with Cetus the sea-monster, who was slain by Perseus as he saved the princess Andromeda from Poseidon's wrath. Cetus is located in a region of the sky called \"The Sea\" because many water-associated constellations are placed there, including Eridanus, Pisces, Piscis Austrinus, Capricornus, and Aquarius.\n\nCetus has been depicted many ways throughout its history. In the 17th century, Cetus was depicted as a \"dragon fish\" by Johann Bayer. Both Willem Blaeu and Andreas Cellarius depicted Cetus as a whale-like creature in the same century. However, Cetus has also been variously depicted with animal heads attached to a piscine body.\n\nIn Chinese astronomy, the stars of Cetus are found among two areas: the Black Tortoise of the North (北方玄武, \"Běi Fāng Xuán Wǔ\") and the White Tiger of the West (西方白虎, \"Xī Fāng Bái Hǔ\").\n\nThe Brazilian Tukano and Kobeua people used the stars of Cetus to create a jaguar, representing the god of hurricanes and other violent storms. Lambda, Mu, Xi, Nu, Gamma, and Alpha Ceti represented its head; Omicron, Zeta, and Chi Ceti represented its body; Eta Eri, Tau Cet, and Upsilon Cet marked its legs and feet; and Theta, Eta, and Beta Ceti delineated its tail.\n\nIn Hawaii, the constellation was called \"Na Kuhi\", and Mira (Omicron Ceti) may have been called \"Kane\".\n\nUSS Cetus (AK-77) was a United States Navy Crater class cargo ship named after the constellation.\n\n\n\n"}
{"id": "6363", "url": "https://en.wikipedia.org/wiki?curid=6363", "title": "Carina (constellation)", "text": "Carina (constellation)\n\nCarina is a constellation in the southern sky. Its name is Latin for the keel of a ship, and it was formerly part of the larger constellation of Argo Navis (the ship \"Argo\") until that constellation was divided into three pieces, the other two being Puppis (the poop deck), and Vela (the sails of the ship).\n\nCarina was once a part of Argo Navis, the great ship of Jason and the Argonauts who searched for the Golden Fleece. The constellation of Argo was introduced in ancient Greece. However, Nicolas Louis de Lacaille divided Argo into three sections in 1763, including Carina the Keel. In the 19th century, these three became established as separate constellations, and were formally included in the list of 88 modern IAU constellations in 1930. Lacaille kept a single set of Bayer designations for the whole of Argo. Therefore, Carina has the α, β and ε, Vela has γ and δ, Puppis has ζ, and so on.\n\nCarina contains Canopus, a white-hued supergiant that is the second brightest star in the night sky at magnitude −0.72, 313 light-years from Earth. Alpha Carinae, as Canopus is formally designated, is a variable star that varies by approximately 0.1 magnitudes. Its traditional name comes from the mythological Canopus, who was a navigator for Menelaus, king of Sparta.\n\nThere are several other stars above magnitude 3 in Carina. Beta Carinae, traditionally called Miaplacidus, is a blue-white hued star of magnitude 1.7, 111 light-years from Earth. Epsilon Carinae is an orange-hued giant star similarly bright to Miaplacidus at magnitude 1.9; it is 630 light-years from Earth. Another fairly bright star is the blue-white hued Theta Carinae; it is a magnitude 2.7 star 440 light-years from Earth. Theta Carinae is also the most prominent member of the cluster IC 2602. Iota Carinae is a white-hued supergiant star of magnitude 2.2, 690 light-years from Earth.\n\nEta Carinae is the most prominent variable star in Carina; with a mass of approximately 100 solar masses and 4 million times as bright as the Sun. It was first discovered to be unusual in 1677, when its magnitude suddenly rose to 4, attracting the attention of Edmond Halley. Eta Carinae is inside NGC 3372, commonly called the Carina Nebula. It had a long outburst in 1827, when it brightened to magnitude 1, only fading to magnitude 1.5 in 1828. Its most prominent outburst made Eta Carinae the equal of Sirius; it brightened to magnitude −1.5 in 1843. However, since 1843, Eta Carinae has remained relatively placid, having a magnitude between 6.5 and 7.9. However, in 1998, it brightened again, though only to magnitude 5.0, a far less drastic outburst. Eta Carinae is a binary star, with a companion that has a period of 5.5 years; the two stars are surrounded by the Homunculus Nebula, which is composed of gas that was ejected in 1843.\n\nThere are several less prominent variable stars in Carina. l Carinae is a Cepheid variable noted for its brightness; it is the brightest Cepheid that is variable to the unaided eye. It is a yellow-hued supergiant star with a minimum magnitude of 4.2 and a maximum magnitude of 3.3; it has a period of 35.5 days.\n\nTwo bright Mira variable stars are in Carina: R Carinae and S Carinae; both stars are red giants. R Carinae has a minimum magnitude of 10.0 and a maximum magnitude of 4.0. Its period is 309 days and it is 416 light-years from Earth. S Carinae is similar, with a minimum magnitude of 10.0 and a maximum magnitude of 5.0. However, S Carinae has a shorter period – 150 days, though it is much more distant at 1300 light-years from Earth.\n\nCarina is home to several double stars and binary stars. Upsilon Carinae is a binary star with two blue-white hued giant components, 1600 light-years from Earth. The primary is of magnitude 3.0 and the secondary is of magnitude 6.0; the two components are distinguishable in a small amateur telescope.\n\nTwo asterisms are prominent in Carina. One is known as the 'Diamond Cross', which is larger than the Southern Cross (but fainter), and, from the perspective of the southern hemisphere viewer, upside down, the long axes of the two crosses being close to parallel. Another asterism in the constellation is the False Cross, often mistaken for the Southern Cross, which is an asterism in Crux. The False Cross consists of two stars in Carina, Iota Carinae and Epsilon Carinae, and two stars in Vela, Kappa Velorum and Delta Velorum.\n\nCarina is known for its namesake nebula, NGC 3372, discovered by French astronomer Nicolas Louis de Lacaille in 1751, which contains several nebulae. The Carina Nebula overall is an extended emission nebula approximately 8,000 light-years away and 300 light-years wide that includes vast star-forming regions. It has an overall magnitude of 8.0 and an apparent diameter of over 2 degrees. Its central region is called the Keyhole, or the Keyhole Nebula. This was described in 1847 by John Herschel, and likened to a keyhole by Emma Converse in 1873. The Keyhole is about seven light-years wide and is composed mostly of ionized hydrogen, with two major star-forming regions. The Homunculus Nebula is a planetary nebula visible to the naked eye that is being ejected by the erratic luminous blue variable star Eta Carinae, the most massive visible star known. Eta Carinae is so massive that it has reached the theoretical upper limit for the mass of a star and is therefore unstable. It is known for its outbursts; in 1840 it briefly became one of the brightest stars in the sky due to a particularly massive outburst, which largely created the Homunculus Nebula. Because of this instability and history of outbursts, Eta Carinae is considered a prime supernova candidate for the next several hundred thousand years because it has reached the end of its estimated million-year life span.\n\nSince the Milky Way runs through Carina, there are a large number of open clusters in the constellation, embedded in rich star fields. NGC 2516 is an open cluster that is both quite large–approximately half a degree square–and bright, visible to the unaided eye. It is located 1100 light-years from Earth and has approximately 80 stars, the brightest of which is a red giant star of magnitude 5.2. NGC 3114 is another open cluster approximately of the same size, though it is more distant at 3000 light-years from Earth. It is more loose and dim than NGC 2516, as its brightest stars are only 6th magnitude. The most prominent open cluster in Carina is IC 2602, also called the \"Southern Pleiades\". It contains Theta Carinae, along with several other stars visible to the unaided eye, in total, the cluster possesses approximately 60 stars. The Southern Pleiades is particularly large for an open cluster, with a diameter of approximately one degree. Like IC 2602, NGC 3532 is visible to the unaided eye and is of comparable size. It possesses approximately 150 stars that are arranged in an unusual shape, approximating an ellipse with a dark central area. Several prominent orange giants are among the cluster's bright stars, of the 7th magnitude. Superimposed on the cluster is Chi Carinae, a yellow-white hued star of magnitude 3.9, far more distant than NGC 3532.\n\nCarina also contains the naked-eye globular cluster NGC 2808. Epsilon Carinae and Upsilon Carinae are double stars visible in small telescopes.\n\nOne noted galaxy cluster is 1E 0657-56, the Bullet Cluster. At a distance of 4 billion light years (redshift 0.296), this galaxy cluster is named for the shock wave seen in the intracluster medium, which resembles the shock wave of a supersonic bullet. The bow shock visible is thought to be due to the smaller galaxy cluster moving through the intracluster medium at a relative speed of 3000–4000 kilometers per second to the larger cluster. Because this gravitational interaction has been ongoing for hundreds of millions of years, the smaller cluster is being destroyed and will eventually merge with the larger cluster.\n\nCarina contains the radiant of the Eta Carinids meteor shower, which peaks around January 21 each year.\n\nFrom China (especially northern China), the stars of Carina can barely be seen. The star Canopus (the south polar star in Chinese astronomy) was located by Chinese astronomers in the Vermilion Bird of the South (南方朱雀, \"Nán Fāng Zhū Què\"). The rest of the stars were first classified by Xu Guanggi during the Ming Dynasty, based on the knowledge acquired from western star charts, and placed among The Southern Asterisms (近南極星區, \"Jìnnánjíxīngōu\").\n\nPolynesian peoples had no name for the constellation in particular, though they had many names for Canopus. \nThe Māori name \"Ariki\" (\"High-born\"), . and the Hawaiian \"Ke Alii-o-kona-i-ka-lewa\", \"The Chief of the southern expanse\". both attest to the star’s prominence in the southern sky, while the Māori \"Atutahi\", \"First-light\" or \"Single-light\", and the Tuamotu \"Te Tau-rari\" and \"Marere-te-tavahi\", \"He-who-stands-alone\". refer to the star’s solitary nature.\nIt was also called \"Kapae-poto\", (\"Short horizon\"), because it rarely sets from the vantage point of New Zealand; and \"Kauanga\" (\"Solitary\"), when it was the last star visible before sunrise.\n\nCarina is located in the southern sky near the south celestial pole, making it circumpolar for most of the southern hemisphere. Due to precession of Earth's axis, by the year 4700 the south celestial pole will be located in Carina. Three bright stars in Carina will come within 1 degree of the southern celestial pole and take turns as the southern pole star: Omega Carinae (mag 3.29) in 5600, Upsilon Carinae (mag 2.97) in 6700, and Iota Carinae (mag 2.21) in 7900.\n\n was a United States Navy Crater class cargo ship named after the constellation.\n\n\n\n"}
{"id": "6364", "url": "https://en.wikipedia.org/wiki?curid=6364", "title": "Camelopardalis", "text": "Camelopardalis\n\nCamelopardalis or the Giraffe constellation is a large, faint grouping of stars in the northern sky. The constellation was introduced in 1612 (or 1613) by Petrus Plancius. Some older astronomy books give an alternative spelling of the name, Camelopardalus, Camelopardi as well as Camelopardus.\n\nFirst attested in English in 1785, the word \"camelopardalis\" comes from Latin, and it is the romanization of the Greek \"καμηλοπάρδαλις\" meaning \"giraffe\", from \"κάμηλος\" (\"kamēlos\"), \"camel\" + \"πάρδαλις\" (\"pardalis\"), \"leopard\", due to its having a long neck like a camel and spots like a leopard.\n\nAlthough Camelopardalis is the 18th largest constellation, it is not a particularly bright constellation, as the brightest stars are only of fourth magnitude. In fact, it only contains four stars below (brighter than) magnitude 5.0.\n\n\nOther variable stars are U Camelopardalis, VZ Camelopardalis, and Mira variables T Camelopardalis, X Camelopardalis, and R Camelopardalis. RU Camelopardalis is one of the brighter Type II Cepheids visible in the night sky.\n\nIn 2011 a supernova was discovered in the constellation.\n\nCamelopardalis is in the part of the celestial sphere facing away from the galactic plane. Accordingly, many distant galaxies are visible within its borders. NGC 2403 is a galaxy in the M81 group of galaxies, located approximately 12 million light-years from Earth with a redshift of 0.00043. It is classified as being between an elliptical and a spiral galaxy because it has faint arms and a large central bulge. NGC 2403 was first discovered by the 18th century astronomer William Herschel, who was working in England at the time. It has an integrated magnitude of 8.0 and is approximately 0.25° long.\n\nNGC 1502 is a magnitude 6.9 open cluster about 3,000 light years from Earth. It has about 45 bright members, and features a double star of magnitude 7.0 at its center. NGC 1502 is also associated with Kemble's Cascade, a simple but beautiful asterism appearing in the sky as a chain of stars 2.5° long that is parallel to the Milky Way and is pointed towards Cassiopeia. NGC 1501 is a planetary nebula located roughly 1.4° south of NGC 1502.\n\nIC 342 is one of the brightest two galaxies in the IC 342/Maffei Group of galaxies. The dwarf irregular galaxy NGC 1569 is a magnitude 11.9 starburst galaxy, about 11 million light years away. NGC 2655 is a large lenticular galaxy with visual magnitude 10.1.\n\nMS0735.6+7421 is a galaxy cluster with a redshift of 0.216, located 2.6 billion light-years from Earth. It is unique for its intracluster medium, which emits X-rays at a very high rate. This galaxy cluster features two cavities 600,000 light-years in diameter, caused by its central supermassive black hole, which emits jets of matter. MS0735.6+7421 is one of the largest and most distant examples of this phenomenon.\n\nTombaugh 5 is a fairly dim open cluster in Camelopardalis. It has an overall magnitude of 8.4 and is located 5,800 light-years from Earth. It is a Shapley class c and Trumpler class III 1 r cluster, meaning that it is irregularly shaped and appears loose. Though it is detached from the star field, it is not concentrated at its center at all. It has more than 100 stars which do not vary widely in brightness, mostly being of the 15th and 16th magnitude.\n\nNGC 2146 is an 11th magnitude barred spiral starburst galaxy conspicuously warped by interaction with a neighbour.\n\nMACS0647-JD, one of the possible candidates for the farthest known galaxies in the universe (z= 10.7), is also in Camelopardalis.\n\nThe annual May meteor shower Camelopardalids from comet 2009P/LINEAR have a radiant in Camelopardalis.\n\nThe space probe \"Voyager 1\" is moving in the direction of this constellation, though it will not be nearing any of the stars in this constellation for many thousands of years, by which time its power source will be long dead.\n\nCamelopardalis was created by Petrus Plancius in 1613 to represent the animal Rebecca rode to marry Isaac in the Bible. It first appeared in a globe designed by him and produced by Pieter van den Keere. One year later, Jakob Bartsch featured it in his atlas. Johannes Hevelius depicted this constellation in his works which were so influential that it was referred to as Camelopardali Hevelii or abbreviated as Camelopard. Hevel.\n\nPart of the constellation was hived off to form the constellation Sciurus Volans, the Flying Squirrel, by William Croswell in 1810. However this was not taken up by later cartographers.\nH. A. Rey has suggested an alternative way to connect the stars of Camelopardalis into a giraffe figure.\n\nThe giraffe's body consists of the quadrangle of stars α Cam, β Cam, BE Cam, and γ Cam: α Cam and β Cam being of the fourth magnitude. The stars HD 42818 (HR 2209) and M Cam form the head of the giraffe, and the stars M Cam and α Cam form the giraffe's long neck. Stars β Cam and 7 Cam form the giraffe's front leg, and variable stars BE Cam and CS Cam form the giraffe's hind leg.\n\nIn Chinese astronomy, the stars of Camelopardalis are located within a group of circumpolar stars called the Purple Forbidden Enclosure (紫微垣 \"Zǐ Wēi Yuán\").\n\n\n\n"}
{"id": "6365", "url": "https://en.wikipedia.org/wiki?curid=6365", "title": "Convention of Kanagawa", "text": "Convention of Kanagawa\n\nOn March 31, 1854, the or was the first treaty between the United States of America and the Tokugawa Shogunate.\n\nSigned under threat of force, it effectively meant the end of Japan’s 220-year-old policy of national seclusion (\"sakoku\"), by opening the ports of Shimoda and Hakodate to American vessels. It also ensured the safety of American castaways and established the position of an American consul in Japan. The treaty also precipitated the signing of similar treaties establishing diplomatic relations with other western powers.\n\nSince the beginning of the seventeenth century, the Tokugawa shogunate pursued a policy of isolating the country from outside influences. Foreign trade was maintained only with the Dutch and the Chinese and was conducted exclusively at Nagasaki under a strict government monopoly. This policy had two main objectives. One was the fear that trade with western powers and the spread of Christianity would serve as a pretext for the invasion of Japan by imperialist forces, as had been the case with most of the nations of Asia. The second objective was fear that foreign trade and the wealth developed would lead to the rise of a \"daimyō\" powerful enough to overthrow the ruling Tokugawa clan.\n\nBy the early nineteenth century, this policy of isolation was increasingly under challenge. In 1844, King William II of the Netherlands sent a letter urging Japan to end the isolation policy on its own before change would be forced from the outside. In 1846, an official American expedition led by Commodore James Biddle arrived in Japan asking for ports to be opened for trade, but was sent away.\n\nIn 1852, United States Navy Commodore Matthew Perry was sent with a fleet of warships by American President Millard Fillmore to force the opening of Japanese ports to American trade, through the use of gunboat diplomacy if necessary. The growing commerce between America and China, the presence of American whalers in waters offshore Japan, and the increasing monopolization of potential coaling stations by the British and French in Asia were all contributing factors. The Americans were also driven by concepts of Manifest Destiny and the desire to impose the benefits of western civilization on what they perceived as backward Asian nations. For the Japanese standpoint, increasing contacts with foreign warships and the increasing disparity between western military technology and the Japanese feudal armies created growing concern. The Japanese had been keeping abreast of world events via information gathered from Dutch traders in Dejima and had been forewarned by the Dutch of Perry’s voyage. There was considerable internal debate in Japan on how best to meet this potential threat to Japan’s economic and political sovereignty in light of events occurring in China with the Opium Wars.\n\nPerry arrived with four warships at Uraga, at the mouth of Edo Bay on July 8, 1853. After refusing Japanese demands that he proceed to Nagasaki, which was the designated port for foreign contact, and after threatening to continue directly on to Edo, the nation’s capital and to burn it to the ground if necessary, he was allowed to land at nearby Kurihama on July 14 and to deliver his letter.\n\nDespite years of debate on the isolation policy, Perry’s letter created great controversy within the highest levels of the Tokugawa shogunate. The Shogun himself, Tokugawa Ieyoshi died days after Perry’s departure, and was succeeded by his sickly young son, Tokugawa Iesada, leaving effective administration in the hands of the Council of Elders (\"rōjū\") led by Abe Masahiro. Abe felt that it was currently impossible for Japan to resist the American demands by military force, and yet was reluctant to take any action on his own authority for such an unprecedented situation. Attempting to legitimize any decision taken, Abe polled all of the daimyō for their opinions. This was the first time that the Tokugawa shogunate had allowed its decision-making to be a matter of public debate, and had the unforeseen consequence of portraying the Shogunate as weak and indecisive. The results of the poll also failed to provide Abe with an answer, as of the 61 known responses, 19 were in favor of accepting the American demands, and 19 were equally opposed. Of the remainder, 14 gave vague responses expressing concern of possible war, 7 suggested making temporary concessions and two advised that they would simply go along with whatever was decided.\n\nPerry returned again on February 13, 1854, with an even larger force of eight warships, and made it clear that he would not be leaving until a treaty was signed. Negotiations began on March 8 and proceeded for around one month. The Japanese side gave in to almost all of Perry’s demands, with the exception of a commercial agreement modeled after previous American treaties with China, which Perry agreed to defer to a later time. The main controversy centered on the selection of the ports to open, with Perry adamantly rejecting Nagasaki. The treaty, written in English, Dutch, Chinese and Japanese was signed on 31 March 1854 at what is now known as Kaiko Hiroba (Port Opening Square)Yokohama, a site adjacent to the current Yokohama Archives of History.\n\nThe \"Japan–US Treaty of Peace and Amity\" has twelve articles: \nThe final article, Article Twelve, stipulated that the terms of the treaty were to be ratified by the President of the United States and the “August Sovereign of Japan” within 18 months. At the time, Shogun Tokugawa Iesada was the de facto ruler of Japan; for the Emperor to interact in any way with foreigners was out of the question. Perry concluded the treaty with representatives of the shogun, led by plenipotentiary and the text was endorsed subsequently, albeit reluctantly, by Emperor Kōmei.\nThe treaty was ratified on 21 February 1855.\n\nIn the short-term, both sides were satisfied with the agreement. Perry had achieved his primary objective of breaking Japan’s \"sakoku\" policy and setting the grounds for protection of American citizens and an eventual commercial agreement. The Tokugawa shogunate could point out that the treaty was not actually signed by the Shogun, or indeed any of his \"rōjū\", and by the agreement made, had at least temporarily averted the possibility of immediate military confrontation.\n\nExternally, the treaty led to the United States-Japan Treaty of Amity and Commerce, the \"Harris Treaty\" of 1858, which allowed the establishment of foreign concessions, extraterritoriality for foreigners, and minimal import taxes for foreign goods. The Japanese chafed under the \"unequal treaty system\" which characterized Asian and western relations during this period. The Kanagawa treaty was also followed by similar agreements with the United Kingdom (Anglo-Japanese Friendship Treaty, October 1854), the Russians (Treaty of Shimoda, 7 February 1855), and the French (Treaty of Amity and Commerce between France and Japan, 9 October 1858).\n\nInternally, the treaty had far-reaching consequences. Decisions to suspend previous restrictions on military activities led to re-armament by many domains and further weakened the position of the Shogun. Debate over foreign policy and popular outrage over perceived appeasement to the foreign powers was a catalyst for the \"sonnō jōi\" movement and a shift in political power from Edo back to the Imperial Court in Kyoto. The opposition of Emperor Kōmei to the treaties further lent support to the tōbaku (overthrow the Shogunate) movement, and eventually to the Meiji Restoration.\n\nThe Convention was negotiated and then signed in a purpose-built house in Yokohama, Japan, the site of which is now the Yokohama Archives of History.\n\n\n\n"}
{"id": "6366", "url": "https://en.wikipedia.org/wiki?curid=6366", "title": "Canis Major", "text": "Canis Major\n\nCanis Major is a constellation in the southern celestial hemisphere. In the second century, it was included in Ptolemy's 48 constellations, and is counted among the 88 modern constellations. Its name is Latin for \"greater dog\" in contrast to Canis Minor, the \"lesser dog\"; both figures are commonly represented as following the constellation of Orion the hunter through the sky. The Milky Way passes through Canis Major and several open clusters lie within its borders, most notably M41.\n\nCanis Major contains Sirius, the brightest star in the night sky, known as the \"dog star\". It is bright because of its proximity to the Solar System. In contrast, the other bright stars of the constellation are stars of great distance and high luminosity. At magnitude 1.5, Epsilon Canis Majoris (Adhara) is the second-brightest star of the constellation and the brightest source of extreme ultraviolet radiation in the night sky. Next in brightness are the yellow-white supergiant Delta (Wezen) at 1.8, the blue-white giant Beta (Mirzam) at 2.0, blue-white supergiants Eta (Aludra) at 2.4 and Omicron at 3.0, and white spectroscopic binary Zeta (Furud), also at 3.0. The red hypergiant VY Canis Majoris is one of the largest stars known, while the neutron star RX J0720.4-3125 has a radius of a mere 5 km.\n\nIn ancient Mesopotamia, Sirius, named KAK.SI.DI by the Babylonians, was seen as an arrow aiming towards Orion, while the southern stars of Canis Major and a part of Puppis were viewed as a bow, named BAN in the \"Three Stars Each\" tablets, dating to around 1100 BC. In the later compendium of Babylonian astronomy and astrology titled \"MUL.APIN\", the arrow, Sirius, was also linked with the warrior Ninurta, and the bow with Ishtar, daughter of Enlil. Ninurta was linked to the later deity Marduk, who was said to have slain the ocean goddess Tiamat with a great bow, and worshipped as the principal deity in Babylon. The Ancient Greeks replaced the bow and arrow depiction with that of a dog.\n\nIn Greek Mythology, Canis Major represented the dog Laelaps, a gift from Zeus to Europa; or sometimes the hound of Procris, Diana's nymph; or the one given by Aurora to Cephalus, so famed for its speed that Zeus elevated it to the sky. It was also considered to represent one of Orion's hunting dogs, pursuing Lepus the Hare or helping Orion fight Taurus the Bull; and is referred to in this way by Aratos, Homer and Hesiod. The ancient Greeks refer only to one dog, but by Roman times, Canis Minor appears as Orion's second dog. Alternative names include Canis Sequens and Canis Alter. Canis Syrius was the name used in the 1521 \"Alfonsine tables\".\n\nThe Roman myth refers to Canis Major as \"Custos Europae\", the dog guarding Europa but failing to prevent her abduction by Jupiter in the form of a bull, and as \"Janitor Lethaeus\", \"the watchdog\". In medieval Arab astronomy, the constellation became \"al-Kalb al-Akbar\", \"the Greater Dog\", transcribed as \"Alcheleb Alachbar\" by 17th century writer Edmund Chilmead. Islamic scholar Abū Rayḥān al-Bīrūnī referred to Orion as \"Kalb al-Jabbār\", \"the Dog of the Giant\". Among the Merazig of Tunisia, shepherds note six constellations that mark the passage of the dry, hot season. One of them, called \"Merzem\", includes the stars of Canis Major and Canis Minor and is the herald of two weeks of hot weather.\n\nIn Chinese astronomy, the modern constellation of Canis Major lies in the Vermilion Bird (南方朱雀, \"Nán Fāng Zhū Què\"), where the stars were classified in several separate asterisms of stars. The Military Market (\"Jūnshì\" 軍市) was a circular pattern of stars containing Nu, Beta, Xi and Xi, and some stars from Lepus. The Wild Cockerel (\"Yějī\" 野雞) was at the centre of the Military Market, although it is uncertain which stars depicted what. Schlegel reported that the stars Omicron and Pi Canis Majoris might have been them, while Beta or Nu have also been proposed. Sirius was \"Tiānláng\" (天狼), the Celestial Wolf, denoting invasion and plunder. Southeast of the Wolf was the asterism \"Húshǐ\" (弧矢), the celestial Bow and Arrow, which was interpreted as containing Delta, Epsilon, Eta and Kappa Canis Majoris and Delta Velorum. Alternatively, the arrow was depicted by Omicron and Eta and aiming at Sirius (the Wolf), while the bow comprised Kappa, Epsilon, Sigma, Delta and 164 Canis Majoris, and Pi and Omicron Puppis.\n\nBoth the Māori people and the people of the Tuamotus recognized the figure of Canis Major as a distinct entity, though it was sometimes absorbed into other constellations. \"Te Huinga-o-Rehua\", also called \"Te Putahi-nui-o-Rehua\" and \"Te Kahui-Takurua\", (\"The Assembly of Rehua\" or \"The Assembly of Sirius\") was a Maori constellation that included both Canis Minor and Canis Major, along with some surrounding stars. Related was \"Taumata-o-Rehua\", also called \"Pukawanui\", the Mirror of Rehua, formed from an undefined group of stars in Canis Major. They called Sirius \"Rehua\" and \"Takarua\", corresponding to two of the names for the constellation, though \"Rehua\" was a name applied to other stars in various Maori groups and other Polynesian cosmologies. The Tuamotu people called Canis Major \"Muihanga-hetika-o-Takurua\", \"the abiding assemblage of Takurua\".\n\nThe Tharumba people of the Shoalhaven River saw three stars of Canis Major as \"Wunbula\" (Bat) and his two wives \"Murrumbool\" (Mrs Brown Snake) and \"Moodtha\" (Mrs Black Snake); bored of following their husband around, the women try to bury him while he is hunting a wombat down its hole. He spears them and all three are placed in the sky as the constellation \"Munowra\". To the Boorong people of Victoria, Sigma Canis Majoris was \"Unurgunite\", and its flanking stars Delta and Epsilon were his two wives. The moon (\"Mityan,\" \"native cat\") sought to lure the further wife (Epsilon) away, but Unurgunite assaulted him and he has been wandering the sky ever since.\n\nCanis Major is a constellation in the Southern Hemisphere's summer (or northern hemisphere's winter) sky, bordered by Monoceros (which lies between it and Canis Minor) to the north, Puppis to the east and southeast, Columba to the southwest, and Lepus to the west. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'CMa'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a quadrilateral; in the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between −11.03° and −33.25°. Covering 380 square degrees or 0.921% of the sky, it ranks 43rd of the 88 currently-recognized constellations in size.\n\nCanis Major is a prominent constellation because of its many bright stars. These include Sirius (Alpha Canis Majoris), the brightest star in the night sky, as well as three other stars above magnitude 2.0. Furthermore, two other stars are thought to have previously outshone all others in the night sky—Adhara (Epsilon Canis Majoris) shone at -3.99 around 4.7 million years ago, and Mirzam (Beta Canis Majoris) peaked at −3.65 around 4.42 million years ago. Another, NR Canis Majoris, will be brightest at magnitude −0.88 in about 2.87 million years' time.\n\nThe German cartographer Johann Bayer used the Greek letters Alpha through Omicron to label the most prominent stars in the constellation, including three adjacent stars as Nu and two further pairs as Xi and Omicron, while subsequent observers designated further stars in the southern parts of the constellation that were hard to discern from Central Europe. Bayer's countryman Johann Elert Bode later added Sigma, Tau and Omega; the French astronomer Nicolas Louis de Lacaille added lettered stars a to k (though none are in use today). John Flamsteed numbered 31 stars, with 3 Canis Majoris being placed by Lacaille into Columba as Delta Columbae (Flamsteed had not recognised Columba as a distinct constellation). He also labelled two stars—his 10 and 13 Canis Majoris—as Kappa and Kappa respectively, but subsequent cartographers such as Francis Baily and John Bevis dropped the fainter former star, leaving Kappa as the sole Kappa. Flamsteed's listing of Nu, Nu, Nu, Xi, Xi, Omicron and Omicron have all remained in use.\n\nSirius is the brightest star in the night sky at apparent magnitude −1.46 and one of the closest stars to Earth at a distance of 8.6 light-years. Its name comes from the Greek word for \"scorching\" or \"searing\". Sirius is also a binary star; its companion Sirius B is a white dwarf with a magnitude of 8.4—10,000 times fainter than Sirius A to observers on Earth. The two orbit each other every 50 years. Their closest approach last occurred in 1993 and they will be at their greatest separation between 2020 and 2025. Sirius was the basis for the ancient Egyptian calendar. The star marked the Great Dog's mouth on Bayer's star atlas.\n\nFlanking Sirius are Beta and Gamma Canis Majoris. Also called Mirzam or Murzim, Beta is a blue-white Beta Cephei variable star of magnitude 2.0, which varies by a few hundredths of a magnitude over a period of six hours. Mirzam is 500 light-years from Earth, and its traditional name means \"the announcer\", referring to its position as the \"announcer\" of Sirius, as it rises a few minutes before Sirius does. Gamma, also known as Muliphein, is a fainter star of magnitude 4.12, in reality a blue-white bright giant of spectral type B8IIe located 441 light-years from earth. Iota Canis Majoris, lying between Sirius and Gamma, is another star that has been classified as a Beta Cephei variable, varying from magnitude 4.36 to 4.40 over a period of 1.92 hours. It is a remote blue-white supergiant star of spectral type B3Ib, around 46,000 times as luminous as the sun and, at 2500 light-years distant, 300 times further away than Sirius.\n\nEpsilon, Omicron, Delta, and Eta Canis Majoris were called \"Al Adzari\" \"the virgins\" in medieval Arabic tradition. Marking the dog's right thigh on Bayer's atlas is Epsilon Canis Majoris, also known as Adhara. At magnitude 1.5, it is the second-brightest star in Canis Major and the 23rd-brightest star in the sky. It is a blue-white supergiant of spectral type B2Iab, around 404 light-years from Earth. This star is one of the brightest known extreme ultraviolet sources in the sky. It is a binary star; the secondary is of magnitude 7.4. Its traditional name means \"the virgins\", having been transferred from the group of stars to Epsilon alone. Nearby is Delta Canis Majoris, also called Wezen. It is a yellow-white supergiant of spectral type F8Iab and magnitude 1.84, around 1605 light-years from Earth. With a traditional name meaning \"the weight\", Wezen is 17 times as massive and 50,000 times as luminous as the Sun. If located in the centre of the Solar System, it would extend out to Earth as its diameter is 200 times that of the Sun. Only around 10 million years old, Wezen has stopped fusing hydrogen in its core. Its outer envelope is beginning to expand and cool, and in the next 100,000 years it will become a red supergiant as its core fuses heavier and heavier elements. Once it has a core of iron, it will collapse and explode as a supernova. Nestled between Adhara and Wezen lies Sigma Canis Majoris, known as Unurgunite to the Boorong and Wotjobaluk people, a red supergiant of spectral type K7Ib that varies irregularly between magnitudes 3.43 and 3.51.\n\nAlso called Aludra, Eta Canis Majoris is a blue-white supergiant of spectral type B5Ia with a luminosity 176,000 times and diameter around 80 times that of the Sun. Classified as an Alpha Cygni type variable star, Aludra varies in brightness from magnitude 2.38 to 2.48 over a period of 4.7 days. It is located 1120 light-years away. To the west of Adhara lies 3.0-magnitude Zeta Canis Majoris or Furud, around 362 light-years distant from Earth. It is a spectroscopic binary, whose components orbit each other every 1.85 years, the combined spectrum indicating a main star of spectral type B2.5V.\n\nBetween these stars and Sirius lie Omicron, Omicron, and Pi Canis Majoris. Omicron is a massive supergiant star about 21 times as massive as the Sun. Only 7 million years old, it has exhausted the supply of hydrogen at its core and is now processing helium. It has an Alpha Cygni variable that undergoes periodic non-radial pulsations, which cause its brightness to cycle from magnitude 2.93 to 3.08 over a 24.44-day interval. Omicron is an orange K-type supergiant of spectral type K2.5Iab that is an irregular variable star, varying between apparent magnitudes 3.78 and 3.99. Around 18 times as massive as the Sun, it shines with 65,000 times its luminosity.\n\nNorth of Sirius lie Theta and Mu Canis Majoris, Theta being the most northerly star with a Bayer designation in the constellation. Around 8 billion years old, it is an orange giant of spectral type K4III that is around as massive as the Sun but has expanded to 30 times the Sun's diameter. Mu is a multiple star system located around 1244 light-years distant, its components discernible in a small telescope as a 5.3-magnitude yellow-hued and 7.1-magnitude bluish star. The brighter star is a giant of spectral type K2III, while the companion is a main sequence star of spectral type B9.5V. Nu Canis Majoris is a yellow-hued giant star of magnitude 5.7, 278 light-years away; it is at the threshold of naked-eye visibility. It has a companion of magnitude 8.1.\n\nAt the southern limits of the constellation lie Kappa and Lambda Canis Majoris. Although of similar spectra and nearby each other as viewed from Earth, they are unrelated. Kappa is a Gamma Cassiopeiae variable of spectral type B2Vne, which brightened by 50% between 1963 and 1978, from magnitude 3.96 or so to 3.52. It is around 659 light-years distant. Lambda is a blue-white B-type main sequence dwarf with an apparent magnitude of 4.48 located around 423 light-years from Earth. It is 3.7 times as wide as and 5.5 times as massive as the Sun, and shines with 940 times its luminosity.\n\nCanis Major is also home to many variable stars. EZ Canis Majoris is a Wolf–Rayet star of spectral type WN4 that varies between magnitudes 6.71 and 6.95 over a period of 3.766 days; the cause of its variability is unknown but thought to be related to its stellar wind and rotation. VY Canis Majoris is one of the largest stars known, a remote red supergiant located around 3800 light-years away from Earth. Estimates of its size, mass and luminosity have varied, with figures of 600 to 3000 times the radius, and 60,000 to 500,000 times the luminosity of the Sun. It was observed in 2011 using interferometry with the Very Large Telescope, yielding a radius of 1420 ± 120 solar radii, surface temperature of around 3490 K (and hence spectral type M4Ia) and a luminosity 270,000 times that of the Sun. Its current mass has been revised at 9–25 solar masses, having shed material from an initial 15–35 solar masses. W Canis Majoris is a type of red giant known as a carbon star—a semiregular variable, it ranges between magnitudes 6.27 and 7.09 over a period of 160 days. A cool star, it has a surface temperature of around 2900 K and a radius 234 times that of the Sun, its distance estimated at 1444–1450 light-years from Earth. At the other extreme in size is RX J0720.4-3125, a neutron star with a radius of around 5 km. Exceedingly faint, it has an apparent magnitude of 26.6. Its spectrum and temperature appear to be mysteriously changing over several years. The nature of the changes are unclear, but it is possible they were caused by an event such as the star's absorption of an accretion disc.\n\nTau Canis Majoris is a Beta Lyrae-type eclipsing multiple star system that varies from magnitude 4.32 to 4.37 over 1.28 days. Its four main component stars are hot O-type stars, with a combined mass 80 times that of the Sun and shining with 500,000 times its luminosity, but little is known of their individual properties. A fifth component, a magnitude 10 star, lies at a distance of . The system is only 5 million years old. UW Canis Majoris is another Beta Lyrae-type star 3000 light-years from Earth; it is an eclipsing binary that ranges in magnitude from a minimum of 5.3 to a maximum of 4.8. It has a period of 4.4 days; its components are two massive hot blue stars, one a blue supergiant of spectral type O7.5-8 Iab, while its companion is a slightly cooler, less evolved and less luminous supergiant of spectral type O9.7Ib. The stars are 200,000 and 63,000 times as luminous as the Sun. However the fainter star is the more massive at 19 solar masses to the primary's 16. R Canis Majoris is another eclipsing binary that varies from magnitude 5.7 to 6.34 over 1.13 days, with a third star orbiting these two every 93 years. The shortness of the orbital period and the low ratio between the two main components make this an unusual Algol-type system.\n\nSeven star systems have been found to have planets. Nu Canis Majoris is an ageing orange giant of spectral type K1III of apparent magnitude 3.91 located around 64 light-years distant. Around 1.5 times as massive and 11 times as luminous as the Sun, it is orbited over a period of 763 days by a planet 2.6 times as massive as Jupiter. HD 47536 is likewise an ageing orange giant found to have a planetary system—echoing the fate of the Solar System in a few billion years as the Sun ages and becomes a giant. Conversely, HD 45364 is a star 107 light-years distant that is a little smaller and cooler than the Sun, of spectral type G8V, which has two planets discovered in 2008. With orbital periods of 228 and 342 days, the planets have a 3:2 orbital resonance, which helps stabilise the system. HD 47186 is another sunlike star with two planets; the inner—HD 47186 b—takes four days to complete an orbit and has been classified as a Hot Neptune, while the outer—HD 47186 c—has an eccentric 3.7-year period orbit and has a similar mass to Saturn. HD 43197 is a sunlike star around 183 light-years distant that has a Jupiter-size planet with an eccentric orbit.\n\nZ Canis Majoris is a star system a mere 300,000 years old composed of two pre-main-sequence stars—a FU Orionis star and a Herbig Ae/Be star, which has brightened episodically by two magnitudes to magnitude 8 in 1987, 2000, 2004 and 2008. The more massive Herbig Ae/Be star is enveloped in an irregular roughly spherical cocoon of dust that has an inner diameter of and outer diameter of . The cocoon has a hole in it through which light shines that covers an angle of 5 to 10 degrees of its circumference. Both stars are surrounded by a large envelope of in-falling material left over from the original cloud that formed the system. Both stars are emitting jets of material, that of the Herbig Ae/Be star being much larger—11.7 light-years long. Meanwhile, FS Canis Majoris is another star with infra-red emissions indicating a compact shell of dust, but it appears to be a main-sequence star that has absorbed material from a companion. These stars are thought to be significant contributors to interstellar dust.\n\nThe band of the Milky Way goes through Canis Major, with only patchy obscurement by interstellar dust clouds. It is bright in the northeastern corner of the constellation, as well as in a triangular area between Adhara, Wezen and Aludra, with many stars visible in binoculars. Canis Major boasts several open clusters. The only Messier object is M41 (NGC 2287), an open cluster with a combined visual magnitude of 4.5, around 2300 light-years from Earth. Located 4 degrees south of Sirius, it contains contrasting blue, yellow and orange stars and covers an area the apparent size of the full moon—in reality around 25 light-years in diameter. Its most luminous stars have already evolved into giants. The brightest is a 6.3-magnitude star of spectral type K3. Located in the field is 12 Canis Majoris, though this star is only 670 light-years distant. NGC 2360, known as Caroline's Cluster after its discoverer Caroline Herschel, is an open cluster located 3.5 degrees west of Muliphein and has a combined apparent magnitude of 7.2. Around 15 light-years in diameter, it is located 3700 light-years away from Earth, and has been dated to around 2.2 billion years old. NGC 2362 is a small, compact open cluster, 5200 light-years from Earth. It contains about 60 stars, of which Tau Canis Majoris is the brightest member. Located around 3 degrees northeast of Wezen, it covers an area around 12 light-years in diameter, though the stars appear huddled around Tau when seen through binoculars. It is a very young open cluster as its member stars are only a few million years old. Lying 2 degrees southwest of NGC 2362 is NGC 2354 a fainter open cluster of magnitude 6.5, with around 15 member stars visible with binoculars. Located around 30' northeast of NGC 2360, NGC 2359 (Thor's Helmet or the Duck Nebula) is a relatively bright emission nebula in Canis Major, with an approximate magnitude of 10, which is 10,000 light-years from Earth. The nebula is shaped by HD 56925, an unstable Wolf-Rayet star embedded within it.\n\nIn 2003, an overdensity of stars in the region was announced to be the Canis Major Dwarf, the closest satellite galaxy to Earth. However, there remains debate over whether it represents a disrupted dwarf galaxy or in fact a variation in the thin and thick disk and spiral arm populations of the Milky Way. Investigation of the area yielded only ten RR Lyrae variables—consistent with the Milky Way's halo and thick disk populations rather than a separate dwarf spheroidal galaxy. On the other hand, a globular cluster in Puppis, NGC 2298—which appears to be part of the Canis Major dwarf system—is extremely metal-poor, suggesting it did not arise from the Milky Way's thick disk, and instead is of extragalactic origin.\n\nNGC 2207 and IC 2163 are a pair of face-on interacting spiral galaxies located 125 million light-years from Earth. About 40 million years ago, the two galaxies had a close encounter and are now moving farther apart; nevertheless, the smaller IC 2163 will eventually be incorporated into NGC 2207. As the interaction continues, gas and dust will be perturbed, sparking extensive star formation in both galaxies. Supernovae have been observed in NGC 2207 in 1975 (type Ia SN 1975a), 1999 (the type Ib SN 1999ec), 2003 (type 1b supernova SN 2003H), and 2013 (type II supernova SN 2013ai). Located 16 million light-years distant, ESO 489-056 is an irregular dwarf- and low-surface-brightness galaxy that has one of the lowest metallicities known.\n\n\n"}
{"id": "6367", "url": "https://en.wikipedia.org/wiki?curid=6367", "title": "Canis Minor", "text": "Canis Minor\n\nCanis Minor is a small constellation in the northern celestial hemisphere. In the second century, it was included as an asterism, or pattern, of two stars in Ptolemy's 48 constellations, and it is counted among the 88 modern constellations. Its name is Latin for \"lesser dog\", in contrast to Canis Major, the \"greater dog\"; both figures are commonly represented as following the constellation of Orion the hunter.\n\nCanis Minor contains only two stars brighter than the fourth magnitude, Procyon (Alpha Canis Minoris), with a magnitude of 0.34, and Gomeisa (Beta Canis Minoris), with a magnitude of 2.9. The constellation's dimmer stars were noted by Johann Bayer, who named eight stars including Alpha and Beta, and John Flamsteed, who numbered fourteen. Procyon is the seventh-brightest star in the night sky, as well as one of the closest. A yellow-white main sequence star, it has a white dwarf companion. Gomeisa is a blue-white main sequence star. Luyten's Star is a ninth-magnitude red dwarf and the Solar System's next closest stellar neighbour in the constellation after Procyon. The fourth-magnitude HD 66141, which has evolved into an orange giant towards the end of its life cycle, was discovered to have a planet in 2012. There are two faint deep sky objects within the constellation's borders. The 11 Canis-Minorids are a meteor shower that can be seen in early December.\n\nThough strongly associated with the Classical Greek uranographic tradition, Canis Minor originates from ancient Mesopotamia. Procyon and Gomeisa were called \"MASH.TAB.BA\" or \"twins\" in the \"Three Stars Each\" tablets, dating to around 1100 BC. In the later \"MUL.APIN\", this name was also applied to the pairs of Pi and Pi Orionis and Zeta and Xi Orionis. The meaning of \"MASH.TAB.BA\" evolved as well, becoming the twin deities Lulal and Latarak, who are on the opposite side of the sky from \"Papsukal\", the True Shepherd of Heaven in Babylonian mythology. Canis Minor was also given the name \"DAR.LUGAL\", which translates to \"the star which stands behind it\", in the \"MUL.APIN\"; the constellation represents a rooster. This name may have also referred to the constellation Lepus. \"DAR.LUGAL\" was also denoted \"DAR.MUŠEN\" and \"DAR.LUGAL.MUŠEN\" in Babylonia. Canis Minor was then called \"tarlugallu\" in Akkadian astronomy.\n\nCanis Minor was one of the original 48 constellations formulated by Ptolemy in his second-century Almagest, in which it was defined as a specific pattern (asterism) of stars; Ptolemy identified only two stars and hence no depiction was possible. The Ancient Greeks called the constellation προκυων/\"Procyon\", \"coming before the dog\", transliterated into Latin as \"Antecanis\", \"Praecanis\", or variations thereof, by Cicero and others. Roman writers also appended the descriptors \"parvus\", \"minor\" or \"minusculus\" (\"small\" or \"lesser\", for its faintness), \"septentrionalis\" (\"northerly\", for its position in relation to Canis Major), \"primus\" (rising \"first\") or \"sinister\" (rising to the \"left\") to its name \"Canis\". \nIn Greek mythology, Canis Minor was sometimes connected with the Teumessian Fox, a beast turned into stone with its hunter, Laelaps, by Zeus, who placed them in heaven as Canis Major (Laelaps) and Canis Minor (Teumessian Fox). Eratosthenes accompanied the Little Dog with Orion, while Hyginus linked the constellation with Maera, a dog owned by Icarius of Athens. On discovering the latter's death, the dog and Icarius' daughter Erigone took their lives and all three were placed in the sky—Erigone as Virgo and Icarius as Boötes. As a reward for his faithfulness, the dog was placed along the \"banks\" of the Milky Way, which the ancients believed to be a heavenly river, where he would never suffer from thirst.\n\nThe medieval Arabic astronomers maintained the depiction of Canis Minor (\"al-Kalb al-Asghar\" in Arabic) as a dog; in his Book of the Fixed Stars, Abd al-Rahman al-Sufi included a diagram of the constellation with a canine figure superimposed. There was one slight difference between the Ptolemaic vision of Canis Minor and the Arabic; al-Sufi claims Mirzam, now assigned to Orion, as part of both Canis Minor—the collar of the dog—and its modern home. The Arabic names for both Procyon and Gomeisa alluded to their proximity and resemblance to Sirius, though they were not direct translations of the Greek; Procyon was called \"ash-Shi'ra ash-Shamiya\", the \"Syrian Sirius\" and Gomeisa was called \"ash-Shira al-Ghamisa\", the Sirius with bleary eyes. Among the Merazig of Tunisia, shepherds note six constellations that mark the passage of the dry, hot season. One of them, called \"Merzem\", includes the stars of Canis Minor and Canis Major and is the herald of two weeks of hot weather.\n\nThe ancient Egyptians thought of this constellation as Anubis, the jackal god.\n\nAlternative names have been proposed: Johann Bayer in the early 17th century termed the constellation \"Fovea\" \"The Pit\", and \"Morus\" \"Sycamine Tree\". Seventeenth-century German poet and author Philippus Caesius linked it to the dog of Tobias from the Apocrypha. Richard A. Proctor gave the constellation the name \"Felis\" \"the Cat\" in 1870 (contrasting with Canis Major, which he had abbreviated to \"Canis\" \"the Dog\"), explaining that he sought to shorten the constellation names to make them more manageable on celestial charts. Occasionally, Canis Minor is confused with Canis Major and given the name \"Canis Orionis\" (\"Orion's Dog\").\n\nIn Chinese astronomy, the stars corresponding to Canis Minor lie in the Vermilion Bird of the South (南方朱雀, \"Nán Fāng Zhū Què\"). Procyon, Gomeisa and Eta Canis Minoris form an asterism known as Nánhé, the Southern River. With its counterpart, the Northern River Beihe (Castor and Pollux), Nánhé was also associated with a gate or sentry. Along with Zeta and 8 Cancri, 6 Canis Minoris and 11 Canis Minoris formed the asterism \"Shuiwei\", which literally means \"water level\". Combined with additional stars in Gemini, Shuiwei represented an official who managed floodwaters or a marker of the water level. Neighboring Korea recognized four stars in Canis Minor as part of a different constellation, \"the position of the water\". This constellation was located in the Red Bird, the southern portion of the sky.\n\nPolynesian peoples often did not recognize Canis Minor as a constellation, but they saw Procyon as significant and often named it; in the Tuamotu Archipelago it was known as \"Hiro\", meaning \"twist as a thread of coconut fiber\", and \"Kopu-nui-o-Hiro\" (\"great paunch of Hiro\"), which was either a name for the modern figure of Canis Minor or an alternative name for Procyon. Other names included \"Vena\" (after a goddess), on Mangaia and \"Puanga-hori\" (false \"Puanga\", the name for Rigel), in New Zealand. In the Society Islands, Procyon was called \"Ana-tahua-vahine-o-toa-te-manava\", literally \"Aster the priestess of brave heart\", figuratively the \"pillar for elocution\". The Wardaman people of the Northern Territory in Australia gave Procyon and Gomeisa the names \"Magum\" and \"Gurumana\", describing them as humans who were transformed into gum trees in the dreamtime. Although their skin had turned to bark, they were able to speak with a human voice by rustling their leaves.\n\nThe Aztec calendar was related to their cosmology. The stars of Canis Minor were incorporated along with some stars of Orion and Gemini into an asterism associated with the day called \"Water\".\n\nLying directly south of Gemini's bright stars Castor and Pollux, Canis Minor is a small constellation bordered by Monoceros to the south, Gemini to the north, Cancer to the northeast, and Hydra to the east. It does not border Canis Major; Monoceros is in between the two. Covering 183 square degrees, Canis Minor ranks seventy-first of the 88 constellations in size. It appears prominently in the southern sky during the Northern Hemisphere's winter. The constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of 14 sides. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between and . Most visible in the evening sky from January to March, Canis Minor is most prominent at 10 PM during mid-February. It is then seen earlier in the evening until July, when it is only visible after sunset before setting itself, and rising in the morning sky before dawn. The constellation's three-letter abbreviation, as adopted by the International Astronomical Union in 1922, is \"CMi\".\n\nCanis Minor contains only two stars brighter than fourth magnitude. At magnitude 0.34, Procyon, or Alpha Canis Minoris, is the seventh-brightest star in the night sky, as well as one of the closest. Its name means \"before the dog\" or \"preceding the dog\" in Greek, as it rises an hour before the \"Dog Star\", Sirius, of Canis Major. It is a binary star system, consisting of a yellow-white main sequence star of spectral type F5 IV-V, named Procyon A, and a faint white dwarf companion of spectral type DA, named Procyon B. Procyon B, which orbits the more massive star every 41 years, is of magnitude 10.7. Procyon A is 1.4 times the Sun's mass, while its smaller companion is 0.6 times as massive as the Sun. The system is from Earth, the shortest distance to a northern-hemisphere star of the first magnitude. Gomeisa, or Beta Canis Minoris, with a magnitude of 2.89, is the second-brightest star in Canis Minor. Lying from the Solar System, it is a blue-white main sequence star of spectral class B8 Ve. Although fainter to Earth observers, it is much brighter than Procyon, and is 250 times as luminous and three times as massive as the Sun. Although its variations are slight, Gomeisa is classified as a shell star (Gamma Cassiopeiae variable), with a maximum magnitude of 2.84 and a minimum magnitude of 2.92. It is surrounded by a disk of gas which it heats and causes to emit radiation.\n\nJohann Bayer used the Greek letters Alpha to Eta to label the most prominent eight stars in the constellation, designating two stars as Delta (named Delta and Delta). John Flamsteed numbered fourteen stars, discerning a third star he named Delta; his star 12 Canis Minoris was not found subsequently. In Bayer's 1603 work \"Uranometria\", Procyon is located on the dog's belly, and Gomeisa on its neck. Gamma, Epsilon and Eta Canis Minoris lie nearby, marking the dog's neck, crown and chest respectively. Although it has an apparent magnitude of 4.34, Gamma Canis Minoris is an orange K-type giant of spectral class K3-III C, which lies away. Its colour is obvious when seen through binoculars. It is a multiple system, consisting of the spectroscopic binary Gamma A and three optical companions, Gamma B, magnitude 13; Gamma C, magnitude 12; and Gamma D, magnitude 10. The two components of Gamma A orbit each other every 389.2 days, with an eccentric orbit that takes their separation between 2.3 and 1.4 astronomical units (AU). Epsilon Canis Minoris is a yellow bright giant of spectral class G6.5IIb of magnitude of 4.99. It lies from Earth, with 13 times the diameter and 750 times the luminosity of the Sun. Eta Canis Minoris is a giant of spectral class F0III of magnitude 5.24, which has a yellowish hue when viewed through binoculars as well as a faint companion of magnitude 11.1. Located 4 arcseconds from the primary, the companion star is actually around 440 AU from the main star and takes around 5000 years to orbit it.\n\nNear Procyon, three stars share the name Delta Canis Minoris. Delta is a yellow-white F-type giant of magnitude 5.25 located around from Earth. About 360 times as luminous and 3.75 times as massive as the Sun, it is expanding and cooling as it ages, having spent much of its life as a main sequence star of spectrum B6V. Also known as 8 Canis Minoris, Delta is an F-type main-sequence star of spectral type F2V and magnitude 5.59 which is distant. The last of the trio, Delta (also known as 9 Canis Minoris), is a white main sequence star of spectral type A0Vnn and magnitude 5.83 which is distant. These stars mark the paws of the Lesser Dog's left hind leg, while magnitude 5.13 Zeta marks the right. A blue-white bright giant of spectral type B8II, Zeta lies around away from the Solar System.\n\nLying approximately 264 light-years (81 parsecs) away with an apparent magnitude of 4.39, HD 66141 is 6.8 billion years old and has evolved into an orange giant of spectral type K2III with a diameter around 22 times that of the Sun, and weighing 1.1 solar masses. It is 174 times as luminous as the Sun, with an absolute magnitude of −0.15. HD 66141 was mistakenly named 13 Puppis, as its celestial coordinates were recorded incorrectly when catalogued and hence mistakenly thought to be in the constellation of Puppis; Bode gave it the name Lambda Canis Minoris, which is now obsolete. The orange giant is orbited by a planet, HD 66141b, which was detected in 2012 by measuring the star's radial velocity. The planet has a mass around 6 times that of Jupiter and a period of 480 days.\n\nA red giant of spectral type M4III, BC Canis Minoris lies around distant from the Solar System. It is a semiregular variable star that varies between a maximum magnitude of 6.14 and minimum magnitude of 6.42. Periods of 27.7, 143.3 and 208.3 days have been recorded in its pulsations. AZ, AD and BI Canis Minoris are Delta Scuti variables—short period (six hours at most) pulsating stars that have been used as standard candles and as subjects to study astroseismology. AZ is of spectral type F0III, and ranges between magnitudes 6.44 and 6.51 over a period of 2.3 hours. AD has a spectral type of F2III, and has a maximum magnitude of 9.21 and minimum of 9.51, with a period of approximately 2.95 hours. BI is of spectral type F2 with an apparent magnitude varying around 9.19 and a period of approximately 2.91 hours.\n\nAt least three red giants are Mira variables in Canis Minor. S Canis Minoris, of spectral type M7e, is the brightest, ranging from magnitude 6.6 to 13.2 over a period of 332.94 days. V Canis Minoris ranges from magnitude 7.4 to 15.1 over a period of 366.1 days. Similar in magnitude is R Canis Minoris, which has a maximum of 7.3, but a significantly brighter minimum of 11.6. An S-type star, it has a period of 337.8 days.\n\nYZ Canis Minoris is a red dwarf of spectral type M4.5V and magnitude 11.2, roughly three times the size of Jupiter and from Earth. It is a flare star, emitting unpredictable outbursts of energy for mere minutes, which might be much more powerful analogues of solar flares. Luyten's Star (GJ 273) is a red dwarf star of spectral type M3.5V and close neighbour of the Solar System. Its visual magnitude of 9.9 renders it too faint to be seen with the naked eye, even though it is only away. Fainter still is PSS 544-7, an eighteenth-magnitude red dwarf around 20 percent the mass of the Sun, located from Earth. First noticed in 1991, it is thought to be a cannonball star, shot out of a star cluster and now moving rapidly through space directly away from the galactic disc.\n\nThe WZ Sagittae-type dwarf nova DY CMi (also known as VSX J074727.6+065050) flared up to magnitude 11.4 over January and February 2008 before dropping eight magnitudes to around 19.5 over approximately 80 days. It is a remote binary star system where a white dwarf and low mass star orbit each other close enough for the former star to draw material off the latter and form an accretion disc. This material builds up until it erupts dramatically.\n\nThe Milky Way passes through much of Canis Minor, yet it has few deep-sky objects. William Herschel recorded four objects in his 1786 work \"Catalogue of Nebulae and Clusters of Stars\", including two he mistakenly believed were star clusters. NGC 2459 is a group of five thirteenth- and fourteenth-magnitude stars that appear to lie close together in the sky but are not related. A similar situation has occurred with NGC 2394, also in Canis Minor. This is a collection of fifteen unrelated stars of ninth-magnitude and fainter.\n\nHerschel also observed three faint galaxies, two of which are interacting with each other. NGC 2508 is a lenticular galaxy of thirteenth-magnitude, estimated at 205 million light-years (63 million parsecs) distance with a diameter of 80 thousand light-years (25 thousand parsecs). Named as a single object by Herschel, NGC 2402 is actually a pair of near-adjacent galaxies that appear to be interacting with each other. Only of fourteenth- and fifteenth-magnitudes respectively, the elliptical and spiral galaxy are thought to be approximately 245 million light-years distant, and each measure 55,000 light-years in diameter.\n\nThe 11 Canis-Minorids, also called the Beta Canis Minorids, are a meteor shower that arise near the fifth-magnitude star 11 Canis Minoris and were discovered in 1964 by Keith Hindley, who investigated their trajectory and proposed a common origin with the comet D/1917 F1 Mellish. However, this conclusion has been refuted subsequently as the number of orbits analysed was low and their trajectories too disparate to confirm a link. They last from 4 to 15 December, peaking over 10 and 11 December.\n\n"}
{"id": "6371", "url": "https://en.wikipedia.org/wiki?curid=6371", "title": "Centaurus", "text": "Centaurus\n\nCentaurus is a bright constellation in the southern sky. One of the largest constellations, Centaurus was included among the 48 constellations listed by the 2nd-century astronomer Ptolemy, and it remains one of the 88 modern constellations. In Greek mythology, Centaurus represents a centaur; a creature that is half human, half horse (another constellation named after a centaur is one from the zodiac: Sagittarius). Notable stars include Alpha Centauri, the nearest star system to the Solar System, its neighbour in the sky Beta Centauri, and V766 Centauri, one of the largest stars yet discovered. The constellation also contains Omega Centauri, the brightest globular cluster as visible from Earth and one of the largest known.\n\nCentaurus contains several very bright stars. Its alpha and beta stars are used as \"pointer stars\" to help observers find the constellation Crux. Centaurus has 281 stars above magnitude 6.5, meaning that they are visible to the unaided eye, the most of any constellation. Alpha Centauri, the closest star system to the Sun, has a high proper motion; it will be a mere half-degree from Beta Centauri in approximately 4000 years.\n\nAlpha Centauri is a triple star system that contains Proxima Centauri, the nearest star to the Sun. Traditionally called Rigil Kentaurus or Toliman, meaning \"foot of the centaur\", the system has an overall magnitude of -0.28 and is 4.4 light-years from Earth. The primary and secondary are both yellow-hued stars; the primary, is of magnitude -0.01 and the secondary is of magnitude 1.35. Proxima, the tertiary star, is a red dwarf of magnitude 11.0; it is almost 2 degrees away from the primary and secondary and has a period of approximately one million years. Also a flare star, Proxima has minutes-long outbursts where it brightens by over a magnitude. The primary and secondary have a period of 80 years and will be closest to each other as seen from Earth in 2037 and 2038.\n\nIn addition to Alpha Centauri (the third-brightest star in the sky), a second first magnitude star, Beta Centauri, is part of Centaurus. Also called Hadar and Agena, Beta Centauri is a double star; the primary is a blue-hued giant star of magnitude 0.6, 525 light-years from Earth. The secondary is of magnitude 4.0 and has a very small separation. A bright binary star in Centaurus is Gamma Centauri, which appears to the naked eye at magnitude 2.2. The primary and secondary are both blue-white hued stars of magnitude 2.9; their period is 85 years.\n\nCentaurus also has many dimmer double stars and binary stars. 3 Centauri is a double star with a blue-white hued primary of magnitude 4.6 and a secondary of magnitude 6.1. The primary is 298 light-years from Earth.\n\nCentaurus is home to many variable stars. R Centauri is a Mira variable star with a minimum magnitude of 11.8 and a maximum magnitude of 5.3; it is 2,100 light-years from Earth and has a period of 18 months. V810 Centauri is a semiregular variable.\n\nBPM 37093 is a white dwarf star whose carbon atoms are thought to have formed a crystalline structure. Since diamond also consists of carbon arranged in a crystalline lattice (though of a different configuration), scientists have nicknamed this star \"Lucy\" after the Beatles song \"\"Lucy in the Sky with Diamonds\".\"\n\nω Centauri (NGC 5139), despite being listed as the constellation's \"omega\" star, is in fact a naked-eye globular cluster, located at a distance of 17,000 light-years with a diameter of 150 light-years. It is the largest and brightest globular cluster in the Milky Way; at ten times the size of the next-largest cluster, it has a magnitude of 3.7. It is also the most luminous globular cluster in the Milky Way, at over one million solar luminosities. Omega Centauri is classified as a Shapley class VIII cluster, which means that its center is loosely concentrated. It is also the only globular cluster to be designated with a Bayer letter; the globular cluster 47 Tucanae is the only one designated with a Flamsteed number. It contains several million stars, most of which are yellow dwarf stars, but also possesses red giants and blue-white stars; the stars have an average age of 12 billion years. This has prompted suspicion that Omega Centauri was the core of a dwarf galaxy that had been absorbed by the Milky Way. Omega Centauri was determined to be nonstellar in 1677 by the English astronomer Edmond Halley, though it was visible as a star to the ancients. Its status as a globular cluster was determined by James Dunlop in 1827. To the unaided eye, Omega Centauri appears fuzzy and is obviously non-circular; it is approximately half a degree in diameter, the same size as the full Moon.\n\nCentaurus is also home to open clusters. NGC 3766 is an open cluster 6,300 light-years from Earth that is visible to the unaided eye. It contains approximately 100 stars, the brightest of which are 7th magnitude. NGC 5460 is another naked-eye open cluster, 2,500 light-years from Earth, that has an overall magnitude of 6 and contains approximately 40 stars.\n\nThere is one bright planetary nebula in Centaurus, NGC 3918, also known as the Blue Planetary. It has an overall magnitude of 8.0 and a central star of magnitude 11.0; it is 2600 light-years from Earth. The Blue Planetary was discovered by John Herschel and named for its color's similarity to Uranus, though the nebula is apparently three times larger than the planet.\n\nCentaurus is rich in galaxies as well. NGC 4622 is a face-on spiral galaxy located 200 million light-years from Earth (redshift 0.0146). Its spiral arms wind in both directions, which makes it nearly impossible for astronomers to determine the rotation of the galaxy. Astronomers theorize that a collision with a smaller companion galaxy near the core of the main galaxy could have led to the unusual spiral structure. NGC 5253, a peculiar irregular galaxy, is located near the border with Hydra and M83, with which it likely had a close gravitational interaction 1-2 billion years ago. This may have sparked the galaxy's high rate of star formation, which continues today and contributes to its high surface brightness. NGC 5253 includes a large nebula and at least 12 large star clusters. In the eyepiece, it is a small galaxy of magnitude 10 with dimensions of 5 arcminutes by 2 arcminutes and a bright nucleus. NGC 4945 is a spiral galaxy seen edge-on from Earth, 13 million light-years away. It is visible with any amateur telescope, as well as binoculars under good conditions; it has been described as \"shaped like a candle flame\", being long and thin (16' by 3'). In the eyepiece of a large telescope, its southeastern dust lane becomes visible. Another galaxy is NGC 5102, found by star-hopping from Iota Centauri. In the eyepiece, it appears as an elliptical object 9 arcminutes by 2.5 arcminutes tilted on a southwest-northeast axis.\n\nOne of the closest active galaxies to Earth is the Centaurus A galaxy, NGC 5128, at a distance of 11 million light-years (redshift 0.00183). It has a supermassive black hole at its core, which expels massive jets of matter that emit radio waves due to synchrotron radiation. Astronomers posit that its dust lanes, not common in elliptical galaxies, are due to a previous merger with another galaxy, probably a spiral galaxy. NGC 5128 appears in the optical spectrum as a fairly large elliptical galaxy with a prominent dust lane. Its overall magnitude is 7.0, and it has been seen under perfect conditions with the naked eye, making it one of the most distant objects visible to the unaided observer. In equatorial and southern latitudes, it is easily found by star hopping from Omega Centauri. In small telescopes, the dust lane is not visible; it begins to appear with about 4 inches of aperture under good conditions. In large amateur instruments, above about 12 inches in aperture, the dust lane's west-northwest to east-southeast direction is easily discerned. Another dim dust lane on the east side of the 12-arcminute-by-15-arcminute galaxy is also visible. ESO 270-17, also called the Fourcade-Figueroa Object, is a low-surface brightness object believed to be the remnants of a galaxy; it does not have a core and is very difficult to observe with an amateur telescope. It measures 7 arcminutes by 1 arcminute. It likely originated as a spiral galaxy and underwent a catastrophic gravitational interaction with Centaurus A around 500 million years ago, stopping its rotation and destroying its structure.\n\nNGC 4650A is a polar-ring galaxy located at a distance of 136 million light-years from Earth (redshift 0.01). It has a central core made of older stars that resembles an elliptical galaxy, and an outer ring of young stars that orbits around the core. The plane of the outer ring is distorted, which suggests that NGC 4650A is the result of a galaxy collision about a billion years ago. This galaxy has also been cited in studies of dark matter, because the stars in the outer ring orbit too quickly for their collective mass. This suggests that the galaxy is surrounded by a dark matter halo, which provides the necessary mass.\n\nOne of the closest galaxy clusters to Earth is the Centaurus Cluster, located at a distance of 160 million light-years (redshift 0.0114). It has a cooler, denser central region of gas and a hotter, more diffuse outer region. The intracluster medium in the Centaurus Cluster has a high concentration of metals (elements heavier than helium) due to a large number of supernovae. This cluster also possesses a plume of gas whose origin is unknown.\n\nWhile Centaurus now has a high southern latitude, at the dawn of civilization it was an equatorial constellation. Precession has been slowly shifting it southward for millennia, and it is now close to its maximal southern declination. Thousands of years from now Centaurus will, once again, be at lower latitudes and be visible worldwide.\n\nThe figure of Centaurus can be traced back to a Babylonian constellation known as the Bison-man (MUL.GUD.ALIM). This being was depicted in two major forms: firstly, as a 4-legged bison with a human head, and secondly, as a being with a man's head and torso attached to the rear legs and tail of a bull or bison. It has been closely associated with the Sun god Utu-Shamash from very early times.\n\nThe Greeks depicted the constellation as a centaur and gave it its current name. It was mentioned by Eudoxus in the 4th century BCE and Aratus in the 3rd century BCE. In the 2nd century AD, Claudius Ptolemy catalogued 37 stars in Centaurus. Large as it is now, in earlier times it was even larger, as the constellation Lupus was treated as an asterism within Centaurus, portrayed in illustrations as an unspecified animal either in the centaur's grasp or impaled on its spear. The Southern Cross, which is now regarded as a separate constellation, was treated by the ancients as a mere asterism formed of the stars composing the centaur's legs. Additionally, what is now the minor constellation Circinus was treated as undefined stars under the centaur's front hooves.\n\nAccording to the Roman poet Ovid (\"Fasti\" v.379), the constellation honors the centaur Chiron, who was tutor to many of the earlier Greek heroes including Heracles (Hercules), Theseus, and Jason, the leader of the Argonauts. However, most authorities consider Sagittarius to be the civilized Chiron, while Centaurus represents a more uncouth member of the species. The legend associated with Chiron says that he was accidentally poisoned with an arrow shot by Hercules, and was subsequently placed in the heavens.\n\nIn Chinese astronomy, the stars of Centaurus are found in three areas: the Azure Dragon of the East (東方青龍, \"Dōng Fāng Qīng Lóng\"), the Vermillion Bird of the South (南方朱雀, \"Nán Fāng Zhū Què\"), and the Southern Asterisms (近南極星區, \"Jìnnánjíxīngōu\"). Not all of the stars of Centaurus can be seen from China, and the unseen stars were classified among the Southern Asterisms by Xu Guangqi, based on his study of western star charts. However, most of the brightest stars of Centaurus, including α Cen, θ Cen, ε Cen and η Cen, can be seen in the Chinese sky.\n\nSome Polynesian peoples considered the stars of Centaurus to be a constellation as well. On Pukapuka, Centaurus had two names: \"Na Mata-o-te-tokolua\" and \"Na Lua-mata-o-Wua-ma-Velo\". In Tonga, the constellation was called by four names: \"O-nga-tangata\", \"Tautanga-ufi\", \"Mamangi-Halahu\", and \"Mau-kuo-mau\". Alpha and Beta Centauri were not named specifically by the people of Pukapuka or Tonga, but they were named by the people of Hawaii and the Tuamotus. In Hawaii, the name for Alpha Centauri was either \"Melemele\" or \"Ka Maile-hope\" and the name for Beta Centauri was either \"Polapola\" or \"Ka Maile-mua\". In the Tuamotu islands, Alpha was called \"Na Kuhi\" and Beta was called \"Tere\".\n\nTwo United States Navy ships, USS Centaurus (AKA-17) and USS Centaurus (AK-264), are named after the constellation.\n\nThe Centaurus is a Mega Mall and commercial/residential complex in Islamabad, Pakistan. Construction started in 2005 and the three 41-storey towers, the tallest structures in Islamabad, were completed by late 2012. The shopping mall was officially opened on February 17, 2013. The Centaurus originally included a 7 star hotel, construction of which is yet to begin.\n\n\n\n"}
{"id": "6416", "url": "https://en.wikipedia.org/wiki?curid=6416", "title": "Impact crater", "text": "Impact crater\n\nAn impact crater is an approximately circular depression in the surface of a planet, moon, or other solid body in the Solar System or elsewhere, formed by the hypervelocity impact of a smaller body. In contrast to volcanic craters, which result from explosion or internal collapse, impact craters typically have raised rims and floors that are lower in elevation than the surrounding terrain. Although Meteor Crater is perhaps the best-known example of a small impact crater on Earth, impact craters range from small, simple, bowl-shaped depressions to large, complex, multi-ringed impact basins.\n\nImpact craters are the dominant geographic features on many solid Solar System objects including the Moon, Mercury, Callisto, Ganymede and most small moons and asteroids. On other planets and moons that experience more active surface geological processes, such as Earth, Venus, Mars, Europa, Io and Titan, visible impact craters are less common because they become eroded, buried or transformed by tectonics over time. Where such processes have destroyed most of the original crater topography, the terms impact structure or astrobleme are more commonly used. In early literature, before the significance of impact cratering was widely recognised, the terms cryptoexplosion or cryptovolcanic structure were often used to describe what are now recognised as impact-related features on Earth.\n\nThe cratering records of very old surfaces, such as Mercury, the Moon, and the southern highlands of Mars, record a period of intense early bombardment in the inner Solar System around 3.9 billion years ago. The rate of crater production on Earth has since been considerably lower, but it is appreciable nonetheless; Earth experiences from one to three impacts large enough to produce a 20 km diameter crater about once every million years on average. This indicates that there should be far more relatively young craters on the planet than have been discovered so far. The cratering rate in the inner solar system fluctuates as a consequence of collisions in the asteroid belt that create a family of fragments that are often sent cascading into the inner solar system. Formed in a collision 160 million years ago, the Baptistina family of asteroids is thought to have caused a large spike in the impact rate, perhaps causing the Chicxulub impact that may have triggered the extinction of the non-avian dinosaurs 66 million years ago. Note that the rate of impact cratering in the outer Solar System could be different from the inner Solar System.\n\nAlthough Earth's active surface processes quickly destroy the impact record, about 190 terrestrial impact craters have been identified. These range in diameter from a few tens of meters up to about 300 km, and they range in age from recent times (e.g. the Sikhote-Alin craters in Russia whose creation was witnessed in 1947) to more than two billion years, though most are less than 500 million years old because geological processes tend to obliterate older craters. They are also selectively found in the stable interior regions of continents. Few undersea craters have been discovered because of the difficulty of surveying the sea floor, the rapid rate of change of the ocean bottom, and the subduction of the ocean floor into Earth's interior by processes of plate tectonics.\n\nImpact craters are not to be confused with landforms that may appear similar, including calderas, sinkholes, glacial cirques, ring dikes, salt domes, and others.\n\nDaniel Barringer (1860–1929) was one of the first to identify an impact crater, Meteor Crater in Arizona; to crater specialists the site is referred to as Barringer Crater in his honor. Initially Barringer's ideas were not widely accepted, and even when the origin of Meteor Crater was finally acknowledged, the wider implications for impact cratering as a significant geological process on Earth were not.\n\nIn the 1920s, the American geologist Walter H. Bucher studied a number of sites now recognized as impact craters in the United States. He concluded they had been created by some great explosive event, but believed that this force was probably volcanic in origin. However, in 1936, the geologists John D. Boon and Claude C. Albritton Jr. revisited Bucher's studies and concluded that the craters that he studied were probably formed by impacts.\n\nThe concept of impact cratering remained more or less speculative until the 1960s. At that time a number of researchers, most notably Eugene Shoemaker, (co-discoverer of the comet Shoemaker-Levy 9), conducted detailed studies of a number of craters and recognized clear evidence that they had been created by impacts, specifically identifying the shock-metamorphic effects uniquely associated with impact events, of which the most familiar is shocked quartz.\n\nArmed with the knowledge of shock-metamorphic features, Carlyle S. Beals and colleagues at the Dominion Astrophysical Observatory in Victoria, British Columbia, Canada and Wolf von Engelhardt of the University of Tübingen in Germany began a methodical search for impact craters. By 1970, they had tentatively identified more than 50. Although their work was controversial, the American Apollo Moon landings, which were in progress at the time, provided supportive evidence by recognizing the rate of impact cratering on the Moon. Because the processes of erosion on the Moon are minimal, craters persist almost indefinitely. Since the Earth could be expected to have roughly the same cratering rate as the Moon, it became clear that the Earth had suffered far more impacts than could be seen by counting evident craters.\n\nImpact cratering involves high velocity collisions between solid objects, typically much greater than the velocity of sound in those objects. Such hyper-velocity impacts produce physical effects such as melting and vaporization that do not occur in familiar sub-sonic collisions. On Earth, ignoring the slowing effects of travel through the atmosphere, the lowest impact velocity with an object from space is equal to the gravitational escape velocity of about 11 km/s. The fastest impacts occur at about 72 km/s in the \"worst case\" scenario in which an object in a retrograde near-parabolic orbit hits Earth. (Because kinetic energy scales as velocity squared, Earth's gravity only contributes 1 km/s to this figure, not 11 km/s). The median impact velocity on Earth is about 20 km/s.\n\nHowever, the slowing effects of travel through the atmosphere rapidly decelerate any potential impactor, especially in the lowest 12 kilometres where 90% of the earth’s atmospheric mass lies. Meteorites of up to 7,000 kg lose all their cosmic velocity due to atmospheric drag at a certain altitude (retardation point), and start to accelerate again due to Earth's gravity until the body reaches its terminal velocity of 0.09 to 0.16 km/s. The larger the meteoroid (i.e. asteroids and comets) the more of its initial cosmic velocity it preserves. While an object of 9,000 kg maintains about 6% of its original velocity, one of 900,000 kg already preserves about 70%. Extremely large bodies (about 100,000 tonnes) are not slowed by the atmosphere at all, and impact with their initial cosmic velocity if no prior disintegration occurs.\n\nImpacts at these high speeds produce shock waves in solid materials, and both impactor and the material impacted are rapidly compressed to high density. Following initial compression, the high-density, over-compressed region rapidly depressurizes, exploding violently, to set in train the sequence of events that produces the impact crater. Impact-crater formation is therefore more closely analogous to cratering by high explosives than by mechanical displacement. Indeed, the energy density of some material involved in the formation of impact craters is many times higher than that generated by high explosives. Since craters are caused by explosions, they are nearly always circular – only very low-angle impacts cause significantly elliptical craters.\n\nThis describes impacts on solid surfaces. Impacts on porous surfaces, such as that of Hyperion, may produce internal compression without ejecta, punching a hole in the surface without filling in nearby craters. This may explain the 'sponge-like' appearance of that moon.\n\nIt is convenient to divide the impact process conceptually into three distinct stages: (1) initial contact and compression, (2) excavation, (3) modification and collapse. In practice, there is overlap between the three processes with, for example, the excavation of the crater continuing in some regions while modification and collapse is already underway in others.\n\nIn the absence of atmosphere, the impact process begins when the impactor first touches the target surface. This contact accelerates the target and decelerates the impactor. Because the impactor is moving so rapidly, the rear of the object moves a significant distance during the short-but-finite time taken for the deceleration to propagate across the impactor. As a result, the impactor is compressed, its density rises, and the pressure within it increases dramatically. Peak pressures in large impacts exceed 1 TPa to reach values more usually found deep in the interiors of planets, or generated artificially in nuclear explosions.\n\nIn physical terms, a shock wave originates from the point of contact. As this shock wave expands, it decelerates and compresses the impactor, and it accelerates and compresses the target. Stress levels within the shock wave far exceed the strength of solid materials; consequently, both the impactor and the target close to the impact site are irreversibly damaged. Many crystalline minerals can be transformed into higher-density phases by shock waves; for example, the common mineral quartz can be transformed into the higher-pressure forms coesite and stishovite. Many other shock-related changes take place within both impactor and target as the shock wave passes through, and some of these changes can be used as diagnostic tools to determine whether particular geological features were produced by impact cratering.\n\nAs the shock wave decays, the shocked region decompresses towards more usual pressures and densities. The damage produced by the shock wave raises the temperature of the material. In all but the smallest impacts this increase in temperature is sufficient to melt the impactor, and in larger impacts to vaporize most of it and to melt large volumes of the target. As well as being heated, the target near the impact is accelerated by the shock wave, and it continues moving away from the impact behind the decaying shock wave.\n\nContact, compression, decompression, and the passage of the shock wave all occur within a few tenths of a second for a large impact. The subsequent excavation of the crater occurs more slowly, and during this stage the flow of material is largely subsonic. During excavation, the crater grows as the accelerated target material moves away from the point of impact. The target's motion is initially downwards and outwards, but it becomes outwards and upwards. The flow initially produces an approximately hemispherical cavity that continues to grow, eventually producing a paraboloid (bowl-shaped) crater in which the centre has been pushed down, a significant volume of material has been ejected, and a topographically elevated crater rim has been pushed up. When this cavity has reached its maximum size, it is called the transient cavity.\n\nThe depth of the transient cavity is typically a quarter to a third of its diameter. Ejecta thrown out of the crater do not include material excavated from the full depth of the transient cavity; typically the depth of maximum excavation is only about a third of the total depth. As a result, about one third of the volume of the transient crater is formed by the ejection of material, and the remaining two thirds is formed by the displacement of material downwards, outwards and upwards, to form the elevated rim. For impacts into highly porous materials, a significant crater volume may also be formed by the permanent compaction of the pore space. Such compaction craters may be important on many asteroids, comets and small moons.\n\nIn large impacts, as well as material displaced and ejected to form the crater, significant volumes of target material may be melted and vaporized together with the original impactor. Some of this impact melt rock may be ejected, but most of it remains within the transient crater, initially forming a layer of impact melt coating the interior of the transient cavity. In contrast, the hot dense vaporized material expands rapidly out of the growing cavity, carrying some solid and molten material within it as it does so. As this hot vapor cloud expands, it rises and cools much like the archetypal mushroom cloud generated by large nuclear explosions. In large impacts, the expanding vapor cloud may rise to many times the scale height of the atmosphere, effectively expanding into free space.\n\nMost material ejected from the crater is deposited within a few crater radii, but a small fraction may travel large distances at high velocity, and in large impacts it may exceed escape velocity and leave the impacted planet or moon entirely. The majority of the fastest material is ejected from close to the center of impact, and the slowest material is ejected close to the rim at low velocities to form an overturned coherent flap of ejecta immediately outside the rim. As ejecta escapes from the growing crater, it forms an expanding curtain in the shape of an inverted cone. The trajectory of individual particles within the curtain is thought to be largely ballistic.\n\nSmall volumes of un-melted and relatively un-shocked material may be spalled at very high relative velocities from the surface of the target and from the rear of the impactor. Spalling provides a potential mechanism whereby material may be ejected into inter-planetary space largely undamaged, and whereby small volumes of the impactor may be preserved undamaged even in large impacts. Small volumes of high-speed material may also be generated early in the impact by jetting. This occurs when two surfaces converge rapidly and obliquely at a small angle, and high-temperature highly shocked material is expelled from the convergence zone with velocities that may be several times larger than the impact velocity.\n\nIn most circumstances, the transient cavity is not stable and collapses under gravity. In small craters, less than about 4 km diameter on Earth, there is some limited collapse of the crater rim coupled with debris sliding down the crater walls and drainage of impact melts into the deeper cavity. The resultant structure is called a simple crater, and it remains bowl-shaped and superficially similar to the transient crater. In simple craters, the original excavation cavity is overlain by a lens of collapse breccia, ejecta and melt rock, and a portion of the central crater floor may sometimes be flat.\n\nAbove a certain threshold size, which varies with planetary gravity, the collapse and modification of the transient cavity is much more extensive, and the resulting structure is called a complex crater. The collapse of the transient cavity is driven by gravity, and involves both the uplift of the central region and the inward collapse of the rim. The central uplift is the result of \"elastic rebound\", which is a process in which a material with elastic strength attempts to return to its original geometry; rather the collapse is a process in which a material with little or no strength attempts to return to a state of gravitational equilibrium.\n\nComplex craters have uplifted centers, and they have typically broad flat shallow crater floors, and terraced walls. At the largest sizes, one or more exterior or interior rings may appear, and the structure may be labeled an \"impact basin\" rather than an impact crater. Complex-crater morphology on rocky planets appears to follow a regular sequence with increasing size: small complex craters with a central topographic peak are called \"central peak craters\", for example Tycho; intermediate-sized craters, in which the central peak is replaced by a ring of peaks, are called \"peak-ring craters\", for example Schrödinger; and the largest craters contain multiple concentric topographic rings, and are called \"multi-ringed basins\", for example Orientale. On icy (as opposed to rocky) bodies, other morphological forms appear that may have central pits rather than central peaks, and at the largest sizes may contain many concentric rings. Valhalla on Callisto is an example of this type.\n\nNon-explosive volcanic craters can usually be distinguished from impact craters by their irregular shape and the association of volcanic flows and other volcanic materials. Impact craters produce melted rocks as well, but usually in smaller volumes with different characteristics.\n\nThe distinctive mark of an impact crater is the presence of rock that has undergone shock-metamorphic effects, such as shatter cones, melted rocks, and crystal deformations. The problem is that these materials tend to be deeply buried, at least for simple craters. They tend to be revealed in the uplifted center of a complex crater, however.\n\nImpacts produce distinctive shock-metamorphic effects that allow impact sites to be distinctively identified. Such shock-metamorphic effects can include:\n\nBecause of the many missions studying Mars since the 1960s, we have good coverage of its surface which contains large numbers of craters. Many of the craters on Mars are different than on our moon and other moons since Mars contains ice under the ground, especially in the higher latitudes. Some of the types of craters that have special shapes due to impact into ice-rich ground are pedestal craters, rampart craters, expanded craters, and LARLE craters.\n\nOn Earth, the recognition of impact craters is a branch of geology, and is related to planetary geology in the study of other worlds. Out of many proposed craters, relatively few are confirmed. The following twenty are a sample of articles of confirmed and well-documented impact sites.\n\nSee the Earth Impact Database, a website concerned with 188 (as of 2016) scientifically-confirmed impact craters on Earth.\n\n\n\nThere are approximately twelve more impact craters/basins larger than 300 km on the Moon, five on Mercury, and four on Mars. Large basins, some unnamed but mostly smaller than 300 km, can also be found on Saturn's moons Dione, Rhea and Iapetus.\n\n\n\n"}
{"id": "6417", "url": "https://en.wikipedia.org/wiki?curid=6417", "title": "Corvus (disambiguation)", "text": "Corvus (disambiguation)\n\nCorvus is a genus of birds including species commonly known as crows, ravens, rooks and jackdaws.\n\nCorvus may also refer to:\n\n"}
{"id": "6420", "url": "https://en.wikipedia.org/wiki?curid=6420", "title": "Corona Borealis", "text": "Corona Borealis\n\nCorona Borealis is a small constellation in the Northern Celestial Hemisphere. It is one of the 48 constellations listed by the 2nd-century astronomer Ptolemy, and remains one of the 88 modern constellations. Its brightest stars form a semicircular arc. Its Latin name, inspired by its shape, means \"northern crown\". In classical mythology Corona Borealis generally represented the crown given by the god Dionysus to the Cretan princess Ariadne and set by him in the heavens. Other cultures likened the pattern to a circle of elders, an eagle's nest, a bear's den, or even a smokehole. Ptolemy also listed a southern counterpart, Corona Australis, with a similar pattern.\n\nThe brightest star is the magnitude 2.2 Alpha Coronae Borealis. The yellow supergiant R Coronae Borealis is the prototype of a rare class of giant stars—the R Coronae Borealis variables—that are extremely hydrogen deficient, and thought to result from the merger of two white dwarfs. T Coronae Borealis, also known as the Blaze Star, is another unusual type of variable star known as a recurrent nova. Normally of magnitude 10, it last flared up to magnitude 2 in 1946. ADS 9731 and Sigma Coronae Borealis are multiple star systems with six and five components respectively. Five star systems have been found to have Jupiter-sized exoplanets. Abell 2065 is a highly concentrated galaxy cluster one billion light-years from the Solar System containing more than 400 members, and is itself part of the larger Corona Borealis Supercluster.\n\nCovering 179 square degrees and hence 0.433% of the sky, Corona Borealis ranks 73rd of the 88 modern constellations by area. Its position in the Northern Celestial Hemisphere means that the whole constellation is visible to observers north of 50°S. It is bordered by Boötes to the north and west, Serpens Caput to the south, and Hercules to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'CrB'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of eight segments (\"illustrated in infobox\"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between 39.71° and 25.54°. It has a counterpart—Corona Australis—in the Southern Celestial Hemisphere.\n\nThe seven stars that make up the constellation's distinctive crown-shaped pattern are all 4th-magnitude stars except for the brightest of them, Alpha Coronae Borealis. The other six stars are Theta, Beta, Gamma, Delta, Epsilon and Iota Coronae Borealis. The German cartographer Johann Bayer gave twenty stars in Corona Borealis Bayer designations from Alpha to Upsilon in his 1603 star atlas \"Uranometria\". Zeta Coronae Borealis was noted to be a double star by later astronomers and its components designated Zeta and Zeta. John Flamsteed did likewise with Nu Coronae Borealis; classed by Bayer as a single star, it was noted to be two close stars by Flamsteed. He named them 20 and 21 Coronae Borealis in his catalogue, alongside the designations Nu and Nu respectively. Chinese astronomers deemed nine stars to make up the asterism, adding Pi and Rho Coronae Borealis. Within the constellation's borders, there are 37 stars brighter than or equal to apparent magnitude 6.5.\nAlpha Coronae Borealis (officially named Alphecca by the IAU, but sometimes also known as Gemma) appears as a blue-white star of magnitude 2.2. In fact, it is an Algol-type eclipsing binary that varies by 0.1 magnitude with a period of 17.4 days. The primary is a white main-sequence star of spectral type A0V that is 2.91 times the mass of the Sun () and 57 times as luminous (), and is surrounded by a debris disk out to a radius of around 60 astronomical units (AU). The secondary companion is a yellow main-sequence star of spectral type G5V that is a little smaller (0.9 times) the diameter of the Sun. Lying 75±0.5 light-years from Earth, Alphecca is believed to be a member of the Ursa Major Moving Group of stars that have a common motion through space.\n\nLocated 112±3 light-years away, Beta Coronae Borealis or Nusakan is a spectroscopic binary system whose two components are separated by 10 AU and orbit each other every 10.5 years. The brighter component is a rapidly oscillating Ap star, pulsating with a period of 16.2 minutes. Of spectral type A5V with a surface temperature of around 7980 K, it has around , 2.6 solar radii (), and . The smaller star is of spectral type F2V with a surface temperature of around 6750 K, and has around , , and between 4 and . Near Nusakan is Theta Coronae Borealis, a binary system that shines with a combined magnitude of 4.13 located 380±20 light-years distant. The brighter component, Theta Coronae Borealis A, is a blue-white star that spins extremely rapidly—at a rate of around 393 km per second. A Be star, it is surrounded by a debris disk.\n\nFlanking Alpha to the east is Gamma Coronae Borealis, yet another binary star system, whose components orbit each other every 92.94 years and are roughly as far apart from each other as the Sun and Neptune. The brighter component has been classed as a Delta Scuti variable star, though this view is not universal. The components are main sequence stars of spectral types B9V and A3V. Located 170±2 light-years away, 4.06-magnitude Delta Coronae Borealis is a yellow giant star of spectral type G3.5III that is around and has swollen to . It has a surface temperature of 5180 K. For most of its existence, Delta Coronae Borealis was a blue-white main-sequence star of spectral type B before it ran out of hydrogen fuel in its core. Its luminosity and spectrum suggest it has just crossed the Hertzsprung gap, having finished burning core hydrogen and just begun burning hydrogen in a shell that surrounds the core.\n\nZeta Coronae Borealis is a double star with two blue-white components 6.3 arcseconds apart that can be readily separated at 100x magnification. The primary is of magnitude 5.1 and the secondary is of magnitude 6.0. Nu Coronae Borealis is an optical double, whose components are a similar distance from Earth but have different radial velocities, hence are assumed to be unrelated. The primary, Nu Coronae Borealis, is a red giant of spectral type M2III and magnitude 5.2, lying 640±30 light-years distant, and the secondary, Nu Coronae Borealis, is an orange-hued giant star of spectral type K5III and magnitude 5.4, estimated to be 590±30 light-years away. Sigma Coronae Borealis, on the other hand, is a true multiple star system divisible by small amateur telescopes. It is actually a complex system composed of two stars around as massive as the Sun that orbit each other every 1.14 days, orbited by a third Sun-like star every 726 years. The fourth and fifth components are a binary red dwarf system that is 14,000 AU distant from the other three stars. ADS 9731 is an even rarer multiple system in the constellation, composed of six stars, two of which are spectroscopic binaries.\n\nCorona Borealis is home to two remarkable variable stars. T Coronae Borealis is a cataclysmic variable star also known as the Blaze Star. Normally placid around magnitude 10—it has a minimum of 10.2 and maximum of 9.9—it brightens to magnitude 2 in a period of hours, caused by a nuclear chain reaction and the subsequent explosion. T Coronae Borealis is one of a handful of stars called recurrent novae, which include T Pyxidis and U Scorpii. An outburst of T Coronae Borealis was first recorded in 1866; its second recorded outburst was in February 1946. T Coronae Borealis is a binary star with a red-hued giant primary and a white dwarf secondary, the two stars orbiting each other over a period of approximately 8 months. R Coronae Borealis is a yellow-hued variable supergiant star, over 7000 light-years from Earth, and prototype of a class of stars known as R Coronae Borealis variables. Normally of magnitude 6, its brightness periodically drops as low as magnitude 15 and then slowly increases over the next several months. These declines in magnitude come about as dust that has been ejected from the star obscures it. Direct imaging with the Hubble Space Telescope shows extensive dust clouds out to a radius of around 2000 AU from the star, corresponding with a stream of fine dust (composed of grains 5 nm in diameter) associated with the star's stellar wind and coarser dust (composed of grains with a diameter of around 0.14 µm) ejected periodically.\n\nThere are several other variables of reasonable brightness for amateur astronomer to observe, including three Mira-type long period variables: S Coronae Borealis ranges between magnitudes 5.8 and 14.1 over a period of 360 days. Located around 1946 light-years distant, it shines with a luminosity 16,643 times that of the Sun and has a surface temperature of 3033 K. One of the reddest stars in the sky, V Coronae Borealis is a cool star with a surface temperature of 2877 K that shines with a luminosity 102,831 times that of the Sun and is a remote 8810 light-years distant from Earth. Varying between magnitudes 6.9 and 12.6 over a period of 357 days, it is located near the junction of the border of Corona Borealis with Hercules and Bootes. Located 1.5° northeast of Tau Coronae Borealis, W Coronae Borealis ranges between magnitudes 7.8 and 14.3 over a period of 238 days. Another red giant, RR Coronae Borealis is a M3-type semiregular variable star that varies between magnitudes 7.3 and 8.2 over 60.8 days. RS Coronae Borealis is yet another semiregular variable red giant, which ranges between magnitudes 8.7 to 11.6 over 332 days. It is unusual in that it is a red star with a high proper motion (greater than 50 milliarcseconds a year). Meanwhile, U Coronae Borealis is an Algol-type eclipsing binary star system whose magnitude varies between 7.66 and 8.79 over a period of 3.45 days\n\nTY Coronae Borealis is a pulsating white dwarf (of ZZ Ceti) type, which is around 70% as massive as the Sun, yet has only 1.1% of its diameter. Discovered in 1990, UW Coronae Borealis is a low-mass X-ray binary system composed of a star less massive than the Sun and a neutron star surrounded by an accretion disk that draws material from the companion star. It varies in brightness in an unusually complex manner: the two stars orbit each other every 111 minutes, yet there is another cycle of 112.6 minutes, which corresponds to the orbit of the disk around the degenerate star. The beat period of 5.5 days indicates the time the accretion disk—which is asymmetrical—takes to precess around the star.\n\nExtrasolar planets have been confirmed in five star systems, four of which were found by the radial velocity method. The spectrum of Epsilon Coronae Borealis was analysed for seven years from 2005 to 2012, revealing a planet around 6.7 times as massive as Jupiter () orbiting every 418 days at an average distance of around 1.3 AU. Epsilon itself is a orange giant of spectral type K2III that has swollen to and . Kappa Coronae Borealis is a spectral type K1IV orange subgiant nearly twice as massive as the Sun; around it lie a dust debris disk, and one planet with a period of 3.4 years. This planet's mass is estimated at . The dimensions of the debris disk indicate it is likely there is a second substellar companion. Omicron Coronae Borealis is a K-type clump giant with one confirmed planet with a mass of that orbits every 187 days—one of the two least massive planets known around clump giants. HD 145457 is an orange giant of spectral type K0III found to have one planet of . Discovered by the Doppler method in 2010, it takes 176 days to complete an orbit. XO-1 is a magnitude 11 yellow main-sequence star located approximately light-years away, of spectral type G1V with a mass and radius similar to the Sun. In 2006 the hot Jupiter exoplanet XO-1b was discovered orbiting XO-1 by the transit method using the XO Telescope. Roughly the size of Jupiter, it completes an orbit around its star every three days.\n\nThe discovery of a Jupiter-sized planetary companion was announced in 1997 via analysis of the radial velocity of Rho Coronae Borealis, a yellow main sequence star and Solar analog of spectral type G0V, around 57 light-years distant from Earth. More accurate measurement of data from the Hipparcos satellite subsequently showed it instead to be a low-mass star somewhere between 100 and 200 times the mass of Jupiter. Possible stable planetary orbits in the habitable zone were calculated for the binary star Eta Coronae Borealis, which is composed of two stars—yellow main sequence stars of spectral type G1V and G3V respectively—similar in mass and spectrum to the Sun. No planet has been found, but a brown dwarf companion about 63 times as massive as Jupiter with a spectral type of L8 was discovered at a distance of 3640 AU from the pair in 2001.\n\nCorona Borealis contains few galaxies observable with amateur telescopes. NGC 6085 and 6086 are a faint spiral and elliptical galaxy respectively close enough to each other to be seen in the same visual field through a telescope. Abell 2142 is a huge (six million light-year diameter), X-ray luminous galaxy cluster that is the result of an ongoing merger between two galaxy clusters. It has a redshift of 0.0909 (meaning it is moving away from us at 27,250 km/s) and a visual magnitude of 16.0. It is about 1.2 billion light-years away. Another galaxy cluster in the constellation, RX J1532.9+3021, is approximately 3.9 billion light-years from Earth. At the cluster's center is a large elliptical galaxy containing one of the most massive and most powerful supermassive black holes yet discovered. Abell 2065 is a highly concentrated galaxy cluster containing more than 400 members, the brightest of which are 16th magnitude; the cluster is more than one billion light-years from Earth. On a larger scale still, Abell 2065, along with Abell 2061, Abell 2067, Abell 2079, Abell 2089, and Abell 2092, make up the Corona Borealis Supercluster. Another galaxy cluster, Abell 2162, is a member of the Hercules Superclusters.\n\nIn Greek mythology, Corona Borealis was linked to the legend of Theseus and the minotaur. It was generally considered to represent a crown given by Dionysus to Ariadne, the daughter of Minos of Crete, after she had been abandoned by the Athenian prince Theseus. When she wore the crown at her marriage to Dionysus, he placed it in the heavens to commemorate their wedding. An alternate version has the besotted Dionysus give the crown to Ariadne, who in turn gives it to Theseus after he arrives in Crete to kill the minotaur that the Cretans have demanded tribute from Athens to feed. The hero uses the crown's light to escape the labyrinth after disposing of the creature, and Dionysus later sets it in the heavens. The Latin author Hyginus linked it to a crown or wreath worn by Bacchus (Dionysus) to disguise his appearance when first approaching Mount Olympus and revealing himself to the gods, having been previously hidden as yet another child of Jupiter's trysts with a mortal, in this case Semele. Corona Borealis was one of the 48 constellations mentioned in the \"Almagest\" of classical astronomer Ptolemy.\n\nIn Welsh mythology, it was called Caer Arianrhod, \"the Castle of the Silver Circle\", and was the heavenly abode of the Lady Arianrhod. To the ancient Balts, Corona Borealis was known as \"Darželis\", the \"flower garden\".\n\nThe Arabs called the constellation Alphecca (a name later given to Alpha Coronae Borealis), which means \"separated\" or \"broken up\" ( '), a reference to the resemblance of the stars of Corona Borealis to a loose string of jewels. This was also interpreted as a broken dish. Among the Bedouins, the constellation was known as ' (), or \"the dish/bowl of the poor people\".\n\nThe Skidi people of Native Americans saw the stars of Corona Borealis representing a council of stars whose chief was Polaris. The constellation also symbolised the smokehole over a fireplace, which conveyed their messages to the gods, as well as how chiefs should come together to consider matters of importance. The Shawnee people saw the stars as the \"Heavenly Sisters\", who descended from the sky every night to dance on earth. Alphecca signifies the youngest and most comely sister, who was seized by a hunter who transformed into a field mouse to get close to her. They married though she later returned to the sky, with her heartbroken husband and son following later. The Mi'kmaq of eastern Canada saw Corona Borealis as \"Mskegwǒm\", the den of the celestial bear (Alpha, Beta, Gamma and Delta Ursae Majoris).\n\nPolynesian peoples often recognized Corona Borealis; the people of the Tuamotus named it \"Na Kaua-ki-tokerau\" and probably \"Te Hetu\". The constellation was likely called \"Kaua-mea\" in Hawaii, \"Rangawhenua\" in New Zealand, and \"Te Wale-o-Awitu\" in the Cook Islands atoll of Pukapuka. Its name in Tonga was uncertain; it was either called \"Ao-o-Uvea\" or \"Kau-kupenga\".\n\nIn Australian Aboriginal astronomy, the constellation is called \"womera\" (\"the boomerang\") due to the shape of the stars. The Wailwun people of northwestern New South Wales saw Corona Borealis as \"mullion wollai\" \"eagle's nest\", with Altair and Vega—each called \"mullion\"—the pair of eagles accompanying it. The Wardaman people of northern Australia held the constellation to be a gathering point for Men's Law, Women's Law and Law of both sexes come together and consider matters of existence.\n\nCorona Borealis was renamed Corona Firmiana in honour of the Archbishop of Salzburg in the 1730 Atlas \"Mercurii Philosophicii Firmamentum Firminianum Descriptionem\" by Corbinianus Thomas, but this was not taken up by subsequent cartographers. The constellation was featured as a main plot ingredient in the short story \"Hypnos\" by H. P. Lovecraft, published in 1923; it is the object of fear of one of the protagonists in the novella. Finnish band Cadacross released an album titled \"Corona Borealis\" in 2002.\n\n"}
{"id": "6421", "url": "https://en.wikipedia.org/wiki?curid=6421", "title": "Cygnus (constellation)", "text": "Cygnus (constellation)\n\nCygnus is a northern constellation lying on the plane of the Milky Way, deriving its name from the Latinized Greek word for swan. The swan is one of the most recognizable constellations of the northern summer and autumn, and it features a prominent asterism known as the Northern Cross (in contrast to the Southern Cross). Cygnus was among the 48 constellations listed by the 2nd century astronomer Ptolemy, and it remains one of the 88 modern constellations.\n\nCygnus contains Deneb, one of the brightest stars in the night sky and one corner of the Summer Triangle, as well as some notable X-ray sources and the giant stellar association of Cygnus OB2. One of the stars of this association, NML Cygni, is one of the largest stars currently known. The constellation is also home to Cygnus X-1, a distant X-ray binary containing a supergiant and unseen massive companion that was the first object widely held to be a black hole. Many star systems in Cygnus have known planets as a result of the Kepler Mission observing one patch of the sky, the patch is the area around Cygnus. In addition, most of the eastern part of Cygnus is dominated by the Hercules–Corona Borealis Great Wall, a giant galaxy filament that is the largest known structure in the observable universe; covering most of the northern sky.\n\nIn Greek mythology, Cygnus has been identified with several different legendary swans. Zeus disguised himself as a swan to seduce Leda, Spartan king Tyndareus's wife, who gave birth to the Gemini, Helen of Troy, and Clytemnestra; Orpheus was transformed into a swan after his murder, and was said to have been placed in the sky next to his lyre (Lyra); and the King Cygnus was transformed into a swan.\n\nThe Greeks also associated this constellation with the tragic story of Phaethon, the son of Helios the sun god, who demanded to ride his father's sun chariot for a day. Phaethon, however, was unable to control the reins, forcing Zeus to destroy the chariot (and Phaethon) with a thunderbolt, causing it to plummet to the earth into the river Eridanus. According to the myth, Phaethon's brother, Cycnus, grieved bitterly and spent many days diving into the river to collect Phaethon's bones to give him a proper burial. The gods were so touched by Cycnus's devotion to his brother that they turned him into a swan and placed him among the stars.\n\nIn Ovid's \"Metamorphoses\", there are three people named Cygnus, all of whom are transformed into swans. Alongside Cycnus, noted above, he mentions a boy from Tempe who commits suicide when Phyllius refuses to give him a tamed bull that he demands, but is transformed into a swan and flies away. He also mentions a son of Neptune who is an invulnerable warrior in the Trojan War who is eventually defeated by Achilles, but Neptune saves him by transforming him into a swan.\n\nTogether with other avian constellations near the summer solstice, Vultur cadens and Aquila, Cygnus may be a significant part of the origin of the myth of the Stymphalian Birds, one of The Twelve Labours of Hercules.\n\nIn Polynesia, Cygnus was often recognized as a separate constellation. In Tonga it was called \"Tuula-lupe\", and in the Tuamotus it was called \"Fanui-tai\". Deneb was also often given a name. The name \"Deneb\" comes from the Arabic name \"dhaneb\", meaning \"tail\", from the phrase \"Dhanab ad-Dajājah\", which means “the tail of the hen”. In New Zealand it was called \"Mara-tea\", in the Society Islands it was called \"Pirae-tea\" or \"Taurua-i-te-haapa-raa-manu\", and in the Tuamotus it was called \"Fanui-raro\". Beta Cygni was named in New Zealand; it was likely called \"Whetu-kaupo\". Gamma Cygni was called \"Fanui-runga\" in the Tuamotus.\n\nA very large constellation, Cygnus is bordered by Cepheus to the north and east, Draco to the north and west, Lyra to the west, Vulpecula to the south, Pegasus to the southeast and Lacerta to the east. The three-letter abbreviation for the constellation, as adopted by the IAU in 1922, is 'Cyg'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined as a polygon of 28 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between 27.73° and 61.36°. Covering 804 square degrees and around 1.9% of the night sky, Cygnus ranks 16th of the 88 constellations in size.\n\nCygnus culminates at midnight on 29 June, and is most visible in the evening from the early summer to mid-autumn in the Northern Hemisphere.\n\nNormally, Cygnus is depicted with Delta and Epsilon Cygni as its wings, Deneb as its tail, and Albireo as the tip of its beak.\n\nThere are several asterisms in Cygnus. In the 17th-century German celestial cartographer Johann Bayer's star atlas the \"Uranometria\", Alpha, Beta and Gamma Cygni form the pole of a cross, while Delta and Epsilon form the cross beam. The nova P Cygni was then considered to be the body of Christ.\n\nBayer catalogued many stars in the constellation, giving them the Bayer designations from Alpha to Omega and then using lowercase Roman letters to g. John Flamsteed added the Roman letters h,i,k,l and m (these stars were considered \"informes\" by Bayer as they lay outside the asterism of Cygnus), but were dropped by Francis Baily.\n\nThere are several bright stars in Cygnus. Alpha Cygni, called Deneb, is the brightest star in Cygnus. It is a white supergiant star of spectral type A2Iae that varies between magnitudes 1.21 and 1.29, one of the largest and most luminous A-class stars known. It is located about 3200 light-years away. Its traditional name means \"tail\" and refers to its position in the constellation. Albireo, designated Beta Cygni, is a celebrated binary star among amateur astronomers for its contrasting hues. The primary is an orange-hued giant star of magnitude 3.1 and the secondary is a blue-green hued star of magnitude 5.1. The system is 380 light-years away and is divisible in large binoculars and all amateur telescopes. Gamma Cygni, traditionally named Sadr, is a yellow-tinged supergiant star of magnitude 2.2, 1500 light-years away. Its traditional name means \"breast\" and refers to its position in the constellation. Delta Cygni is another bright binary star in Cygnus, 171 light-years with a period of 800 years. The primary is a blue-white hued giant star of magnitude 2.9, and the secondary is a star of magnitude 6.6. The two components are divisible in a medium-sized amateur telescope. The fifth star in Cygnus above magnitude 3 is Gienah, designated Epsilon Cygni. It is an orange-hued giant star of magnitude 2.5, 72 light-years from Earth.\n\nThere are several other dimmer double and binary stars in Cygnus. Mu Cygni is a binary star with an optical tertiary component. The binary system has a period of 790 years and is 73 light-years from Earth. The primary and secondary, both white stars, are of magnitude 4.8 and 6.2, respectively. The unrelated tertiary component is of magnitude 6.9. Though the tertiary component is divisible in binoculars, the primary and secondary currently require a medium-sized amateur telescope to split, as they will through the year 2020. The two stars will be closest between 2043 and 2050, when they will require a telescope with larger aperture to split. The stars 30 and 31 Cygni form a contrasting double star similar to the brighter Albireo. The two are divisible in binoculars. The primary, 31 Cygni, is an orange-hued star of magnitude 3.8, 1400 light-years from Earth. The secondary, 30 Cygni, appears blue-green. It is of spectral type A5IIIn and magnitude 4.83, and is around 610 light-years from Earth. 31 Cygni itself is a binary star; the tertiary component is a blue star of magnitude 7.0. Psi Cygni is a binary star divisible in small amateur telescopes, with two white components. The primary is of magnitude 5.0 and the secondary is of magnitude 7.5. 61 Cygni is a binary star divisible in large binoculars or a small amateur telescope. It is 11.4 light-years from Earth and has a period of 750 years. Both components are orange-hued dwarf (main sequence) stars; the primary is of magnitude 5.2 and the secondary is of magnitude 6.1. 61 Cygni is significant because Friedrich Wilhelm Bessel determined its parallax in 1838, the first star to have a known parallax.\n\nLocated near Eta Cygni is the X-ray source Cygnus X-1, which is now thought to be caused by a black hole accreting matter in a binary star system. This was the first x-ray source widely believed to be a black hole.\n\nCygnus also contains several other noteworthy X-ray sources. Cygnus X-3 is a microquasar containing a Wolf–Rayet star in orbit around a very compact object, with a period of only 4.8 hours. The system is one of the most intrinsically luminous X-ray sources observed. Interestingly, the system undergoes periodic outbursts of unknown nature, and during one such outburst, the system was found to be emitting muons, likely caused by neutrinos. While the compact object is thought to be a neutron star or possibly a black hole, it is possible that the object is instead a more exotic stellar remnant, possibly the first discovered quark star, hypothesized due to its production of cosmic rays that cannot be explained if the object is a normal neutron star. The system also emits cosmic rays and gamma rays, and has helped shed insight on to the formation of such rays. Cygnus X-2 is another X-ray binary, containing an A-type giant in orbit around a neutron star with a 9.8 day period. The system is interesting due to the rather small mass of the companion star, as most millisecond pulsars have much more massive companions. Another black hole in Cygnus is V404 Cygni, which consists of a K-type star orbiting around a black hole of around 12 solar masses. The black hole, similar to that of Cygnus X-3, has been hypothesized to be a quark star. 4U 2129+ 47 is another X-ray binary containing a neutron star which undergoes outbursts, as is EXO 2030+ 375.\n\nCygnus is also home to several variable stars. SS Cygni is a dwarf nova which undergoes outbursts every 7–8 days. The system's total magnitude varies from 12th magnitude at its dimmest to 8th magnitude at its brightest. The two objects in the system are incredibly close together, with an orbital period of less than 0.28 days. Chi Cygni is a red giant and the second-brightest Mira variable star at its maximum. It ranges between magnitudes 3.3 and 14.2, and spectral types S6,2e to S10,4e (MSe) over a period of 408 days; it has a diameter of 300 solar diameters and is 350 light-years from Earth. P Cygni is a luminous blue variable that brightened suddenly to 3rd magnitude in 1600 AD. Since 1715, the star has been of 5th magnitude, despite being more than 5000 light-years from Earth. The star's spectrum is unusual in that it contains very strong emission lines resulting from surrounding nebulosity. W Cygni is a semi-regular variable red giant star, 618 light-years from Earth.It has a maximum magnitude of 5.10 and a minimum magnitude 6.83; its period of 131 days. It is a red giant ranging between spectral types M4e-M6e(Tc:)III, NML Cygni, a red hypergiant semi-regular variable star and possibly the largest star currently known in the galaxy, is in Cygnus. It is about 5,300 light-years from Earth, has an estimated radius of about 1,650 solar radii, and a mass 40 times that of the Sun. Its magnitude is around 16.6, its period is about 940 days.\n\nCygnus contains the binary star system KIC 9832227. It is predicted that the two stars will coalesce in about 2022, briefly forming a new naked-eye object. \n\nCygnus is one of the constellations that the Kepler satellite surveyed in its search for extrasolar planets, and as a result, there are about a hundred stars in Cygnus with known planets, the most of any constellation. One of the most notable systems is the Kepler-11 system, containing six transiting planets, all within a plane of approximately one degree. With a spectral type of G6V, the star is somewhat cooler than the Sun. The planets are very close to the star; all but the last planet are closer to Kepler-11 than Mercury is to the Sun, and all the planets are more massive than Earth. The naked-eye star 16 Cygni, a triple star approximately 70 light-years from Earth composed two Sun-like stars and a red dwarf, contains a planet orbiting one of the sun-like stars, found due to variations in the star's radial velocity. Gliese 777, another naked-eye multiple star system containing a yellow star and a red dwarf, also contains a planet. The planet is somewhat similar to Jupiter, but with slightly more mass and a more eccentric orbit. The Kepler-22 system is also notable, in that its extrasolar planet is believed to be the first \"Earth-twin\" planet ever discovered.\n\nThere is an abundance of deep-sky objects, with many open clusters, nebulae of various types and supernova remnants found in Cygnus due to its position on the Milky Way. Some open clusters can be difficult to make out from a rich background of stars.\n\nM39 (NGC 7092) is an open cluster 950 light-years from Earth that is visible to the unaided eye under dark skies. It is loose, with about 30 stars arranged over a wide area; their conformation appears triangular. The brightest stars of M39 are of the 7th magnitude. Another open cluster in Cygnus is NGC 6910, also called the Rocking Horse Cluster, possessing 16 stars with a diameter of 5 arcminutes visible in a small amateur instrument; it is of magnitude 7.4. The brightest of these are two gold-hued stars, which represent the bottom of the toy it is named for. A larger amateur instrument reveals 8 more stars, nebulosity to the east and west of the cluster, and a diameter of 9 arcminutes. The nebulosity in this region is part of the Gamma Cygni Nebula. The other stars, approximately 3700 light-years from Earth, are mostly blue-white and very hot.\n\nOther open clusters in Cygnus include Dolidze 9, Collinder 421, Dolidze 11, and Berkeley 90. Dolidze 9, 2800 light-years from Earth and relatively young at 20 million light-years old, is a faint open cluster with up to 22 stars visible in small and medium-sized amateur telescopes. Nebulosity is visible to the north and east of the cluster, which is 7 arcminutes in diameter. The brightest star appears in the eastern part of the cluster and is of the 7th magnitude; another bright star has a yellow hue. Dolidze 11 is an open cluster 400 million years old, farthest away of the three at 3700 light-years. More than 10 stars are visible in an amateur instrument in this cluster, of similar size to Dolidze 9 at 7 arcminutes in diameter, whose brightest star is of magnitude 7.5. It, too, has nebulosity in the east. Collinder 421 is a particularly old open cluster at an age of approximately 1 billion years; it is of magnitude 10.1. 3100 light-years from Earth, more than 30 stars are visible in a diameter of 8 arcseconds. The prominent star in the north of the cluster has a golden color, whereas the stars in the south of the cluster appear orange. Collinder 421 appears to be embedded in nebulosity, which extends past the cluster's borders to its west. Berkeley 90 is a smaller open cluster, with a diameter of 5 arcminutes. More than 16 members appear in an amateur telescope.\n\nNGC 6826, the Blinking Planetary Nebula, is a planetary nebula with a magnitude of 8.5, 3200 light-years from Earth. It appears to \"blink\" in the eyepiece of a telescope because its central star is unusually bright (10th magnitude). When an observer focuses on the star, the nebula appears to fade out. Less than one degree from the Blinking Planetary is the double star 16 Cygni.\n\nThe North America Nebula (NGC 7000) is one of the most well-known nebulae in Cygnus, because it is visible to the unaided eye under dark skies, as a bright patch in the Milky Way. However, its characteristic shape is only visible in long-exposure photographs – it is difficult to observe in telescopes because of its low surface brightness. It has low surface brightness because it is so large; at its widest, the North America Nebula is 2 degrees across. Illuminated by a hot embedded star of magnitude 6, NGC 7000 is 1500 light-years from Earth.\n\nTo the south of Epsilon Cygni is the Veil Nebula (NGC 6960, 6962, 6979, 6992, and 6995), a 5,000-year-old supernova remnant covering approximately 3 degrees of the sky - it is over 50 light-years long. Because of its appearance, it is also called the Cygnus Loop. The Loop is only visible in long-exposure astrophotographs. However, the brightest portion, NGC 6992, is faintly visible in binoculars, and a dimmer portion, NGC 6960, is visible in wide-angle telescopes.\n\nThe Northern Coalsack Nebula, also called the Cygnus Rift, is a dark nebula located in the Cygnus Milky Way.\n\nThe Gamma Cygni Nebula (IC 1318) includes both bright and dark nebulae in an area of over 4 degrees. DWB 87 is another of the many bright emission nebulae in Cygnus, 7.8 by 4.3 arcminutes. It is in the Gamma Cygni area. Two other emission nebulae include Sharpless 2-112 and Sharpless 2-115. When viewed in an amateur telescope, Sharpless 2–112 appears to be in a teardrop shape. More of the nebula's eastern portion is visible with an O III (doubly ionized oxygen) filter. There is an orange star of magnitude 10 nearby and a star of magnitude 9 near the nebula's northwest edge. Further to the northwest, there is a dark rift and another bright patch. The whole nebula measures 15 arcminutes in diameter. Sharpless 2–115 is another emission nebula with a complex pattern of light and dark patches. Two pairs of stars appear in the nebula; it is larger near the southwestern pair. The open cluster Berkeley 90 is embedded in this large nebula, which measures 30 by 20 arcminutes.\n\nAlso of note is the Crescent Nebula (NGC 6888), located between Gamma and Eta Cygni, which was formed by the Wolf–Rayet star HD 192163.\n\nIn recent years, amateur astronomers have made some notable Cygnus discoveries. The \"Soap bubble nebula\" (PN G75.5+1.7), near the Crescent nebula, was discovered on a digital image by Dave Jurasevich in 2007. In 2011, Austrian amateur Matthias Kronberger discovered a planetary nebula (Kronberger 61, now nicknamed \"The Soccer Ball\") on old survey photos, confirmed recently in images by the Gemini Observatory; both of these are likely too faint to be detected by eye in a small amateur scope.\n\nBut a much more obscure and relatively 'tiny' object—one which is readily seen in dark skies by amateur telescopes, under good conditions—is the newly discovered nebula (likely reflection type) associated with the star 4 Cygni (HD 183056): an approximately fan-shaped glowing region of several arcminutes' diameter, to the south and west of the fifth-magnitude star. It was first discovered visually near San Jose, California and publicly reported by amateur astronomer Stephen Waldee in 2007, and was confirmed photographically by Al Howard in 2010. California amateur astronomer Dana Patchick also says he detected it on the Palomar Observatory survey photos in 2005 but had not published it for others to confirm and analyze at the time of Waldee's first official notices and later 2010 paper.\n\nCygnus X is the largest star-forming region in the Solar neighborhood and includes not only some of the brightest and most massive stars known (such as Cygnus OB2-12), but also Cygnus OB2, a massive stellar association classified by some authors as a young globular cluster.\n\nMore supernovae have been seen in the Fireworks Galaxy (NGC 6946) than in any other galaxy.\n\nCygnus A is the first radio galaxy discovered; at a distance of 730 million light-years from Earth, it is the closest powerful radio galaxy. In the visible spectrum, it appears as an elliptical galaxy in a small cluster. It is classified as an active galaxy because the supermassive black hole at its nucleus is accreting matter, which produces two jets of matter from the poles. The jets' interaction with the interstellar medium creates radio lobes, one source of radio emissions.\n\nCygnus is also the apparent source of the WIMP-wind due to the orientation of the solar system's rotation through the galactic halo.\n\n\n\n"}
{"id": "6422", "url": "https://en.wikipedia.org/wiki?curid=6422", "title": "Communion", "text": "Communion\n\nCommunion may refer to:\n\n\n\n"}
{"id": "6423", "url": "https://en.wikipedia.org/wiki?curid=6423", "title": "Calorie", "text": "Calorie\n\nCalories are units of energy. Various definitions exist but fall into two broad categories.\n\nAlthough these units relate to the metric system all forms of the calorie were deemed obsolete in science after the SI system was adopted in the 1950s. The unit of energy in the International System of Units is the joule. One small calorie is approximately 4.2 joules (so one large calorie is about 4.2 kilojoules). The factor used to convert calories to joules at a given temperature is numerically equivalent to the specific heat capacity of water expressed in joules per kelvin per gram or per kilogram. The precise conversion factor depends on the definition adopted.\n\nIn spite of its non-official status, the large calorie is still widely used as a unit of food energy. The small calorie is also often used for measurements in chemistry, although the amounts involved are typically recorded in kilocalories.\n\nThe calorie was first defined by Nicolas Clément in 1824 as a unit of heat energy and entered French and English dictionaries between 1841 and 1867. The word comes .\n\nThe energy needed to increase the temperature of a given mass of water by 1 °C depends on the atmospheric pressure and the starting temperature. Accordingly, several different precise definitions of the calorie have been used.\n\nThe pressure is usually taken to be the standard atmospheric pressure (). The temperature increase can be expressed as one kelvin, which means the same as an increment of one degree Celsius.\n\nThe two definitions most common in older literature appear to be the \"15 °C calorie\" and the \"thermochemical calorie\". Until 1948, the latter was defined as 4.1833 international joules; the current standard of 4.184 J was chosen to have the new thermochemical calorie represent the same quantity of energy as before.\n\nThe calorie was first defined specifically to measure energy in the form of heat, especially in experimental calorimetry.\n\nIn a nutritional context, the kilojoule (kJ) is the SI unit of food energy, although the kilocalorie is still in common use. The word \"calorie\" is popularly used with the number of kilocalories of nutritional energy measured. As if to avoid confusion, it is sometimes written \"Calorie\" (with a capital \"C\") in an attempt to make the distinction, although this is not widely understood. Capitalization contravenes the rule that the initial letter of a unit name or its derivative shall be lower case in English.\n\nTo facilitate comparison, specific energy or energy density figures are often quoted as \"calories per serving\" or \"kilocalories per 100 g\". A nutritional requirement or consumption is often expressed in calories per day. One gram of fat in food contains nine kilocalories, while a gram of either a carbohydrate or a protein contains approximately four kilocalories. Alcohol in a food contains seven kilocalories per gram.\n\nIn other scientific contexts, the term \"calorie\" almost always refers to the small calorie. Even though it is not an SI unit, it is still used in chemistry. For example, the energy released in a chemical reaction per mole of reagent is occasionally expressed in kilocalories per mole. Typically, this use was largely due to the ease with which it could be calculated in laboratory reactions, especially in aqueous solution: a volume of reagent dissolved in water forming a solution, with concentration expressed in moles per liter (1 liter weighing 1 kg), will induce a temperature change in degrees Celsius in the total volume of water solvent, and these quantities (volume, molar concentration and temperature change) can then be used to calculate energy per mole. It is also occasionally used to specify energy quantities that relate to reaction energy, such as enthalpy of formation and the size of activation barriers. However, its use is being superseded by the SI unit, the joule, and multiples thereof such as the kilojoule.\n"}

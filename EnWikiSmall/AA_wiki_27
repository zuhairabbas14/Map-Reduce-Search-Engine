{"id": "4181", "url": "https://en.wikipedia.org/wiki?curid=4181", "title": "Battle", "text": "Battle\n\nA battle is a combat in warfare between two or more armed forces, or combatants. A war sometimes consists of many battles. Battles generally are well defined in duration, area, and force commitment. A battle with only limited engagement between the forces and without decisive results is sometimes called a skirmish.\n\nWars and military campaigns are guided by strategy, whereas battles take place on a level of planning and execution known as operational mobility. German strategist Carl von Clausewitz stated that \"the employment of battles ... to achieve the object of war\" was the essence of strategy.\n\nBattle is a loanword from the Old French \"bataille\", first attested in 1297, from Late Latin \"battualia\", meaning \"exercise of soldiers and gladiators in fighting and fencing\", from Late Latin (taken from Germanic) \"battuere\" \"beat\", from which the English word battery is also derived via Middle English \"batri\".\n\nThe defining characteristic of the fight as a concept in Military science has been a dynamic one through the course of military history, changing with the changes in the organisation, employment and technology of military forces.\n\nWhile the English military historian Sir John Keegan suggested an ideal definition of battle as \"something which happens between two armies leading to the moral then physical disintegration of one or the other of them\", the origins and outcomes of battles can rarely be summarized so neatly.\n\nIn general a battle during the 20th century was, and continues to be, defined by the combat between opposing forces representing major components of total forces committed to a military campaign, used to achieve specific military objectives. Where the duration of the battle is longer than a week, it is often for reasons of staff operational planning called an \"operation\". Battles can be planned, encountered, or forced by one force on the other when the latter is unable to withdraw from combat.\n\nA battle always has as its purpose the reaching of a mission goal by use of military force. A victory in the battle is achieved when one of the opposing sides forces the other to abandon its mission, or to surrender its forces, or routs the other, i.e., forces it to retreat or renders it militarily ineffective for further combat operations. However, a battle may end in a Pyrrhic victory, which ultimately favors the defeated party. If no resolution is reached in a battle, it can result in a stalemate. A conflict in which one side is unwilling to reach a decision by a direct battle using conventional warfare often becomes an insurgency.\n\nUntil the 19th century the majority of battles were of short duration, many lasting a part of a day. (The Battle of Nations (1813) and the Battle of Gettysburg (1863) were exceptional in lasting three days.) This was mainly due to the difficulty of supplying armies in the field, or conducting night operations. The means of prolonging a battle was typically by employment of siege warfare. Improvements in transportation and the sudden evolving of trench warfare, with its siege-like nature during World War I in the 20th century, lengthened the duration of battles to days and weeks. This created the requirement for unit rotation to prevent combat fatigue, with troops preferably not remaining in a combat area of operations for more than a month. Trench warfare had become largely obsolete in conflicts between advanced armies by the start of the Second World War.\n\nThe use of the term \"battle\" in military history has led to its misuse when referring to almost any scale of combat, notably by strategic forces involving hundreds of thousands of troops that may be engaged in either a single battle at one time (Battle of Leipzig) or multiple operations (Battle of Kursk). The space a battle occupies depends on the range of the weapons of the combatants. A \"battle\" in this broader sense may be of long duration and take place over a large area, as in the case of the Battle of Britain or the Battle of the Atlantic. Until the advent of artillery and aircraft, battles were fought with the two sides within sight, if not reach, of each other. The depth of the battlefield has also increased in modern warfare with inclusion of the supporting units in the rear areas; supply, artillery, medical personnel etc. often outnumber the front-line combat troops.\n\nBattles are, on the whole, made up of a multitude of individual combats, skirmishes and small engagements within the context of which the combatants will usually only experience a small part of the events of the battle's entirety. To the infantryman, there may be little to distinguish between combat as part of a minor raid or as a major offensive, nor is it likely that he anticipates the future course of the battle; few of the British infantry who went over the top on the first day on the Somme, July 1, 1916, would have anticipated that they would be fighting the same battle in five months' time. Conversely, some of the Allied infantry who had just dealt a crushing defeat to the French at the Battle of Waterloo fully expected to have to fight again the next day (at the Battle of Wavre).\n\nBattlespace is a unified strategy to integrate and combine armed forces for the military theatre of operations, including air, information, land, sea and space. It includes the environment, factors and conditions that must be understood to successfully apply combat power, protect the force, or complete the mission. This includes enemy and friendly armed forces; facilities; weather; terrain; and the electromagnetic spectrum within the operational areas and areas of interest.\n\nBattles are decided by various factors. The number and quality of combatants and equipment, the skill of the commanders of each army, and the terrain advantages are among the most prominent factors. A unit may charge with high morale but less discipline and still emerge victorious. This tactic was effectively used by the early French Revolutionary Armies.\n\nWeapons and armour can be a decisive factor. On many occasions armies have achieved victories largely owing to the employment of more advanced weapons than those of their opponents. An extreme example was in the Battle of Omdurman, in which a large army of Sudanese Mahdists armed in a traditional manner were destroyed by an Anglo-Egyptian force equipped with Maxim guns.\n\nOn some occasions, simple weapons employed in an unorthodox fashion have proven advantageous, as with the Swiss pikemen who gained many victories through their ability to transform a traditionally defensive weapon into an offensive one. Likewise, the Zulus in the early 19th century were victorious in battles against their rivals in part because they adopted a new kind of spear, the iklwa. Even so, forces with inferior weapons have still emerged victorious at times, for example in the Wars of Scottish Independence and in the First Italo–Ethiopian War. Discipline within the troops is often of greater importance; at the Battle of Alesia, the Romans were greatly outnumbered but won because of superior training. \nBattles can also be determined by terrain. Capturing high ground, for example, has been the central strategy in innumerable battles. An army that holds the high ground forces the enemy to climb, and thus wear themselves down. Areas of dense vegetation, such as jungles and forest, act as force-multipliers, of benefit to inferior armies. Arguably, terrain is of less importance in modern warfare, due to the advent of aircraft, though terrain is still vital for camouflage, especially for guerrilla warfare.\n\nGenerals and commanders also play a decisive role during combat. Hannibal, Julius Caesar, Khalid ibn Walid and Napoleon Bonaparte were all skilled generals and, consequently, their armies were extremely successful. An army that can trust the commands of their leaders with conviction in its success invariably has a higher morale than an army that doubts its every move. The British in the naval Battle of Trafalgar, for example, owed its success to the reputation of celebrated admiral Lord Nelson.\n\nBattles can be fought on land, at sea and, in the modern age, in the air. Naval battles have occurred since before the 5th century BC. Air battles have been far less common, due to their late conception, the most prominent being the Battle of Britain in 1940. However, since the Second World War land or sea battles have come to rely on air support. Indeed, during the Battle of Midway, five aircraft carriers were sunk without either fleet coming into direct contact.\nThere are numerous types of battles:\n\n\nBattles frequently do not fit one particular type perfectly, and are usually hybrids of different types listed above.\n\nA \"decisive battle\" is one of particular importance; often by bringing hostilities to an end, such as the Battle of Hastings or the Battle of Hattin, or as a turning point in the fortunes of the belligerents, such as the Battle of Stalingrad. A decisive battle can have political as well as military impact, changing the balance of power or boundaries between countries. The concept of the \"decisive battle\" became popular with the publication in 1851 of Edward Creasy's \"The Fifteen Decisive Battles of the World\". British military historians J.F.C. Fuller (\"The Decisive Battles of the Western World\") and B.H. Liddell Hart (\"Decisive Wars of History\"), among many others, have written books in the style of Creasy's work.\n\nThere is an obvious difference in the way battles have been fought throughout time. Early battles were probably fought between rival hunting bands as disorganized mobs. However, during the Battle of Megiddo, the first reliably documented battle in the fifteenth century BC, actual discipline was instilled in both armies. However, during the many wars of the Roman Empire, barbarians continued using mob tactics.\n\nAs the Age of Enlightenment dawned, armies began to fight in highly disciplined lines. Each would follow the orders from their officers and fight as a single unit instead of individuals. Each army was successively divided into regiments, battalions, companies, and platoons. These armies would march, line up, and fire in divisions.\n\nNative Americans, on the other hand, did not fight in lines, utilizing instead guerrilla tactics. American colonists and European forces continued using disciplined lines, continuing into the American Civil War.\n\nA new style, during World War I, known as trench warfare, developed nearly half a century later. This also led to radio for communication between battalions. Chemical warfare also emerged with the use of poisonous gas during World War I.\n\nBy World War II, the use of the smaller divisions, platoons and companies became much more important as precise operations became vital. Instead of the locked trench warfare of World War I, during World War II, a dynamic network of battles developed where small groups encountered other platoons. As a result, elite squads became much more recognized and distinguishable.\n\nManeuver warfare also developed with an astonishing pace with the advent of the tank, replacing the archaic cannons of the Enlightenment Age. Artillery has since gradually replaced the use of frontal troops. Modern battles now continue to resemble those of World War II, though prominent innovations have been added. Indirect combat through the use of aircraft and missiles now constitutes a large portion of wars in place of battles, where battles are now mostly reserved for capturing cities.\n\nOne significant difference of modern naval battles as opposed to earlier forms of combat is the use of marines, which introduced amphibious warfare. Today, a marine is actually an infantry regiment that sometimes fights solely on land and is no longer tied to the navy. A good example of an old naval battle is the Battle of Salamis.\n\nMost ancient naval battles were fought by fast ships using the battering ram to sink opposing fleets or steer close enough for boarding in hand-to-hand combat. Troops were often used to storm enemy ships as used by Romans and pirates. This tactic was usually used by civilizations that could not beat the enemy with ranged weaponry.\n\nAnother invention in the late Middle Ages was the use of Greek fire by the Byzantines, which was used to set enemy fleets on fire. Empty demolition ships utilized the tactic to crash into opposing ships and set it afire with an explosion. After the invention of cannons, naval warfare became useful as support units for land warfare.\n\nDuring the 19th century, the development of mines led to a new type of naval warfare. The ironclad, first used in the American Civil War, resistant to cannons, soon made the wooden ship obsolete. The invention of military submarines, during World War I, brought naval warfare to both above and below the surface. With the development of military aircraft during World War II, battles were fought in the sky as well as below the ocean. Aircraft carriers have since become the central unit in naval warfare, acting as a mobile base for lethal aircraft.\n\nAlthough the use of aircraft has for the most part always been used as a supplement to land or naval engagements, since their first major military use in World War I aircraft have increasingly taken on larger roles in warfare. During World War I, the primary use was for reconnaissance, and small-scale bombardment.\n\nAircraft began becoming much more prominent in the Spanish Civil War and especially World War II. Aircraft design began specializing, primarily into two types: bombers, which carried explosive payloads to bomb land targets or ships; and fighter-interceptors, which were used to either intercept incoming aircraft or to escort and protect bombers (engagements between fighter aircraft were known as dog fights). Some of the more notable aerial battles in this period include the Battle of Britain and the Battle of Midway.\n\nAnother important use of aircraft came with the development of the helicopter, which first became heavily used during the Vietnam War, and still continues to be widely used today to transport and augment ground forces.\n\nToday, direct engagements between aircraft are rare – the most modern fighter-interceptors carry much more extensive bombing payloads, and are used to bomb precision land targets, rather than to fight other aircraft. Anti-aircraft batteries are used much more extensively to defend against incoming aircraft than interceptors. Despite this, aircraft today are much more extensively used as the primary tools for both army and navy, as evidenced by the prominent use of helicopters to transport and support troops, the use of aerial bombardment as the \"first strike\" in many engagements, and the replacement of the battleship with the aircraft carrier as the center of most modern navies.\n\nBattles are usually named after some feature of the battlefield geography, such as the name of a town, forest or river, commonly prefixed \"Battle of...\". Occasionally battles are named after the date on which they took place, such as The Glorious First of June.\n\nIn the Middle Ages it was considered important to settle on a suitable name for a battle which could be used by the chroniclers. For example, after Henry V of England defeated a French army on October 25, 1415, he met with the senior French herald and they agreed to name the battle after the nearby castle and so it was called the Battle of Agincourt.\n\nIn other cases, the sides adopted different names for the same battle, such as the Battle of Gallipoli which is known in Turkey as the Battle of Çanakkale. During the American Civil War, the Union tended to name the battles after the nearest watercourse, such as the Battle of Wilsons Creek and the Battle of Stones River, whereas the Confederates favoured the nearby towns, as in the Battles of Chancellorsville and Murfreesboro. Occasionally both names for the same battle entered the popular culture, such as the First and Second Battle of Bull Run, which are also referred to as the First and Second Battle of Manassas.\n\nSometimes in desert warfare, there is no nearby town name to use; map coordinates gave the name to the Battle of 73 Easting in the First Gulf War.\n\nSome place names have become synonymous with the battles that took place there, such as the Passchendaele, Pearl Harbor, the Alamo, Thermopylae, or Waterloo. Military operations, many of which result in battle, are given codenames, which are not necessarily meaningful or indicative of the type or the location of the battle. Operation Market Garden and Operation Rolling Thunder are examples of battles known by their military codenames.\n\nWhen a battleground is the site of more than one battle in the same conflict, the instances are distinguished by ordinal number, such as the First and Second Battles of Bull Run. An extreme case are the twelve Battles of the Isonzo—First to Twelfth—between Italy and Austria-Hungary during the First World War.\n\nSome battles are named for the convenience of military historians so that periods of combat can be neatly distinguished from one another. Following the First World War, the British Battles Nomenclature Committee was formed to decide on standard names for all battles and subsidiary actions. To the soldiers who did the fighting, the distinction was usually academic; a soldier fighting at Beaumont Hamel on November 13, 1916 was probably unaware he was taking part in what the committee would call the \"Battle of the Ancre\".\n\nMany combats are too small to merit a name. Terms such as \"action\", \"skirmish\", \"firefight\", \"raid\" or \"offensive patrol\" are used to describe small-scale battle-like encounters. These combats often take place within the time and space of a battle and while they may have an objective, they are not necessarily \"decisive\". Sometimes the soldiers are unable to immediately gauge the significance of the combat; in the aftermath of the Battle of Waterloo, some British officers were in doubt as to whether the day's events merited the title of \"battle\" or would be passed off as merely an \"action\".\n\nBattles affect the individuals who take part, as well as the political actors. Personal effects of battle range from mild psychological issues to permanent and crippling injuries. Some battle-survivors have nightmares about the conditions they encountered, or abnormal reactions to certain sights or sounds. Some suffer flashbacks. Physical effects of battle can include scars, amputations, lesions, loss of bodily functions, blindness, paralysis — and death.\n\nBattles also affect politics. A decisive battle can cause the losing side to surrender, while a Pyrrhic Victory such as the Battle of Asculum can cause the winning side to reconsider its long-term goals. Battles in civil wars have often decided the fate of monarchs or political factions. Famous examples include the War of the Roses, as well as the Jacobite Uprisings. Battles also affect the commitment of one side or the other to the continuance of a war, for example the Battle of Incheon and the Battle of Hue during the Tet Offensive.\n\n\n"}
{"id": "4182", "url": "https://en.wikipedia.org/wiki?curid=4182", "title": "Berry Berenson", "text": "Berry Berenson\n\nBerinthia \"Berry\" Berenson-Perkins (April 14, 1948 – September 11, 2001) was an American photographer, actress, and model. Perkins, who was the widow of actor Anthony Perkins, died in the September 11 attacks as a passenger on American Airlines Flight 11.\n\nBerenson was born in Murray Hill, Manhattan. Her father, Robert Lawrence Berenson, was an American career diplomat turned shipping executive; he was of Lithuanian Jewish descent, and his family's original surname was Valvrojenski. Her mother was born Maria-Luisa Yvonne Radha de Wendt de Kerlor, better known as Gogo Schiaparelli, a socialite of Italian, Swiss, French, and Egyptian ancestry.\n\nHer maternal grandmother was the Italian-born fashion designer Elsa Schiaparelli, and her maternal grandfather was Wilhelm de Wendt de Kerlor, a Theosophist and psychic medium. Her elder sister, Marisa Berenson, became a well-known model and actress. She also was a great-grandniece of Giovanni Schiaparelli, an Italian astronomer who believed he had discovered the supposed canals of Mars, and a second cousin, once removed, of art expert Bernard Berenson (1865–1959) and his sister Senda Berenson (1868–1954), an athlete and educator who was one of the first two women elected to the Basketball Hall of Fame.\n\nFollowing a brief modeling career in the late 1960s, Berenson became a freelance photographer. By 1973, her photographs had been published in \"Life\", \"Glamour\", \"Vogue\" and \"Newsweek\".\n\nShe studied acting at New York's The American Place Theatre with Wynn Handman along with Richard Gere, Philip Anglim, Penelope Milford, Robert Ozn.Ingrid Bolting and her sister Marisa.\n\nShe also appeared in several motion pictures, including \"Cat People\" with Malcolm McDowell. She starred opposite Anthony Perkins in the 1978 Alan Rudolph film \"Remember My Name\" and opposite Jeff Bridges in the 1979 film \"Winter Kills\".\n\nOn August 9, 1973, in Cape Cod, Massachusetts, Berenson married her future \"Remember My Name\" costar Anthony Perkins. The couple had two sons: actor-musician Oz Perkins (born February 2, 1974) and folk/rock recording artist Elvis Perkins (born February 9, 1976). They remained married until Perkins' death from AIDS-related complications on September 12, 1992.\n\nBerenson died at age 53 in the September 11 attacks aboard American Airlines Flight 11, one day before the ninth anniversary of Perkins' death. She was returning to her California home following a holiday on Cape Cod.\n\nAt the National September 11 Memorial & Museum, Berenson is memorialized at the North Pool, on Panel N-76.\n\nBerenson is survived by her two sons and two grandchildren, a grandson born in 2004 and a granddaughter born in 2008.\n\n"}
{"id": "4183", "url": "https://en.wikipedia.org/wiki?curid=4183", "title": "Botany", "text": "Botany\n\nBotany, also called plant science(s), plant biology or phytology, is the science of plant life and a branch of biology. A botanist or plant scientist is a scientist who specialises in this field. The term \"botany\" comes from the Ancient Greek word (\"botanē\") meaning \"pasture\", \"grass\", or \"fodder\"; is in turn derived from (\"boskein\"), \"to feed\" or \"to graze\". Traditionally, botany has also included the study of fungi and algae by mycologists and phycologists respectively, with the study of these three groups of organisms remaining within the sphere of interest of the International Botanical Congress. Nowadays, botanists (in the strict sense) study approximately 410,000 species of land plants of which some 391,000 species are vascular plants (including ca 369,000 species of flowering plants), and ca 20,000 are bryophytes.\n\nBotany originated in prehistory as herbalism with the efforts of early humans to identify – and later cultivate – edible, medicinal and poisonous plants, making it one of the oldest branches of science. Medieval physic gardens, often attached to monasteries, contained plants of medical importance. They were forerunners of the first botanical gardens attached to universities, founded from the 1540s onwards. One of the earliest was the Padua botanical garden. These gardens facilitated the academic study of plants. Efforts to catalogue and describe their collections were the beginnings of plant taxonomy, and led in 1753 to the binomial system of Carl Linnaeus that remains in use to this day.\n\nIn the 19th and 20th centuries, new techniques were developed for the study of plants, including methods of optical microscopy and live cell imaging, electron microscopy, analysis of chromosome number, plant chemistry and the structure and function of enzymes and other proteins. In the last two decades of the 20th century, botanists exploited the techniques of molecular genetic analysis, including genomics and proteomics and DNA sequences to classify plants more accurately.\n\nModern botany is a broad, multidisciplinary subject with inputs from most other areas of science and technology. Research topics include the study of plant structure, growth and differentiation, reproduction, biochemistry and primary metabolism, chemical products, development, diseases, evolutionary relationships, systematics, and plant taxonomy. Dominant themes in 21st century plant science are molecular genetics and epigenetics, which are the mechanisms and control of gene expression during differentiation of plant cells and tissues. Botanical research has diverse applications in providing staple foods, materials such as timber, oil, rubber, fibre and drugs, in modern horticulture, agriculture and forestry, plant propagation, breeding and genetic modification, in the synthesis of chemicals and raw materials for construction and energy production, in environmental management, and the maintenance of biodiversity.\n\nBotany originated as herbalism, the study and use of plants for their medicinal properties. Many records of the Holocene period date early botanical knowledge as far back as 10,000 years ago. This early unrecorded knowledge of plants was discovered in ancient sites of human occupation within Tennessee, which make up much of the Cherokee land today. The early recorded history of botany includes many ancient writings and plant classifications. Examples of early botanical works have been found in ancient texts from India dating back to before 1100 BC, in archaic Avestan writings, and in works from China before it was unified in 221 BC.\n\nModern botany traces its roots back to Ancient Greece specifically to Theophrastus (c. 371–287 BC), a student of Aristotle who invented and described many of its principles and is widely regarded in the scientific community as the \"Father of Botany\". His major works, \"Enquiry into Plants\" and \"On the Causes of Plants\", constitute the most important contributions to botanical science until the Middle Ages, almost seventeen centuries later.\n\nAnother work from Ancient Greece that made an early impact on botany is \"De Materia Medica\", a five-volume encyclopedia about herbal medicine written in the middle of the first century by Greek physician and pharmacologist Pedanius Dioscorides. \"De Materia Medica\" was widely read for more than 1,500 years. Important contributions from the medieval Muslim world include Ibn Wahshiyya's \"Nabatean Agriculture\", Abū Ḥanīfa Dīnawarī's (828–896) the \"Book of Plants\", and Ibn Bassal's \"The Classification of Soils\". In the early 13th century, Abu al-Abbas al-Nabati, and Ibn al-Baitar (d. 1248) wrote on botany in a systematic and scientific manner.\n\nIn the mid-16th century, \"botanical gardens\" were founded in a number of Italian universities – the Padua botanical garden in 1545 is usually considered to be the first which is still in its original location. These gardens continued the practical value of earlier \"physic gardens\", often associated with monasteries, in which plants were cultivated for medical use. They supported the growth of botany as an academic subject. Lectures were given about the plants grown in the gardens and their medical uses demonstrated. Botanical gardens came much later to northern Europe; the first in England was the University of Oxford Botanic Garden in 1621. Throughout this period, botany remained firmly subordinate to medicine.\n\nGerman physician Leonhart Fuchs (1501–1566) was one of \"the three German fathers of botany\", along with theologian Otto Brunfels (1489–1534) and physician Hieronymus Bock (1498–1554) (also called Hieronymus Tragus). Fuchs and Brunfels broke away from the tradition of copying earlier works to make original observations of their own. Bock created his own system of plant classification.\n\nPhysician Valerius Cordus (1515–1544) authored a botanically and pharmacologically important herbal \"Historia Plantarum\" in 1544 and a pharmacopoeia of lasting importance, the \"Dispensatorium\" in 1546. Naturalist Conrad von Gesner (1516–1565) and herbalist John Gerard (1545–c. 1611) published herbals covering the medicinal uses of plants. Naturalist Ulisse Aldrovandi (1522–1605) was considered the \"father of natural history\", which included the study of plants. In 1665, using an early microscope, Polymath Robert Hooke discovered cells, a term he coined, in cork, and a short time later in living plant tissue.\n\nDuring the 18th century, systems of plant identification were developed comparable to dichotomous keys, where unidentified plants are placed into taxonomic groups (e.g. family, genus and species) by making a series of choices between pairs of characters. The choice and sequence of the characters may be artificial in keys designed purely for identification (diagnostic keys) or more closely related to the natural or phyletic order of the taxa in synoptic keys. By the 18th century, new plants for study were arriving in Europe in increasing numbers from newly discovered countries and the European colonies worldwide. In 1753 Carl von Linné (Carl Linnaeus) published his Species Plantarum, a hierarchical classification of plant species that remains the reference point for modern botanical nomenclature. This established a standardised binomial or two-part naming scheme where the first name represented the genus and the second identified the species within the genus. For the purposes of identification, Linnaeus's \"Systema Sexuale\" classified plants into 24 groups according to the number of their male sexual organs. The 24th group, \"Cryptogamia\", included all plants with concealed reproductive parts, mosses, liverworts, ferns, algae and fungi.\n\nIncreasing knowledge of plant anatomy, morphology and life cycles led to the realisation that there were more natural affinities between plants than the artificial sexual system of Linnaeus. Adanson (1763), de Jussieu (1789), and Candolle (1819) all proposed various alternative natural systems of classification that grouped plants using a wider range of shared characters and were widely followed. The Candollean system reflected his ideas of the progression of morphological complexity and the later classification by Bentham and Hooker, which was influential until the mid-19th century, was influenced by Candolle's approach. Darwin's publication of the \"Origin of Species\" in 1859 and his concept of common descent required modifications to the Candollean system to reflect evolutionary relationships as distinct from mere morphological similarity.\n\nBotany was greatly stimulated by the appearance of the first \"modern\" textbook, Matthias Schleiden's \"\", published in English in 1849 as \"Principles of Scientific Botany\". Schleiden was a microscopist and an early plant anatomist who co-founded the cell theory with Theodor Schwann and Rudolf Virchow and was among the first to grasp the significance of the cell nucleus that had been described by Robert Brown in 1831.\nIn 1855, Adolf Fick formulated Fick's laws that enabled the calculation of the rates of molecular diffusion in biological systems.\n\nBuilding upon the gene-chromosome theory of heredity that originated with Gregor Mendel (1822–1884), August Weismann (1834–1914) proved that inheritance only takes place through gametes. No other cells can pass on inherited characters. The work of Katherine Esau (1898–1997) on plant anatomy is still a major foundation of modern botany. Her books \"Plant Anatomy\" and \"Anatomy of Seed Plants\" have been key plant structural biology texts for more than half a century.\n\nThe discipline of plant ecology was pioneered in the late 19th century by botanists such as Eugenius Warming, who produced the hypothesis that plants form communities, and his mentor and successor Christen C. Raunkiær whose system for describing plant life forms is still in use today. The concept that the composition of plant communities such as temperate broadleaf forest changes by a process of ecological succession was developed by Henry Chandler Cowles, Arthur Tansley and Frederic Clements. Clements is credited with the idea of climax vegetation as the most complex vegetation that an environment can support and Tansley introduced the concept of ecosystems to biology. Building on the extensive earlier work of Alphonse de Candolle, Nikolai Vavilov (1887–1943) produced accounts of the biogeography, centres of origin, and evolutionary history of economic plants.\n\nParticularly since the mid-1960s there have been advances in understanding of the physics of plant physiological processes such as transpiration (the transport of water within plant tissues), the temperature dependence of rates of water evaporation from the leaf surface and the molecular diffusion of water vapour and carbon dioxide through stomatal apertures. These developments, coupled with new methods for measuring the size of stomatal apertures, and the rate of photosynthesis have enabled precise description of the rates of gas exchange between plants and the atmosphere. Innovations in statistical analysis by Ronald Fisher, Frank Yates and others at Rothamsted Experimental Station facilitated rational experimental design and data analysis in botanical research. The discovery and identification of the auxin plant hormones by Kenneth V. Thimann in 1948 enabled regulation of plant growth by externally applied chemicals. Frederick Campion Steward pioneered techniques of micropropagation and plant tissue culture controlled by plant hormones. The synthetic auxin 2,4-Dichlorophenoxyacetic acid or 2,4-D was one of the first commercial synthetic herbicides.\n\n20th century developments in plant biochemistry have been driven by modern techniques of organic chemical analysis, such as spectroscopy, chromatography and electrophoresis. With the rise of the related molecular-scale biological approaches of molecular biology, genomics, proteomics and metabolomics, the relationship between the plant genome and most aspects of the biochemistry, physiology, morphology and behaviour of plants can be subjected to detailed experimental analysis. The concept originally stated by Gottlieb Haberlandt in 1902 that all plant cells are totipotent and can be grown \"in vitro\" ultimately enabled the use of genetic engineering experimentally to knock out a gene or genes responsible for a specific trait, or to add genes such as GFP that report when a gene of interest is being expressed. These technologies enable the biotechnological use of whole plants or plant cell cultures grown in bioreactors to synthesise pesticides, antibiotics or other pharmaceuticals, as well as the practical application of genetically modified crops designed for traits such as improved yield.\n\nModern morphology recognises a continuum between the major morphological categories of root, stem (caulome), leaf (phyllome) and trichome. Furthermore, it emphasises structural dynamics. Modern systematics aims to reflect and discover phylogenetic relationships between plants. Modern Molecular phylogenetics largely ignores morphological characters, relying on DNA sequences as data. Molecular analysis of DNA sequences from most families of flowering plants enabled the Angiosperm Phylogeny Group to publish in 1998 a phylogeny of flowering plants, answering many of the questions about relationships among angiosperm families and species. The theoretical possibility of a practical method for identification of plant species and commercial varieties by DNA barcoding is the subject of active current research.\n\nThe study of plants is vital because they underpin almost all animal life on Earth by generating a large proportion of the oxygen and food that provide humans and other organisms with aerobic respiration with the chemical energy they need to exist. Plants, algae and cyanobacteria are the major groups of organisms that carry out photosynthesis, a process that uses the energy of sunlight to convert water and carbon dioxide into sugars that can be used both as a source of chemical energy and of organic molecules that are used in the structural components of cells. As a by-product of photosynthesis, plants release oxygen into the atmosphere, a gas that is required by nearly all living things to carry out cellular respiration. In addition, they are influential in the global carbon and water cycles and plant roots bind and stabilise soils, preventing soil erosion. Plants are crucial to the future of human society as they provide food, oxygen, medicine, and products for people, as well as creating and preserving soil.\n\nHistorically, all living things were classified as either animals or plants and botany covered the study of all organisms not considered animals. Botanists examine both the internal functions and processes within plant organelles, cells, tissues, whole plants, plant populations and plant communities. At each of these levels, a botanist may be concerned with the classification (taxonomy), phylogeny and evolution, structure (anatomy and morphology), or function (physiology) of plant life.\n\nThe strictest definition of \"plant\" includes only the \"land plants\" or embryophytes, which include seed plants (gymnosperms, including the pines, and flowering plants) and the free-sporing cryptogams including ferns, clubmosses, liverworts, hornworts and mosses. Embryophytes are multicellular eukaryotes descended from an ancestor that obtained its energy from sunlight by photosynthesis. They have life cycles with alternating haploid and diploid phases. The sexual haploid phase of embryophytes, known as the gametophyte, nurtures the developing diploid embryo sporophyte within its tissues for at least part of its life, even in the seed plants, where the gametophyte itself is nurtured by its parent sporophyte. Other groups of organisms that were previously studied by botanists include bacteria (now studied in bacteriology), fungi (mycology) – including lichen-forming fungi (lichenology), non-chlorophyte algae (phycology), and viruses (virology). However, attention is still given to these groups by botanists, and fungi (including lichens) and photosynthetic protists are usually covered in introductory botany courses.\n\nPalaeobotanists study ancient plants in the fossil record to provide information about the evolutionary history of plants. Cyanobacteria, the first oxygen-releasing photosynthetic organisms on Earth, are thought to have given rise to the ancestor of plants by entering into an endosymbiotic relationship with an early eukaryote, ultimately becoming the chloroplasts in plant cells. The new photosynthetic plants (along with their algal relatives) accelerated the rise in atmospheric oxygen started by the cyanobacteria, changing the ancient oxygen-free, reducing, atmosphere to one in which free oxygen has been abundant for more than 2 billion years.\n\nAmong the important botanical questions of the 21st century are the role of plants as primary producers in the global cycling of life's basic ingredients: energy, carbon, oxygen, nitrogen and water, and ways that our plant stewardship can help address the global environmental issues of resource management, conservation, human food security, biologically invasive organisms, carbon sequestration, climate change, and sustainability.\n\nVirtually all staple foods come either directly from primary production by plants, or indirectly from animals that eat them. Plants and other photosynthetic organisms are at the base of most food chains because they use the energy from the sun and nutrients from the soil and atmosphere, converting them into a form that can be used by animals. This is what ecologists call the first trophic level. The modern forms of the major staple foods, such as maize, rice, wheat and other cereal grasses, pulses, bananas and plantains, as well as flax and cotton grown for their fibres, are the outcome of prehistoric selection over thousands of years from among wild ancestral plants with the most desirable characteristics.\n\nBotanists study how plants produce food and how to increase yields, for example through plant breeding, making their work important to mankind's ability to feed the world and provide food security for future generations. Botanists also study weeds, which are a considerable problem in agriculture, and the biology and control of plant pathogens in agriculture and natural ecosystems. Ethnobotany is the study of the relationships between plants and people. When applied to the investigation of historical plant–people relationships ethnobotany may be referred to as archaeobotany or palaeoethnobotany. Some of the earliest plant-people relationships arose between the indigenous people of Canada in identifying edible plants from inedible plants. This relationship the indigenous people had with plants was recorded by ethnobotanists.\n\nPlant biochemistry is the study of the chemical processes used by plants. Some of these processes are used in their primary metabolism like the photosynthetic Calvin cycle and crassulacean acid metabolism. Others make specialised materials like the cellulose and lignin used to build their bodies, and secondary products like resins and aroma compounds.\n\nPlants and various other groups of photosynthetic eukaryotes collectively known as \"algae\" have unique organelles known as chloroplasts. Chloroplasts are thought to be descended from cyanobacteria that formed endosymbiotic relationships with ancient plant and algal ancestors. Chloroplasts and cyanobacteria contain the blue-green pigment chlorophyll \"a\". Chlorophyll \"a\" (as well as its plant and green algal-specific cousin chlorophyll \"b\") absorbs light in the blue-violet and orange/red parts of the spectrum while reflecting and transmitting the green light that we see as the characteristic colour of these organisms. The energy in the red and blue light that these pigments absorb is used by chloroplasts to make energy-rich carbon compounds from carbon dioxide and water by oxygenic photosynthesis, a process that generates molecular oxygen (O) as a by-product.\n\nThe light energy captured by chlorophyll \"a\" is initially in the form of electrons (and later a proton gradient) that's used to make molecules of ATP and NADPH which temporarily store and transport energy. Their energy is used in the light-independent reactions of the Calvin cycle by the enzyme rubisco to produce molecules of the 3-carbon sugar glyceraldehyde 3-phosphate (G3P). Glyceraldehyde 3-phosphate is the first product of photosynthesis and the raw material from which glucose and almost all other organic molecules of biological origin are synthesised. Some of the glucose is converted to starch which is stored in the chloroplast. Starch is the characteristic energy store of most land plants and algae, while inulin, a polymer of fructose is used for the same purpose in the sunflower family Asteraceae. Some of the glucose is converted to sucrose (common table sugar) for export to the rest of the plant.\n\nUnlike in animals (which lack chloroplasts), plants and their eukaryote relatives have delegated many biochemical roles to their chloroplasts, including synthesising all their fatty acids, and most amino acids. The fatty acids that chloroplasts make are used for many things, such as providing material to build cell membranes out of and making the polymer cutin which is found in the plant cuticle that protects land plants from drying out. \n\nPlants synthesise a number of unique polymers like the polysaccharide molecules cellulose, pectin and xyloglucan from which the land plant cell wall is constructed.\nVascular land plants make lignin, a polymer used to strengthen the secondary cell walls of xylem tracheids and vessels to keep them from collapsing when a plant sucks water through them under water stress. Lignin is also used in other cell types like sclerenchyma fibres that provide structural support for a plant and is a major constituent of wood. Sporopollenin is a chemically resistant polymer found in the outer cell walls of spores and pollen of land plants responsible for the survival of early land plant spores and the pollen of seed plants in the fossil record. It is widely regarded as a marker for the start of land plant evolution during the Ordovician period.\nThe concentration of carbon dioxide in the atmosphere today is much lower than it was when plants emerged onto land during the Ordovician and Silurian periods. Many monocots like maize and the pineapple and some dicots like the Asteraceae have since independently evolved pathways like Crassulacean acid metabolism and the carbon fixation pathway for photosynthesis which avoid the losses resulting from photorespiration in the more common carbon fixation pathway. These biochemical strategies are unique to land plants.\n\nPhytochemistry is a branch of plant biochemistry primarily concerned with the chemical substances produced by plants during secondary metabolism. Some of these compounds are toxins such as the alkaloid coniine from hemlock. Others, such as the essential oils peppermint oil and lemon oil are useful for their aroma, as flavourings and spices (e.g., capsaicin), and in medicine as pharmaceuticals as in opium from opium poppies. Many medicinal and recreational drugs, such as tetrahydrocannabinol (active ingredient in cannabis), caffeine, morphine and nicotine come directly from plants. Others are simple derivatives of botanical natural products. For example, the pain killer aspirin is the acetyl ester of salicylic acid, originally isolated from the bark of willow trees, and a wide range of opiate painkillers like heroin are obtained by chemical modification of morphine obtained from the opium poppy. Popular stimulants come from plants, such as caffeine from coffee, tea and chocolate, and nicotine from tobacco. Most alcoholic beverages come from fermentation of carbohydrate-rich plant products such as barley (beer), rice (sake) and grapes (wine). Native Americans have used various plants as ways of treating illness or disease for thousands of years. This knowledge Native Americans have on plants has been recorded by enthnobotanists and then in turn has been used by pharmaceutical companies as a way of drug discovery.\n\nPlants can synthesise useful coloured dyes and pigments such as the anthocyanins responsible for the red colour of red wine, yellow weld and blue woad used together to produce Lincoln green, indoxyl, source of the blue dye indigo traditionally used to dye denim and the artist's pigments gamboge and rose madder.\nSugar, starch, cotton, linen, hemp, some types of rope, wood and particle boards, papyrus and paper, vegetable oils, wax, and natural rubber are examples of commercially important materials made from plant tissues or their secondary products. Charcoal, a pure form of carbon made by pyrolysis of wood, has a long history as a metal-smelting fuel, as a filter material and adsorbent and as an artist's material and is one of the three ingredients of gunpowder. Cellulose, the world's most abundant organic polymer, can be converted into energy, fuels, materials and chemical feedstock. Products made from cellulose include rayon and cellophane, wallpaper paste, biobutanol and gun cotton. Sugarcane, rapeseed and soy are some of the plants with a highly fermentable sugar or oil content that are used as sources of biofuels, important alternatives to fossil fuels, such as biodiesel. Sweetgrass was used by NativeAmericanse to ward of bugs like mosquitoes. These bug repelling properties of sweetgrass were later found by the American Chemical Society in the molecules phytol and coumarin.\n\nPlant ecology is the science of the functional relationships between plants and their habitats—the environments where they complete their life cycles. Plant ecologists study the composition of local and regional floras, their biodiversity, genetic diversity and fitness, the adaptation of plants to their environment, and their competitive or mutualistic interactions with other species. Some ecologists even rely on empirical data from indigenous people that is gathered by ethnobotanists. This information can relay a great deal of information on how the land once was thousands of years ago and how it has changed over that time. The goals of plant ecology are to understand the causes of their distribution patterns, productivity, environmental impact, evolution, and responses to environmental change.\n\nPlants depend on certain edaphic (soil) and climatic factors in their environment but can modify these factors too. For example, they can change their environment's albedo, increase runoff interception, stabilise mineral soils and develop their organic content, and affect local temperature. Plants compete with other organisms in their ecosystem for resources. They interact with their neighbours at a variety of spatial scales in groups, populations and communities that collectively constitute vegetation. Regions with characteristic vegetation types and dominant plants as well as similar abiotic and biotic factors, climate, and geography make up biomes like tundra or tropical rainforest.\nHerbivores eat plants, but plants can defend themselves and some species are parasitic or even carnivorous. Other organisms form mutually beneficial relationships with plants. For example, mycorrhizal fungi and rhizobia provide plants with nutrients in exchange for food, ants are recruited by ant plants to provide protection, honey bees, bats and other animals pollinate flowers and humans and other animals act as dispersal vectors to spread spores and seeds.\n\nPlant responses to climate and other environmental changes can inform our understanding of how these changes affect ecosystem function and productivity. For example, plant phenology can be a useful proxy for temperature in historical climatology, and the biological impact of climate change and global warming. Palynology, the analysis of fossil pollen deposits in sediments from thousands or millions of years ago allows the reconstruction of past climates. Estimates of atmospheric concentrations since the Palaeozoic have been obtained from stomatal densities and the leaf shapes and sizes of ancient land plants. Ozone depletion can expose plants to higher levels of ultraviolet radiation-B (UV-B), resulting in lower growth rates. Moreover, information from studies of community ecology, plant systematics, and taxonomy is essential to understanding vegetation change, habitat destruction and species extinction.\n\nInheritance in plants follows the same fundamental principles of genetics as in other multicellular organisms. Gregor Mendel discovered the genetic laws of inheritance by studying inherited traits such as shape in \"Pisum sativum\" (peas). What Mendel learned from studying plants has had far reaching benefits outside of botany. Similarly, \"jumping genes\" were discovered by Barbara McClintock while she was studying maize. Nevertheless, there are some distinctive genetic differences between plants and other organisms.\n\nSpecies boundaries in plants may be weaker than in animals, and cross species hybrids are often possible. A familiar example is peppermint, \"Mentha\" × \"piperita\", a sterile hybrid between \"Mentha aquatica\" and spearmint, \"Mentha spicata\". The many cultivated varieties of wheat are the result of multiple inter- and intra-specific crosses between wild species and their hybrids. Angiosperms with monoecious flowers often have self-incompatibility mechanisms that operate between the pollen and stigma so that the pollen either fails to reach the stigma or fails to germinate and produce male gametes. This is one of several methods used by plants to promote outcrossing. In many land plants the male and female gametes are produced by separate individuals. These species are said to be dioecious when referring to vascular plant sporophytes and dioicous when referring to bryophyte gametophytes.\n\nUnlike in higher animals, where parthenogenesis is rare, asexual reproduction may occur in plants by several different mechanisms. The formation of stem tubers in potato is one example. Particularly in arctic or alpine habitats, where opportunities for fertilisation of flowers by animals are rare, plantlets or bulbs, may develop instead of flowers, replacing sexual reproduction with asexual reproduction and giving rise to clonal populations genetically identical to the parent. This is one of several types of apomixis that occur in plants. Apomixis can also happen in a seed, producing a seed that contains an embryo genetically identical to the parent.\n\nMost sexually reproducing organisms are diploid, with paired chromosomes, but doubling of their chromosome number may occur due to errors in cytokinesis. This can occur early in development to produce an autopolyploid or partly autopolyploid organism, or during normal processes of cellular differentiation to produce some cell types that are polyploid (endopolyploidy), or during gamete formation. An allopolyploid plant may result from a hybridisation event between two different species. Both autopolyploid and allopolyploid plants can often reproduce normally, but may be unable to cross-breed successfully with the parent population because there is a mismatch in chromosome numbers. These plants that are reproductively isolated from the parent species but live within the same geographical area, may be sufficiently successful to form a new species. Some otherwise sterile plant polyploids can still reproduce vegetatively or by seed apomixis, forming clonal populations of identical individuals. Durum wheat is a fertile tetraploid allopolyploid, while bread wheat is a fertile hexaploid. The commercial banana is an example of a sterile, seedless triploid hybrid. Common dandelion is a triploid that produces viable seeds by apomictic seed.\n\nAs in other eukaryotes, the inheritance of endosymbiotic organelles like mitochondria and chloroplasts in plants is non-Mendelian. Chloroplasts are inherited through the male parent in gymnosperms but often through the female parent in flowering plants.\n\nA considerable amount of new knowledge about plant function comes from studies of the molecular genetics of model plants such as the Thale cress, \"Arabidopsis thaliana\", a weedy species in the mustard family (Brassicaceae). The genome or hereditary information contained in the genes of this species is encoded by about 135 million base pairs of DNA, forming one of the smallest genomes among flowering plants. \"Arabidopsis\" was the first plant to have its genome sequenced, in 2000. The sequencing of some other relatively small genomes, of rice (\"Oryza sativa\") and \"Brachypodium distachyon\", has made them important model species for understanding the genetics, cellular and molecular biology of cereals, grasses and monocots generally.\n\nModel plants such as \"Arabidopsis thaliana\" are used for studying the molecular biology of plant cells and the chloroplast. Ideally, these organisms have small genomes that are well known or completely sequenced, small stature and short generation times. Corn has been used to study mechanisms of photosynthesis and phloem loading of sugar in plants. The single celled green alga \"Chlamydomonas reinhardtii\", while not an embryophyte itself, contains a green-pigmented chloroplast related to that of land plants, making it useful for study. A red alga \"Cyanidioschyzon merolae\" has also been used to study some basic chloroplast functions. Spinach, peas, soybeans and a moss \"Physcomitrella patens\" are commonly used to study plant cell biology.\n\n\"Agrobacterium tumefaciens\", a soil rhizosphere bacterium, can attach to plant cells and infect them with a callus-inducing Ti plasmid by horizontal gene transfer, causing a callus infection called crown gall disease. Schell and Van Montagu (1977) hypothesised that the Ti plasmid could be a natural vector for introducing the Nif gene responsible for nitrogen fixation in the root nodules of legumes and other plant species. Today, genetic modification of the Ti plasmid is one of the main techniques for introduction of transgenes to plants and the creation of genetically modified crops.\n\nEpigenetics is the study of heritable changes in gene function that cannot be explained by changes in the underlying DNA sequence but cause the organism's genes to behave (or \"express themselves\") differently. One example of epigenetic change is the marking of the genes by DNA methylation which determines whether they will be expressed or not. Gene expression can also be controlled by repressor proteins that attach to silencer regions of the DNA and prevent that region of the DNA code from being expressed. Epigenetic marks may be added or removed from the DNA during programmed stages of development of the plant, and are responsible, for example, for the differences between anthers, petals and normal leaves, despite the fact that they all have the same underlying genetic code. Epigenetic changes may be temporary or may remain through successive cell divisions for the remainder of the cell's life. Some epigenetic changes have been shown to be heritable, while others are reset in the germ cells.\n\nEpigenetic changes in eukaryotic biology serve to regulate the process of cellular differentiation. During morphogenesis, totipotent stem cells become the various pluripotent cell lines of the embryo, which in turn become fully differentiated cells. A single fertilised egg cell, the zygote, gives rise to the many different plant cell types including parenchyma, xylem vessel elements, phloem sieve tubes, guard cells of the epidermis, etc. as it continues to divide. The process results from the epigenetic activation of some genes and inhibition of others.\n\nUnlike animals, many plant cells, particularly those of the parenchyma, do not terminally differentiate, remaining totipotent with the ability to give rise to a new individual plant. Exceptions include highly lignified cells, the sclerenchyma and xylem which are dead at maturity, and the phloem sieve tubes which lack nuclei. While plants use many of the same epigenetic mechanisms as animals, such as chromatin remodelling, an alternative hypothesis is that plants set their gene expression patterns using positional information from the environment and surrounding cells to determine their developmental fate.\n\nThe chloroplasts of plants have a number of biochemical, structural and genetic similarities to cyanobacteria, (commonly but incorrectly known as \"blue-green algae\") and are thought to be derived from an ancient endosymbiotic relationship between an ancestral eukaryotic cell and a cyanobacterial resident.\n\nThe algae are a polyphyletic group and are placed in various divisions, some more closely related to plants than others. There are many differences between them in features such as cell wall composition, biochemistry, pigmentation, chloroplast structure and nutrient reserves. The algal division Charophyta, sister to the green algal division Chlorophyta, is considered to contain the ancestor of true plants. The Charophyte class Charophyceae and the land plant sub-kingdom Embryophyta together form the monophyletic group or clade Streptophytina.\n\nNonvascular land plants are embryophytes that lack the vascular tissues xylem and phloem. They include mosses, liverworts and hornworts. Pteridophytic vascular plants with true xylem and phloem that reproduced by spores germinating into free-living gametophytes evolved during the Silurian period and diversified into several lineages during the late Silurian and early Devonian. Representatives of the lycopods have survived to the present day. By the end of the Devonian period, several groups, including the lycopods, sphenophylls and progymnosperms, had independently evolved \"megaspory\" – their spores were of two distinct sizes, larger megaspores and smaller microspores. Their reduced gametophytes developed from megaspores retained within the spore-producing organs (megasporangia) of the sporophyte, a condition known as endospory. Seeds consist of an endosporic megasporangium surrounded by one or two sheathing layers (integuments). The young sporophyte develops within the seed, which on germination splits to release it. The earliest known seed plants date from the latest Devonian Famennian stage. Following the evolution of the seed habit, seed plants diversified, giving rise to a number of now-extinct groups, including seed ferns, as well as the modern gymnosperms and angiosperms. Gymnosperms produce \"naked seeds\" not fully enclosed in an ovary; modern representatives include conifers, cycads, \"Ginkgo\", and Gnetales. Angiosperms produce seeds enclosed in a structure such as a carpel or an ovary. Ongoing research on the molecular phylogenetics of living plants appears to show that the angiosperms are a sister clade to the gymnosperms.\n\nPlant physiology encompasses all the internal chemical and physical activities of plants associated with life. Chemicals obtained from the air, soil and water form the basis of all plant metabolism. The energy of sunlight, captured by oxygenic photosynthesis and released by cellular respiration, is the basis of almost all life. Photoautotrophs, including all green plants, algae and cyanobacteria gather energy directly from sunlight by photosynthesis. Heterotrophs including all animals, all fungi, all completely parasitic plants, and non-photosynthetic bacteria take in organic molecules produced by photoautotrophs and respire them or use them in the construction of cells and tissues. Respiration is the oxidation of carbon compounds by breaking them down into simpler structures to release the energy they contain, essentially the opposite of photosynthesis.\n\nMolecules are moved within plants by transport processes that operate at a variety of spatial scales. Subcellular transport of ions, electrons and molecules such as water and enzymes occurs across cell membranes. Minerals and water are transported from roots to other parts of the plant in the transpiration stream. Diffusion, osmosis, and active transport and mass flow are all different ways transport can occur. Examples of elements that plants need to transport are nitrogen, phosphorus, potassium, calcium, magnesium, and sulphur. In vascular plants, these elements are extracted from the soil as soluble ions by the roots and transported throughout the plant in the xylem. Most of the elements required for plant nutrition come from the chemical breakdown of soil minerals. Sucrose produced by photosynthesis is transported from the leaves to other parts of the plant in the phloem and plant hormones are transported by a variety of processes.\n\nPlants are not passive, but respond to external signals such as light, touch, and injury by moving or growing towards or away from the stimulus, as appropriate. Tangible evidence of touch sensitivity is the almost instantaneous collapse of leaflets of \"Mimosa pudica\", the insect traps of Venus flytrap and bladderworts, and the pollinia of orchids.\n\nThe hypothesis that plant growth and development is coordinated by plant hormones or plant growth regulators first emerged in the late 19th century. Darwin experimented on the movements of plant shoots and roots towards light and gravity, and concluded \"It is hardly an exaggeration to say that the tip of the radicle . . acts like the brain of one of the lower animals . . directing the several movements\". About the same time, the role of auxins (from the Greek auxein, to grow) in control of plant growth was first outlined by the Dutch scientist Frits Went. The first known auxin, indole-3-acetic acid (IAA), which promotes cell growth, was only isolated from plants about 50 years later. This compound mediates the tropic responses of shoots and roots towards light and gravity. The finding in 1939 that plant callus could be maintained in culture containing IAA, followed by the observation in 1947 that it could be induced to form roots and shoots by controlling the concentration of growth hormones were key steps in the development of plant biotechnology and genetic modification.\n\nAnother class of phytohormones is the jasmonates, first isolated from the oil of \"Jasminum grandiflorum\" which regulates wound responses in plants by unblocking the expression of genes required in the systemic acquired resistance response to pathogen attack.\n\nIn addition to being the primary energy source for plants, light functions as a signalling device, providing information to the plant, such as how much sunlight the plant receives each day. This can result in adaptive changes in a process known as photomorphogenesis. Phytochromes are the photoreceptors in a plant that are sensitive to light.\n\nPlant anatomy is the study of the structure of plant cells and tissues, whereas plant morphology is the study of their external form.\nAll plants are multicellular eukaryotes, their DNA stored in nuclei. The characteristic features of plant cells that distinguish them from those of animals and fungi include a primary cell wall composed of the polysaccharides cellulose, hemicellulose and pectin, larger vacuoles than in animal cells and the presence of plastids with unique photosynthetic and biosynthetic functions as in the chloroplasts. Other plastids contain storage products such as starch (amyloplasts) or lipids (elaioplasts). Uniquely, streptophyte cells and those of the green algal order Trentepohliales divide by construction of a phragmoplast as a template for building a cell plate late in cell division.\n\nThe bodies of vascular plants including clubmosses, ferns and seed plants (gymnosperms and angiosperms) generally have aerial and subterranean subsystems. The shoots consist of stems bearing green photosynthesising leaves and reproductive structures. The underground vascularised roots bear root hairs at their tips and generally lack chlorophyll. Non-vascular plants, the liverworts, hornworts and mosses do not produce ground-penetrating vascular roots and most of the plant participates in photosynthesis. The sporophyte generation is nonphotosynthetic in liverworts but may be able to contribute part of its energy needs by photosynthesis in mosses and hornworts.\n\nThe root system and the shoot system are interdependent – the usually nonphotosynthetic root system depends on the shoot system for food, and the usually photosynthetic shoot system depends on water and minerals from the root system. Cells in each system are capable of creating cells of the other and producing adventitious shoots or roots. Stolons and tubers are examples of shoots that can grow roots. Roots that spread out close to the surface, such as those of willows, can produce shoots and ultimately new plants. In the event that one of the systems is lost, the other can often regrow it. In fact it is possible to grow an entire plant from a single leaf, as is the case with \"Saintpaulia\", or even a single cell – which can dedifferentiate into a callus (a mass of unspecialised cells) that can grow into a new plant.\nIn vascular plants, the xylem and phloem are the conductive tissues that transport resources between shoots and roots. Roots are often adapted to store food such as sugars or starch, as in sugar beets and carrots.\n\nStems mainly provide support to the leaves and reproductive structures, but can store water in succulent plants such as cacti, food as in potato tubers, or reproduce vegetatively as in the stolons of strawberry plants or in the process of layering. Leaves gather sunlight and carry out photosynthesis. Large, flat, flexible, green leaves are called foliage leaves. Gymnosperms, such as conifers, cycads, \"Ginkgo\", and gnetophytes are seed-producing plants with open seeds. Angiosperms are seed-producing plants that produce flowers and have enclosed seeds. Woody plants, such as azaleas and oaks, undergo a secondary growth phase resulting in two additional types of tissues: wood (secondary xylem) and bark (secondary phloem and cork). All gymnosperms and many angiosperms are woody plants. Some plants reproduce sexually, some asexually, and some via both means.\n\nAlthough reference to major morphological categories such as root, stem, leaf, and trichome are useful, one has to keep in mind that these categories are linked through intermediate forms so that a continuum between the categories results. Furthermore, structures can be seen as processes, that is, process combinations.\n\nSystematic botany is part of systematic biology, which is concerned with the range and diversity of organisms and their relationships, particularly as determined by their evolutionary history. It involves, or is related to, biological classification, scientific taxonomy and phylogenetics. Biological classification is the method by which botanists group organisms into categories such as genera or species. Biological classification is a form of scientific taxonomy. Modern taxonomy is rooted in the work of Carl Linnaeus, who grouped species according to shared physical characteristics. These groupings have since been revised to align better with the Darwinian principle of common descent – grouping organisms by ancestry rather than superficial characteristics. While scientists do not always agree on how to classify organisms, molecular phylogenetics, which uses DNA sequences as data, has driven many recent revisions along evolutionary lines and is likely to continue to do so. The dominant classification system is called Linnaean taxonomy. It includes ranks and binomial nomenclature. The nomenclature of botanical organisms is codified in the International Code of Nomenclature for algae, fungi, and plants (ICN) and administered by the International Botanical Congress.\n\nKingdom Plantae belongs to Domain Eukarya and is broken down recursively until each species is separately classified. The order is: Kingdom; Phylum (or Division); Class; Order; Family; Genus (plural \"genera\"); Species. The scientific name of a plant represents its genus and its species within the genus, resulting in a single worldwide name for each organism. For example, the tiger lily is \"Lilium columbianum\". \"Lilium\" is the genus, and \"columbianum\" the specific epithet. The combination is the name of the species. When writing the scientific name of an organism, it is proper to capitalise the first letter in the genus and put all of the specific epithet in lowercase. Additionally, the entire term is ordinarily italicised (or underlined when italics are not available).\n\nThe evolutionary relationships and heredity of a group of organisms is called its phylogeny. Phylogenetic studies attempt to discover phylogenies. The basic approach is to use similarities based on shared inheritance to determine relationships. As an example, species of \"Pereskia\" are trees or bushes with prominent leaves. They do not obviously resemble a typical leafless cactus such as an \"Echinocactus\". However, both \"Pereskia\" and \"Echinocactus\" have spines produced from areoles (highly specialised pad-like structures) suggesting that the two genera are indeed related.\nJudging relationships based on shared characters requires care, since plants may resemble one another through convergent evolution in which characters have arisen independently. Some euphorbias have leafless, rounded bodies adapted to water conservation similar to those of globular cacti, but characters such as the structure of their flowers make it clear that the two groups are not closely related. The cladistic method takes a systematic approach to characters, distinguishing between those that carry no information about shared evolutionary history – such as those evolved separately in different groups (homoplasies) or those left over from ancestors (plesiomorphies) – and derived characters, which have been passed down from innovations in a shared ancestor (apomorphies). Only derived characters, such as the spine-producing areoles of cacti, provide evidence for descent from a common ancestor. The results of cladistic analyses are expressed as cladograms: tree-like diagrams showing the pattern of evolutionary branching and descent.\n\nFrom the 1990s onwards, the predominant approach to constructing phylogenies for living plants has been molecular phylogenetics, which uses molecular characters, particularly DNA sequences, rather than morphological characters like the presence or absence of spines and areoles. The difference is that the genetic code itself is used to decide evolutionary relationships, instead of being used indirectly via the characters it gives rise to. Clive Stace describes this as having \"direct access to the genetic basis of evolution.\" As a simple example, prior to the use of genetic evidence, fungi were thought either to be plants or to be more closely related to plants than animals. Genetic evidence suggests that the true evolutionary relationship of multicelled organisms is as shown in the cladogram below – fungi are more closely related to animals than to plants.\n\nIn 1998 the Angiosperm Phylogeny Group published a phylogeny for flowering plants based on an analysis of DNA sequences from most families of flowering plants. As a result of this work, many questions, such as which families represent the earliest branches of angiosperms, have now been answered. Investigating how plant species are related to each other allows botanists to better understand the process of evolution in plants. Despite the study of model plants and increasing use of DNA evidence, there is ongoing work and discussion among taxonomists about how best to classify plants into various taxa. Technological developments such as computers and electron microscopes have greatly increased the level of detail studied and speed at which data can be analysed.\n\n\n"}
{"id": "4184", "url": "https://en.wikipedia.org/wiki?curid=4184", "title": "Bacillus thuringiensis", "text": "Bacillus thuringiensis\n\nBacillus thuringiensis (or Bt) is a Gram-positive, soil-dwelling bacterium, commonly used as a biological pesticide. \"B. thuringiensis\" also occurs naturally in the gut of caterpillars of various types of moths and butterflies, as well on leaf surfaces, aquatic environments, animal feces, insect-rich environments, and flour mills and grain-storage facilities.\n\nDuring sporulation, many Bt strains produce crystal proteins (proteinaceous inclusions), called δ-endotoxins, that have insecticidal action. This has led to their use as insecticides, and more recently to genetically modified crops using Bt genes, such as Bt corn. Many crystal-producing Bt strains, though, do not have insecticidal properties.\n\n\"B. thuringiensis\" was first discovered in 1901 by Japanese biologist Shigetane Ishiwata. In 1911, \"B. thuringiensis\" was rediscovered in Germany by Ernst Berliner, who isolated it as the cause of a disease called \"Schlaffsucht\" in flour moth caterpillars. In 1976, Robert A. Zakharyan reported the presence of a plasmid in a strain of \"B. thuringiensis\" and suggested the plasmid's involvement in endospore and crystal formation. \"B. thuringiensis\" is closely related to \"B. cereus\", a soil bacterium, and \"B. anthracis\", the cause of anthrax; the three organisms differ mainly in their plasmids. Like other members of the genus, all three are aerobes capable of producing endospores.\n\nThere are several dozen recognized subspecies of bacillus thuringiensis. Subspecies commonly used as insecticides include \"Bacillus thuringiensis\" subspecies \"kurstaki\" (Btk) and subspecies \"israelensis\" (Bti) and subspecies \"aizawa\" (Bta).\n\nUpon sporulation, \"B. thuringiensis\" forms crystals of proteinaceous insecticidal δ-endotoxins (called crystal proteins or Cry proteins), which are encoded by \"cry\" genes. In most strains of \"B. thuringiensis\", the \"cry\" genes are located on a plasmid (\"cry\" is not a chromosomal gene in most strains).\n\nCry toxins have specific activities against insect species of the orders Lepidoptera (moths and butterflies), Diptera (flies and mosquitoes), Coleoptera (beetles), Hymenoptera (wasps, bees, ants and sawflies) and nematodes. Thus, \"B. thuringiensis\" serves as an important reservoir of Cry toxins for production of biological insecticides and insect-resistant genetically modified crops. When insects ingest toxin crystals, their alkaline digestive tracts denature the insoluble crystals, making them soluble and thus amenable to being cut with proteases found in the insect gut, which liberate the toxin from the crystal. The Cry toxin is then inserted into the insect gut cell membrane, paralyzing the digestive tract and forming a pore. The insect stops eating and starves to death; live Bt bacteria may also colonize the insect which can contribute to death. The midgut bacteria of susceptible larvae may be required for \"B. thuringiensis\" insecticidal activity.\n\nIn 1996 another class of insecticidal proteins in Bt was discovered; the vegetative insecticidal proteins (Vip). Vip proteins do not share sequence homology with Cry proteins, in general do not compete for the same receptors, and some kill different insects than do Cry proteins.\n\nIn 2000, a novel functional group of Cry protein, designated parasporin, was discovered from noninsecticidal \"B. thuringiensis\" isolates. The proteins of parasporin group are defined as \"B. thuringiensis\" and related bacterial parasporal proteins that are not hemolytic, but capable of preferentially killing cancer cells. As of January 2013, parasporins comprise six subfamilies (PS1 to PS6).\n\nSpores and crystalline insecticidal proteins produced by \"B. thuringiensis\" have been used to control insect pests since the 1920s and are often applied as liquid sprays. They are now used as specific insecticides under trade names such as DiPel and Thuricide. Because of their specificity, these pesticides are regarded as environmentally friendly, with little or no effect on humans, wildlife, pollinators, and most other beneficial insects, and are used in organic farming; however, the manuals for these products do contain many environmental and human health warnings, and a 2012 European regulatory peer review of five approved strains found, while data exist to support some claims of low toxicity to humans and the environment, the data are insufficient to justify many of these claims.\n\nNew strains of Bt are developed and introduced over time as insects develop resistance to Bt, or the desire occurs to force mutations to modify organism characteristics or to use homologous recombinant genetic engineering to improve crystal size and increase pesticidal activity or broaden the host range of Bt and obtain more effective formulations. Each new strain is given a unique number and registered with the U.S. EPA and allowances may be given for genetic modification depending on \"its parental strains, the proposed pesticide use pattern, and the manner and extent to which the organism has been genetically modified\". Formulations of Bt that are approved for organic farming in the US are listed at the website of the Organic Materials Review Institute (OMRI) and several university extension websites offer advice on how to use Bt spore or protein preparations in organic farming.\n\nThe Belgian company Plant Genetic Systems (now part of Bayer CropScience) was the first company (in 1985) to develop genetically modified crops (tobacco) with insect tolerance by expressing \"cry\" genes from \"B. thuringiensis\"; the resulting crops contain delta endotoxin. The Bt tobacco was never commercialized; tobacco plants are used to test genetic modifications since they are easy to manipulate genetically and are not part of the food supply.\nIn 1995, potato plants producing CRY 3A Bt toxin were approved safe by the Environmental Protection Agency, making it the first human-modified pesticide-producing crop to be approved in the USA, though many plants produce pesticides naturally, including tobacco, coffee plants, cocoa, and black walnut. This was the 'New Leaf' potato, and it was removed from the market in 2001 due to lack of interest. For current crops and their acreage under cultivation, see genetically modified crops.\n\nIn 1996, genetically modified maize producing Bt Cry protein was approved, which killed the European corn borer and related species; subsequent Bt genes were introduced that killed corn rootworm larvae.\n\nThe Bt genes engineered into crops and approved for release include, singly and stacked: Cry1A.105, CryIAb, CryIF, Cry2Ab, Cry3Bb1, Cry34Ab1, Cry35Ab1, mCry3A, and VIP, and the engineered crops include corn and cotton.\n\nCorn genetically modified to produce VIP was first approved in the US in 2010.\n\nIn India, by 2014, more than seven million cotton farmers, occupying twenty-six million acres, had adopted Bt cotton.\n\nMonsanto developed a soybean expressing Cry1Ac and the glyphosate-resistance gene for the Brazilian market, which completed the Brazilian regulatory process in 2010.\n\nThe use of Bt toxins as plant-incorporated protectants prompted the need for extensive evaluation of their safety for use in foods and potential unintended impacts on the environment.\n\nConcerns over the safety of consumption of genetically-modified plant materials that contain Cry proteins have been addressed in extensive dietary risk assessment studies. While the target pests are exposed to the toxins primarily through leaf and stalk material, Cry proteins are also expressed in other parts of the plant, including trace amounts in maize kernels which are ultimately consumed by both humans and animals.\n\nAnimal models have been used to assess human health risk from consumption of products containing Cry proteins. The United States Environmental Protection Agency recognizes mouse acute oral feeding studies where doses as high as 5,000 mg/kg body weight resulted in no observed adverse effects. Research on other known toxic proteins suggests that toxicity occurs at much lower doses, further suggesting that Bt toxins are not toxic to mammals. The results of toxicology studies are further strengthened by the lack of observed toxicity from decades of use of \"B. thuringiensis\" and its crystalline proteins as an insecticidal spray.\n\nIntroduction of a new protein raised concerns regarding the potential for allergic responses in sensitive individuals. Bioinformatic analysis of known allergens has indicated there is no concern of allergic reactions as a result of consumption of Bt toxins. Additionally, skin prick testing using purified Bt protein resulted in no detectable production of toxin-specific IgE antibodies, even in atopic patients.\n\nStudies have been conducted to evaluate the fate of Bt toxins that are ingested in foods. Bt toxin proteins have been shown to digest within minutes of exposure to simulated gastric fluids. The instability of the proteins in digestive fluids is an additional indication that Cry proteins are unlikely to be allergenic, since most known food allergens resist degradation and are ultimately absorbed in the small intestine.\n\nEcological risk assessment aims to ensure there is no unintended impact on non-target organisms and no contamination of natural resources as a result of the use of a new substance, such as the use of Bt in genetically-modified crops. The impact of Bt toxins on the environments where transgenic plants are grown has been evaluated to ensure no adverse effects outside of targeted crop pests.\n\nConcerns over possible environmental impact from accumulation of Bt toxins from plant tissues, pollen dispersal, and direct secretion from roots have been investigated. Bt toxins may persist in soil for over 200 days, with half-lives between 1.6 and 22 days. Much of the toxin is initially degraded rapidly by microorganisms in the environment, while some is adsorbed by organic matter and persists longer. Some studies, in contrast, claim that the toxins do not persist in the soil. Bt toxins are less likely to accumulate in bodies of water, but pollen shed or soil runoff may deposit them in an aquatic ecosystem. Fish species are not susceptible to Bt toxins if exposed.\n\nThe toxic nature of Bt proteins has an adverse impact on many major crop pests, but ecological risk assessments have been conducted to ensure safety of beneficial non-target organisms that may come into contact with the toxins. Widespread concerns over toxicity in non-target lepidopterans, such as the monarch butterfly, have been disproved through proper exposure characterization, where it was determined that non-target organisms are not exposed to high enough amounts of the Bt toxins to have an adverse effect on the population. Soil-dwelling organisms, potentially exposed to Bt toxins through root exudates, are not impacted by the growth of Bt crops.\n\nIn November 2009, Monsanto scientists found the pink bollworm had become resistant to the first-generation Bt cotton in parts of Gujarat, India - that generation expresses one Bt gene, \"Cry1Ac\". This was the first instance of Bt resistance confirmed by Monsanto anywhere in the world. Monsanto immediately responded by introducing a second-generation cotton with multiple Bt proteins, which was rapidly adopted. Bollworm resistance to first-generation Bt cotton was also identified in Australia, China, Spain, and the United States.\n\nSeveral studies have documented surges in \"sucking pests\" (which are not affected by Bt toxins) within a few years of adoption of Bt cotton. In China, the main problem has been with mirids, which have in some cases \"completely eroded all benefits from Bt cotton cultivation\". The increase in sucking pests depended on local temperature and rainfall conditions and increased in half the villages studied. The increase in insecticide use for the control of these secondary insects was far smaller than the reduction in total insecticide use due to Bt cotton adoption. Another study in five provinces in China found the reduction in pesticide use in Bt cotton cultivars is significantly lower than that reported in research elsewhere, consistent with the hypothesis suggested by recent studies that more pesticide sprayings are needed over time to control emerging secondary pests, such as aphids, spider mites, and lygus bugs.\n\nSimilar problems have been reported in India, with both mealy bugs and aphids although a survey of small Indian farms between 2002 and 2008 concluded Bt cotton adoption has led to higher yields and lower pesticide use, decreasing over time.\n\nThere are controversies around GMOs on several levels, including whether making them is ethical, whether food produced with them is safe, whether such food should be labeled and if so how, whether agricultural biotech is needed to address world hunger now or in the future, and more specifically to GM crops—intellectual property and market dynamics; environmental effects of GM crops; and GM crops' role in industrial agricultural more generally. There are also issues specific to Bt transgenic crops.\n\nThe most publicised problem associated with Bt crops is the claim that pollen from Bt maize could kill the monarch butterfly. The paper produced a public uproar and demonstrations against Bt maize; however by 2001 several follow-up studies coordinated by the USDA had asserted that \"the most common types of Bt maize pollen are not toxic to monarch larvae in concentrations the insects would encounter in the fields.\"\n\nA study published in \"Nature\" in 2001 reported Bt-containing maize genes were found in maize in its center of origin, Oaxaca, Mexico. In 2002, paper concluded, \"the evidence available is not sufficient to justify the publication of the original paper.\" A significant controversy happened over the paper and \"Nature\"s unprecedented notice.\n\nA subsequent large-scale study, in 2005, failed to find any evidence of genetic mixing in Oaxaca. A 2007 study found the \"transgenic proteins expressed in maize were found in two (0.96%) of 208 samples from farmers' fields, located in two (8%) of 25 sampled communities.\" Mexico imports a substantial amount of maize from the US, and due to formal and informal seed networks among rural farmers, many potential routes are available for transgenic maize to enter into food and feed webs. One study found small-scale (about 1%) introduction of transgenic sequences in sampled fields in Mexico; it did not find evidence for or against this introduced genetic material being inherited by the next generation of plants. That study was immediately criticized, with the reviewer writing, \"Genetically, any given plant should be either nontransgenic or transgenic, therefore for leaf tissue of a single transgenic plant, a GMO level close to 100% is expected. In their study, the authors chose to classify leaf samples as transgenic despite GMO levels of about 0.1%. We contend that results such as these are incorrectly interpreted as positive and are more likely to be indicative of contamination in the laboratory.\"\n\nAs of 2007, a new phenomenon called colony collapse disorder (CCD) began affecting bee hives all over North America. Initial speculation on possible causes included new parasites, pesticide use, and the use of Bt transgenic crops. The Mid-Atlantic Apiculture Research and Extension Consortium found no evidence that pollen from Bt crops is adversely affecting bees. According to the USDA, \"Genetically modified (GM) crops, most commonly Bt corn, have been offered up as the cause of CCD. But there is no correlation between where GM crops are planted and the pattern of CCD incidents. Also, GM crops have been widely planted since the late 1990s, but CCD did not appear until 2006. In addition, CCD has been reported in countries that do not allow GM crops to be planted, such as Switzerland. German researchers have noted in one study a possible correlation between exposure to Bt pollen and compromised immunity to Nosema.\" The actual cause of CCD was unknown in 2007, and scientists believe it may have multiple exacerbating causes.\n\nSome isolates of \"B. thuringiensis\" produce a class of insecticidal small molecules called beta-exotoxin, the common name for which is thuringiensin. A consensus document produced by the OECD says: \"Beta-exotoxin and the other \"Bacillus\" toxins may contribute to the insecticidal toxicity of the bacterium to lepidopteran, dipteran, and coleopteran insects. Beta-exotoxin is known to be toxic to humans and almost all other forms of life and its presence is prohibited in\" B. thuringiensis\" microbial products. Engineering of plants to contain and express only the genes for δ-endotoxins avoids the problem of assessing the risks posed by these other toxins that may be produced in microbial preparations.\"\n\n\n"}
{"id": "4185", "url": "https://en.wikipedia.org/wiki?curid=4185", "title": "Bacteriophage", "text": "Bacteriophage\n\nA bacteriophage , also known informally as a phage , is a virus that infects and replicates within a bacterium. The term is derived from \"bacteria\" and the (\"phagein\"), \"to devour\". Bacteriophages are composed of proteins that encapsulate a DNA or RNA genome, and may have relatively simple or elaborate structures. Their genomes may encode as few as four genes, and as many as hundreds of genes. Phages replicate within the bacterium following the injection of their genome into its cytoplasm. Bacteriophages are among the most common and diverse entities in the biosphere. Bacteriophages are ubiquitous viruses, found wherever bacteria exist. It’s estimated there are more than 10 bacteriophages on the planet, more than every other organism on Earth, including bacteria, combined.\n\nPhages are widely distributed in locations populated by bacterial hosts, such as soil or the intestines of animals. One of the densest natural sources for phages and other viruses is sea water, where up to 9×10 virions per milliliter have been found in microbial mats at the surface, and up to 70% of marine bacteria may be infected by phages.\nThey have been used for over 90 years as an alternative to antibiotics in the former Soviet Union and Central Europe, as well as in France. They are seen as a possible therapy against multi-drug-resistant strains of many bacteria (see phage therapy). Nevertheless, phages of Inoviridae have been shown to complicate biofilms involved in pneumonia and cystic fibrosis, and shelter the bacteria from drugs meant to eradicate disease and promote persistent infection.\n\nBacteriophages occur abundantly in the biosphere, with different virions, genomes, and lifestyles. Phages are classified by the International Committee on Taxonomy of Viruses (ICTV) according to morphology and nucleic acid.\n\nNineteen families are currently recognized by the ICTV that infect bacteria and archaea. Of these, only two families have RNA genomes, and only five families are enveloped. Of the viral families with DNA genomes, only two have single-stranded genomes. Eight of the viral families with DNA genomes have circular genomes while nine have linear genomes. Nine families infect bacteria only, nine infect archaea only, and one (\"Tectiviridae\") infects both bacteria and archaea.\n\nIn 1896, Ernest Hanbury Hankin reported that something in the waters of the Ganges and Yamuna rivers in India had marked antibacterial action against cholera and could pass through a very fine porcelain filter. In 1915, British bacteriologist Frederick Twort, superintendent of the Brown Institution of London, discovered a small agent that infected and killed bacteria. He believed the agent must be one of the following:\n\nTwort's work was interrupted by the onset of World War I and shortage of funding. Independently, French-Canadian microbiologist Félix d'Hérelle, working at the Pasteur Institute in Paris, announced on 3 September 1917, that he had discovered \"an invisible, antagonistic microbe of the dysentery bacillus\". For d’Hérelle, there was no question as to the nature of his discovery: \"In a flash I had understood: what caused my clear spots was in fact an invisible microbe … a virus parasitic on bacteria.\" D'Hérelle called the virus a bacteriophage or bacteria-eater (from the Greek \"phagein\" meaning to eat). He also recorded a dramatic account of a man suffering from dysentery who was restored to good health by the bacteriophages. It was D'Herelle who conducted much research into bacteriophages and introduced the concept of phage therapy.\n\nIn 1969, Max Delbrück, Alfred Hershey and Salvador Luria were awarded the Nobel Prize in Physiology or Medicine for their discoveries of the replication of viruses and their genetic structure.\n\nPhages were discovered to be antibacterial agents and were used in the former Soviet Republic of Georgia (pioneered there by Giorgi Eliava with help from the co-discoverer of bacteriophages, Felix d'Herelle) and the United States during the 1920s and 1930s for treating bacterial infections. They had widespread use, including treatment of soldiers in the Red Army. However, they were abandoned for general use in the West for several reasons:\n\nTheir use has continued since the end of the Cold War in Georgia and elsewhere in Central and Eastern Europe. The first regulated, randomized, double-blind clinical trial was reported in the Journal of Wound Care in June 2009, which evaluated the safety and efficacy of a bacteriophage cocktail to treat infected venous leg ulcers in human patients. The FDA approved the study as a Phase I clinical trial. The study's results demonstrated the safety of therapeutic application of bacteriophages but did not show efficacy. The authors explain that the use of certain chemicals that are part of standard wound care (e.g. lactoferrin or silver) may have interfered with bacteriophage viability. Another controlled clinical trial in Western Europe (treatment of ear infections caused by \"Pseudomonas aeruginosa\") was reported shortly after in the journal Clinical Otolaryngology in August 2009. The study concludes that bacteriophage preparations were safe and effective for treatment of chronic ear infections in humans. Additionally, there have been numerous animal and other experimental clinical trials evaluating the efficacy of bacteriophages for various diseases, such as infected burns and wounds, and cystic fibrosis associated lung infections, among others. Meanwhile, bacteriophage researchers are developing engineered viruses to overcome antibiotic resistance, and engineering the phage genes responsible for coding enzymes which degrade the biofilm matrix, phage structural proteins and also enzymes responsible for lysis of bacterial cell wall.\n\nD'Herelle \"quickly learned that bacteriophages are found wherever bacteria thrive: in sewers, in rivers that catch waste runoff from pipes, and in the stools of convalescent patients.\" This includes rivers traditionally thought to have healing powers, including India's Ganges River.\n\nBacteriophages may have a lytic cycle or a lysogenic cycle, and a few viruses are capable of carrying out both. With \"lytic phages\" such as the T4 phage, bacterial cells are broken open (lysed) and destroyed after immediate replication of the virion. As soon as the cell is destroyed, the phage progeny can find new hosts to infect. Lytic phages are more suitable for phage therapy. Some lytic phages undergo a phenomenon known as lysis inhibition, where completed phage progeny will not immediately lyse out of the cell if extracellular phage concentrations are high. This mechanism is not identical to that of temperate phage going dormant and is usually temporary.\n\nIn contrast, the \"lysogenic cycle\" does not result in immediate lysing of the host cell. Those phages able to undergo lysogeny are known as temperate phages. Their viral genome will integrate with host DNA and replicate along with it relatively harmlessly, or may even become established as a plasmid. The virus remains dormant until host conditions deteriorate, perhaps due to depletion of nutrients; then, the endogenous phages (known as prophages) become active. At this point they initiate the reproductive cycle, resulting in lysis of the host cell. As the lysogenic cycle allows the host cell to continue to survive and reproduce, the virus is replicated in all of the cell’s offspring.\nAn example of a bacteriophage known to follow the lysogenic cycle and the lytic cycle is the phage lambda of \"E. coli.\"\n\nSometimes prophages may provide benefits to the host bacterium while they are dormant by adding new functions to the bacterial genome in a phenomenon called lysogenic conversion. Examples are the conversion of harmless strains of \"Corynebacterium diphtheriae\" or \"Vibrio cholerae\" by bacteriophages to highly virulent ones, which cause diphtheria or cholera, respectively. Strategies to combat certain bacterial infections by targeting these toxin-encoding prophages have been proposed.\n\nTo enter a host cell, bacteriophages attach to specific receptors on the surface of bacteria, including lipopolysaccharides, teichoic acids, proteins, or even flagella. This specificity means a bacteriophage can infect only certain bacteria bearing receptors to which they can bind, which in turn determines the phage's host range. Host growth conditions also influence the ability of the phage to attach and invade them. As phage virions do not move independently, they must rely on random encounters with the right receptors when in solution (blood, lymphatic circulation, irrigation, soil water, etc.).\n\nMyovirus bacteriophages use a hypodermic syringe-like motion to inject their genetic material into the cell. After making contact with the appropriate receptor, the tail fibers flex to bring the base plate closer to the surface of the cell; this is known as reversible binding. Once attached completely, irreversible binding is initiated and the tail contracts, possibly with the help of ATP present in the tail, injecting genetic material through the bacterial membrane.\nPodoviruses lack an elongated tail sheath similar to that of a myovirus, so they instead use their small, tooth-like tail fibers enzymatically to degrade a portion of the cell membrane before inserting their genetic material.\n\nWithin minutes, bacterial ribosomes start translating viral mRNA into protein. For RNA-based phages, RNA replicase is synthesized early in the process. Proteins modify the bacterial RNA polymerase so it preferentially transcribes viral mRNA. The host’s normal synthesis of proteins and nucleic acids is disrupted, and it is forced to manufacture viral products instead. These products go on to become part of new virions within the cell, helper proteins that help assemble the new virions, or proteins involved in cell lysis. Walter Fiers (University of Ghent, Belgium) was the first to establish the complete nucleotide sequence of a gene (1972) and of the viral genome of bacteriophage MS2 (1976).\n\nIn the case of the T4 phage, the construction of new virus particles involves the assistance of helper proteins. The base plates are assembled first, with the tails being built upon them afterward. The head capsids, constructed separately, will spontaneously assemble with the tails. The DNA is packed efficiently within the heads. The whole process takes about 15 minutes.\nPhages may be released via cell lysis, by extrusion, or, in a few cases, by budding. Lysis, by tailed phages, is achieved by an enzyme called endolysin, which attacks and breaks down the cell wall peptidoglycan. An altogether different phage type, the filamentous phages, make the host cell continually secrete new virus particles. Released virions are described as free, and, unless defective, are capable of infecting a new bacterium. Budding is associated with certain \"Mycoplasma\" phages. In contrast to virion release, phages displaying a lysogenic cycle do not kill the host but, rather, become long-term residents as prophage.\n\nGiven the millions of different phages in the environment, phages' genomes come in a variety of forms and sizes. RNA phage such as MS2 have the smallest genomes of only a few kilobases. However, some DNA phages such as T4 may have large genomes with hundreds of genes; the size and shape of the capsid varies along with the size of the genome.\n\nBacteriophage genomes can be highly mosaic, i.e. the genome of many phage species appear to be composed of numerous individual modules. These modules may be found in other phage species in different arrangements. Mycobacteriophages – bacteriophages with mycobacterial hosts – have provided excellent examples of this mosaicism. In these mycobacteriophages, genetic assortment may be the result of repeated instances of site-specific recombination and illegitimate recombination (the result of phage genome acquisition of bacterial host genetic sequences). It should be noted, however, that evolutionary mechanisms shaping the genomes of bacterial viruses vary between different families and depend on the type of the nucleic acid, characteristics of the virion structure, as well as the mode of the viral life cycle.\n\nPhages often have dramatic effects on their hosts. As a consequence, the transcription pattern of the infected bacterium may change considerably. For instance, infection of \"Pseudomonas aeruginosa\" by the temperate phage PaP3 changed the expression of 38% (2160/5633) of its host's genes. Many of these effects are probably indirect, hence the challenge becomes to identify the direct interactions among bacteria and phage.\n\nSeveral attempts have been made to map Protein–protein interactions among phage and their host. For instance, bacteriophage lambda was found to interact with its host E. coli by 31 interactions. However, a large-scale study revealed 62 interactions, most of which were new. Again, the significance of many of these interactions remains unclear, but these studies suggest that there are most likely several key interactions and many indirect interactions whose role remains uncharacterized.\n\nMetagenomics has allowed the in-water detection of bacteriophages that was not possible previously.\n\nBacteriophages have also been used in hydrological tracing and modelling in river systems, especially where surface water and groundwater interactions occur. The use of phages is preferred to the more conventional dye marker because they are significantly less absorbed when passing through ground waters and they are readily detected at very low concentrations. Non-polluted water may contain ca. 2×10 bacteriophages per mL.\n\nBacteriophages are thought to extensively contribute to horizontal gene transfer in natural environments, principally via transduction but also via transformation. Metagenomics-based studies have also revealed that viromes from a variety of environments harbor antibiotic resistance genes, including those that could confer multidrug resistance.\n\nSince 2006, the United States Food and Drug Administration (FDA) and United States Department of Agriculture (USDA) have approved several bacteriophage products. LMP-102 (Intralytix) was approved for treating ready-to-eat (RTE) poultry and meat products. In that same year, the FDA approved LISTEX (developed and produced by Micreos) using bacteriophages on cheese to kill \"Listeria monocytogenes\" bacteria, giving them generally recognized as safe (GRAS) status. In July 2007, the same bacteriophage were approved for use on all food products. In 2011 USDA confirmed that LISTEX is a clean label processing aid and is included in USDA. Research in the field of food safety is continuing to see if lytic phages are a viable option to control other food-borne pathogens in various food products.\n\nIn 2011 the FDA cleared the first bacteriophage-based product for in vitro diagnostic use. The KeyPath MRSA/MSSA Blood Culture Test uses a cocktail of bacteriophage to detect \"Staphylococcus aureus\" in positive blood cultures and determine methicillin resistance or susceptibility. The test returns results in about 5 hours, compared to 2–3 days for standard microbial identification and susceptibility test methods. It was the first accelerated antibiotic susceptibility test approved by the FDA.\n\nGovernment agencies in the West have for several years been looking to Georgia and the former Soviet Union for help with exploiting phages for counteracting bioweapons and toxins, such as anthrax and botulism. Developments are continuing among research groups in the US. Other uses include spray application in horticulture for protecting plants and vegetable produce from decay and the spread of bacterial disease. Other applications for bacteriophages are as biocides for environmental surfaces, e.g., in hospitals, and as preventative treatments for catheters and medical devices before use in clinical settings. The technology for phages to be applied to dry surfaces, e.g., uniforms, curtains, or even sutures for surgery now exists. Clinical trials reported in \"Clinical Otolaryngology\" show success in veterinary treatment of pet dogs with otitis.\n\nPhage display is a different use of phages involving a library of phages with a variable peptide linked to a surface protein. Each phage's genome encodes the variant of the protein displayed on its surface (hence the name), providing a link between the peptide variant and its encoding gene. Variant phages from the library can be selected through their binding affinity to an immobilized molecule (e.g., botulism toxin) to neutralize it. The bound, selected phages can be multiplied by reinfecting a susceptible bacterial strain, thus allowing them to retrieve the peptides encoded in them for further study.\n\nThe SEPTIC bacterium sensing and identification method uses the ion emission and its dynamics during phage infection and offers high specificity and speed for detection.\n\nPhage-ligand technology makes use of proteins, which are identified from bacteriophages, characterized and recombinantly expressed for various applications such as binding of bacteria and bacterial components (e.g. endotoxin) and lysis of bacteria.\n\nBacteriophages are also important model organisms for studying principles of evolution and ecology.\n\nThe following bacteriophages are extensively studied:\n\n\n"}
{"id": "4187", "url": "https://en.wikipedia.org/wiki?curid=4187", "title": "Bactericide", "text": "Bactericide\n\nA bactericide or bacteriocide, sometimes abbreviated Bcidal, is a substance that kills bacteria. Bactericides are disinfectants, antiseptics, or antibiotics.\n\nThe most used disinfectants are those applying\n\nAs antiseptics (i.e., germicide agents that can be used on human or animal body, skin, mucoses, wounds and the like), few of the above-mentioned disinfectants can be used, under proper conditions (mainly concentration, pH, temperature and toxicity toward humans and animals). Among them, some important are\nOthers are generally not applicable as safe antiseptics, either because of their corrosive or toxic nature.\n\nBactericidal antibiotics kill bacteria; bacteriostatic antibiotics slow their growth or reproduction.\n\nBactericidal antibiotics that inhibit cell wall synthesis: the Beta-lactam antibiotics (penicillin derivatives (penams), cephalosporins (cephems), monobactams, and carbapenems) and vancomycin.\n\nAlso bactericidal are daptomycin, fluoroquinolones, metronidazole, nitrofurantoin, co-trimoxazole, telithromycin.\n\nAminoglycosidic antibiotics are usually considered bactericidal, although they may be bacteriostatic with some organisms\n\nThe distinction between bactericidal and bacteriostatic agents appears to be clear according to the basic/clinical definition, but this only applies under strict laboratory conditions and it is important to distinguish microbiological and clinical definitions. The distinction is more arbitrary when agents are categorized in clinical situations. The supposed superiority of bactericidal agents over bacteriostatic agents is of little relevance when treating the vast majority of infections with gram-positive bacteria, particularly in patients with uncomplicated infections and noncompromised immune systems. Bacteriostatic agents have been effectively used for treatment that are considered to require bactericidal activity. Furthermore, some broad classes of antibacterial agents considered bacteriostatic can exhibit bactericidal activity against some bacteria on the basis of in vitro determination of MBC/MIC values. At high concentrations, bacteriostatic agents are often bactericidal against some susceptible organisms. The ultimate guide to treatment of any infection must be clinical outcome.\n\n"}
{"id": "4188", "url": "https://en.wikipedia.org/wiki?curid=4188", "title": "Brion Gysin", "text": "Brion Gysin\n\nBrion Gysin (19 January 1916 – 13 July 1986) was a painter, writer, sound poet, and performance artist born in Taplow, Buckinghamshire.\n\nHe is best known for his discovery of the cut-up technique, used by his friend, the novelist William S. Burroughs. With the engineer Ian Sommerville he invented the Dreamachine, a flicker device designed as an art object to be viewed with the eyes closed. It was in painting and drawing, however, that Gysin devoted his greatest efforts, creating calligraphic works inspired by the cursive Japanese \"grass\" script and Arabic script. Burroughs later stated that \"Brion Gysin was the only man I ever respected.\"\n\nJohn Clifford Brian Gysin was born at the Canadian military hospital in the grounds of Cliveden, Taplow, England. His mother, Stella Margaret Martin, was a Canadian from Deseronto, Ontario. His father, Leonard Gysin, a captain with the Canadian Expeditionary Force, was killed in action eight months after his son's birth. Stella returned to Canada and settled in Edmonton, Alberta where her son became \"the only Catholic day-boy at an Anglican boarding school\". Graduating at fifteen, Gysin was sent to Downside School in Stratton-on-the-Fosse, near Bath, Somerset in England, a prestigious college run by the Benedictines and known as \"the Eton of Catholic public schools\". Despite, or because of, attending a Catholic school, Gysin became an atheist.\n\nIn 1934, he moved to Paris to study \"La Civilisation Française\", an open course given at the Sorbonne where he made literary and artistic contacts through Marie Berthe Aurenche, Max Ernst's second wife. He joined the Surrealist Group and began frequenting Valentine Hugo, Leonor Fini, Salvador Dalí, Picasso and Dora Maar. A year later, he had his first exhibition at the \"Galerie Quatre Chemins\" in Paris with Ernst, Picasso, Hans Arp, Hans Bellmer, Victor Brauner, Giorgio de Chirico, Dalí, Marcel Duchamp, René Magritte, Man Ray and Yves Tanguy. On the day of the preview, however, he was expelled from the Surrealist Group by André Breton, who ordered the poet Paul Éluard to take down his pictures. Gysin was 19 years old. His biographer, John Geiger, suggests the arbitrary expulsion \"had the effect of a curse. Years later, he blamed other failures on the Breton incident. It gave rise to conspiracy theories about the powerful interests who seek control of the art world. He gave various explanations for the expulsion, the more elaborate involving 'insubordination' or \"lèse majesté\" towards Breton\".\n\nAfter serving in the U.S. army during World War II, Gysin published a biography of Josiah \"Uncle Tom\" Henson titled, \"To Master, a Long Goodnight: The History of Slavery in Canada\" (1946). A gifted draughtsman, he took an 18-month course learning the Japanese language (including calligraphy) that would greatly influence his artwork. In 1949, he was among the first Fulbright Fellows. His goal: to research the history of slavery at the University of Bordeaux and in the Archivo de Indias in Seville, Spain, a project that he later abandoned. He moved to Tangier, Morocco after visiting the city with novelist and composer Paul Bowles in 1950. In 1952/3 he met the travel writer and sexual adventurer Anne Cumming and they remained friends until his death.\n\nIn 1954 in Tangier, Gysin opened a restaurant called The 1001 Nights, with his friend Mohamed Hamri, who was the cook. Gysin hired the Master Musicians of Jajouka from the village of Jajouka to perform alongside entertainment that included acrobats, a dancing boy and fire eaters. The musicians performed there for an international clientele that included William S. Burroughs. Gysin lost the business in 1958, and the restaurant closed permanently. That same year, Gysin returned to Paris, taking lodgings in a flophouse located at 9 rue Gît-le-Coeur that would become famous as the Beat Hotel. Working on a drawing, he discovered a Dada technique by accident:\nWilliam Burroughs and I first went into techniques of writing, together, back in room No. 15 of the Beat Hotel during the cold Paris spring of 1958... Burroughs was more intent on Scotch-taping his photos together into one great continuum on the wall, where scenes faded and slipped into one another, than occupied with editing the monster manuscript... \"Naked Lunch\" appeared and Burroughs disappeared. He kicked his habit with Apomorphine and flew off to London to see Dr Dent, who had first turned him on to the cure. While cutting a mount for a drawing in room No. 15, I sliced through a pile of newspapers with my Stanley blade and thought of what I had said to Burroughs some six months earlier about the necessity for turning painters' techniques directly into writing. I picked up the raw words and began to piece together texts that later appeared as \"First Cut-Ups\" in \"Minutes to Go\" (Two Cities, Paris 1960).\n\nWhen Burroughs returned from London in September 1959, Gysin not only shared his discovery with his friend but the new techniques he had developed for it. Burroughs then put the techniques to use while completing \"Naked Lunch\" and the experiment dramatically changed the landscape of American literature. Gysin helped Burroughs with the editing of several of his novels including \"Interzone\", and wrote a script for a film version of \"Naked Lunch\", which was never produced. The pair collaborated on a large manuscript for Grove Press titled \"The Third Mind\" but it was determined that it would be impractical to publish it as originally envisioned. The book later published under that title incorporates little of this material. Interviewed for \"The Guardian\" in 1997, Burroughs explained that Gysin was \"the only man that I've ever respected in my life. I've admired people, I've liked them, but he's the only man I've ever respected.\" In 1969, Gysin completed his finest novel, \"The Process\", a work judged by critic Robert Palmer as \"a classic of 20th century modernism\".\n\nA consummate innovator, Gysin altered the cut-up technique to produce what he called permutation poems in which a single phrase was repeated several times with the words rearranged in a different order with each reiteration. An example of this is \"I don't dig work, man/Man, work I don't dig.\" Many of these permutations were derived using a random sequence generator in an early computer program written by Ian Sommerville. Commissioned by the BBC in 1960 to produce material for broadcast, Gysin's results included \"Pistol Poem\", which was created by recording a gun firing at different distances and then splicing the sounds. That year, the piece was subsequently used as a theme for the Paris performance of Le Domaine Poetique, a showcase for experimental works by people like Gysin, François Dufrêne, Bernard Heidsieck, and Henri Chopin.\n\nWith Sommerville, he built the Dreamachine in 1961. Described as \"the first art object to be seen with the eyes closed\", the flicker device uses alpha waves in the 8-16 Hz range to produce a change of consciousness in receptive viewers.\n\nIn 1985 Gysin was made an American Commander of the French Ordre des Arts et des Lettres. He'd begun to work extensively with noted jazz soprano saxophonist Steve Lacy. They recorded an album in 1986 with French musician Ramuntcho Matta, featuring Gysin singing/rapping his own texts, with performances by Lacy, Don Cherry, Elli Medeiros, Lizzy Mercier Descloux and more. The album was reissued on CD in 1993 by Crammed Discs, under the title \"Self-Portrait Jumping\".\n\nOn July 13, 1986 Brion Gysin died of lung cancer. Anne Cumming arranged his funeral and for his ashes to be scattered at the Caves of Hercules in Morocco. An obituary by Robert Palmer published in \"The New York Times\" described him as a man who \"threw off the sort of ideas that ordinary artists would parlay into a lifetime career, great clumps of ideas, as casually as a locomotive throws off sparks\". Later that year a heavily edited version of his novel, \"The Last Museum\", was published posthumously by Faber & Faber (London) and by Grove Press (New York).\n\nAs a joke, Gysin had contributed a recipe for marijuana fudge to a cookbook by Alice B. Toklas; it was unintentionally included for publication, becoming famous under the name Alice B. Toklas brownies.\n\nIn a 1966 interview by Conrad Knickerbocker for The Paris Review, William S. Burroughs explained that Brion Gysin was, to his knowledge, \"the first to create cut-ups\".\nINTERVIEWER: How did you become interested in the cut-up technique?\n\nBURROUGHS: A friend, Brion Gysin, an American poet and painter, who has lived in Europe for thirty years, was, as far as I know, the first to create cut-ups. His cut-up poem, \"Minutes to Go\", was broadcast by the BBC and later published in a pamphlet. I was in Paris in the summer of 1960; this was after the publication there of \"Naked Lunch\". I became interested in the possibilities of this technique, and I began experimenting myself. Of course, when you think of it, \"The Waste Land\" was the first great cut-up collage, and Tristan Tzara had done a bit along the same lines. Dos Passos used the same idea in 'The Camera Eye' sequences in \"USA\". I felt I had been working toward the same goal; thus it was a major revelation to me when I actually saw it being done.\nAccording to José Férez Kuri, author of \"Brion Gysin: Tuning in to the Multimedia Age\" (2003) and co-curator of a major retrospective of the artist's work at The Edmonton Art Gallery in 1998, Gysin's wide range of \"radical ideas would become a source of inspiration for artists of the Beat Generation, as well as for their successors (among them David Bowie, Mick Jagger, Keith Haring, and Laurie Anderson)\". Other artists include Genesis P-Orridge, John Zorn (as displayed on the 2013's Dreamachines album) and Brian Jones.\n\nGysin is the subject of John Geiger's biography, \"Nothing Is True Everything Is Permitted: The Life of Brion Gysin\", and features in \"Chapel of Extreme Experience: A Short History of Stroboscopic Light and the Dream Machine\", also by Geiger. \"Man From Nowhere: Storming the Citadels of Enlightenment with William Burroughs and Brion Gysin\", a biographical study of Burroughs and Gysin with a collection of homages to Gysin, was authored by Joe Ambrose, Frank Rynne, and Terry Wilson with contributions by Marianne Faithfull, John Cale, William S. Burroughs, John Giorno, Stanley Booth, Bill Laswell, Mohamed Hamri, Keith Haring and Paul Bowles. A monograph on Gysin was published in 2003 by Thames and Hudson.\n\nProse\n\nRadio\n\nCinema\nMusic\nPainting\n\n\n"}
{"id": "4190", "url": "https://en.wikipedia.org/wiki?curid=4190", "title": "Bulgarian", "text": "Bulgarian\n\nBulgarian may refer to:\n\n\n"}
{"id": "4191", "url": "https://en.wikipedia.org/wiki?curid=4191", "title": "BCG vaccine", "text": "BCG vaccine\n\nBacillus Calmette–Guérin (BCG) vaccine is a vaccine primarily used against tuberculosis. In countries where tuberculosis is common, one dose is recommended in healthy babies as close to the time of birth as possible. Babies with HIV/AIDS should not be vaccinated. In areas where tuberculosis is not common, only children at high risk are typically immunized, while suspected cases of tuberculosis are individually tested for and treated. Adults who do not have tuberculosis and have not been previously immunized but are frequently exposed to drug-resistant tuberculosis may be immunized as well. The vaccine is also often used as part of the treatment of bladder cancer.\nRates of protection against tuberculosis infection vary widely and protection lasts between ten and twenty years. Among children it prevents about 20% from getting infected and among those who do get infected it protects half from developing disease. The vaccine is given by injection into the skin. Additional doses are not supported by evidence. It may also be used in the treatment of some types of bladder cancers.\nSerious side effects are rare. Often there is redness, swelling, and mild pain at the site of injection. A small ulcer may also form with some scarring after healing. Side effects are more common and potentially more severe in those with poor immune function. It is not safe for use during pregnancy. The vaccine was originally developed from \"Mycobacterium bovis\" which is commonly found in cows. While it has been weakened, it is still live.\nThe BCG vaccine was first used medically in 1921. It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. Between 2011 and 2014 the wholesale price was $0.16 to $1.11 USD a dose in the developing world. In the United States it costs $100 to $200 USD. As of 2004 the vaccine is given to about 100 million children per year globally.\n\nThe main use of BCG is for vaccination against tuberculosis. BCG vaccine can be administered after birth intradermally. BCG vaccination is recommended to be given intradermally. A previous BCG vaccination can cause a false positive Mantoux test, although a very high-grade reading is usually due to active disease.\n\nThe most controversial aspect of BCG is the variable efficacy found in different clinical trials, which appears to depend on geography. Trials conducted in the UK have consistently shown a protective effect of 60 to 80%, but those conducted elsewhere have shown no protective effect, and efficacy appears to fall the closer one gets to the equator.\n\nA 1994 systematic review found that BCG reduces the risk of getting TB by about 50%. There are differences in effectiveness, depending on region, due to factors such as genetic differences in the populations, changes in environment, exposure to other bacterial infections, and conditions in the lab where the vaccine is grown, including genetic differences between the strains being cultured and the choice of growth medium.\n\nA systematic review and meta analysis conducted in 2014 demonstrated that the BCG vaccine reduced infections by 19–27% and reduced progression to active TB by 71%. The studies included in this review were limited to those that used interferon gamma release assay.\n\nThe duration of protection of BCG is not clearly known. In those studies showing a protective effect, the data are inconsistent. The MRC study showed protection waned to 59% after 15 years and to zero after 20 years; however, a study looking at Native Americans immunized in the 1930s found evidence of protection even 60 years after immunization, with only a slight waning in efficacy.\n\nBCG seems to have its greatest effect in preventing miliary TB or TB meningitis, so it is still extensively used even in countries where efficacy against pulmonary tuberculosis is negligible.\n\nA number of possible reasons for the variable efficacy of BCG in different countries have been proposed. None have been proven, some have been disproved, and none can explain the lack of efficacy in both low-TB burden countries (US) and high-TB burden countries (India). The reasons for variable efficacy have been discussed at length in a WHO document on BCG.\n\n\nBCG has been one of the most successful immunotherapies. BCG vaccine has been the \"standard of care for patients with bladder cancer (NMIBC)\" since 1977. By 2014 there were more than eight different considered biosimilar agents or strains used for the treatment of non–muscle-invasive bladder cancer (NMIBC).\n\nExcept in neonates, a tuberculin skin test should always be done before administering BCG. A reactive tuberculin skin test is a contraindication to BCG. Someone with a positive tuberculin reaction is not given BCG, because the risk of severe local inflammation and scarring is high, not because of the common misconception that tuberculin reactors \"are already immune\" and therefore do not need BCG. People found to have reactive tuberculin skin tests should be screened for active tuberculosis. BCG is also contraindicated in certain people who have IL-12 receptor pathway defects.\n\nBCG is given as a single intradermal injection at the insertion of the deltoid. If BCG is accidentally given subcutaneously, then a local abscess may form (a \"BCG-oma\") that can sometimes ulcerate, and may require treatment with antibiotics immediately, otherwise without treatment it could spread the infection causing severe damage to vital organs. However, it is important to note an abscess is not always associated with incorrect administration, and it is one of the more common complications that can occur with the vaccination. Numerous medical studies on treatment of these abscesses with antibiotics have been done with varying results, but the consensus is once pus is aspirated and analysed, provided no unusual bacilli are present, the abscess will generally heal on its own in a matter of weeks.\n\nThe characteristic raised scar BCG immunization leaves is often used as proof of prior immunization. This scar must be distinguished from that of smallpox vaccination, which it may resemble.\n\nBCG immunization generally causes some pain and scarring at the site of injection. The main adverse effects are keloids—large, raised scars. The insertion of deltoid is most frequently used because the local complication rate is smallest when that site is used. Nonetheless, the buttock is an alternative site of administration because it provides better cosmetic outcomes.\n\nBCG vaccine should be given intradermally. If given subcutaneously, it may induce local infection and spread to the regional lymph nodes, causing either suppurative and nonsuppurative lymphadenitis. Conservative management is usually adequate for nonsuppurative lymphadenitis. If suppuration occurs, it may need needle aspiration. For nonresolving suppuration, surgical excision may be required. Evidence for the treatment of these complications is scarce.\n\nUncommonly, breast and gluteal abscesses can occur due to haematogenous and lymphangiomatous spread. Regional bone infection (BCG osteomyelitis or osteitis) and disseminated BCG infection are rare complications of BCG vaccination, but potentially life-threatening. Systemic antituberculous therapy may be helpful in severe complications.\n\nIf BCG is accidentally given to an immunocompromised patient (e.g., an infant with SCID), it can cause disseminated or life-threatening infection. The documented incidence of this happening is less than one per million immunizations given. In 2007, The WHO stopped recommending BCG for infants with HIV, even if there is a high risk of exposure to TB, because of the risk of disseminated BCG infection (which is approximately 400 per 100,000 in that higher risk context).\n\nThe age of the person and the frequency with which BCG is given has always varied from country to country.\n\n\nBCG is prepared from a strain of the attenuated (virulence-reduced) live bovine tuberculosis bacillus, \"Mycobacterium bovis\", that has lost its ability to cause disease in humans. Because the living bacilli evolve to make the best use of available nutrients, they become less well-adapted to human blood and can no longer induce disease when introduced into a human host. Still, they are similar enough to their wild ancestors to provide some degree of immunity against human tuberculosis. The BCG vaccine can be anywhere from 0 to 80% effective in preventing tuberculosis for a duration of 15 years; however, its protective effect appears to vary according to geography and the lab in which the vaccine strain was grown.\n\nA number of different companies make BCG, sometimes using different genetic strains of the bacterium. This may result in different product characteristics. OncoTICE, used for bladder instillation for bladder cancer, was developed by Organon Laboratories (since acquired by Schering-Plough, and in turn acquired by Merck, Inc.). Pacis BCG, made from the Montréal (Institut Armand-Frappier) strain, was first marketed by Urocor in about 2002. Urocor was since acquired by Dianon Systems. Evans Vaccines (a subsidiary of PowderJect Pharmaceuticals). Statens Serum Institut in Denmark markets BCG vaccine prepared using Danish strain 1331. Japan BCG Laboratory markets its vaccine, based on the Tokyo 172 substrain of Pasteur BCG, in 50 countries worldwide.\n\nAccording to a UNICEF report published in December 2015 on BCG vaccine supply security, global demand increased in 2015 from 123 to 152.2 million doses. In order to improve security and to [diversify] sources of affordable and flexible supply,\" UNICEF awarded seven new manufacturers contracts to produce BCG. Along with supply availability from existing manufacturers, and a \"new WHO prequalified vaccine\" the total supply will be \"sufficient to meet both suppressed 2015 demand carried over to 2016, as well as total forecast demand through 2016-2018.\"\n\nIn the fall of 2011 the Sanofi Pasteur plant flooded causing problems with mold. The facility, located in Toronto, Ontario, Canada, produced BCG vaccine products, made with substrain Connaught, such as a tuberculosis vaccine ImmuCYST, a BCG Immunotherapeutic – a bladder cancer drug. By April 2012 the FDA had found dozens of documented problems with sterility at the plant including mold, nesting birds and rusted electrical conduits. The resulting closure of the plant for over two years resulting in shortages of bladder cancer and tuberculosis vaccines. On October 29, 2014 Health Canada gave the permission for Sanofi to resume production of BCG.\n\nA weakened strain of bovine tuberculosis bacillus, \"Mycobacterium bovis\" is specially subcultured in a culture medium, usually Middlebrook 7H9.\n\nSome BCG vaccines are freeze dried and become fine powder. Sometimes the powder are sealed with vacuum in a glass ampoule. Such a glass ampoule has to be opened slowly to prevent the airflow from blowing out the powder. Then the powder has to be diluted with saline water before injecting.\n\nThe history of BCG is tied to that of smallpox. Jean Antoine Villemin first recognized bovine tuberculosis in 1854 and transmitted it, and Robert Koch first distinguished \"Mycobacterium bovis\" from \"Mycobacterium tuberculosis\". Following the success of vaccination in preventing smallpox, established during the 18th century, scientists thought to find a corollary in tuberculosis by drawing a parallel between bovine tuberculosis and cowpox: it was hypothesized that infection with bovine tuberculosis might protect against infection with human tuberculosis. In the late 19th century, clinical trials using \"M. bovis\" were conducted in Italy with disastrous results, because \"M. bovis\" was found to be just as virulent as \"M. tuberculosis\".\n\nAlbert Calmette, a French physician and bacteriologist, and his assistant and later colleague, Camille Guérin, a veterinarian, were working at the Institut Pasteur de Lille (Lille, France) in 1908. Their work included subculturing virulent strains of the tubercle bacillus and testing different culture media. They noted a glycerin-bile-potato mixture grew bacilli that seemed less virulent, and changed the course of their research to see if repeated subculturing would produce a strain that was attenuated enough to be considered for use as a vaccine. The BCG strain was isolated after subculturing 239 times during 13 years from virulent strain on glycerine potato medium. The research continued throughout World War I until 1919, when the now avirulent bacilli were unable to cause tuberculosis disease in research animals. They transferred to the Paris Pasteur Institute in 1919. The BCG vaccine was first used in humans in 1921.\n\nPublic acceptance was slow, and one disaster, in particular, did much to harm public acceptance of the vaccine. In the summer of 1930 in Lübeck, 240 infants were vaccinated in the first 10 days of life; almost all developed tuberculosis and 72 infants died. It was subsequently discovered that the BCG administered there had been contaminated with a virulent strain that was being stored in the same incubator, which led to legal action against the manufacturers of the vaccine.\n\nDr. R.G. Ferguson, working at the Fort Qu'Appelle Sanatorium in Saskatchewan, was among the pioneers in developing the practice of vaccination against tuberculosis. In 1928, BCG was adopted by the Health Committee of the League of Nations (predecessor to the WHO). Because of opposition, however, it only became widely used after World War II. From 1945 to 1948, relief organizations (International Tuberculosis Campaign or Joint Enterprises) vaccinated over 8 million babies in eastern Europe and prevented the predicted typical increase of TB after a major war.\n\nBCG is very efficacious against tuberculous meningitis in the pediatric age group, but its efficacy against pulmonary tuberculosis appears to be variable. As of 2006, only a few countries do not use BCG for routine vaccination. Two countries that have never used it routinely are the USA and the Netherlands (in both countries, it is felt that having a reliable Mantoux test and being able to accurately detect active disease is more beneficial to society than vaccinating against a condition that is now relatively rare there).\n\nOther names include \"Vaccin Bilié de Calmette et Guérin vaccine\" and \"Bacille de Calmette et Guérin vaccine\".\n\nTentative evidence exists for a beneficial non-specific effect of BCG vaccination on overall mortality in low income countries, or for its reducing other health problems including sepsis and respiratory infections when given early, with greater benefit the earlier it is used.\n\n"}
{"id": "4192", "url": "https://en.wikipedia.org/wiki?curid=4192", "title": "Bunsen", "text": "Bunsen\n\nBunsen may refer to:\n"}
{"id": "4193", "url": "https://en.wikipedia.org/wiki?curid=4193", "title": "Common buzzard", "text": "Common buzzard\n\nThe common buzzard (\"Buteo buteo\") is a medium-to-large bird of prey whose range covers most of Europe and extends into Asia. Over much of its range, it is resident year-round, but birds from the colder parts of the Northern Hemisphere typically migrate south (some well into the Southern Hemisphere) for the northern winter.\n\nThe first formal description of the common buzzard was by the Swedish naturalist Carl Linnaeus in 1758 in the tenth edition of his \"Systema Naturae\" under the binomial name \"Falco buteo\". The genus \"Buteo\" was introduced by the French naturalist Bernard Germain de Lacépède in 1799 by tautonymy with the specific name of this species. The word \"buteo\" is Latin for a buzzard.\n\nBuzzard subspecies fall into two groups.\nThe western \"Buteo\" group is mainly resident or short-distance migrants. They are:\n\nThe eastern \"vulpinus\" group includes\n\nThe common buzzard measures between in length with a wingspan and a body mass of , making it a medium-sized raptor.\n\nThis broad-winged raptor has a wide variety of plumages, and in Europe can be confused with the similar rough-legged buzzard (\"Buteo lagopus\") and the distantly related European honey buzzard (\"Pernis apivorus\"), which mimics the common buzzard's plumage for a degree of protection from northern goshawks. The plumage can vary in Britain from almost pure white to black, but is usually shades of brown, with a pale 'necklace' of feathers.\n\nThe common buzzard breeds in woodlands, usually on the fringes, but favours hunting over open land. It eats mainly small mammals, and will come to carrion. A great opportunist, it adapts well to a varied diet of pheasant, rabbit, other small mammals to medium mammals, snakes and lizards, and can often be seen walking over recently ploughed fields looking for worms and insects.\n\nBuzzards do not normally form flocks, but several may be seen together on migration or in good habitat. The Victorian writer on Dartmoor, William Crossing, noted he had on occasions seen flocks of 15 or more at some places. Though a rare occurrence, as many as 20 buzzards can be spotted in one field area, approximately apart, so cannot be classed as a flock in the general sense, consisting of birds without a mate or territory. They are fiercely territorial, and, though rare, fights do break out if one strays onto another pair's territory, but dominant displays of aggression will normally see off the interloper. Pairs mate for life. To attract a mate (or impress his existing mate) the male performs a ritual aerial display before the beginning of spring. This spectacular display is known as 'the roller coaster'. He will rise high up in the sky, to turn and plummet downward, in a spiral, twisting and turning as he comes down. He then rises immediately upward to repeat the exercise.\n\nThe call is a plaintive \"peea-ay\", similar to a cat's meow.\n\nIn parts of its range it is increasing in numbers. In Ireland it became extinct about 1910, but began to slowly recolonise the country in the 1950s, and is now a common and familiar sight over much of Ireland.\n\nThe steppe buzzard, \"B. (b.) vulpinus\" breeds from east Europe eastward to the Far East (including Eastern China and South Asia), excluding Japan. It is a long-distance migrant, excepting some north Himalayan birds, and winters in Africa, India and southeastern Asia. In the open country favoured on the wintering grounds, steppe buzzards are often seen perched on roadside telephone poles.\n\nThe steppe buzzard is some times split off as a separate species, \"B. vulpinus\". Compared to the nominate form, it is slightly smaller ( long), longer winged and longer tailed. The two colour morphs are the rufous form which gives this subspecies its scientific name (\"vulpes\" is Latin for \"fox\") and a dark grey form.\n\nThe tail of \"B. vulpinus\" is paler than the nominate form, and often quite rufous, recalling North American red-tailed hawk. The upper wings have pale primary patches, and the primary flight feathers are also paler when viewed from below. Adults have a black trailing edge to the wings, and both morphs often have plain underparts, lacking the breast band frequently seen in \"B. b. buteo\".\n\n\n"}
{"id": "4194", "url": "https://en.wikipedia.org/wiki?curid=4194", "title": "Bohrium", "text": "Bohrium\n\nBohrium is a chemical element with symbol Bh and atomic number 107. It is named after Danish physicist Niels Bohr. It is a synthetic element (an element that can be created in a laboratory but is not found in nature) and radioactive; the most stable known isotope, Bh, has a half-life of approximately 61 seconds, though the unconfirmed Bh may have a longer half-life of about 690 seconds.\n\nIn the periodic table of the elements, it is a d-block transactinide element. It is a member of the 7th period and belongs to the group 7 elements as the fifth member of the 6d series of transition metals. Chemistry experiments have confirmed that bohrium behaves as the heavier homologue to rhenium in group 7. The chemical properties of bohrium are characterized only partly, but they compare well with the chemistry of the other group 7 elements.\n\nTwo groups claimed discovery of the element. Evidence of bohrium was first reported in 1976 by a Russian research team led by Yuri Oganessian, in which targets of bismuth-209 and lead-208 were bombarded with accelerated nuclei of chromium-54 and manganese-55 respectively. Two activities, one with a half-life of one to two milliseconds, and the other with an approximately five-second half-life, were seen. Since the ratio of the intensities of these two activities was constant throughout the experiment, it was proposed that the first was from the isotope bohrium-261 and that the second was from its daughter dubnium-257. Later, the dubnium isotope was corrected to dubnium-258, which indeed has a five-second half-life (dubnium-257 has a one-second half-life); however, the half-life observed for its parent is much shorter than the half-lives later observed in the definitive discovery of bohrium at Darmstadt in 1981. The IUPAC/IUPAP Transfermium Working Group (TWG) concluded that while dubnium-258 was probably seen in this experiment, the evidence for the production of its parent bohrium-262 was not convincing enough.\n\nIn 1981, a German research team led by Peter Armbruster and Gottfried Münzenberg at the GSI Helmholtz Centre for Heavy Ion Research (GSI Helmholtzzentrum für Schwerionenforschung) in Darmstadt bombarded a target of bismuth-209 with accelerated nuclei of chromium-54 to produce 5 atoms of the isotope bohrium-262:\n\nThis discovery was further substantiated by their detailed measurements of the alpha decay chain of the produced bohrium atoms to previously known isotopes of fermium and californium. The IUPAC/IUPAP Transfermium Working Group (TWG) recognised the GSI collaboration as official discoverers in their 1992 report.\n\nThe German group suggested the name \"nielsbohrium\" with symbol \"Ns\" to honor the Danish physicist Niels Bohr. The Soviet scientists at the Joint Institute for Nuclear Research in Dubna, Russia had suggested this name be given to element 105 (which was finally called dubnium) and the German team wished to recognise both Bohr and the fact that the Dubna team had been the first to propose the cold fusion reaction to solve the controversial problem of the naming of element 105. The Dubna team agreed with the German group's naming proposal for element 107.\n\nThere was an element naming controversy as to what the elements from 104 to 106 were to be called; the IUPAC adopted \"unnilseptium\" (symbol \"Uns\") as a temporary, systematic element name for this element. In 1994 a committee of IUPAC recommended that element 107 be named \"bohrium\", not \"nielsbohrium\", since there was no precedence for using a scientist's complete name in the naming of an element. This was opposed by the discoverers as there was some concern that the name might be confused with boron and in particular the distinguishing of the names of their respective oxyanions, \"bohrate\" and \"borate\". The matter was handed to the Danish branch of IUPAC which, despite this, voted in favour of the name \"bohrium\", and thus the name \"bohrium\" for element 107 was recognized internationally in 1997. It was subsequently decided that oxyanions of bohrium should be called \"bohriates\" to avoid confusion.\n\nBohrium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Twelve different isotopes of bohrium have been reported with atomic masses 260–262, 264–267, 270–272, 274, and 278, one of which, bohrium-262, has a known metastable state. All of these but the unconfirmed Bh decay only through alpha decay, although some unknown bohrium isotopes are predicted to undergo spontaneous fission.\n\nThe lighter isotopes usually have shorter half-lives; half-lives of under 100 ms for Bh, Bh, Bh, and Bh were observed. Bh, Bh, Bh, and Bh are more stable at around 1 s, and Bh and Bh have half-lives of about 10 s. The heaviest isotopes are the most stable, with Bh and Bh having measured half-lives of about 61 s and 54 s respectively, and the even heavier unconfirmed isotope Bh appearing to have an even longer half-life of about 690 s. The unknown isotopes Bh and Bh are predicted to have even longer half-lives of around 90 minutes and 40 minutes respectively. Before its discovery, Bh was also predicted to have a long half-life of 90 minutes, but it was found to have a shorter half-life of only about 54 seconds.\n\nThe proton-rich isotopes with masses 260, 261, and 262 were directly produced by cold fusion, those with mass 262 and 264 were reported in the decay chains of meitnerium and roentgenium, while the neutron-rich isotopes with masses 265, 266, 267 were created in irradiations of actinide targets. The five most neutron-rich ones with masses 270, 271, 272, 274, and 278 (unconfirmed) appear in the decay chains of Nh, Mc, Mc, Ts, and Fl respectively. These eleven isotopes have half-lives ranging from about ten milliseconds for Bh to about one minute for Bh and Bh, extending to about twelve minutes for the unconfirmed Bh, one of the longest-lived known superheavy nuclides.\n\nBohrium is the fifth member of the 6d series of transition metals and the heaviest member of group 7 in the periodic table, below manganese, technetium and rhenium. All the members of the group readily portray their group oxidation state of +7 and the state becomes more stable as the group is descended. Thus bohrium is expected to form a stable +7 state. Technetium also shows a stable +4 state whilst rhenium exhibits stable +4 and +3 states. Bohrium may therefore show these lower states as well. The higher +7 oxidation state is more likely to exist in oxyanions, such as perbohriate, , analogous to the lighter permanganate, pertechnetate, and perrhenate. Nevertheless, bohrium(VII) is likely to be unstable in aqueous solution, and would probably be easily reduced to the more stable bohrium(IV).\n\nTechnetium and rhenium are known to form volatile heptoxides MO (M = Tc, Re), so bohrium should also form the volatile oxide BhO. The oxide should dissolve in water to form perbohric acid, HBhO.\nRhenium and technetium form a range of oxyhalides from the halogenation of the oxide. The chlorination of the oxide forms the oxychlorides MOCl, so BhOCl should be formed in this reaction. Fluorination results in MOF and MOF for the heavier elements in addition to the rhenium compounds ReOF and ReF. Therefore, oxyfluoride formation for bohrium may help to indicate eka-rhenium properties. Since the oxychlorides are asymmetrical, and they should have increasingly large dipole moments going down the group, they should become less volatile in the order TcOCl > ReOCl > BhOCl: this was experimentally confirmed in 2000 by measuring the enthalpies of adsorption of these three compounds. The values are for TcOCl and ReOCl are −51 kJ/mol and −61 kJ/mol respectively; the experimental value for BhOCl is −77.8 kJ/mol, very close to the theoretically expected value of −78.5 kJ/mol.\n\nBohrium is expected to be a solid under normal conditions and assume a hexagonal close-packed crystal structure (/ = 1.62), similar to its lighter congener rhenium. It should be a very heavy metal with a density of around 37.1 g/cm, which would be the third-highest of any of the 118 known elements, lower than only meitnerium (37.4 g/cm) and hassium (41 g/cm), the two following elements in the periodic table. In comparison, the densest known element that has had its density measured, osmium, has a density of only 22.61 g/cm. This results from bohrium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough bohrium to measure this quantity would be impractical, and the sample would quickly decay.\n\nThe atomic radius of bohrium is expected to be around 128 pm. Due to the relativistic stabilization of the 7s orbital and destabilization of the 6d orbital, the Bh ion is predicted to have an electron configuration of [Rn] 5f 6d 7s, giving up a 6d electron instead of a 7s electron, which is the opposite of the behavior of its lighter homologues manganese and technetium. Rhenium, on the other hand, follows its heavier congener bohrium in giving up a 5d electron before a 6s electron, as relativistic effects have become significant by the sixth period, where they cause among other things the yellow color of gold and the low melting point of mercury. The Bh ion is expected to have an electron configuration of [Rn] 5f 6d 7s; in contrast, the Re ion is expected to have a [Xe] 4f 5d configuration, this time analogous to manganese and technetium. The ionic radius of hexacoordinate heptavalent bohrium is expected to be 58 pm (heptavalent manganese, technetium, and rhenium having values of 46, 57, and 53 pm respectively). Pentavalent bohrium should have a larger ionic radius of 83 pm.\n\nIn 1995, the first report on attempted isolation of the element was unsuccessful, prompting new theoretical studies to investigate how best to investigate bohrium (using its lighter homologs technetium and rhenium for comparison) and removing unwanted contaminating elements such as the trivalent actinides, the group 5 elements, and polonium.\n\nIn 2000, it was confirmed that although relativistic effects are important, bohrium behaves like a typical group 7 element. A team at the Paul Scherrer Institute (PSI) conducted a chemistry reaction using six atoms of Bh produced in the reaction between Bk and Ne ions. The resulting atoms were thermalised and reacted with a HCl/O mixture to form a volatile oxychloride. The reaction also produced isotopes of its lighter homologues, technetium (as Tc) and rhenium (as Re). The isothermal adsorption curves were measured and gave strong evidence for the formation of a volatile oxychloride with properties similar to that of rhenium oxychloride. This placed bohrium as a typical member of group 7. The adsorption enthalpies of the oxychlorides of technetium, rhenium, and bohrium were measured in this experiment, agreeing very well with the theoretical predictions and implying a sequence of decreasing oxychloride volatility down group 7 of TcOCl > ReOCl > BhOCl.\n\n"}
{"id": "4195", "url": "https://en.wikipedia.org/wiki?curid=4195", "title": "Barbara Olson", "text": "Barbara Olson\n\nBarbara Kay Olson (née Bracher; December 27, 1955 September 11, 2001) was an American lawyer and conservative television commentator who worked for CNN, Fox News Channel, and several other outlets. She was a passenger on American Airlines Flight 77 en route to a taping of Bill Maher's television show \"Politically Incorrect\" when it was flown into the Pentagon in the September 11 attacks. Her original plan had been to fly to California on September 10, but she delayed her departure until the next morning so that she could wake up with her husband on his birthday, September 11.\n\nOlson was born Barbara Kay Bracher in Houston, Texas. Her older sister, Toni Bracher-Lawrence, was a member of the Houston City Council from 2004 to 2010. She graduated from Waltrip High School and earned a Bachelor of Arts from the University of Saint Thomas in Houston. She earned a Juris Doctor degree from the Yeshiva University Benjamin N. Cardozo School of Law.\n\nAs a newcomer, she achieved a surprising measure of success, working for HBO and Stacy Keach Productions. In the early 1990s, she worked as an associate at the Washington, D.C.-based law firm of Wilmer Cutler & Pickering where she did civil litigation for several years before becoming an Assistant U.S. Attorney.\n\nOlson's support in 1991 of Supreme Court nominee Clarence Thomas led to the formation of the Independent Women's Forum. At that time, Olson and friend Rosalie (Ricky) Gaull Silberman started an informal network of women who supported the Thomas nomination to the Supreme Court despite allegations of sexual harassment by Anita Hill, a former subordinate of Thomas at the Equal Employment Opportunity Commission. Olson, who had also worked under Thomas at the EEOC and was a close friend of Thomas, spoke out on his behalf during his contentious Senate confirmation hearings. Olson later helped edit \"The Real Anita Hill\", a book by David Brock that savaged Hill and portrayed the harassment claim as a political dirty trick (Brock later recanted his claims and apologized to Hill). The Independent Women's Forum continued on with a goal of retaining a high profile group of women to advocate for economic and political freedom and personal responsibility.\n\nIn 1994, Olson became chief investigative counsel for the U.S. House of Representatives Committee on Oversight and Government Reform. In that position, she led the Travelgate and Filegate investigations into the Clinton administration. She was later a partner in the Washington, D.C. office of the Birmingham, Alabama law firm Balch & Bingham.\n\nShe married Theodore Olson in 1996, becoming his third wife. Theodore went on to successfully represent presidential candidate George W. Bush in the Supreme Court case of \"Bush v. Gore\", and subsequently served as U.S. Solicitor General in the Bush administration.\n\nOlson was a frequent critic of the Bill Clinton administration and wrote a book about then First Lady Hillary Clinton, \"Hell to Pay: The Unfolding Story of Hillary Rodham Clinton\" (1999). Olson's second book, \"The Final Days: The Last, Desperate Abuses of Power by the Clinton White House\" was published posthumously.\n\nShe was a resident of Great Falls, Virginia.\n\nOlson was a passenger on American Airlines Flight 77 on her way to a taping of \"Politically Incorrect\" in Los Angeles, when it was flown into the Pentagon in the September 11 attacks. Her original plan had been to fly to California on September 10, but she waited until the next day so that she could wake up with her husband on his birthday, September 11. Bill Maher, host of \"Politically Incorrect\", left a panel chair empty for a week in her memory. At the National September 11 Memorial, Olson's name is located on Panel S-70 of the South Pool, along with those of other passengers of Flight 77.\n\n\n"}
{"id": "4196", "url": "https://en.wikipedia.org/wiki?curid=4196", "title": "Barnard's Star", "text": "Barnard's Star\n\nBarnard's Star is a very-low-mass red dwarf about 6 light-years away from Earth in the constellation of Ophiuchus. It is the fourth-nearest known individual star to the Sun (after the three components of the Alpha Centauri system) and the closest star in the Northern Celestial Hemisphere. Despite its proximity, with a dim apparent magnitude of +9.5, the star is invisible to the unaided eye; it is much brighter in the infrared than in visible light.\n\nThe star is named after the American astronomer E. E. Barnard. He was not the first to observe the star (it appeared on Harvard University plates in 1888 and 1890), but in 1916 he measured its proper motion (which is a function of its close proximity to Earth, and not of its actual space velocity) as 10.3 arcseconds per year, which remains the largest proper motion of any star relative to the Sun. This is likely to be the fastest star, as its close proximity to the Sun, as well as its high velocity make it unlikely any faster object remains undiscovered. In 2016, the International Astronomical Union organized a Working Group on Star Names (WGSN) to catalogue and standardize proper names for stars. The WGSN approved the name \"Barnard's Star\" for this star on 1 February 2017 and it is now so entered in the IAU Catalog of Star Names.\n\nBarnard's Star is among the most studied red dwarfs because of its proximity and favorable location for observation near the celestial equator. Historically, research on Barnard's Star has focused on measuring its stellar characteristics, its astrometry, and also refining the limits of possible extrasolar planets. Although Barnard's Star is an ancient star, it still experiences star flare events, one being observed in 1998.\n\nThe star has also been the subject of some controversy. For a decade, from the early 1960s to the early 1970s, Peter van de Kamp claimed that there were one or more gas giants in orbit around it. Although the presence of small terrestrial planets around Barnard's Star remains a possibility, Van de Kamp's specific claims of large gas giants were refuted in the mid-1970s.\n\nBarnard's Star is a red dwarf of the dim spectral type M4, and it is too faint to see without a telescope. Its apparent magnitude is 9.5.\n\nAt 7–12 billion years of age, Barnard's Star is considerably older than the Sun, which is 4.5 billion years old, and it might be among the oldest stars in the Milky Way galaxy. Barnard's Star has lost a great deal of rotational energy, and the periodic slight changes in its brightness indicate that it rotates once in 130 days (the Sun rotates in 25). Given its age, Barnard's Star was long assumed to be quiescent in terms of stellar activity. In 1998, astronomers observed an intense stellar flare, showing that Barnard's Star is a flare star. Barnard's Star has the variable star designation V2500 Ophiuchi. In 2003, Barnard's Star presented the first detectable change in the radial velocity of a star caused by its motion. Further variability in the radial velocity of Barnard's Star was attributed to its stellar activity.\n\nThe proper motion of Barnard's Star corresponds to a relative lateral speed of 90 km/s. The 10.3 seconds of arc it travels annually amount to a quarter of a degree in a human lifetime, roughly half the angular diameter of the full Moon.\n\nThe radial velocity of Barnard's Star towards the Sun is measured from its blue shift to be 110 km/s. Combined with its proper motion, this gives a space velocity (actual velocity relative to the Sun) of 142.6 ± 0.2 km/s. Barnard's Star will make its closest approach to the Sun around AD 11,800, when it will approach to within about 3.75 light-years.\n\nProxima Centauri is the closest star to the Sun at a position currently 4.24 light-years distant from it. However, despite Barnard's star's even closer pass to the Sun in 11,800 AD, it will still not then be the nearest star, since by that time Proxima Centauri will have moved to a yet nearer proximity to the Sun. At the time of the star's closest pass by the Sun, Barnard's Star will still be too dim to be seen with the naked eye, since its apparent magnitude will only have increased by one magnitude to about 8.5 by then, still being 2.5 magnitudes short of visibility to the naked eye.\n\nBarnard's Star has a mass of about 0.14 solar masses (), and a radius 15% to 20% of that of the Sun. Thus, although Barnard's Star has roughly 150 times the mass of Jupiter (), its radius is only 1.5 to 2.0 times larger, due to its much higher density. Its effective temperature is 3,100 kelvins, and it has a visual luminosity of 0.0004 solar luminosities. Barnard's Star is so faint that if it were at the same distance from Earth as the Sun is, it would appear only 100 times brighter than a full moon, comparable to the brightness of the Sun at 80 astronomical units.\n\nBarnard's Star's has 10–32% of the solar metallicity. Metallicity is the proportion of stellar mass made up of elements heavier than helium and helps classify stars relative to the galactic population. Barnard's Star seems to be typical of the old, red dwarf population II stars, yet these are also generally metal-poor halo stars. While sub-solar, Barnard's Star's metallicity is higher than that of a halo star and is in keeping with the low end of the metal-rich disk star range; this, plus its high space motion, have led to the designation \"intermediate population II star\", between a halo and disk star.\n\nFor a decade from 1963 to about 1973, a substantial number of astronomers accepted a claim by Peter van de Kamp that he had detected, by using astrometry, a perturbation in the proper motion of Barnard's Star consistent with its having one or more planets comparable in mass with Jupiter. Van de Kamp had been observing the star from 1938, attempting, with colleagues at the Swarthmore College observatory, to find minuscule variations of one micrometre in its position on photographic plates consistent with orbital perturbations that would indicate a planetary companion; this involved as many as ten people averaging their results in looking at plates, to avoid systemic individual errors. Van de Kamp's initial suggestion was a planet having about at a distance of 4.4 AU in a slightly eccentric orbit, and these measurements were apparently refined in a 1969 paper. Later that year, Van de Kamp suggested that there were two planets of 1.1 and .\n\nOther astronomers subsequently repeated Van de Kamp's measurements, and two papers in 1973 undermined the claim of a planet or planets. George Gatewood and Heinrich Eichhorn, at a different observatory and using newer plate measuring techniques, failed to verify the planetary companion. Another paper published by John L. Hershey four months earlier, also using the Swarthmore observatory, found that changes in the astrometric field of various stars correlated to the timing of adjustments and modifications that had been carried out on the refractor telescope's objective lens; the claimed planet was attributed to an artifact of maintenance and upgrade work. The affair has been discussed as part of a broader scientific review.\n\nVan de Kamp never acknowledged any error and published a further claim of two planets' existence as late as 1982; he died in 1995. Wulff Heintz, Van de Kamp's successor at Swarthmore and an expert on double stars, questioned his findings and began publishing criticisms from 1976 onwards. The two men were reported to have become estranged from each other because of this.\n\nWhile not completely ruling out the possibility of planets, null results for planetary companions continued throughout the 1980s and 1990s, the latest based on interferometric work with the Hubble Space Telescope in 1999. By refining the values of a star's motion, the mass and orbital boundaries for possible planets are tightened: in this way astronomers are often able to describe what types of planets cannot orbit a given star.\n\nM dwarfs such as Barnard's Star are more easily studied than larger stars in this regard because their lower masses render perturbations more obvious. Gatewood was thus able to show in 1995 that planets with were impossible around Barnard's Star, in a paper which helped refine the negative certainty regarding planetary objects in general. In 1999, work with the Hubble Space Telescope further excluded planetary companions of with an orbital period of less than 1,000 days (Jupiter's orbital period is 4,332 days), while Kuerster determined in 2003 that within the habitable zone around Barnard's Star, planets are not possible with an \"\"M\" sin \"i\"\" value greater than 7.5 times the mass of the Earth (), or with a mass greater than 3.1 times the mass of Neptune (much lower than van de Kamp's smallest suggested value).\n\nEven though this research has greatly restricted the possible properties of planets around Barnard's Star, it has not ruled them out completely; terrestrial planets would be difficult to detect. NASA's Space Interferometry Mission, which was to begin searching for extrasolar Earth-like planets, was reported to have chosen Barnard's Star as an early search target. This mission was shut down in 2010. ESA's similar Darwin interferometry mission had the same goal, but was stripped of funding in 2007.\n\nBarnard's Star was studied as part of Project Daedalus. Undertaken between 1973 and 1978, the study suggested that rapid, unmanned travel to another star system was possible with existing or near-future technology. Barnard's Star was chosen as a target partly because it was believed to have planets.\n\nThe theoretical model suggested that a nuclear pulse rocket employing nuclear fusion (specifically, electron bombardment of deuterium and helium-3) and accelerating for four years could achieve a velocity of 12% of the speed of light. The star could then be reached in 50 years, within a human lifetime. Along with detailed investigation of the star and any companions, the interstellar medium would be examined and baseline astrometric readings performed.\n\nThe initial Project Daedalus model sparked further theoretical research. In 1980, Robert Freitas suggested a more ambitious plan: a self-replicating spacecraft intended to search for and make contact with extraterrestrial life. Built and launched in Jupiter's orbit, it would reach Barnard's Star in 47 years under parameters similar to those of the original Project Daedalus. Once at the star, it would begin automated self-replication, constructing a factory, initially to manufacture exploratory probes and eventually to create a copy of the original spacecraft after 1,000 years.\n\nIn 1998 a stellar flare on Barnard's Star was detected based on changes in the spectral emissions on July 17, 1998, during an unrelated search for variations in the proper motion. Four years passed before the flare was fully analyzed, at which point it was suggested that the flare's temperature was 8000 K, more than twice the normal temperature of the star. Given the essentially random nature of flares, Diane Paulson, one of the authors of that study, noted that \"the star would be fantastic for amateurs to observe\".\nThe flare was surprising because intense stellar activity is not expected in stars of such age. Flares are not completely understood, but are believed to be caused by strong magnetic fields, which suppress plasma convection and lead to sudden outbursts: strong magnetic fields occur in rapidly rotating stars, while old stars tend to rotate slowly. For Barnard's Star to undergo an event of such magnitude is thus presumed to be a rarity. Research on the star's periodicity, or changes in stellar activity over a given timescale, also suggest it ought to be quiescent; 1998 research showed weak evidence for periodic variation in the star's brightness, noting only one possible starspot over 130 days.\n\nStellar activity of this sort has created interest in using Barnard's Star as a proxy to understand similar stars. It is hoped that photometric studies of its X-ray and UV emissions will shed light on the large population of old M dwarfs in the galaxy. Such research has astrobiological implications: given that the habitable zones of M dwarfs are close to the star, any planets would be strongly influenced by solar flares, winds, and plasma ejection events.\n\nBarnard's Star shares much the same neighborhood as the Sun. The neighbors of Barnard's Star are generally of red dwarf size, the smallest and most common star type. Its closest neighbor is currently the red dwarf Ross 154, at 1.66 parsecs (5.41 light-years) distance. The Sun and Alpha Centauri are, respectively, the next closest systems. From Barnard's Star, the Sun would appear on the diametrically opposite side of the sky at coordinates RA=, Dec=, in the eastern part of the constellation Monoceros. The absolute magnitude of the Sun is 4.83, and at a distance of 1.834 parsecs, it would be a first-magnitude star, as Pollux is from the Earth.\n\n\n"}
{"id": "4199", "url": "https://en.wikipedia.org/wiki?curid=4199", "title": "Bayer designation", "text": "Bayer designation\n\nA Bayer designation is a stellar designation in which a specific star is identified by a Greek letter, followed by the genitive form of its parent constellation's Latin name. The original list of Bayer designations contained 1,564 stars.\n\nMost of the brighter stars were assigned their first systematic names by the German astronomer Johann Bayer in 1603, in his star atlas \"Uranometria\". Bayer assigned a lower-case Greek letter, such as alpha (α), beta (β), gamma (γ), etc., to each star he catalogued, combined with the Latin name of the star’s parent constellation in genitive (possessive) form. (See 88 modern constellations for the genitive forms.) For example, Aldebaran is designated \"α Tauri\" (pronounced \"Alpha Tauri\"), which means \"Alpha of the constellation Taurus\".\n\nA single constellation may contain fifty or more stars visible to the naked eye, but the Greek alphabet has only twenty-four letters. When these ran out, Bayer began using Latin letters: upper case \"A\", followed by lower case \"b\" through \"z\" (omitting \"j\" and \"v\"), for a total of another 24 letters. Bayer never went beyond \"z\", but later astronomers added more designations using both upper and lower case Latin letters, the upper case letters following the lower case ones in general. Examples include \"s Carinae\" (\"s\" of the constellation Carina), \"d Centauri\" (\"d\" of the constellation Centaurus), \"G Scorpii\" (\"G\" of the constellation Scorpius), and \"N Velorum\" (\"N\" of the constellation Vela). The last upper-case letter used in this way was \"Q\".\n\nBayer catalogued only a few stars too far south to be seen from Germany, but later astronomers (notably Lacaille and Gould) supplemented Bayer's catalog with entries for southern constellations.\n\nIn most constellations, Bayer assigned Greek and Latin letters to stars within a constellation in rough order of apparent brightness, from brightest to dimmest. Since the brightest star in a majority of constellations is designated Alpha (α), many people wrongly assume that Bayer meant to order the stars exclusively by brightness. In Bayer's day, however, stellar brightness could not be measured precisely. Stars were traditionally assigned to one of six magnitude classes (the brightest to first magnitude, the dimmest to sixth), and Bayer typically ordered stars within a constellation by class: all the first-magnitude stars, followed by all the second-magnitude stars, and so on. Within each magnitude class, Bayer made no attempt to arrange stars by relative brightness. As a result, the brightest star in each class did not always get listed first in Bayer's order.\n\nBut in addition, Bayer did not always follow the magnitude class rule; he sometimes assigned letters to stars according to their location within a constellation, or the order of their rising, or to historical or mythological details. Occasionally the order looks quite arbitrary.\n\nOf the 88 modern constellations, there are at least 30 in which \"Alpha\" is not the brightest star, and four of those lack an alpha star altogether. (Constellations with no alpha include Vela and Puppis – both formerly part of Argo Navis, whose alpha is Canopus in Carina.)\n\nOrion provides a good example of Bayer's method. Bayer first designated Betelgeuse and Rigel, the two 1st-magnitude stars (those of magnitude 1.5 or less), as Alpha and Beta from north to south, with Betelgeuse (the shoulder) coming ahead of Rigel (the foot), even though the latter is usually the brighter. (Betelgeuse is a variable star and can at its maximum occasionally outshine Rigel.) Bayer then repeated the procedure for the stars of the 2nd magnitude (those between magnitudes 1.51 and 2.5), labeling them from \"gamma\" through \"zeta\" in \"top-down\" (north-to-south) order.\n\nThe \"First to Rise in the East\" order is used in a number of instances. Castor and Pollux of Gemini may be an example of this: Pollux is brighter than Castor, but the latter rises earlier and was assigned \"alpha\". In this case, Bayer may also have been influenced by the traditional order of the mythological names \"Castor and Pollux\": Castor is generally named first whenever the twins are mentioned.\n\nAlthough the brightest star in Draco is Eltanin (Gamma Draconis), Thuban was assigned \"alpha\" (α) by Bayer because, due to precession, Thuban was the north pole star 4,000 years ago. Sometimes there is no apparent order, as exemplified by the stars in Sagittarius, where Bayer's designations appear almost random to the modern eye. Alpha and Beta Sagittarii are perhaps the most anomalously designated stars in the sky. They are more than two magnitudes fainter than the brightest star (designated Epsilon), they lie several degrees south of the main pattern (the \"teapot\" asterism), they are more than 20 degrees off the ecliptic in a Zodiacal constellation, and they do not even rise from Bayer's native Germany (while Epsilon and several other brighter stars do). The order of the letters assigned in Sagittarius does correspond to the magnitudes as illustrated on Bayer's chart; but the latter do not agree with modern determinations of the magnitudes.\n\nBayer designations added by later astronomers generally were ordered by magnitude, but care was usually taken to avoid conflict with designations already assigned. In Libra, for example, the new designations sigma, tau, and upsilon were chosen to avoid conflict with Bayer's earlier designations, even though several stars with earlier letters are not as bright.\n\nAlthough Bayer did not use upper-case Latin letters (except \"A\") for \"fixed stars\", he did use them to label other items shown on his charts, such as neighboring constellations, \"temporary stars\", miscellaneous astronomical objects, or reference lines like the Tropic of Cancer. In Cygnus, for example, Bayer's fixed stars run through \"g\", and on this chart Bayer employs \"H\" through \"P\" as miscellaneous labels, mostly for neighboring constellations. Bayer did not intend such labels as catalog designations, but some have survived to refer to astronomical objects: P Cygni for example is still used as a designation for Nova Cyg 1600. Tycho's Star (SN 1572), another \"temporary star\", appears as B Cassiopeiae. In charts for constellations that did not exhaust the Greek letters, Bayer sometimes used the left-over Greek letters for miscellaneous labels as well.\n\nPtolemy designated four stars as \"border stars\", each shared by two constellations: Alpheratz (in Andromeda and Pegasus), Elnath (in Taurus and Auriga), Nu Boötis (in Boötes and Hercules), and Fomalhaut (in Piscis Austrinus and Aquarius). Bayer assigned the first three of these stars a Greek letter from both constellations: , , and . (He catalogued Fomalhaut only once, as Alpha Piscis Austrini.) When the International Astronomical Union (IAU) assigned definite boundaries to the constellations in 1930, it declared that stars and other celestial objects can belong to only one constellation. Consequently, the redundant second designation in each pair above has dropped out of use.\n\nBayer assigned two stars duplicate names by mistake: (duplicated as ) and (duplicated as ). He corrected these in a later atlas, and the duplicate names were no longer used.\n\nOther cases of multiple Bayer designations arose when stars named by Bayer in one constellation were transferred by later astronomers to a different constellation. Bayer's Gamma and Omicron Scorpii, for example, were later reassigned from Scorpius to Libra and given the new names Sigma and Upsilon Librae. (To add to the confusion, the star now known as Omicron Scorpii was not named by Bayer but was assigned the designation o Scorpii (Latin lower case 'o') by Lacaille – which later astronomers misinterpreted as omicron once Bayer's omicron had been reassigned to Libra.)\n\nA few stars no longer lie (according to the modern constellation boundaries) within the constellation for which they are named. The proper motion of Rho Aquilae, for example, carried it across the boundary into Delphinus in 1992.\n\nBayer designations are most often written as the Greek or Latin letter followed by the standard 3-character constellation abbreviation: α CMa, β Per; or occasionally with the constellation genitive in full: α Canis Majoris, β Persei. Earlier 4-letter abbreviations (α CMaj, β Pers) are rarely used today. The Greek letter names are sometimes written out as well: Alpha Canis Majoris, Beta Persei.\n\nThe Latin-letter extended designations are not as commonly used as the Greek-letter ones (especially in constellations with Flamsteed designations), but there are some exceptions such as h Persei (which is actually a star cluster) and P Cygni. Uppercase Latin Bayer designations in modern use do not go beyond Q; names such as R Leporis and W Ursae Majoris are variable star designations, not Bayer designations.\n\nA further complication is the use of numeric superscripts to distinguish neighboring stars that Bayer (or a later astronomer) labeled with a common letter. Usually these are double stars (mostly optical doubles rather than true binary stars), but there are some exceptions such as the chain of stars π, π, π, π, π and π Orionis.\n\nBayer did not label \"permanent\" stars with uppercase letters (except for \"A\", which he used in place of \"a\"). However, a number of stars in southern constellations have upper-case letter designations, like B Centauri and G Scorpii. These letters were assigned by later astronomers, notably Lacaille in his \"Coelum Australe Stelliferum\" and Gould in his \"Uranometria Argentina\". Lacaille followed Bayer's use of Greek letters, but this was insufficient for many constellations. He used first the lowercase letters, starting with \"a\", and if needed the uppercase letters, starting with \"A\", thus deviating somewhat from Bayer's practice. Lacaille used the Latin alphabet three times over in the large constellation Argo Navis, once for each of the three areas that are now the constellations of Carina, Puppis, and Vela. That was still insufficient for the number of stars, so he also used uppercase Latin letters such as N Velorum and Q Puppis. Lacaille assigned uppercase letters between R and Z in several constellations, but these have either been dropped to allow the assignment of those letters to variable stars or have actually turned out to be variable.\n\n"}
{"id": "4200", "url": "https://en.wikipedia.org/wiki?curid=4200", "title": "Boötes", "text": "Boötes\n\nBoötes is a constellation in the northern sky, located between 0° and +60° declination, and 13 and 16 hours of right ascension on the celestial sphere. The name comes from the Greek Βοώτης, \"Boōtēs\", meaning \"herdsman\" or \"plowman\" (literally, \"ox-driver\"; from βοῦς \"bous\" “cow”).\n\nOne of the 48 constellations described by the 2nd-century astronomer Ptolemy, Boötes is now one of the 88 modern constellations. It contains the fourth-brightest star in the night sky, the orange giant star Arcturus. Epsilon Bootis, or Izar, is a colourful multiple star popular with amateur astronomers. Boötes is home to many other bright stars, including eight above the fourth magnitude and an additional 21 above the fifth magnitude, making a total of 29 stars easily visible to the naked eye.\n\nIn ancient Babylon, the stars of Boötes were known as SHU.PA. They were apparently depicted as the god Enlil, who was the leader of the Babylonian pantheon and special patron of farmers. Boötes may have been represented by the foreleg constellation in ancient Egypt. According to this interpretation, the constellation depicts the shape of an animal foreleg. \n\nThe name \"Boötes\" was first used by Homer in his Odyssey as a celestial reference point for navigation, described as \"late-setting\" or \"slow to set\", translated as the \"Plowman\". Exactly whom Boötes is supposed to represent in Greek mythology is not clear. According to one version, he was a son of Demeter, Philomenus, twin brother of Plutus, a ploughman who drove the oxen in the constellation Ursa Major. This is corroborated by the constellation's name, which itself means \"ox-driver\" or \"herdsman.\" The ancient Greeks saw the asterism now called the \"Big Dipper\" or \"Plough\" as a cart with oxen. This influenced the name's etymology, derived from the Greek for \"noisy\" or \"ox-driver\". Another myth associated with Boötes tells that he invented the plow and was memorialized for his ingenuity as a constellation.\n\nAnother myth associated with Boötes by Hyginus is that of Icarius, who was schooled as a grape farmer and winemaker by Dionysus. Icarius made wine so strong that those who drank it appeared poisoned, which caused shepherds to avenge their supposedly poisoned friends by killing Icarius. Maera, Icarius's dog, brought his daughter Erigone to her father's body, whereupon both she and the dog committed suicide. Zeus then chose to honor all three by placing them in the sky as constellations: Icarius as Boötes, Erigone as Virgo, and Maera as Canis Major or Canis Minor.\n\nFollowing another reading, the constellation is identified with Arcas and also referred to as Arcas and Arcturus, son of Zeus and Callisto. Arcas was brought up by his maternal grandfather Lycaon, to whom one day Zeus went and had a meal. To verify that the guest was really the king of the gods, Lycaon killed his grandson and prepared a meal made from his flesh. Zeus noticed and became very angry, transforming Lycaon into a wolf and gave back life to his son. In the meantime Callisto had been transformed into a she-bear, by Zeus's wife, Hera, who was angry at Zeus's infidelity. This is corroborated by the Greek name for Boötes, \"Arctophylax\", which means \"Bear Watcher\". Callisto in form of a bear was almost killed by her son who was out hunting. Zeus rescued her, taking her into the sky where she became Ursa Major, \"the Great Bear\". The name Arcturus (the constellation's brightest star) comes from the Greek word meaning \"guardian of the bear\". Sometimes Arcturus is depicted as leading the hunting dogs of nearby Canes Venatici and driving the bears of Ursa Major and Ursa Minor.\n\nSeveral former constellations were formed from stars now included in Boötes. Quadrans Muralis, the Quadrant, was a constellation created near Beta Boötis from faint stars. It was invented in 1795 by Jérôme Lalande, an astronomer who used a quadrant to perform detailed astronometric measurements. Lalande worked with Nicole-Reine Lepaute and others to predict the 1758 return of Halley's Comet. Quadrans Muralis was formed from the stars of eastern Boötes, western Hercules, and Draco. It was originally called \"Le Mural\" by Jean Fortin in his 1795 Atlas Céleste; it was not given the name \"Quadrans Muralis\" until Johann Bode's 1801 \"Uranographia\". The constellation was quite faint, with its brightest stars reaching the 5th magnitude. Mons Maenalus, representing the Maenalus mountains, was created by Johannes Hevelius in 1687 at the foot of the constellation's figure. The mountain was named for the son of Lycaon, Maenalus. The mountain, one of Diana's hunting grounds, was also holy to Pan.\n\nThe stars of Boötes were incorporated into many different Chinese constellations. Arcturus was part of the most prominent of these, variously designated as the celestial king's throne (\"Tian Wang\") or the Blue Dragon's horn (\"Daijiao\"); the name \"Daijiao\", meaning \"great horn\", is more common. Arcturus was given such importance in Chinese celestial mythology because of its status marking the beginning of the lunar calendar, as well as its status as the brightest star in the northern night sky. Two constellations flanked \"Daijiao\", \"Yousheti\" to the right and \"Zuosheti\" to the left; they represented companions that orchestrated the seasons. \"Zuosheti\" was formed from modern Zeta, Omicron, and Pi Boötis, while \"Yousheti\" was formed from modern Eta, Tau, and Upsilon Boötis. \"Dixi\", the Emperor's ceremonial banquet mat, was north of Arcturus, consisting of the stars 12, 11, and 9 Boötis. Another northern constellation was \"Qigong\", the Seven Dukes, which was mostly across the Boötes-Hercules border. It included either Delta Boötis or Beta Boötis as its terminus.\n\nThe other Chinese constellations made up of the stars of Boötes existed in the modern constellation's north; they are all representations of weapons. \"Tianqiang\", the spear, was formed from Iota, Kappa, and Theta Boötis; \"Genghe\", variously representing a lance or shield, was formed from Epsilon, Rho, and Sigma Boötis. There were also two weapons made up of a singular star. \"Xuange\", the halberd, was represented by Lambda Boötis, and \"Zhaoyao\", either the sword or the spear, was represented by Gamma Boötis.\n\nTwo Chinese constellations have an uncertain placement in Boötes. \"Kangchi\", the lake, was placed south of Arcturus, though its specific location is disputed. It may have been placed entirely in Boötes, on either side of the Boötes-Virgo border, or on either side of the Virgo-Libra border. The constellation \"Zhouding\", a bronze tripod-mounted container used for food, was sometimes cited as the stars 1, 2, and 6 Boötis. However, it has also been associated with three stars in Coma Berenices.\n\nBoötes is also known to North American cultures. In Yup'ik Boötes is \"Taluyaq\", literally \"fish trap,\" and the funnel shaped part of the fish trap is known as \"Ilulirat.\"\n\nBoötes is a constellation bordered by Virgo to the south, Coma Berenices and Canes Venatici to the west, Ursa Major to the northwest, Draco to the northeast, and Hercules, Corona Borealis and Serpens Caput to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Boo'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of 16 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates stretch from +7.36° to +55.1°. Covering 907 square degrees, Boötes culminates at midnight around 2 May and ranks 13th in area.\n\nColloquially, its pattern of stars has been likened to a kite or ice cream cone. However, depictions of Boötes have varied historically. Aratus described him circling the north pole, herding the two bears. Later ancient Greek depictions, described by Ptolemy, have him holding the reins of his hunting dogs (Canes Venatici) in his left hand, with a spear, club, or staff in his right hand. After Hevelius introduced Mons Maenalus in 1681, Boötes was often depicted standing on the Peloponnese mountain. By 1801, when Johann Bode published his \"Uranographia\", Boötes had acquired a sickle, which was also held in his left hand.\n\nThe placement of Arcturus has also been mutable through the centuries. Traditionally, Arcturus lay between his thighs, as Ptolemy depicted him. However, Germanicus Caesar deviated from this tradition by placing Arcturus \"where his garment is fastened by a knot\".\n\nIn his \"Uranometria\", Johann Bayer used the Greek letters Alpha through to Omega and then A to k to label what he saw as the most prominent 35 stars in the constellation, with subsequent astronomers splitting Kappa, Mu, Nu and Pi as two stars each. Nu is also the same star as Psi Herculis. John Flamsteed numbered 54 stars for the constellation.\n\nLocated 36.7 light-years from Earth, Arcturus, or Alpha Boötis, is the brightest star in Boötes and the fourth-brightest star in the sky at an apparent magnitude of −0.05; It is also the brightest star north of the celestial equator, just shading out Vega and Capella. Its name comes from the Greek for \"bear-keeper\". An orange giant of spectral class K1.5III, Arcturus is an ageing star that has exhausted its core supply of hydrogen and cooled and expanded to a diameter of 27 solar diameters, equivalent to approximately 32 million kilometers. Though its mass is approximately one solar mass (), Arcturus shines with 133 times the luminosity of the Sun (). Bayer located Arcturus above the herdsman's left knee in his \"Uranometria\". Nearby Eta Boötis, or Muphrid, is the uppermost star denoting the left leg. It is a 2.68-magnitude star 37 light-years distant with a spectral class of G0IV, indicating it has just exhausted its core hydrogen and is beginning to expand and cool. It is 9 times as luminous as the Sun and has 2.7 times its diameter. Analysis of its spectrum reveals that it is a spectroscopic binary. Muphrid and Arcturus lie only 3.3 light-years away from each other. Viewed from Arcturus, Muphrid would have a visual magnitude of -2½, while Arcturus would be around visual magnitude -4½ when seen from Muphrid.\n\nMarking the herdsman's head is Beta Boötis, or Nekkar, a yellow giant of magnitude 3.5 and spectral type G8IIIa. Like Arcturus, it has expanded and cooled off the main sequence—likely to have lived most of its stellar life as a blue-white B-type main sequence star. Its common name comes from the Arabic phrase for \"ox-driver\". It is 219 light-years away and has a luminosity of . Located 86 light-years distant, Gamma Boötis, or Seginus, is a white giant star of spectral class A7III, with a luminosity 34 times and diameter 3.5 times that of the Sun. It is a Delta Scuti variable, ranging between magnitudes 3.02 and 3.07 every 7 hours. These stars are short period (six hours at most) pulsating stars that have been used as standard candles and as subjects to study astroseismology. Delta Boötis is a wide double star with a primary of magnitude 3.5 and a secondary of magnitude 7.8. The primary is a yellow giant that has cooled and expanded to 10.4 times the diameter of the Sun. Of spectral class G8IV, it is around 121 light-years away, while the secondary is a yellow main sequence star of spectral type G0V. The two are thought to take 120,000 years to orbit each other. Mu Boötis, known as Alkalurops, is a triple star popular with amateur astronomers. It has an overall magnitude of 4.3 and is 121 light-years away. Its name is from the Arabic phrase for \"club\" or \"staff\". The primary appears to be of magnitude 4.3 and is blue-white. The secondary appears to be of magnitude 6.5, but is actually a close double star itself with a primary of magnitude 7.0 and a secondary of magnitude 7.6. The secondary and tertiary stars have an orbital period of 260 years. The primary has an absolute magnitude of 2.6 and is of spectral class F0. The secondary and tertiary stars are separated by 2 arcseconds; the primary and secondary are separated by 109.1 arcseconds at an angle of 171 degrees. Nu Boötis is an optical double star. The primary is an orange giant of magnitude 5.0 and the secondary is a white star of magnitude 5.0. The primary is 870 light-years away and the secondary is 430 light-years.\n\nEpsilon Boötis, also known as \"Izar\" or \"Pulcherrima\", is a close triple star popular with amateur astronomers and the most prominent binary star in Boötes. The primary is a yellow- or orange-hued magnitude 2.5 giant star, the secondary is a magnitude 4.6 blue-hued main-sequence star, and the tertiary is a magnitude 12.0 star. The system is 210 light-years away. The name \"Izar\" comes from the Arabic word for \"girdle\" or \"loincloth\", referring to its location in the constellation. The name \"Pulcherrima\" comes from the Latin phrase for \"most beautiful\", referring to its contrasting colors in a telescope. The primary and secondary stars are separated by 2.9 arcseconds at an angle of 341 degrees; the primary's spectral class is K0 and it has a luminosity of . To the naked eye, Izar has a magnitude of 2.37. Nearby Rho and Sigma Boötis denote the herdsman's waist. Rho is an orange giant of spectral type K3III located around 160 light-years from Earth. It is ever so slightly variable, wavering by 0.003 of a magnitude from its average of 3.57. Sigma, a yellow-white main sequence star of spectral type F3V, is suspected of varying in brightness from 4.45 to 4.49. It is around 52 light-years distant.\n\nTraditionally known as \"Aulād al Dhiʼbah\" (أولاد الضباع - \"aulād al dhiʼb\"), \"the Whelps of the Hyenas\", Theta, Iota, Kappa and Lambda Boötis are a small group of stars in the far north of the constellation. The magnitude 4.05 Theta Boötis has a spectral type of F7 and an absolute magnitude of 3.8. Iota Boötis is a triple star with a primary of magnitude 4.8 and spectral class of A7, a secondary of magnitude 7.5, and a tertiary of magnitude 12.6. The primary is 97 light-years away. The primary and secondary stars are separated by 38.5 arcseconds, at an angle of 33 degrees. The primary and tertiary stars are separated by 86.7 arcseconds at an angle of 194 degrees. Both the primary and tertiary appear white in a telescope, but the secondary appears yellow-hued. Kappa Boötis is another wide double star. The primary is 155 light-years away and has a magnitude of 4.5. The secondary is 196 light-years away and has a magnitude of 6.6. The two components are separated by 13.4 arcseconds, at an angle of 236 degrees. The primary, with spectral class A7, appears white and the secondary appears bluish. An apparent magnitude 4.18 type A0p star, Lambda Boötis is the prototype of a class of chemically peculiar stars, only some of which pulsate as Delta Scuti-type stars. The distinction between the Lambda Boötis stars as a class of stars with peculiar spectra, and the delta Scuti stars whose class describes pulsation in low-overtone pressure modes, is an important one. While many Lambda Boötis stars pulsate and are delta Scuti stars, not many delta Scuti stars have Lambda Boötis peculiarities, since the Lambda Boötis stars are a much rarer class whose members can be found both inside and outside the delta Scuti instability strip. Lambda Boötis stars are dwarf stars that can be either spectral class A or F. Like BL Boötis-type stars they are metal-poor. Scientists have had difficulty explaining the characteristics of Lambda Boötis stars, partly because only around 60 confirmed members exist, but also due to heterogeneity in the literature. Lambda has an absolute magnitude of 1.8.\n\nThere are two dimmer F-type stars, magnitude 4.83 12 Boötis, class F8; and magnitude 4.93 45 Boötis, class F5. Xi Boötis is a G8 yellow dwarf of magnitude 4.55, and absolute magnitude is 5.5. Two dimmer G-type stars are magnitude 4.86 31 Boötis, class G8, and magnitude 4.76 44 Boötis, class G0.\n\nOf apparent magnitude 4.06, Upsilon Boötis has a spectral class of K5 and an absolute magnitude of −0.3. Dimmer than Upsilon Boötis is magnitude 4.54 Phi Boötis, with a spectral class of K2 and an absolute magnitude of −0.1. Just slightly dimmer than Phi at magnitude 4.60 is O Boötis, which, like Izar, has a spectral class of K0. O Boötis has an absolute magnitude of 0.2. The other four dim stars are magnitude 4.91 6 Boötis, class K4; magnitude 4.86 20 Boötis, class K3; magnitude 4.81 Omega Boötis, class K4; and magnitude 4.83 A Boötis, class K1.\n\nThere is one bright B-class star in Boötes; magnitude 4.93 Pi Boötis, also called Alazal. It has a spectral class of B9 and is 40 parsecs from Earth. There is also one M-type star, magnitude 4.81 34 Boötis. It is of class gM0.\n\nBesides Pulcherrima and Alkalurops, there are several other binary stars in Boötes:\n\n44 Boötis (i Boötis) is a double variable star 42 light-years away. It has an overall magnitude of 4.8 and appears yellow to the naked eye. The primary is of magnitude 5.3 and the secondary is of magnitude 6.1; their orbital period is 220 years. The secondary is itself an eclipsing variable star with a range of 0.6 magnitudes; its orbital period is 6.4 hours. It is a W Ursae Majoris variable that ranges in magnitude from a minimum of 7.1 to a maximum of 6.5 every 0.27 days. Both stars are G-type stars. Another eclipsing binary star is ZZ Boötis, which has two F2-type components of almost equal mass, and ranges in magnitude from a minimum of 6.79 to a maximum of 7.44 over a period of 5.0 days.\n\nTwo of the brighter Mira-type variable stars in the constellation are R and S Boötis. Both are red giants that range greatly in magnitude—from 6.2 to 13.1 over 223.4 days, and 7.8 to 13.8 over a period of 270.7 days respectively. Also red giants, V and W Boötis are semi-regular variable stars that range in magnitude from 7.0 to 12.0 over a period of 258 days, and magnitude 4.7 to 5.4 over 450 days, respectively.\n\nBL Boötis is the prototype of its class of pulsating variable stars, the anomalous Cepheids. These stars are somewhat similar to Cepheid variables, but they do not have the same relationship between their period and luminosity. Their periods are similar to RRAB variables; however, they are far brighter than these stars. BL Boötis is a member of the cluster NGC 5466. Anomalous Cepheids are metal poor and have masses not much larger than the Sun's, on average, . BL Boötis type stars are a subtype of RR Lyrae variables.\n\nT Boötis was a nova observed in April 1860 at a magnitude of 9.7. It has never been observed since, but that does not preclude the possibility of it being a highly irregular variable star or a recurrent nova.\n\nExtrasolar planets have been discovered encircling ten stars in Boötes as of 2012. Tau Boötis is orbited by a large planet, discovered in 1999. The host star itself is a magnitude 4.5 star of type F7V, 15.6 parsecs from Earth. It has a mass of and a radius of 1.331 solar radii (); a companion, GJ527B, orbits at a distance of 240 AU. Tau Boötis b, the sole planet discovered in the system, orbits at a distance of 0.046 AU every 3.31 days. Discovered through radial velocity measurements, it has a mass of 5.95 Jupiter masses (). This makes it a hot Jupiter. The host star and planet are tidally locked, meaning that the planet's orbit and the star's particularly high rotation are synchronized. Furthermore, a slight variability in the host star's light may be caused by magnetic interactions with the planet. Carbon monoxide is present in the planet's atmosphere. Tau Boötis b does not transit its star, rather, its orbit is inclined 46 degrees. Like Tau Boötis b, HAT-P-4 b is also a hot Jupiter. It is noted for orbiting a particularly metal-rich host star and being of low density. Discovered in 2007, HAT-P-4 b has a mass of and a radius of . It orbits every 3.05 days at a distance of 0.04 AU. HAT-P-4, the host star, is an F-type star of magnitude 11.2, 310 parsecs from Earth. It is larger than the Sun, with a mass of and a radius of .\n\nBoötes is also home to multiple-planet systems. HD 128311 is the host star for a two-planet system, consisting of HD 128311 b and HD 128311 c, discovered in 2002 and 2005, respectively. HD 128311 b is the smaller planet, with a mass of ; it was discovered through radial velocity observations. It orbits at almost the same distance as Earth, at 1.099 AU; however, its orbital period is significantly longer at 448.6 days. The larger of the two, HD 128311 c, has a mass of and was discovered in the same manner. It orbits every 919 days inclined at 50°, and is 1.76 AU from the host star. The host star, HD 128311, is a K0V-type star located 16.6 parsecs from Earth. It is smaller than the Sun, with a mass of and a radius of ; it also appears below the threshold of naked-eye visibility at an apparent magnitude of 7.51.\n\nThere are several single-planet systems in Boötes. HD 132406 is a Sun-like star of spectral type G0V with an apparent magnitude of 8.45, 231.5 light-years from Earth. It has a mass of and a radius of . The star is orbited by a gas giant, HD 132406 b, discovered in 2007. HD 132406 orbits 1.98 AU from its host star with a period of 974 days and has a mass of . The planet was discovered by the radial velocity method. WASP-23 is a star with one orbiting planet, WASP-23 b. The planet, discovered by the transit method in 2010, orbits every 2.944 very close to its Sun, at 0.0376 AU. It is smaller than Jupiter, at and . Its star is a K1V type star of apparent magnitude 12.7, far below naked-eye visibility, and smaller than the Sun at and . HD 131496 is also encircled by one planet, HD 131496 b. The star is of type K0 and is located 110 parsecs from Earth; it appears at a visual magnitude of 7.96. It is significantly larger than the Sun, with a mass of and a radius of 4.6 solar radii. Its one planet, discovered in 2011 by the radial velocity method, has a mass of ; its radius is as yet undetermined. HD 131496 b orbits at a distance of 2.09 AU with a period of 883 days.\n\nAnother single planetary system in Boötes is the HD 132563 system, a triple star system. The parent star, technically HD 132563B, is a star of magnitude 9.47, 96 parsecs from Earth. It is almost exactly the size of the Sun, with the same radius and a mass only 1% greater. Its planet, HD 132563B b, was discovered in 2011 by the radial velocity method. , it orbits 2.62 AU from its star with a period of 1544 days. Its orbit is somewhat elliptical, with an eccentricity of 0.22. HD 132563B b is one of very few planets found in triple star systems; it orbits the isolated member of the system, which is separated from the other components, a spectroscopic binary, by 400 AU. Also discovered through the radial velocity method, albeit a year earlier, is HD 136418 b, a 2-Jupiter mass planet that orbits the star HD 136418 at a distance of 1.32 AU with a period of 464.3 days. Its host star is a magnitude 7.88 G5-type star, 98.2 parsecs from Earth. It has a radius of and a mass of .\n\nWASP-14 b is one of the most massive and dense exoplanets known, with a mass of and a radius of . Discovered via the transit method, it orbits 0.036 AU from its host star with a period of 2.24 days. WASP-14 b has a density of 4.6 grams per cubic centimeter, making it one of the densest exoplanets known. Its host star, WASP-14, is an F5V-type star of magnitude 9.75, 160 parsecs from Earth. It has a radius of and a mass of . It also has a very high proportion of lithium.\n\nBoötes is in a part of the celestial sphere facing away from the plane of our home Milky Way galaxy, and so does not have open clusters or nebulae. Instead, it has one bright globular cluster and many faint galaxies. The globular cluster NGC 5466 has an overall magnitude of 9.1 and a diameter of 11 arcminutes. It is a very loose globular cluster with fairly few stars and may appear as a rich, concentrated open cluster in a telescope. NGC 5466 is classified as a Shapley-Sawyer Concentration Class 12 cluster, reflecting its sparsity. Its fairly large diameter means that it has a low surface brightness, so it appears far dimmer than the catalogued magnitude of 9.1 and requires a large amateur telescope to view. Only approximately 12 stars are resolved by an amateur instrument.\n\nBoötes has two bright galaxies. NGC 5248 (Caldwell 45) is a type Sc galaxy (a variety of spiral galaxy) of magnitude 10.2. It measures 6.5 by 4.9 arcminutes. Fifty million light-years from Earth, NGC 5248 is a member of the Virgo Cluster of galaxies; it has dim outer arms and obvious H II regions, dust lanes, and young star clusters. NGC 5676 is another type Sc galaxy of magnitude 10.9. It measures 3.9 by 2.0 arcminutes. Other galaxies include NGC 5008, a type Sc emission-line galaxy, NGC 5548, a type S Seyfert galaxy, NGC 5653, a type S HII galaxy, NGC 5778 (also classified as NGC 5825), a type E galaxy that is the brightest of its cluster, NGC 5886, and NGC 5888, a type SBb galaxy. NGC 5698 is a barred spiral galaxy, notable for being the host of the 2005 supernova SN 2005bc, which peaked at magnitude 15.3.\n\nFurther away lies the 250-million-light-year-diameter Boötes void, a huge space largely empty of galaxies. Discovered by Robert Kirshner and colleagues in 1981, it is roughly 700 million light-years from Earth. Beyond it and within the bounds of the constellation, lie two superclusters at around 830 million and 1 billion light-years distant.\n\nThe Hercules–Corona Borealis Great Wall, the largest known structure in the Universe, covers a significant part of Boötes.\n\nBoötes is home to the Quadrantid meteor shower, the most prolific annual meteor shower. It was discovered in January 1835 and named in 1864 by Alexander Hershell. The radiant is located in northern Boötes near Kappa Boötis, in its namesake former constellation of Quadrans Muralis. Quadrantid meteors are dim, but have a peak visible hourly rate of approximately 100 per hour on January 3–4. The zenithal hourly rate of the Quadrantids is approximately 130 meteors per hour at their peak; it is also a very narrow shower. The Quadrantids are notoriously difficult to observe because of a low radiant and often inclement weather. The parent body of the meteor shower has been disputed for decades; however, Peter Jenniskens has proposed 2003 EH, a minor planet, as the parent. 2003 EH may be linked to C/1490 Y, a comet previously thought to be a potential parent body for the Quadrantids. 2003 EH is a short-period comet of the Jupiter family; 500 years ago, it experienced a catastrophic breakup event. It is now dormant. The Quadrantids had notable displays in 1982, 1985, and 2004. Meteors from this shower often appear to have a blue hue and travel at a moderate speed of 41.5–43 kilometers per second.\n\nOn April 28, 1984, a remarkable outburst of the normally placid Alpha Bootids was observed by visual observer Frank Witte from 00:00 to 2:30 UTC. In a 6 cm telescope, he observed 433 meteors in a field of view near Arcturus with a diameter of less than 1°. Peter Jenniskens comments that this outburst resembled a \"typical dust trail crossing\". The Alpha Bootids normally begin on April 14, peaking on April 27 and 28, and finishing on May 12. Its meteors are slow-moving, with a velocity of 20.9 kilometers per second. They may be related to Comet 73P/Schwassmann-Wachmann 3, but this connection is only theorized.\n\nThe June Bootids, also known as the Iota Draconids, is a meteor shower associated with the comet 7P/Pons-Winnecke, first recognized on May 27, 1916, by William F. Denning. The shower, with its slow meteors, was not observed prior to 1916 because Earth did not cross the comet's dust trail until Jupiter perturbed Pons-Winnecke's orbit, causing it to come within 0.03 astronomical units of Earth's orbit the first year the June Bootids were observed. In 1982, E. A. Reznikov discovered that the 1916 outburst was caused by material released from the comet in 1819. Another outburst of the June Bootids was not observed until 1998, because Comet Pons-Winnecke's orbit was not in a favorable position. However, on June 27, 1998, an outburst of meteors radiating from Boötes, later confirmed to be associated with Pons-Winnecke, was observed. They were incredibly long-lived, with trails of the brightest meteors lasting several seconds at times. Many fireballs, green-hued trails, and even some meteors that cast shadows were observed throughout the outburst, which had a maximum zenithal hourly rate of 200–300 meteors per hour. In 2002, two Russian astronomers determined that material ejected from the comet in 1825 was responsible for the 1998 outburst. Ejecta from the comet dating to 1819, 1825, and 1830 was predicted to enter Earth's atmosphere on June 23, 2004. The predictions of a shower less spectacular than the 1998 showing were borne out in a display that had a maximum zenithal hourly rate of 16–20 meteors per hour that night. The June Bootids are not expected to have another outburst in the next 50 years. Typically, only 1–2 dim, very slow meteors are visible per hour; the average June Bootid has a magnitude of 5.0. It is related to the Alpha Draconids and the Bootids-Draconids. The shower lasts from June 27 to July 5, with a peak on the night of June 28. The June Bootids are classified as a class III shower (variable), and has an average entry velocity of 18 kilometers per second. Its radiant is located 7 degrees north of Beta Boötis.\n\nThe Beta Bootids is a weak shower that begins on January 5, peaks on January 16, and ends on January 18. Its meteors travel at 43 kilometers/second. The January Bootids is a short, young meteor shower that begins on January 9, peaks from January 16 to January 18, and ends on January 18. The Phi Bootids is another weak shower radiating from Boötes. It begins on April 16, peaks on April 30 and May 1, and ends on May 12. Its meteors are slow-moving, with a velocity of 15.1 km/s. They were discovered in 2006. The shower's peak hourly rate can be as high as 6 meteors per hour. Though named for a star in Boötes, the Phi Bootid radiant has moved into Hercules. The meteor stream is associated with three different asteroids: 1620 Geographos, 2062 Aten, and 1978 CA. The Lambda Bootids, part of the Bootid-Coronae Borealid Complex, are a weak annual shower with moderately fast meteors; 41.75 km/s. The complex includes the Lambda Bootids, as well as the Theta Coronae Borealids and Xi Coronae Borealids. All of the Bootid-Coronae Borealid showers are Jupiter family comet showers; the streams in the complex have highly inclined orbits.\n\nThere are several minor showers in Boötes, some of whose existence is yet to be verified. The Rho Bootids radiate from near the namesake star, and were hypothesized in 2010. The average Rho Bootid has an entry velocity of 43 km/s. It peaks in November and lasts for 3 days. The Rho Bootid shower is part of the SMA complex, a group of meteor showers related to the Taurids, which is in turn linked to the comet 2P/Encke. However, the link to the Taurid shower remains unconfirmed and may be a chance correlation. Another such shower is the Gamma Bootids, which were hypothesized in 2006. Gamma Bootids have an entry velocity of 50.3 km/s. The Nu Bootids, hypothesized in 2012, have faster meteors, with an entry velocity of 62.8 km/s.\n\nCitations\n\nReferences\n\n"}
{"id": "4203", "url": "https://en.wikipedia.org/wiki?curid=4203", "title": "Bernardino Ochino", "text": "Bernardino Ochino\n\nBernardino Ochino (1487–1564) was an Italian, who was raised a Roman Catholic and later turned to Protestantism.\n\nBernardino Ochino was born in Siena, the son of the barber Domenico Ochino, and at the age of 7 or 8, in around 1504, was entrusted to the order of Franciscan Friars. From 1510 he studied medicine at Perugia.\n\nAt the age of 38, Ochino transferred himself in 1534 to the newly founded Order of Friars Minor Capuchin. By then he was the close friend of Juan de Valdés, Pietro Bembo, Vittoria Colonna, Pietro Martire, Carnesecchi. In 1538 he was elected vicar-general of his order. In 1539, urged by Bembo, he visited Venice and delivered a course of sermons showing a sympathy with justification by faith, which appeared more clearly in his \"Dialogues\" published the same year. He was suspected and denounced, but nothing ensued until the establishment of the Inquisition in Rome in June 1542, at the instigation of Cardinal Giovanni Pietro Carafa. Ochino received a citation to Rome, and set out to obey it about the middle of August. According to his own statement, he was deterred from presenting himself at Rome by the warnings of Cardinal Contarini, whom he found at Bologna, dying of poison administered by the reactionary party.\n\nOchino turned aside to Florence, and after some hesitation went across the Alps to Geneva. He was cordially received by John Calvin, and published within two years several volumes of \"Prediche\", controversial tracts rationalizing his change of religion. He also addressed replies to marchioness Vittoria Colonna, Claudio Tolomei, and other Italian sympathizers who were reluctant to go to the same length as himself. His own breach with the Roman Catholic Church was final.\n\nIn 1545 Ochino became minister of the Italian Protestant congregation at Augsburg. From this time dates his contact with Caspar Schwenckfeld. He was compelled to flee when, in January 1547, the city was occupied by the imperial forces for the Diet of Augsburg.\n\nOchino found asylum in England, where he was made a prebendary of Canterbury Cathedral, received a pension from Edward VI's privy purse, and composed his major work, the \"Tragoedie or Dialoge of the unjuste usurped primacie of the Bishop of Rome\". This text, originally written in Latin, is extant only in the 1549 translation of Bishop John Ponet. The form is a series of dialogues. Lucifer, enraged at the spread of Jesus's kingdom, convokes the fiends in council, and resolves to set up the pope as antichrist. The state, represented by the emperor Phocas, is persuaded to connive at the pope's assumption of spiritual authority; the other churches are intimidated into acquiescence; Lucifer's projects seem fully accomplished, when Heaven raises up Henry VIII of England and his son for their overthrow.\n\nSeveral of Ochino's \"Prediche\" were translated into English by Anna Cooke; and he published numerous controversial treatises on the Continent.\n\nIn 1553 the accession of Mary I drove Ochino from England. He went to Basel, where Lelio Sozzini and the lawyer Martino Muralto were sent to secure Ochino as pastor of the Italian church at Zürich, which Ochino accepted. The Italian congregation there was composed mainly of refugees from Locarno. There for 10 years Ochino wrote books which gave increasing evidence of his alienation from the orthodoxy around him. The most important of these was the \"Labyrinth\", a discussion of the freedom of the will, covertly undermining the Calvinistic doctrine of predestination.\n\nIn 1563 a long simmering storm burst on Ochino with the publication of his \"Thirty Dialogues\", in one of which his adversaries maintained that he had justified polygamy under the disguise of a pretended refutation. His dialogues on divorce and against the Trinity were also considered heretical.\n\nOchino was not given opportunity to defend himself, and was banished from Zürich. After being refused admission by other Protestant cities, he directed his steps towards Poland, at that time the most tolerant state in Europe. He had not resided there long when an edict appeared (August 8, 1564) banishing all foreign dissidents. Fleeing the country, he encountered the plague at Pińczów; three of his four children were carried off; and he himself, worn out by misfortune, died in solitude and obscurity at Slavkov in Moravia, about the end of 1564.\n\nOchino's reputation among Protestants was low. He was charged by Thomas Browne in 1643 with the authorship of the legendary-apocryphal heretical treatise \"De tribus Impostoribus\", as well as with having carried his alleged approval of polygamy into practice.\n\nHis biographer Karl Benrath justified him, representing him as a fervent evangelist and at the same time as a speculative thinker with a passion for free inquiry. The picture is of Ochino always learning and unlearning and arguing out difficult questions with himself in his dialogues, frequently without attaining to any absolute conviction. \n\n\n\n"}
{"id": "4204", "url": "https://en.wikipedia.org/wiki?curid=4204", "title": "Bay of Quinte", "text": "Bay of Quinte\n\nThe Bay of Quinte is a long, narrow bay shaped like the letter \"Z\" on the northern shore of Lake Ontario in the province of Ontario, Canada. It is just west of the head of the Saint Lawrence River that drains the Great Lakes into the Gulf of Saint Lawrence. It is located about 200 kilometres east of Toronto and 400 kilometres west of Montreal.\n\nThe name \"Quinte\" is derived from \"\"Kenté\"\", which was the name of an early French Catholic mission located on the north shore of what is now Prince Edward County. Officially, in the Mohawk language, the community is called \"Kenhtè:ke\" which means \"the place of the bay\". The Cayuga name is \"Tayęda:ne:gęˀ or Detgayę:da:negęˀ\", \"land of two logs.\"\n\nThe Bay, as it is known locally, provides some of the best trophy Walleye angling in North America as well as most sport fish common to the great lakes. The bay is subject to algae blooms in late summer which are a naturally occurring phenomenon and do not indicate pollution other than from agricultural runoff. Zebra mussels as well as the other invasive species found in the great lakes are present.\n\nThe Quinte area played a vital role in bootlegging during Prohibition in the United States, with large volumes of booze being produced in the area, and shipped via boat on the Bay to Lake Ontario finally arriving in New York State where it was distributed. Illegal sales of liquor accounted for many fortunes in and around Belleville.\n\nTourism in the area is significant, especially in the summer months due to the Bay of Quinte and its fishing, local golf courses, provincial parks, and wineries.\n\nThe northern side of the bay is defined by Ontario's mainland, while the southern side follows the shore of the Prince Edward County headland. Beginning in the east with the outlet to Lake Ontario, the bay runs west-southwest for 25 kilometres to Picton (although this section is also called Adolphus Reach), where it turns north-northwest for another 20 kilometres as far as Deseronto. From there it turns south-southwest again for another 40 kilometres, running past Big Island on the south and Belleville on the north. The width of the bay rarely exceeds two kilometres. The bay ends at Trenton (Quinte West) and the Trent River, both also on the north side. The Murray Canal has been cut through the \"Carrying Place\", the few miles separating the end of the bay and Lake Ontario on the west side. The Trent River is part of the Trent-Severn Waterway, a canal connecting Lake Ontario to Lake Simcoe and then Georgian Bay on Lake Huron.\n\nThere are several sub-bays off the Bay of Quinte, including Hay Bay, Big Bay, and Muscote Bay.\n\nQuinte is also a region comprising several communities situated along the Bay of Quinte, including Belleville which is the largest city in the Quinte Region, and represents a midpoint between Montreal, Ottawa, and Toronto.\n\nThe Greater Bay of Quinte area includes the municipalities of Brighton, Quinte West, Belleville, Prince Edward County, and Greater Napanee as well as the Native Tyendinaga Mohawk Territory. Overall population of the area exceeds 200,000.\n\nThe Mohawks of the Bay of Quinte (Kenhtè:ke Kanyen'kehá:ka) on traditional Tyendinaga Mohawk Territory. Their reserve Band number 244, their current land base, is a 73 km² (18000-acre) on the Bay of Quinte in southeastern Ontario, Canada, east of Belleville and immediately to the west of Deseronto.\n\nThe community takes its name from a variant spelling of Mohawk leader Joseph Brant's traditional Mohawk name, Thayendanegea (standardized spelling Thayentiné:ken), which means 'two pieces of fire wood beside each other'. Officially, in the Mohawk language, the community is called \"Kenhtè:ke\" (Tyendinaga) which means \"on the bay\" the birthplace of Tekanawí:ta. The Cayuga name is Tyendinaga, \"Tayęda:ne:gęˀ or Detgayę:da:negęˀ\", \"land of two logs.\")\n\n\nThe Quinte Region, specifically the City of Belleville, is home to \"Loyalist College of Applied Arts and Technology.\" Other post-secondary schools in the region include; \"Maxwell College of Advanced Technology,\" \"CDI College,\" \"Ontario Business College,\" and \"Quinte Literacy.\" Secondary Schools in the region include \"Albert College\" (private school) and \"Sir James Whitney\" (a school for the deaf and severely hearing-impaired).\n\nThe Quinte Region is home to a large number of national and international food processing manufacturers. Quinte also houses a large number of industries in the plastics & packaging sector, transportation sector, logistics sector and advanced manufacturing sector, including the following (just a few of over 350 industries located in the Bay of Quinte Region) :\n\n"}
{"id": "4207", "url": "https://en.wikipedia.org/wiki?curid=4207", "title": "Bassoon", "text": "Bassoon\n\nThe bassoon is a woodwind instrument in the double reed family that typically plays music written in the bass and tenor clefs, and occasionally the treble. Appearing in its modern form in the 19th century, the bassoon figures prominently in orchestral, concert band, and chamber music literature. The bassoon is a non-transposing instrument known for its distinctive tone colour, wide range, variety of character and agility. Listeners often compare its warm, dark, reedy timbre to that of a male baritone voice. Someone who plays the bassoon is called a bassoonist.\n\nThe word bassoon comes from French \"basson\" and from Italian \"bassone\" (\"basso\" with the augmentative suffix \"-one\"). However, the Italian name for the same instrument is \"fagotto\" and in Spanish it is fagot. Fagot is a germanic word which means bundle of sticks. \n\nThe range of the bassoon begins at B (the first one below the bass staff) and extends upward over three octaves, roughly to the G above the treble staff (G). Higher notes are possible but difficult to produce, and rarely called for: orchestral and concert band parts rarely go higher than C or D. Even Stravinsky's famously difficult opening solo in \"The Rite of Spring\" only ascends to D.\n\nA is possible with a special extension to the instrument—see \"Extended techniques\" below.\n\nThe bassoon disassembles into six main pieces, including the reed. The bell (6), extending upward; the bass joint (or long joint) (5), connecting the bell and the boot; the boot (or butt) (4), at the bottom of the instrument and folding over on itself; the wing joint (or tenor joint) (3), which extends from boot to bocal; and the bocal (or crook) (2), a crooked metal tube that attaches the wing joint to a reed (1) (). Bassoons are double reed instruments like the oboe and the English horn.\n\nA modern beginner's bassoon is generally made of maple, with medium-hardness types such as sycamore maple and sugar maple preferred. Less-expensive models are also made of materials such as polypropylene and ebonite, primarily for student and outdoor use; metal bassoons were made in the past but have not been produced by any major manufacturer since 1889. The bore of the bassoon is conical, like that of the oboe and the saxophone, and the two adjoining bores of the boot joint are connected at the bottom of the instrument with a U-shaped metal connector. Both bore and tone holes are precision-machined, and each instrument is finished by hand for proper tuning. The walls of the bassoon are thicker at various points along the bore; here, the tone holes are drilled at an angle to the axis of the bore, which reduces the distance between the holes on the exterior. This ensures coverage by the fingers of the average adult hand. Wooden instruments are lined with hard rubber along the interior of the wing and boot joints to prevent damage from moisture; wooden instruments are also stained and varnished. The end of the bell is usually fitted with a ring, either of metal, plastic or ivory. The joints between sections consist of a tenon fitting into a socket; the tenons are wrapped in either cork or string as a seal against air leaks. The bocal connects the reed to the rest of the instrument and is inserted into a socket at the top of the wing joint. Bocals come in many different lengths and styles, depending on the desired tuning and playing characteristics.\n\nFolded upon itself, the bassoon stands tall, but the total sounding length is . Playing is facilitated by doubling the tube back on itself and by closing the distance between the widely spaced holes with a complex system of key work, which extends throughout nearly the entire length of the instrument. There are also short-reach bassoons made for the benefit of young or petite players.\n\nMusic historians generally consider the dulcian to be the forerunner of the modern bassoon, as the two instruments share many characteristics: a double reed fitted to a metal crook, obliquely drilled tone holes and a conical bore that doubles back on itself. The origins of the dulcian are obscure, but by the mid-16th century it was available in as many as eight different sizes, from soprano to great bass. A full consort of dulcians was a rarity; its primary function seems to have been to provide the bass in the typical wind band of the time, either loud (shawms) or soft (recorders), indicating a remarkable ability to vary dynamics to suit the need. Otherwise, dulcian technique was rather primitive, with eight finger holes and two keys, indicating that it could play in only a limited number of key signatures.\n\nThe dulcian came to be known as \"fagotto\" in Italy. However, the usual etymology that equates \"fagotto\" with \"bundle of sticks\" is somewhat misleading, as the latter term did not come into general use until later. Some think it may resemble the Roman Fasces, a standard of bound sticks with an ax. A further discrepancy lies in the fact that the dulcian was carved out of a single block of wood—in other words, a single \"stick\" and not a bundle.\n\nCircumstantial evidence indicates that the baroque bassoon was a newly invented instrument, rather than a simple modification of the old dulcian. The dulcian was not immediately supplanted, but continued to be used well into the 18th century by Bach and others. The man most likely responsible for developing the true bassoon was Martin Hotteterre (d.1712), who may also have invented the three-piece \"flûte traversière\" and the \"hautbois\" (baroque oboe). Some historians believe that sometime in the 1650s, Hotteterre conceived the bassoon in four sections (bell, bass joint, boot and wing joint), an arrangement that allowed greater accuracy in machining the bore compared to the one-piece dulcian. He also extended the compass down to B by adding two keys. An alternate view maintains Hotteterre was one of several craftsmen responsible for the development of the early bassoon. These may have included additional members of the Hotteterre family, as well as other French makers active around the same time. No original French bassoon from this period survives, but if it did, it would most likely resemble the earliest extant bassoons of Johann Christoph Denner and Richard Haka from the 1680s. Sometime around 1700, a fourth key (G♯) was added, and it was for this type of instrument that composers such as Antonio Vivaldi, Bach, and Georg Philipp Telemann wrote their demanding music. A fifth key, for the low E, was added during the first half of the 18th century. Notable makers of the 4-key and 5-key baroque bassoon include J.H. Eichentopf (c. 1678–1769), J. Poerschmann (1680–1757), Thomas Stanesby, Jr. (1668–1734), G.H. Scherer (1703–1778), and Prudent Thieriot (1732–1786).\n\nIncreasing demands on capabilities of instruments and players in the 19th century—particularly larger concert halls requiring greater volume and the rise of virtuoso composer-performers—spurred further refinement. Increased sophistication, both in manufacturing techniques and acoustical knowledge, made possible great improvements in the instrument's playability.\n\nThe modern bassoon exists in two distinct primary forms, the Buffet system and the Heckel system. Most of the world plays the Heckel system, while the Buffet system is primarily played in France, Belgium, and parts of Latin America. A number of other types of bassoons have been constructed by various instrument makers, such as the rare Galandronome.\n\nThe design of the modern bassoon owes a great deal to the performer, teacher, and composer Carl Almenräder. Assisted by the German acoustic researcher Gottfried Weber, he developed the 17-key bassoon with a range spanning four octaves. Almenräder's improvements to the bassoon began with an 1823 treatise describing ways of improving intonation, response, and technical ease of playing by augmenting and rearranging the keywork. Subsequent articles further developed his ideas. His employment at Schott gave him the freedom to construct and test instruments according to these new designs, and he published the results in \"Caecilia\", Schott's house journal. Almenräder continued publishing and building instruments until his death in 1846, and Ludwig van Beethoven himself requested one of the newly made instruments after hearing of the papers. In 1831, Almenräder left Schott to start his own factory with a partner, Johann Adam Heckel.\n\nHeckel and two generations of descendants continued to refine the bassoon, and their instruments became the standard, with other makers following. Because of their superior singing tone quality (an improvement upon one of the main drawbacks of the Almenräder instruments), the Heckel instruments competed for prominence with the reformed Wiener system, a Boehm-style bassoon, and a completely keyed instrument devised by Charles-Joseph Sax, father of Adolphe Sax. F.W. Kruspe implemented a latecomer attempt in 1893 to reform the fingering system, but it failed to catch on. Other attempts to improve the instrument included a 24-keyed model and a single-reed mouthpiece, but both these had adverse effects on tone and were abandoned.\n\nComing into the 20th century, the Heckel-style German model of bassoon dominated the field. Heckel himself had made over 1,100 instruments by the turn of the 20th century (serial numbers begin at 3,000), and the British makers' instruments were no longer desirable for the changing pitch requirements of the symphony orchestra, remaining primarily in military band use.\nExcept for a brief 1940s wartime conversion to ball bearing manufacture, the Heckel concern has produced instruments continuously to the present day. Heckel bassoons are considered by many to be the best, although a range of Heckel-style instruments is available from several other manufacturers, all with slightly different playing characteristics.\n\nBecause its mechanism is primitive compared to most modern woodwinds, makers have occasionally attempted to \"reinvent\" the bassoon. In the 1960s, Giles Brindley began to develop what he called the \"logical bassoon,\" which aimed to improve intonation and evenness of tone through use of an electrically activated mechanism, making possible key combinations too complex for the human hand to manage. Brindley's logical bassoon was never marketed.\n\nThe Buffet system bassoon achieved its basic acoustical properties somewhat earlier than the Heckel. Thereafter, it continued to develop in a more conservative manner. While the early history of the Heckel bassoon included a complete overhaul of the instrument in both acoustics and key work, the development of the Buffet system consisted primarily of incremental improvements to the key work. This minimalist approach of the Buffet deprived it of improved consistency of intonation, ease of operation, and increased power, which is found in Heckel bassoons, but the Buffet is considered by some to have a more vocal and expressive quality. The conductor John Foulds lamented in 1934 the dominance of the Heckel-style bassoon, considering them too homogeneous in sound with the horn. The modern Buffet system has 22 keys with its range being about the same as the Heckel.\n\nCompared to the Heckel bassoon, Buffet system bassoons have a narrower bore and simpler mechanism, requiring different fingerings for many notes. Switching between Heckel and Buffet requires extensive retraining. Buffet instruments are known for a reedier sound and greater facility in the upper registers, reaching e<nowiki>\"</nowiki> and f<nowiki>\"</nowiki> with far greater ease and less air resistance. French woodwind instruments' tone in general exhibits a certain amount of \"edge,\" with more of a vocal quality than is usual elsewhere, and the Buffet bassoon is no exception. This type of sound can be beneficial in music by French composers, but has drawn criticism for being too intrusive. As with all bassoons, the tone varies considerably, depending on individual instrument and performer. In the hands of a lesser player, the Heckel bassoon can sound flat and woody, but good players succeed in producing a vibrant, singing tone. Conversely, a poorly played Buffet can sound buzzy and nasal, but good players succeed in producing a warm, expressive sound, different from—but not inferior to—the Heckel.\n\nThough the United Kingdom once favored the French system, Buffet-system instruments are no longer made there and the last prominent British player of the French system retired in the 1980s. However, with continued use in some regions and its distinctive tone, the Buffet continues to have a place in modern bassoon playing, particularly in France, where it is originated. Buffet-model bassoons are currently made in Paris by Buffet Crampon and the atelier Ducasse (Romainville, France). The Selmer Company stopped fabrication of French system bassoons a few years ago. Some players, for example the late Gerald Corey in Canada, have learned to play both types and will alternate between them depending on the repertoire.\n\nOrchestras first used the bassoon to reinforce the bass line, and as the bass of the double reed choir (oboes and taille). Baroque composer Jean-Baptiste Lully and his \"Les Petits Violons\" included oboes and bassoons along with the strings in the 16-piece (later 21-piece) ensemble, as one of the first orchestras to include the newly invented double reeds. Antonio Cesti included a bassoon in his 1668 opera \"Il pomo d'oro\" (The Golden Apple). However, use of bassoons in concert orchestras was sporadic until the late 17th century when double reeds began to make their way into standard instrumentation. This was largely due to the spread of the \"hautbois\" to countries outside France. Increasing use of the bassoon as a \"basso continuo\" instrument meant that it began to be included in opera orchestras, first in France and later in Italy, Germany and England. Meanwhile, composers such as Joseph Bodin de Boismortier, Michel Corrette, Johann Ernst Galliard, Jan Dismas Zelenka, Johann Friedrich Fasch and Telemann wrote demanding solo and ensemble music for the instrument. Antonio Vivaldi brought the bassoon to prominence by featuring it in 37 concerti for the instrument.\n\nBy the mid-18th century, the bassoon's function in the orchestra was still mostly limited to that of a continuo instrument—since scores often made no specific mention of the bassoon, its use was implied, particularly if there were parts for oboes or other winds. Beginning in the early Rococo era, composers such as Joseph Haydn, Michael Haydn, Johann Christian Bach, Giovanni Battista Sammartini and Johann Stamitz included parts that exploited the bassoon for its unique color, rather than for its perfunctory ability to double the bass line. Orchestral works with fully independent parts for the bassoon would not become commonplace until the Classical era. Wolfgang Amadeus Mozart's \"Jupiter\" symphony is a prime example, with its famous bassoon solos in the first movement. The bassoons were generally paired, as in current practice, though the famed Mannheim orchestra boasted four.\n\nAnother important use of the bassoon during the Classical era was in the \"Harmonie\", a chamber ensemble consisting of pairs of oboes, horns and bassoons; later, two clarinets would be added to form an octet. The \"Harmonie\" was an ensemble maintained by German and Austrian noblemen for private music-making, and was a cost-effective alternative to a full orchestra. Haydn, Mozart, Ludwig van Beethoven and Franz Krommer all wrote considerable amounts of music for the \"Harmonie\".\n\nThe modern symphony orchestra typically calls for two bassoons, often with a third playing the contrabassoon. Some works call for four or more players. The first player is frequently called upon to perform solo passages. The bassoon's distinctive tone suits it for both plaintive, lyrical solos such as Maurice Ravel's \"Boléro\" and more comical ones, such as the grandfather's theme in \"Peter and the Wolf\". Its agility suits it for passages such as the famous running line (doubled in the violas and cellos) in the overture to \"The Marriage of Figaro\". In addition to its solo role, the bassoon is an effective bass to a woodwind choir, a bass line along with the cellos and double basses, and harmonic support along with the French horns.\n\nA wind ensemble will usually also include two bassoons and sometimes contrabassoon, each with independent parts; other types of concert wind ensembles will often have larger sections, with many players on each of first or second parts; in simpler arrangements there will be only one bassoon part and no contrabassoon. The bassoon's role in the concert band is similar to its role in the orchestra, though when scoring is thick it often cannot be heard above the brass instruments also in its range. \"La Fiesta Mexicana\", by H. Owen Reed, features the instrument prominently, as does the transcription of Malcolm Arnold's \"Four Scottish Dances\", which has become a staple of the concert band repertoire.\n\nThe bassoon is part of the standard wind quintet instrumentation, along with the flute, oboe, clarinet, and horn; it is also frequently combined in various ways with other woodwinds. Richard Strauss's \"Duet-Concertino\" pairs it with the clarinet as \"concertante\" instruments, with string orchestra in support. An ensemble known as the \"reed quintet\" also makes use of the bassoon. A reed quintet is made up of an oboe, clarinet, saxophone, bass clarinet, and bassoon.\n\nThe bassoon quartet has also gained favor in recent times. The bassoon's wide range and variety of tone colors make it well suited to grouping in a like-instrument ensemble. Peter Schickele's \"Last Tango in Bayreuth\" (after themes from \"Tristan und Isolde\") is a popular work; Schickele's fictional alter ego P. D. Q. Bach exploits the more humorous aspects with his quartet \"Lip My Reeds,\" which at one point calls for players to perform on the reed alone. It also calls for a low A at the very end of the prelude section in the fourth bassoon part. It is written so that the first bassoon does not play; instead, the player's role is to place an extension in the bell of the fourth bassoon so that the note can be played.\n\nThe bassoon is infrequently used as a jazz instrument and rarely seen in a jazz ensemble. It first began appearing in the 1920s, including specific calls for its use in Paul Whiteman's group, the unusual octets of Alec Wilder, and a few other session appearances. The next few decades saw the instrument used only sporadically, as symphonic jazz fell out of favor, but the 1960s saw artists such as Yusef Lateef and Chick Corea incorporate bassoon into their recordings. Lateef's diverse and eclectic instrumentation saw the bassoon as a natural addition (see, e.g., \"The Centaur and the Phoenix\" (1960) which features bassoon as part of a 6-man horn section, including a few solos) while Corea employed the bassoon in combination with flautist Hubert Laws.\n\nMore recently, Illinois Jacquet, Ray Pizzi, Frank Tiberi, and Marshall Allen have both doubled on bassoon in addition to their saxophone performances. Bassoonist Karen Borca, a performer of free jazz, is one of the few jazz musicians to play only bassoon; Michael Rabinowitz, the Spanish bassoonist Javier Abad, and James Lassen, an American resident in Bergen, Norway, are others. Katherine Young plays the bassoon in the ensembles of Anthony Braxton. Lindsay Cooper, Paul Hanson, the Brazilian bassoonist Alexandre Silverio, Trent Jacobs and Daniel Smith are also currently using the bassoon in jazz. French bassoonists Jean-Jacques Decreux and Alexandre Ouzounoff have both recorded jazz, exploiting the flexibility of the Buffet system instrument to good effect.\n\nThe bassoon is even rarer as a regular member of rock bands. However, several 1960s pop music hits feature the bassoon, including \"The Tears of a Clown\" by Smokey Robinson and the Miracles (the bassoonist was Charles R. Sirard), \"Jennifer Juniper\" by Donovan, \"The Turtles\" \"Happy Together\"(third verse,overdub), \"59th Street Bridge Song\" by Harpers Bizarre, and the oompah bassoon underlying The New Vaudeville Band's \"Winchester Cathedral\". From 1974 to 1978, the bassoon was played by Lindsay Cooper in the British avant-garde band Henry Cow. In the 1970s it was played, in the British medieval/progressive rock band Gryphon, by Brian Gulland, as well as by the American band Ambrosia, where it was played by drummer Burleigh Drummond. The Belgian Rock in Opposition-band Univers Zero is also known for its use of the bassoon.\n\nIn the 1990s, Madonna Wayne Gacy provided bassoon for the alternative metal band Marilyn Manson as did Aimee DeFoe, in what is self-described as \"grouchily lilting garage bassoon\" in the indie-rock band Blogurt from Pittsburgh, Pennsylvania. More recently, These New Puritans's 2010 album Hidden makes heavy use of the instrument throughout; their principal songwriter, Jack Barnett, claimed repeatedly to be \"\"writing a lot of music for bassoon\"\" in the run-up to its recording. In early 2011, American hip-hop artist Kanye West updated his Twitter account to inform followers that he recently added the bassoon to a yet unnamed song.\n\nThe rock band Better Than Ezra took their name from a passage in Ernest Hemingway's \"A Moveable Feast\" in which the author comments that listening to an annoyingly talkative person is still “better than Ezra learning how to play the bassoon,” referring to Ezra Pound.\n\nBritish psychedelic/progressive rock band Knifeworld features the bassoon playing of Chloe Herrington, who also plays for experimental chamber rock orchestra Chrome Hoof.\n\nThe bassoon is held diagonally in front of the player, but unlike the flute, oboe and clarinet, it cannot be easily supported by the player's hands alone. Some means of additional support is usually required; the most common ones are a seat strap attached to the base of the boot joint, which is laid across the chair seat prior to sitting down, or a neck strap or shoulder harness attached to the top of the boot joint. Occasionally a spike similar to those used for the cello or the bass clarinet is attached to the bottom of the boot joint and rests on the floor. It is possible to play while standing up if the player uses a neck strap or similar harness, or if the seat strap is tied to the belt. Sometimes a device called a \"balance hanger\" is used when playing in a standing position. This is installed between the instrument and the neck strap, and shifts the point of support closer to the center of gravity, adjusting the distribution of weight between the two hands.\n\nThe bassoon is played with both hands in a stationary position, the left above the right, with five main finger holes on the front of the instrument (nearest the audience) plus a sixth that is activated by an open-standing key. Five additional keys on the front are controlled by the little fingers of each hand. The back of the instrument (nearest the player) has twelve or more keys to be controlled by the thumbs, the exact number varying depending on model.\n\nTo stabilize the right hand, many bassoonists use an adjustable comma-shaped apparatus called a \"crutch,\" or a hand rest, which mounts to the boot joint. The crutch is secured with a thumb screw, which also allows the distance that it protrudes from the bassoon to be adjusted. Players rest the curve of the right hand where the thumb joins the palm against the crutch. The crutch also keeps the right hand from tiring and enables the player to keep the finger pads flat on the finger holes and keys.\n\nAn aspect of bassoon technique not found on any other woodwind is called \"flicking\". It involves the left hand thumb momentarily pressing, or 'flicking' the high A, C and D keys at the beginning of certain notes in the middle octave to achieve a clean slur from a lower note. This eliminates cracking, or brief multiphonics that happens without the use of this technique.\n\nFlicking is not universal amongst bassoonists; some American players, principally on the East Coast, use it sparingly, if at all. The rest use it virtually 100% of the time—it has become in essence part of the fingering.\n\nThe alternative method is \"venting\", which requires that the register key be used as part of the full fingering as opposed to being open momentarily at the start of the note. This is sometimes called the \"European Style.\"\n\nWhile flicking is used to higher notes, the whisper key is used for lower notes. From the A right below middle C and lower, the whisper key is pressed with the left thumb and held for the duration of the note. This prevents cracking, as low notes can sometimes crack into a higher octave. Both flicking and using the whisper key is especially important to ensure notes speak properly during slurring between high and low registers.\n\nWhile bassoons are usually critically tuned at the factory, the player nonetheless has a great degree of flexibility of pitch control through the use of breath support, embouchure, and reed profile. Players can also use alternate fingerings to adjust the pitch of many notes. Similar to other woodwind instruments, the length of the bassoon can be increased to lower pitch or decreased to raise pitch. On the bassoon, this is done preferably by changing the bocal to one of a different length, (lengths are denoted by a number on the bocal, usually starting at 0 for the shortest length, and 3 for the longest, but there are some manufacturers who will use other numbers) but it is possible to push the bocal in or out to adjust the pitch.\nThe bassoon embouchure is a very important aspect of producing a full, round, and dark bassoon tone. The bassoon embouchure is made by opening one's mouth, rolling lips inward to cover the teeth, and then dropping the jaw down as in a yawning motion (without actually yawning or opening the mouth). Both upper and lower teeth should be covered by the lips in order to protect the reed and control applied pressure. The reed is then placed in the mouth, with the lips and facial muscles maintaining an airtight seal around the reed. The upper lip will be farther forward on the reed than the lower lip, as in an \"overbite\" of the upper jaw.\n\nMany extended techniques can be performed on the bassoon, such as multiphonics, flutter-tonguing, circular breathing, double tonguing, and harmonics. In the case of the bassoon, flutter-tonguing may be accomplished by \"gargling\" in the back of the throat as well as by the conventional method of rolling Rs. Multiphonics on the bassoon can be achieved by using particular alternative fingerings.\n\nAlso, using certain fingerings, notes may be produced on the instrument that sound lower pitches than the actual range of the instrument. These \"impossible notes\" tend to sound very gravelly and out of tune, but technically sound below the low B. Alternatively, lower notes can be produced by inserting a small paper or rubber tube into the end of the bell, which converts the lower B into a lower note such as an A natural; this lowers the pitch of the instrument, but has the positive effect of bringing the lowest register (which is typically quite sharp) into tune. A notable piece that calls for the use of a low A bell is Carl Nielsen's Wind Quintet, op. 43, which includes an optional low A for the final cadence of the work. Bassoonists sometimes use the end bell segment of an English horn or clarinet if one is available instead of a specially made extension. This often yields unsatisfactory results, though, as the resultant A can be quite sharp. The idea of using low A was begun by Richard Wagner, who wanted to extend the range of the bassoon. Many passages in his later operas require the low A as well as the B-flat above. (This is impossible on a normal bassoon using an A extension as the fingering for the B-flat yields the low A.) These passages are typically realized on the contrabassoon, as recommended by the composer. Some bassoons have been made to allow bassoonists to realize similar passages. These bassoons are made with a \"Wagner bell\" which is an extended bell with a key for both the low A and the low B-flat. Bassoons with Wagner bells suffer similar intonational deficiencies as a bassoon with an A extension. Another composer who has required the bassoon to be chromatic down to low A is Gustav Mahler. Richard Strauss also calls for the low A in his opera \"Intermezzo\".\n\nThe left thumb alone operates nine keys. B, B, C, D, D, C (also B), two keys when combined create A, and the whisper key. Additional notes can be created with these keys. The D and bottom key above whisper key on the tenor joint creates both C and C. The same bottom tenor-joint key is also used, with additional fingering setup on the instrument, to create E and F. D and C together create C. When the two keys on the tenor joint to create A are used with slightly altered fingering on the boot joint, B is created. The whisper key is used throughout the instrument's register, along with other fingerings, to produce either a more muted or more piercing sound. The right thumb operates four keys. The top lever, which is very thin, is used to produce B and B, and is used in B, C, D, F, and E. The large, circular key (otherwise known as the \"pancake key\"), is used in mostly the bass register. From B to E, it is held constantly. It is also used, like the whisper key, in additional fingerings for muting the sound. For example, in Ravel's \"Boléro\", the bassoon is asked to play the ostinato on G. This is easy to perform with the normal fingering for G, but with the E key (pancake key). The next key operated by the right thumb is known as the \"spatula key\". Its primary use is to produce F and F. The bottom key is used the least often. With the combination of the backmost key of the right hand little finger it produces A and A. This is most advantageous in pieces like Dukas's \"The Sorcerer's Apprentice\". The four fingers of the left hand can each be used in at least two different positions, three in the case of the index finger. The key normally operated by this finger is primarily used for E. (Rarely it can be used as a trill key.) This key has a small hole drilled into it. The player can lift the finger completely off the key, or press the key down, closing the hole. A third possibility is to slide the finger down, so the key remains closed, but the hole is open. The middle finger typically stays on the centre hole on the tenor joint. It can also move to a lever used for E and a rarely-used trill key. The ring finger operates, on most models, one key. Some models, like a Polisi Artist bassoon, have two assignments. The upper assignment is used for alternate fingerings in the alto register. The smallest finger operates two side keys on the bass joint. The lower key is typically used for C. The upper key is used for E, E, F, F, A, B, B, C, C, and D. The four fingers of the right hand have at least one assignment each. The index finger stays over one hole, except when E is played. A side key at the top of the boot is used. The middle finger remains stationary over the hole with a ring around it. The ring, with other pads, are lifted when the smallest finger on the right hands pushes a lever. The ring finger typically remains stationary on the lower ring-finger key. However, the upper ring-finger key can be used in place of the top thumb key on the front of the boot joint. The smallest finger operates three keys. The backmost one, closest to the bassoonist, is held down throughout most of the bass register. The key is not used after F is played. F is created with this key, as well as G, B, B, and C. The lowest key for the smallest finger on the right hand is primarily used for A and , but can be used E, and F. The frontmost key is used with the bottom thumb key on the boot joint to create A and A.\n\nThe complicated fingering and the problem of reeds make the bassoon more of a challenge to learn than some of the other woodwind instruments. Cost is another big factor in a person's decision to pursue the bassoon. Prices range from $3,000 up to $250,000 for a good-quality instrument. In North America, schoolchildren typically take up bassoon only after starting on another reed instrument, such as clarinet or saxophone.\n\nStudents in America often begin to pursue the study of bassoon performance and technique in the middle years of their music education. Students are often provided with a school instrument and encouraged to pursue lessons with private instructors. Students typically receive instruction in proper posture, hand position, embouchure, tone production, and reed making.\n\nBassoon reeds, made of \"Arundo donax\" cane, are often made by the players themselves, although beginner bassoonists tend to buy their reeds from professional reed makers or use reeds made by their teachers. Reeds begin with a length of tube cane that is split into three or four pieces using a tool called a cane splitter. The cane is then trimmed and \"gouged\" to the desired thickness, leaving the bark attached. After soaking, the gouged cane is cut to the proper shape and milled to the desired thickness, or \"profiled\", by removing material from the bark side. This can be done by hand with a file; more frequently it is done with a machine or tool designed for the purpose. After the profiled cane has soaked once again it is folded over in the middle. Prior to soaking, the reed maker will have lightly scored the bark with parallel lines with a knife; this ensures that the cane will assume a cylindrical shape during the forming stage. On the bark portion, the reed maker binds on one, two, or three coils or loops of brass wire to aid in the final forming process. The exact placement of these loops can vary somewhat depending on the reed maker. The bound reed blank is then wrapped with thick cotton or linen thread to protect it, and a conical steel mandrel (which sometimes has been heated in a flame) is quickly inserted in between the blades. Using a special pair of pliers, the reed maker presses down the cane, making it conform to the shape of the mandrel. (The steam generated by the heated mandrel causes the cane to permanently assume the shape of the mandrel.) The upper portion of the cavity thus created is called the \"throat,\" and its shape has an influence on the final playing characteristics of the reed. The lower, mostly cylindrical portion will be reamed out with a special tool called a reamer, allowing the reed to fit on the bocal.\n\nAfter the reed has dried, the wires are tightened around the reed, which has shrunk after drying, or replaced completely. The lower part is sealed (a nitrocellulose-based cement such as Duco may be used) and then wrapped with thread to ensure both that no air leaks out through the bottom of the reed and that the reed maintains its shape. The wrapping itself is often sealed with Duco or clear nail varnish (polish). Electrical tape can also be used as a wrapping for amateur reed makers. The bulge in the wrapping is sometimes referred to as the \"Turk's head\"—it serves as a convenient handle when inserting the reed on the bocal.\nRecently, more players are choosing the more modern heat-shrink tubing instead of the time-consuming and fiddly thread.\n\nTo finish the reed, the end of the reed blank, originally at the center of the unfolded piece of cane, is cut off, creating an opening. The blades above the first wire are now roughly long. For the reed to play, a slight bevel must be created at the tip with a knife, although there is also a machine that can perform this function. Other adjustments with the reed knife may be necessary, depending on the hardness, the profile of the cane, and the requirements of the player. The reed opening may also need to be adjusted by squeezing either the first or second wire with the pliers. Additional material may be removed from the sides (the \"channels\") or tip to balance the reed. Additionally, if the \"e\" in the bass clef staff is sagging in pitch, it may be necessary to \"clip\" the reed by removing from its length using a pair of very sharp scissors or the equivalent.\n\nPlaying styles of individual bassoonists vary greatly; because of this, most advanced players will make their own reeds, in the process customizing them to their individual playing requirements. Many companies and individuals do offer reeds for sale, but even with store-bought reeds, players must know how to make adjustments to suit their particular playing style.\n\nThe tools used for adjusting reeds are very similar to those used to make the reeds themselves.\n\nLittle is known about the early construction of the bassoon reed, as few examples survive, and much of what is known is only what can be gathered from artistic representations. The earliest known written instructions date from the middle of the 17th century, describing the reed as being held together by wire or resined thread; the earliest actual reeds that survive are more than a century younger, a collection of 21 reeds from the late 18th-century Spanish \"bajon\".\n\n\n\n\n\n\n\nCompanies that manufacture German (Heckel system) bassoons include:\nIn addition, several factories in the People's Republic of China are producing inexpensive instruments under such labels as Laval, Haydn, and Lark, and these have been available in the West for some time now. However, they are generally of marginal quality and are usually avoided by serious players.\n\nCompanies that manufacture Buffet (French) system bassoons include:\n\n\n\n\n\n"}
{"id": "4210", "url": "https://en.wikipedia.org/wiki?curid=4210", "title": "Bipedalism", "text": "Bipedalism\n\nBipedalism is a form of terrestrial locomotion where an organism moves by means of its two rear limbs or legs. An animal or machine that usually moves in a bipedal manner is known as a biped , meaning \"two feet\" (from the Latin \"bis\" for \"double\" and \"pes\" for \"foot\"). Types of bipedal movement include walking, running, or hopping.\n\nFew modern species are habitual bipeds whose normal method of locomotion is two-legged. Within mammals, habitual bipedalism has evolved multiple times, with the macropods, kangaroo rats and mice, springhare, hopping mice, pangolins and homininan apes, as well as various other extinct groups evolving the trait independently. In the Triassic period some groups of archosaurs (a group that includes the ancestors of crocodiles) developed bipedalism; among their descendants the dinosaurs, all the early forms and many later groups were habitual or exclusive bipeds; the birds descended from one group of exclusively bipedal dinosaurs.\n\nA larger number of modern species intermittently or briefly use a bipedal gait. Several non-archosaurian lizard species move bipedally when running, usually to escape from threats. Many primate and bear species will adopt a bipedal gait in order to reach food or explore their environment. Several arboreal primate species, such as gibbons and indriids, exclusively walk on two legs during the brief periods they spend on the ground. Many animals rear up on their hind legs whilst fighting or copulating. Some animals commonly stand on their hind legs, in order to reach food, to keep watch, to threaten a competitor or predator, or to pose in courtship, but do not move bipedally.\n\nThe word is derived from the Latin words \"bi(s)\" 'two' and \"ped-\" 'foot', as contrasted with quadruped 'four feet'.\n\nLimited and exclusive bipedalism can offer a species several advantages. Bipedalism raises the head; this allows a greater field of vision with improved detection of distant dangers or resources, access to deeper water for wading animals and allows the animals to reach higher food sources with their mouths. While upright, non-locomotory limbs become free for other uses, including manipulation (in primates and rodents), flight (in birds), digging (in giant pangolin), combat (in bears, great apes and the large monitor lizard) or camouflage (in certain species of octopus). The maximum bipedal speed appears less fast than the maximum speed of quadrupedal movement with a flexible backbone – both the ostrich and the red kangaroo can reach speeds of , while the cheetah can exceed .\nEven though bipedalism is slower at first, over long distances, it has allowed humans to outrun most other animals according to the endurance running hypothesis. Bipedality in kangaroo rats has been hypothesized to improve locomotor performance, which could aid in escaping from predators.\n\nZoologists often label behaviors, including bipedalism, as \"facultative\" (i.e. optional) or \"obligate\" (the animal has no reasonable alternative). Even this distinction is not completely clear-cut — for example, humans other than infants normally walk and run in biped fashion, but almost all can crawl on hands and knees when necessary. There are even reports of humans who normally walk on all fours with their feet but not their knees on the ground, but these cases are a result of conditions such as Uner Tan syndrome — very rare genetic neurological disorders rather than normal behavior. Even if one ignores exceptions caused by some kind of injury or illness, there are many unclear cases, including the fact that \"normal\" humans can crawl on hands and knees. This article therefore avoids the terms \"facultative\" and \"obligate\", and focuses on the range of styles of locomotion \"normally\" used by various groups of animals.\n\nThere are a number of states of movement commonly associated with bipedalism.\n\n\nThe great majority of living terrestrial vertebrates are quadrupeds, with bipedalism exhibited by only a handful of living groups. Humans, gibbons and large birds walk by raising one foot at a time. On the other hand, most macropods, smaller birds, lemurs and bipedal rodents move by hopping on both legs simultaneously. Tree kangaroos are able to walk or hop, most commonly alternating feet when moving arboreally and hopping on both feet simultaneously when on the ground.\n\nThere are no known living or fossil bipedal amphibians.\n\nMany species of lizards become bipedal during high-speed, sprint locomotion, including the world's fastest lizard, the spiny-tailed iguana (genus \"Ctenosaura\").\n\nThe first known biped is the bolosaurid \"Eudibamus\" whose fossils date from 290 million years ago. Its long hindlegs, short forelegs, and distinctive joints all suggest bipedalism. The species became extinct in the early Permian.\n\nAll birds are bipeds when on the ground, a feature inherited from their dinosaur ancestors.\n\nBipedalism evolved more than once in archosaurs, the group that includes both dinosaurs and crocodilians. All dinosaurs are thought to be descended from a fully bipedal ancestor, perhaps similar to \"Eoraptor\". Bipedal movement also re-evolved in a number of other dinosaur lineages such as the iguanodons. Some extinct members of the crocodilian line, a sister group to the dinosaurs and birds, also evolved bipedal forms - a crocodile relative from the triassic, \"Effigia okeeffeae\", is thought to be bipedal. Pterosaurs were previously thought to have been bipedal, but recent trackways have all shown quadrupedal locomotion. Bipedalism also evolved independently among the dinosaurs. Dinosaurs diverged from their archosaur ancestors approximately 230 million years ago during the Middle to Late Triassic period, roughly 20 million years after the Permian-Triassic extinction event wiped out an estimated 95% of all life on Earth. Radiometric dating of fossils from the early dinosaur genus \"Eoraptor\" establishes its presence in the fossil record at this time. Paleontologists suspect \"Eoraptor\" resembles the common ancestor of all dinosaurs; if this is true, its traits suggest that the first dinosaurs were small, bipedal predators. The discovery of primitive, dinosaur-like ornithodirans such as \"Marasuchus\" and \"Lagerpeton\" in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators.\n\nA number of groups of extant mammals have independently evolved bipedalism as their main form of locomotion - for example humans, giant pangolins, the extinct giant ground sloths, numerous species of jumping rodents and macropods. Humans, as their bipedalism has been extensively studied, are documented in the next section. Macropods are believed to have evolved bipedal hopping only once in their evolution, at some time no later than 45 million years ago.\nBipedal movement is less common among mammals, most of which are quadrupedal. All primates possess some bipedal ability, though most species primarily use quadrupedal locomotion on land. Primates aside, the macropods (kangaroos, wallabies and their relatives), kangaroo rats and mice, hopping mice and springhare move bipedally by hopping. Very few mammals other than primates commonly move bipedally by an alternating gait rather than hopping. Exceptions are the ground pangolin and in some circumstances the tree kangaroo. One black bear, Pedals, became famous locally and on the internet for having a frequent bipedal gait, although this is attributed to injuries on the bear's front paws.\n\nMost bipedal animals move with their backs close to horizontal, using a long tail to balance the weight of their bodies. The primate version of bipedalism is unusual because the back is close to upright (completely upright in humans). Many primates can stand upright on their hind legs without any support. \nChimpanzees, bonobos, gibbons and baboons exhibit forms of bipedalism. On the ground sifakas move like all indrids with bipedal sideways hopping movements of the hind legs, holding their forelimbs up for balance. Geladas, although usually quadrupedal, will sometimes move between adjacent feeding patches with a squatting, shuffling bipedal form of locomotion.\n\nThe evolution of human bipedalism, began in primates about four million years ago, or as early as seven million years ago with \"Sahelanthropus\". One hypothesis for human bipedalism is that it evolved as a result of differentially successful survival from carrying food to share with group members, although there are alternative hypotheses.\n\nInjured individuals\n\nInjured chimpanzees and bonobos have been capable of sustained bipedalism.\n\nThree captive primates, one macaque Natasha and two chimps, Oliver and Poko (chimpanzee), were found to move bipedally . Natasha switched to exclusive bipedalism after an illness, while Poko was discovered in captivity in a tall, narrow cage. Oliver reverted to knuckle-walking after developing arthritis. Non-human primates often use bipedal locomotion when carrying food.\n\nOther mammals engage in limited, non-locomotory, bipedalism. A number of other animals, such as rats, raccoons, and beavers will squat on their hindlegs to manipulate some objects but revert to four limbs when moving (the beaver will move bipedally if transporting wood for their dams, as will the raccoon when holding food). Bears will fight in a bipedal stance to use their forelegs as weapons. A number of mammals will adopt a bipedal stance in specific situations such as for feeding or fighting. Ground squirrels and meerkats will stand on hind legs to survey their surroundings, but will not walk bipedally. Dogs (e.g. Faith) can stand or move on two legs if trained, or if birth defect or injury precludes quadrupedalism. The gerenuk antelope stands on its hind legs while eating from trees, as did the extinct giant ground sloth and chalicotheres. The spotted skunk will walk on its front legs when threatened, rearing up on its front legs while facing the attacker so that its anal glands, capable of spraying an offensive oil, face its attacker.\n\nBipedalism is unknown among the amphibians. Among the non-archosaur reptiles bipedalism is rare, but it is found in the 'reared-up' running of lizards such as agamids and monitor lizards. Many reptile species will also temporarily adopt bipedalism while fighting. One genus of basilisk lizard can run bipedally across the surface of water for some distance. Among arthropods, cockroaches are known to move bipedally at high speeds. Bipedalism is rarely found outside terrestrial animals, though at least two types of octopus walk bipedally on the sea floor using two of their arms, allowing the remaining arms to be used to camouflage the octopus as a mat of algae or a floating coconut.\n\nThere are at least twelve distinct hypotheses as to how and why bipedalism evolved in humans, and also some debate as to when. Bipedalism evolved well before the large human brain or the development of stone tools. Bipedal specializations are found in \"Australopithecus\" fossils from 4.2-3.9 million years ago, although \"Sahelanthropus\" may have walked on two legs as early as seven million years ago. Nonetheless, the evolution of bipedalism was accompanied by significant evolutions in the spine including the forward movement in position of the foramen magnum, where the spinal cord leaves the cranium. Recent evidence regarding modern human sexual dimorphism (physical differences between male and female) in the lumbar spine has been seen in pre-modern primates such as \"Australopithecus africanus\". This dimorphism has been seen as an evolutionary adaptation of females to bear lumbar load better during pregnancy, an adaptation that non-bipedal primates would not need to make. Adapting bipedalism would have required less shoulder stability, which allowed the shoulder and other limbs to become more independent of each other and adapt for specific suspensory behaviors. In addition to the change in shoulder stability, changing locomotion would have increased the demand for shoulder mobility, which would have propelled the evolution of bipedalism forward. The different hypotheses are not necessarily mutually exclusive and a number of selective forces may have acted together to lead to human bipedalism. It is important to distinguish between adaptations for bipedalism and adaptations for running, which came later still.\n\nNumerous causes for the evolution of human bipedalism involve freeing the hands for carrying and using tools, sexual dimorphism in provisoning, changes in climate and environment (from jungle to savanna) that favored a more elevated eye-position, and to reduce the amount of skin exposed to the tropical sun. It is possible that bipedalism provided a variety of benefits to the hominin species, and scientists have suggested multiple reasons for evolution of human bipedalism. There also is not only question of why were the earliest hominins partially bipedal but also why did hominins become more bipedal over time. For example, the postural feeding hypothesis explains for how earliest hominins became for the benefit of reaching out for food in trees while the savanna-based theory describes how the late hominins that started to settle on the ground became increasingly bipedal.\n\nNapier (1963) argued that it was very unlikely that single factor drove the evolution of Bipedalism. He stated \"\"It seems unlikely that any single factor was responsible for such a dramatic change in behaviour. In addition to the advantages of accruing from ability to carry objects - food or otherwise - the improvement of the visual range and the freeing of the hands for purposes of defence and offence must equally have played their part as catalysts.”\" Sigmon argued that chimpanzees demonstrate bipedalism in different contexts, and one single factor should be used to explain bipedalism: preadaptation for human bipedalism. Day (1986) emphasized three major pressures that drove evolution of bipedalism 1.food acquisition 2. predator avoidance 3. Reproductive success. Ko (2015) states there are two questions regarding bipedalism 1. Why were the earliest hominins partially bipedal 2. why did hominins become more bipedal over time. He argues that these questions can be answered with combination of prominent theories such as Savanna-based, Postural feeding, and Provisioning.\n\nAccording to the Savanna-based theory, hominines descended from the trees and adapted to life on the savanna by walking erect on two feet. The theory suggests that early hominids were forced to adapt to bipedal locomotion on the open savanna after they left the trees. This theory is closely related to the knuckle-walking hypothesis, which states that human ancestors used quadrupedal locomotion on the savanna, as evidenced by morphological characteristics found in \"Australopithecus anamensis\" and \"Australopithecus afarensis\" forelimbs, and that it is less parsimonious to assume that knuckle walking developed twice in genera Pan and Gorilla instead of evolving it once as synapomorphy for Pan and Gorilla before losing it in Australopithecus. The evolution of an orthograde posture would have been very helpful on a savanna as it would allow the ability to look over tall grasses in order to watch out for predators, or terrestrially hunt and sneak up on prey. It was also suggested in P.E. Wheeler's \"The evolution of bipedality and loss of functional body hair in hominids\", that a possible advantage of bipedalism in the savanna was reducing the amount of surface area of the body exposed to the sun, helping regulate body temperature. In fact, Elizabeth Vrba’s turnover pulse hypothesis supports the savanna-based theory by explaining the shrinking of forested areas due to global warming and cooling, which forced animals out into the open grasslands and caused the need for hominids to acquire bipedality.\n\nRather, the bipedal adaptation hominines had already achieved was used in the savanna. The fossil evidence reveals that early bipedal hominins were still adapted to climbing trees at the time they were also walking upright. It is possible that Bipedalism evolved in the trees, and was later applied to the savanna as a vestigial trait. Humans and orangutans are both unique to a bipedal reactive adaptation when climbing on thin branches, in which they have increased hip and knee extension in relation to the diameter of the branch, which can increase an arboreal feeding range and can be attributed to a convergent evolution of bipedalism evolving in arboreal environments. Hominine fossils found in dry grassland environments led anthropologists to believe hominines lived, slept, walked upright, and died only in those environments because no hominine fossils were found in forested areas. However, fossilization is a rare occurrence—the conditions must be just right in order for an organism that dies to become fossilized for somebody to find later, which is also a rare occurrence. The fact that no hominine fossils were found in forests does not ultimately lead to the conclusion that no hominines ever died there. The convenience of the savanna-based theory caused this point to be overlooked for over a hundred years.\n\nSome of the fossils found actually showed that there was still an adaptation to arboreal life. For example, Lucy, the famous \"Australopithecus afarensis\", found in Hadar in Ethiopia, which may have been forested at the time of Lucy’s death, had curved fingers that would still give her the ability to grasp tree branches, but she walked bipedally. “Little Foot,” the collection of \"Australopithecus africanus\" foot bones, has a divergent big toe as well as the ankle strength to walk upright. “Little Foot” could grasp things using his feet like an ape, perhaps tree branches, and he was bipedal. Ancient pollen found in the soil in the locations in which these fossils were found suggest that the area used to be much more wet and covered in thick vegetation and has only recently become the arid desert it is now.\n\nAn alternative explanation is the mixture of savanna and scattered forests increased terrestrial travel by proto-humans between clusters of trees, and bipedalism offered greater efficiency for long-distance travel between these clusters than quadrupedalism. In an experiment monitoring chimpanzee metabolic rate via oxygen consumption, it was found that the quadrupedal and bipedal energy costs were very similar, implying that this transition in early ape-like ancestors would have not have been very difficult or energetically costing. This increased travel efficiency is likely to have been selected for as it assisted the wide dispersal of early hominids across the savanna to create start populations.\n\nThe postural feeding hypothesis has been recently supported by Dr. Kevin Hunt, a professor at Indiana University. This hypothesis asserts that chimpanzees were only bipedal when they eat. While on the ground, they would reach up for fruit hanging from small trees and while in trees, bipedalism was used to reach up to grab for an overhead branch. These bipedal movements may have evolved into regular habits because they were so convenient in obtaining food. Also, Hunt's hypotheses states that these movements coevolved with chimpanzee arm-hanging, as this movement was very effective and efficient in harvesting food. When analyzing fossil anatomy, \"Australopithecus afarensis\" has very similar features of the hand and shoulder to the chimpanzee, which indicates hanging arms. Also, the \"Australopithecus\" hip and hind limb very clearly indicate bipedalism, but these fossils also indicate very inefficient locomotive movement when compared to humans. For this reason, Hunt argues that bipedalism evolved more as a terrestrial feeding posture than as a walking posture.\n\nA similar study conducted by Thorpe et al. looked at how the most arboreal great ape, the orangutan, held onto supporting branches in order to navigate branches that were too flexible or unstable otherwise. They found that in more than 75% of locomotive instances the orangutans used their hands to stabilize themselves while they navigated thinner branches. They hypothesized that increased fragmentation of forests where A. afarensis as well as other ancestors of modern humans and other apes resided could have contributed to this increase of bipedalism in order to navigate the diminishing forests. Their findings also shed light on a couple of discrepancies observed in the anatomy of A. afarensis, such as the ankle joint, which allowed it to “wobble” and long, highly flexible forelimbs. The idea that bipedalism started from walking in trees explains both the increased flexibility in the ankle as well as the long limbs which would be used to grab hold of branches.\n\nOne theory on the origin of bipedalism is the behavioral model presented by C. Owen Lovejoy, known as \"male provisioning\". Lovejoy theorizes that the evolution of bipedalism was linked to monogamy. In the face of long inter-birth intervals and low reproductive rates typical of the apes, early hominids engaged in pair-bonding that enabled greater parental effort directed towards rearing offspring. Lovejoy proposes that male provisioning of food would improve the offspring survivorship and increase the pair's reproductive rate. Thus the male would leave his mate and offspring to search for food and return carrying the food in his arms walking on his legs. This model is supported by the reduction (\"feminization\") of the male canine teeth in early hominids such as \"Sahelanthropus tchadensis\" and \"Ardipithecus ramidus\", which along with low body size dimorphism in \"Ardipithecus\" and \"Australopithecus\", suggests a reduction in inter-male antagonistic behavior in early hominids. In addition, this model is supported by a number of modern human traits associated with concealed ovulation (permanently enlarged breasts, lack of sexual swelling) and low sperm competition (moderate sized testes, low sperm mid-piece volume) that argues against recent adaptation to a polygynous reproductive system.\n\nHowever, this model has generated some controversy, as others have argued that early bipedal hominids were instead polygynous. Among most monogamous primates, males and females are about the same size. That is sexual dimorphism is minimal, and other studies have suggested that Australopithecus afarensis males were nearly twice the weight of females. However, Lovejoy's model posits that the larger range a provisioning male would have to cover (to avoid competing with the female for resources she could attain herself) would select for increased male body size to limit predation risk. Furthermore, as the species became more bipedal, specialized feet would prevent the infant from conveniently clinging to the mother - hampering the mother's freedom and thus make her and her offspring more dependent on resources collected by others. Modern monogamous primates such as gibbons tend to be also territorial, but fossil evidence indicates that \"Australopithecus afarensis\" lived in large groups. However, while both gibbons and hominids have reduced canine sexual dimorphism, female gibbons enlarge ('masculinize') their canines so they can actively share in the defense of their home territory. Instead, the reduction of the male hominid canine is consistent with reduced inter-male aggression in a group living primate.\n\nRecent studies of 4.4 million years old \"Ardipithecus ramidus\" suggest bipedalism, it is thus possible that bipedalism evolved very early in homininae and was reduced in chimpanzee and gorilla when they became more specialized. According to Richard Dawkins in his book \"The Ancestor's Tale\", chimps and bonobos are descended from \"Australopithecus\" gracile type species while gorillas are descended from Paranthropus. These apes may have once been bipedal, but then lost this ability when they were forced back into an arboreal habitat, presumably by those australopithecines from whom eventually evolved hominins. Early homininaes such as \"Ardipithecus ramidus\" may have possessed an arboreal type of bipedalism that later independently evolved towards knuckle-walking in chimpanzees and gorillas and towards efficient walking and running in modern humans (see figure). It is also proposed that one cause of Neanderthal extinction was a less efficient running.\n\nJoseph Jordania from the University of Melbourne recently (2011) suggested that bipedalism was one of the central elements of the general defense strategy of early hominids, based on aposematism, or warning display and intimidation of potential predators and competitors with exaggerated visual and audio signals. According to this model, hominids were trying to stay as visible and as loud as possible all the time. Several morphological and behavioral developments were employed to achieve this goal: upright bipedal posture, longer legs, long tightly coiled hair on the top of the head, body painting, threatening synchronous body movements, loud voice and extremely loud rhythmic singing/stomping/drumming on external subjects. Slow locomotion and strong body odor (both characteristic for hominids and humans) are other features often employed by aposematic species to advertise their non-profitability for potential predators.\n\nThere are a variety of ideas which promote a specific change in behaviour as the key driver for the evolution of hominid bipedalism. For example, Wescott (1967) and later Jablonski & Chaplin (1993) suggest that bipedal threat displays could have been the transitional behaviour which led to some groups of apes beginning to adopt bipedal postures more often. Others (\"e.g.\" Dart 1925) have offered the idea that the need for more vigilance against predators could have provided the initial motivation. Dawkins (\"e.g.\" 2004) has argued that it could have begun as a kind of fashion that just caught on and then escalated through sexual selection. And it has even been suggested (\"e.g.\" Tanner 1981:165) that male phallic display could have been the initial incentive, as well as increased sexual signaling in upright female posture.\n\nThe thermoregulatory model explaining the origin of bipedalism is one of the simplest theories so far advanced, but it is a viable explanation. Dr. Peter Wheeler, a professor of evolutionary biology, proposes that bipedalism raises the amount of body surface area higher above the ground which results in a reduction in heat gain and helps heat dissipation. When a hominid is higher above the ground, the organism accesses more favorable wind speeds and temperatures. During heat seasons, greater wind flow results in a higher heat loss, which makes the organism more comfortable. Also, Wheeler explains that a vertical posture minimizes the direct exposure to the sun whereas quadrupedalism exposes more of the body to direct exposure. Analysis and interpretations of Ardipithecus reveal that this hypothesis needs modification to consider that the forest and woodland environmental preadaptation of early-stage hominid bipedalism preceded further refinement of bipedalism by the pressure of natural selection. This then allowed for the more efficient exploitation of the hotter conditions ecological niche, rather than the hotter conditions being hypothetically bipedalism's initial stimulus. A feedback mechanism from the advantages of bipedality in hot and open habitats would then in turn make a forest preadaptation solidify as a permanent state.\n\nCharles Darwin wrote that \"Man could not have attained his present dominant position in the world without the use of his hands, which are so admirably adapted to the act of obedience of his will\". Darwin (1871:52) and many models on bipedal origins are based on this line of thought. Gordon Hewes (1961) suggested that the carrying of meat \"over considerable distances\" (Hewes 1961:689) was the key factor. Isaac (1978) and Sinclair et al. (1986) offered modifications of this idea, as indeed did Lovejoy (1981) with his \"provisioning model\" described above. Others, such as Nancy Tanner (1981), have suggested that infant carrying was key, while others again have suggested stone tools and weapons drove the change. This stone-tools theory is very unlikely, as though ancient humans were known to hunt, the discovery of tools was not discovered for thousands of years after the origin of bipedalism, chronologically precluding it from being a driving force of evolution. (Wooden tools and spears fossilize poorly and therefore it is difficult to make a judgment about their potential usage.)\n\nThe observation that large primates, including especially the great apes, that predominantly move quadrupedally on dry land, tend to switch to bipedal locomotion in waist deep water, has led to the idea that the origin of human bipedalism may have been influenced by waterside environments. This idea, labelled \"the wading hypothesis\", was originally suggested by the Oxford marine biologist Alister Hardy who said: \"It seems to me likely that Man learnt to stand erect first in water and then, as his balance improved, he found he became better equipped for standing up on the shore when he came out, and indeed also for running.\" It was then promoted by Elaine Morgan, as part of the aquatic ape hypothesis, who cited bipedalism among a cluster of other human traits unique among primates, including voluntary control of breathing, hairlessness and subcutaneous fat. The \"aquatic ape hypothesis\", as originally formulated, has not been accepted or considered a serious theory within the anthropological scholarly community. Others, however, have sought to promote wading as a factor in the origin of human bipedalism without referring to further (\"aquatic ape\" related) factors. Since 2000 Carsten Niemitz has published a series of papers and a book on a variant of the wading hypothesis, which he calls the \"amphibian generalist theory\" ().\n\nOther theories have been proposed that suggest wading and the exploitation of aquatic food sources (providing essential nutrients for human brain evolution or critical fallback foods) may have exerted evolutionary pressures on human ancestors promoting adaptations which later assisted full-time bipedalism. It has also been thought that consistent water-based food sources had developed early hominid dependency and facilitated dispersal along seas and rivers.\n\nBipedal movement occurs in a number of ways, and requires many mechanical and neurological adaptations. Some of these are described below.\n\nEnergy-efficient means of standing bipedally involve constant adjustment of balance, and of course these must avoid overcorrection. The difficulties associated with simple standing in upright humans are highlighted by the greatly increased risk of falling present in the elderly, even with minimal reductions in control system effectiveness.\n\nShoulder stability would decrease with the evolution of bipedalism. Shoulder mobility would increase because the need for a stable shoulder is only present in arboreal habitats. Shoulder mobility would support suspensory locomotion behaviors which are present in human bipedalism. The forelimbs are freed from weight bearing capabilities which makes the shoulder a place of evidence for the evolution of bipedalism.\n\nWalking is characterized by an \"inverted pendulum\" movement in which the center of gravity vaults over a stiff leg with each step. Force plates can be used to quantify the whole-body kinetic & potential energy, with walking displaying an out-of-phase relationship indicating exchange between the two. Interestingly, this model applies to all walking organisms regardless of the number of legs, and thus bipedal locomotion does not differ in terms of whole-body kinetics.\n\nIn humans, walking is composed of several separate processes:\n\nRunning is characterized by a spring-mass movement. Kinetic and potential energy are in phase, and the energy is stored & released from a spring-like limb during foot contact. Again, the whole-body kinetics are similar to animals with more limbs.\n\nBipedalism requires strong leg muscles, particularly in the thighs. Contrast in domesticated poultry the well muscled legs, against the small and bony wings. Likewise in humans, the quadriceps and hamstring muscles of the thigh are both so crucial to bipedal activities that each alone is much larger than the well-developed biceps of the arms.\n\nA biped has the ability to breathe while running, without strong coupling to stride cycle. Humans usually take a breath every other stride when their aerobic system is functioning. During a sprint the anaerobic system kicks in and breathing slows until the anaerobic system can no longer sustain a sprint.\n\nFor nearly the whole of the 20th century, bipedal robots were very difficult to construct and robot locomotion involved only wheels, treads, or multiple legs. Recent cheap and compact computing power has made two-legged robots more feasible. Some notable biped robots are ASIMO, HUBO, MABEL and QRIO. Recently, spurred by the success of creating a fully passive, un-powered bipedal walking robot, those working on such machines have begun using principles gleaned from the study of human and animal locomotion, which often relies on passive mechanisms to minimize power consumption.\n\n\n\n"}
{"id": "4211", "url": "https://en.wikipedia.org/wiki?curid=4211", "title": "Bootstrapping", "text": "Bootstrapping\n\nIn general, bootstrapping usually refers to a self-starting process that is supposed to proceed without external input. In computer technology the term (usually shortened to booting) usually refers to the process of loading the basic software into the memory of a computer after power-on or general reset, especially the operating system which will then take care of loading other software as needed.\n\nThe term appears to have originated in the early 19 century United States (particularly in the phrase \"pull oneself over a fence by one's bootstraps\"), to mean an absurdly impossible action, an adynaton.\n\nTall boots may have a tab, loop or handle at the top known as a bootstrap, allowing one to use fingers or a boot hook tool to help pulling the boots on. The saying \"to pull oneself up by one's bootstraps\" was already in use during the 19 century as an example of an impossible task. The idiom dates at least to 1834, when it appeared in the \"Workingman's Advocate\": \"It is conjectured that Mr. Murphee will now be enabled to hand himself over the Cumberland river or a barn yard fence by the straps of his boots.\" In 1860 it appeared in a comment on philosophy of mind: \"The attempt of the mind to analyze itself [is] an effort analogous to one who would lift himself by his own bootstraps.\" Bootstrap as a metaphor, meaning to better oneself by one's own unaided efforts, was in use in 1922. This metaphor spawned additional metaphors for a series of self-sustaining processes that proceed without external help.\n\nThe term is sometimes attributed to a story in Rudolf Erich Raspe's \"\", but in that story Baron Munchausen pulls himself (and his horse) out of a swamp by his hair (specifically, his pigtail), not by his bootstraps and no explicit reference to bootstraps has been found elsewhere in the various versions of the Munchausen tales.\n\nBooting is the process of starting a computer, specifically with regard to starting its software. The process involves a chain of stages, in which at each stage a smaller, simpler program loads and then executes the larger, more complicated program of the next stage. It is in this sense that the computer \"pulls itself up by its bootstraps\", \"i.e.\" it improves itself by its own efforts. Booting is a chain of events that starts with execution of hardware-based procedures and may then hand-off to firmware and software which is loaded into main memory. Booting often involves processes such as performing self-tests, loading configuration settings, loading a BIOS, resident monitors, a hypervisor, an operating system, or utility software.\n\nThe computer term bootstrap began as a metaphor in the 1950s. In computers, pressing a bootstrap button caused a hardwired program to read a bootstrap program from an input unit. The computer would then execute the bootstrap program, which caused it to read more program instructions. It became a self-sustaining process that proceeded without external help from manually entered instructions. As a computing term, bootstrap has been used since at least 1953.\n\nBootstrapping can also refer to the development of successively more complex, faster programming environments. The simplest environment will be, perhaps, a very basic text editor (\"e.g.\", ed) and an assembler program. Using these tools, one can write a more complex text editor, and a simple compiler for a higher-level language and so on, until one can have a graphical IDE and an extremely high-level programming language.\n\nHistorically, bootstrapping also refers to an early technique for computer program development on new hardware. The technique described in this paragraph has been replaced by the use of a cross compiler executed by a pre-existing computer. Bootstrapping in program development began during the 1950s when each program was constructed on paper in decimal code or in binary code, bit by bit (1s and 0s), because there was no high-level computer language, no compiler, no assembler, and no linker. A tiny assembler program was hand-coded for a new computer (for example the IBM 650) which converted a few instructions into binary or decimal code: A1. This simple assembler program was then rewritten in its just-defined assembly language but with extensions that would enable the use of some additional mnemonics for more complex operation codes. The enhanced assembler's source program was then assembled by its predecessor's executable (A1) into binary or decimal code to give A2, and the cycle repeated (now with those enhancements available), until the entire instruction set was coded, branch addresses were automatically calculated, and other conveniences (such as conditional assembly, macros, optimisations, etc.) established. This was how the early assembly program SOAP (Symbolic Optimal Assembly Program) was developed. Compilers, linkers, loaders, and utilities were then coded in assembly language, further continuing the bootstrapping process of developing complex software systems by using simpler software.\n\nThe term was also championed by Doug Engelbart to refer to his belief that organizations could better evolve by improving the process they use for improvement (thus obtaining a compounding effect over time). His SRI team that developed the NLS hypertext system applied this strategy by using the tool they had developed to improve the tool.\n\nThe development of compilers for new programming languages first developed in an existing language but then rewritten in the new language and compiled by itself, is another example of the bootstrapping notion. Using an existing language to bootstrap a new language is one way to solve the \"chicken or the egg\" causality dilemma.\n\nDuring the installation of computer programs it is sometimes necessary to update the installer or package manager itself. The common pattern for this is to use a small executable bootstrapper file (\"e.g.\" setup.exe) which updates the installer and starts the real installation after the update. Sometimes the bootstrapper also installs other prerequisites for the software during the bootstrapping process.\n\nA bootstrapping node, also known as a rendezvous host, is a node in an overlay network that provides initial configuration information to newly joining nodes so that they may successfully join the overlay network.\n\nA type of computer simulation called discrete event simulation represents the operation of a system as a chronological sequence of events. A technique called \"bootstrapping the simulation model\" is used, which bootstraps initial data points using a pseudorandom number generator to schedule an initial set of pending events, which schedule additional events, and with time, the distribution of event times approaches its steady state—the bootstrapping behavior is overwhelmed by steady-state behavior.\n\nBootstrapping is a technique used to iteratively improve a classifier's performance. Seed AI is a hypothesized type of artificial intelligence capable of recursive self-improvement. Having improved itself, it would become better at improving itself, potentially leading to an exponential increase in intelligence. No such AI is known to exist, but it remains an active field of research.\n\nSeed AI is a significant part of some theories about the technological singularity: proponents believe that the development of seed AI will rapidly yield ever-smarter intelligence (via bootstrapping) and thus a new era.\n\nBootstrapping is a resampling technique used to obtain estimates of summary statistics.\n\nBootstrapping in business means starting a business without external help or capital. Such startups fund the development of their company through internal cash flow and are cautious with their expenses. Generally at the start of a venture, a small amount of money will be set aside for the bootstrap process. Bootstrapping can also be a supplement for econometric models. Bootstrapping was also expanded upon in the book \"Bootstrap Business\" by Richard Christiansen, the Harvard Business Review article \"The Art of Bootstrapping\" and the follow-up book \"The Origin and Evolution of New Businesses\" by Amar Bhide.\n\n\nRichard Dawkins in his book \"River Out of Eden\" used the computer bootstrapping concept to explain how biological cells differentiate: \"Different cells receive different combinations of chemicals, which switch on different combinations of genes, and some genes work to switch other genes on or off. And so the bootstrapping continues, until we have the full repertoire of different kinds of cells.\"\n\nBootstrapping analysis gives a way to judge the strength of support for clades on phylogenetic trees. A number is written by a node, which reflects the percentage of bootstrap trees which also resolve the clade at the endpoints of that branch.\n\nBootstrapping is a rule preventing the admission of hearsay evidence in conspiracy cases.\n\nBootstrapping is a theory of language acquisition.\n\nBootstrapping is using very general consistency criteria to determine the form of a quantum theory from some assumptions on the spectrum of particles or operators.\n\nIn tokamak fusion devices, bootstrapping refers to the process in which a bootstrap current is self-generated by the plasma, which reduces or eliminates the need for an external current driver. Maximising the bootstrap current is a major goal of advanced tokamak designs.\n\nBootstrapping in Inertial Confinement Fusion refers to the alpha particles produced in the fusion reaction providing further heating to the plasma. This heating leads to ignition and an overall energy gain.\n\nBootstrapping is a form of positive feedback in analog circuit design.\n\nAn electric power grid is almost never brought down intentionally. Generators and power stations are started and shut down as necessary. A typical power station requires power for start up prior to being able to generate power. This power is obtained from the grid, so if the entire grid is down these stations cannot be started.\n\nTherefore, to get a grid started, there must be at least a small number of power stations that can start entirely on their own. A black start is the process of restoring a power station to operation without relying on external power. In the absence of grid power, one or more black starts are used to bootstrap the grid.\n\nA Bootstrapping Server Function (BSF) is an intermediary element in cellular networks which provides application independent functions for mutual authentication of user equipment and servers unknown to each other and for 'bootstrapping' the exchange of secret session keys afterwards. The term 'bootstrapping' is related to building a security relation with a previously unknown device first and to allow installing security elements (keys) in the device and the BSF afterwards.\n\nA media bootstrap is the process whereby a story or meme is deliberately (but artificially) produced by self and peer-referential journalism, originally within a tight circle of media content originators, often commencing with stories written within the same media organization. This story is then expanded into a general media \"accepted wisdom\" with the aim of having it accepted as self-evident \"common knowledge\" by the reading, listening and viewing publics. The key feature of a media bootstrap is that as little hard, verifiable, external evidence as possible is used to support the story, preference being given to the citation (often unattributed) of other media stories, \"i.e.\" \"journalists interviewing journalists\".\n\nBecause the campaign is usually originated and at least initially concocted internally by a media organization with a particular agenda in mind, within a closed loop of reportage and opinionation, the campaign is said to have \"pulled itself up by its own bootstraps\".\n\nA bootstrap campaign should be distinguished from a genuine news story of genuine interest, such as a natural disaster that kills thousands, or the death of a respected public figure. It is legitimate for these stories to be given coverage across all media platforms. What distinguishes a bootstrap from a real story is the contrived and organized manner in which the bootstrap appears to come out of nowhere. A bootstrap commonly claims to be tapping a hitherto unrecognized phenomenon within society.\n\nAs self-levitating by pulling on one's bootstraps is physically impossible, this is often used by the bootstrappers themselves to deny the possibility that the bootstrap campaign is indeed concocted and artificial. They assert that it has arisen via a groundswell of public opinion. Media campaigns that are openly admitted as concocted (\"e.g.\" a public service campaign titled \"Let's Clean Up Our City\") are usually ignored by other media organizations for reasons related to competition. On the other hand, the true bootstrap welcomes the participation of other media organizations, indeed encourages it, as this participation gains the bootstrap notoriety and, most importantly, legitimacy.\n\n\n"}
{"id": "4213", "url": "https://en.wikipedia.org/wiki?curid=4213", "title": "Baltic languages", "text": "Baltic languages\n\nThe Baltic languages belong to the Balto-Slavic branch of the Indo-European language family. Baltic languages are spoken by the Balts, mainly in areas extending east and southeast of the Baltic Sea in Northern Europe.\n\nScholars usually regard them as a single language family divided into two groups: Western Baltic (containing only extinct languages), and Eastern Baltic (containing two living languages, Lithuanian and Latvian). The range of the Eastern Baltic linguistic influence once possibly reached as far as the Ural mountains, but this hypothesis has been questioned.\n\nOld Prussian, a Western Baltic language that became extinct in the 18th century, ranks as the most archaic of the Baltic languages.\n\nAlthough morphologically related, the Lithuanian, the Latvian, and particularly the Old Prussian vocabularies differ substantially from one another, and as such they are not mutually intelligible, mainly due to a substantial number of false friends, and foreign words, borrowed from surrounding language families, which are used differently.\n\nThe Baltic languages are generally thought to form a single family with two branches, Eastern and Western. However, these two branches are sometimes classified as independent branches of Balto-Slavic.\n\n\n\"(\"†\"—Extinct language)\"\n\nThe Baltic-speaking peoples likely encompassed an area in Eastern Europe much larger than their modern range: as in the case of the Celtic languages of Western Europe, they were reduced with invasions, exterminations and assimilations. Studies in comparative linguistics point to genetic relationship between the languages of the Baltic family and the following extinct languages:\nThe Baltic classification of Dacian and Thracian has been proposed by the Lithuanian scientist Jonas Basanavičius, who insisted this is the most important work of his life and listed 600 identical words of Balts and Thracians. He also theoretically included Phrygian in the related group, but this did not found subsequent support, but disapprovement among other authors, such as the linguistic analysis of Ivan Duridanov, which found Phrygian completely lacking parallels in either Thracian or Baltic languages.\n\nThe Bulgarian linguist Ivan Duridanov, who improved the most extensive list of toponyms, in his first publication claimed that Thracian is genetically linked to the Baltic languages and in the next one he made the following classification:\"\"The Thracian language formed a close group with the Baltic (resp. Balto-Slavic), the Dacian and the \"Pelasgian\" languages. More distant were its relations with the other Indo-European languages, and especially with Greek, the Italic and Celtic languages, which exhibit only isolated phonetic similarities with Thracian; the Tokharian and the Hittite were also distant. \"\"\nOf about 200 reconstructed Thracian words by Duridanov most cognates (138) appear in the Baltic languages, mostly in Lithuanian, followed by Germanic (61), Indo-Aryan (41), Greek (36), Bulgarian (23), Latin (10) and Albanian (8). The cognates of the reconstructed Dacian words in his publication are found mostly in the Baltic languages, followed by Albanian. Parallels have enabled linguists, using the techniques of comparative linguistics, to decipher the meanings of several Dacian and Thracian placenames with, they claim, a high degree of probability. Of 74 Dacian placenames attested in primary sources and considered by Duridanov, a total of 62 have Baltic cognates, most of which were rated \"certain\" by Duridanov. For a big number of 300 Thracian geographic names most parallels were found between Thracian and Baltic geographic names in the study of Duridanov. According to him the most important impression make the geographic cognates of Baltic and Thracian \"\"the similarity of these parallels stretching frequently on the main element and the suffix simultaneously, which makes a strong impression\"\". Some scholars, including Toparov, have found many linguistic similarities between Baltic and ancient Balkan languages pointing to the many close parallels between Dacian and Thracian placenames and those of the Baltic language-zone. A number of possible parallels of the Thracian ethnonym in Lithuania were not found in any other country, including the former capital Trakai (cf. Lith. Trakai, meaning \"Thrcians\"), Trakininkai, Trakiškiai, Trakiškiemiai, Traksėda and others. Other Slavic authors noted that Dacian and Thracian have much in common with Baltic onomastics and explicitly not in any similar way with Slavic onomastics, including cognates and parallels of lexical isoglosses, which implies a recent common ancestor.\n\nAfter creating a list of names of rivers and personal names with a high number of parallels, the Romanian linguist Mircea M. Radulescu classified the Daco-Moesian and Thracian as Baltic languages expanding to the south and also proposed such classification for Illyrian. The German linguist Schall attributed a South Baltic classification to the Dacian language. The Venezualian-Lithuanian historian Jurate Rosales classifies Dacian and Thracian as Baltic languages. The Czech archaeologist Kristian Turnvvald classified such languages as Danubian Baltic.\n\nThe American linguist Harvey Mayer refers to both Dacian and Thracian as Baltic languages. Mayer claims that he extracted an unambiguous evidence for regarding Dacian and Thracian as more tied to Lithuanian than to Latvian. In his first publication he claims to have sufficient evidence for classifying them as Baltoidic or at least \"Baltic-like,\" if not exactly, Baltic dialects or languages still maintaining a genetic link within a common language family. In the next and final publication he refers to them as South or East Baltic languages and classifies Dacians and Thracians as \"Balts by extension\". \"\"Finally, I label Thracian and Dacian as East Baltic...The fitting of special Dacian and Thracian features (which I identified from Duridanov’s listings) into Baltic isogloss patterns so that I identified Dacian and Thracian as southeast Baltic. South Baltic because, like Old Prussian, they keep unchanged the diphthongs ei, ai, en, an (north Baltic Lithuanian and Latvian show varying percentages of ei, ai to ie, and en, an to ę, ą (to ē, ā) in Lithuanian, to ie, uo in Latvian). East Baltic because the Dacian word žuvete (now in Rumanian spelled juvete) has ž, not z as in west Baltic, and the Thracian word pušis (the Latin-Greek transcription shows pousis which, I believe, reflects -š-.) with zero grade puš- as in Lithuanian pušìs rather than with e-grade *peuš- as in Prussian peusē. Zero grade in this word is east Baltic, e-grade here is west Baltic, while the other word for “pine, evergreen”, preidē (Prussian and Dacian), priede (Latvian), is marginal in Lithuanian matched by no *peus- in Latvian.\"\" In regards to other languages, he claims that Albanian is a descendant of Illyrian and escaped any heavy Baltic influence of Thracian.\n\nThe Baltic languages are of particular interest to linguists because they retain many archaic features, which are believed to have been present in the early stages of the Proto-Indo-European language. However, linguists have had a hard time establishing the precise relationship of the Baltic languages to other languages in the Indo-European family. Several of the extinct Baltic languages have a limited or nonexistent written record, their existence being known only from the records of ancient historians and personal or place names. All of the languages in the Baltic group (including the living ones) were first written down relatively late in their probable existence as distinct languages. These two factors combined with others have obscured the history of the Baltic languages, leading to a number of theories regarding their position in the Indo-European family.\n\nThe Baltic languages show a close relationship with the Slavic languages, and are grouped with them in a Balto-Slavic family by most scholars. This family is considered to have developed from a common ancestor, Proto-Balto-Slavic. Later on, several lexical, phonological and morphological dialectisms developed, separating the various Balto-Slavic languages from each other. Although it is generally agreed that the Slavic languages developed from a single more-or-less unified dialect (Proto-Slavic) that split off from common Balto-Slavic, there is more disagreement about the relationship between the Baltic languages.\n\nThe traditional view is that the Balto-Slavic languages split into two branches, Baltic and Slavic, with each branch developing as a single common language (Proto-Baltic and Proto-Slavic) for some time afterwards. Proto-Baltic is then thought to have split into East Baltic and West Baltic branches. However, more recent scholarship has suggested that there was no unified Proto-Baltic stage, but that Proto-Balto-Slavic split directly into three groups: Slavic, East Baltic and West Baltic. Under this view, the Baltic family is paraphyletic, and consists of all Balto-Slavic languages that are not Slavic. This would imply that Proto-Baltic, the last common ancestor of all Baltic languages, would be identical to Proto-Balto-Slavic itself, rather than distinct from it. In the 1960's Vladimir Toporov and Vyacheslav Ivanov made the following conclusions about the relationship between the Baltic and Slavic languages: a) Proto-Slavic language formed from the peripheral-type Baltic dialects; b) Slavic linguistic type formed later from the Baltic languages structural model; c) the Slavic structural model is a result of the Baltic languages structural model transformation. These scholars’ theses do not contradict the Baltic and Slavic languages closeness and from a historical perspective specify the Baltic-Slavic languages evolution.\n\nFinally, there is a minority of scholars who argue that Baltic descended directly from Proto-Indo-European, without an intermediate common Balto-Slavic stage. They argue that the many similarities and shared innovations between Baltic and Slavic are due to several millennia of contact between the groups, rather than shared heritage.\n\nSpeakers of modern Baltic languages are generally concentrated within the borders of Lithuania and Latvia, and in emigrant communities in the United States, Canada, Australia and the countries within the former borders of the Soviet Union.\n\nHistorically the languages were spoken over a larger area: west to the mouth of the Vistula river in present-day Poland, at least as far east as the Dniepr river in present-day Belarus, perhaps even to Moscow, and perhaps as far south as Kiev. Key evidence of Baltic language presence in these regions is found in hydronyms (names of bodies of water) that are characteristically Baltic. The use of hydronyms is generally accepted to determine the extent of a culture's influence, but \"not\" the date of such influence.\n\nThe eventual expansion of the use of Slavic languages in the south and east, and Germanic languages in the west, reduced the geographic distribution of Baltic languages to a fraction of the area that they formerly covered. The Russian geneticist Oleg Balanovsky speculated that there is a predominance of the assimilated pre-Slavic substrate in the genetics of East and West Slavic populations, according to him the common genetic structure which contrasts East Slavs and Balts frоm other populations may suggest that the pre-Slavic substrate of the East Slavs consists most significantly of Baltic-speakers, which predated the Slavs in the cultures of the Eurasian steppe according to archaeological references he cites.\n\nThough included among the Baltic states due to its location, the language of Estonia, Estonian, is a Uralic language and is not related to the Baltic languages, which are Indo-European.\n\nIt is believed that the Baltic languages are among the most archaic of the currently remaining Indo-European languages, despite their late attestation.\n\nAlthough the various Baltic tribes were mentioned by ancient historians as early as 98 B.C., the first attestation of a Baltic language was about 1350, with the creation of the \"Elbing Prussian Vocabulary\", a German to Prussian translation dictionary. Lithuanian was first attested in a hymnal translation in 1545; the first printed book in Lithuanian, a Catechism by Martynas Mažvydas was published in 1547 in Königsberg, Prussia (now Kaliningrad, Russia). Latvian appeared in a hymnal in 1530 and in a printed Catechism in 1585.\n\nOne reason for the late attestation is that the Baltic peoples resisted Christianization longer than any other Europeans, which delayed the introduction of writing and isolated their languages from outside influence.\n\nWith the establishment of a German state in Prussia, and the eradication or flight of much of the Baltic Prussian population in the 13th century, the remaining Prussians began to be assimilated, and by the end of the 17th century, the Prussian language had become extinct.\n\nDuring the years of the Polish–Lithuanian Commonwealth (1569–1795), official documents were written in Polish, Ruthenian and Latin.\n\nAfter the Partitions of Commonwealth, most of the Baltic lands were under the rule of the Russian Empire, where the native languages or alphabets were sometimes prohibited from being written down or used publicly in a Russification effort (see Lithuanian press ban for the ban in force from 1865 to 1904).\n\n\n\n"}
{"id": "4214", "url": "https://en.wikipedia.org/wiki?curid=4214", "title": "Bioinformatics", "text": "Bioinformatics\n\nBioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data. Bioinformatics has been used for \"in silico\" analyses of biological queries using mathematical and statistical techniques.\n\nBioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis \"pipelines\" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences, called proteomics.\n\nBioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions.\n\nHistorically, the term \"bioinformatics\" did not mean what it means today. Paulien Hogeweg and Ben Hesper coined it in 1970 to refer to the study of information processes in biotic systems. This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).\n\nComputers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the \"mother and father of bioinformatics.\" Dayhoff compiled one of the first protein sequence databases, initially published as books and pioneered methods of sequence alignment and molecular evolution. Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.\n\nTo study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This includes nucleotide and amino acid sequences, protein domains, and protein structures. The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:\n\n\nThe primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein–protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.\n\nBioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.\n\nOver the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.\n\nCommon activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.\n\nBioinformatics is a science field that is similar to but distinct from biological computation, while it is often considered synonymous to computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology involve the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.\n\nAnalyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence, soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.\n\nSince the Phage Φ-X174 was sequenced in 1977, the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides. These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this sequence alignment is used in the sequencing process itself.\n\nBefore sequences can be analyzed they have to be obtained. DNA sequencing is still a non-trivial problem as the raw data may be noisy or afflicted by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.\n\nMost DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, \"Haemophilus influenzae\") generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.\n\nIn the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.\n\nThe first genome annotation software system was designed in 1995 by Owen White, who was part of the team at The Institute for Genomic Research that sequenced and analyzed the first genome of a free-living organism to be decoded, the bacterium \"Haemophilus influenzae\". White built a software system to find the genes (fragments of genomic sequence that encode proteins), the transfer RNAs, and to make initial assignments of function to those genes. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in \"Haemophilus influenzae\", are constantly changing and improving.\n\nFollowing the goals that the Human Genome Project left to achieve after its closure in 2003, a new project developed by the National Human Genome Research Institute in the U.S appeared. The so-called ENCODE project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to generate automatically large amounts of data with lower research costs but with the same quality and viability.\n\nEvolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:\nFuture work endeavours to reconstruct the now more complex tree of life.\n\nThe area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.\n\nThe core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion.\nUltimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectra of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov Chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.\n\nMany of these studies are based on the homology detection and protein families computation.\n\nPan genomics is a concept introduced in 2005 by Tettelin and Medini which eventually took root in bioinformatics. Pan genome is the complete gene repertoire of a particular taxonomic group: although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum etc. It is divided in two parts- The Core genome: Set of genes common to all the genomes under study (These are often housekeeping genes vital for survival) and The Dispensable/Flexible Genome: Set of genes not present in all but one or some genomes under study.\na bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.\n\nWith the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as diabetes, infertility, breast cancer or Alzheimer's Disease. Genome-wide association studies are a useful approach to pinpoint the mutations responsible for such complex diseases. Through these studies, thousands of DNA variants have been identified that are associated with similar diseases and traits. Furthermore, the possibility for genes to be used at prognosis, diagnosis or treatment is one of the most essential applications. Many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis.\n\nIn cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known \"point mutations\". These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.\n\nWith the breakthroughs that this next-generation sequencing technology is providing to the field of Bioinformatics, cancer genomics could drastically change. These new methods and software allow bioinformaticians to sequence many cancer genomes quickly and affordably. This could create a more flexible process for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples.\n\nAnother type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.\n\nThe expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as \"Whole Transcriptome Shotgun Sequencing\" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies. Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.\n\nProtein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected.\n\nRegulation is the complex orchestration of events by which a signal, potentially an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.\n\nFor example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.\n\nExpression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.\n\nSeveral approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. This is relevant as the location of these components affects the events within a cell and thus helps us to predict the behavior of biological systems. A gene ontology category, \"cellular compartment\", has been devised to capture subcellular localization in many biological databases.\n\nMicroscopic pictures allow us to locate both organelles as well as molecules. It may also help us to distinguish between normal and abnormal cells, e.g. in cancer.\n\nThe localization of proteins helps us to evaluate the role of a protein. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. Protein localization is thus an important component of protein function prediction. There are well developed protein subcellular localization prediction resources available, including protein subcellualr location databases and prediction tools.\n\nData from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the spatial proximity of DNA loci. Analysis of these experiments can determine the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.\n\nProtein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy – a.k.a. Mad Cow Disease – prion.) Knowledge of this structure is vital in understanding the function of the protein. Structural information is usually classified as one of \"secondary\", \"tertiary\" and \"quaternary\" structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.\n\nOne of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene \"A\", whose function is known, is homologous to the sequence of gene \"B,\" whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably.\n\nOne example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.\n\nOther techniques for predicting protein structure include protein threading and \"de novo\" (from scratch) physics-based modeling.\n\n\"Network analysis\" seeks to understand the relationships within biological networks such as metabolic or protein–protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.\n\n\"Systems biology\" involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.\n\nTens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein–protein interactions only based on these 3D shapes, without performing protein–protein interaction experiments. A variety of methods have been developed to tackle the protein–protein docking problem, though it seems that there is still much work to be done in this field.\n\nOther interactions encountered in the field include Protein–ligand (including drug) and protein–peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.\n\nThe growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:\n\nThe area of research draws from statistics and computational linguistics.\n\nComputational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:\n\nComputational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.\n\nBiodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools.\n\nDatabases are essential for bioinformatics research and applications. Many databases exist, covering various information types: for example, DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases may contain empirical data (obtained directly from experiments), predicted data (obtained from analysis), or, most commonly, both. They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. These databases vary in their format, access mechanism, and whether they are public or not.\n\nSome of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.\n\n\nSoftware tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.\n\nMany free and open-source software tools have existed and continued to grow since the 1980s. The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative \"in silico\" experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide \"de facto\" standards and shared object models for assisting with the challenge of bioinformation integration.\n\nThe range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD. To maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.\n\nAn alternative method to build public bioinformatics databases is to use the MediaWiki engine with the \"WikiOpener\" extension. This system allows the database to be accessed and updated by all experts in the field.\n\nSOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.\n\nBasic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis). The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.\n\nA Bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to\n\nSome of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril.\n\nSoftware platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273π project or 4273pi project also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils. 4273π is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4273π operating system.\n\nMOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera's Bioinformatics Specialization (UC San Diego) and Genomic Data Science Specialization (Johns Hopkins) as well as EdX's Data Analysis for Life Sciences XSeries (Harvard).\n\nThere are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).\n\n"}
{"id": "4216", "url": "https://en.wikipedia.org/wiki?curid=4216", "title": "Brian De Palma", "text": "Brian De Palma\n\nBrian Russell De Palma (born September 11, 1940) is an American film director and screenwriter. He is considered part of the New Hollywood wave of filmmaking.\n\nIn a career spanning over 50 years, he is best known for his suspense, psychological thriller, and crime films. He directed successful and popular films such as the supernatural horror \"Carrie\", the erotic crime thriller \"Dressed to Kill\", the thriller \"Blow Out\", the crime dramas \"Scarface\", \"The Untouchables\", and \"Carlito's Way\", and the action spy film \"\".\n\nDe Palma, who is of Italian ancestry, is the youngest of three boys and was born in Newark, New Jersey to Vivienne (née Muti) and Anthony Federico De Palma, an orthopedic surgeon. He was raised in Philadelphia, Pennsylvania and New Hampshire, and attended various Protestant and Quaker schools, eventually graduating from Friends' Central School. When he was in high school, he built computers. He won a regional science-fair prize for a project titled \"An Analog Computer to Solve Differential Equations\".\n\nEnrolled at Columbia as a physics student, De Palma became enraptured with the filmmaking process after viewing \"Citizen Kane\" and \"Vertigo\". De Palma subsequently enrolled at the newly coed Sarah Lawrence College as a graduate student in their theater department in the early 1960s, becoming one of the first male students among a female population. Once there, influences as various as drama teacher Wilford Leach, the Maysles brothers, Michelangelo Antonioni, Jean-Luc Godard, Andy Warhol, and Alfred Hitchcock impressed upon De Palma the many styles and themes that would shape his own cinema in the coming decades.\n\nAn early association with a young Robert De Niro resulted in \"The Wedding Party\". The film, which was co-directed with Leach and producer Cynthia Munroe, had been shot in 1963 but remained unreleased until 1969, when De Palma's star had risen sufficiently within the Greenwich Village filmmaking scene. De Niro was unknown at the time; the credits mistakenly display his name as \"Robert .\" The film is noteworthy for its invocation of silent film techniques and an insistence on the jump-cut for effect. De Palma followed this style with various small films for the NAACP and The Treasury Department.\n\nDuring the 1960s, De Palma began making a living producing documentary films, notably \"The Responsive Eye\", a 1966 movie about \"The Responsive Eye\" op-art exhibit curated by William Seitz for MOMA in 1965. In an interview with Gelmis from 1969, De Palma described the film as \"very good and very successful. It's distributed by Pathe Contemporary and makes lots of money. I shot it in four hours, with synched sound. I had two other guys shooting people's reactions to the paintings, and the paintings themselves.\"\n\n\"Dionysus in 69\" (1969) was De Palma's other major documentary from this period. The film records The Performance Group's performance of Euripides' The Bacchae, starring, amongst others, De Palma regular William Finley. The play is noted for breaking traditional barriers between performers and audience. The film's most striking quality is its extensive use of the split-screen. De Palma recalls that he was \"floored\" by this performance upon first sight, and in 1973 recounts how he \"began to try and figure out a way to capture it on film. I came up with the idea of split-screen, to be able to show the actual audience involvement, to trace the life of the audience and that of the play as they merge in and out of each other.\"\n\nDe Palma's most significant features from this decade are \"Greetings\" (1968) and \"Hi, Mom!\" (1970). Both films star Robert De Niro and espouse a Leftist revolutionary viewpoint common to their era. \"Greetings\" was entered into the 19th Berlin International Film Festival, where it won a Silver Bear award. His other major film from this period is the slasher comedy \"Murder a la Mod\". Each of these films contains experiments in narrative and intertextuality, reflecting De Palma's stated intention to become the \"American Godard\" while integrating several of the themes which permeated Hitchcock's work.\n\n\"Greetings\" is about three New Yorkers dealing with the draft. The film is often considered the first to deal explicitly with the draft. The film is noteworthy for its use of various experimental techniques to convey its narrative in ultimately unconventional ways. Footage was sped up, rapid cutting was used to distance the audience from the narrative, and it was difficult to discern with whom the audience must ultimately align. \"Greetings\" ultimately grossed over $1 million at the box office and cemented De Palma's position as a bankable filmmaker.\n\nAfter the success of his 1968 breakthrough, De Palma and his producing partner, Charles Hirsch, were given the opportunity by Sigma 3 to make an unofficial sequel of sorts, initially entitled \"Son of Greetings\", and subsequently released as \"Hi, Mom!\". While \"Greetings\" accentuated its varied cast, \"Hi, Mom!\" focuses on De Niro's character, Jon Rubin, an essential carry-over from the previous film. The film is ultimately significant insofar as it displays the first enunciation of De Palma's style in all its major traits – voyeurism, guilt, and a hyper-consciousness of the medium are all on full display, not just as hallmarks, but built into this formal, material apparatus itself.\n\nThese traits come to the fore in \"Hi, Mom!\"'s \"Be Black, Baby\" sequence. This sequence parodies cinéma vérité, the dominant documentary tradition of the 1960s, while simultaneously providing the audience with a visceral and disturbingly emotional experience. De Palma describes the sequence as a constant invocation of Brechtian distanciation: \"First of all, I am interested in the medium of film itself, and I am constantly standing outside and making people aware that they are always watching a film. At the same time I am evolving it. In \"Hi, Mom!\" for instance, there is a sequence where you are obviously watching a ridiculous documentary and you are told that and you are aware of it, but it still sucks you in. There is a kind of Brechtian alienation idea here: you are aware of what you are watching at the same time that you are emotionally involved with it.\"\n\n\"Be Black, Baby\" was filmed in black and white stock on 16 mm, in low-light conditions that stress the crudity of the direct cinema aesthetic. It is precisely from this crudity that the film itself gains a credibility of \"realism.\" In an interview with Michael Bliss, De Palma notes \"[Be Black, Baby] was rehearsed for almost three weeks... In fact, it's all scripted. But once the thing starts, they just go with the way it's going. I specifically got a very good documentary camera filmmaker (Robert Elfstrom) to just shoot it like a documentary to follow the action.\" Furthermore, \"I wanted to show in \"Hi, Mom!\" how you can really involve an audience. You take an absurd premise – \"Be Black, Baby\" – and totally involve them and really frighten them at the same time. It's very Brechtian. You suck 'em in and annihilate 'em. Then you say, \"It's just a movie, right? It's not real.\" It's just like television. You're sucked in all the time, and you're being lied to in a very documentary-like setting. The \"Be Black, Baby\" section of \"Hi, Mom!\" is probably the most important piece of film I've ever done.\"\n\nIn the 1970s, De Palma went to Hollywood where he worked on bigger budget films. In 1970, De Palma left New York for Hollywood at age thirty to make \"Get to Know Your Rabbit\", starring Orson Welles and Tommy Smothers. Making the film was a crushing experience for De Palma, as Tommy Smothers did not like many of De Palma's ideas.\n\nAfter several small, studio and independent released films that included stand-outs \"Sisters\", \"Phantom of the Paradise\", and \"Obsession\", a small film based on a novel called \"Carrie\" was released directed by Brian De Palma. The psychic thriller \"Carrie\" is seen by some as De Palma's bid for a blockbuster. In fact, the project was small, underfunded by United Artists, and well under the cultural radar during the early months of production, as Stephen King's source novel had yet to climb the bestseller list. De Palma gravitated toward the project and changed crucial plot elements based upon his own predilections, not the saleability of the novel. The cast was young and relatively new, though Sissy Spacek and John Travolta had gained attention for previous work in, respectively, film and episodic sitcoms. \"Carrie\" became a hit, the first genuine box-office success for De Palma. It garnered Spacek and Piper Laurie Oscar nominations for their performances. Preproduction for the film had coincided with the casting process for George Lucas's \"Star Wars\", and many of the actors cast in De Palma's film had been earmarked as contenders for Lucas's movie, and vice versa. The \"shock ending\" finale is effective even while it upholds horror-film convention, its suspense sequences are buttressed by teen comedy tropes, and its use of split-screen, split-diopter and slow motion shots tell the story visually rather than through dialogue.\n\nThe financial and critical success of \"Carrie\" allowed De Palma to pursue more personal material. \"The Demolished Man\" was a novel that had fascinated De Palma since the late 1950s and appealed to his background in mathematics and avant-garde storytelling. Its unconventional unfolding of plot (exemplified in its mathematical layout of dialogue) and its stress on perception have analogs in De Palma's filmmaking. He sought to adapt it on numerous occasions, though the project would carry a substantial price tag, and has yet to appear onscreen (Steven Spielberg's adaptation of Philip K. Dick's \"Minority Report\" bears striking similarities to De Palma's visual style and some of the themes of \"The Demolished Man\"). The result of his experience with adapting \"The Demolished Man\" was \"The Fury\", a science fiction psychic thriller that starred Kirk Douglas, Carrie Snodgress, John Cassavetes and Amy Irving. The film was admired by Jean-Luc Godard, who featured a clip in his mammoth Histoire(s) du cinéma, and Pauline Kael who championed both \"The Fury\" and De Palma. The film boasted a larger budget than \"Carrie\", though the consensus view at the time was that De Palma was repeating himself, with diminishing returns. As a film it retains De Palma's considerable visual flair, but points more toward his work in mainstream entertainments such as \"The Untouchables\" and \"\", the thematic complex thrillers for which he is now better known.\n\nFor many film-goers, De Palma's gangster films, most notably \"Scarface\" and \"Carlito's Way\", pushed the envelope of on-screen violence and depravity, and yet greatly vary from one another in both style and content and also illustrate De Palma's evolution as a film-maker. In essence, the excesses of \"Scarface\" contrast with the more emotional tragedy of \"Carlito's Way\". Both films feature Al Pacino in what has become a fruitful working relationship. In 1984, he directed the music video of Bruce Springsteen's song \"Dancing in the Dark\". The 1980s were denoted by De Palma's other films \"Dressed To Kill\", \"Blow Out\", and \"Body Double\".\n\nLater into the 1990s and 2000s, De Palma did other films. He attempted to do dramas and a few thrillers plus science fiction. Some of these movies (\"Mission: Impossible\", \"Carlito's Way\") worked and some others (\"Raising Cain\", \"Mission to Mars\") failed at the box office. However, \"The Bonfire of the Vanities\" would be De Palma's biggest box office disaster, losing millions. \n\nA more political controversy erupted in a later movie from De Palma, \"Redacted\" (2007), which had the subject of American involvement in Iraq, including the committing of war atrocities there. It received limited release in the United States and grossed less than $1 million.\n\nIn 2012, his film \"Passion\" was selected to compete for the Golden Lion at the 69th Venice International Film Festival. In 2015, he was the subject of a documentary film, \"De Palma\".\n\nDe Palma's films can fall into two categories, his psychological thrillers (\"Sisters\", \"Body Double\", \"Obsession\", \"Dressed to Kill\", \"Blow Out\", \"Raising Cain\") and his mainly commercial films (\"Scarface\", \"The Untouchables\", \"Carlito's Way\", and \"Mission: Impossible\"). He has often produced \"De Palma\" films one after the other before going on to direct a different genre, but would always return to his familiar territory. Because of the subject matter and graphic violence of some of De Palma's films, such as \"Dressed to Kill\", \"Scarface\" and \"Body Double\", they are often at the center of controversy with the Motion Picture Association of America, film critics and the viewing public.\n\nDe Palma is known for quoting and referencing other directors' work throughout his career. Michelangelo Antonioni's \"Blowup\" and Francis Ford Coppola's \"The Conversation\" plots were used for the basis of \"Blow Out\". \"The Untouchables\" finale shoot out in the train station is a clear borrow from the Odessa Steps sequence in Sergei Eisenstein's \"The Battleship Potemkin\". The main plot from \"Rear Window\" was used for \"Body Double\", while it also used elements of \"Vertigo\". \"Vertigo\" was also the basis for \"Obsession\". \"Dressed to Kill\" was a note-for-note homage to Hitchcock's \"Psycho\", including such moments as the surprise death of the lead actress and the exposition scene by the psychiatrist at the end.\n\nFilm critics have often noted De Palma's penchant for unusual camera angles and compositions throughout his career. He often frames characters against the background using a canted angle shot. Split-screen techniques have been used to show two separate events happening simultaneously. To emphasize the dramatic impact of a certain scene De Palma has employed a 360-degree camera pan. Slow sweeping, panning and tracking shots are often used throughout his films, often through precisely-choreographed long takes lasting for minutes without cutting. Split focus shots, often referred to as \"di-opt\", are used by De Palma to emphasize the foreground person/object while simultaneously keeping a background person/object in focus. Slow-motion is frequently used in his films to increase suspense.\n\nVilmos Zsigmond\nStephen H. Burum\nJosé Luis Alcaine\nDe Palma has been married and divorced three times, to actress Nancy Allen (1979–1983), producer Gale Anne Hurd (1991–1993), and Darnell Gregorio (1995–1997). He has one daughter from his marriage to Gale Anne Hurd, Lolita de Palma, born in 1991, and one daughter from his marriage to Darnell Gregorio, Piper De Palma, born in 1996. He resides in Manhattan, New York.\n\nDe Palma is often cited as a leading member of the New Hollywood generation of film directors, a distinct pedigree who either emerged from film schools or are overtly cine-literate. His contemporaries include Martin Scorsese, Paul Schrader, John Milius, George Lucas, Francis Ford Coppola, Steven Spielberg, John Carpenter, and Ridley Scott. His artistry in directing and use of cinematography and suspense in several of his films has often been compared to the work of Alfred Hitchcock. Psychologists have been intrigued by De Palma's fascination with pathology, by the aberrant behavior aroused in characters who find themselves manipulated by others.\n\nDe Palma has encouraged and fostered the filmmaking careers of directors such as Mark Romanek and Keith Gordon. During an interview with De Palma, Tarantino said that \"Blow Out\" is one of his all-time favorite films, and that after watching \"Scarface\" he knew how to make his own film. Terrence Malick credits seeing De Palma's early films on college campus tours as a validation of independent film, and subsequently switched his attention from philosophy to filmmaking. Other filmmakers influenced by De Palma include Quentin Tarantino, Ronny Yu, Don Mancini, Nacho Vigalondo, and Jack Thomas Smith.\n\nCritics who frequently admire De Palma's work include Pauline Kael and Roger Ebert, among others. Kael wrote in her review of \"Blow Out\", \"At forty, Brian De Palma has more than twenty years of moviemaking behind him, and he has been growing better and better. Each time a new film of his opens, everything he has done before seems to have been preparation for it.\" In his review of \"Femme Fatale\", Roger Ebert wrote about the director: \"De Palma deserves more honor as a director. Consider also these titles: \"Sisters\", \"Blow Out\", \"The Fury\", \"Dressed to Kill\", \"Carrie\", \"Scarface\", \"Wise Guys\", \"Casualties of War\", \"Carlito's Way\", \"Mission: Impossible\". Yes, there are a few failures along the way (\"Snake Eyes\", \"Mission to Mars\", \"The Bonfire of the Vanities\"), but look at the range here, and reflect that these movies contain treasure for those who admire the craft as well as the story, who sense the glee with which De Palma manipulates images and characters for the simple joy of being good at it. It's not just that he sometimes works in the style of Hitchcock, but that he has the nerve to.\"\n\nJulie Salamon has written that De Palma has been accused of being \"a perverse misogynist\" by critics. De Palma has responded to accusations of misogyny by saying: \"I'm always attacked for having an erotic, sexist approach---chopping up women, putting women in peril. I'm making suspense movies! What else is going to happen to them?\"\n\nDavid Thomson wrote in his entry for De Palma, \"There is a self-conscious cunning in De Palma's work, ready to control everything except his own cruelty and indifference.\"\n\n\n\n\n\n"}
{"id": "4218", "url": "https://en.wikipedia.org/wiki?curid=4218", "title": "North American B-25 Mitchell", "text": "North American B-25 Mitchell\n\nThe North American B-25 Mitchell is an American twin-engine, medium bomber manufactured by North American Aviation (NAA).\n\nThe design was named in honor of Major General William \"Billy\" Mitchell, a pioneer of U.S. military aviation. Used by many Allied air forces, the B-25 served in every theater of World War II and after the war ended many remained in service, operating across four decades. Produced in numerous variants, nearly 10,000 Mitchells rolled from NAA factories. These included a few limited models, such as the United States Marine Corps' PBJ-1 patrol bomber and the United States Army Air Forces' F-10 reconnaissance aircraft and AT-24 trainers.\n\nThe Air Corps issued a circular (Number 38-385) in March 1938 describing the performance they required from the next bombers — a payload of with a range of at more than . Those performance specifications led NAA to submit their NA-40 design. The NA-40 had benefited from the North American XB-21 (NA-39) of 1936, which was the company's partly successful design for an earlier medium bomber that had been initially accepted and ordered, but then cancelled. However, the company's experience from the XB-21 contributed to the design and development of the NA-40. The single NA-40 built flew first at the end of January 1939. It went through several modifications to correct problems. These improvements included fitting Wright R-2600 \"Twin Cyclone\" radial engines, in March 1939, which solved the lack of power.\n\nIn March 1939, North American delivered the substantially redesigned and improved NA-40 (as NA-40B) to the United States Army Air Corps for evaluation. It was in competition with other manufacturers' designs (Douglas 7B, Stearman X-100, and the Martin Model 167F) but failed to win orders. The aircraft was originally intended to be an attack bomber for export to the United Kingdom and France, both of which had a pressing requirement for such aircraft in the early stages of World War II. However, the French had already opted for a revised Douglas 7B (as the DB-7). Unfortunately, the NA-40B was destroyed in a crash on 11 April 1939 while undergoing testing. Although the crash was not considered due to a fault with the aircraft design, the Army ordered the DB-7 as the A-20.\n\nThe Air Corps issued a specification for a medium bomber in March 1939: over at NAA used the NA-40B design to develop the NA-62, which competed for the medium bomber contract. There was no YB-25 for prototype service tests. In September 1939, the Air Corps ordered the NA-62 into production as the B-25, along with the other new Air Corps medium bomber, the Martin B-26 Marauder \"off the drawing board\".\n\nEarly into B-25 production, NAA incorporated a significant redesign to the wing dihedral. The first nine aircraft had a constant-dihedral, meaning the wing had a consistent, upward angle from the fuselage to the wingtip. This design caused stability problems. \"Flattening\" the outer wing panels by giving them a slight anhedral angle just outboard of the engine nacelles nullified the problem, and gave the B-25 its gull wing configuration. Less noticeable changes during this period included an increase in the size of the tail fins and a decrease in their inward tilt at their tops.\n\nNAA continued design and development in 1940 and 1941. Both the B-25A and B-25B series entered USAAF service. The B-25B was operational in 1942. Combat requirements lead to further developments. Before the year was over, NAA was producing the B-25C and B-25D series at different plants. Also in 1942, the manufacturer began design work on the cannon-armed B-25G series. The NA-100 of 1943 and 1944 was an interim armament development at the Kansas City complex known as the B-25D2. Similar armament upgrades by U.S-based commercial modification centers involved about half of the B-25G series. Further development led to the B-25H, B-25J, and B-25J2. The gunship design concept dates to late 1942 and NAA sent a field technical representative to the SWPA. The factory-produced B-25G entered production during the NA-96 order followed by the redesigned B-25H gunship. The B-25J reverted to the bomber role, but it, too, could be outfitted as a strafer.\n\nNAA manufactured the greatest number of aircraft in World War II, the first time a company had produced trainers, bombers, and fighters simultaneously (the AT-6/SNJ Texan, B-25 Mitchell, and the P-51 Mustang). It produced B-25s at both its Inglewood main plant and an additional 6,608 aircraft at its Kansas City, Kansas plant at Fairfax Airport.\n\nAfter the war, the USAF placed a contract for the TB-25L trainer in 1952. This was a modification program by Hayes of Birmingham, Alabama. Its primary role was reciprocal engine pilot training.\n\nA development of the B-25 was the North American XB-28, designed as a high-altitude bomber. Two prototypes were built with the second prototype, the XB-28A, evaluated as a photo-reconnaissance platform, but the aircraft did not enter production.\n\nThe majority of B-25s in American service were used in the war against Japan in Asia and the Pacific. The Mitchell fought from the Northern Pacific to the South Pacific and the Far East. These areas included the campaigns in the Aleutian Islands, Papua New Guinea, the Solomon Islands, New Britain, China, Burma and the island hopping campaign in the Central Pacific. The aircraft's potential as a ground-attack aircraft emerged during the Pacific war. The jungle environment reduced the usefulness of medium-level bombing, and made low-level attack the best tactic. Using similar mast height level tactics and skip bombing, the B-25 proved itself to be a capable anti-shipping weapon and sank many enemy sea vessels of various types. An ever-increasing number of forward firing guns made the B-25 a formidable strafing aircraft for island warfare. The strafer versions were the B-25C1/D1, the B-25J1 and with the NAA strafer nose, the J2 sub-series.\n\nIn Burma, the B-25 was often used to attack Japanese communication links, especially bridges in central Burma. It also helped supply the besieged troops at Imphal in 1944. The China Air Task Force, the Chinese American Composite Wing, the First Air Commando Group, the 341st Bomb Group, and eventually, the relocated 12th Bomb Group, all operated the B-25 in the China Burma India Theater (CBI). Many of these missions involved battle field isolation, interdiction and close air support.\n\nLater in the war, as the USAAF acquired bases in other parts of the Pacific, the Mitchell could strike targets in Indochina, Formosa and Kyushu, increasing the usefulness of the B-25. It was also used in some of the shortest raids of the Pacific War, striking from Saipan against Guam and Tinian. The 41st Bomb Group used it against Japanese-occupied islands that had been bypassed by the main campaign, such as happened in the Marshall Islands.\n\nThe first B-25s arrived in Egypt and were carrying out independent operations by October 1942. Operations there against Axis airfields and motorized vehicle columns supported the ground actions of the Second Battle of El Alamein. Thereafter, the aircraft took part in the rest of the campaign in North Africa, the invasion of Sicily and the advance up Italy. In the Strait of Messina to the Aegean Sea the B-25 conducted sea sweeps as part of the coastal air forces. In Italy, the B-25 was used in the ground attack role, concentrating on attacks against road and rail links in Italy, Austria and the Balkans. The B-25 had a longer range than the Douglas A-20 Havoc and Douglas A-26 Invaders, allowing it to reach further into occupied Europe. The five bombardment groups – 20 squadrons – of the Ninth and Twelfth Air Forces that used the B-25 in the Mediterranean Theater of Operations were the only U.S. units to employ the B-25 in Europe.\n\nThe RAF received nearly 900 Mitchells, using them to replace Douglas Bostons, Lockheed Venturas and Vickers Wellington bombers. The Mitchell entered active RAF service on 22 January 1943. At first, it was used to bomb targets in occupied Europe. After the Normandy invasion, the RAF and France used Mitchells in support of the Allies in Europe. Several squadrons moved to forward airbases on the continent. The USAAF did not use the B-25 in combat in the ETO.\n\nThe B-25B first gained fame as the bomber used in the 18 April 1942 Doolittle Raid, in which 16 B-25Bs led by Lieutenant Colonel Jimmy Doolittle attacked mainland Japan, four months after the bombing of Pearl Harbor. The mission gave a much-needed lift in spirits to the Americans, and alarmed the Japanese, who had believed their home islands to be inviolable by enemy forces. Although the amount of actual damage done was relatively minor, it forced the Japanese to divert troops for home defense for the remainder of the war.\n\nThe raiders took off from the carrier and successfully bombed Tokyo and four other Japanese cities without loss. Fifteen of the bombers subsequently crash-landed en route to recovery fields in eastern China. These losses were the result of the task force being spotted by a Japanese vessel, forcing the bombers to take off early, fuel exhaustion, stormy nighttime conditions with zero visibility, and lack of electronic homing aids at the recovery bases. Only one B-25 bomber landed intact, in Siberia where its five-man crew was interned and the aircraft confiscated. Of the 80 aircrew, 69 survived their historic mission and eventually made it back to American lines.\n\nFollowing a number of additional modifications, including the addition of Plexiglas dome for navigational sightings to replace the overhead window for the navigator and heavier nose armament, de-icing and anti-icing equipment, the B-25C entered USAAF operations. Through block 20 the B-25C and B-25D differed only in location of manufacture: C series at Inglewood, California; D series at Kansas City, Kansas. After block 20 some NA-96 began the transition to the G series while some NA-87 acquired interim modifications eventually produced as the B-25D2 and ordered as the NA-100. NAA built a total of 3,915 B-25Cs and Ds during World War II.\n\nAlthough the B-25 was originally designed to bomb from medium altitudes in level flight, it was used frequently in the Southwest Pacific theatre in treetop-level strafing and missions with parachute-retarded fragmentation bombs against Japanese airfields in New Guinea and the Philippines. These heavily armed Mitchells were field-modified at Townsville, Australia, under the direction of Major Paul I. \"Pappy\" Gunn and North American tech rep Jack Fox, These \"commerce destroyers\" were also used on strafing and skip bombing missions against Japanese shipping trying to resupply their armies.\n\nUnder the leadership of Lieutenant General George C. Kenney, Mitchells of the Far East Air Forces and its existing components, the Fifth and Thirteenth Air Forces devastated Japanese targets in the Southwest Pacific Theater during 1944 to 1945. The USAAF played a significant role in pushing the Japanese back to their home islands. The type operated with great effect in the Central Pacific, Alaska, North Africa, Mediterranean and China-Burma-India (CBI) theaters.\n\nThe USAAF Antisubmarine Command made great use of the B-25 in 1942 and 1943. Some of the earliest B-25 Bomb Groups also flew the Mitchell on coastal patrols after the Pearl Harbor attack, prior to the AAFAC organization. Many of the two dozen or so Antisubmarine Squadrons flew the B-25C, D and G series in the American Theater Antisubmarine campaign, often in the distinctive, white sea search camouflage.\n\nIn anti-shipping operations, the USAAF had urgent need for hard-hitting aircraft, and North American responded with the B-25G. In this series the transparent nose and bombardier/navigator position was changed for a shorter, hatched nose with two fixed .50 in (12.7 mm) machine guns and a 75 mm (2.95 in) M4 cannon, one of the largest weapons fitted to an aircraft, similar to the British 57 mm gun-armed Mosquito Mk. XVIII and the German Henschel Hs 129B-3, and Ju 88P heavy cannon (up to a 75 mm long-barrel \"Bordkanone BK 7,5\"). The shorter nose placed the cannon breech behind the pilot where it could be manually loaded and serviced by the navigator; his crew station was moved to just behind the pilot. The navigator signalled the pilot when the gun was ready and the pilot fired the weapon using a button on his control wheel.\n\nThe Royal Air Force, U.S. Navy and the Soviet VVS each conducted trials with this series but none adopted it. The G series comprised one prototype, five pre-production C conversions, 58 C series modifications and 400 production aircraft for a total of 464 B-25G. In its final version, the G-12, an interim armament modification, eliminated the lower Bendix turret and added a starboard dual gun pack, waist guns and a canopy for the tail gunner to improve the view when firing the single tail gun. In April 1945 the air depots in Hawaii refurbished about two dozen of these and included the eight gun nose and rocket launchers in the upgrade.\n\nThe B-25H series continued the development of the gunship concept. NAA Inglewood produced 1000. The H had even more firepower. Most replaced the M4 gun with the lighter T13E1, designed specifically for the aircraft but 20-odd H-1 block aircraft completed by the Republic Aviation modification center at Evansville had the M4 and two machine gun nose armament. The 75 mm (2.95 in) gun fired at a muzzle velocity of . Due to its low rate of fire (about four rounds could be fired in a single strafing run), relative ineffectiveness against ground targets, and the substantial recoil, the 75 mm gun was sometimes removed from both G and H models and replaced with two additional .50 in (12.7 mm) machine guns as a field modification. In the new FEAF these were re-designated the G1 and H1 series respectively.\n\nThe H series normally came from the factory mounting four fixed, forward-firing .50  (12.7 mm) machine guns in the nose; four more fixed guns in forward-firing, individual gun packages; two more in the manned dorsal turret, re-located forward to a position just behind the cockpit; one each in a pair of new waist positions, introduced simultaneously with the forward-relocated dorsal turret; and lastly, a pair of guns in a new tail gunner's position. Company promotional material bragged that the B-25H could \"bring to bear 10 machine guns coming and four going, in addition to the 75 mm cannon, eight rockets and 3,000 lb (1,360 kg) of bombs.\"\n\nThe H had a modified cockpit with single flight controls operated by pilot. The co-pilot's station and controls were deleted, and instead had a smaller seat used by the navigator/cannoneer, The radio operator crew position was aft the bomb bay with access to the waist guns. Factory production total were 405 B-25Gs and 1,000 B-25Hs, with 248 of the latter being used by the Navy as PBJ-1H. Elimination of the co-pilot saved weight, moving the dorsal turret forward counterbalanced in part the waist guns and the manned rear turret.\nFollowing the two gunship series NAA again produced the medium bomber configuration with the B-25J series.. It optimized the mix of the interim NA-100 and the H series having both the bombardier's station and fixed guns of the D and the forward turret and refined armament of the H series. NAA also produced a strafer nose first shipped to air depots as kits, then introduced on the production line in alternating blocks with the bombardier nose. The solid-metal \"strafer\" nose housed eight centerline Browning M2 .50 calibre machine guns. The remainder of the armament was as in the H-5. NAA also supplied kits to mount eight underwing 5 \"high velocity airborne rockets\" (HVAR) just outside the propeller arcs. These were mounted on zero length launch rails, four to a wing.\n\nThe final, and the most built, series of the Mitchell, the B-25J, looked less like earlier series apart from the well-glazed bombardier's nose of nearly-identical appearance to the earliest B-25 subtypes. Instead, the J followed the overall configuration of the H series from the cockpit aft. It had the forward dorsal turret and other armament and airframe advancements. All J models included four .50 in (12.7 mm) light-barrel Browning AN/M2 guns in a pair of \"fuselage package\", conformal gun pods each flanking the lower cockpit, each pod containing two Browning M2s. By 1945, however, combat squadrons removed these. The J series restored the co-pilot's seat and dual flight controls. The factory made available kits to the Air Depot system to create the strafer-nose B-25J-2. This configuration carried a total of 18 .50 in (12.7 mm) light-barrel AN/M2 Browning M2 machine guns: eight in the nose, four in the flank-mount conformal gun pod packages, two in the dorsal turret, one each in the pair of waist positions, and a pair in the tail – with 14 of the guns either aimed directly forward, or aimed to fire directly forward for strafing missions. Some aircraft had eight 5 in (130 mm) high-velocity aircraft rockets (HVAR). NAA introduced the J-2 into production in alternating blocks at the J-22. Total J series production was 4,318.\n\nThe B-25 was a safe and forgiving aircraft to fly. With one engine out, 60° banking turns into the dead engine were possible, and control could be easily maintained down to 145 mph (230 km/h). The pilot had to remember to maintain engine-out directional control at low speeds after takeoff with rudder; if this maneuver was attempted with ailerons, the aircraft could snap out of control. The tricycle landing gear made for excellent visibility while taxiing. The only significant complaint about the B-25 was the extremely high noise level produced by its engines; as a result, many pilots eventually suffered from varying degrees of hearing loss.\n\nThe high noise level was due to design and space restrictions in the engine cowlings which resulted in the exhaust \"stacks\" protruding directly from the cowling ring and partly covered by a small triangular fairing. This arrangement directed exhaust and noise directly at the pilot and crew compartments.\n\nThe Mitchell was an exceptionally sturdy aircraft that could withstand tremendous punishment. One B-25C of the 321st Bomb Group was nicknamed \"Patches\" because its crew chief painted all the aircraft's flak hole patches with the bright yellow zinc chromate primer. By the end of the war, this aircraft had completed over 300 missions, had been belly-landed six times and had over 400 patched holes. The airframe of \"Patches\" was so distorted from battle damage that straight-and-level flight required 8° of left aileron trim and 6° of right rudder, causing the aircraft to \"crab\" sideways across the sky.\n\nIn 1947 legislation created an independent United States Air Force and by that time the B-25 inventory numbered only a few hundred. Some B-25s continued in service into the 1950s in a variety of training, reconnaissance and support roles. The principal use during this period was undergraduate training of multi-engine aircraft pilots slated for reciprocating engine or turboprop cargo, aerial refuelling or reconnaissance aircraft. Others were assigned to units of the Air National Guard in training roles in support of Northrop F-89 Scorpion and Lockheed F-94 Starfire operations. \n\nIn its USAF tenure, many B-25s received the so-called \"Hayes modification\" and as a result, surviving B-25 often have exhaust system with a semi-collector ring that splits emissions into two different systems. The upper seven cylinders are collected by a ring while the other cylinders remain directed to individual ports.\n\nTB-25J-25-NC Mitchell, \"44-30854\", the last B-25 in the USAF inventory, assigned at March AFB, California as of March 1960, was flown to Eglin AFB, Florida, from Turner Air Force Base, Georgia, on 21 May 1960, the last flight by a USAF B-25, and presented by Brigadier General A. J. Russell, Commander of SAC's 822d Air Division at Turner AFB, to the Air Proving Ground Center Commander, Brigadier General Robert H. Warren, who in turn presented the bomber to Valparaiso, Florida Mayor Randall Roberts on behalf of the Niceville-Valparaiso Chamber of Commerce. Four of the original Tokyo Raiders were present for the ceremony, Colonel (later Major General) David Jones, Colonel Jack Simms, Lieutenant Colonel Joseph Manske, and retired Master Sergeant Edwin W. Horton. It was donated back to the Air Force Armament Museum c. 1974 and marked as Doolittle's \"40-2344\".\n\nThe U.S. Navy designation for the Mitchell was the PBJ-1 and apart from increased use of radar, it was configured like its Army Air Forces counterparts. Under the pre-1962 USN/USMC/USCG aircraft designation system, PBJ-1 stood for Patrol (P) Bomber (B) built by North American Aviation (J), first variant (-1) under the existing American naval aircraft designation system of the era. The PBJ had its origin in an inter-service agreement of mid-1942 between the Navy and the USAAF exchanging the Boeing Renton plant for the Kansas plant for B-29 Superfortress production. The Boeing XPBB Sea Ranger flying boat, competing for B-29 engines, was cancelled in exchange for part of the Kansas City Mitchell production. Other terms included the inter-service transfer of 50 B-25C and 152 B-25D to the Navy. The bombers carried Navy bureau numbers (BuNos), beginning with BuNo 34998. The first PBJ-1 arrived in February 1943 and nearly all reached Marine Corps squadrons, beginning with Marine Bombing Squadron 413 (VMB-413). Following the AAFAC format, the Marine Mitchells had search radar in a retractable radome replacing the remotely-operated ventral turret. Later D and J series had nose mounted APS-3 radar; and later still, J and H series mounted radar in the starboard wingtip. The large quantities of B-25H and J series became known as PBJ-1H and PBJ-1J respectively. These aircraft often operated along with earlier PBJ series in Marine squadrons.\n\nThe PBJs were operated almost exclusively by the Marine Corps as land-based bombers. To operate them, the U.S. Marine Corps established a number of Marine bomber squadrons (VMB), beginning with VMB-413, in March 1943 at MCAS Cherry Point, North Carolina. Eight VMB squadrons were flying PBJs by the end of 1943, forming the initial Marine medium bombardment group. Four more squadrons were in the process of formation in late 1945, but had not yet deployed by the time the war ended.\n\nOperational use of the Marine Corps PBJ-1s began in March 1944. The Marine PBJs operated from the Philippines, Saipan, Iwo Jima and Okinawa during the last few months of the Pacific war. Their primary mission was the long range interdiction of enemy shipping trying to run the blockade which was strangling Japan. The weapon of choice during these missions was usually the five-inch HVAR rocket, eight of which could be carried. Some VMB-612 intruder PBJ-1D and J series flew without top turrets to save weight and increase range on night patrols, especially towards the end of the war when air superiority existed.\n\nDuring the war the Navy tested the cannon-armed G series and conducted carrier trial with an H equipped with arresting gear. After World War II, some PBJs stationed at the Navy's then-rocket laboratory site in Inyokern, California, site of the present-day Naval Air Weapons Station China Lake, tested various air-to-ground rockets and arrangements. One arrangement was a twin-barrel nose arrangement that could fire 10 spin-stabilized five-inch rockets in one salvo.\n\nThe Royal Air Force (RAF) was an early customer for the B-25 via Lend-Lease. The first Mitchells were given the service name Mitchell I by the RAF and were delivered in August 1941, to No. 111 Operational Training Unit based in the Bahamas. These bombers were used exclusively for training and familiarization and never achieved operational status. The B-25Cs and Ds were designated Mitchell II. Altogether, 167 B-25Cs and 371 B-25Ds were delivered to the RAF. The RAF tested the cannon-armed G series but did not adopt the series nor the follow on H series.\n\nBy the end of 1942 the RAF had taken delivery of a total of 93 Mitchell marks I and II. Some served with squadrons of No. 2 Group RAF, the RAF's tactical medium bomber force. The first RAF operation with the Mitchell II took place on 22 January 1943, when six aircraft from No. 180 Squadron RAF attacked oil installations at Ghent. After the invasion of Europe (by which point 2 Group was part of Second Tactical Air Force), all four Mitchell squadrons moved to bases in France and Belgium (Melsbroek) to support Allied ground forces. The British Mitchell squadrons were joined by No. 342 (Lorraine) Squadron of the French Air Force in April 1945.\n\nAs part of its move from Bomber Command, No 305 (Polish) Squadron flew Mitchell IIs from September to December 1943 before converting to the de Havilland Mosquito. In addition to No. 2 Group, the B-25 was used by various second-line RAF units in the UK and abroad. In the Far East, No. 3 PRU, which consisted of Nos. 681 and 684 Squadrons, flew the Mitchell (primarily Mk IIs) on photographic reconnaissance sorties.\n\nThe RAF was allocated 316 B-25J which entered service as the Mitchell III. Deliveries took place between August 1944 and August 1945. However, only about 240 of these bombers actually reached Britain, with some being diverted to No. 111 OTU in the Bahamas, some crashing during delivery and some being retained in the United States.\n\nThe Royal Canadian Air Force (RCAF) used the B-25 Mitchell for training during the war. Post-war use saw continued operations with most of 162 Mitchells received. The first B-25s had originally been diverted to Canada from RAF orders. These included one Mitchell I, 42 Mitchell IIs, and 19 Mitchell IIIs. No 13 (P) Squadron was formed unofficially at RCAF Rockcliffe in May 1944 and used Mitchell IIs on high-altitude aerial photography sorties. No. 5 OTU (Operational Training Unit) at Boundary Bay, British Columbia and Abbotsford, British Columbia, operated the B-25D Mitchell in the training role together with B-24 Liberators for Heavy Conversion as part of the BCATP. The RCAF retained the Mitchell until October 1963.\n\nNo 418 (Auxiliary) Squadron received its first Mitchell IIs in January 1947. It was followed by No 406 (auxiliary), which flew Mitchell IIs and IIIs from April 1947 to June 1958. No 418 Operated a mix of IIs and IIIs until March 1958. No 12 Squadron of Air Transport Command also flew Mitchell IIIs along with other types from September 1956 to November 1960. In 1951, the RCAF received an additional 75 B-25Js from USAF stocks to make up for attrition and to equip various second-line units.\n\nThe Australians received Mitchells by the spring of 1944. The joint Australian-Dutch No. 18 (Netherlands East Indies) Squadron RAAF had more than enough Mitchells for one squadron, so the surplus went to re-equip the RAAF's No. 2 Squadron, replacing their Beauforts.\n\nDuring World War II, the Mitchell served in fairly large numbers with the Air Force of the Dutch government-in-exile. They participated in combat in the East Indies as well as on the European front. On 30 June 1941, the Netherlands Purchasing Commission, acting on behalf of the Dutch government-in-exile in London, signed a contract with North American Aviation for 162 B-25C aircraft. The bombers were to be delivered to the Netherlands East Indies to help deter any Japanese aggression into the region.\n\nIn February 1942, the British Overseas Airways Corporation (BOAC) agreed to ferry 20 Dutch B-25s from Florida to Australia travelling via Africa and India, and an additional ten via the South Pacific route from California. During March, five of the bombers on the Dutch order had reached Bangalore, India and 12 had reached Archerfield in Australia. It was agreed that the B-25s in Australia would be used as the nucleus of a new squadron, designated No. 18. This squadron was staffed jointly by Australian and Dutch aircrews plus a smattering of aircrews from other nations, and operated under Royal Australian Air Force command for the remainder of the war.\n\nThe B-25s of No. 18 Squadron were painted with the Dutch national insignia (at this time a rectangular Netherlands flag) and carried NEIAF serials. Discounting the ten \"temporary\" B-25s delivered to 18 Squadron in early 1942, a total of 150 Mitchells were taken on strength by the NEIAF, 19 in 1942, 16 in 1943, 87 in 1944, and 28 in 1945. They flew bombing raids against Japanese targets in the East Indies. In 1944, the more capable B-25J Mitchell replaced most of the earlier C and D models.\n\nIn June 1940, No. 320 Squadron RAF had been formed from personnel formerly serving with the Royal Dutch Naval Air Service who had escaped to England after the German occupation of the Netherlands. Equipped with various British aircraft, No. 320 Squadron flew anti-submarine patrols, convoy escort missions, and performed air-sea rescue duties. They acquired the Mitchell II in September 1943, performing operations over Europe against gun emplacements, railway yards, bridges, troops and other tactical targets. They moved to Belgium in October 1944, and transitioned to the Mitchell III in 1945. No. 320 Squadron was disbanded in August 1945. Following the war, B-25s were used by Dutch forces during the Indonesian National Revolution.\n\nThe U.S. supplied 862 B-25s (B, D, G, and J types) to the Soviet Union under Lend-Lease during World War II via the Alaska–Siberia ALSIB ferry route.\n\nOther damaged aircraft arrived or crashed in the Far East of Russia, and one Doolittle Raid aircraft landed there short of fuel after attacking Japan. The lone airworthy aircraft to reach the Soviet Union was lost in a hangar fire in the early 50s while undergoing routine maintenance. In general, the B-25 was operated as a ground-support and tactical daylight bomber (as similar Douglas A-20 Havocs were used). It saw action in fights from Stalingrad (with B/D models) to the German surrender during May 1945 (with G/J types).\n\nB-25s that remained in Soviet Air Force service after the war were assigned the NATO reporting name \"Bank\".\n\nWell over 100 B-25Cs and Ds were supplied to the Nationalist Chinese during the Second Sino-Japanese War. In addition, a total of 131 B-25Js were supplied to China under Lend-Lease.\n\nThe four squadrons of the 1st BG (1st, 2nd, 3rd, and 4th) of the 1st Medium Bomber Group were formed during the war. They formerly operated Russian-built Tupolev SB bombers, then transferred to the B-25. The 1st BG was under the command of CACW (Chinese-American Composite Wing) while operating B-25s. Following the end of the war in the Pacific, these four bombardment squadrons were established to fight against the Communist insurgency that was rapidly spreading throughout the country. During the Chinese Civil War, Chinese Mitchells fought alongside de Havilland Mosquitos.\n\nIn December 1948, the Nationalists were forced to retreat to the island of Taiwan, taking many of their Mitchells with them. However, some B-25s were left behind and were impressed into service with the air force of the new People's Republic of China.\n\nDuring the war, the Força Aérea Brasileira (FAB) received a few B-25s under Lend-Lease. Brazil declared war against the Axis powers in August 1942 and participated in the war against the U-boats in the southern Atlantic. The last Brazilian B-25 was finally declared surplus in 1970.\n\nThe Royal Air Force issued at least 21 Mitchell IIIs to No 342 Squadron, which was made up primarily of Free French aircrews. Following the liberation of France, this squadron transferred to the newly formed French Air Force (\"Armée de l'Air\") as GB I/20 Lorraine. The aircraft continued in operation after the war, with some being converted into fast VIP transports. They were struck off charge in June 1947.\n\n\n\n\nMost models of the B-25 were used at some point as training aircraft.\n\n\n\n\nMany B-25s are currently kept in airworthy condition by air museums and collectors.\n\n\nAt 9:40 on Saturday, 28 July 1945, a USAAF B-25D crashed in thick fog into the north side of the Empire State Building between the 79th and 80th floors. Fourteen people died — 11 in the building and the three occupants of the aircraft, including the pilot, Colonel William F. Smith. Betty Lou Oliver, an elevator attendant, survived the impact and the subsequent fall of the elevator cage 75 stories to the basement.\n\n"}
{"id": "4219", "url": "https://en.wikipedia.org/wiki?curid=4219", "title": "British Open (disambiguation)", "text": "British Open (disambiguation)\n\nThe British Open is the Open Championship men's golf tournament.\n\nBritish Open may also refer to:\n"}
{"id": "4224", "url": "https://en.wikipedia.org/wiki?curid=4224", "title": "Bobby Charlton", "text": "Bobby Charlton\n\nSir Robert Charlton CBE (born 11 October 1937) is an English former football player, regarded as one of the greatest midfielders of all time, and an essential member of the England team who won the World Cup in 1966, the year he also won the Ballon d'Or. He played almost all of his club football at Manchester United, where he became renowned for his attacking instincts and passing abilities from midfield and his ferocious long-range shot. He was also well known for his fitness and stamina. He was cautioned only twice in his career; once against Argentina in the 1966 World Cup, and once in a league match against Chelsea. His elder brother Jack, who was also in the World Cup-winning team, is a former defender for Leeds United and international manager.\n\nBorn in Ashington, Northumberland, Charlton made his debut for the Manchester United first-team in 1956, and over the next two seasons gained a regular place in the team, during which time he survived the Munich air disaster of 1958 after being rescued by Harry Gregg. After helping United to win the Football League in 1965, he won a World Cup medal with England in 1966 and another Football League title with United the following year. In 1968, he captained the Manchester United team that won the European Cup, scoring two goals in the final to help his team be the first English side to win the competition. He is United's second all-time leading goal scorer (249), being surpassed by Wayne Rooney, and held the distinction of being England's all-time top goal scorer (49) from May 1968 to September 2015, when again Wayne Rooney surpassed his record. Charlton held the record for most appearances for Manchester United (758), before being surpassed by Ryan Giggs in the 2008 UEFA Champions League Final.\n\nHe was selected for four World Cups (1958, 1962, 1966 and 1970), and helped England to win the competition in 1966. At the time of his retirement from the England team in 1970, he was the nation's most capped player, having turned out 106 times at the highest level. This record has since been held by Bobby Moore and Peter Shilton.\n\nHe left Manchester United to become manager of Preston North End for the 1973–74 season. He changed to player-manager the following season. He next accepted a post as a director with Wigan Athletic, then became a member of Manchester United's board of directors in 1984 and remains one as of the 2016/17 season.\n\nCharlton is related to several professional footballers on his mother's side of the family: his uncles were Jack Milburn (Leeds United and Bradford City), George Milburn (Leeds United and Chesterfield), Jim Milburn (Leeds United and Bradford City) and Stan Milburn (Chesterfield, Leicester City and Rochdale), and legendary Newcastle United and England footballer Jackie Milburn, was his mother's cousin. However, Charlton credits much of the early development of his career to his grandfather Tanner and his mother Cissie. His elder brother, Jack, initially went to work applying to the Police Service before also becoming a professional footballer with Leeds United.\n\nOn 9 February 1953, then a Bedlington Grammar School pupil, Charlton was spotted playing for East Northumberland schools by Manchester United chief scout Joe Armstrong. Charlton went on to play for England Schoolboys and the 15-year-old signed with United on 1 January 1953, along with Wilf McGuinness, also aged 15. Initially his mother was reluctant to let him commit to an insecure football career, so he began an apprenticeship as an electrical engineer; however, he went on to turn professional in October 1954.\n\nCharlton became one of the famed Busby Babes, the collection of talented footballers who emerged through the system at Old Trafford in the 1940s, 1950s and 1960s as Matt Busby set about a long-term plan of rebuilding the club after the Second World War. He worked his way through the pecking order of teams, scoring regularly for the youth and reserve sides before he was handed his first team debut against Charlton Athletic in October 1956. At the same time, he was doing his National service with the Royal Army Ordnance Corps in Shrewsbury, where Busby had advised him to apply as it meant he could still play for Manchester United at the weekend. Also doing his army service in Shrewsbury at the same time was his United team-mate Duncan Edwards.\n\nCharlton played 14 times for United in that first season, scoring twice on his debut and managing a total of 12 goals in all competitions, and including a hat-trick in a 5–1 away win over Charlton Athletic in the February. United won the league championship but were denied the 20th century's first \"double\" when they controversially lost the 1957 FA Cup Final to Aston Villa. Charlton, still only 19, was selected for the game, which saw United goalkeeper Ray Wood carried off with a broken cheekbone after a clash with Villa centre forward Peter McParland. Though Charlton was a candidate to go in goal to replace Wood (in the days before substitutes, and certainly before goalkeeping substitutes), it was teammate Jackie Blanchflower who ended up between the posts.\n\nCharlton was an established player by the time the next season was fully underway, which saw United, as current League champions, become the first English team to compete in the European Cup. Previously, the Football Association had scorned the competition, but United made progress, reaching the semi-finals where they lost to holders Real Madrid. Their reputation was further enhanced the next season as they reached the quarter finals to play Red Star Belgrade. In the first leg at home, United won 2–1. The return in Yugoslavia saw Charlton score twice as United stormed 3–0 ahead, although the hosts came back to earn a 3–3 draw. However, United maintained their aggregate lead to reach the last four and were in jubilant mood as they left to catch their flight home, thinking of an important League game against Wolves at the weekend.\n\nThe aeroplane which took the United players and staff home from Zemun Airport needed to stop in Munich to refuel. This was carried out in worsening weather, and by the time the refuelling was complete and the call was made for the passengers to re-board the aircraft, the wintry showers had taken hold and snow had settled heavily on the runway and around the airport. There were two aborted take-offs which led to concern on board, and the passengers were advised by a stewardess to disembark again while a minor technical error was fixed.\n\nThe team was back in the airport terminal barely ten minutes when the call to reconvene on the plane came, and a number of passengers began to feel nervous. Charlton and teammate Dennis Viollet swapped places with Tommy Taylor and David Pegg, who had decided they would be safer at the back of the plane.\n\nThe plane clipped the fence at the end of the runway on its next take-off attempt and a wing tore through a nearby house, setting it alight. The wing and part of the tail came off and hit a tree and a wooden hut, the plane spinning along the snow until coming to a halt. It had been cut in half.\n\nCharlton, strapped into his seat, had fallen out of the cabin and when United goalkeeper Harry Gregg (who had somehow got through a hole in the plane unscathed and begun a one-man rescue mission) found him, he thought he was dead. That said, he grabbed both Charlton and Viollet by their trouser waistbands and dragged them away from the plane in constant fear that it would explode. Gregg returned to the plane to try to help the appallingly injured Busby and Blanchflower, and when he turned around again, he was relieved to see that Charlton and Viollet, both of whom he had presumed to be dead, had got out of their detached seats and were looking into the wreckage.\n\nCharlton suffered cuts to his head and severe shock and was in hospital for a week. Seven of his teammates had perished at the scene, including Taylor and Pegg, with whom he and Viollet had swapped seats prior to the fatal take-off attempt. Club captain Roger Byrne was also killed, along with Mark Jones, Billy Whelan, Eddie Colman and Geoff Bent. Duncan Edwards died a fortnight later from the injuries he had sustained. In total, the crash claimed 23 lives. Initially, ice on the wings was blamed, but a later inquiry declared that slush on the runway had made a safe take-off almost impossible.\n\nOf the 44 passengers and crew (including the 17-strong Manchester United squad), 23 people (eight of them Manchester United players) died as a result of their injuries in the crash. Charlton survived with minor injuries. Of the eight other players who survived, two of them were injured so badly that they never played again.\n\nCharlton was the first injured survivor to leave hospital. Harry Gregg and Bill Foulkes were not hospitalized since they escaped uninjured. He arrived back in England on 14 February 1958, eight days after the crash. As he convalesced with family in Ashington, he spent some time kicking a ball around with local youths, and a famous photograph of him was taken. He was still only 20 years old, yet now there was an expectation that he would help with the rebuilding of the club as Busby's aides tried to piece together what remained of the season.\n\nCharlton returned to playing in an FA cup tie against West Bromwich Albion on 1 March; the game was a draw and United won the replay 1–0. Not unexpectedly, United went out of the European Cup to Milan in the semi-finals to a 5–2 aggregate defeat and fell behind in the League. Yet somehow they reached their second consecutive FA Cup final, and the big day at Wembley coincided with Busby's return to work. However, his words could not inspire a side which was playing on a nation's goodwill and sentiment, and Nat Lofthouse scored twice to give Bolton Wanderers a 2–0 win.\n\nFurther success with Manchester United came at last when they beat Leicester City 3–1 in the FA Cup final of 1963, with Charlton finally earning a winners' medal in his third final. Busby's post-Munich rebuilding programme continued to progress with two League championships within three seasons, with United taking the title in 1965 and 1967. A successful (though trophyless) season with Manchester United had seen him take the honours of \"Football Writers' Association Footballer of the Year\" and \"European Footballer of the Year\" into the competition.\n\nIn 1968, Manchester United reached the European Cup final, ten seasons after Munich. Even though other clubs had taken part in the competition in the intervening decade, the team which got to this final was still the first English side to do so. On a highly emotional night at Wembley, Charlton scored twice in a 4–1 win after extra time against Benfica and, as United captain, lifted the trophy.\n\nDuring the early 1970s, Manchester United were no longer competing among the top teams in England, and at several stages were battling against relegation. At times, Charlton was not on speaking terms with United's other superstars George Best and Denis Law, and Best refused to play in Charlton's testimonial match against Celtic, saying that \"to do so would be hypocritical\". Charlton left Manchester United at the end of the 1972–73 season, having scored 249 goals and set a club record of 758 appearances, a record which Ryan Giggs broke in the 2008 UEFA Champions League Final.\n\nHis last game was against Chelsea at Stamford Bridge on 28 April 1973, and before the game the BBC cameras for \"Match of the Day\" captured the Chelsea chairman handing Charlton a commemorative cigarette case. The match ended in a 1-0 defeat. His final goal came a month earlier, on 31 March, in a 2-0 win at Southampton, also in the First Division.\n\nHe was the subject of \"This Is Your Life\" in 1969 when he was surprised by Eamonn Andrews at The Sportsman's Club in central London.\n\nCharlton's emergence as the country's leading young football talent was completed when he was called up to join the England squad for a British Home Championship game against Scotland at Hampden Park on 19 April 1958, just over two months after he had survived the Munich air disaster.\n\nCharlton was handed his debut as England romped home 4–0, with the new player gaining even more admirers after scoring a magnificent thumping volley dispatched with authority after a cross by the left winger Tom Finney. He scored both goals in his second game as England beat Portugal 2–1 in a friendly at Wembley; and overcame obvious nerves on a return to Belgrade to play his third match against Yugoslavia. England lost that game 5–0 and Charlton played poorly.\n\nHe was selected for the squad which competed at the 1958 World Cup in Sweden, but did not kick a ball, something at which critics expressed surprise and bewilderment, even allowing for his lacklustre performance in Belgrade.\n\nIn 1959 he scored a hat-trick as England demolished the US 8–1; and his second England hat-trick came in 1961 in an 8–0 thrashing of Mexico. He also managed to score in every British Home Championship tournament he played in except 1963 in an association with the tournament that lasted from 1958 to 1970 and included 16 goals and 10 tournament victories (five shared).\n\nHe played in qualifiers for the 1962 World Cup in Chile against Luxembourg and Portugal and was named in the squad for the finals themselves. His goal in the 3–1 group win over Argentina was his 25th for England in just 38 appearances, and he was still only 24 years old, but his individual success could not be replicated by that of the team, which was eliminated in the quarter final by Brazil, who went on to win the tournament.\n\nBy now, England were coached by Alf Ramsey who had managed to gain sole control of the recruitment and team selection procedure from the committee-based call-up system which had lasted up to the previous World Cup. Ramsey had already cleared out some of the older players who had been reliant on the loyalty of the committee for their continued selection – it was well known that decorum on the pitch at club level had been just as big a factor in playing for England as ability and form. Luckily for Charlton, he had all three.\nA hat-trick in the 8–1 rout of Switzerland in June 1963 took Charlton's England goal tally to 30, equalling the record jointly held by Tom Finney and Nat Lofthouse and Charlton's 31st goal against Wales in October the same year gave him the record alone.\n\nCharlton's role was developing from traditional inside-forward to what today would be termed an attacking midfield player, with Ramsey planning to build the team for the 1966 World Cup around him. When England beat the USA 10-0 in a friendly on 27 May 1964, he scored one goal, his 33rd at senior level for England.\n\nHis goals became a little less frequent, and indeed Jimmy Greaves, playing purely as a striker, would overtake Charlton's England tally in October 1964. Nevertheless, he was still scoring and creating freely and as the tournament was about to start, he was expected to become one of its stars and galvanise his established reputation as one of the world's best footballers.\n\nEngland drew the opening game of the tournament 0–0 with Uruguay, and Charlton scored the first goal in the 2–0 win over Mexico. This was followed by an identical scoreline against France, allowing England to qualify for the quarter finals.\n\nEngland defeated Argentina 1–0 – the game was the only international match in which Charlton received a caution – and faced Portugal in the semi finals. This turned out to be one of Charlton's most important games for England.\n\nCharlton opened the scoring with a crisp side-footed finish after a run by Roger Hunt had forced the Portuguese goalkeeper out of his net; his second was a sweetly struck shot after a run and pull-back from Geoff Hurst. Charlton and Hunt were now England's joint-highest scorers in the tournament with three each, and a final against West Germany beckoned.\n\nThe final turned out to be one of Charlton's quieter days; he and a young Franz Beckenbauer effectively marked each other out of the game. England won 4–2 after extra time.\n\nCharlton's next England game was his 75th as England beat Northern Ireland; 2 caps later and he had become England's second most-capped player, behind the veteran Billy Wright, who was approaching his 100th appearance when Charlton was starting out and ended with 105 caps.\n\nWeeks later he scored his 45th England goal in a friendly against Sweden, breaking the record of 44 set the previous year by Jimmy Greaves. He was then in the England team which made it to the semi-finals of the 1968 European Championships where they were knocked out by Yugoslavia in Florence. During the match Charlton struck a Yugoslav post. England defeated the Soviet Union 2–0 in the third place match.\n\nIn 1969, Charlton was appointed an OBE for services to football. More milestones followed as he won his 100th England cap on 21 April 1970 against Northern Ireland, and was made captain by Ramsey for the occasion. Inevitably, he scored. This was his 48th goal for his country – his 49th and final goal would follow a month later in a 4–0 win over Colombia during a warm-up tour for the 1970 World Cup, designed to get the players adapted to altitude conditions. Charlton's inevitable selection by Ramsey for the tournament made him the first – and still, to date, only – England player to feature in four World Cup squads.\n\nShortly before the World Cup Charlton was involved in the Bogotá Bracelet incident in which he and Bobby Moore were accused of stealing a bracelet from a jewellery store. Moore was later arrested and detained for four days before being granted a conditional release, while Charlton was not arrested.\n\nEngland began the tournament with two victories in the group stages, plus a memorable defeat against Brazil. Charlton played in all three, though was substituted for Alan Ball in the final game of the group against Czechoslovakia. Ramsey, confident of victory and progress to the quarter final, wanted Charlton to rest.\n\nEngland duly reached the last eight where they again faced West Germany. Charlton controlled the midfield and suppressed Franz Beckenbauer's runs from deep as England coasted to a 2–0 lead. Beckenbauer pulled a goal back for the Germans and Ramsey replaced the ageing and tired Charlton with Colin Bell who further tested the German keeper Maier and also provided a great cross for Geoff Hurst who uncharacteristically squandered the chance. West Germany, who had a habit of coming back from behind, eventually scored twice – a back header from Uwe Seeler made it 2–2. In extra-time, Geoff Hurst had a goal mysteriously ruled out after which Gerd Müller's goal won the match 3–2. England were out and, after a record 106 caps and 49 goals, Charlton decided to end his international career at the age of 32. On the flight home from Mexico, he asked Ramsey not to consider him again. His brother Jack, two years his senior but 71 caps his junior, did likewise.\n\nDespite popular opinion the substitution did not change the game as Franz Beckenbauer had scored before Charlton left the field, hence Charlton had failed to cancel out the German. Charlton himself conceded that the substitution did not affect the game in a BBC documentary. His caps record lasted until 1973 when Bobby Moore overtook him, and Charlton currently lies seventh in the all-time England appearances list behind Moore, Wayne Rooney, Ashley Cole, Steven Gerrard, David Beckham and Peter Shilton, whose own England career began in the first game after Charlton's had ended. Charlton's goalscoring record was surpassed by Wayne Rooney on 8 September 2015, when Rooney scored a penalty in a 2–0 win over Switzerland in a qualifying match for UEFA Euro 2016.\n\nCharlton became the manager of Preston North End in 1973, signing his former United and England team-mate Nobby Stiles as player-coach. His first season ended in relegation and although he began playing again he left Preston early in the 1975–76 season after a disagreement with the board over the transfer of John Bird to Newcastle United. He was appointed a CBE that year and began a casual association with BBC for punditry on matches, which continued for many years. In early 1976, he scored once in 3 league appearances for Waterford United.\n\nHe joined Wigan Athletic as a director, and was briefly caretaker manager there in 1983. He then spent some time playing in South Africa. He also built up several businesses in areas such as travel, jewellery and hampers, and ran soccer schools in the UK, the US, Canada, Australia and China. In 1984, he was invited to become member of the board of directors at Manchester United, partly because of his football knowledge and partly because it was felt that the club needed a \"name\" on the board after the resignation of Sir Matt Busby. He remains a director of Manchester United as of 2014 and his continued presence was a factor in placating many fans opposed to the club's takeover by Malcolm Glazer.\n\nHe met his wife, Norma Ball, at an ice rink in Manchester in 1959 and they married in 1961. They have two daughters – Suzanne and Andrea. Suzanne was a weather forecaster for the BBC during the 1990s. They now have grandchildren, including Suzanne's son Robert, who is named in honour of his grandfather.\n\nIn 2007, while publicising his forthcoming autobiography, Charlton revealed that he had a long-running feud with his brother, Jack. They have rarely spoken since a falling-out between his wife Norma and his mother Cissie (who died on 25 March 1996 at the age of 83). Charlton did not see his mother after 1992 as a result of the feud.\n\nJack presented him with his BBC Sports Personality of the Year Lifetime Achievement Award on 14 December 2008. He said that he was 'knocked out' as he was presented the award by his brother. He received a standing ovation as he stood waiting for his prize.\n\nCharlton helped to promote Manchester's bids for the 1996 and 2000 Olympic Games and the 2002 Commonwealth Games, England's bid for the 2006 FIFA World Cup and London's successful bid for the 2012 Summer Olympics. He received a knighthood in 1994 and was an Inaugural Inductee to the English Football Hall of Fame in 2002. On accepting his award he commented \"I'm really proud to be included in the National Football Museum's Hall of Fame. It's a great honour. If you look at the names included I have to say I couldn't argue with them. They are all great players and people I would love to have played with.\" He is also the (honorary) president of the National Football Museum, an organisation about which he said \"I can't think of a better Museum anywhere in the world.\"\n\nOn 2 March 2009, Charlton was given the freedom of the city of Manchester, stating \"I'm just so proud, it's fantastic. It's a great city. I have always been very proud of it.\"\n\nCharlton is involved in a number of charitable activities including fund raising for cancer hospitals. Charlton became involved in the cause of land mine clearance after visits to Bosnia and Cambodia and supports the Mines Advisory Group as well as founding his own charity Find a Better Way which funds research into improved civilian landmine clearance.\n\nIn January 2011 Charlton was voted the 4th greatest Manchester United player of all time by the readers of Inside United and ManUtd.com, behind Ryan Giggs (who topped the poll), Eric Cantona and George Best.\n\nHe is a member of the Laureus World Sports Academy. On 6 February 2012 Sir Bobby Charlton was taken to hospital after falling ill, and subsequently had a gallstone removed. This prevented him from collecting a Lifetime Achievement award at the Laureus World Sports Awards.\n\nOn 15 February 2016 Manchester United announced the South Stand of Old Trafford would be renamed in honour of Sir Bobby Charlton. The unveiling took place at the home game against Everton on 3 April 2016.\n\n\nSource:\n\nSource:\n\nSource:\n\n\n\n"}
{"id": "4227", "url": "https://en.wikipedia.org/wiki?curid=4227", "title": "Barry Lyndon", "text": "Barry Lyndon\n\nBarry Lyndon is a 1975 British-American period drama film written, produced, and directed by Stanley Kubrick, based on the 1844 novel \"The Luck of Barry Lyndon\" by William Makepeace Thackeray. It stars Ryan O'Neal, Marisa Berenson, Patrick Magee, and Hardy Krüger. The film recounts the exploits of a fictional 18th-century Irish adventurer. Exteriors were shot on location in Ireland, England and Germany. \n\nAt the 1975 Academy Awards, the film won four Oscars in production categories. Although having had a modest commercial success and a mixed reception from critics on release, \"Barry Lyndon\" is today regarded as one of Kubrick's finest films. In numerous polls, including those of \"Village Voice\" (1999), \"Sight & Sound\" (2002, 2012), \"Time\" (2005) and BBC, it has been named one of the greatest films ever made.\n\nAn omniscient (though possibly unreliable) narrator relates that in 1750s Ireland, the father of Redmond Barry is killed in a duel over a sale of some horses. The widow, disdaining offers of marriage, devotes herself to her only son.\n\nAs a despondent young man, Barry becomes infatuated with his older cousin, Nora Brady. Though she charms him during a card game, she later shows interest in a well-off British Army captain, John Quin, much to Barry's dismay. Nora and her family plan to leverage their finances through marriage, while Barry holds Quin in contempt and escalates the situation until a fateful duel beside a river when Barry shoots Quin. In the aftermath, Barry is urged to flee from incoming police and head through the countryside towards Dublin, but along the way he is robbed of purse, pistol, and horse by Captain Feeney, an infamous highwayman.\n\nDejected, Barry carries on to the next town, where he hears a promotional spiel to join the British Army, offering the chance at fame and glory (and a lifelong pension) in return for good service. Barry enlists. Some time after joining the regiment, Barry encounters Captain Grogan, a warm-hearted family friend. Grogan informs him that Barry did not in fact kill Quin, his dueling pistol having only been loaded with tow. The duel was staged by Nora's family to be rid of Barry so that their finances would be secured through a lucrative marriage.\n\nBarry’s regiment is sent to Germany to fight in the Seven Years' War, where Captain Grogan is fatally wounded by the French in a skirmish at the Battle of Minden. Fed up with the war, Barry deserts the army, stealing an officer courier's uniform, horse, and identification papers. En route to neutral Holland he encounters the Prussian Captain Potzdorf, who, seeing through his disguise, offers him the choice of being turned back over to the British where he will be shot as a deserter, or enlisting in the Prussian Army. Barry enlists in his second army and later receives a special commendation from Frederick the Great for saving Potzdorf's life in a battle.\n\nTwo years later, after the war ends in 1763, Barry is employed by Captain Potzdorf's uncle in the Prussian Ministry of Police to become the servant of the Chevalier de Balibari, an expirate Irishman and professional gambler. The Prussians suspect he is a spy and send Barry as an undercover agent to verify this. Barry reveals himself to the Chevalier right away and they become confederates at the card table, where Barry and his fine eyesight relay information to his partner. After he and the Chevalier cheat the Prince of Tübingen at the card table, the Prince accuses the Chevalier (without proof) and refuses to pay his debt and demands satisfaction. When Barry relays this to his Prussian handlers, they (still suspecting that the Chevalier is a spy) are wary of allowing another meeting between the Chevalier and the Prince. So, the Prussians arrange for the Chevalier to be expelled from the country. Barry conveys this plan to the Chevalier, who flees in the night. The next morning, Barry, under disguise as the Chevalier, is escorted from Prussian territory by Prussian army officers.\n\nOver the next few years, Barry and the Chevalier travel the spas and parlors of Europe, profiting from their gambling with Barry forcing payment from reluctant debtors with sword duels. Seeing that his life is going nowhere, Barry decides to marry into wealth. At a gambling table in Spa, he encounters the beautiful and wealthy Countess of Lyndon. He seduces and later marries her after the death of her elderly husband, Sir Charles Lyndon.\n\nIn 1773, Barry takes the Countess' last name in marriage and settles in England to enjoy her wealth, still with no money of his own. Lord Bullingdon, Lady Lyndon's ten-year-old son by Sir Charles, does not approve of the marriage and quickly comes to despise Barry, calling him a 'common opportunist' who does not truly love his mother. Barry retaliates by subjecting Bullingdon to systematic physical abuse. \n\nThe Countess bears Barry a son, Bryan Patrick, but the marriage is unhappy: Barry is openly unfaithful and enjoys spending his wife's money on self-indulgent luxuries, while keeping his wife in seclusion.\n\nSome years later, Barry's mother comes to live with him at the Lyndon estate. She warns her son that if Lady Lyndon were to die, all her wealth would go to her first-born son Lord Bullingdon, leaving Barry and his son Bryan penniless. Barry's mother advises him to obtain a noble title to protect himself. To further this goal, he cultivates the acquaintance of the influential Lord Wendover and begins to expend even larger sums of money to ingratiate himself to high society. All this effort is wasted, however, during a birthday party for Lady Lyndon. A now young adult Lord Bullingdon crashes the event where he publicly enumerates the reasons that he detests his stepfather so dearly, declaring it his intent to leave the family estate for as long as Barry remains there and married to his mother. Seething with hatred, Barry savagely assaults Bullingdon until he is pulled off by the guests. This loses Barry all the wealthy and powerful friends he has worked so hard to entreat and he is cast out of polite society. Nevertheless, Bullingdon makes good on his word by leaving the estate and England itself for parts unknown.\n\nIn contrast to his mistreatment of his stepson, Barry proves an overindulgent and doting father to Bryan, with whom he spends all his time after Bullingdon's departure. He cannot refuse his son anything, and succumbs to Bryan's insistence on receiving a full-grown horse for his ninth birthday. The spoiled Bryan disobeys his parents' direct instructions that Bryan ride the horse only in the presence of his father, is thrown by the horse, is paralyzed, and dies a few days later from his injuries.\n\nThe grief-stricken Barry turns to alcohol, while Lady Lyndon seeks solace in religion, assisted by the Reverend Samuel Runt, who had been tutor first to Lord Bullingdon and then to Bryan. Left in charge of the families' affairs while Barry and Lady Lyndon grieve, Barry's mother dismisses the Reverend, both because the family no longer needs (nor can afford, due to Barry's spending debts) a tutor and for fear that his influence worsens Lady Lyndon's condition. Plunging even deeper into grief, Lady Lyndon later attempts suicide (though she ingests only enough poison to make herself ill). The Reverend and the family's accountant Graham then seek out Lord Bullingdon. Upon hearing of these events, Lord Bullingdon returns to England where he finds Barry drunk in a gentlemen's club, mourning the loss of his son rather than being with Lady Lyndon. Bullingdon demands satisfaction for Barry's public assault, challenging him to a duel.\n\nThe duel with pistols is held in a tithe barn. A coin-toss gives Bullingdon the right of first fire, but he nervously misfires his pistol as he prepares to shoot. Barry, reluctant to shoot Bullingdon, magnanimously fires into the ground, but the unmoved Bullingdon refuses to let the duel end, claiming he has not received \"satisfaction\". In the second round, Bullingdon shoots Barry in his left leg. At a nearby inn, a surgeon informs Barry that the leg will need to be amputated below the knee if he is to survive.\n\nWhile Barry is recovering, Bullingdon re-takes control of the Lyndon estate. A few days later, Lord Bullingdon sends a very nervous Graham to the inn with a proposition: Lord Bullingdon will grant Barry an annuity of five hundred guineas a year on the condition that he leave England, with payments ending the moment should Barry ever return. Otherwise, with his credit and bank accounts exhausted, Barry's creditors and bill collectors will assuredly see that he is jailed. Defeated in mind and body, Barry accepts. \n\nThe narrator states that Barry went first back to Ireland with his mother, then to the European continent to resume his former profession of gambler (though without his former success). Barry kept his word and never returned to England or ever saw Lady Lyndon again. The final scene (set in December 1789) shows a middle-aged Lady Lyndon signing Barry's annuity cheque as her son looks on.\n\n\nCritic Tim Robey suggests that the film \"makes you realise that the most undervalued aspect of Kubrick's genius could well be his way with actors.\" He adds that the supporting cast is a \"glittering procession of cameos, not from star names but from vital character players.\"\n\nThe cast featured Leon Vitali as the older Lord Bullingdon, who would then become Kubrick's personal assistant, working as the casting director on his following films, and supervising film-to-video transfers for Kubrick. Their relationship lasted until Kubrick's death. The film's cinematographer, John Alcott, appears at the men's club in the non-speaking role of the man asleep in a chair near the title character when Lord Bullingdon challenges Barry to a duel. Kubrick's daughter Vivian also appears (in an uncredited role) as a guest at Bryan's birthday party.\n\nKubrick stalwarts Patrick Magee (who had played the handicapped writer in \"A Clockwork Orange\") and Philip Stone (who had played Alex's father in the same film, and would go on to play the dead caretaker Grady in \"The Shining\") are featured as the Chevalier du Balibari and as Graham, respectively.\n\nAfter \"\", Kubrick made plans for a film about Napoleon. During pre-production, however, Sergei Bondarchuk and Dino De Laurentiis' \"Waterloo\" was released and subsequently failed at the box office. As a result, Kubrick's financiers pulled their funding for the film and he turned his attention to his next film, \"A Clockwork Orange\". Subsequently, Kubrick showed an interest in Thackeray's \"Vanity Fair\" but dropped the project when a serialised version for television was produced. He told an interviewer, \"At one time, \"Vanity Fair\" interested me as a possible film but, in the end, I decided the story could not be successfully compressed into the relatively short time-span of a feature film...as soon as I read \"Barry Lyndon\" I became very excited about it.\"\n\nHaving garnered Oscar nominations for \"Dr. Strangelove\", \"2001: A Space Odyssey\" and \"A Clockwork Orange\", Kubrick's reputation in the early 1970s was that of \"a perfectionist auteur who loomed larger over his movies than any concept or star.\" His studio—Warner Bros.—was therefore \"eager to bankroll\" his next project, which Kubrick kept \"shrouded in secrecy\" from the press partly due to the furor surrounding the controversially violent \"A Clockwork Orange\" (particularly in the UK) and partly due to his \"long-standing paranoia about the tabloid press.\"\n\nHaving felt compelled to set aside his plans for a film about Napoleon Bonaparte, Kubrick set his sights on Thackeray's 1844 \"satirical picaresque about the fortune-hunting of an Irish rogue,\" \"Barry Lyndon\", the setting of which allowed Kubrick to take advantage of the copious period research he had done for the now-aborted \"Napoleon\". At the time, Kubrick merely announced that his next film would star Ryan O'Neal (deemed \"a seemingly un-Kubricky choice of leading man\") and Marisa Berenson, a former \"Vogue\" and \"Time\" magazine cover model, and be shot largely in Ireland. So heightened was the secrecy surrounding the film that \"Even Berenson, when Kubrick first approached her, was told only that it was to be an 18th-century costume piece [and] she was instructed to keep out of the sun in the months before production, to achieve the period-specific pallor he required.\"\n\nPrincipal photography took 300 days, from spring 1973 through early 1974, with a break for Christmas.\n\nMany of the film's exteriors were shot in Ireland, playing \"itself, England, and Prussia during the Seven Years' War.\" Drawing inspiration from \"the landscapes of Watteau and Gainsborough,\" Kubrick and cinematographer Alcott also relied on the \"scrupulously researched art direction\" of Ken Adam and Roy Walker. Alcott, Adam and Walker would be among those who would win Oscars for their \"amazing work\" on the film.\n\nSeveral of the interior scenes were filmed in Powerscourt House, a famous 18th-century mansion in County Wicklow, Republic of Ireland. The house was destroyed in an accidental fire several months after filming (November 1974), so the film serves as a record of the lost interiors, particularly the \"Saloon\" which was used for more than one scene. The Wicklow Mountains are visible, for example, through the window of the saloon during a scene set in Berlin. Other locations included Kells Priory (the English Redcoat encampment) Blenheim Palace, Castle Howard (exteriors of the Lyndon estate), Huntington Castle, Clonegal (exterior), Corsham Court (various interiors and the music room scene), Petworth House (chapel, and so on.), Stourhead (lake and temple), Longleat, and Wilton House (interior and exterior) in England, Dunrobin Castle (exterior and garden as Spa) in Scotland, Dublin Castle in Ireland (the chevalier's home), Ludwigsburg Palace near Stuttgart and Frederick the Great's Neues Palais at Potsdam near Berlin (suggesting Berlin's main street Unter den Linden as construction in Potsdam had just begun in 1763). Some exterior shots were also filmed at Waterford Castle (now a luxury hotel and golf course) and Little Island, Waterford. Moorstown Castle in Tipperary also featured. Several scenes were filmed at Castletown House outside Carrick-on-Suir, Co. Tipperary, and at Youghal, Co. Cork.\n\nThe film—as with \"almost every Kubrick film\"—is a \"showcase for [a] major innovation in technique.\" While \"2001: A Space Odyssey\" had featured \"revolutionary effects,\" and \"The Shining\" would later feature heavy use of the Steadicam, \"Barry Lyndon\" saw a considerable number of sequences shot \"without recourse to electric light.\" Cinematography was overseen by director of photography John Alcott (who won an Oscar for his work), and is particularly noted for the technical innovations that made some of its most spectacular images possible. To achieve photography without electric lighting \"[f]or the many densely furnished interior scenes... meant shooting by candlelight,\" which is known to be difficult in still photography, \"let alone with moving images.\"\nKubrick was \"determined not to reproduce the set-bound, artificially lit look of other costume dramas from that time.\" After \"tinker[ing] with different combinations of lenses and film stock,\" the production obtained three super-fast 50mm lenses (Carl Zeiss Planar 50mm f/0.7) developed by Zeiss for use by NASA in the Apollo moon landings, which Kubrick had discovered. These super-fast lenses \"with their huge aperture (the film actually features the lowest f-stop in film history) and fixed focal length\" were problematic to mount, and were extensively modified into three versions by Cinema Products Corp. for Kubrick so to gain a wider angle of view, with input from optics expert Richard Vetter of Todd-AO. The rear element of the lens had to be 2.5mm away from the film plane, requiring special modification to the rotating camera shutter. This allowed Kubrick and Alcott to shoot scenes lit with actual candles to an average lighting volume of only three candela, \"recreating the huddle and glow of a pre-electrical age.\" In addition, Kubrick had the entire film push-developed by one stop.\n\nAlthough Kubrick's express desire was to avoid electric lighting where possible, most shots were achieved with conventional lenses and lighting, but were lit to deliberately mimic natural light rather than for compositional reasons. In addition to potentially seeming more realistic, these methods also gave a particular period look to the film which has often been likened to 18th-century paintings (which were, of course, depicting a world devoid of electric lighting), in particular owing \"a lot to William Hogarth, with whom Thackeray had always been fascinated.\"\nAccording to critic Tim Robey, the film has a \"stately, painterly, often determinedly static quality.\" For example, to help light some interior scenes, lights were placed outside and aimed through the windows, which were covered in a diffuse material to scatter the light evenly through the room rather than being placed inside for maximum use as most conventional films do. A sign of this method occurs in the scene where Barry duels Lord Bullingdon. Though it appears to be lit entirely with natural light, one can see that the light coming in through the cross-shaped windows in the tithe barn appears blue in color, while the main lighting of the scene coming in from the side is not. This is because the light through the cross-shaped windows is daylight from the sun, which when recorded on the film stock used by Kubrick showed up as blue-tinted compared to the incandescent electric light coming in from the side.\n\nDespite such slight tinting effects, this method of lighting not only gave the look of natural daylight coming in through the windows, but it also protected the historic locations from the damage caused by mounting the lights on walls or ceilings and the heat from the lights. This helped the film \"fit... perfectly with Kubrick's gilded-cage aesthetic – the film is consciously a museum piece, its characters pinned to the frame like butterflies.\"\n\nThe film's period setting allowed Kubrick to indulge his penchant for classical music, and the film score uses pieces by Johann Sebastian Bach (an arrangement of the Concerto for violin and oboe in C minor), Antonio Vivaldi (Cello Concerto in E-Minor, a transcription of the Cello Sonata in E Minor RV 40), Giovanni Paisiello, Wolfgang Amadeus Mozart, and Franz Schubert (German Dance No. 1 in C major, Piano Trio in E-Flat, Opus 100 and Impromptu No. 1 in C minor), as well as the Hohenfriedberger March. The piece most associated with the film, however, is the main title music: George Frideric Handel's stately \"Sarabande\" from the Suite in D minor HWV 437. Originally for solo harpsichord, the versions for the main and end titles are performed very romantically with orchestral strings, harpsichord, and timpani. It is used at various points in the film, in various arrangements, to indicate the implacable working of impersonal fate.\n\nThe score also includes Irish folk music, including Seán Ó Riada's song \"Women of Ireland\", arranged by Paddy Moloney and performed by The Chieftains. The British Grenadiers also features in scenes with Redcoats marching.\n\nThe film \"was not the commercial success Warner Bros. had been hoping for\" within the United States, although it fared better in Europe. This mixed reaction saw the film (in the words of one retrospective review) \"greeted, on its release, with dutiful admiration – but not love. Critics... rail[ed] against the perceived coldness of Kubrick's style, the film's self-conscious artistry and slow pace. Audiences, on the whole, rather agreed...\" This \"air of disappointment\" factored into Kubrick's decision for his next film – Stephen King's \"The Shining\" – a project that would not only please him artistically, but also be more likely to succeed financially. Still, several other critics, including Gene Siskel, praised the film's technical quality and strong narrative, and Siskel himself counted it as one of the five best films of the year.\n\nIn recent years, the film has gained a more positive reaction. it holds a 97% \"Certified Fresh\" rating on Rotten Tomatoes based on 52 reviews, eight of which are from the site's \"top critics.\" Roger Ebert added the film to his 'Great Movies' list on 9 September 2009, writing, \"It defies us to care, it asks us to remain only observers of its stately elegance\", and it \"must be one of the most beautiful films ever made.\"\n\nDirector Martin Scorsese has named \"Barry Lyndon\" as his favorite Kubrick film, and it is also one of Lars von Trier's favorite films. Quotations from its script have also appeared in such disparate works as Ridley Scott's \"The Duellists\", Scorsese's \"The Age of Innocence\", and Wes Anderson's \"Rushmore\".\n\nIn 1976, at the 48th Academy Awards, the film won four awards, for Best Art Direction (Ken Adam, Roy Walker, Vernon Dixon), Best Cinematography (John Alcott), Best Costume Design (Milena Canonero, Ulla-Britt Söderlund) and Best Musical Score (Leonard Rosenman, \"for his arrangements of Schubert and Handel\".) Kubrick was nominated three times, for Best Director, Best Picture, and Best Adapted Screenplay.\n\nKubrick won the British Academy of Film and Television Arts Award for Best Direction. John Alcott won for Best Cinematography. \"Barry Lyndon\" was also nominated for Best Film, Art Direction, and Costume Design.\n\nKubrick based his adapted screenplay on William Makepeace Thackeray's \"The Luck of Barry Lyndon\" (republished as the novel \"Memoirs of Barry Lyndon, Esq.),\" a picaresque tale written and published in serial form in 1844.\n\nThe film departs from the novel in several ways. In Thackeray's writings, events are related in the first person by Barry himself. A comic tone pervades the work, as Barry proves both a raconteur and an unreliable narrator. Kubrick's film, by contrast, presents the story objectively. Though the film contains voice-over (by actor Michael Hordern), the comments expressed are not Barry's, but those of an omniscient narrator. Kubrick felt that using a first-person narrative would not be useful in a film adaptation:\n\nKubrick also changed the plot. For example, the novel does not include a final duel. The film begins with a duel where Barry's father is shot dead, and duels recur throughout the film.\n\n"}
{"id": "4230", "url": "https://en.wikipedia.org/wiki?curid=4230", "title": "Cell (biology)", "text": "Cell (biology)\n\nThe cell (from Latin \"cella\", meaning \"small room\") is the basic structural, functional, and biological unit of all known living organisms. A cell is the smallest unit of life that can replicate independently, and cells are often called the \"building blocks of life\". The study of cells is called cell biology.\n\nCells consist of cytoplasm enclosed within a membrane, which contains many biomolecules such as proteins and nucleic acids. Organisms can be classified as unicellular (consisting of a single cell; including bacteria) or multicellular (including plants and animals). While the number of cells in plants and animals varies from species to species, humans contain more than 10 trillion (10) cells. Most plant and animal cells are visible only under a microscope, with dimensions between 1 and 100 micrometres.\n\nThe cell was discovered by Robert Hooke in 1665, who named the biological units for their resemblance to cells inhabited by Christian monks in a monastery. Cell theory, first developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that cells are the fundamental unit of structure and function in all living organisms, that all cells come from preexisting cells, and that all cells contain the hereditary information necessary for regulating cell functions and for transmitting information to the next generation of cells. Cells emerged on Earth at least 3.5 billion years ago.\n\nCells are of two types, eukaryotic, which contain a nucleus, and prokaryotic, which do not. Prokaryotes are single-celled organisms, while eukaryotes can be either single-celled or multicellular.\n\nProkaryotic cells were the first form of life on Earth, characterised by having vital biological processes including cell signaling and being self-sustaining. They are simpler and smaller than eukaryotic cells, and lack membrane-bound organelles such as the nucleus. Prokaryotes include two of the domains of life, bacteria and archaea. The DNA of a prokaryotic cell consists of a single chromosome that is in direct contact with the cytoplasm. The nuclear region in the cytoplasm is called the nucleoid. Most prokaryotes are the smallest of all organisms ranging from 0.5 to 2.0 µm in diameter.\n\nA prokaryotic cell has three architectural regions:\n\nPlants, animals, fungi, slime moulds, protozoa, and algae are all eukaryotic. These cells are about fifteen times wider than a typical prokaryote and can be as much as a thousand times greater in volume. The main distinguishing feature of eukaryotes as compared to prokaryotes is compartmentalization: the presence of membrane-bound organelles (compartments) in which specific metabolic activities take place. Most important among these is a cell nucleus, an organelle that houses the cell's DNA. This nucleus gives the eukaryote its name, which means \"true kernel (nucleus)\". Other differences include:\n\nAll cells, whether prokaryotic or eukaryotic, have a membrane that envelops the cell, regulates what moves in and out (selectively permeable), and maintains the electric potential of the cell. Inside the membrane, the cytoplasm takes up most of the cell's volume. All cells (except red blood cells which lack a cell nucleus and most organelles to accommodate maximum space for hemoglobin) possess DNA, the hereditary material of genes, and RNA, containing the information necessary to build various proteins such as enzymes, the cell's primary machinery. There are also other kinds of biomolecules in cells. This article lists these primary components of the cell, then briefly describes their function.\n\nThe cell membrane, or plasma membrane, is a biological membrane that surrounds the cytoplasm of a cell. In animals, the plasma membrane is the outer boundary of the cell, while in plants and prokaryotes it is usually covered by a cell wall. This membrane serves to separate and protect a cell from its surrounding environment and is made mostly from a double layer of phospholipids, which are amphiphilic (partly hydrophobic and partly hydrophilic). Hence, the layer is called a phospholipid bilayer, or sometimes a fluid mosaic membrane. Embedded within this membrane is a variety of protein molecules that act as channels and pumps that move different molecules into and out of the cell. The membrane is said to be 'semi-permeable', in that it can either let a substance (molecule or ion) pass through freely, pass through to a limited extent or not pass through at all. Cell surface membranes also contain receptor proteins that allow cells to detect external signaling molecules such as hormones.\n\nThe cytoskeleton acts to organize and maintain the cell's shape; anchors organelles in place; helps during endocytosis, the uptake of external materials by a cell, and cytokinesis, the separation of daughter cells after cell division; and moves parts of the cell in processes of growth and mobility. The eukaryotic cytoskeleton is composed of microfilaments, intermediate filaments and microtubules. There are a great number of proteins associated with them, each controlling a cell's structure by directing, bundling, and aligning filaments. The prokaryotic cytoskeleton is less well-studied but is involved in the maintenance of cell shape, polarity and cytokinesis. The subunit protein of microfilaments is a small, monomeric protein called actin. The subunit of microtubules is a dimeric molecule called tubulin. Intermediate filaments are heteropolymers whose subunits vary among the cell types in different tissues. But some of the subunit protein of intermediate filaments include vimentin, desmin, lamin (lamins A, B and C), keratin (multiple acidic and basic keratins), neurofilament proteins (NF - L, NF - M).\n\nTwo different kinds of genetic material exist: deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). Cells use DNA for their long-term information storage. The biological information contained in an organism is encoded in its DNA sequence. RNA is used for information transport (e.g., mRNA) and enzymatic functions (e.g., ribosomal RNA). Transfer RNA (tRNA) molecules are used to add amino acids during protein translation.\n\nProkaryotic genetic material is organized in a simple circular DNA molecule (the bacterial chromosome) in the nucleoid region of the cytoplasm. Eukaryotic genetic material is divided into different, linear molecules called chromosomes inside a discrete nucleus, usually with additional genetic material in some organelles like mitochondria and chloroplasts (see endosymbiotic theory).\n\nA human cell has genetic material contained in the cell nucleus (the nuclear genome) and in the mitochondria (the mitochondrial genome). In humans the nuclear genome is divided into 46 linear DNA molecules called chromosomes, including 22 homologous chromosome pairs and a pair of sex chromosomes. The mitochondrial genome is a circular DNA molecule distinct from the nuclear DNA. Although the mitochondrial DNA is very small compared to nuclear chromosomes, it codes for 13 proteins involved in mitochondrial energy production and specific tRNAs.\n\nForeign genetic material (most commonly DNA) can also be artificially introduced into the cell by a process called transfection. This can be transient, if the DNA is not inserted into the cell's genome, or stable, if it is. Certain viruses also insert their genetic material into the genome.\n\nOrganelles are parts of the cell which are adapted and/or specialized for carrying out one or more vital functions, analogous to the organs of the human body (such as the heart, lung, and kidney, with each organ performing a different function). Both eukaryotic and prokaryotic cells have organelles, but prokaryotic organelles are generally simpler and are not membrane-bound.\n\nThere are several types of organelles in a cell. Some (such as the nucleus and golgi apparatus) are typically solitary, while others (such as mitochondria, chloroplasts, peroxisomes and lysosomes) can be numerous (hundreds to thousands). The cytosol is the gelatinous fluid that fills the cell and surrounds the organelles.\n\n\n\nMany cells also have structures which exist wholly or partially outside the cell membrane. These structures are notable because they are not protected from the external environment by the semipermeable cell membrane. In order to assemble these structures, their components must be carried across the cell membrane by export processes.\n\nMany types of prokaryotic and eukaryotic cells have a cell wall. The cell wall acts to protect the cell mechanically and chemically from its environment, and is an additional layer of protection to the cell membrane. Different types of cell have cell walls made up of different materials; plant cell walls are primarily made up of cellulose, fungi cell walls are made up of chitin and bacteria cell walls are made up of peptidoglycan.\n\nA gelatinous capsule is present in some bacteria outside the cell membrane and cell wall. The capsule may be polysaccharide as in pneumococci, meningococci or polypeptide as \"Bacillus anthracis\" or hyaluronic acid as in streptococci.\nCapsules are not marked by normal staining protocols and can be detected by India ink or methyl blue; which allows for higher contrast between the cells for observation.\n\nFlagella are organelles for cellular mobility. The bacterial flagellum stretches from cytoplasm through the cell membrane(s) and extrudes through the cell wall. They are long and thick thread-like appendages, protein in nature. A different type of flagellum is found in archaea and a different type is found in eukaryotes.\n\nA fimbria also known as a pilus is a short, thin, hair-like filament found on the surface of bacteria. Fimbriae, or pili are formed of a protein called pilin (antigenic) and are responsible for attachment of bacteria to specific receptors of human cell (cell adhesion). There are special types of specific pili involved in bacterial conjugation.\n\nBetween successive cell divisions, cells grow through the functioning of cellular metabolism. Cell metabolism is the process by which individual cells process nutrient molecules. Metabolism has two distinct divisions: catabolism, in which the cell breaks down complex molecules to produce energy and reducing power, and anabolism, in which the cell uses energy and reducing power to construct complex molecules and perform other biological functions.\nComplex sugars consumed by the organism can be broken down into simpler sugar molecules called monosaccharides such as glucose. Once inside the cell, glucose is broken down to make adenosine triphosphate (ATP), a molecule that possesses readily available energy, through two different pathways.\n\nCell division involves a single cell (called a \"mother cell\") dividing into two daughter cells. This leads to growth in multicellular organisms (the growth of tissue) and to procreation (vegetative reproduction) in unicellular organisms. Prokaryotic cells divide by binary fission, while eukaryotic cells usually undergo a process of nuclear division, called mitosis, followed by division of the cell, called cytokinesis. A diploid cell may also undergo meiosis to produce haploid cells, usually four. Haploid cells serve as gametes in multicellular organisms, fusing to form new diploid cells.\n\nDNA replication, or the process of duplicating a cell's genome, always happens when a cell divides through mitosis or binary fission. This occurs during the S phase of the cell cycle.\n\nIn meiosis, the DNA is replicated only once, while the cell divides twice. DNA replication only occurs before meiosis I. DNA replication does not occur when the cells divide the second time, in meiosis II. Replication, like all cellular activities, requires specialized proteins for carrying out the job.\n\nCells are capable of synthesizing new proteins, which are essential for the modulation and maintenance of cellular activities. This process involves the formation of new protein molecules from amino acid building blocks based on information encoded in DNA/RNA. Protein synthesis generally consists of two major steps: transcription and translation.\n\nTranscription is the process where genetic information in DNA is used to produce a complementary RNA strand. This RNA strand is then processed to give messenger RNA (mRNA), which is free to migrate through the cell. mRNA molecules bind to protein-RNA complexes called ribosomes located in the cytosol, where they are translated into polypeptide sequences. The ribosome mediates the formation of a polypeptide sequence based on the mRNA sequence. The mRNA sequence directly relates to the polypeptide sequence by binding to transfer RNA (tRNA) adapter molecules in binding pockets within the ribosome. The new polypeptide then folds into a functional three-dimensional protein molecule.\n\nUnicellular organisms can move in order to find food or escape predators. Common mechanisms of motion include flagella and cilia.\n\nIn multicellular organisms, cells can move during processes such as wound healing, the immune response and cancer metastasis. For example, in wound healing in animals, white blood cells move to the wound site to kill the microorganisms that cause infection. Cell motility involves many receptors, crosslinking, bundling, binding, adhesion, motor and other proteins. The process is divided into three steps – protrusion of the leading edge of the cell, adhesion of the leading edge and de-adhesion at the cell body and rear, and cytoskeletal contraction to pull the cell forward. Each step is driven by physical forces generated by unique segments of the cytoskeleton.\n\nMulticellular organisms are organisms that consist of more than one cell, in contrast to single-celled organisms.\n\nIn complex multicellular organisms, cells specialize into different cell types that are adapted to particular functions. In mammals, major cell types include skin cells, muscle cells, neurons, blood cells, fibroblasts, stem cells, and others. Cell types differ both in appearance and function, yet are genetically identical. Cells are able to be of the same genotype but of different cell type due to the differential expression of the genes they contain.\n\nMost distinct cell types arise from a single totipotent cell, called a zygote, that differentiates into hundreds of different cell types during the course of development. Differentiation of cells is driven by different environmental cues (such as cell–cell interaction) and intrinsic differences (such as those caused by the uneven distribution of molecules during division).\n\nMulticellularity has evolved independently at least 25 times, including in some prokaryotes, like cyanobacteria, myxobacteria, actinomycetes, \"Magnetoglobus multicellularis\" or \"Methanosarcina\". However, complex multicellular organisms evolved only in six eukaryotic groups: animals, fungi, brown algae, red algae, green algae, and plants. It evolved repeatedly for plants (Chloroplastida), once or twice for animals, once for brown algae, and perhaps several times for fungi, slime molds, and red algae. Multicellularity may have evolved from colonies of interdependent organisms, from cellularization, or from organisms in symbiotic relationships.\n\nThe first evidence of multicellularity is from cyanobacteria-like organisms that lived between 3 and 3.5 billion years ago. Other early fossils of multicellular organisms include the contested Grypania spiralis and the fossils of the black shales of the Palaeoproterozoic Francevillian Group Fossil B Formation in Gabon.\n\nThe evolution of multicellularity from unicellular ancestors has been replicated in the laboratory, in evolution experiments using predation as the selective pressure.\n\nThe origin of cells has to do with the origin of life, which began the history of life on Earth.\n\nThere are several theories about the origin of small molecules that led to life on the early Earth. They may have been carried to Earth on meteorites (see Murchison meteorite), created at deep-sea vents, or synthesized by lightning in a reducing atmosphere (see Miller–Urey experiment). There is little experimental data defining what the first self-replicating forms were. RNA is thought to be the earliest self-replicating molecule, as it is capable of both storing genetic information and catalyzing chemical reactions (see RNA world hypothesis), but some other entity with the potential to self-replicate could have preceded RNA, such as clay or peptide nucleic acid.\n\nCells emerged at least 3.5 billion years ago. The current belief is that these cells were heterotrophs. The early cell membranes were probably more simple and permeable than modern ones, with only a single fatty acid chain per lipid. Lipids are known to spontaneously form bilayered vesicles in water, and could have preceded RNA, but the first cell membranes could also have been produced by catalytic RNA, or even have required structural proteins before they could form.\n\nThe eukaryotic cell seems to have evolved from a symbiotic community of prokaryotic cells. DNA-bearing organelles like the mitochondria and the chloroplasts are descended from ancient symbiotic oxygen-breathing proteobacteria and cyanobacteria, respectively, which were endosymbiosed by an ancestral archaean prokaryote.\n\nThere is still considerable debate about whether organelles like the hydrogenosome predated the origin of mitochondria, or vice versa: see the hydrogen hypothesis for the origin of eukaryotic cells.\n\n\n\n"}
{"id": "4231", "url": "https://en.wikipedia.org/wiki?curid=4231", "title": "Buffy the Vampire Slayer (film)", "text": "Buffy the Vampire Slayer (film)\n\nBuffy the Vampire Slayer is a 1992 American comedy horror film about a Valley girl cheerleader named Buffy who learns that it is her fate to hunt vampires. The film starred Kristy Swanson, Donald Sutherland, Paul Reubens, Rutger Hauer, Luke Perry, and Hilary Swank. It was a moderate success at the box office, but received mixed reception from critics. The film was taken in a different direction from the one its writer Joss Whedon intended, and five years later, he created the darker and acclaimed TV series of the same name.\n\nHigh school senior Buffy Summers (Kristy Swanson) is introduced as a stereotypical, shallow cheerleader at Hemery High School in Los Angeles. She is a carefree popular girl whose main concerns are shopping and spending time with her rich, snooty friends and her boyfriend, Jeffrey. While at school one day, she is approached by a man who calls himself Merrick (Donald Sutherland). He informs her that she is The Slayer, or Chosen One, destined to kill vampires, and he is a Watcher whose duty it is to guide and train her. She initially rebukes his claims, but is convinced that he is right when he is able to describe a recurring dream of hers in detail. In addition, Buffy is exhibiting uncanny abilities not known to her, including heightened agility, senses, and endurance, yet she repeatedly tries Merrick's patience with her frivolous nature, indifference to slaying and sharp-tongued remarks.\n\nAfter several successful outings, Buffy is drawn into conflict with Lothos (Rutger Hauer), a local vampire king and his acolyte, Amilyn (Paul Reubens). Two young men, Oliver Pike (Luke Perry), and best friend Benny (David Arquette), who resented Buffy and her friends due to differing social circles, are out drinking when they are attacked by Amilyn. Benny is turned but Pike is saved by Merrick. As a vampire, Benny visits Pike and tries to get him to join him. Later, when Pike and his boss are discussing Benny, Pike tells him to run if he sees him. Not only this, but a studious girl from Buffy's class, Cassandra, is abducted one night by Amilyn and sacrificed to Lothos. When her body is found, the news spreads through LA and Hemery High, but her murder is met with indifference from Buffy's clique.\n\nWhen Pike realizes there is something wrong with Benny and that he is no longer safe, he decides to leave town. His plan is thwarted, however, when he encounters Amilyn and his tribe of vampires. Amilyn hitches a ride on the hood of his van which crashes into a tree just before Amilyn loses an arm. Buffy and Merrick arrive to rescue him and Amilyn flees the fight to talk to Lothos. After this encounter, Buffy and Pike start a friendship, which eventually becomes romantic and Pike becomes Buffy's partner in fighting the undead.\n\nDuring a basketball game, Buffy finds out that one of the players, and a friend of Jeffrey's, is a vampire. After a quick chase to a parade float storage yard, Buffy finally confronts Lothos, shortly after she and Pike take down his gang. Lothos puts Buffy in a hypnotic trance, which is broken due to Merrick's intervention. Lothos turns on Merrick and stab him with the stake he attempted to use on him. Lothos leaves, saying that Buffy is not ready. As Merrick dies, he tells Buffy to do things her own way rather than live by the rules of others and he says \"remember about the music.\" Because of her new life, responsibilities, and heartbreak, Buffy becomes emotionally shocked and starts dropping her Slayer duties. When she arrives at school, she attempts to explain everything to her friends, but they refuse to understand her as they are more concerned with their upcoming school dance, and Buffy falls out with them as she realizes she is outgrowing their immature, selfish behavior.\n\nAt the senior dance, Buffy tries to patch things up with her friends but they turn against her, and she is dismayed to find Jeffrey has dumped her for one of her friends. However, she meets up with Pike and as they start to dance and kiss, Lothos leads the remainder of his minions to the school and attacks the students and the attending faculty. Buffy confronts the vampires outside while Pike fights the vampiric Benny. After overpowering the vampires, she confronts Lothos inside the school and kills Amilyn. Lothos hypnotizes Buffy again and when the dance music stops, she remembers Merrick's words and is ready to defend herself. Lothos ignites her cross but she uses hairspray to create a makeshift flame-thrower and burns him before escaping back into the gym. Buffy sees everybody recover from the attack, but Lothos emerges again getting into a fight with Buffy, who then stakes him.\n\nAs all of the survivors leave, Buffy and Pike decide to finish their dance. The film then ends with the two of them leaving the dance on a motorcycle, and a news crew interviewing the students and the principal about the attack during the credits.\n\nMany of the details given in the film differ from the continuity of the later television series. For example, Buffy's age and history are dissimilar; she is a senior in high school in the film, but the series starts with her as a sophomore. However, the film does portray who Buffy in the TV series was before she learned of her destiny as the Slayer: a popular but selfish and air-headed cheerleader. In the film, her parents are wealthy but negligent socialites who care little for her and spend their time at parties and golf tournaments; in the TV show, Buffy has a caring, single mother named Joyce. The supernatural abilities of both vampires and the Slayer are depicted differently. The vampires in the film die like humans, while in the TV show they turn to dust, and, unlike the TV show, their faces remain human, albeit pale and fanged, whereas in the series they are able to take on a demonic aspect. In the film, Merrik is hundreds of years old, having lived many lives training many slayers, whereas in the series, watchers are mortal. Merrik's British accent and the manner of his death are also changed when he appears in flash-backs in the series.\n\nJoss Whedon has expressed his disapproval with the movie's interpretation of the script, stating \"I finally sat down and had written it and somebody had made it into a movie, and I felt like — well, that's not quite her. It's a start, but it's not quite the girl.\n\nAccording to the \"Official Buffy Watcher's Guide\", Whedon wrote the pilot to the TV series as a sequel to his original script, which is why the show makes references to events that did not occur in the film. In 1999, Dark Horse Comics released a graphic novel adaptation of Whedon's original script under the title \"The Origin\". Whedon stated: \"The \"Origin\" comic, though I have issues with it, CAN pretty much be accepted as canonical. They did a cool job of combining the movie script with the series, that was nice, and using the series Merrick and not a certain OTHER thespian who shall remain hated.\"\n\nWriter Whedon sold the movie to Dolly Parton’s production company Sandollar, in the fall of 1991.\n\nThe film debuted at #5 at the North American box office and eventually grossed a modest $16,624,456 against a $7 million production budget.\n\nThe film was released on VHS and Laserdisc in the U.S. in 1992 by Fox Video and re-released in 1995 under the \"Twentieth Century Fox Selections\" banner. It was released on DVD in the US in 2001 and on Blu-ray in 2011.\n\nThe soundtrack does not include every song played in the film, which also included \"In the Wind\" by War Babies and \"Inner Mind\" by Eon.\n\nThe film received generally mixed reviews. It holds a rating of 48% on review aggregator website Metacritic, indicating mixed or average reviews.\n\nOn May 25, 2009, \"The Hollywood Reporter\" reported that Roy Lee and Doug Davison of Vertigo Entertainment were working with Fran Rubel Kuzui and Kaz Kuzui on a re-envisioning or relaunch of the \"Buffy\" film for the big screen. The film would not be a sequel or prequel to the existing film or television franchise and Joss Whedon would have no involvement in the project. None of the characters, cast, or crew from the television series would be featured. Television series executive producer Marti Noxon later reflected that this story might have been produced by the studio in order to frighten Whedon into taking the reins of the project. On November 22, 2010, \"The Hollywood Reporter\" confirmed that Warner Bros. had picked up the movie rights to the remake. The film was set for release sometime in 2012. 20th Century Fox, which usually holds rights to the more successful \"Buffy\"/\"Angel\" television franchise, will retain merchandising and some distribution rights.\n\nThe idea of the remake caused wrath among fans of the TV series, since Whedon is not involved and the project does not have any connection with the show and will not conform to the continuity maintained with the \"Buffy the Vampire Slayer Season Eight\" and \"Season Nine\" comic book titles. Not only the fandom, but the main cast members of both the \"Buffy\" and \"Angel\" series expressed disagreement with the report on Twitter and in recent interviews. Sarah Michelle Gellar said, \"I think it's a horrible idea. To try to do a \"Buffy\" without Joss Whedon... to be incredibly non-eloquent: that's the dumbest idea I've ever heard.\" Proposed shooting locations included Black Wood and other areas in rural England, due to budgetary constraints and the potential setting being outside of the city, an unusual change for the franchise.\n\nIn December 2011, more than a year after the official reboot announcement, the \"Los Angeles Times\" site reported that Whit Anderson, the writer picked for the new \"Buffy\" movie, had her script rejected by the producers behind the project, and that a new writer was being sought. Sources also stated that \"If you're going to bring it back, you have to do it right. [Anderson] came in with some great ideas and she had reinvented some of the lore and it was pretty cool but in the end there just wasn't enough on the page.\"\n\nAs of June 2017, there have been no further developments regarding the reboot.\n\n\n"}
{"id": "4232", "url": "https://en.wikipedia.org/wiki?curid=4232", "title": "Barter", "text": "Barter\n\nBarter is a system of exchange where goods or services are directly exchanged for other goods or services without using a medium of exchange, such as money. It is distinguishable from gift economies in many ways; one of them is that the reciprocal exchange is immediate and not delayed in time. It is usually bilateral, but may be multilateral (i.e., mediated through a trade exchange) and, in most developed countries, usually only exists parallel to monetary systems to a very limited extent. Barter, as a replacement for money as the method of exchange, is used in times of monetary crisis, such as when the currency may be either unstable (e.g., hyperinflation or deflationary spiral) or simply unavailable for conducting commerce.\n\nEconomists since Adam Smith, looking at non-specific archaic societies as examples, have used the inefficiency of barter to explain the emergence of money, the economy, and hence the discipline of economics itself. However, ethnographic studies have shown no present or past society has used barter without any other medium of exchange or measurement, nor have anthropologists found evidence that money emerged from barter, instead finding that gift-giving (credit extended on a personal basis with an inter-personal balance maintained over the long term) was the most usual means of exchange of goods and services.\n\nSince the 1830s, barter in some western market economies has been aided by exchanges that use alternative currencies based on the labour theory of value, and are designed to prevent profit taking by intermediators. Examples include the Owenite socialists, the Cincinnati Time store, and more recently Ithaca HOURS (Time banking) and the LETS system.\n\nAdam Smith, the father of modern economics, sought to demonstrate that markets (and economies) pre-existed the state, and hence should be free of government regulation. He argued (against conventional wisdom) that money was not the creation of governments. Markets emerged, in his view, out of the division of labour, by which individuals began to specialize in specific crafts and hence had to depend on others for subsistence goods. These goods were first exchanged by barter. Specialization depended on trade, but was hindered by the \"double coincidence of wants\" which barter requires, i.e., for the exchange to occur, each participant must want what the other has. To complete this hypothetical history, craftsmen would stockpile one particular good, be it salt or metal, that they thought no one would refuse. This is the origin of money according to Smith. Money, as a universally desired medium of exchange, allows each half of the transaction to be separated.\n\nBarter is characterized in Adam Smith's \"\"The Wealth of Nations\"\" by a disparaging vocabulary: \"higgling, haggling, swapping, dickering.\" It has also been characterized as negative reciprocity, or \"selfish profiteering.\"\n\nAnthropologists have argued, in contrast, \"that when something resembling barter \"does\" occur in stateless societies it is almost always between strangers.\" Barter occurred between strangers, not fellow villagers, and hence cannot be used to naturalistically explain the origin of money without the state. Since most people engaged in trade knew each other, exchange was fostered through the extension of credit. Marcel Mauss, author of 'The Gift', argued that the first economic contracts were to \"not\" act in one's economic self-interest, and that before money, exchange was fostered through the processes of reciprocity and redistribution, not barter. Everyday exchange relations in such societies are characterized by generalized reciprocity, or a non-calculative familial \"communism\" where each takes according to their needs, and gives as they have.\n\nSince direct barter does not require payment in money, it can be utilized when money is in short supply, when there is little information about the credit worthiness of trade partners, or when there is a lack of trust between those trading.\n\nBarter is an option to those who cannot afford to store their small supply of wealth in money, especially in hyperinflation situations where money devalues quickly.\n\nThe limitations of barter are often explained in terms of its inefficiencies in facilitating exchange in comparison to money.\n\nIt is said that barter is 'inefficient' because:\n\n\nOther anthropologists have questioned whether barter is typically between \"total\" strangers, a form of barter known as \"silent trade\". Silent trade, also called silent barter, dumb barter (\"dumb\" here used in its old meaning of \"mute\"), or depot trade, is a method by which traders who cannot speak each other's language can trade without talking. However, Benjamin Orlove has shown that while barter occurs through \"silent trade\" (between strangers), it also occurs in commercial markets as well. \"Because barter is a difficult way of conducting trade, it will occur only where there are strong institutional constraints on the use of money or where the barter symbolically denotes a special social relationship and is used in well-defined conditions. To sum up, multipurpose money in markets is like lubrication for machines - necessary for the most efficient function, but not necessary for the existence of the market itself.\"\n\nIn his analysis of barter between coastal and inland villages in the Trobriand Islands, Keith Hart highlighted the difference between highly ceremonial gift exchange between community leaders, and the barter that occurs between individual households. The haggling that takes place between strangers is possible because of the larger temporary political order established by the gift exchanges of leaders. From this he concludes that barter is \"an atomized interaction predicated upon the presence of society\" (i.e. that social order established by gift exchange), and not typical between complete strangers.\n\nAs Orlove noted, barter may occur in commercial economies, usually during periods of monetary crisis. During such a crisis, currency may be in short supply, or highly devalued through hyperinflation. In such cases, money ceases to be the universal medium of exchange or standard of value. Money may be in such short supply that it becomes an item of barter itself rather than the means of exchange. Barter may also occur when people cannot afford to keep money (as when hyperinflation quickly devalues it).\n\nEconomic historian Karl Polanyi has argued that where barter is widespread, and cash supplies limited, barter is aided by the use of credit, brokerage, and money as a unit of account (i.e. used to price items). All of these strategies are found in ancient economies including Ptolemaic Egypt. They are also the basis for more recent barter exchange systems.\n\nWhile one-to-one bartering is practiced between individuals and businesses on an informal basis, organized barter exchanges have developed to conduct third party bartering which helps overcome some of the limitations of barter. A barter exchange operates as a broker and bank in which each participating member has an account that is debited when purchases are made, and credited when sales are made.\n\nModern barter and trade has evolved considerably to become an effective method of increasing sales, conserving cash, moving inventory, and making use of excess production capacity for businesses around the world. Businesses in a barter earn trade credits (instead of cash) that are deposited into their account. They then have the ability to purchase goods and services from other members utilizing their trade credits – they are not obligated to purchase from those whom they sold to, and vice versa. The exchange plays an important role because they provide the record-keeping, brokering expertise and monthly statements to each member. Commercial exchanges make money by charging a commission on each transaction either all on the buy side, all on the sell side, or a combination of both. Transaction fees typically run between 8 and 15%.\n\nThroughout the 18th century, retailers began to adandon the prevailing system of bartering. Retailers operating out of the Palais complex in Paris, France were among the first in Europe to abandon the bartering, and adopt fixed-prices thereby sparing their clientele the hassle of bartering. The Palais retailers stocked luxury goods that appealed to the wealthy elite and upper middle classes. Stores were fitted with long glass exterior windows which allowed the emerging middle-classes to window shop and indulge in fantasies, even when they may not have been able to afford the high retail prices. Thus, the Palais-Royal became one of the first examples of a new style of shopping arcade, which adopted the trappings of a sophisticated, modern shopping complex and also changed pricing structures. y both the aristocracy and the middle classes. \n\nThe Owenite socialists in Britain and the United States in the 1830s were the first to attempt to organize barter exchanges. Owenism developed a \"theory of equitable exchange\" as a critique of the exploitative wage relationship between capitalist and labourer, by which all profit accrued to the capitalist. To counteract the uneven playing field between employers and employed, they proposed \"schemes of labour notes based on labour time, thus institutionalizing Owen's demand that human labour, not money, be made the standard of value.\" This alternate currency eliminated price variability between markets, as well as the role of merchants who bought low and sold high. The system arose in a period where paper currency was an innovation. Paper currency was an I.O.U. circulated by a bank (a promise to pay, not a payment in itself). Both merchants and an unstable paper currency created difficulties for direct producers.\n\nAn alternate currency, denominated in labour time, would prevent profit taking by middlemen; all goods exchanged would be priced only in terms of the amount of labour that went into them as expressed in the maxim 'Cost the limit of price'. It became the basis of exchanges in London, and in America, where the idea was implemented at the New Harmony communal settlement by Josiah Warren in 1826, and in his Cincinnati 'Time store' in 1827. Warren ideas were adopted by other Owenites and currency reformers, even though the labour exchanges were relatively short lived.\n\nIn England, about 30 to 40 cooperative societies sent their surplus goods to an \"exchange bazaar\" for direct barter in London, which later adopted a similar labour note. The British Association for Promoting Cooperative Knowledge established an \"equitable labour exchange\" in 1830. This was expanded as the National Equitable Labour Exchange in 1832 on Grays Inn Road in London. These efforts became the basis of the British cooperative movement of the 1840s. In 1848, the socialist and first self-designated anarchist Pierre-Joseph Proudhon postulated a system of \"time chits\". In 1875, Karl Marx wrote of \"Labor Certificates\" (\"Arbeitszertifikaten\") in his Critique of the Gotha Program of a \"certificate from society that [the labourer] has furnished such and such an amount of labour\", which can be used to draw \"from the social stock of means of consumption as much as costs the same amount of labour.\"\n\nThe first exchange system was the Swiss WIR Bank. It was founded in 1934 as a result of currency shortages after the stock market crash of 1929. \"WIR\" is both an abbreviation of Wirtschaftsring and the word for \"we\" in German, reminding participants that the economic circle is also a community.\n\nIn Spain (particularly the Catalonia region) there is a growing number of exchange markets. These barter markets or swap meets work without money. Participants bring things they do not need and exchange them for the unwanted goods of another participant. Swapping among three parties often helps satisfy tastes when trying to get around the rule that money is not allowed.\n\nMichael Linton originated the term \"local exchange trading system\" (LETS) in 1983 and for a time ran the Comox Valley LETSystems in Courtenay, British Columbia. LETS networks use interest-free local credit so direct swaps do not need to be made. For instance, a member may earn credit by doing childcare for one person and spend it later on carpentry with another person in the same network. In LETS, unlike other local currencies, no scrip is issued, but rather transactions are recorded in a central location open to all members. As credit is issued by the network members, for the benefit of the members themselves, LETS are considered mutual credit systems.\n\nAccording to the International Reciprocal Trade Association, the industry trade body, more than 450,000 businesses transacted $10 billion globally in 2008 – and officials expect trade volume to grow by 15% in 2009.\n\nIt is estimated that over 450,000 businesses in the United States were involved in barter exchange activities in 2010. There are approximately 400 commercial and corporate barter companies serving all parts of the world. There are many opportunities for entrepreneurs to start a barter exchange. Several major cities in the U.S. and Canada do not currently have a local barter exchange. There are two industry groups in the United States, the National Association of Trade Exchanges (NATE) and the International Reciprocal Trade Association (IRTA). Both offer training and promote high ethical standards among their members. Moreover, each has created its own currency through which its member barter companies can trade. NATE's currency is the known as the BANC and IRTA's currency is called Universal Currency (UC).\n\nIn Canada, barter continues to thrive. The largest b2b barter exchange is Tradebank, founded in 1987. P2P bartering has seen a renaissance in major Canadian cities through Bunz - built as a network of Facebook groups that went on to become a stand-alone bartering based app in January 2016. Within the first year, Bunz accumulated over 75,000 users in over 200 cities worldwide.\n\nIn the United States, the largest barter exchange and corporate trade group is International Monetary Systems, founded in 1985, now with representation in various countries.\n\nIn Australia and New Zealand the largest barter exchange is Bartercard, founded in 1991, with offices in the United Kingdom, United States, Cyprus, UAE and Thailand.\n\nCorporate barter focuses on larger transactions, which is different from a traditional, retail oriented barter exchange. Corporate barter exchanges typically use media and advertising as leverage for their larger transactions. It entails the use of a currency unit called a \"trade-credit\". The trade-credit must not only be known and guaranteed, but also be valued in an amount the media and advertising could have been purchased for had the \"client\" bought it themselves (contract to eliminate ambiguity and risk).\n\nSoviet bilateral trade is occasionally called \"barter trade\", because although the purchases were denominated in U.S. dollars, the transactions were credited to an international clearing account, avoiding the use of hard cash.\n\nIn the United States, Karl Hess used bartering to make it harder for the IRS to seize his wages and as a form of tax resistance. Hess explained how he turned to barter in an op-ed for \"The New York Times\" in 1975. However the IRS now requires barter exchanges to be reported as per the Tax Equity and Fiscal Responsibility Act of 1982. Barter exchanges are considered taxable revenue by the IRS and must be reported on a 1099-B form. According to the IRS, \"The fair market value of goods and services exchanged must be included in the income of both parties.\"\n\nOther countries, though, do not have the reporting requirement that the U.S. does concerning proceeds from barter transactions, but taxation is handled the same way as a cash transaction. If one barters for a profit, one pays the appropriate tax; if one generates a loss in the transaction, they have a loss. Bartering for business is also taxed accordingly as business income or business expense. Many barter exchanges require that one register as a business. Barter of America.com 's policies of trading are found here http://barterofamerica.com/go/go/laws.asp\n\n"}
{"id": "4233", "url": "https://en.wikipedia.org/wiki?curid=4233", "title": "Berthe Morisot", "text": "Berthe Morisot\n\nBerthe Marie Pauline Morisot (; January 14, 1841 – March 2, 1895) was a painter and a member of the circle of painters in Paris who became known as the Impressionists. She was described by Gustave Geffroy in 1894 as one of \"les trois grandes dames\" of Impressionism alongside Marie Bracquemond and Mary Cassatt.\n\nIn 1864, she exhibited for the first time in the highly esteemed Salon de Paris. Sponsored by the government, and judged by Academicians, the Salon was the official, annual exhibition of the Académie des beaux-arts in Paris. Her work was selected for exhibition in six subsequent Salons until, in 1874, she joined the \"\"rejected\"\" Impressionists in the first of their own exhibitions, which included Paul Cézanne, Edgar Degas, Claude Monet, Camille Pissarro, Pierre-Auguste Renoir, and Alfred Sisley. It was held at the studio of the photographer Nadar.\n\nShe was married to Eugène Manet, the brother of her friend and colleague Édouard Manet.\n\nMorisot was born in Bourges, France, into an affluent bourgeois family. Her father, Edmé Tiburce Morisot, was the prefect (senior administrator) of the department of Cher. He also studied architecture at École des Beaux Arts. Her mother, Marie-Joséphine-Cornélie Thomas, was the great-niece of Jean-Honoré Fragonard, one of the most prolific Rococo painters of the ancien régime. She had two older sisters, Yves (1838–1893) and Edma (1839–1921), plus a younger brother, Tiburce, born in 1848. The family moved to Paris in 1852, when Morisot was a child.\n\nIt was common practice for daughters of bourgeois families to receive art education, so Berthe and her sisters Yves and Edma were taught privately by Geoffroy-Alphonse Chocarne and . Morisot and her sisters initially started taking lessons so that they could each make a drawing for their father for his birthday. In 1857 Guichard, who ran a school for girls in Rue des Moulins, introduced Berthe and Edma to the Louvre gallery where they could learn by looking, and from 1858 they learned by copying paintings. He also introduced them to the works of Gavarni. Guichard later became the director of École des Beaux Arts where Morisot's father earned his degree.\n\nAs art students, Berthe and Edma worked closely together until Edma married Adolphe Pontillon, a naval officer, moved to Cherbourg, had children, and had less time to paint. Letters between the sisters show a loving relationship, underscored by Berthe's regret at the distance between them and Edma's withdrawal from painting. Edma wholeheartedly supported Berthe's continued work and their families always remained close. Edma wrote \"“… I am often with you in thought, dear Berthe. I’m in your studio and I like to slip away, if only for a quarter of an hour, to breathe that atmosphere that we shared for many years…”\".\n\nHer sister Yves married Theodore Gobillard, a tax inspector, in 1866, and was painted by Edgar Degas as \"Mrs Theodore Gobillard\" (Metropolitan Museum of Art, New York).\n\nMorisot registered as a copyist at the Louvre where she befriended other artists and teachers including Camille Corot, the pivotal landscape painter of the Barbizon School who also excelled in figure painting. In 1860, under Corot's influence she took up the plein air (outdoors) method of working. By 1863 she was studying under , another Barbizon painter. In the winter of 1863–64 she studied sculpture under Aimé Millet, but none of her sculpture is known to survive.\n\nMorisot's first appearance in the Salon de Paris came at the age of twenty-three in 1864, with the acceptance of two landscape paintings. She continued to show regularly in the Salon, to generally favorable reviews, until 1873, the year before the first Impressionist exhibition. She exhibited with the Impressionists from 1874 onwards, only missing the exhibition in 1878 when her daughter was born.\n\nMorisot's mature career began in 1872. She found an audience for her work with Durand-Ruel, the private dealer, who bought twenty-two paintings. In 1877, she was described by the critic for \"Le Temps\" as the \"one real Impressionist in this group.\" She chose to exhibit under her full maiden name instead of using a pseudonym or her married name. In the 1880 exhibition, many reviews judged Morisot among the best, including \"Le Figaro\" critic Albert Wolff.\n\n\nIn 1868 Morisot became friends with Édouard Manet who painted several portraits of her, including a striking study in a black veil while in mourning for her father. Correspondence between them shows warm affection, and Manet gave her an easel as a Christmas present. To her dismay he interfered with one of her Salon submissions whilst he was engaged to transport it, mistaking her self-criticism as an invitation to add corrections. Manet wrote: \"The young Morisot girls are charming. It's annoying that they are not men. However, as women, they could serve the cause of painting by each marrying a member of the French Academy and sowing discord in the camp of those dotards.\" \n\nAlthough Manet is regarded as the master and Morisot as the follower, there is evidence that their relationship was reciprocal. Records show Manet's appreciation of her distinctive original style and compositional decisions, some of which he incorporated into his own work. It was Morisot who persuaded Manet to attempt plein air painting, which she had been practising since having been introduced to it by Corot.\n\nMorisot drew Manet into the circle of painters who became known as the Impressionists. In 1874, she married Manet's brother, Eugène, and they had one daughter, Julie, who became the subject for many of her mother's paintings. Julie's memoirs, \"Growing Up with the Impressionists: The Diary of Julie Manet\", were published in 1987.\n\nMorisot’s works are almost always small in scale. She worked in oil paint, watercolors, or pastel, and sketched using various drawing media. Around 1880 she began painting on unprimed canvases—a technique Manet and Eva Gonzalès also experimented with at the time—and her brushwork became looser. In 1888–89, her brushstrokes transitioned from short, rapid strokes to long, sinuous ones that define form. The outer edges of her paintings were often left unfinished, allowing the canvas to show through and increasing the sense of spontaneity. After 1885, she worked mostly from preliminary drawings before beginning her oil paintings.\n\nAmong her contemporary art critics such as Gustave Geoffrey in 1881, Morisot was hailed as \"no one represents Impressionism with more refined talent or more authority than Morisot\" \n\nMorisot creates a sense of space and depth through the use of color. Although her color palette was somewhat limited, her fellow impressionists regarded her as a \"virtuoso colorist\". She typically made expansive use of white, whether used as a pure white or mixed with other colors. In her large painting, \"The Cherry Tree\", colors are more vivid but are still used to emphasize form.\n\nMorisot painted what she experienced on a daily basis. Her paintings reflect the 19th-century cultural restrictions of her class and gender. She avoided urban and street scenes and seldom painted the nude figure. Like her fellow Impressionist Mary Cassatt, she focused on domestic life and portraits in which she could use family and personal friends as models, including her daughter Julie and sister Edma. Prior to the 1860s, Morisot painted subjects in line with the Barbizon school before turning to scenes of contemporary femininity. Paintings like \"The Cradle\" (1872), in which she depicted current trends for nursery furniture, reflect her sensitivity to fashion and advertising, both of which would have been apparent to her female audience. Her works also include landscapes, portraits, garden settings and boating scenes. Later in her career Morisot worked with more ambitious themes, such as nudes. Corresponding with Morisot's interest in nude subjects, Morisot also began to focus more on preliminary drawings, completing many drypoints, charcoal, and color pencil drawings.\n\nMorisot was married to Eugène Manet, the brother of her friend and colleague Édouard Manet, from 1874 until his death in 1892. In 1878 she gave birth to her only child, Julie, who posed frequently for her mother and other Impressionist artists, including Renoir and her uncle Édouard.\n\nMorisot died on March 2, 1895, in Paris, of pneumonia contracted while attending to her daughter Julie's similar illness, and thus orphaning her at the age of 16. She was interred in the Cimetière de Passy.\n\nShe was portrayed by actress Marine Delterme in a 2012 French biographical TV film directed by Caroline Champetier. The character of Beatrice de Clerval in Elizabeth Kostova's \"The Swan Thieves\" is largerly based on Morisot.\n\nMorisot's work sold comparatively well. She achieved the two highest prices at a Hôtel Drouot auction in 1875, the \"Interior (Young Woman with Mirror)\" sold for 480 francs, and her pastel \"On the Lawn\" sold for 320 francs. Her works averaged 250 francs, the best relative prices at the auction.\n\nIn February 2013, Morisot became the highest priced female artist, when \"After Lunch\" (1881), a portrait of a young redhead in a straw hat and purple dress, sold for $10.9 million at a Christie's auction. The painting achieved roughly three times its upper estimate, exceeding the $10.7 million for a sculpture by Louise Bourgeois in 2012.\n\nThis limited selection is based in part on the book \"Berthe Morisot\" by Charles F. Stuckey, William P. Scott and Susan G. Lindsay, which is in turn drawn from the 1961 catalogue by Marie-Louise Bataille, Rouaart Denis and Georges Wildenstein. There are variations between the dates of execution, first showing and purchase. Titles may vary between sources.\n\n\n\n\n\n\n"}
{"id": "4237", "url": "https://en.wikipedia.org/wiki?curid=4237", "title": "Barnard College", "text": "Barnard College\n\nBarnard College is a private women's liberal arts college in New York City. Founded in 1889 as a response to Columbia University's refusal to admit women into their institution, it is the only women's college in New York City today. It has been affiliated with Columbia University since 1900.\n\nThe school was founded by Annie Nathan Meyer and named after Frederick Barnard, tenth president of Columbia. When Columbia became coeducational in 1983 Barnard elected to remain legally and financially separate but maintained its affiliated relationship with the university. This includes shared academic programs and other academic privileges, and a combined athletic program. Barnard students also receive Columbia University degrees and graduate at Columbia University graduation. Barnard confers the Bachelor of Arts degree in about 50 areas of study. Students may also pursue elements of their education at the Juilliard School and The Jewish Theological Seminary.\n\nBarnard's campus is located in the Manhattan neighborhood of Morningside Heights, stretching along Broadway between 116th and 120th Streets. It is directly across from Columbia's campus and near several other academic institutions. The college is a member of the Seven Sisters, an association of seven prominent women's liberal arts colleges.\n\nFor its first 229 years Columbia College of Columbia University admitted only men for undergraduate study. Barnard College was founded in 1889 as a response to Columbia's refusal to admit women into its institution.\n\nThe college was named after Frederick Augustus Porter Barnard, a deaf American educator and mathematician who served as the tenth president of Columbia from 1864 to 1889. He advocated equal educational privileges for men and women, preferably in a coeducational setting, and began proposing in 1879 that Columbia admit women. The board of trustees repeatedly rejected Barnard's suggestion, but in 1883 agreed to create a detailed syllabus of study for women. While they could not attend Columbia classes, those who passed examinations based on the syllabus would receive a degree. The first such woman graduate received her bachelor's degree in 1887. A former student of the program, Annie Meyer, and other prominent New York women persuaded the board in 1889 to create a women's college connected to Columbia.\n\nBarnard College's original 1889 home was a rented brownstone at 343 Madison Avenue, where a faculty of six offered instruction to 14 students in the School of Arts, as well as to 22 \"specials\", who lacked the entrance requirements in Greek and so enrolled in science. When Columbia University announced in 1892 its impending move to Morningside Heights, Barnard built a new campus on 119th-120th Streets with gifts from Mary E. Brinckerhoff, Elizabeth Milbank Anderson and Martha Fiske. Milbank, Brinckerhoff, and Fiske Halls, built in 1897–1898, were listed on the National Register of Historic Places in 2003.\n\nElla Weed supervised the college in its first four years; Emily James Smith succeeded her as Barnard's first dean. As the college grew it needed additional space, and in 1903 it received the three blocks south of 119th Street from Anderson who had purchased a former portion of the Bloomingdale Asylum site from the New York Hospital. By the mid-20th century Barnard had succeeded in its original goal of providing a top tier education to women. Between 1920 and 1974, only the much larger Hunter College and University of California, Berkeley produced more women graduates who later received doctorate degrees. Students' Hall, now known as Barnard Hall, was built in 1916. Brooks and Hewitt Halls were built in 1906–1907 and 1926–1927, respectively. They were listed on the National Register of Historic Places in 2003. Jessica Finch is credited with coining the phrase, \"current events,\" while teaching at Barnard College in the 1890s.\n\nBarnard confers the Bachelor of Arts degree in about 50 areas of study. Joint programs for the Bachelor of Science and other degrees exist with Columbia University, Juilliard School, and The Jewish Theological Seminary. The six most popular majors at the college are English, psychology, political science, economics, history, and biology.\n\nThe liberal arts requirements are called the Nine Ways of Knowing. Students must take one year of one laboratory science, study a single foreign language for four semesters, and complete one 3-credit course in each of the following categories: reason and value, social analysis, historical studies, cultures in comparison, quantitative and deductive reasoning, literature, and visual and performing arts. The use of AP or IB credit to fulfill these requirements is very limited, but Nine Ways of Knowing courses may overlap with major or minor requirements. In addition to the Nine Ways of Knowing, students must complete a first-year seminar, a first-year English course, and one semester of physical education.\nThe Nine Ways of Knowing was replaced with Foundations in 2016. Students must take the First Year Experience which includes two semesters of seminars, complete Distributional Requirements within many subjects, and six Modes of Thinking courses.\n\"Foundations is uniquely Barnard. It's a diverse and\nforward-looking curriculum that asks our students to think\ntheoretically, empirically, and technologically, to write\neffectively; and to speak persuasively — all while giving them\nthe freedom to shape their own educational experience.\"\n— Linda A. Bell, Provost\n\nAdmissions to Barnard is considered very selective by \"U.S. News & World Report\". It is the most selective women's college in the nation; in 2016, Barnard had the lowest acceptance rate of the five Seven Sisters that remain single-sex in admissions.\n\nThe class of 2021's admission rate was 14.8% of the 7,716 applicants, the lowest acceptance rate in the institution's history. The early-decision admission rate for the class of 2020 was 47.7%, out of 787 applications. The median SAT Combined was 2080, with median subscores of 700 in Math, in 705 Critical Reading, and 720 in Writing. The Median ACT score was 32. Of the women in the class of 2012, 89.4% ranked in first or second decile at their high school (of the 41.3% ranked by their schools). The average GPA of the class of 2012 was 94.3 on a 100-point scale and 3.88 on a 4.0 scale. In 2015 Barnard announced that it would admit transgender women who \"consistently live and identify as women, regardless of the gender assigned to them at birth\", and would continue to support and enroll those students who transitioned to males after they had already been admitted.\n\nIn the 2014 U.S. News & World Report rankings, Barnard was ranked as the 32nd best liberal arts college in the country. The ranking came under widespread criticism, as it only accounted for institution-specific resources. Greg Brown, chief operating officer at Barnard, said, \"I believe that our ranking is lower than it should be, primarily because the methodology simply can't account for the Barnard-Columbia relationship. Because the Columbia relationship doesn't fit neatly into any of the survey categories, it is essentially ignored. Rankings are inherently limited in this way.\"\n\nIn 1998, then president Judith Shapiro compared the ranking service to the \"equivalent of \"Sport's Illustrated\" swimsuit issue.\" According to Shapiro's letter, \"Such a ranking system certainly does more harm than good in terms of educating the public.\" On June 19, 2007, following a meeting of the Annapolis Group, which represents over 100 liberal arts colleges, Barnard announced that it would no longer participate in the U.S. News annual survey, and that they would fashion their own way to collect and report common data.\n\n While students are allowed to use the libraries at Columbia University, Barnard has always maintained a library of its own. Lehman Hall was the site of Barnard's Wollman Library from its opening in 1959 until 2015. , the Lehman Hall building is being demolished to make way for a new library facility. Barnard's Teaching and Learning Center, the planned replacement for the earlier building, is scheduled to open in August 2018.\n\nIn 2016, portions of the Barnard Library were relocated to the former LeFrak Gymnasium as well as the first two floors of Barnard Hall. 18,000 volumes were also moved to the Milstein rooms in Columbia University's Butler Library. The relocation plans proved to be contentious among faculty at the college, who objected to sending a large portion of the library's holdings off site, as well as a \"lack of transparency surrounding the decision-making process\", according to \"Library Journal\".\n\nThe LeFrak Center houses study space, librarians' offices, the zine collection, course reserves, and new books acquired since July 2015. The Barnard Library also houses the Archives and Special Collections, a repository of official and student publications, photographs, letters, alumnae scrapbooks and other material that documents Barnard's history from its founding in 1889 to the present day. Among the collections are the Ntozake Shange papers and various student publications.\n\nBorne of a proposal by longtime zinester Jenna Freedman, Barnard collects zines in an effort to document third-wave feminism and Riot Grrrl culture. According to Freedman, zine collections such as Barnard's provide a home for the voices of young women otherwise not represented in library collections. The Zine Collection's website states: \n\n, the library had approximately 4,000 different zines available to library patrons, including zines about race, gender, sexuality, childbirth, motherhood, politics, and relationships. The library keeps a collection of zines for lending and another archived collection in the Barnard Archives. Both collections are catalogued in \"CLIO\", the Columbia/Barnard Online public access catalog.\n\nEvery Barnard student is part of the Student Government Association (SGA), which elects a representative student government. SGA aims to facilitate the expression of opinions on matters that directly affect the Barnard community.\n\nStudent groups include theatre and vocal music groups, language clubs, literary magazines, a freeform radio station called WBAR, a biweekly magazine called the \"Barnard Bulletin\", community service groups, and others.\n\nBarnard students can also join extracurricular activities or organizations at Columbia University, while Columbia University students are allowed in most, but not all, Barnard organizations. Barnard's McIntosh Activities Council (commonly known as McAC), named after the first President of Barnard, Millicent McIntosh, organizes various community focused events on campus, such as Big Sub and Midnight Breakfast. McAC is made up of five sub-committees which are the Mosaic committee (formerly known as Multicultural), the Wellness committee, the Network committee, the Community committee, and the Action committee. Each committee has a different focus, such as hosting and publicizing identity and cultural events (Mosaic), having health and wellness related events (Wellness), giving students opportunities to be involved with Alumnae and various professionals (Network), planning events that bring the entire student body together (Community), and planning community service events that give back to the surrounding community (Action).\n\nBarnard College officially banned sororities in 1913, but Barnard students continue to participate in Columbia's six National Panhellenic Conference sororities—Alpha Chi Omega, Alpha Omicron Pi, Delta Gamma, Gamma Phi Beta, Kappa Alpha Theta, and Sigma Delta Tau—and the National Pan-Hellenic Council Sororities- Alpha Kappa Alpha (Lambda chapter) and Delta Sigma Theta (Rho chapter) as well as other sororities in the Multicultural Greek Council. Two National Panhellenic Conference organizations were founded at Barnard College. The Alpha Omicron Pi Fraternity, founded on January 2, 1897, left campus during the 1913 ban but returned to establish its Alpha chapter in 2013. The Alpha Epsilon Phi, founded on October 24, 1909, is no longer on campus. As of 2010, Barnard does not fully recognize the National Panhellenic Conference sororities at Columbia, but it does provide some funding to account for Barnard students living in Columbia housing through these organizations.\n\nTake Back the Night: Each April, Barnard and Columbia students participate in the Take Back the Night march and speak-out. This annual event grew out of a 1988 Seven Sisters conference. The march has grown from under 200 participants in 1988 to more than 2,500 in 2007.\n\nMidnight Breakfast marks the beginning of finals week. As a highly popular event and long-standing college tradition, Midnight Breakfast is hosted by the student-run activities council, McAC (McIntosh Activities Council). In addition to providing standard breakfast foods, each year's theme is also incorporated into the menu. Past themes have included \"I YUMM the 90s,\" \"Grease,\" and \"Take Me Out to the Ballgame.\" The event is a school-wide affair as college deans, trustees and the President, Debora Spar, serve food to about a thousand students. It takes place the- night before finals begin every semester.\n\nNight Carnival: In the spring of each year, Barnard holds the Night Carnival, in which many of Barnard's student groups set up tables with games and prizes. The event is organized by the student-run activities council, McAC (McIntosh Activities Council).\n\nThe \"Barnard Bulletin\" in 1976 described the relationship between the college and Columbia University as \"intricate and ambiguous\". Barnard president Debora Spar said in 2012 that \"the relationship is admittedly a complicated one, a unique one and one that may take a few sentences to explain to the outside community\".\n\nOutside sources often describe Barnard as part of Columbia; \"The New York Times\" in 2013, for example, called Barnard \"an undergraduate women's college of Columbia University\". The college's front gates state \"Barnard College of Columbia University\". Barnard describes itself as \"both an independently incorporated educational institution and an official college of Columbia University\", and advises students to state \"Barnard College, Columbia University\" or \"Barnard College of Columbia University\" on résumés. Facebook includes Barnard students and alumnae within the Columbia interest group.\n\nColumbia describes Barnard as an affiliated institution that is a faculty of the university or is \"in partnership with\" it. Both the college and Columbia evaluate Barnard faculty for tenure, and Barnard graduates receive Columbia University diplomas signed by both the Barnard and Columbia presidents.\n\nSmith and Columbia president Seth Low worked to open Columbia classes to Barnard students. By 1900 they could attend Columbia classes in philosophy, political science, and several scientific fields. That year Barnard formalized an affiliation with the university which made available to its students the instruction and facilities of Columbia. Franz Boas, who taught at both Columbia and Barnard in the early 1900s, was among those faculty members who reportedly found Barnard students superior to their male Columbia counterparts. From 1955 Columbia and Barnard students could register for the other school's classes with the permission of the instructor; from 1973 no permission was needed.\n\nExcept for Columbia College, by the 1940s other undergraduate and graduate divisions of Columbia University admitted women. Columbia president William J. McGill predicted in 1970 that Barnard College and Columbia College would merge within five years. In 1973 Columbia and Barnard signed a three-year agreement to increase sharing classrooms, facilities, and housing, and cooperation in faculty appointments, which they described as \"integration without assimilation\"; by the mid-1970s most Columbia dormitories were coed. The university's financial difficulties during the decade increased its desire to merge to end what Columbia described as the \"anachronism\" of single-sex education, but Barnard resisted doing so because of Columbia's large debt, rejecting in 1975 Columbia dean Peter Pouncey's proposal to merge Barnard and the three Columbia undergraduate schools. The 1973-1976 chairwoman of the board at Barnard, Eleanor Thomas Elliott, led the resistance to this takeover. The college's marketing emphasized the Columbia relationship, however, the \"Bulletin\" in 1976 stating that Barnard described it as identical to the one between Harvard College and Radcliffe College (\"who are merged in practically everything but name at this point\").\n\nAfter Barnard rejected subsequent merger proposals from Columbia and a one-year extension to the 1973 agreement expired, in 1977 the two schools began discussing their future relationship. By 1979 the relationship had so deteriorated that Barnard officials stopped attending meetings. Because of an expected decline in enrollment, In 1980 a Columbia committee recommended that Columbia College begin admitting women without Barnard's cooperation. A 1981 committee found that Columbia was no longer competitive with other Ivy League universities without women, and that admitting women would not affect Barnard's applicant pool. That year Columbia president Michael Sovern agreed for the two schools to cooperate in admitting women to Columbia, but Barnard faculty's opposition caused president Ellen Futter to reject the agreement.\n\nA decade of negotiations for a Columbia-Barnard merger akin to Harvard and Radcliffe had failed. In January 1982, the two schools instead announced that Columbia College would begin admitting women in 1983, and Barnard's control over tenure for its faculty would increase; previously, a committee on which Columbia faculty outnumbered Barnard's three to two controlled the latter's tenure. Applications to Columbia rose 56% that year, making admission more selective, and nine Barnard students transferred to Columbia. Eight students admitted to both Columbia and Barnard chose Barnard, while 78 chose Columbia. Within a few years, however, selectivity rose at both schools as they received more women applicants than expected.\n\nThe Columbia-Barnard affiliation continued. Barnard pays Columbia about $5 million a year under the terms of the \"interoperate relationship\", which the two schools renegotiate every 15 years. Despite the affiliation Barnard is legally and financially separate from Columbia, with an independent faculty and board of trustees. It is responsible for its own separate admissions, health, security, guidance and placement services, and has its own alumnae association. Nonetheless, Barnard students participate in the academic, social, athletic and extracurricular life of the broader University community on a reciprocal basis. The affiliation permits the two schools to share some academic resources; for example, only Barnard has an urban studies department, and only Columbia has a computer science department. Most Columbia classes are open to Barnard students and vice versa. Barnard students and faculty are represented in the University Senate, and student organizations such as the \"Columbia Daily Spectator\" are open to all students. Barnard students play on Columbia athletics teams, and Barnard uses Columbia email, telephone and network services.\n\nBarnard athletes compete in the Ivy League (NCAA Division I) through the Columbia/Barnard Athletic Consortium, which was established in 1983. Through this arrangement, Barnard is the only women's college offering Division I athletics. There are 15 intercollegiate teams, and students also compete at the intramural and club levels. From 1975–1983, before the establishment of the Columbia/Barnard Athletic Consortium, Barnard students competed as the \"Barnard Bears\". Prior to 1975, students referred to themselves as the \"Barnard honeybears\".\n\nEstablished within the Barnard Student Government Association (SGA), The Seven Sisters Governing Board represents Barnard College as part of the Seven Sisters Coalition, which is a group of representatives from student councils of the historic Seven Sisters colleges. The reps on the coordinating board of Seven Sisters Coalition are rotating every year to hold the annual Seven Sisters Conference in a serious but informal setting. The first Seven Sisters Conference was hosted by SGA student representatives at Barnard College in 2009. In fall 2013, the conference was hosted by Vassar College during the first weekend of November. The major topic focused on inner college collaborations and differences in student government structures among Seven Sisters Colleges. The Seven Sisters Coordinating Board of Barnard brought six Barnard student representatives to attend the Fall Semester conference, which was hosted at Vassar College in the past fall semester. Based on the Coalition Coordinating Board Constitution established in February 2013, Students delegates were initiating projects in the aspects of public relations, alumni outreach and website management to promote the presence and development of the seven sisters culture. Meanwhile, The Barnard delegates engaged in discussions about the various structures of the student governments among the historic seven sisters colleges.\n\nBarnard College has issued a statement affirming its commitment to environmental sustainability, a major part of which is the goal of reducing its greenhouse gas emissions by 30% by 2017. Student EcoReps work as a resource on environmental issues for students in Barnard's residence halls, while the student-run Earth Coalition works on outreach initiatives such as local park clean-ups, tutoring elementary school students in environmental education, and sponsoring environmental forums. Barnard earned a \"C-\" for its sustainability efforts on the College Sustainability Report Card 2009 published by the Sustainable Endowments Institute. Its highest marks were in Student Involvement and Food and Recycling, receiving a \"B\" in both categories.\n\nIn the spring of 1960, Columbia University president Grayson Kirk complained to the president of Barnard that Barnard students were wearing inappropriate clothing. The garments in question were pants and Bermuda shorts. The administration forced the student council to institute a dress code. Students would be allowed to wear shorts and pants only at Barnard and only if the shorts were no more than two inches above the knee and the pants were not tight. Barnard women crossing the street to enter the Columbia campus wearing shorts or pants were required to cover themselves with a long coat.\n\nIn March 1968, \"The New York Times\" ran an article on students who cohabited, identifying one of the persons they interviewed as a student at Barnard College from New Hampshire named \"Susan\". Barnard officials searched their records for women from New Hampshire and were able to determine that \"Susan\" was the pseudonym of a student (Linda LeClair) who was living with her boyfriend, a student at Columbia University. She was called before Barnard's student-faculty administration judicial committee, where she faced the possibility of expulsion. A student protest included a petition signed by 300 other Barnard women, admitting that they too had broken the regulations against cohabitating. The judicial committee reached a compromise and the student was allowed to remain in school, but was denied use of the college cafeteria and barred from all social activities. The student briefly became a focus of intense national attention. She eventually dropped out of Barnard.\n\nBarnard College has graduated many prominent leaders in science, religion, politics, the Peace Corps, medicine, law, education, communications, and business; and acclaimed actors, architects, artists, astronauts, engineers, human rights activists, inventors, musicians, philanthropists, and writers. Among these include: United Nations Development Programme Representative of Japan, Akiko Yuge (1975), author Zora Neale Hurston, author and political activist Grace Lee Boggs (1935), television host Ronnie Eldridge (1952), U.S. Representative Helen Gahagan (1924), CEO of CARE USA and chair of the Presidential Advisory Council on HIV/AIDS Helene D. Gayle (1970), President of the American Civil Liberties Union Susan Herman (1968), Chief Judge of the New York Court of Appeals Judith Kaye (1958), Chair of the National Labor Relations Board Wilma B. Liebman (1971), and author of \"The Sisterhood of the Traveling Pants\" Ann Brashares (1989).\n\n\nNotesSources\n\n"}
{"id": "4240", "url": "https://en.wikipedia.org/wiki?curid=4240", "title": "Order of Saint Benedict", "text": "Order of Saint Benedict\n\nThe Order of Saint Benedict (OSB; Latin: \"Ordo Sancti Benedicti\"), also knownin reference to the colour of its members' habitsas the Black Monks, is a Catholic religious order of independent monastic communities that observe the Rule of Saint Benedict. Each community (monastery, priory or abbey) within the order maintains its own autonomy, while the order itself represents their mutual interests. The terms \"Order of Saint Benedict\" and \"Benedictine Order\" are, however, also used to refer to \"all\" Benedictine communities collectively, sometimes giving the incorrect impression that there exists a generalate or motherhouse with jurisdiction over them.\n\nInternationally, the order is governed by the Benedictine Confederation, a body, established in 1883 by Pope Leo XIII's Brief \"Summum semper\", whose head is known as the Abbot Primate. Individuals whose communities are members of the order generally add the initials \"OSB\" after their names.\n\nThe monastery at Subiaco in Italy, established by Saint Benedict of Nursia circa 529, was the first of the dozen monasteries he founded. He later founded the Abbey of Monte Cassino. There is no evidence, however, that he intended to found an order and the Rule of Saint Benedict presupposes the autonomy of each community. When Monte Cassino was sacked by the Lombards about the year 580, the monks fled to Rome, and it seems probable that this constituted an important factor in the diffusion of a knowledge of Benedictine monasticism.\n\nIt was from the monastery of St. Andrew in Rome that Augustine, the prior, and his forty companions set forth in 595 on their mission for the evangelization of England. At various stopping places during the journey, the monks left behind them traditions concerning their rule and form of life, and probably also some copies of the Rule. Lérins Abbey, for instance, founded by Honoratus in 375, probably received its first knowledge of the Benedictine Rule from the visit of St. Augustine and his companions in 596.\n\nGregory of Tours says that at Ainay, in the sixth century, the monks \"followed the rules of Basil, Cassian, Caesarius, and other fathers, taking and using whatever seemed proper to the conditions of time and place\", and doubtless the same liberty was taken with the Benedictine Rule when it reached them. In Gaul and Switzerland, it supplemented the much stricter Irish or Celtic Rule introduced by Columbanus and others. In many monasteries it eventually entirely displaced the earlier codes.\nBy the ninth century, however, the Benedictine had become the standard form of monastic life throughout the whole of Western Europe, excepting Scotland, Wales, and Ireland, where the Celtic observance still prevailed for another century or two. Largely through the work of Benedict of Aniane, it became the rule of choice for monasteries throughout the Carolingian empire.\n\nMonastic scriptoria flourished from the ninth through the twelfth centuries. Sacred Scripture was always at the heart of every monastic scriptorium. As a general rule those of the monks who possessed skill as writers made this their chief, if not their sole active work. An anonymous writer of the ninth or tenth century speaks of six hours a day as the usual task of a scribe, which would absorb almost all the time available for active work in the day of a medieval monk.\n\nIn the Middle Ages monasteries were often founded by the nobility. Cluny Abbey was founded by William I, Duke of Aquitaine in 910. The abbey was noted for its strict adherence to the Rule of St. Benedict. The abbot of Cluny was the superior of all the daughter houses, through appointed priors.\n\nOne of the earliest reforms of Benedictine practice was that initiated in 980 by Romuald, who founded the Camaldolese community.\n\nThe English Benedictine Congregation is the oldest of the nineteen Benedictine congregations. Augustine of Canterbury and his monks established the first English Benedictine monastery at Canterbury soon after their arrival in 597. Other foundations quickly followed. Through the influence of Wilfrid, Benedict Biscop, and Dunstan, the Benedictine Rule spread with extraordinary rapidity, and in the North it was adopted in most of the monasteries that had been founded by the Celtic missionaries from Iona. Many of the episcopal sees of England were founded and governed by the Benedictines, and no less than nine of the old cathedrals were served by the black monks of the priories attached to them. Monasteries served as hospitals and places of refuge for the weak and homeless. The monks studied the healing properties of plants and minerals to alleviate the sufferings of the sick.\n\nGermany was evangelized by English Benedictines. Willibrord and Boniface preached there in the seventh and eighth centuries and founded several abbeys.\nIn the English Reformation, all monasteries were dissolved and their lands confiscated by the Crown, forcing their Catholic members to flee into exile on the Continent. During the 19th century they were able to return to England, including to Selby Abbey in Yorkshire, one of the few great monastic churches to survive the Dissolution.\nSt. Mildred's Priory, on the Isle of Thanet, Kent, was built in 1027 on the site of an abbey founded in 670 by the daughter of the first Christian King of Kent. Currently the priory is home to a community of Benedictine nuns. Five of the most notable English abbeys are the Basilica of St Gregory the Great at Downside, commonly known as Downside Abbey, The Abbey of St Edmund, King and Martyr commonly known as Douai Abbey in Upper Woolhampton, Reading, Berkshire, Ealing Abbey in Ealing, West London, St. Lawrence's in Yorkshire (Ampleforth Abbey), and Worth Abbey. Prinknash Abbey, used by Henry VIII as a hunting lodge, was officially returned to the Benedictines four hundred years later, in 1928. During the next few years, so-called Prinknash Park was used as a home until it was returned to the order.\n\nSince the Oxford Movement, there has also been a modest flourishing of Benedictine monasticism in the Anglican Church and Protestant Churches. Anglican Benedictine Abbots are invited guests of the Benedictine Abbot Primate in Rome at Abbatial gatherings at Sant'Anselmo. There are an estimated 2,400 celibate Anglican Religious (1,080 men and 1,320 women) in the Anglican Communion as a whole, some of whom have adopted the Rule of St. Benedict.\n\nAs of 2015, the English Congregation consists of three abbeys of nuns and ten abbeys of monks. Members of the congregation are found in England, Wales, the United States of America, Peru and Zimbabwe.\n\nThe forty-eighth rule of Saint Benedict prescribes extensive and habitual \"holy reading\" for the brethren. Three primary types of reading were done by the monks during this time. Monks would read privately during their personal time, as well as publicly during services and at meal times. In addition to these three mentioned in the Rule, monks would also read in the infirmary.\n\nHowever, Benedictine monks were disallowed worldly possessions, thus necessitating the preservation and collection of sacred texts in monastic libraries for communal use. For the sake of convenience, the books in the monastery were housed in a few different places, namely the sacristy, which contained books for the choir and other liturgical books, the rectory, which housed books for public reading such as sermons and lives of the saints, and the library, which contained the largest collection of books and was typically in the cloister.\n\nThe first record of a monastic library in England is in Canterbury. To assist with Augustine of Canterbury's English mission, Pope Gregory the Great gave him nine books which included the Gregorian Bible in two volumes, the Psalter of Augustine, two copies of the Gospels, two martyrologies, an Exposition of the Gospels and Epistles, and a Psalter. Theodore of Tarsus brought Greek books to Canterbury more than seventy years later, when he founded a school for the study of Greek.\n\nMonasteries were among the institutions of the Catholic Church swept away during the French Revolution. Monasteries were again allowed to form in the 19th century under the Bourbon Restoration. Later that century, under the Third French Republic, laws were enacted preventing religious teaching. The original intent was to allow secular schools. Thus in 1880 and 1882, Benedictine teaching monks were effectively exiled; this was not completed until 1901.\n\nThe first Benedictine to live in the United States was Pierre-Joseph Didier. He came to the United States in 1790 from Paris and served in the Ohio and St. Louis areas until his death. The first actual Benedictine monastery founded was Saint Vincent Archabbey, located in Latrobe, Pennsylvania. It was founded in 1832 by Bonifice Wimmer, a German monk, who sought to serve German immigrants in America. In 1856, Wimmer started to lay the foundations for St. John's Abbey in Minnesota. By his death in 1887, Wimmer had sent Benedictine monks to Kansas, New Jersey, North Carolina, Georgia, Florida, Alabama, Illinois, and Colorado.\n\nWimmer also asked for Benedictine sisters to be sent to America by St. Walburg Convent in Eichstätt, Bavaria. In 1852, Sister Benedicta Riepp and two other sisters founded St. Marys, Pennsylvania. Soon they would send sisters to Michigan, New Jersey, and Minnesota.\n\nBy 1854, Swiss monks began to arrive and founded St. Meinrad Abbey in Indiana, and they soon spread to Arkansas and Louisiana. They were soon followed by Swiss sisters.\n\nThere are now over 100 Benedictine houses across America. Most Benedictine houses are part of one of four large Federations: American-Cassinese, Swiss-American, St. Scholastica, and St. Benedict. The federations mostly are made up of monasteries that share the same lineage. For instance the American-Cassinese Federation included the 22 monasteries that descended from Boniface Wimmer.\n\nToday, Benedictine monasticism is fundamentally different from other Western religious orders insofar as its individual communities are not part of a religious order with \"Generalates\" and \"Superiors General\". Rather, in modern times, the various autonomous houses have formed themselves loosely into congregations (for example, Cassinese, English, Solesmes, Subiaco, Camaldolese, Sylvestrines) that in turn are represented in the Benedictine Confederation that came into existence through Pope Leo XIII's Apostolic Brief \"Summum semper\" on 12 July 1883. This organization facilitates dialogue of Benedictine communities with each other and the relationship between Benedictine communities and other religious orders and the church at large. \n\nThe Rule of Saint Benedict is also used by a number of religious orders that began as reforms of the Benedictine tradition such as the Cistercians and Trappists although none of these groups are part of the Benedictine Confederation. \n\nAlthough Benedictines traditionally refer to Catholics, there are also some within the Anglican Communion and occasionally within other Christian denominations as well, for example, within the Lutheran Church, that claim adherence to the Rule of Saint Benedict. \n\nSection 17 in chapter 58 of the Rule of Saint Benedict states the solemn promise candidates for reception into a Benedictine community are required to make: a promise of stability (i.e. to remain in the same community), \"conversatio morum\" (an idiomatic Latin phrase suggesting \"conversion of manners\"; see below) and obedience (to the community's superior, seen as holding the place of Christ within it). This solemn commitment tends to be referred to as the \"Benedictine vow\" and is the Benedictine antecedent and equivalent of the evangelical counsels professed by candidates for reception into a religious order.\n\nMuch scholarship over the last fifty years has been dedicated to the translation and interpretation of \"\"conversatio morum\"\". The older translation \"conversion of life\" has generally been replaced with phrases such as \"[conversion to] a monastic manner of life\", drawing from the Vulgate's use of \"conversatio\" as a translation of \"citizenship\" or \"homeland\" in Philippians 3:20. Some scholars have claimed that the vow formula of the Rule is best translated as \"to live in this place as a monk, in obedience to its rule and abbot.\"\n\nBenedictine abbots and abbesses have full jurisdiction of their abbey and thus absolute authority over the monks or nuns who are resident. This authority includes the power to assign duties, to decide which books may or may not be read, to regulate comings and goings, and to punish and to excommunicate, in the sense of an enforced isolation from the monastic community.\n\nA tight communal timetablethe horariumis meant to ensure that the time given by God is not wasted but used in God's service, whether for prayer, work, meals, spiritual reading or sleep.\n\nAlthough Benedictines do not take a vow of silence, hours of strict silence are set, and at other times silence is maintained as much as is practically possible. Social conversations tend to be limited to communal recreation times. But such details, like the many other details of the daily routine of a Benedictine house that the Rule of St Benedict leaves to the discretion of the superior, are set out in its 'customary'. A ' customary' is the code adopted by a particular Benedictine house, adapting the Rule to local conditions.\n\nIn the Roman Catholic Church, according to the norms of the 1983 Code of Canon Law, a Benedictine abbey is a \"religious institute\" and its members are therefore members of the consecrated life. While Canon Law 588 §1 explains that Benedictine monks are \"neither clerical nor lay\", they can, however, be ordained. Benedictine Oblates endeavor to embrace the spirit of the Benedictine vow in their own life in the world.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4241", "url": "https://en.wikipedia.org/wiki?curid=4241", "title": "Bayezid I", "text": "Bayezid I\n\nBayezid I (; ; nicknamed \"Yıldırım\" (Ottoman Turkish: یلدیرم), \"Lightning\"; 1360 – 8 March 1403) was the Ottoman Sultan from 1389 to 1402. He was the son of Murad I and Gülçiçek Hatun. He built one of the largest armies in the known world at the time and unsuccessfully besieged Constantinople. He adopted the title of Sultan-i Rûm, Rûm being an old Islamic name for the Roman Empire. He was defeated and captured by Timur at the Battle of Ankara in 1402 and died in captivity in March 1403.\n\nThe first major role of Bayezid was as governor of Kütahya, a city that was conquered from the Germiyanids. He was an impetuous soldier, earning the nickname Lightning in a battle against the Karamanids.\nBayezid ascended to the throne following the death of his father Murad I, who was killed by Serbian knight Miloš Obilić during (15 June), or immediately after (16 June), the Battle of Kosovo in 1389, by which Serbia became a vassal of the Ottoman Sultanate. Immediately after obtaining the throne, he had his younger brother strangled to avoid a plot. In 1390, Bayezid took as a wife Princess Olivera Despina, the daughter of Prince Lazar of Serbia, who also lost his life in Kosovo. Bayezid recognized Stefan Lazarević, the son of Lazar, as the new Serbian leader (later despot), with considerable autonomy.\n\nThe upper Serbia resisted the Ottomans until general Pashayigit captured the city of Skopje in 1391, converting the city to an important base of operations.\n\nMeanwhile, the sultan began unifying Anatolia under his rule. Forcible expansion into Muslim territories could endanger the Ottoman relationship with the gazis, who were an important source of warriors for this ruling house on the European frontier. So Bayezid began the practice to first secure \"fatwa\"s, or legal rulings from Islamic scholars, justifying their wars against these Muslim states. However he suspected the loyalty of his Muslim Turkoman followers, for Bayezid relied heavily on his Serbian and Byzantine vassal troops to perform these conquests.\n\nIn a single campaign over the summer and fall of 1390, Bayezid conquered the beyliks of Aydin, Saruhan and Menteşe. His major rival Sulayman, the emir of Karaman, responded by allying himself with the ruler of Sivas, Kadi Burhan al-Din and the remaining Turkish beyliks. Nevertheless, Bayezid pushed on and in the fall and winter of 1390 overwhelmed the remaining beyliks -- Hamid, Teke, and Germiyan—as well as taking the cities of Akşehir and Niğde, as well as their capital Konya from the Karaman. At this point, Bayezid accepted peace proposals from Karaman (1391), concerned that further advances would antagonize his Turkoman followers and lead them to ally with Kadi Burhan al-Din. Once peace had been made with Karaman, Bayezid moved north against Kastamonu which had given refuge to many fleeing from his forces, and conquered both that city as well as Sinop.\n\nFrom 1389 to 1395 he conquered Bulgaria and northern Greece. In 1394 Bayezid crossed the River Danube to attack Wallachia, ruled at that time by Mircea the Elder. The Ottomans were superior in number, but on 10 October 1394 (or 17 May 1395), in the Battle of Rovine, on forested and swampy terrain, the Wallachians won the fierce battle and prevented Bayezid's army from advancing beyond the Danube.\n\nIn 1394, Bayezid laid siege to Constantinople, the capital of the Byzantine Empire. Anadoluhisarı fortress was built between 1393 and 1394 as part of preparations for the Second Ottoman Siege of Constantinople, which took place in 1395. On the urgings of the Byzantine emperor Manuel II Palaeologus a new crusade was organized to defeat him. This proved unsuccessful: in 1396 the Christian allies, under the leadership of the King of Hungary and future Holy Roman Emperor (in 1433) Sigismund, were defeated in the Battle of Nicopolis. Bayezid built the magnificent Ulu Cami in Bursa, to celebrate this victory.\n\nThus the siege of Constantinople continued, lasting until 1402. The beleaguered Byzantines had their reprieve when Bayezid fought the Timurid Empire in the East. At this time, the empire of Bayezid included Thrace (except Constantinople), Macedonia, Bulgaria, and parts of Serbia in Europe. In Asia, his domains extended to the Taurus Mountains. His army was considered one of the best in the Islamic world.\n\nIn 1397, Bayezid defeated the emir of Karaman in Akçay, killing him and annexing his territory. In 1398, the sultan conquered the Djanik emirate and the territory of Burhan al-Din, violating the accord with Timur. Finally, Bayezid occupied Elbistan and Malatya.\n\nIn 1400, the Central Asian warlord Timur succeeded in rousing the local Turkic beyliks who had been vassals of the Ottomans to join him in his attack on Bayezid, who was also considered one of the most powerful rulers in the Muslim world during that period. In the fateful Battle of Ankara, on 20 July 1402, Bayezid was captured by Timur and the Ottoman army was defeated. Many writers claim that Bayezid was mistreated by the Timurids. However, writers and historians from Timur's own court reported that Bayezid was treated well, and that Timur even mourned his death. One of Bayezid's sons, Mustafa Çelebi, was captured with him and held captive in Samarkand until 1405.\n\nFour of Bayezid's sons, specifically Süleyman Çelebi, İsa Çelebi, Mehmed Çelebi, and Musa Çelebi, however, escaped from the battlefield and later started a civil war for the Ottoman throne known as the Ottoman Interregnum. After Mehmed's victory, his coronation as Mehmed I, and the death of all four but Mehmed, Bayezid's other son Mustafa Çelebi emerged from hiding and began two failed rebellions against his brother Mehmed and, after Mehmed's death, his nephew Murat II.\n\nA commando battalion in the Pakistan Army is named Yaldaram Battalion after him.\nYildirim Beyazit University, a state university in Turkey, is also named after him.\n\nHis mother was Gülçiçek Hatun who was of ethnic Greek descent.\n\nBayezid had eight consorts\n\n\n\nThe defeat of Bayezid became a popular subject for later Western writers, composers, and painters. They embellished the legend that he was taken by Timur to Samarkand with a cast of characters to create an oriental fantasy that has maintained its appeal. Christopher Marlowe's play \"Tamburlaine the Great\" was first performed in London in 1587, three years after the formal opening of English-Ottoman trade relations when William Harborne sailed for Constantinople as an agent of the Levant Company.\n\nIn 1648, the play \"Le Gran Tamerlan et Bejezet\" by Jean Magnon appeared in London, and in 1725, Handel's \"Tamerlano\" was first performed and published in London; Vivaldi's version of the story, \"Bajazet\", was written in 1735. Magnon had given Bayezid an intriguing wife and daughter; the Handel and Vivaldi renditions included, as well as Tamerlane and Bayezid and his daughter, a prince of Byzantium and a princess of Trebizond (Trabzon) in a passionate love story. A cycle of paintings in Schloss Eggenberg, near Graz in Austria, translated the theme to a different medium; this was completed in the 1670s shortly before the Ottoman army attacked the Habsburgs in central Europe.\n\nBayezid (spelled Bayazid) is a central character in the Robert E. Howard story \"Lord of Samarcand.\"\n\n\n[aged 47–48]\n"}
{"id": "4242", "url": "https://en.wikipedia.org/wiki?curid=4242", "title": "Bayezid II", "text": "Bayezid II\n\nBayezid II (3 December 1447 – 26 May 1512) (Ottoman Turkish: بايزيد ثانى \"Bāyezīd-i sānī\", Turkish: \"II. Bayezid\" or \"II. Beyazıt\") was the eldest son and successor of Mehmed II, ruling as Sultan of the Ottoman Empire from 1481 to 1512. During his reign, Bayezid II consolidated the Ottoman Empire and thwarted a Safavid rebellion soon before abdicating his throne to his son, Selim I. He is most notable for evacuating Sephardi Jews from Spain after the proclamation of the Alhambra Decree and resettling them throughout the Ottoman Empire.\n\nBayezid II was the son of Mehmed II (1432–81) and Emine Gülbahar Hatun.\n\nBayezid II married Gülbahar Hatun, who was the mother of Bayezid II's successor, Selim I and nephew of Sitti Mükrime Hatun.\n\nBayezid II's overriding concern was the quarrel with his brother Cem, who claimed the throne and sought military backing from the Mamluks in Egypt. Having been defeated by his brother's armies, Cem sought protection from the Knights of St. John in Rhodes. Eventually, the Knights handed Cem over to Pope Innocent VIII (1484–1492). The Pope thought of using Cem as a tool to drive the Turks out of Europe, but as the papal crusade failed to come to fruition, Cem was left to languish and die in a Neapolitan prison. Bayezid II paid both the Knights Hospitaller and the pope to keep his brother prisoner.\n\nBayezid II ascended the Ottoman throne in 1481. Like his father, Bayezid II was a patron of western and eastern culture. Unlike many other Sultans, he worked hard to ensure a smooth running of domestic politics, which earned him the epithet of \"the Just\". Throughout his reign, Bayezid II engaged in numerous campaigns to conquer the Venetian possessions in Morea, accurately defining this region as the key to future Ottoman naval power in the Eastern Mediterranean. The last of these wars ended in 1501 with Bayezid II in control of the whole Peloponnese. Rebellions in the east, such as that of the Qizilbash, plagued much of Bayezid II's reign and were often backed by the Shah of Persia, Ismail, who was eager to promote Shi'ism to undermine the authority of the Ottoman state. Ottoman authority in Anatolia was indeed seriously threatened during this period and at one point Bayezid II's vizier, Ali Pasha, was killed in battle against rebels.\n\nIn July 1492, the new state of Spain expelled its Jewish and Muslim populations as part of the Spanish Inquisition. Bayezid II sent out the Ottoman Navy under the command of admiral Kemal Reis to Spain in 1492 in order to evacuate them safely to Ottoman lands. He sent out proclamations throughout the empire that the refugees were to be welcomed. He granted the refugees the permission to settle in the Ottoman Empire and become Ottoman citizens. He ridiculed the conduct of Ferdinand II of Aragon and Isabella I of Castile in expelling a class of people so useful to their subjects. \"You venture to call Ferdinand a wise ruler,\" he said to his courtiers, \"he who has impoverished his own country and enriched mine!\" Bayezid addressed a firman to all the governors of his European provinces, ordering them not only to refrain from repelling the Spanish refugees, but to give them a friendly and welcome reception. He threatened with death all those who treated the Jews harshly or refused them admission into the empire. Moses Capsali, who probably helped to arouse the sultan's friendship for the Jews, was most energetic in his assistance to the exiles. He made a tour of the communities and was instrumental in imposing a tax upon the rich, to ransom the Jewish victims of the persecution.\n\nThe Muslims and Jews of al-Andalus (Iberia) contributed much to the rising power of the Ottoman Empire by introducing new ideas, methods and craftsmanship. The first printing press in Constantinople was established by the Sephardic Jews in 1493. It is reported that under Bayezid's reign, Jews enjoyed a period of cultural flourishing, with the presence of such scholars as the Talmudist and scientist Mordecai Comtino; astronomer and poet Solomon ben Elijah Sharbiṭ ha-Zahab; Shabbethai ben Malkiel Cohen, and the liturgical poet Menahem Tamar.\n\nOn 14 September 1509, Constantinople was devastated by an earthquake. During Bayezid II's final years, a succession battle developed between his sons Selim I and Ahmet. Ahmet unexpectedly captured Karaman, an Ottoman city, and began marching to Constantinople to exploit his triumph. Fearing for his safety, Selim staged a revolt in Thrace but was defeated by Bayezid and forced to flee back to the Crimean Peninsula. Bayezid II developed fears that Ahmet might in turn kill him to gain the throne, so he refused to allow his son to enter Constantinople.\n\nSelim returned from Crimea and, with support from the Janissaries, forced his father to abdicate the throne on 25 April 1512. Bayezid departed for retirement in his native Demotika, but he died on 26 May 1512 at Büyükçekmece before reaching his destination and only a month after his abdication. He was buried next to the Bayezid Mosque in Istanbul.\nBayezid had eight consorts:\n\nBayezid had eight sons:\n\nBayezid had twelve daughters:\n\n"}
{"id": "4243", "url": "https://en.wikipedia.org/wiki?curid=4243", "title": "Boxing", "text": "Boxing\n\nBoxing is a combat sport in which two people, usually wearing protective gloves, throw punches at each other for a predetermined set of time in a boxing ring.\n\nAmateur boxing is both an Olympic and Commonwealth Games sport and is a common fixture in most international games—it also has its own World Championships. Boxing is supervised by a referee over a series of one- to three-minute intervals called rounds. The result is decided when an opponent is deemed incapable to continue by a referee, is disqualified for breaking a rule, resigns by throwing in a towel, or is pronounced the winner or loser based on the judges' scorecards at the end of the contest. In the event that both fighters gain equal scores from the judges, the fight is considered a draw (professional boxing). In Olympic boxing, due to the fact that a winner must be declared, in the case of a draw - the judges use technical criteria to choose the most deserving winner of the bout.\n\nWhile people have fought in hand-to-hand combat since before the dawn of history, the origin of boxing as an organized sport may be its acceptance by the ancient Greeks as an Olympic game in BC 688. Boxing evolved from 16th- and 18th-century prizefights, largely in Great Britain, to the forerunner of modern boxing in the mid-19th century with the 1867 introduction of the Marquess of Queensberry Rules.\n\nThe earliest known depiction of boxing comes from a Sumerian relief in Iraq from the 3rd millennium BCE Later depictions from the 2nd millennium BC are found in reliefs from the Mesopotamian nations of Assyria and Babylonia, and in Hittite art from Asia Minor. The earliest evidence for fist fighting with any kind of gloves can be found on Minoan Crete (c.1650–1400 BCE), and on Sardinia, if we consider the boxing statues of Prama mountains (c. 2000–1000 BC).\n\nIn Ancient Greece boxing was a well developed sport and enjoyed consistent popularity. In Olympic terms, it was first introduced in the 23rd Olympiad, 688 B.C. The boxers would wind leather thongs around their hands in order to protect them. There were no rounds and boxers fought until one of them acknowledged defeat or could not continue. Weight categories were not used, which meant heavyweights had a tendency to dominate. The style of boxing practiced typically featured an advanced left leg stance, with the left arm semi-extended as a guard, in addition to being used for striking, and with the right arm drawn back ready to strike. It was the head of the opponent which was primarily targeted, and there is little evidence to suggest that targeting the body was common.\n\nBoxing was a popular spectator sport in Ancient Rome. In order for the fighters to protect themselves against their opponents they wrapped leather thongs around their fists. Eventually harder leather was used and the thong soon became a weapon. The Romans even introduced metal studs to the thongs to make the cestus which then led to a more sinister weapon called the myrmex ('limb piercer'). Fighting events were held at Roman Amphitheatres. The Roman form of boxing was often a fight until death to please the spectators who gathered at such events. However, especially in later times, purchased slaves and trained combat performers were valuable commodities, and their lives were not given up without due consideration. Often slaves were used against one another in a circle marked on the floor. This is where the term ring came from. In AD 393, during the Roman gladiator period, boxing was abolished due to excessive brutality. It was not until the late 16th century that boxing re-surfaced in London.\n\nRecords of Classical boxing activity disappeared after the fall of the Western Roman Empire when the wearing of weapons became common once again and interest in fighting with the fists waned. However, there are detailed records of various fist-fighting sports that were maintained in different cities and provinces of Italy between the 12th and 17th centuries. There was also a sport in ancient Rus called Kulachniy Boy or \"Fist Fighting\".\n\nAs the wearing of swords became less common, there was renewed interest in fencing with the fists. The sport would later resurface in England during the early 16th century in the form of bare-knuckle boxing sometimes referred to as prizefighting. The first documented account of a bare-knuckle fight in England appeared in 1681 in the \"London Protestant Mercury\", and the first English bare-knuckle champion was James Figg in 1719. This is also the time when the word \"boxing\" first came to be used. It should be noted, that this earliest form of modern boxing was very different. Contests in Mr. Figg's time, in addition to fist fighting, also contained fencing and cudgeling. On 6 January 1681, the first recorded boxing match took place in Britain when Christopher Monck, 2nd Duke of Albemarle (and later Lieutenant Governor of Jamaica) engineered a bout between his butler and his butcher with the latter winning the prize.\n\nEarly fighting had no written rules. There were no weight divisions or round limits, and no referee. In general, it was extremely chaotic. An early article on boxing was published in Nottingham, 1713, by Sir Thomas Parkyns, a successful Wrestler from Bunny, Nottinghamshire, who had practised the techniques he described. The article, a single page in his manual of wrestling and fencing, \"Progymnasmata: The inn-play, or Cornish-hugg wrestler\", described a system of headbutting, punching, eye-gouging, chokes, and hard throws, not recognized in boxing today.\n\nThe first boxing rules, called the Broughton's rules, were introduced by champion Jack Broughton in 1743 to protect fighters in the ring where deaths sometimes occurred. Under these rules, if a man went down and could not continue after a count of 30 seconds, the fight was over. Hitting a downed fighter and grasping below the waist were prohibited. Broughton encouraged the use of 'mufflers', a form of padded bandage or mitten, to be used in 'jousting' or sparring sessions in training, and in exhibition matches.\nThese rules did allow the fighters an advantage not enjoyed by today's boxers; they permitted the fighter to drop to one knee to end the round and begin the 30-second count at any time. Thus a fighter realizing he was in trouble had an opportunity to recover. However, this was considered \"unmanly\" and was frequently disallowed by additional rules negotiated by the Seconds of the Boxers. In modern boxing, there is a three-minute limit to rounds (unlike the downed fighter ends the round rule). Intentionally going down in modern boxing will cause the recovering fighter to lose points in the scoring system. Furthermore, as the contestants did not have heavy leather gloves and wristwraps to protect their hands, they used different punching technique to preserve their hands, because the head was a common target to hit full out. Almost all period manuals have powerful straight punches with the whole body behind them to the face (including forehead) as the basic blows.\n\nThe London Prize Ring Rules introduced measures that remain in effect for professional boxing to this day, such as outlawing butting, gouging, scratching, kicking, hitting a man while down, holding the ropes, and using resin, stones or hard objects in the hands, and biting.\n\nIn 1867, the Marquess of Queensberry rules were drafted by John Chambers for amateur championships held at Lillie Bridge in London for Lightweights, Middleweights and Heavyweights. The rules were published under the patronage of the Marquess of Queensberry, whose name has always been associated with them.\n\nThere were twelve rules in all, and they specified that fights should be \"a fair stand-up boxing match\" in a 24-foot-square or similar ring. Rounds were three minutes with one-minute rest intervals between rounds. Each fighter was given a ten-second count if he was knocked down, and wrestling was banned.\nThe introduction of gloves of \"fair-size\" also changed the nature of the bouts. An average pair of boxing gloves resembles a bloated pair of mittens and are laced up around the wrists.\nThe gloves can be used to block an opponent's blows. As a result of their introduction, bouts became longer and more strategic with greater importance attached to defensive maneuvers such as slipping, bobbing, countering and angling. Because less defensive emphasis was placed on the use of the forearms and more on the gloves, the classical forearms outwards, torso leaning back stance of the bare knuckle boxer was modified to a more modern stance in which the torso is tilted forward and the hands are held closer to the face.\n\nThrough the late nineteenth century, the martial art of boxing or prizefighting was primarily a sport of dubious legitimacy. Outlawed in England and much of the United States, prizefights were often held at gambling venues and broken up by police. Brawling and wrestling tactics continued, and riots at prizefights were common occurrences. Still, throughout this period, there arose some notable bare knuckle champions who developed fairly sophisticated fighting tactics.\n\nThe English case of \"R v. Coney\" in 1882 found that a bare-knuckle fight was an assault occasioning actual bodily harm, despite the consent of the participants. This marked the end of widespread public bare-knuckle contests in England.\n\nThe first world heavyweight champion under the Queensberry Rules was \"Gentleman Jim\" Corbett, who defeated John L. Sullivan in 1892 at the Pelican Athletic Club in New Orleans.\n\nThe first instance of film censorship in the United States occurred in 1897 when several states banned the showing of prize fighting films from the state of Nevada, where it was legal at the time.\n\nThroughout the early twentieth century, boxers struggled to achieve legitimacy. They were aided by the influence of promoters like Tex Rickard and the popularity of great champions such as John L. Sullivan.\n\nThe \"Marquess of Queensberry rules\" have been the general rules governing modern boxing since their publication in 1867.\n\nA boxing match typically consists of a determined number of three-minute rounds, a total of up to 9 to 12 rounds. A minute is typically spent between each round with the fighters in their assigned corners receiving advice and attention from their coach and staff. The fight is controlled by a referee who works within the ring to judge and control the conduct of the fighters, rule on their ability to fight safely, count knocked-down fighters, and rule on fouls.\n\nUp to three judges are typically present at ringside to score the bout and assign points to the boxers, based on punches and elbows that connect, defense, knockdowns, hugging and other, more subjective, measures. Because of the open-ended style of boxing judging, many fights have controversial results, in which one or both fighters believe they have been \"robbed\" or unfairly denied a victory. Each fighter has an assigned corner of the ring, where his or her coach, as well as one or more \"seconds\" may administer to the fighter at the beginning of the fight and between rounds. Each boxer enters into the ring from their assigned corners at the beginning of each round and must cease fighting and return to their corner at the signalled end of each round.\n\nA bout in which the predetermined number of rounds passes is decided by the judges, and is said to \"go the distance\". The fighter with the higher score at the end of the fight is ruled the winner. With three judges, unanimous and split decisions are possible, as are draws. A boxer may win the bout before a decision is reached through a knock-out; such bouts are said to have ended \"inside the distance\". If a fighter is knocked down during the fight, determined by whether the boxer touches the canvas floor of the ring with any part of their body other than the feet as a result of the opponent's punch and not a slip, as determined by the referee, the referee begins counting until the fighter returns to his or her feet and can continue. Some jurisdictions require the referee to count to eight regardless of if the fighter gets up before.\n\nShould the referee count to ten, then the knocked-down boxer is ruled \"knocked out\" (whether unconscious or not) and the other boxer is ruled the winner by knockout (KO). A \"technical knock-out\" (TKO) is possible as well, and is ruled by the referee, fight doctor, or a fighter's corner if a fighter is unable to safely continue to fight, based upon injuries or being judged unable to effectively defend themselves. Many jurisdictions and sanctioning agencies also have a \"three-knockdown rule\", in which three knockdowns in a given round result in a TKO. A TKO is considered a knockout in a fighter's record. A \"standing eight\" count rule may also be in effect. This gives the referee the right to step in and administer a count of eight to a fighter that he or she feels may be in danger, even if no knockdown has taken place. After counting the referee will observe the fighter, and decide if he or she is fit to continue. For scoring purposes, a standing eight count is treated as a knockdown.\n\nIn general, boxers are prohibited from hitting below the belt, holding, tripping, pushing, biting, or spitting. The boxer's shorts are raised so the opponent is not allowed to hit to the groin area with intent to cause pain or injury. Failure to abide by the former may result in a foul. They also are prohibited from kicking, head-butting, or hitting with any part of the arm other than the knuckles of a closed fist (including hitting with the elbow, shoulder or forearm, as well as with open gloves, the wrist, the inside, back or side of the hand). They are prohibited as well from hitting the back, back of the head or neck (called a \"rabbit-punch\") or the kidneys. They are prohibited from holding the ropes for support when punching, holding an opponent while punching, or ducking below the belt of their opponent (dropping below the waist of your opponent, no matter the distance between).\n\nIf a \"clinch\" – a defensive move in which a boxer wraps his or her opponents arms and holds on to create a pause – is broken by the referee, each fighter must take a full step back before punching again (alternatively, the referee may direct the fighters to \"punch out\" of the clinch). When a boxer is knocked down, the other boxer must immediately cease fighting and move to the furthest neutral corner of the ring until the referee has either ruled a knockout or called for the fight to continue.\n\nViolations of these rules may be ruled \"fouls\" by the referee, who may issue warnings, deduct points, or disqualify an offending boxer, causing an automatic loss, depending on the seriousness and intentionality of the foul. An intentional foul that causes injury that prevents a fight from continuing usually causes the boxer who committed it to be disqualified. A fighter who suffers an accidental low-blow may be given up to five minutes to recover, after which they may be ruled knocked out if they are unable to continue. Accidental fouls that cause injury ending a bout may lead to a \"no contest\" result, or else cause the fight to go to a decision if enough rounds (typically four or more, or at least three in a four-round fight) have passed.\n\nUnheard of in the modern era, but common during the early 20th Century in North America, a \"newspaper decision (NWS)\" might be made after a no decision bout had ended. A \"no decision\" bout occurred when, by law or by pre-arrangement of the fighters, if both boxers were still standing at the fight's conclusion and there was no knockout, no official decision was rendered and neither boxer was declared the winner. But this did not prevent the pool of ringside newspaper reporters from declaring a consensus result among themselves and printing a newspaper decision in their publications. Officially, however, a \"no decision\" bout resulted in neither boxer winning or losing. Boxing historians sometimes use these unofficial newspaper decisions in compiling fight records for illustrative purposes only. Often, media outlets covering a match will personally score the match, and post their scores as an independent sentence in their report.\n\nThroughout the 17th to 19th centuries, boxing bouts were motivated by money, as the fighters competed for prize money, promoters controlled the gate, and spectators bet on the result. The modern Olympic movement revived interest in amateur sports, and amateur boxing became an Olympic sport in 1908. In their current form, Olympic and other amateur bouts are typically limited to three or four rounds, scoring is computed by points based on the number of clean blows landed, regardless of impact, and fighters wear protective headgear, reducing the number of injuries, knockdowns, and knockouts. Currently scoring blows in amateur boxing are subjectively counted by ringside judges, but the Australian Institute for Sport has demonstrated a prototype of an Automated Boxing Scoring System, which introduces scoring objectivity, improves safety, and arguably makes the sport more interesting to spectators. Professional boxing remains by far the most popular form of the sport globally, though amateur boxing is dominant in Cuba and some former Soviet republics. For most fighters, an amateur career, especially at the Olympics, serves to develop skills and gain experience in preparation for a professional career.\n\nAmateur boxing may be found at the collegiate level, at the Olympic Games and Commonwealth Games, and in many other venues sanctioned by amateur boxing associations. Amateur boxing has a point scoring system that measures the number of clean blows landed rather than physical damage. Bouts consist of three rounds of three minutes in the Olympic and Commonwealth Games, and three rounds of three minutes in a national ABA (Amateur Boxing Association) bout, each with a one-minute interval between rounds.\n\nCompetitors wear protective headgear and gloves with a white strip or circle across the knuckle. There are cases however, where white ended gloves are not required but any solid color may be worn. The white end just is a way to make it easier for judges to score clean hits. Each competitor must have their hands properly wrapped, pre-fight, for added protection on their hands and for added cushion under the gloves. Gloves worn by the fighters must be twelve ounces in weight unless, the fighters weigh under 165 pounds, thus allowing them to wear 10 ounce gloves. A punch is considered a scoring punch only when the boxers connect with the white portion of the gloves. Each punch that lands cleanly on the head or torso with sufficient force is awarded a point. A referee monitors the fight to ensure that competitors use only legal blows. A belt worn over the torso represents the lower limit of punches – any boxer repeatedly landing low blows below the belt is disqualified. Referees also ensure that the boxers don't use holding tactics to prevent the opponent from swinging. If this occurs, the referee separates the opponents and orders them to continue boxing. Repeated holding can result in a boxer being penalized or ultimately disqualified. Referees will stop the bout if a boxer is seriously injured, if one boxer is significantly dominating the other or if the score is severely imbalanced. Amateur bouts which end this way may be noted as \"RSC\" (referee stopped contest) with notations for an outclassed opponent (RSCO), outscored opponent (RSCOS), injury (RSCI) or head injury (RSCH).\n\nProfessional bouts are usually much longer than amateur bouts, typically ranging from ten to twelve rounds, though four-round fights are common for less experienced fighters or club fighters. There are also some two- and three-round professional bouts, especially in Australia. Through the early 20th century, it was common for fights to have unlimited rounds, ending only when one fighter quit, benefiting high-energy fighters like Jack Dempsey. Fifteen rounds remained the internationally recognized limit for championship fights for most of the 20th century until the early 1980s, when the death of boxer Duk Koo Kim eventually prompted the World Boxing Council and other organizations sanctioning professional boxing to reduce the limit to twelve rounds.\n\nHeadgear is not permitted in professional bouts, and boxers are generally allowed to take much more damage before a fight is halted. At any time, the referee may stop the contest if he believes that one participant cannot defend himself due to injury. In that case, the other participant is awarded a technical knockout win. A technical knockout would also be awarded if a fighter lands a punch that opens a cut on the opponent, and the opponent is later deemed not fit to continue by a doctor because of the cut. For this reason, fighters often employ cutmen, whose job is to treat cuts between rounds so that the boxer is able to continue despite the cut. If a boxer simply quits fighting, or if his corner stops the fight, then the winning boxer is also awarded a technical knockout victory. In contrast with amateur boxing, professional male boxers have to be bare-chested.\n\n\"Style\" is often defined as the strategic approach a fighter takes during a bout. No two fighters' styles are alike, as it is determined by that individual's physical and mental attributes. Three main styles exist in boxing: outside fighter (\"boxer\"), brawler (or \"slugger\"), and Inside fighter (\"swarmer\"). These styles may be divided into several special subgroups, such as counter puncher, etc. The main philosophy of the styles is, that each style has an advantage over one, but disadvantage over the other one. It follows the rock-paper-scissors scenario - boxer beats brawler, brawler beats swarmer, and swarmer beats boxer.\n\nA classic \"boxer\" or stylist (also known as an \"out-fighter\") seeks to maintain distance between himself and his opponent, fighting with faster, longer range punches, most notably the jab, and gradually wearing his opponent down. Due to this reliance on weaker punches, out-fighters tend to win by point decisions rather than by knockout, though some out-fighters have notable knockout records. They are often regarded as the best boxing strategists due to their ability to control the pace of the fight and lead their opponent, methodically wearing him down and exhibiting more skill and finesse than a brawler. Out-fighters need reach, hand speed, reflexes, and footwork.\n\nNotable out-fighters include Muhammad Ali, Larry Holmes, Joe Calzaghe, Wilfredo Gómez, \nSalvador Sanchez, Cecilia Brækhus, Gene Tunney, Ezzard Charles, Willie Pep, Meldrick Taylor, Ricardo Lopez, Floyd Mayweather, Roy Jones, Jr., Sugar Ray Leonard, Miguel Vazquez, Sergio \"Maravilla\" Martínez, Vitali Klitschko, Wladimir Klitschko and Guillermo Rigondeaux. This style was also used by fictional boxer Apollo Creed.\n\nA boxer-puncher is a well-rounded boxer who is able to fight at close range with a combination of technique and power, often with the ability to knock opponents out with a combination and in some instances a single shot. Their movement and tactics are similar to that of an out-fighter (although they are generally not as mobile as an out-fighter), but instead of winning by decision, they tend to wear their opponents down using combinations and then move in to score the knockout. A boxer must be well rounded to be effective using this style.\n\nNotable boxer-punchers include Muhammad Ali, Saúl Álvarez, Wladimir Klitschko, Vasyl Lomachenko, Lennox Lewis, Joe Louis, Wilfredo Gómez, Oscar de la Hoya, Archie Moore, Miguel Cotto, Nonito Donaire, Sam Langford, Henry Armstrong, Sugar Ray Robinson, Tony Zale, Carlos Monzón, Alexis Argüello, Erik Morales, Terry Norris, Marco Antonio Barrera, Naseem Hamed and Thomas Hearns.\n\nCounter punchers are slippery, defensive style fighters who often rely on their opponent's mistakes in order to gain the advantage, whether it be on the score cards or more preferably a knockout. They use their well-rounded defense to avoid or block shots and then immediately catch the opponent off guard with a well placed and timed punch. A fight with a skilled counter-puncher can turn into a war of attrition, where each shot landed is a battle in itself. Thus, fighting against counter punchers requires constant feinting and the ability to avoid telegraphing one's attacks. To be truly successful using this style they must have good reflexes, a high level of prediction and awareness, pinpoint accuracy and speed, both in striking and in footwork.\n\nNotable counter punchers include Muhammad Ali, Vitali Klitschko, Evander Holyfield, Max Schmeling, Chris Byrd, Jim Corbett, Jack Johnson, Bernard Hopkins, Laszlo Papp, Jerry Quarry, Anselmo Moreno, James Toney, Marvin Hagler, Juan Manuel Márquez, Humberto Soto, Floyd Mayweather, Jr., Roger Mayweather, Pernell Whitaker, Sergio Gabriel Martinez and Guillermo Rigondeaux. This style of boxing is also used by fictional boxer Little Mac.\n\nCounter punchers usually wear their opponents down by causing them to miss their punches. The more the opponent misses, the faster they tire, and the psychological effects of being unable to land a hit will start to sink in. The counter puncher often tries to outplay their opponent entirely, not just in a physical sense, but also in a mental and emotional sense. This style can be incredibly difficult, especially against seasoned fighters, but winning a fight without getting hit is often worth the pay-off. They usually try to stay away from the center of the ring, in order to outmaneuver and chip away at their opponents. A large advantage in counter-hitting is the forward momentum of the attacker, which drives them further into your return strike. As such, knockouts are more common than one would expect from a defensive style.\n\nA brawler is a fighter who generally lacks finesse and footwork in the ring, but makes up for it through sheer punching power. Many brawlers tend to lack mobility, preferring a less mobile, more stable platform and have difficulty pursuing fighters who are fast on their feet. They may also have a tendency to ignore combination punching in favor of continuous beat-downs with one hand and by throwing slower, more powerful single punches (such as hooks and uppercuts). Their slowness and predictable punching pattern (single punches with obvious leads) often leaves them open to counter punches, so successful brawlers must be able to absorb substantial amounts of punishment. However, not all brawler/slugger fighters are not mobile; some can move around and switch styles if needed but still have the brawler/slugger style such as Wilfredo Gómez, Prince Naseem Hamed and Danny García.\n\nA brawler's most important assets are power and chin (the ability to absorb punishment while remaining able to continue boxing). Examples of this style include George Foreman, Rocky Marciano, Julio Cesar Chavez, Roberto Duran, Danny García, Wilfredo Gómez, Sonny Liston, John L. Sullivan, Max Baer, Prince Naseem Hamed, Ray Mancini, David Tua, Arturo Gatti, Micky Ward, Brandon Ríos, Ruslan Provodnikov, Michael Katsidis, James Kirkland, Marcos Maidana, Jake Lamotta, Manny Pacquiao, and Ireland's John Duddy. This style of boxing was also used by fictional boxers Rocky Balboa and James \"Clubber\" Lang. This style was also used by the Street Fighter character Balrog.\n\nBrawlers tend to be more predictable and easy to hit but usually fare well enough against other fighting styles because they train to take punches very well. They often have a higher chance than other fighting styles to score a knockout against their opponents because they focus on landing big, powerful hits, instead of smaller, faster attacks. Oftentimes they place focus on training on their upper body instead of their entire body, to increase power and endurance. They also aim to intimidate their opponents because of their power, stature and ability to take a punch.\n\nIn-fighters/swarmers (sometimes called \"pressure fighters\") attempt to stay close to an opponent, throwing intense flurries and combinations of hooks and uppercuts. Mainly Mexican, Irish, Irish-American, Puerto Rican, and Mexican-American boxers popularized this style. A successful in-fighter often needs a good \"chin\" because swarming usually involves being hit with many jabs before they can maneuver inside where they are more effective. In-fighters operate best at close range because they are generally shorter and have less reach than their opponents and thus are more effective at a short distance where the longer arms of their opponents make punching awkward. However, several fighters tall for their division have been relatively adept at in-fighting as well as out-fighting.\n\nThe essence of a swarmer is non-stop aggression. Many short in-fighters utilize their stature to their advantage, employing a bob-and-weave defense by bending at the waist to slip underneath or to the sides of incoming punches. Unlike blocking, causing an opponent to miss a punch disrupts his balance, this permits forward movement past the opponent's extended arm and keeps the hands free to counter. A distinct advantage that in-fighters have is when throwing uppercuts, they can channel their entire bodyweight behind the punch; Mike Tyson was famous for throwing devastating uppercuts. Marvin Hagler was known for his hard \"chin\", punching power, body attack and the stalking of his opponents. Some in-fighters, like Mike Tyson, have been known for being notoriously hard to hit. The key to a swarmer is aggression, endurance, chin, and bobbing-and-weaving.\n\nNotable in-fighters include Henry Armstrong, Aaron Pryor, Julio César Chávez, Jack Dempsey, Miguel Cotto, Joe Frazier, Danny García, Mike Tyson, Manny Pacquiao, Rocky Marciano, Wayne McCullough, Gerry Penalosa, Harry Greb, David Tua, Ricky Hatton and Gennady Golovkin.\n\nAll fighters have primary skills with which they feel most comfortable, but truly elite fighters are often able to incorporate auxiliary styles when presented with a particular challenge. For example, an out-fighter will sometimes plant his feet and counter punch, or a slugger may have the stamina to pressure fight with his power punches.\n\nThere is a generally accepted rule of thumb about the success each of these boxing styles has against the others. In general, an in-fighter has an advantage over an out-fighter, an out-fighter has an advantage over a brawler, and a brawler has an advantage over an in-fighter; these form a cycle with each style being stronger relative to one, and weaker relative to another, with none dominating, as in rock-paper-scissors. Naturally, many other factors, such as the skill level and training of the combatants, determine the outcome of a fight, but the widely held belief in this relationship among the styles is embodied in the cliché amongst boxing fans and writers that \"styles make fights.\"\n\nBrawlers tend to overcome swarmers or in-fighters because, in trying to get close to the slugger, the in-fighter will invariably have to walk straight into the guns of the much harder-hitting brawler, so, unless the former has a very good chin and the latter's stamina is poor, the brawler's superior power will carry the day. A famous example of this type of match-up advantage would be George Foreman's knockout victory over Joe Frazier in their original bout \"The Sunshine Showdown\".\n\nAlthough in-fighters struggle against heavy sluggers, they typically enjoy more success against out-fighters or boxers. Out-fighters prefer a slower fight, with some distance between themselves and the opponent. The in-fighter tries to close that gap and unleash furious flurries. On the inside, the out-fighter loses a lot of his combat effectiveness, because he cannot throw the hard punches. The in-fighter is generally successful in this case, due to his intensity in advancing on his opponent and his good agility, which makes him difficult to evade. For example, the swarming Joe Frazier, though easily dominated by the slugger George Foreman, was able to create many more problems for the boxer Muhammad Ali in their three fights. Joe Louis, after retirement, admitted that he hated being crowded, and that swarmers like untied/undefeated champ Rocky Marciano would have caused him style problems even in his prime.\n\nThe boxer or out-fighter tends to be most successful against a brawler, whose slow speed (both hand and foot) and poor technique makes him an easy target to hit for the faster out-fighter. The out-fighter's main concern is to stay alert, as the brawler only needs to land one good punch to finish the fight. If the out-fighter can avoid those power punches, he can often wear the brawler down with fast jabs, tiring him out. If he is successful enough, he may even apply extra pressure in the later rounds in an attempt to achieve a knockout. Most classic boxers, such as Muhammad Ali, enjoyed their best successes against sluggers.\n\nAn example of a style matchup was the historical fight of Julio César Chávez, a swarmer or in-fighter, against Meldrick Taylor, the boxer or out-fighter (see Julio César Chávez vs. Meldrick Taylor). The match was nicknamed \"Thunder Meets Lightning\" as an allusion to punching power of Chávez and blinding speed of Taylor. Chávez was the epitome of the \"Mexican\" style of boxing. Taylor's hand and foot speed and boxing abilities gave him the early advantage, allowing him to begin building a large lead on points. Chávez remained relentless in his pursuit of Taylor and due to his greater punching power Chávez slowly punished Taylor. Coming into the later rounds, Taylor was bleeding from the mouth, his entire face was swollen, the bones around his eye socket had been broken, he had swallowed a considerable amount of his own blood, and as he grew tired, Taylor was increasingly forced into exchanging blows with Chávez, which only gave Chávez a greater chance to cause damage. While there was little doubt that Taylor had solidly won the first three quarters of the fight, the question at hand was whether he would survive the final quarter. Going into the final round, Taylor held a secure lead on the scorecards of two of the three judges. Chávez would have to knock Taylor out to claim a victory, whereas Taylor merely needed to stay away from the Mexican legend. However, Taylor did not stay away, but continued to trade blows with Chávez. As he did so, Taylor showed signs of extreme exhaustion, and every tick of the clock brought Taylor closer to victory unless Chávez could knock him out.\nWith about a minute left in the round, Chávez hit Taylor squarely with several hard punches and stayed on the attack, continuing to hit Taylor with well-placed shots. Finally, with about 25 seconds to go, Chávez landed a hard right hand that caused Taylor to stagger forward towards a corner, forcing Chávez back ahead of him. Suddenly Chávez stepped around Taylor, positioning him so that Taylor was trapped in the corner, with no way to escape from Chávez' desperate final flurry. Chávez then nailed Taylor with a tremendous right hand that dropped the younger man. By using the ring ropes to pull himself up, Taylor managed to return to his feet and was given the mandatory 8-count. Referee Richard Steele asked Taylor twice if he was able to continue fighting, but Taylor failed to answer. Steele then concluded that Taylor was unfit to continue and signaled that he was ending the fight, resulting in a TKO victory for Chávez with only two seconds to go in the bout.\n\nSince boxing involves forceful, repetitive punching, precautions must be taken to prevent damage to bones in the hand. Most trainers do not allow boxers to train and spar without wrist wraps and boxing gloves. Hand wraps are used to secure the bones in the hand, and the gloves are used to protect the hands from blunt injury, allowing boxers to throw punches with more force than if they did not utilise them. Gloves have been required in competition since the late nineteenth century, though modern boxing gloves are much heavier than those worn by early twentieth-century fighters. Prior to a bout, both boxers agree upon the weight of gloves to be used in the bout, with the understanding that lighter gloves allow heavy punchers to inflict more damage. The brand of gloves can also affect the impact of punches, so this too is usually stipulated before a bout. Both sides are allowed to inspect the wraps and gloves of the opponent to help ensure both are within agreed upon specifications and no tampering has taken place.\n\nA mouth guard is important to protect the teeth and gums from injury, and to cushion the jaw, resulting in a decreased chance of knockout. Both fighters must wear soft soled shoes to reduce the damage from accidental (or intentional) stepping on feet. While older boxing boots more commonly resembled those of a professional wrestler, modern boxing shoes and boots tend to be quite similar to their amateur wrestling counterparts.\n\nBoxers practice their skills on two basic types of punching bags. A small, tear-drop-shaped \"speed bag\" is used to hone reflexes and repetitive punching skills, while a large cylindrical \"heavy bag\" filled with sand, a synthetic substitute, or water is used to practice power punching and body blows. In addition to these distinctive pieces of equipment, boxers also utilize sport-nonspecific training equipment to build strength, speed, agility, and stamina. Common training equipment includes free weights, rowing machines, jump rope, and medicine balls.\n\nBoxing matches typically take place in a boxing ring, a raised platform surrounded by ropes attached to posts rising in each corner. The term \"ring\" has come to be used as a metaphor for many aspects of prize fighting in general.\n\nThe modern boxing stance differs substantially from the typical boxing stances of the 19th and early 20th centuries. The modern stance has a more upright vertical-armed guard, as opposed to the more horizontal, knuckles-facing-forward guard adopted by early 20th century hook users such as Jack Johnson.\n\nIn a fully upright stance, the boxer stands with the legs shoulder-width apart and the rear foot a half-step in front of the lead man. Right-handed or orthodox boxers lead with the left foot and fist (for most penetration power). Both feet are parallel, and the right heel is off the ground. The lead (left) fist is held vertically about six inches in front of the face at eye level. The rear (right) fist is held beside the chin and the elbow tucked against the ribcage to protect the body. The chin is tucked into the chest to avoid punches to the jaw which commonly cause knock-outs and is often kept slightly off-center. Wrists are slightly bent to avoid damage when punching and the elbows are kept tucked in to protect the ribcage. Some boxers fight from a crouch, leaning forward and keeping their feet closer together. The stance described is considered the \"textbook\" stance and fighters are encouraged to change it around once it's been mastered as a base. Case in point, many fast fighters have their hands down and have almost exaggerated footwork, while brawlers or bully fighters tend to slowly stalk their opponents.\n\nLeft-handed or southpaw fighters use a mirror image of the orthodox stance, which can create problems for orthodox fighters unaccustomed to receiving jabs, hooks, or crosses from the opposite side. The southpaw stance, conversely, is vulnerable to a straight right hand.\n\nNorth American fighters tend to favor a more balanced stance, facing the opponent almost squarely, while many European fighters stand with their torso turned more to the side. The positioning of the hands may also vary, as some fighters prefer to have both hands raised in front of the face, risking exposure to body shots.\n\nModern boxers can sometimes be seen tapping their cheeks or foreheads with their fists in order to remind themselves to keep their hands up (which becomes difficult during long bouts). Boxers are taught to push off with their feet in order to move effectively. Forward motion involves lifting the lead leg and pushing with the rear leg. Rearward motion involves lifting the rear leg and pushing with the lead leg. During lateral motion the leg in the direction of the movement moves first while the opposite leg provides the force needed to move the body.\n\nThere are four basic punches in boxing: the jab, cross, hook and uppercut. Any punch other than a jab is considered a power punch. If a boxer is right-handed (orthodox), his left hand is the lead hand and his right hand is the rear hand. For a left-handed boxer or southpaw, the hand positions are reversed. For clarity, the following discussion will assume a right-handed boxer.\n\n\n\nThese different punch types can be thrown in rapid succession to form combinations or \"combos.\" The most common is the jab and cross combination, nicknamed the \"one-two combo.\" This is usually an effective combination, because the jab blocks the opponent's view of the cross, making it easier to land cleanly and forcefully.\n\nA large, swinging circular punch starting from a cocked-back position with the arm at a longer extension than the hook and all of the fighter's weight behind it is sometimes referred to as a \"roundhouse,\" \"haymaker,\" \"overhand,\" or sucker-punch. Relying on body weight and centripetal force within a wide arc, the roundhouse can be a powerful blow, but it is often a wild and uncontrolled punch that leaves the fighter delivering it off balance and with an open guard.\n\nWide, looping punches have the further disadvantage of taking more time to deliver, giving the opponent ample warning to react and counter. For this reason, the haymaker or roundhouse is not a conventional punch, and is regarded by trainers as a mark of poor technique or desperation. Sometimes it has been used, because of its immense potential power, to finish off an already staggering opponent who seems unable or unlikely to take advantage of the poor position it leaves the puncher in.\n\nAnother unconventional punch is the rarely used bolo punch, in which the opponent swings an arm out several times in a wide arc, usually as a distraction, before delivering with either that or the other arm.\n\nAn illegal punch to the back of the head or neck is known as a rabbit punch.\n\nIt should be noted both the hook and uppercut may be thrown with both hands, resulting in differing footwork and positioning from that described above if thrown by the other hand. Generally the analogous opposite is true of the footwork and torso movement.\n\nThere are several basic maneuvers a boxer can use in order to evade or block punches, depicted and discussed below.\n\n\n\nIn boxing, each fighter is given a corner of the ring where he rests in between rounds for 1 minute and where his trainers stand. Typically, three men stand in the corner besides the boxer himself; these are the trainer, the assistant trainer and the cutman. The trainer and assistant typically give advice to the boxer on what he is doing wrong as well as encouraging him if he is losing. The cutman is a cutaneous doctor responsible for keeping the boxer's face and eyes free of cuts and blood. This is of particular importance because many fights are stopped because of cuts that threaten the boxer's eyes.\n\nIn addition, the corner is responsible for stopping the fight if they feel their fighter is in grave danger of permanent injury. The corner will occasionally throw in a white towel to signify a boxer's surrender (the idiomatic phrase \"to throw in the towel\", meaning to give up, derives from this practice). This can be seen in the fight between Diego Corrales and Floyd Mayweather. In that fight, Corrales' corner surrendered despite Corrales' steadfast refusal.\n\nKnocking a person unconscious or even causing a concussion may cause permanent brain damage. There is no clear division between the force required to knock a person out and the force likely to kill a person. Since 1980, more than 200 amateur boxers, professional boxers and Toughman fighters have died due to ring or training injuries. In 1983, editorials in the \"Journal of the American Medical Association\" called for a ban on boxing. The editor, Dr. George Lundberg, called boxing an \"obscenity\" that \"should not be sanctioned by any civilized society.\" Since then, the British, Canadian and Australian Medical Associations have called for bans on boxing.\n\nSupporters of the ban state that boxing is the only sport where hurting the other athlete is the goal. Dr. Bill O'Neill, boxing spokesman for the British Medical Association, has supported the BMA's proposed ban on boxing: \"It is the only sport where the intention is to inflict serious injury on your opponent, and we feel that we must have a total ban on boxing.\" Opponents respond that such a position is misguided opinion, stating that amateur boxing is scored solely according to total connecting blows with no award for \"injury\". They observe that many skilled professional boxers have had rewarding careers without inflicting injury on opponents by accumulating scoring blows and avoiding punches winning rounds scored 10-9 by the 10-point must system, and they note that there are many other sports where concussions are much more prevalent.\n\nIn 2007, one study of amateur boxers showed that protective headgear did not prevent brain damage, and another found that amateur boxers faced a high risk of brain damage. The Gothenburg study analyzed temporary levels of neurofiliment light in cerebral spinal fluid which they conclude is evidence of damage, even though the levels soon subside. More comprehensive studies of neurologiocal function on larger samples performed by Johns Hopkins University and accident rates analyzed by National Safety Council show amateur boxing is a comparatively safe sport.\n\nIn 1997, the American Association of Professional Ringside Physicians was established to create medical protocols through research and education to prevent injuries in boxing.\n\nProfessional boxing is forbidden in Iceland, Iran, Saudi Arabia and North Korea. It was banned in Sweden until 2007 when the ban was lifted but strict restrictions, including four three-minute rounds for fights, were imposed. It was banned in Albania from 1965 till the fall of Communism in 1991; it is now legal there. Norway legalized professional boxing in December 2014.\n\nThe sport of boxing has two internationally recognized boxing halls of fame; the International Boxing Hall of Fame (IBHOF) and the World Boxing Hall of Fame (WBHF), with the IBHOF being the more widely recognized boxing hall of fame. In 2013, The Boxing Hall of Fame Las Vegas opened in Las Vegas, NV founded by Steve Lott, former assistant manager for Mike Tyson.\n\nThe WBHF was founded by Everett L. Sanders in 1980. Since its inception the WBHOF has never had a permanent location or museum, which has allowed the more recent IBHOF to garner more publicity and prestige. Among the notable names in the WBHF are Ricardo \"Finito\" Lopez, Gabriel \"Flash\" Elorde, Michael Carbajal, Khaosai Galaxy, Henry Armstrong, Jack Johnson, Roberto Durán, George Foreman, Ceferino Garcia and Salvador Sanchez. Boxing's International Hall of Fame was inspired by a tribute an American town held for two local heroes in 1982. The town, Canastota, New York, (which is about east of Syracuse, via the New York State Thruway), honored former world welterweight/middleweight champion Carmen Basilio and his nephew, former world welterweight champion Billy Backus. The people of Canastota raised money for the tribute which inspired the idea of creating an official, annual hall of fame for notable boxers.\n\nThe International Boxing Hall of Fame opened in Canastota in 1989. The first inductees in 1990 included Jack Johnson, Benny Leonard, Jack Dempsey, Henry Armstrong, Sugar Ray Robinson, Archie Moore, and Muhammad Ali. Other world-class figures include Salvador Sanchez, Jose Napoles, Roberto \"Manos de Piedra\" Durán, Ricardo Lopez, Gabriel \"Flash\" Elorde, Vicente Saldivar, Ismael Laguna, Eusebio Pedroza, Carlos Monzón, Azumah Nelson, Rocky Marciano, Pipino Cuevas and Ken Buchanan. The Hall of Fame's induction ceremony is held every June as part of a four-day event. The fans who come to Canastota for the Induction Weekend are treated to a number of events, including scheduled autograph sessions, boxing exhibitions, a parade featuring past and present inductees, and the induction ceremony itself.\n\nThe Boxing Hall of Fame Las Vegas features the $75 million ESPN Classic Sports fight film and tape library and radio broadcast collection. The collection includes the fights of all the great champions including: Muhammad Ali, Mike Tyson, George Foreman, Roberto Duran, Marvin Hagler, Jack Dempsey, Joe Louis, Joe Frazier, Rocky Marciano and Sugar Ray Robinson. It is this exclusive fight film library that will separate the Boxing Hall of Fame Las Vegas from the other halls of fame which do not have rights to any video of their sports. The inaugural inductees included Muhammad Ali, Henry Armstrong, Tony Canzoneri, Ezzard Charles, Julio Cesar Chavez Sr., Jack Dempsey, Roberto Duran, Joe Louis, and Sugar Ray Robinson\n\n\n\n\nThere are various organization and websites, that rank boxers in both weight class and pound-for-pound manner.\n\n\n"}
{"id": "4246", "url": "https://en.wikipedia.org/wiki?curid=4246", "title": "Bollywood", "text": "Bollywood\n\nBollywood is the sobriquet for India's Hindi language film industry, based in the city of Mumbai (formerly Bombay), Maharashtra. It is more formally referred to as Hindi cinema. The term \"Bollywood\" is often used by non-Indians as a synecdoche to refer to the whole of Indian cinema; however, Bollywood proper is only a part of the larger Indian film industry, which includes other production centres producing films in many other Indian languages.\n\nBy revenue, Bollywood is the largest film producer in India, representing 43% of the net box office, while Tamil and Telugu cinema represent 36%, and the rest of the regional cinema constitutes 21% as of 2014. Bollywood is also one of the largest centers of film production in the world. Furthermore, Bollywood is one of the biggest film industries in the world in terms of the number of people employed and the number of films produced. According to J. Matusitz and P. Payano, in 2011, over 3.5 billion tickets were sold across the globe, which in comparison is 900,000 tickets more than Hollywood. Bollywood produced 252 films in 2014 out of a total of 1969 films produced in Indian cinema.\n\nThe name \"Bollywood\" is a portmanteau derived from Bombay (the former name for Mumbai) and Hollywood (in California), the center of the American film industry. The naming scheme for \"Bollywood\" was inspired by \"Tollywood\", the name that was used to refer to the cinema of West Bengal. Dating back to 1932, \"Tollywood\" was the earliest Hollywood-inspired name, referring to the Bengali film industry based in Tollygunge (in Calcutta, West Bengal), whose name is reminiscent of \"Hollywood\" and was the centre of the cinema of India at the time. It was this \"chance juxtaposition of two pairs of rhyming syllables,\" Holly and Tolly, that led to the portmanteau name \"Tollywood\" being coined. The name \"Tollywood\" went on to be used as a nickname for the Bengali film industry by the popular Calcutta-based \"Junior Statesman\" youth magazine, establishing a precedent for other film industries to use similar-sounding names, eventually leading to the coining of \"Bollywood\". \"Tollywood\" is now also popularly used to refer to the Telugu film industry in Telangana and Andhra Pradesh.\n\nThe term \"Bollywood\" itself has origins in the 1970s, when India overtook the United States as the world's largest film producer. Credit for the term has been claimed by several different people, including the lyricist, filmmaker and scholar Amit Khanna, and the journalist Bevinda Collaco. Bollywood does not exist as a physical place. Some deplore the name, arguing that it makes the industry look like a poor cousin to Hollywood.\n\n\"Raja Harishchandra\" (1913), by Dadasaheb Phalke, is known as the first silent feature film made in India. By the 1930s, the industry was producing over 200 films per annum. The first Indian sound film, Ardeshir Irani's \"Alam Ara\" (1931), was a major commercial success. There was clearly a huge market for talkies and musicals; Bollywood and all the regional film industries quickly switched to sound filming.\n\nThe 1930s and 1940s were tumultuous times: India was buffeted by the Great Depression, World War II, the Indian independence movement, and the violence of the Partition. Most Bollywood films were unabashedly escapist, but there were also a number of filmmakers who tackled tough social issues, or used the struggle for Indian independence as a backdrop for their plots.\n\nIn 1937, Ardeshir Irani, of \"Alam Ara\" fame, made the first colour film in Hindi, \"Kisan Kanya\". The next year, he made another colour film, a version of \"Mother India\". However, colour did not become a popular feature until the late 1950s. At this time, lavish romantic musicals and melodramas were the staple fare at the cinema.\n\nFollowing India's independence, the period from the late 1940s to the 1960s is regarded by film historians as the \"Golden Age\" of Hindi cinema. Some of the most critically acclaimed Hindi films of all time were produced during this period. Examples include \"Pyaasa\" (1957) and \"Kaagaz Ke Phool\" (1959) directed by Guru Dutt and written by Abrar Alvi, \"Awaara\" (1951) and \"Shree 420\" (1955) directed by Raj Kapoor and written by Khwaja Ahmad Abbas, and Dilip Kumar's \"Aan\" (1952). These films expressed social themes mainly dealing with working-class urban life in India; \"Awaara\" presented the city as both a nightmare and a dream, while \"Pyaasa\" critiqued the unreality of city life. Some of the most famous epic films of Hindi cinema were also produced at the time, including Mehboob Khan's \"Mother India\" (1957), which was nominated for the Academy Award for Best Foreign Language Film, and K. Asif's \"Mughal-e-Azam\" (1960). \"Madhumati\" (1958), directed by Bimal Roy and written by Ritwik Ghatak, popularised the theme of reincarnation in Western popular culture. \"Gunga Jumna\" (1961), written and produced by Dilip Kumar, was a crime drama about two brothers on opposite sides of the law, a theme that later became common in Indian films since the 1970s. Other acclaimed mainstream Hindi filmmakers at the time included Kamal Amrohi and Vijay Bhatt.\n\nSuccessful actors at the time included Dilip Kumar, Raj Kapoor, Dev Anand, and Guru Dutt, while successful actresses included Nargis, Vyjayanthimala, Meena Kumari, Nutan, Madhubala, Sadhana, Waheeda Rehman and Mala Sinha. The three most popular male Indian actors of the 1950s and 1960s were Dilip Kumar, Raj Kapoor, and Dev Anand, each with their own unique acting style. Kapoor followed the \"tramp\" style of Charlie Chaplin, Anand modelled himself after the \"suave\" style of Hollywood movie stars like Gregory Peck and Cary Grant, and Kumar pioneered a form of method acting that was similar to yet predated Hollywood method actors such as Marlon Brando. Kumar, who was described as \"the ultimate method actor\" by Satyajit Ray and is considered one of India's greatest actors, inspired future generations of Indian actors; much like Brando's influence on Robert De Niro and Al Pacino, Kumar had a similar influence on later Indian actors such as Amitabh Bachchan, Naseeruddin Shah, Shah Rukh Khan and Nawazuddin Siddiqui.\n\nWhile commercial Hindi cinema was thriving, the 1950s also saw the emergence of a new Parallel Cinema movement. Though the movement was mainly led by Bengali cinema, it also began gaining prominence in Hindi cinema. Early examples of Hindi films in this movement include \"Neecha Nagar\" (1946) directed by Chetan Anand and written by Khwaja Ahmad Abbas, and Bimal Roy's \"Do Bigha Zamin\" (1953). Their critical acclaim, as well as the latter's commercial success, paved the way for Indian neorealism and the \"Indian New Wave\". Some of the internationally acclaimed Hindi filmmakers involved in the movement included Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani, Shyam Benegal and Vijaya Mehta.\n\nEver since the social realist film \"Neecha Nagar\" won the Grand Prize at the first Cannes Film Festival, Hindi films were frequently in competition for the Palme d'Or at the Cannes Film Festival throughout the 1950s and early 1960s, with some of them winning major prizes at the festival. Guru Dutt, while overlooked in his own lifetime, had belatedly generated international recognition much later in the 1980s. Dutt is now regarded as one of the greatest Asian filmmakers of all time, alongside the more famous Indian Bengali filmmaker Satyajit Ray. The 2002 \"Sight & Sound\" critics' and directors' poll of greatest filmmakers ranked Dutt at No. 73 on the list. Some of his films are now included among the greatest films of all time, with \"Pyaasa\" (1957) being featured in Time magazine's \"All-TIME\" 100 best movies list, and with both \"Pyaasa\" and \"Kaagaz Ke Phool\" (1959) tied at No. 160 in the 2002 \"Sight & Sound\" critics' and directors' poll of all-time greatest films. Several other Hindi films from this era were also ranked in the \"Sight & Sound\" poll, including Raj Kapoor's \"Awaara\" (1951), Vijay Bhatt's \"Baiju Bawra\" (1952), Mehboob Khan's \"Mother India\" (1957) and K. Asif's \"Mughal-e-Azam\" (1960) all tied at No. 346 on the list.\n\nThe 1970s was when the name \"Bollywood\" was coined, and when the quintessential conventions of commercial Bollywood films were established. In the late 1960s and early 1970s, romance movies and action films starred actors like Shammi Kapoor, Jeetendra, Rajesh Khanna, Dharmendra, Sanjeev Kumar and Shashi Kapoor and actresses like Sharmila Tagore, Mumtaz, Saira Banu and Asha Parekh. In the mid-1970s, romantic confections made way for gritty, violent films about gangsters (see Indian mafia) and bandits. Amitabh Bachchan, the star known for his \"angry young man\" roles, rode the crest of this trend with actors like Feroz Khan, Mithun Chakraborty, Naseeruddin Shah, Jackie Shroff, Sanjay Dutt, Anil Kapoor and Sunny Deol, which lasted into the early 1990s. Actresses from this era included Hema Malini, Jaya Bachchan, Raakhee, Shabana Azmi, Zeenat Aman, Parveen Babi, Rekha, Dimple Kapadia, Smita Patil, Jaya Prada and Padmini Kolhapure.\n\nThe 1970s saw the emergence of the masala film genre, which combines elements of multiple genres (action, comedy, romance, drama, melodrama, musical). \"Yaadon Ki Baarat\" (1973), directed by Nasir Hussain and written by Salim-Javed, has been identified as the first masala film and the \"first\" quintessentially \"Bollywood\" film. The screenwriter duo Salim-Javed, consisting of Salim Khan and Javed Akhtar, went on to write more successful masala films in the 1970s and 1980s. A landmark for the masala film genre was \"Amar Akbar Anthony\" (1977), directed by Manmohan Desai and written by Kader Khan. Manmohan Desai went on to successfully exploit the genre in the 1970s and 1980s. The \"Curry Western\" trend that began with \"Sholay\" (1975), written by Salim-Javed, also falls under the masala film genre. During the 1970s, commercial Bollywood films drew from several foreign influences, including New Hollywood, Hong Kong martial arts cinema, and Italian exploitation films. Masala films launched Amitabh Bachchan into the biggest Bollywood movie star of the 1970s and 1980s.\n\nSome Hindi filmmakers such as Shyam Benegal continued to produce realistic Parallel Cinema throughout the 1970s, alongside Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani and Vijaya Mehta. However, the 'art film' bent of the Film Finance Corporation came under criticism during a Committee on Public Undertakings investigation in 1976, which accused the body of not doing enough to encourage commercial cinema. The 1970s thus saw the rise of commercial cinema in the form of enduring films such as \"Sholay\" (1975), which consolidated Amitabh Bachchan's position as a lead actor. The devotional classic \"Jai Santoshi Ma\" was also released in 1975. Another important film from 1975 was \"Deewar\", directed by Yash Chopra and written by Salim-Javed. A crime film pitting \"a policeman against his brother, a gang leader based on real-life smuggler Haji Mastan\", portrayed by Amitabh Bachchan; it was described as being \"absolutely key to Indian cinema\" by Danny Boyle. \"Deewar\" was also inspired by the 1961 Dilip Kumar film \"Gunga Jumna\". The most internationally acclaimed Hindi film of the 1980s was Mira Nair's \"Salaam Bombay!\" (1988), which won the Camera d'Or at the 1988 Cannes Film Festival and was nominated for the Academy Award for Best Foreign Language Film.\n\nThe period of Hindi cinema from the 1990s onwards is referred to as \"New Bollywood\" cinema. During the late 1980s and 1990s, the pendulum swung back toward family-centric romantic musicals with the success of such films as \"Qayamat Se Qayamat Tak\" (1988), \"Maine Pyar Kiya\" (1989), \"Chandni\" (1989), \"Hum Aapke Hain Kaun\" (1994), \"Dilwale Dulhania Le Jayenge\" (1995), \"Raja Hindustani\" (1996), \"Dil To Pagal Hai\" (1997), \"Pyaar To Hona Hi Tha\" (1998) and \"Kuch Kuch Hota Hai\" (1998). A new generation of popular actors emerged such as Aamir Khan, Ajay Devgan, Akshay Kumar, Salman Khan and Shahrukh Khan and actresses such as Madhuri Dixit, Sridevi, Juhi Chawla, Meenakshi Seshadri, Kajol and Karisma Kapoor. In that point of time, action films and comedy films were also successful, with actors like Govinda, Sunny Deol, Sunil Shetty, Akshay Kumar and Ajay Devgan with Akshay Kumar gaining popularity for performing dangerous stunts in action films in his well-known Khiladi (film series) and other action films. Actresses during this time included Raveena Tandon, Twinkle Khanna, Sonali Bendre, Sushmita Sen, Mahima Chaudhary and Shilpa Shetty. Furthermore, this decade marked the entry of new performers in arthouse and independent films, some of which succeeded commercially, the most influential example being \"Satya\" (1998), directed by Ram Gopal Varma and written by Anurag Kashyap. The critical and commercial success of \"Satya\" led to the emergence of a distinct genre known as \"Mumbai noir\", urban films reflecting social problems in the city of Mumbai. This led to a resurgence of Parallel Cinema by the end of the decade. These films often featured actors like Nana Patekar and Manoj Bajpai, and actresses like Manisha Koirala, Tabu, Pooja Bhatt and Urmila Matondkar, whose performances were usually critically acclaimed.\n\nSince the 1990s, the three biggest Bollywood movie stars have been the \"Three Khans\": Aamir Khan, Shah Rukh Khan, and Salman Khan. Combined, they have starred in most of the top ten highest-grossing Bollywood films. The three Khans have had successful careers since the late 1980s, and have dominated the Indian box office since the 1990s, across three decades. Shah Rukh Khan was the most successful Indian actor for most of the 1990s and 2000s, while Aamir Khan has been the most successful Indian actor since the late 2000s.\n\nThe 2000s saw a growth in Bollywood's recognition across the world due to a growing and prospering NRI community. A fast growth in the Indian economy and a demand for quality entertainment in this era, led the nation's film-making to new heights in terms of production values, cinematography and innovative story lines as well as technical advances in areas such as special effects and animation. Some of the largest production houses, among them Yash Raj Films and Dharma Productions were the producers of new modern films. Some popular films of the decade were \"Kaho Naa... Pyaar Hai\" (2000), \"\" (2001), \"Lagaan\" (2001), \"Koi... Mil Gaya\" (2003), \"Munna Bhai M.B.B.S.\" (2003), \"Rang De Basanti\" (2006), \"Lage Raho Munna Bhai\" (2006), \"Dhoom 2\" (2006), \"Krrish\" (2006) and \"Jab We Met\" (2007) among others. This decade also saw the rise of popular actors and movie stars like Hrithik Roshan, Abhishek Bachchan, Vivek Oberoi, Shahid Kapoor and John Abraham, as well as actresses like Aishwarya Rai, Rani Mukerji, Preity Zinta, Ameesha Patel, Lara Dutta, Bipasha Basu, Kareena Kapoor, Priyanka Chopra and Katrina Kaif.\n\nIn the 2010s, the industry saw the trend of established movie stars like Salman Khan, Akshay Kumar and Shahrukh Khan making big-budget masala entertainers like \"Dabangg\" (2010), \"Ek Tha Tiger\" (2012), \"Rowdy Rathore\" (2012), \"Chennai Express\" (2013), \"Kick\" (2014) and \"Happy New Year\" (2014) opposite much younger actresses. These films were often not the subject of critical acclaim, but were nonetheless major commercial successes. While most stars from the 2000s continued their successful careers into the next decade, the 2010s also saw the rise of a new generation of popular actors like Ranbir Kapoor, Ranveer Singh, Varun Dhawan, Sidharth Malhotra, Sushant Singh Rajput, Arjun Kapoor, Aditya Roy Kapur and Tiger Shroff, as well as actresses like Vidya Balan, Kangana Ranaut, Deepika Padukone, Sonam Kapoor, Anushka Sharma, Sonakshi Sinha, Jacqueline Fernandez, Shraddha Kapoor and Alia Bhatt, with Balan and Ranaut gaining wide recognition for successful female-centric films such as \"The Dirty Picture\" (2011), \"Kahaani\" (2012) and \"Queen\" (2014), \"Tanu Weds Manu Returns\" (2015). While Kareena Kapoor and Bipasha Basu are few of the working actresses from the 2000's who completed successful 15 years in the industry.\n\nGokulsing and Dissanayake identify six major influences that have shaped the conventions of Indian popular cinema:\n\nPerhaps the biggest influence of Bollywood has been on nationalism in India itself, where along with rest of Indian cinema, it has become part and parcel of the 'Indian story'. In the words of the economist and Bollywood biographer Lord Meghnad Desai,\nCinema actually has been the most vibrant medium for telling India its own story, the story of its struggle for independence, its constant struggle to achieve national integration and to emerge as a global presence.\nIn the 2000s, Bollywood began influencing musical films in the Western world, and played a particularly instrumental role in the revival of the American musical film genre. Baz Luhrmann stated that his musical film \"Moulin Rouge!\" (2001) was directly inspired by Bollywood musicals. The film incorporated an Indian-themed play based on the ancient Sanskrit drama \"Mṛcchakatika\" and a Bollywood-style dance sequence with a song from the film \"China Gate\". The critical and financial success of \"Moulin Rouge!\" renewed interest in the then-moribund Western musical genre, and subsequently films such as \"Chicago, The Producers, Rent\", \"Dreamgirls\", \"Hairspray\", \"\", \"Across the Universe\", \"The Phantom of the Opera\", \"Enchanted\" and \"Mamma Mia!\" were produced, fuelling a renaissance of the genre.\n\nA. R. Rahman, an Indian film composer, wrote the music for Andrew Lloyd Webber's \"Bombay Dreams\", and a musical version of \"Hum Aapke Hain Koun\" has played in London's West End. The Bollywood musical \"Lagaan\" (2001) was nominated for the Academy Award for Best Foreign Language Film, and two other Bollywood films \"Devdas\" (2002) and \"Rang De Basanti\" (2006) were nominated for the BAFTA Award for Best Film Not in the English Language. Danny Boyle's \"Slumdog Millionaire\" (2008), which has won four Golden Globes and eight Academy Awards, was also directly inspired by Bollywood films, and is considered to be a \"homage to Hindi commercial cinema\". The theme of reincarnation was also popularised in Western popular culture through Bollywood films, with \"Madhumati\" (1958) inspiring the Hollywood film \"The Reincarnation of Peter Proud\" (1975), which in turn inspired the Bollywood film \"Karz\" (1980), which in turn influenced another Hollywood film \"Chances Are\" (1989). The 1975 film \"Chhoti Si Baat\" is believed to have inspired \"Hitch\" (2005), which in turn inspired the Bollywood film \"Partner\" (2007).\n\nThe influence of Bollywood \"filmi\" music can also be seen in popular music elsewhere in the world. In 1978, technopop pioneers Haruomi Hosono and Ryuichi Sakamoto of the Yellow Magic Orchestra produced an electronic album \"Cochin Moon\" based on an experimental fusion between electronic music and Bollywood-inspired Indian music. Devo's 1988 hit song \"Disco Dancer\" was inspired by the song \"I am a Disco Dancer\" from the Bollywood film \"Disco Dancer\" (1982). The 2002 song \"Addictive\", sung by Truth Hurts and produced by DJ Quik and Dr. Dre, was lifted from Lata Mangeshkar's \"Thoda Resham Lagta Hai\" from \"Jyoti\" (1981). The Black Eyed Peas' Grammy Award winning 2005 song \"Don't Phunk with My Heart\" was inspired by two 1970s Bollywood songs: \"Ye Mera Dil Yaar Ka Diwana\" from \"Don\" (1978) and \"Ae Nujawan Hai Sub\" from \"Apradh\" (1972). Both songs were originally composed by Kalyanji Anandji, sung by Asha Bhosle, and featured the dancer Helen. Also in 2005, the Kronos Quartet re-recorded several R. D. Burman compositions, with Asha Bhosle as the singer, into an album \"You've Stolen My Heart: Songs from R.D. Burman's Bollywood\", which was nominated for \"Best Contemporary World Music Album\" at the 2006 Grammy Awards. \"Filmi\" music composed by A. R. Rahman (who would later win two Academy Awards for the \"Slumdog Millionaire\" soundtrack) has frequently been sampled by musicians elsewhere in the world, including the Singaporean artist Kelly Poon, the Uzbek artist Iroda Dilroz, the French rap group La Caution, the American artist Ciara, and the German band Löwenherz, among others. Many Asian Underground artists, particularly those among the overseas Indian diaspora, have also been inspired by Bollywood music.\n\nBollywood films are mostly musicals and are expected to contain catchy music in the form of song-and-dance numbers woven into the script. A film's success often depends on the quality of such musical numbers. Indeed, a film's music is often released before the movie and helps increase the audience.\n\nIndian audiences expect full value for their money, with a good entertainer generally referred to as \"paisa\" \"vasool\", (literally, \"money's worth\"). Songs and dances, love triangles, comedy and dare-devil thrills are all mixed up in a three-hour extravaganza with an intermission. They are called \"Masala films\", after the Hindi word for a spice mixture. Like \"masalas\", these movies are a mixture of many things such as action, comedy, romance and so on. Most films have heroes who are able to fight off villains all by themselves.\n\nBollywood plots have tended to be melodramatic. They frequently employ formulaic ingredients such as star-crossed lovers and angry parents, love triangles, family ties, sacrifice, corrupt politicians, kidnappers, conniving villains, courtesans with hearts of gold, long-lost relatives and siblings separated by fate, dramatic reversals of fortune, and convenient coincidences.\n\nThere have always been Indian films with more artistic aims and more sophisticated stories, both inside and outside the Bollywood tradition (see Parallel Cinema). They often lost out at the box office to movies with more mass appeal. Bollywood conventions are changing, however. A large Indian diaspora in English-speaking countries, and increased Western influence at home, have nudged Bollywood films closer to Hollywood models.\n\nFilm critic Lata Khubchandani writes, \"our earliest films ... had liberal doses of sex and kissing scenes in them. Strangely, it was after Independence the censor board came into being and so did all the strictures.\" Plots now tend to feature Westernised urbanites dating and dancing in clubs rather than centring on pre-arranged marriages. Though these changes can widely be seen in contemporary Bollywood, traditional conservative ways of Indian culture continue to exist in India outside the industry and an element of resistance by some to western-based influences. Despite this, Bollywood continues to play a major role in fashion in India. Some studies into fashion in India have revealed that some people are unaware that the changing nature of fashion in Bollywood films are often influenced by globalisation; many consider the clothes worn by Bollywood actors as authentically Indian.\n\nBollywood employs people from all parts of India. It attracts thousands of aspiring actors and actresses, all hoping for a break in the industry. Models and beauty contestants, television actors, theatre actors and even common people come to Mumbai with the hope and dream of becoming a star. Just as in Hollywood, very few succeed. Since many Bollywood films are shot abroad, many foreign extras are employed too.\n\nVery few non-Indian actors are able to make a mark in Bollywood, though many have tried from time to time. There have been some exceptions, of which one recent example is the hit film \"Rang De Basanti\", where the lead actress is Alice Patten, an Englishwoman. \"Kisna\", \"Lagaan\", and \"\" also featured foreign actors. Of late, Emma Brown Garett, an Australian born actress, has starred in a few Indian films.\n\nBollywood can be very clannish, and the relatives of film-industry insiders have an edge in getting coveted roles in films or being part of a film's crew. However, industry connections are no guarantee of a long career: competition is fierce and if film industry scions do not succeed at the box office, their careers will falter. Some of the biggest stars, such as Dilip Kumar, Amitabh Bachchan, Rajesh Khanna, Dharmendra, Madhuri Dixit, Shah Rukh Khan and Akshay Kumar have succeeded despite a lack of any show business connections. For film clans, see List of Hindi film clans.\n\nSound in Bollywood films was once rarely recorded on location (otherwise known as sync sound). Therefore, the sound was usually created (or re-created) entirely in the studio, with the actors reciting their lines as their images appear on-screen in the studio in the process known as \"looping in the sound\" or ADR—with the foley and sound effects added later. This created several problems, since the sound in these films usually occurs a frame or two earlier or later than the mouth movements or gestures. The actors had to act twice: once on-location, once in the studio—and the emotional level on set is often very difficult to re-create. Commercial Indian films, not just the Hindi-language variety, are known for their lack of ambient sound, so there is a silence underlying everything instead of the background sound and noises usually employed in films to create aurally perceivable depth and environment.\n\nThe ubiquity of ADR in Bollywood cinema became prevalent in the early 1960s with the arrival of the Arriflex 3 camera, which required a blimp (cover) to shield the sound of the camera, for which it was notorious, from on-location filming. Commercial Indian filmmakers, known for their speed, never bothered to blimp the camera, and its excessive noise required that everything had to be re-created in the studio. Eventually, this became the standard for Indian films.\n\nThe trend was bucked in 2001, after a 30-year hiatus of synchronised sound, with the film \"Lagaan\", in which the sound was done on the location. This opened up a heated debate on the use and economic feasibility of on-location sound, and several Bollywood films have employed on-location sound since then.\n\nIn 1955 the Bollywood group Cine Costume Make-Up Artist & Hair Dressers' Association (CCMAA) created a rule that did not allow women to obtain memberships as makeup artists. However, in 2014 the Supreme Court of India ruled that this rule was in violation of the Indian constitutional guarantees granted under Article 14 (right to equality), 19(1)(g) (freedom to carry out any profession) and Article 21 (right to liberty). The judges of the Supreme Court of India stated that the ban on women makeup artist members had no \"rationale nexus\" to the cause sought to be achieved and was \"unacceptable, impermissible and inconsistent\" with the constitutional rights guaranteed to the citizens. The Court also found illegal the rule which mandated that for any artist, female or male, to work in the industry, they must have domicile status of five years in the state where they intend to work. In 2015 it was announced that Charu Khurana had become the first woman to be registered by the Cine Costume Make-Up Artist & Hair Dressers' Association.\n\nBollywood film music is called filmi music (from Hindi, meaning \"of films\"). Songs from Bollywood movies are generally pre-recorded by professional playback singers, with the actors then lip synching the words to the song on-screen, often while dancing. While most actors, especially today, are excellent dancers, few are also singers. One notable exception was Kishore Kumar, who starred in several major films in the 1950s while also having a stellar career as a playback singer. K. L. Saigal, Suraiyya, and Noor Jehan were also known as both singers and actors. Some actors in the last thirty years have sung one or more songs themselves; for a list, see Singing actors and actresses in Indian cinema.\n\nSongs are what make and break the movie; they determine if it is going to be a flop or a hit: \"Few films without successful musical tracks, and even fewer without any songs and dances, succeed\" With the increase of globalization, there has also been a change in the type of music that Bollywood films entail; the lyrics of the songs have increasingly been a mix of Hindi and English languages, as opposed to the strict Hindi prior to Globalization. Also, with the inspiration of global trends, such as Salsa, Pop and Hip Hop, there has been a modification of the type of music heard in Bollywood films.\n\nPlayback singers are prominently featured in the opening credits and have their own fans who will go to an otherwise lackluster movie just to hear their favourites. Going by the quality as well as the quantity of the songs they rendered, most notable singers of Bollywood are Lata Mangeshkar, Asha Bhosle, Geeta Dutt, Shamshad Begum, Kavita Krishnamurthy, Sadhana Sargam and Alka Yagnik among female playback singers; and K. L. Saigal, Talat Mahmood, Mukesh, Mohammed Rafi, Manna Dey, Hemant Kumar, Kishore Kumar, Kumar Sanu, Udit Narayan and Sonu Nigam among male playback singers. Kishore Kumar and Mohammed Rafi are often considered arguably the finest of the singers that have lent their voice to Bollywood songs, followed by Lata Mangeshkar, who, through the course of a career spanning over six decades, has recorded thousands of songs for Indian movies. The composers of film music, known as music directors, are also well-known. Their songs can make or break a film and usually do. Remixing of film songs with modern beats and rhythms is a common occurrence today, and producers may even release remixed versions of some of their films' songs along with the films' regular soundtrack albums.\n\nThe dancing in Bollywood films, especially older ones, is primarily modelled on Indian dance: classical dance styles, dances of historic northern Indian courtesans (tawaif), or folk dances. In modern films, Indian dance elements often blend with Western dance styles (as seen on MTV or in Broadway musicals), though it is usual to see Western pop \"and\" pure classical dance numbers side by side in the same film. The hero or heroine will often perform with a troupe of supporting dancers. Many song-and-dance routines in Indian films feature unrealistically instantaneous shifts of location or changes of costume between verses of a song. If the hero and heroine dance and sing a duet, it is often staged in beautiful natural surroundings or architecturally grand settings. This staging is referred to as a \"picturisation\".\n\nSongs typically comment on the action taking place in the movie, in several ways. Sometimes, a song is worked into the plot, so that a character has a reason to sing. Other times, a song is an externalisation of a character's thoughts, or presages an event that has not occurred yet in the plot of the movie. In this case, the event is often two characters falling in love. The songs are also often referred to as a \"dream sequence\", and anything can happen that would not normally happen in the real world.\n\nPreviously song and dance scenes often used to be shot in Kashmir, but due to political unrest in Kashmir since the end of the 1980s, those scenes have since then often been shot in Western Europe, particularly in Switzerland and Austria.\n\nRenowned contemporary Bollywood dancers include Madhuri Dixit, Hrithik Roshan, Aishwarya Rai Bachchan, Sridevi, Meenakshi Seshadri, Malaika Arora Khan, Shahid Kapoor and Tiger Shroff. Older Bollywood dancers are people such as Helen, known for her cabaret numbers, Madhubala, Vyjanthimala, Padmini, Hema Malini, Mumtaz, Cuckoo Moray, Parveen Babi, Waheeda Rahman, Meena Kumari, and Shammi Kapoor.\n\nFor the last few decades Bollywood producers have been releasing the film's soundtrack, as tapes or CDs, before the main movie release, hoping that the music will pull audiences into the cinema later. Often the soundtrack is more popular than the movie. In the last few years some producers have also been releasing music videos, usually featuring a song from the film. However, some promotional videos feature a song which is not included in the movie.\n\nThe film script or lines of dialogue (called \"dialogues\" in Indian English) and the song lyrics are often written by different people.\n\nDialogues are usually written in an unadorned Hindi that would be understood by the largest possible audience. Some movies, however, have used regional dialects to evoke a village setting, or old-fashioned, courtly, formal Urdu in medieval era historical films. Jyotika Virdi, in her book \"The cinematic imagiNation\" , wrote about the presence of Urdu in Hindi films: \"Urdu is often used in film titles, screenplay, lyrics, the language of love, war, and martyrdom.\" However, she further discussed its decline over the years: \"The extent of Urdu used in commercial Hindi cinema has not been stable ... the decline of Urdu is mirrored in Hindi films ... It is true that many Urdu words have survived and have become part of Hindi cinema's popular vocabulary. But that is as far as it goes.\" Contemporary mainstream movies also make great use of English. According to \"Bollywood Audiences Editorial\", \"English has begun to challenge the ideological work done by Urdu.\" Some movie scripts are first written in Latin script. Characters may shift from one language to the other to express a certain atmosphere (for example, English in a business setting and Hindi in an informal one).\n\nCinematic language, whether in dialogues or lyrics, is often melodramatic and invokes God, family, mother, duty, and self-sacrifice liberally. Song lyrics are often about love. Bollywood song lyrics, especially in the old movies, frequently use the poetic vocabulary of court Urdu, with many Persian loanwords. Another source for love lyrics is the long Hindu tradition of poetry about the amours of Krishna, Radha, and the gopis, as referenced in films such as \"Jhanak Jhanak Payal Baje\" and \"Lagaan\".\n\nMusic directors often prefer working with certain lyricists, to the point that the lyricist and composer are seen as a team. This phenomenon is compared to the pairings of American composers and songwriters that created old-time Broadway musicals.\n\nBollywood films are multi-million dollar productions, with the most expensive productions costing up to 1 billion rupees (roughly USD 20 million). The latest Science fiction movie \"Ra.One\" was made at an immense budget of 1.35 billion (roughly USD 27 million), making it the most expensive movie ever produced in Bollywood. Sets, costumes, special effects, and cinematography were less than world-class up until the mid-to-late 1990s, although with some notable exceptions. As Western films and television gain wider distribution in India itself, there is an increasing pressure for Bollywood films to attain the same production levels, particularly in areas such as action and special effects. Recent Bollywood films have employed international technicians to improve in these areas, such as \"Krrish\" (2006) which has action choreographed by Hong Kong based Tony Ching. The increasing accessibility to professional action and special effects, coupled with rising film budgets, has seen an explosion in the action and sci-fi genres.\n\nSequences shot overseas have proved a real box office draw, so Mumbai film crews are increasingly filming in Australia, Canada, New Zealand, the United Kingdom, the United States, continental Europe and elsewhere. Nowadays, Indian producers are winning more and more funding for big-budget films shot within India as well, such as \"Lagaan\", \"Devdas\" and other recent films.\n\nFunding for Bollywood films often comes from private distributors and a few large studios. Indian banks and financial institutions were forbidden from lending money to movie studios. However, this ban has now been lifted. As finances are not regulated, some funding also comes from illegitimate sources, such as the Mumbai underworld. The Mumbai underworld has been known to be involved in the production of several films, and are notorious for patronising several prominent film personalities. On occasion, they have been known to use money and muscle power to get their way in cinematic deals. In January 2000, Mumbai mafia hitmen shot Rakesh Roshan, a film director and father of star Hrithik Roshan. In 2001, the Central Bureau of Investigation seized all prints of the movie \"Chori Chori Chupke Chupke\" after the movie was found to be funded by members of the Mumbai underworld.\n\nAnother problem facing Bollywood is widespread copyright infringement of its films. Often, bootleg DVD copies of movies are available before the prints are officially released in cinemas. Manufacturing of bootleg DVD, VCD, and VHS copies of the latest movie titles is a well established 'small scale industry' in parts of South Asia and South East Asia. The Federation of Indian Chambers of Commerce and Industry (FICCI) estimates that the Bollywood industry loses $100 million annually in loss of revenue from unlicensed home videos and DVDs. Besides catering to the homegrown market, demand for these copies is large amongst some sections of the Indian diaspora, too. (In fact, bootleg copies are the only way people in Pakistan can watch Bollywood movies, since the Government of Pakistan has banned their sale, distribution and telecast). Films are frequently broadcast without compensation by countless small cable TV companies in India and other parts of South Asia. Small convenience stores run by members of the Indian diaspora in the US and the UK regularly stock tapes and DVDs of dubious provenance, while consumer copying adds to the problem. The availability of illegal copies of movies on the Internet also contributes to the industry's losses.\n\nSatellite TV, television and imported foreign films are making huge inroads into the domestic Indian entertainment market. In the past, most Bollywood films could make money; now fewer tend to do so. However, most Bollywood producers make money, recouping their investments from many sources of revenue, including selling ancillary rights. There are also increasing returns from theatres in Western countries like the United Kingdom, Canada, and the United States, where Bollywood is slowly getting noticed. As more Indians migrate to these countries, they form a growing market for upscale Indian films.\n\nFor a comparison of Hollywood and Bollywood financial figures, see chart. It shows tickets sold in 2002 and total revenue estimates. Bollywood sold 3.6 billion tickets and had total revenues (theatre tickets, DVDs, television and so on.) of US$1.3 billion, whereas Hollywood films sold 2.6 billion tickets and generated total revenues (again from all formats) of US$51 billion.\n\nMany Indian artists used to make a living by hand-painting movie billboards and posters (The well-known artist M.F. Hussain used to paint film posters early in his career). This was because human labour was found to be cheaper than printing and distributing publicity material. Now, a majority of the huge and ubiquitous billboards in India's major cities are created with computer-printed vinyl. The old hand-painted posters, once regarded as ephemera, are becoming increasingly collectible as folk art.\n\nReleasing the film music, or music videos, before the actual release of the film can also be considered a form of advertising. A popular tune is believed to help pull audiences into the theatres.\n\nBollywood publicists have begun to use the Internet as a venue for advertising. Most of the better-funded film releases now have their own websites, where browsers can view trailers, stills, and information about the story, cast, and crew.\n\nBollywood is also used to advertise other products. Product placement, as used in Hollywood, is widely practised in Bollywood.\n\nBollywood movie stars appear in print and television advertisements for other products, such as watches or soap (see Celebrity endorsement). Advertisers say that a star endorsement boosts sales.\n\nWith the increasing prominence of international setting such as Switzerland, London, Paris, New York, Singapore and so on, it does not entail that the people and cultures residing in these exotic settings are represented. Contrary to these spaces and geographies being filmed as they are, they are actually Indianized by adding Bollywood actors and Hindi speaking extras to them. While immersing in Bollywood films, viewers get to see their local experiences duplicated in different locations around the world.\n\nRao states that \"Media representation can depict India's shifting relation with the world economy, but must retain its 'Indianness' in moments of dynamic hybridity\", where \"Indianness\" refers to the cultural identity and political affiliation. With Bollywood's popularity among diasporic audiences, \"Indianness\" poses a problem, but at the same time, it gives back to its homeland audience, a sense of uniqueness from other immigrant groups.\n\nThe Filmfare Awards ceremony is one of the most prominent film events given for Hindi films in India. The Indian screen magazine \"Filmfare\" started the first Filmfare Awards in 1954, and awards were given to the best films of 1953. The ceremony was referred to as the \"Clare Awards\" after the magazine's editor. Modelled after the poll-based merit format of the Academy of Motion Picture Arts and Sciences, individuals may submit their votes in separate categories. A dual voting system was developed in 1956. The Filmfare awards are frequently accused of bias towards commercial success rather than artistic merit.\n\nThe National Film Awards were introduced in 1954. Since 1973, the Indian government has sponsored the National Film Awards, awarded by the government run Directorate of Film Festivals (DFF). The DFF screens not only Bollywood films, but films from all the other regional movie industries and independent/art films. These awards are handed out at an annual ceremony presided over by the President of India. Under this system, in contrast to the National Film Awards, which are decided by a panel appointed by Indian Government, the Filmfare Awards are voted for by both the public and a committee of experts.\n\nNotable private awards ceremonies for Hindi films, held within India are:\n\nNotable private awards ceremonies for Hindi films, held overseas are:\n\nMost of these award ceremonies are lavishly staged spectacles, featuring singing, dancing, and numerous celebrities.\n\n\nBesides being popular among the India diaspora, such far off locations as Nigeria to Egypt to Senegal and to Russia generations of non-Indian fans have grown up with Bollywood during the years, bearing witness to the cross-cultural appeal of Indian movies. Indian cinema's early contacts with other regions became visible with its films making early inroads into the Soviet Union, Middle East, Southeast Asia, and China.\n\nOver the last years of the 20th century and beyond, Bollywood progressed in its popularity as it entered the consciousness of Western audiences and producers, with Western actors now actively seeking roles in Bollywood movies.\n\nHistorically, Hindi films have been distributed to some parts of Africa, largely by Lebanese businessmen. \"Mother India\" (1957), for example, continued to be played in Nigeria decades after its release. Indian movies have also gained ground so as to alter the style of Hausa fashions, songs have also been copied by Hausa singers and stories have influenced the writings of Nigerian novelists. Stickers of Indian films and stars decorate taxis and buses in Northern Nigeria, while posters of Indian films adorn the walls of tailor shops and mechanics' garages in the country. Unlike in Europe and North America where Indian films largely cater to the expatriate Indian market yearning to keep in touch with their homeland, in West Africa, as in many other parts of the world, such movies rose in popularity despite the lack of a significant Indian audience, where movies are about an alien culture, based on a religion wholly different, and, for the most part, a language that is unintelligible to the viewers. One such explanation for this lies in the similarities between the two cultures. Other similarities include wearing turbans; the presence of animals in markets; porters carrying large bundles, chewing sugar cane; youths riding Bajaj motor scooters; wedding celebrations, and so forth. With the strict Muslim culture, Indian movies were said to show \"respect\" toward women, where Hollywood movies were seen to have \"no shame\". In Indian movies women were modestly dressed, men and women rarely kiss, and there is no nudity, thus Indian movies are said to \"have culture\" that Hollywood films lack. The latter choice was a failure because \"they don't base themselves on the problems of the people,\" where the former is based socialist values and on the reality of developing countries emerging from years of colonialism. Indian movies also allowed for a new youth culture to follow without such ideological baggage as \"becoming western.\" The first ever movie to be shot in Mauritius was Souten starring Rajesh Khanna in 1983.\n\nSeveral Bollywood personalities have avenued to the continent for both shooting movies and off-camera projects. The film \"Padmashree Laloo Prasad Yadav\" (2005) was one of many movies shot in South Africa. \"Dil Jo Bhi Kahey\" (2005) was shot almost entirely in Mauritius, which has a large ethnically Indian population.\n\nOminously, however, the popularity of old Bollywood versus a new, changing Bollywood seems to be diminishing the popularity on the continent. The changing style of Bollywood has begun to question such an acceptance. The new era features more sexually explicit and violent films. Nigerian viewers, for example, commented that older films of the 1950s and 1960s had culture to the newer, more westernised picturisations. The old days of India avidly \"advocating decolonization ... and India's policy was wholly influenced by his missionary zeal to end racial domination and discrimination in the African territories\" were replaced by newer realities. The emergence of Nollywood, Africa's local movie industry has also contributed to the declining popularity of Bollywood films. A greater globalised world worked in tandem with the sexualisation of Indian films so as to become more like American films, thus negating the preferred values of an old Bollywood and diminishing Indian soft power.\n\nAdditionally, classic Bollywood actors like Kishore Kumar and Amitabh Bachchan have historically enjoyed popularity in Egypt and Somalia. In Ethiopia, Bollywood movies are shown alongside Hollywood productions in Piazza theatres, such as the Cinema Ethiopia in Addis Ababa. In the other countries of North Africa, Bollywood films are also broadcast, though local aesthetics tend much more toward expressive or auteur cinema than commercial fare.\n\nBollywood films are widely watched in other South Asian countries, including Afghanistan, Bangladesh, Nepal, Pakistan and Sri Lanka.\n\nMany Pakistanis watch Bollywood films, as they understand Hindi (due to its linguistic similarity to Urdu). Pakistan banned the legal import of Bollywood movies in 1965. However, trade in unlicensed DVDs and illegal cable broadcasts ensured the continued popularity of Bollywood releases in Pakistan. Exceptions were made for a few films, such as the 2006 colorised re-release of the classic \"Mughal-e-Azam\" or the 2006 film \"Taj Mahal\". Early in 2008, the Pakistani government eased the ban and allowed the import of even more movies; 16 were screened in 2008. Continued easing followed in 2009 and 2010. The new policy is opposed by nationalists and representatives of Pakistan's small film industry but is embraced by cinema owners, who are making profits after years of low receipts.\nBollywood movies are so much popular in Nepal that, Bollywood movies earn more than Nepali movies. Actors like Salman Khan, Akshay Kumar and Shah Rukh Khan are most popular in Nepal and their movies sees the audience full pack all over the Cinema halls and also are so popular in Afghanistan due to the country's proximity to the Indian subcontinent and cultural perspectives present in the movies. A number of Bollywood movies were filmed inside Afghanistan while some dealt with the country, including \"Dharmatma\", \"Kabul Express\", \"Khuda Gawah\" and \"Escape From Taliban\".\n\nBollywood films are widely appreciated in East Asian countries such as China, Japan, and South Korea. Some Hindi movies had success in the China and South Korea, Japan in the 1940s and 1950s and are popular till today. The most popular Hindi films in that country were \"Dr. Kotnis Ki Amar Kahani\" (1946), \"Awaara\" (1951) and \"Do Bigha Zamin\" (1953). Raj Kapoor was a famous movie star in China, and the song \"Awara Hoon\" (\"I am a Tramp\") was popular in the country. Since then, Hindi films significantly declined in popularity in China, until the Academy Award nominated \"Lagaan\" (2001) became the first Indian film to have a nationwide release there in decades. The Chinese filmmaker He Ping was impressed by \"Lagaan\", especially its soundtrack, and thus hired the film's music composer A. R. Rahman to score the soundtrack for his film \"Warriors of Heaven and Earth\" (2003). Several older Hindi films have a cult following in Japan, particularly the films directed by Guru Dutt. Bollywood films are also popular in Southeast Asia (particularly in Maritime Southeast Asia)\n\nAamir Khan has been credited with opening up the Chinese market for Indian films. When \"3 Idiots\" (2009) released in China, the country was only the 15th largest film market, partly due to China's widespread pirate DVD distribution at the time. However, it was the pirate market that introduced \"3 Idiots\" to most Chinese audiences, becoming a cult hit in the country. It became China's 12th favourite film of all time, according to ratings on Chinese film review site Douban, with only one domestic Chinese film (\"Farewell My Concubine\") ranked higher. Aamir Khan gained a large growing Chinese fanbase as a result. By 2013, China grew to become the world's second largest film market (after the United States), paving the way for Aamir Khan's Chinese box office success, with \"Dhoom 3\" (2013), \"PK\" (2014), and \"Dangal\" (2016), which became the 16th highest-grossing film in China and the fifth highest-grossing non-English language film worldwide.\n\nHindi films have been popular in Arab countries, including Palestine, Jordan, Egypt and the Gulf countries.\nImported Indian films are usually subtitled in Arabic upon the film's release. Since the early 2000s, Bollywood has progressed in Israel. Special channels dedicated to Indian films have been displayed on cable television. There are channels such as MBC Bollywood and Zee Aflam, which show Hindi movies and serials. In Egypt, Bollywood movies used to be massively popular in the 1970's and 1980's. In 1987 however, Bollywood films were restricted to only a handful of films by the Egyptian Government. Bollywood movies are regularly screened in Dubai cinemas because of the high demand. Recently in Turkey, Bollywood has been gaining popularity as Barfi! was the first Hindi film to have a wide theatrical release. Bollywood also has viewership in Central Asia (particularly in Uzbekistan and Tajikistan).\n\nThe awareness of Hindi cinema is substantial in the United Kingdom, where they frequently enter the UK top ten. Many films, such as \"Kabhi Khushi Kabhie Gham\" (2001) have been set in London. Bollywood is also appreciated in France, Germany, the Netherlands, and the Scandinavian countries. Various Bollywood movies are dubbed in German and shown on the German television channel RTL II on a regular basis.\n\nBollywood films are particularly popular in the former Soviet Union. Bollywood films have been dubbed into Russian, and shown in prominent theatres such as Mosfilm and Lenfilm.\n\nIndian films were popular in the Soviet Union, more so than Hollywood films and occasionally even domestic Soviet films. From 1954 to 1991, 206 Indian films (175 of which were Bollywood films) were imported in the Soviet Union, drawing higher average audience figures than domestic Soviet productions, with 50 Indian films drawing more than 20 million viewers (compared to 41 Hollywood films), with some such as \"Awaara\" (1951) and \"Disco Dancer\" (1982) drawing more than 60 million viewers, establishing Indian actors like Raj Kapoor, Nargis, Rishi Kapoor and Mithun Chakraborty as household names in the country.\n\nAshok Sharma, Indian Ambassador to Suriname, who has served three times in the Commonwealth of Independent States region during his diplomatic career said:\nThe film \"Mera Naam Joker\" (1970), sought to cater to such an appeal and the popularity of Raj Kapoor in Russia, when it recruited Russian actress Kseniya Ryabinkina for the movie. In the contemporary era, \"\" (2005) was shot entirely in Russia. After the collapse of the Soviet film distribution system, Hollywood occupied the void created in the Russian film market. This made things difficult for Bollywood as it was losing market share to Hollywood. However, Russian newspapers report that there is a renewed interest in Bollywood among young Russians.\n\nBollywood has experienced a marked growth in revenue in Canada and the United States, particularly popular amongst the South Asian communities in large cities, such as Toronto, Chicago, and New York City. Yash Raj Films, one of India's largest production houses and distributors, reported in September 2005 that Bollywood films in the United States earn around $100 million a year through theatre screenings, video sales and the sale of movie soundtracks. In other words, films from India do more business in the United States than films from any other non-English speaking country. Numerous films in the mid-1990s and onwards have been largely, or entirely, shot in New York, Los Angeles, Vancouver and Toronto. Bollywood's immersion in the traditional Hollywood domain was further tied with such films as \"The Guru\" (2002) and \"\" (2007) trying to popularise the Bollywood-theme for Hollywood.\n\nBollywood is not as successful in the Oceanic countries and Pacific Islands such as New Guinea. However, it ranks second to Hollywood in countries such as Fiji, with its large Indian minority, Australia and New Zealand.\n\nAustralia is one of the countries where there is a large South Asian Diaspora. Bollywood is popular amongst non-Asians in the country as well. Since 1997 the country has provided a backdrop for an increasing number of Bollywood films. Indian filmmakers have been attracted to Australia's diverse locations and landscapes, and initially used it as the setting for song-and-dance sequences, which demonstrated the contrast between the values. However, nowadays, Australian locations are becoming more important to the plot of Bollywood films. Hindi films shot in Australia usually incorporate aspects of Australian lifestyle. The Yash Raj Film \"Salaam Namaste\" (2005) became the first Indian film to be shot entirely in Australia and was the most successful Bollywood film of 2005 in the country. This was followed by \"Heyy Babyy\" (2007) \"Chak De! India\" (2007) and \"Singh Is Kinng\" (2008) which turned out to be box office successes. Following the release of \"Salaam Namaste\", on a visit to India the then prime minister John Howard also sought, having seen the film, to have more Indian movies shooting in the country to boost tourism, where the Bollywood and cricket nexus, was further tightened with Steve Waugh's appointment as tourism ambassador to India. Australian actress Tania Zaetta, who co-starred in \"Salaam Namaste\", among other Bollywood films, expressed her keenness to expand her career in Bollywood.\n\nBollywood movies are not influential in many countries of South America, though Bollywood culture and dance is recognised. However, due to significant South Asian diasporic communities in Suriname and Guyana, Hindi-language movies are popular. In 2006, \"Dhoom 2\" became the first Bollywood film to be shot in Rio de Janeiro, Brazil.\n\nIn January 2012, it was announced that UTV Motion Pictures would be releasing movies in Peru, starting with \"Guzaarish\".\n\nConstrained by rushed production schedules and small budgets, some Bollywood writers and musicians have been known to resort to plagiarism. Ideas, plot lines, tunes or riffs have been copied from other Indian film industries or foreign films (including Hollywood and other Asian films) without acknowledgement of the original source. This has led to criticism towards the film industry.\n\nBefore the 1990s, this could be done with impunity. Copyright enforcement was lax in India and few actors or directors ever saw an official contract. The Hindi film industry was not widely known to non-Indian audiences (excluding the Soviet states), who would not even be aware that their material was being copied. Audiences may also not have been aware of the plagiarism since many audiences in India were unfamiliar with foreign films and music. While copyright enforcement in India is still somewhat lenient, Bollywood and other film industries are much more aware of each other now and Indian audiences are more familiar with foreign movies and music. Organisations like the India EU Film Initiative seek to foster a community between film makers and industry professional between India and the EU.\n\nOne of the common justifications of plagiarism in Bollywood in the media is that producers often play a safer option by remaking popular Hollywood films in an Indian context. Screenwriters generally produce original scripts, but due to financial uncertainty and insecurity over the success of a film many were rejected. Screenwriters themselves have been criticised for lack of creativity which happened due to tight schedules and restricted funds in the industry to employ better screenwriters. Certain filmmakers see plagiarism in Bollywood as an integral part of globalisation where American and western cultures are firmly embedding themselves into Indian culture, which is manifested, amongst other mediums, in Bollywood films. Vikram Bhatt, director of films such as \"Raaz\" which stars Bipasha Basu, a remake of \"What Lies Beneath\", and \"Kasoor\", a remake of \"Jagged Edge\", has spoken about the strong influence of American culture and desire to produce box office hits based along the same lines in Bollywood. He said, \"Financially, I would be more secure knowing that a particular piece of work has already done well at the box office. Copying is endemic everywhere in India. Our TV shows are adaptations of American programmes. We want their films, their cars, their planes, their Diet Cokes and also their attitude. The American way of life is creeping into our culture.\" Mahesh Bhatt has said, \"If you hide the source, you're a genius. There's no such thing as originality in the creative sphere\".\n\nThere have been very few cases of film copyright violations taken to court because of serious delays in the legal process, and due to the long time they take to decide a case. There have been some notable cases of conflict though. The makers of \"Partner\" (2007) and \"Zinda\" (2005) have been targeted by the owners and distributors of the original films, \"Hitch\" and \"Oldboy\". American Studio Twentieth Century Fox brought the Mumbai-based B.R. Films to court over its forthcoming \"Banda Yeh Bindaas Hai\", allegedly an illegal remake of its 1992 film \"My Cousin Vinny\". B.R. Films eventually settled out of court by paying the studio at a cost of about $200,000, paving the way for the film's release. Some on the other hand do comply with copyright law, with Orion Pictures in 2008 securing the rights to remake the Hollywood film \"Wedding Crashers\".\n\n\n"}
{"id": "4248", "url": "https://en.wikipedia.org/wiki?curid=4248", "title": "Bowls", "text": "Bowls\n\nBowls or lawn bowls is a sport in which the objective is to roll biased balls so that they stop close to a smaller ball called a \"jack\" or \"kitty\". It is played on a bowling green which may be flat (for \"flat-green bowls\") or convex or uneven (for \"crown green bowls\"). It is normally played outdoors (although there are many indoor venues) and the outdoor surface is either natural grass, artificial turf, or cotula (in New Zealand).\n\nIt has been traced certainly to the 13th century, and conjecturally to the 12th. William Fitzstephen (d. about 1190), in his biography of Thomas Becket, gives a graphic sketch of the London of his day and, writing of the summer amusements of the young men, says that on holidays they were \"exercised in Leaping, Shooting, Wrestling, Casting of Stones [in jactu lapidum], and Throwing of Javelins fitted with Loops for the Purpose, which they strive to fling before the Mark; they also use Bucklers, like fighting Men.\" It is commonly supposed that by jactus lapidum, Fitzstephen meant the game of bowls, but though it is possible that round stones may sometimes have been employed in an early variety of the game - and there is a record of iron bowls being used, though at a much later date, on festive occasions at Nairn, - nevertheless the inference seems unwarranted. The jactus lapidum of which he speaks may have been more akin to shotput. It is beyond dispute, however, that the game, at any rate in a rudimentary form, was played in the 13th century. A manuscript of that period in the royal library, Windsor (No. 20, E iv.), contains a drawing representing two players aiming at a small cone instead of an earthenware ball or jack. The world's oldest surviving bowling green is the Southampton Old Bowling Green, which was first used in 1299.\n\nAnother manuscript of the same century has a crude but spirited picture which brings us into close touch with the existing game. Three figures are introduced and a jack. The first player's bowl has come to rest just in front of the jack; the second has delivered his bowl and is following after it with one of those eccentric contortions still not unusual on modern greens, the first player meanwhile making a repressive gesture with his hand, as if to urge the bowl to stop short of his own; the third player is depicted as in the act of delivering his bowl. A 14th-century manuscript, Book of Prayers, in the Francis Douce collection in the Bodleian Library at Oxford contains a drawing in which two persons are shown, but they bowl to no mark. Strutt (Sports and Pastimes) suggests that the first player's bowl may have been regarded by the second player as a species of jack; but in that case it is not clear what was the first player's target. In these three earliest illustrations of the pastime it is worth noting that each player has one bowl only, and that the attitude in delivering it was as various five or six hundred years ago as it is today. In the third he stands almost upright; in the first he kneels; in the second he stoops, halfway between the upright and the kneeling position.\n\nThe game eventually came under the ban of king and parliament, both fearing it might jeopardise the practice of archery, then so important in battle. Statutes forbidding it and other sports were enacted in the reigns of Edward III, Richard II and other monarchs. Even when, on the invention of gunpowder and firearms, the bow had fallen into disuse as a weapon of war, the prohibition was continued. The discredit attaching to bowling alleys, first established in London in 1455, probably encouraged subsequent repressive legislation, for many of the alleys were connected with taverns frequented by the dissolute and gamesters. The word \"bowls\" occurs for the first time in the statute of 1511 in which Henry VIII confirmed previous enactments against unlawful games. By a further act of 1541—which was not repealed until 1845—artificers, labourers, apprentices, servants and the like were forbidden to play bowls at any time except Christmas, and then only in their master's house and presence. It was further enjoined that any one playing bowls outside his own garden or orchard was liable to a penalty of 6s. 8d.(6 shillings and 8 pence), while those possessed of lands of the yearly value of £100 might obtain licences to play on their own private greens.\n\nIn 1864 William Wallace Mitchell (1803–1884), a Glasgow Cotton Merchant, published his \"Manual of Bowls Playing\" following his work as the secretary formed in 1849 by Scottish bowling clubs which became the basis of the rules of the modern game.\nYoung Mitchell was only 11 when he played on Kilmarnock Bowling green, the oldest club in Scotland, instituted in 1740.\nThe patenting of the first lawn mower in 1830, in Britain, is strongly believed to have been the catalyst, worldwide, for the preparation of modern-style greens, sporting ovals, playing fields, pitches, grass courts, etc. This is turn led to the codification of modern rules for many sports, including lawn bowls, most football codes, lawn tennis and others.\n\nNational Bowling Associations were established in the late 1800s. In the then Victorian Colony (now State of Victoria in Australia), the (Royal) Victorian Bowling Association was formed in 1880 and The Scottish Bowling Association was established in 1892, although there had been a failed attempt in 1848 by 200 Scottish clubs.\n\nToday the sport is played in over 40 countries with more than 50 member national authorities.\nThe home of the modern game is still Scotland with the World Bowls centre in Edinburgh at Caledonia House,1 Redheughs Rigg, South Gyle, Edinburgh, EH12 9DQ.\n\nLawn bowls is usually played on a large, rectangular, precisely levelled and manicured grass or synthetic surface known as a bowling green which is divided into parallel playing strips called rinks. In the simplest competition, singles, one of the two opponents flips a coin to see who wins the \"mat\" and begins a segment of the competition (in bowling parlance, an \"end\"), by placing the mat and rolling the jack to the other end of the green to serve as a target. Once it has come to rest, the jack is aligned to the centre of the rink and the players take turns to roll their bowls from the mat towards the jack and thereby build up the \"head\".\n\nA bowl may curve outside the rink boundary on its path, but must come to rest within the rink boundary to remain in play. Bowls falling into the ditch are dead and removed from play, except in the event when one has \"touched\" the jack on its way. \"Touchers\" are marked with chalk and remain alive in play even though they are in the ditch. Similarly if the jack is knocked into the ditch it is still alive unless it is out of bounds to the side resulting in a \"dead\" end which is replayed, though according to international rules the jack is \"respotted\" to the centre of the rink and the end is continued. After each competitor has delivered all of their bowls (four each in singles and pairs, three each in triples, and two bowls each in fours), the distance of the closest bowls to the jack is determined (the jack may have been displaced) and points, called \"shots\", are awarded for each bowl which a competitor has closer than the opponent's nearest to the jack. For instance, if a competitor has bowled two bowls closer to the jack than their opponent's nearest, they are awarded two shots. The exercise is then repeated for the next end, a game of bowls typically being of twenty-one ends.\n\nLawn bowls is played on grass and variations from green to green are common. Greens come in all shapes and sizes, fast, slow, big crown, small crown and so on.\n\nScoring systems vary from competition to competition. Games can be decided when:\n\nGames to a specified number of ends may also be drawn. The draw may stand, or the opponents may be required to play an extra end to decide the winner. These provisions are always published beforehand in the event's Conditions of Play.\n\nIn the Laws of the Sport of Bowls\nthe winner in a singles game is the first player to score 21 shots. In all other disciplines (pairs, triples, fours) the winner is the team who has scored the most shots after 21/25 ends of play. Often local tournaments will play shorter games (often 10 or 12 ends). Some competitions use a \"set\" scoring system, with the first to seven points awarded a set in a best-or-three or best-of-five set match. As well as singles competition, there can be two (pairs), three (triples) and four-player (fours) teams. In these, teams bowl alternately, with each player within a team bowling all their bowls, then handing over to the next player. The team captain or \"skip\" always plays last and is instrumental in directing his team's shots and tactics. The current method of scoring in the professional tour (World Bowls Tour) is sets. Each set consists of nine ends and the player with the most shots at the end of a set wins the set. If the score is tied the set is halved. If a player wins two sets, or gets a win and a tie, that player wins the game. If each player wins a set, or both sets end tied, there is a 3-end tiebreaker to determine a winner.\n\nBowls are designed to travel a curved path because of a weight bias which was originally produced by inserting weights in one side of the bowl. This is no longer permitted by the rules and bias is now produced entirely by the shape of the bowl. A bowler determines the bias direction of the bowl in his hand by a dimple or symbol on one side. Regulations determine the minimum bias allowed, and the range of diameters (11.6 to 13.1 cm), but within these rules bowlers can and do choose bowls to suit their own preference. They were originally made from lignum vitae, a dense wood giving rise to the term \"woods\" for bowls, but are now more typically made of a hard plastic composite material.\n\nBowls were once only available coloured black or brown but they are now available in a variety of colours. They have unique symbol markings engraved on them for identification. Since many bowls look the same, coloured, adhesive stickers or labels are also used to mark the bowls of each team in bowls matches. Some local associations agree on specific colours for stickers for each of the clubs in their area. Provincial or national colours are often assigned in national and international competitions. These stickers are used by officials to distinguish teams.\n\nBowls have symbols unique to the set of four for identification. The side of the bowl with a larger symbol within a circle indicates the side away from the bias. That side with a smaller symbol within a smaller circle is the bias side toward which the bowl will turn. It is not uncommon for players to deliver a \"wrong bias\" shot from time to time and see their carefully aimed bowl crossing neighbouring rinks rather than heading towards their jack.\n\nWhen bowling there are several types of delivery. \"Draw\" shots are those where the bowl is rolled to a specific location without causing too much disturbance of bowls already in the head. For a right-handed bowler, \"forehand draw\" or \"finger peg\" is initially aimed to the right of the jack, and curves in to the left. The same bowler can deliver a \"backhand draw\" or \"thumb peg\" by turning the bowl over in his hand and curving it the opposite way, from left to right. In both cases, the bowl is rolled as close to the jack as possible, unless tactics demand otherwise. A \"drive\" or \"fire\" or \"strike\" involves bowling with force with the aim of knocking either the jack or a specific bowl out of play - and with the drive's speed, there is virtually no noticeable (or, at least, much less) curve on the shot. An \"upshot\" or \"yard on\" shot involves delivering the bowl with an extra degree of weight (often referred to as \"controlled\" weight or \"rambler\"), enough to displace the jack or disturb other bowls in the head without killing the end. A \"block\" shot is one that is intentionally placed short to defend from a drive or to stop an oppositions draw shot. The challenge in all these shots is to be able to adjust line and length accordingly, the faster the delivery, the narrower the line or \"green\".\n\nParticularly in team competition there can be a large number of bowls on the green towards the conclusion of the end, and this gives rise to complex tactics. Teams \"holding shot\" with the closest bowl will often make their subsequent shots not with the goal of placing the bowl near the jack, but in positions to make it difficult for opponents to get their bowls into the head, or to places where the jack might be deflected to if the opponent attempts to disturb the head.\n\nThere are many different ways to set up the game. Crown Green Bowling utilises the entire green. A player can send the jack anywhere on the green in this game and the green itself is more akin to a golf green, with lots of undulation. It is only played with two bowls each, the Jack also has a bias and is only slightly smaller than the Bowls. The game is played usually to 21-up in Singles and Doubles format with some competitions playing to 31-up. The Panel (Professional Crown Green Bowls) is played at the Red Lion, Westhoughton daily and is played to 41-up with greenside betting throughout play.\n\nSingles, triples and fours and Australian pairs are some ways the game can be played. In singles, two people play against each other and the first to reach 21, 25 or 31 shots (as decided by the controlling body) is the winner. In one variation of singles play, each player uses two bowls only and the game is played over 21 ends. A player concedes the game before the 21st end if the score difference is such that it is impossible to draw equal or win within the 21 ends. If the score is equal after 21 ends, an extra end is played to decide the winner. An additional scoring method is set play. This comprises two sets over nine ends. Should a player win a set each, they then play a further 3 ends that will decide the winner.\n\nPairs allows both people on a team to play Skip and Lead. The lead throws two bowls, the skip delivers two, then the lead delivers his remaining two, the skip then delivers his remaining two bowls. Each end, the leads and skips switch positions. This is played over 21 ends or sets play. Triples is with three players while Fours is with four players in each team and is played over 21 ends.\n\nAnother pairs variation is 242 pairs (also known as Australian Pairs). In the first end of the game the A players lead off with 2 bowls each, then the B players play 4 bowls each, before the A players complete the end with their final 2 bowls. The A players act as lead and skip in the same end. In the second end the roles are reversed with the A players being in the middle. This alternating pattern continues through the game which is typically over 15 ends.\n\nShort Mat Bowls is an all-year sport unaffected by weather conditions and it does not require a permanent location as the rink mats can be rolled up and stowed away. This makes it particularly appropriate for small communities as it can be played in village halls, schools, sports and social clubs, hotels and so on. where space is restricted and is also required for other purposes: it is even played on North Sea oil rigs where space is really at a premium.\n\nBowls are played by the blind and paraplegic. Blind bowlers are extremely skilful. A string is run out down the centre of the lane & wherever the jack lands it is moved across to the string and the length is called out by a sighted marker, when the woods are sent the distance from the jack is called out, in yards, feet and inches-the position in relation to the jack is given using the clock, 12.00 is behind the jack..\n\nIn the province of West-Flanders (and surrounding regions), tra bowls is the most popular variation of bowls. As opposed to playing it on a flat or uneven terrain, the terrain is made smooth but hollow (tra just means \"hollow road\" in Flemish). The hollow road causes the path to be curving even more.\n\nThe balls are biased in the same way as the lawn bowls balls but with a diameter of about 20 cm, a thickness of 12 cm and a weight of about 2 kg, they are a bit bigger than usual bowls. The target is an unmovable feather or metal plate on the ground, instead of a small ball. The length of the tra is about 18 m.\n\nThe scoring is also different, as a point is awarded for every shot that brings the ball closer to the target than any opponent's ball. This causes pure blocking strategies to be less effective.\n\nIn 1972, the West-Flemish tra bowls federation was founded to uniform the local differing rules and to organise a match calendar. Meanwhile, they also organise championships and tournaments.\n\nThere is a World Indoor Bowls Championships and also World Bowls Events.\n\nBowls is one of the \"core sports\" that must be included at each edition of the Commonwealth Games. With the exception of the 1966 Games, the sport has been included in all Games since their inception in 1930. Glasgow, Scotland hosted the 2014 Commonwealth Games, with Jo Edwards (New Zealand) and Darren Burnett (Scotland) winning the singles gold medals. Gold Coast, Australia will host the 2018 Commonwealth Games.\n\n\n\n"}
{"id": "4249", "url": "https://en.wikipedia.org/wiki?curid=4249", "title": "Barcelonnette", "text": "Barcelonnette\n\nBarcelonnette (; ) is a commune of France and a subprefecture in the department of Alpes-de-Haute-Provence, in the Provence-Alpes-Côte d'Azur region. It is located in the southern French Alps, at the crossroads between Provence, Piedmont and the Dauphiné, and is the largest town in the Ubaye Valley. The town's inhabitants are known as \"Barcelonnettes\".\n\nBarcelonnette was founded and named in 1231, by Ramon Berenguer IV, Count of Provence. While the town's name is generally seen as a diminutive form of Barcelona in Catalonia, Albert Dauzat and Charles Rostaing point out an earlier attestation of the name \"Barcilona\" in Barcelonnette in around 1200, and suggest that it is derived instead from two earlier stems signifying a mountain, *\"bar\" and *\"cin\" (the latter of which is also seen in the name of Mont Cenis).\n\nIn the Vivaro-Alpine dialect of Occitan, the town is known as \"Barcilona de Provença\" or more rarely \"Barciloneta\" according to the classical norm; under the Mistralian norm it is called \"Barcilouna de Prouvença\" or \"Barcilouneto\". In \"Valéian\" (the dialect of Occitan spoken in the Ubaye Valley), it is called \"Barcilouna de Prouvença\" or \"Barcilounéta\". \"Barcino Nova\" is the town's Latin name meaning \"new Barcelona\"; \"Barcino\" was the Roman name for Barcelona in Catalonia from its foundation by Emperor Augustus in 10 BC, and it was only changed to \"Barcelona\" in the Middle Ages.\n\nThe inhabitants of the town are called \"Barcelonnettes\", or \"Vilandroises\" in Valéian.\n\nThe Barcelonnette region was populated by Ligures from the 1st millennium BC onwards, and the arrival of the Celts several centuries later led to the formation of a mixed Celto-Ligurian people, the Vesubians. Polybius described the Vesubians as belligerent but nonetheless civilised and mercantile, and Julius Caesar praised their bravery. The work \"History of the Gauls\" also places the Vesubians in the Ubaye Valley.\n\nFollowing the Roman conquest of Provence, Barcelonnette was included in a small province with modern Embrun as its capital and governed by Albanus Bassalus. This was integrated soon afterwards into Gallia Narbonensis. In 36 AD, Emperor Nero transferred Barcelonnette to the province of the Cottian Alps. The town was known as \"Rigomagensium\" under the Roman Empire and was the capital of a civitas (a provincial subdivision), though no Roman money has yet been found in the canton of Barcelonnette.\n\nThe town of Barcelonnette was founded in 1231 by Ramon Berenguer IV, Count of Provence. According to Charles Rostaing, this act of formal \"foundation\", according certain privileges to the town, was a means of regenerating the destroyed town of \"Barcilona\". The town was afforded a \"consulat\" (giving it the power to administer and defend itself) in 1240.\n\nControl of the area in the Middle Ages swung between the Counts of Savoy and of Provence. In 1388, after Count Louis II of Provence had left to conquer Naples, the Count of Savoy Amadeus VIII took control of Barcelonnette; however, it returned to Provençal control in 1390, with the d'Audiffret family as its lords. On the death of Louis II in 1417 it reverted to Savoy, and, although Count René again retook the area for Provence in 1471, it had returned to Savoyard dominance by the start of the 16th century, by which point the County of Provence had become united with the Kingdom of France due to the death of Count Charles V in 1481.\n\nDuring Charles V's invasion of Provence in 1536, Francis I of France sent the Count of Fürstenberg's 6000 \"Landsknechte\" to ravage the area in a scorched earth policy. Barcelonnette and the Ubaye Valley remained under French sovereignty until the second Treaty of Cateau-Cambrésis on 3 April 1559.\n\nIn 1588 the troops of François, Duke of Lesdiguières entered the town and set fire to the church and convent during their campaign against the Duke of Savoy. In 1600, after the Treaty of Vervins, conflict returned between Henry IV of France and Savoy, and Lesdiguières retook Barcelonnette until the conclusion of the Treaty of Lyon on 17 January the following year. In 1628, during the War of the Mantuan Succession, Barcelonnette and the other towns of the Ubaye Valley were pillaged and burned by Jacques du Blé d'Uxelles and his troops, as they passed through towards Italy to the Duke of Mantua's aid. The town was retaken by the Duke of Savoy in 1630; and in 1691 it was captured by the troops of the Marquis de Vins during the War of the League of Augsburg.\n\nBetween 1614 and 1713, Barcelonnette was the seat of one of the four prefectures under the jurisdiction of the Senate of Nice. At this time, the community of Barcelonnette successfully purchased the \"seigneurie\" of the town as it was put to auction by the Duke of Savoy; it thereby gained its own justicial powers. In 1646, a college was founded in Barcelonnette.\n\nA \"significant\" part of the town's inhabitants had, by the 16th century, converted to Protestantism, and were repressed during the French Wars of Religion.\n\nThe \"viguerie\" of Barcelonnette (also comprising Saint-Martin and Entraunes) was reattached to France in 1713 as part of a territorial exchange with the Duchy of Savoy during the Treaties of Utrecht. The town remained the site of a \"viguerie\" until the French Revolution. A decree of the council of state on 25 December 1714 reunited Barcelonnete with the general government of Provence.\n\nBarcelonnette was one of few settlements in Haute-Provence to acquire a Masonic Lodge before the Revolution, in fact having two:\n\nIn March 1789, riots took place as a result of a crisis in wheat production. In July, the Great Fear of aristocratic reprisal against the ongoing French Revolution struck France, arriving in the Barcelonnette area on 31 July 1789 (when the news of the storming of the Bastille first reached the town) before spreading towards Digne.\n\nThis agitation continued in the Ubaye Valley; a new revolt broke out on 14 June, and famine was declared in April 1792. The patriotic society of the commune was one of the first 21 created in Alpes-de-Haute-Provence, in spring 1792, by the envoys of the departmental administration. Around a third of the male population attended at the club. Another episode of political violence occurred in August 1792.\n\nBarcelonnette was the seat of the District of Barcelonnette from 1790 to 1800.\n\nIn December 1851, the town was home to a movement of republican resistance towards Napoleon III's coup. Though only a minority of the population, the movement rebelled on Sunday 7 December, the day after the news of the coup arrived. Town officials and gendarmes were disarmed and placed in the maison d'arrêt. A committee of public health was created on 8 December; on 9 December the inhabitants of Jausiers and its surroundings formed a colony under the direction of general councillor Brès, and Mayor Signoret of Saint-Paul-sur-Ubaye. This was stopped, however, on 10 December before it could reach Barcelonnette, as the priest of the subprefecture had intervened. On 11 December, several officials escaped and found refuge in L'Argentière in Piedmont. The arrival of troops on 16 December put a final end to the republican resistance without bloodshed, and 57 insurgents were tried; 38 were condemned to deportation (though several were pardoned in April).\n\nBetween 1850 and 1950, Barcelonnette was the source of a wave of emigration to Mexico. Among these emigrants was Jean Baptiste Ebrard, founder of the Liverpool department store chain in Mexico; Marcelo Ebrard, the head of government of Mexico City from 2006 to 2012, is also descended from them. On the edges of Barcelonnette and Jausiers there are several houses and villas of colonial style (known as \"maisons mexicaines\"), constructed by emigrants to Mexico who returned to France between 1870 and 1930. A plaque in the town commemorates the deaths of ten Mexican citizens who returned to Barcelonnette to fight in the First World War.\n\nDuring the Second World War, 26 Jews were arrested in Barcelonnette before being deported. The 89th \"compagnie de travailleurs étrangers\" (Company of Foreign Workers), consisting of foreigners judged as undesirable by the Third Republic and the Vichy regime and committed to forced labour, was established in Barcelonnette.\n\nThe 11th Battalion of \"Chasseurs alpins\" was garrisoned at Barcelonnette between 1948 and 1990.\n\nBarcelonnette is situated in the wide and fertile Ubaye Valley, of which it is the largest town. It lies at an elevation of 1132 m (3717 ft) on the right bank of the Ubaye River, and is surrounded by mountains which reach peaks of over 3000 m; the tallest of these is the Needle of Chambeyron at 3412 m. Barcelonnette is situated 210 km from Turin, 91 km from Nice and 68 km from Gap.\n\nAs a result of its relief and geographic situation, the Ubaye Valley has an \"abundance of plant and animal species\". The fauna is largely constituted of golden eagles, marmots, ibex and vultures, and the flora includes a large proportion of larches, génépis and white asphodels.\n\nThe Ubaye Valley has an alpine climate and winters are harsh as a result of the altitude, but there are only light winds as a result of the relief. There are on average almost 300 days of sun and 700 mm of rain per year.\n\nNone of the 200 communes of the department is entirely free of seismic risk; the canton of Barcelonnette is placed in zone 1b (low risk) by the determinist classification of 1991 based on seismic history, and zone 4 (average risk) according to the probabilistic EC8 classification of 2011. The commune is also vulnerable to avalanches, forest fires, floods, and landslides. Barcelonnette is also exposed to the possibility of a technological hazard in that road transport of dangerous materials is allowed to pass through on the RD900.\n\nThe town has been subject to several orders of natural disaster: floods and mudslides in 1994 and 2008, and landslides in 1996 and 1999. The strongest recorded earthquakes in the region occurred on 5 April 1959, with its epicentre at Saint-Paul-sur-Ubaye and a recorded intensity of 6.5 at Barcelonnette, and on 17 February 1947, with its epicentre at Prazzo over the Italian border.\n\n\nThe subprefecture has been situated since 1978 in a \"maison mexicaine\", the Villa l'Ubayette, constructed between 1901 and 1903.\n\nIn 1471, the community of Barcelonnette (including several surrounding parishes) comprised 421 fires (households). In 1765, it had 6674 inhabitants, but emigration, particularly to Mexico, slowed the town's growth in the period before the Second World War. According to the census of 2007, Barcelonnette has a population of 2766 (municipal population) or 2939 (total) across a total of 16.42 km. The town is characterised by low population density. Between 1990 and 1999 the town's annual mean population growth was -0.6%, though between 1999 and 2007 this increased to an average of -0.1%.\n\nThe city is mainly a tourist and resort centre, serving many ski lodges. The Pra-Loup resort is 7 km from Barcelonnette; Le Sauze is 5 km away. It and the Ubaye Valley are served by the Barcelonnette – Saint-Pons Airfield. Notably, Barcelonnette is the only subprefecture of France not served by rail transport; the Ubaye line which would have linked Chorges to Barcelonnette was never completed as a result of the First World War and the construction of the Serre-Ponçon Dam between 1955 and 1961.\n\nAn \"école normale\" (an institute for training primary school teachers) was founded in Barcelonnette in 1833, and remained there until 1888 when it was transferred to Digne. The \"lycée André-Honnorat de Barcelonnette\", originally the \"collège Saint-Maurice\" and renamed after the politician André Honnorat in 1919, is located in the town; Pierre-Gilles de Gennes and Carole Merle both studied there. Currently, three schools exist in Barcelonnette: a public nursery school, a public elementary school, and a private school (under a contract by which the teachers are paid by the national education system).\n\nIn 2010 the \"lycée André-Honnorat\" opened a boarding school aimed at gifted students of poorer social backgrounds, in order to give them better conditions in which to study. It is located in the \"Quartier Craplet\", formerly the garrison of the 11th Battalion of \"Chasseurs Alpins\" and then the French Army's \"Centre d'instruction et d'entraînement au combat en montagne\" (CIECM).\n\nBarcelonnette is twinned with:\nIt is also the site of a Mexican honorary consulate.\n\n\n"}
{"id": "4251", "url": "https://en.wikipedia.org/wiki?curid=4251", "title": "Bahá'í Faith", "text": "Bahá'í Faith\n\nThe Bahá'í Faith ( ') is an Abrahamic religion teaching the essential worth of all religions, and the unity and equality of all people. Established by Bahá'u'lláh in 1863, it initially grew in the Middle East and now has between 5-7 million adherents, known as Bahá'ís, spread out into most of the world's countries and territories, with the highest concentrations in India and Iran.\nThe religion was born in Iran, where it has faced ongoing persecutions since its inception. It grew from the mid-19th century Bábí religion, whose founder reinterpreted Shia Islam and said that God would soon send a prophet in the manner of Jesus or Muhammad. In 1863, after being banished from his native Iran, Bahá'u'lláh announced that he was this prophet. He was further exiled, spending over a decade in the prison city of Akka in the Ottoman province of Syria, in what is now Israel. Following Bahá'u'lláh's death in 1892, leadership of the religion fell to his son `Abdu'l-Bahá (1844-1921), and later his great-grandson Shoghi Effendi (1897-1957). Bahá'ís around the world annually elect local, regional, and national Spiritual Assemblies that govern the affairs of the religion, and every five years the members of all National Spiritual Assemblies elect the Universal House of Justice, the nine-member supreme governing institution of the worldwide Bahá'í community, which sits in Haifa, Israel near the shrine of Báb.\n\nBahá'í teachings are in some ways similar to other monotheistic faiths: God is considered single and all-powerful. However, Bahá'u'lláh taught that religion is orderly and progressively revealed by one God through Manifestations of God who are the founders of major world religions throughout history; Buddha, Jesus, and Muhammad being the most recent in the period before the Báb and Bahá'u'lláh. As such, Bahá'ís regard the major religions as fundamentally unified in purpose, though varied in social practices and interpretations. There is a similar emphasis on the unity of all people, openly rejecting notions of racism and nationalism. At the heart of Bahá'í teachings is the goal of a unified world order that ensures the prosperity of all nations, races, creeds, and classes.\n\nLetters written by Bahá'u'lláh to various individuals, including some heads of state, have been collected and canonized into a body of Bahá'í scripture that includes works by his son `Abdu'l-Bahá, and also the Báb, who is regarded as Bahá'u'lláh's forerunner. Prominent among Bahá'í literature are the \"Kitáb-i-Aqdas\", \"Kitáb-i-Íqán\", \"Some Answered Questions\", and \"The Dawn-Breakers\".\n\nIn English-language use, the word \"Bahá'í \" is used either as an adjective to refer to the Bahá'í Faith or as a term for a follower of Bahá'u'lláh. The word is not a noun meaning the religion as a whole. It is derived from the Arabic \"Bahá'\" (), meaning \"glory\" or \"splendor\", although the word \"Bahá'í \" is actually derived from its use as a loan word in Persian, in particular the \"'i\" suffix is Persian rather than Arabic. The term \"Bahaism\" (or \"Baha'ism\") is still used, mainly in a pejorative sense.\n\nThree core principles establish a basis for Bahá'í teachings and doctrine: the unity of God, the unity of religion, and the unity of humanity. From these postulates stems the belief that God periodically reveals his will through divine messengers, whose purpose is to transform the character of humankind and to develop, within those who respond, moral and spiritual qualities. Religion is thus seen as orderly, unified, and progressive from age to age.\n\nThe Bahá'í writings describe a single, personal, inaccessible, omniscient, omnipresent, imperishable, and almighty God who is the creator of all things in the universe. The existence of God and the universe is thought to be eternal, without a beginning or end. Though inaccessible directly, God is nevertheless seen as conscious of creation, with a will and purpose that is expressed through messengers termed Manifestations of God.\n\nBahá'í teachings state that God is too great for humans to fully comprehend, or to create a complete and accurate image of, by themselves. Therefore, human understanding of God is achieved through his revelations via his Manifestations. In the Bahá'í religion, God is often referred to by titles and attributes (for example, the All-Powerful, or the All-Loving), and there is a substantial emphasis on monotheism; such doctrines as the Trinity are seen as compromising, if not contradicting, the Bahá'í view that God is single and has no equal. The Bahá'í teachings state that the attributes which are applied to God are used to translate Godliness into human terms and also to help individuals concentrate on their own attributes in worshipping God to develop their potentialities on their spiritual path. According to the Bahá'í teachings the human purpose is to learn to know and love God through such methods as prayer, reflection, and being of service to others.\nBahá'í notions of progressive religious revelation result in their accepting the validity of the well known religions of the world, whose founders and central figures are seen as Manifestations of God. Religious history is interpreted as a series of dispensations, where each \"manifestation\" brings a somewhat broader and more advanced revelation that is rendered as a text of scripture and passed on through history with greater or lesser reliability but at least true in substance, suited for the time and place in which it was expressed. Specific religious social teachings (for example, the direction of prayer, or dietary restrictions) may be revoked by a subsequent manifestation so that a more appropriate requirement for the time and place may be established. Conversely, certain general principles (for example, neighbourliness, or charity) are seen to be universal and consistent. In Bahá'í belief, this process of progressive revelation will not end; however, it is believed to be cyclical. Bahá'ís do not expect a new manifestation of God to appear within 1000 years of Bahá'u'lláh's revelation.\n\nBahá'í beliefs are sometimes described as syncretic combinations of earlier religious beliefs. Bahá'ís, however, assert that their religion is a distinct tradition with its own scriptures, teachings, laws, and history. While the religion was initially seen as a sect of Islam, most religious specialists now see it as an independent religion, with its religious background in Shi'a Islam being seen as analogous to the Jewish context in which Christianity was established. Muslim institutions and clergy, both Sunni and Shia, consider Bahá'ís to be deserters or apostates from Islam, which has led to Bahá'ís being persecuted. Bahá'ís describe their faith as an independent world religion, differing from the other traditions in its relative age and in the appropriateness of Bahá'u'lláh's teachings to the modern context. Bahá'u'lláh is believed to have fulfilled the messianic expectations of these precursor faiths.\n\nThe Bahá'í writings state that human beings have a \"rational soul\", and that this provides the species with a unique capacity to recognize God's station and humanity's relationship with its creator. Every human is seen to have a duty to recognize God through His messengers, and to conform to their teachings. Through recognition and obedience, service to humanity and regular prayer and spiritual practice, the Bahá'í writings state that the soul becomes closer to God, the spiritual ideal in Bahá'í belief. When a human dies, the soul passes into the next world, where its spiritual development in the physical world becomes a basis for judgment and advancement in the spiritual world. Heaven and Hell are taught to be spiritual states of nearness or distance from God that describe relationships in this world and the next, and not physical places of reward and punishment achieved after death.\n\nThe Bahá'í writings emphasize the essential equality of human beings, and the abolition of prejudice. Humanity is seen as essentially one, though highly varied; its diversity of race and culture are seen as worthy of appreciation and acceptance. Doctrines of racism, nationalism, caste, social class, and gender-based hierarchy are seen as artificial impediments to unity. The Bahá'í teachings state that the unification of humanity is the paramount issue in the religious and political conditions of the present world.\n\nShoghi Effendi, the Guardian of the religion from 1921 to 1957, wrote the following summary of what he considered to be the distinguishing principles of Bahá'u'lláh's teachings, which, he said, together with the laws and ordinances of the \"Kitáb-i-Aqdas\" constitute the bedrock of the Bahá'í Faith:\n\nThe following principles are frequently listed as a quick summary of the Bahá'í teachings. They are derived from transcripts of speeches given by `Abdu'l-Bahá during his tour of Europe and North America in 1912. The list is not authoritative and a variety of such lists circulate.\n\nWith specific regard to the pursuit of world peace, Bahá'u'lláh prescribed a world-embracing collective security arrangement for the establishment of a temporary era of peace referred to in the Baha'i teachings as the Lesser Peace. For the establishment of a lasting peace (The Most Great Peace) and the purging of the 'overwhelming Corruptions' it is necessary that all the people of the world universally unite under a universal Faith.\n\nThe Bahá'í teachings speak of both a \"Greater Covenant\", being universal and endless, and a \"Lesser Covenant\", being unique to each religious dispensation. The Lesser Covenant is viewed as an agreement between a Messenger of God and his followers and includes social practices and the continuation of authority in the religion. At this time Bahá'ís view Bahá'u'lláh's revelation as a binding lesser covenant for his followers; in the Bahá'í writings being firm in the covenant is considered a virtue to work toward. The Greater Covenant is viewed as a more enduring agreement between God and humanity, where a Manifestation of God is expected to come to humanity about every thousand years, at times of turmoil and uncertainty.\nWith unity as an essential teaching of the religion, Bahá'ís follow an administration they believe is divinely ordained, and therefore see attempts to create schisms and divisions as efforts that are contrary to the teachings of Bahá'u'lláh. Schisms have occurred over the succession of authority, but any Bahá'í divisions have had relatively little success and have failed to attract a sizeable following. The followers of such divisions are regarded as Covenant-breakers and shunned, essentially excommunicated.\n\nThe \"canonical texts\" are the writings of the Báb, Bahá'u'lláh, `Abdu'l-Bahá, Shoghi Effendi and the Universal House of Justice, and the authenticated talks of `Abdu'l-Bahá. The writings of the Báb and Bahá'u'lláh are considered as divine revelation, the writings and talks of `Abdu'l-Bahá and the writings of Shoghi Effendi as authoritative interpretation, and those of the Universal House of Justice as authoritative legislation and elucidation. Some measure of divine guidance is assumed for all of these texts. Some of Bahá'u'lláh's most important writings include the Kitáb-i-Aqdas, literally the \"Most Holy Book\", which defines many laws and practices for individuals and society, the Kitáb-i-Íqán, literally the \"Book of Certitude\", which became the foundation of much of Bahá'í belief, the Gems of Divine Mysteries, which includes further doctrinal foundations, and the Seven Valleys and the Four Valleys which are mystical treatises.\n\nAlthough the Bahá'í teachings have a strong emphasis on social and ethical issues, there exist a number of foundational texts that have been described as mystical. The \"Seven Valleys\" is considered Bahá'u'lláh's \"greatest mystical composition.\" It was written to a follower of Sufism, in the style of `Attar, The Persian Muslim poet, and sets forth the stages of the soul's journey towards God. It was first translated into English in 1906, becoming one of the earliest available books of Bahá'u'lláh to the West. The \"Hidden Words\" is another book written by Bahá'u'lláh during the same period, containing 153 short passages in which Bahá'u'lláh claims to have taken the basic essence of certain spiritual truths and written them in brief form.\n\nThe Bahá'í Faith formed from the Persian religion of the Báb, a merchant who began preaching a new interpretation of Shia Islam in 1844. The Báb's claim to divine revelation was rejected by the generality of Islamic clergy in Iran, ending in his public execution by authorities in 1850. The Báb taught that God would soon send a new messenger, and Bahá'ís consider Bahá'u'lláh to be that person. Although they are distinct movements, the Báb is so interwoven into Bahá'í theology and history that Bahá'ís celebrate his birth, death, and declaration as holy days, consider him one of their three central figures (along with Bahá'u'lláh and `Abdu'l-Bahá), and a historical account of the Bábí movement (\"The Dawn-Breakers\") is considered one of three books that every Bahá'í should \"master\" and read \"over and over again\". \n\nThe Bahá'í community was mostly confined to the Persian and Ottoman empires until after the death of Bahá'u'lláh in 1892, at which time he had followers in 13 countries of Asia and Africa. Under the leadership of his son, `Abdu'l-Bahá, the religion gained a footing in Europe and America, and was consolidated in Iran, where it still suffers intense persecution. After the death of `Abdu'l-Bahá in 1921, the leadership of the Bahá'í community entered a new phase, evolving from a single individual to a administrative order with both elected bodies and appointed individuals.\n\nOn the evening of 22 May 1844, Siyyid `Alí-Muhammad of Shiraz proclaimed that he was \"the Báb\" ( \"the Gate\"), referring to his later claim to the station of Mahdi, the Twelfth Imam of Shi`a Islam. His followers were therefore known as Bábís. As the Báb's teachings spread, which the Islamic clergy saw as a threat, his followers came under increased persecution and torture. The conflicts escalated in several places to military sieges by the Shah's army. The Báb himself was imprisoned and eventually executed in 1850.\n\nBahá'ís see the Báb as the forerunner of the Bahá'í Faith, because the Báb's writings introduced the concept of \"He whom God shall make manifest\", a Messianic figure whose coming, according to Bahá'ís, was announced in the scriptures of all of the world's great religions, and whom Bahá'u'lláh, the founder of the Bahá'í Faith, claimed to be in 1863. The Báb's tomb, located in Haifa, Israel, is an important place of pilgrimage for Bahá'ís. The remains of the Báb were brought secretly from Iran to the Holy Land and eventually interred in the tomb built for them in a spot specifically designated by Bahá'u'lláh. The main written works translated into English of the Báb's are collected in Selections from the Writings of the Báb out of the estimated 135 works.\n\nMírzá Husayn `Alí Núrí was one of the early followers of the Báb, and later took the title of Bahá'u'lláh. Bábís faced a period of persecution that peaked in 1852-53 after a few individuals made a failed attempt to assassinate the Shah. Although they acted alone, the government responded with collective punishment, killing many Bábís. Bahá'u'lláh was put in prison. He claimed that in 1853, while incarcerated in the dungeon of the Síyáh-Chál in Tehran, he received the first intimations that he was the one anticipated by the Báb when he received a visit from the Maid of Heaven.\n\nShortly thereafter he was expelled from Tehran to Baghdad, in the Ottoman Empire; then to Constantinople (now Istanbul); and then to Adrianople (now Edirne). In 1863, at the time of his banishment from Baghdad to Constantinople, Bahá'u'lláh declared his claim to a divine mission to his family and followers. Tensions then grew between him and Subh-i-Azal, the appointed leader of the Bábís who did not recognize Bahá'u'lláh's claim. Throughout the rest of his life Bahá'u'lláh gained the allegiance of most of the Bábís, who came to be known as Bahá'ís. Beginning in 1866, he began declaring his mission as a Messenger of God in letters to the world's religious and secular rulers, including Pope Pius IX, Napoleon III, and Queen Victoria.\n\nBahá'u'lláh was banished by Sultan Abdülâziz a final time in 1868 to the Ottoman penal colony of `Akká, in present-day Israel. Towards the end of his life, the strict and harsh confinement was gradually relaxed, and he was allowed to live in a home near `Akká, while still officially a prisoner of that city. He died there in 1892. Bahá'ís regard his resting place at Bahjí as the Qiblih to which they turn in prayer each day.\n\n`Abbás Effendi was Bahá'u'lláh's eldest son, known by the title of `Abdu'l-Bahá (Servant of Bahá). His father left a Will that appointed `Abdu'l-Bahá as the leader of the Bahá'í community, and designated him as the \"Centre of the Covenant\", \"Head of the Faith\", and the sole authoritative interpreter of Bahá'u'lláh's writings. `Abdu'l-Bahá had shared his father's long exile and imprisonment, which continued until `Abdu'l-Bahá's own release as a result of the Young Turk Revolution in 1908. Following his release he led a life of travelling, speaking, teaching, and maintaining correspondence with communities of believers and individuals, expounding the principles of the Bahá'í Faith.\n\nThere are over 27,000 extant documents by `Abdu'l-Bahá, mostly letters, of which only a fraction have been translated into English. Among the more well known are \"The Secret of Divine Civilization\", the \"Tablet to Auguste-Henri Forel\", and \"Some Answered Questions\". Additionally notes taken of a number of his talks were published in various volumes like \"Paris Talks\" during his journeys to the West.\n\nBahá'u'lláh's \"Kitáb-i-Aqdas\" and \"The Will and Testament of `Abdu'l-Bahá\" are foundational documents of the Bahá'í administrative order. Bahá'u'lláh established the elected Universal House of Justice, and `Abdu'l-Bahá established the appointed hereditary Guardianship and clarified the relationship between the two institutions. In his Will, `Abdu'l-Bahá appointed his eldest grandson, Shoghi Effendi, as the first Guardian of the Bahá'í Faith, serving as head of the religion until his death, for 36 years.\n\nShoghi Effendi throughout his lifetime translated Bahá'í texts; developed global plans for the expansion of the Bahá'í community; developed the Bahá'í World Centre; carried on a voluminous correspondence with communities and individuals around the world; and built the administrative structure of the religion, preparing the community for the election of the Universal House of Justice. He died in 1957 under conditions that did not allow for a successor to be appointed.\n\nAt local, regional, and national levels, Bahá'ís elect members to nine-person Spiritual Assemblies, which run the affairs of the religion. There are also appointed individuals working at various levels, including locally and internationally, which perform the function of propagating the teachings and protecting the community. The latter do not serve as clergy, which the Bahá'í Faith does not have. The Universal House of Justice, first elected in 1963, remains the successor and supreme governing body of the Bahá'í Faith, and its 9 members are elected every five years by the members of all National Spiritual Assemblies. Any male Bahá'í, 21 years or older, is eligible to be elected to the Universal House of Justice; all other positions are open to male and female Bahá'ís.\n\nIn 1937, Shoghi Effendi launched a seven-year plan for the Bahá'ís of North America, followed by another in 1946. In 1953, he launched the first international plan, the Ten Year World Crusade. This plan included extremely ambitious goals for the expansion of Bahá'í communities and institutions, the translation of Bahá'í texts into several new languages, and the sending of Bahá'í pioneers into previously unreached nations. He announced in letters during the Ten Year Crusade that it would be followed by other plans under the direction of the Universal House of Justice, which was elected in 1963 at the culmination of the Crusade. The House of Justice then launched a nine-year plan in 1964, and a series of subsequent multi-year plans of varying length and goals followed, guiding the direction of the international Bahá'í community.\n\nAnnually, on 21 April, the Universal House of Justice sends a ‘Ridván’ message to the worldwide Bahá’í community, which generally gives an update on the progress made concerning the current plan, and provides further guidance for the year to come. The Bahá'ís around the world are currently being encouraged to focus on capacity building through children's classes, youth groups, devotional gatherings, and a systematic study of the religion known as study circles. Further focuses are involvement in social action and participation in the prevalent discourses of society. The years from 2001 until 2021 represent four successive five-year plans, culminating in the centennial anniversary of the passing of `Abdu'l-Bahá.\n\nA Bahá'í published document reported 4.74 million Bahá'ís in 1986 growing at a rate of 4.4%. Bahá'í sources since 1991 usually estimate the worldwide Bahá'í population to be above 5 million. The \"World Christian Encyclopedia\" estimated 7.1 million Bahá'ís in the world in 2000, representing 218 countries, and 7.3 million in 2010 with the same source. They further state: \"The Baha'i Faith is the only religion to have grown faster in every United Nations region over the past 100 years than the general population; Baha’i was thus the fastest-growing religion between 1910 and 2010, growing at least twice as fast as the population of almost every UN region.\" This source's only systematic flaw was to consistently have a higher estimate of Christians than other cross-national data sets.\n\nFrom its origins in the Persian and Ottoman Empires, by the early 20th century there were a number of converts in South and South East Asia, Europe, and North America. During the 1950s and 1960s, vast travel teaching efforts brought the religion to almost every country and territory of the world. By the 1990s, Bahá'ís were developing programs for systematic consolidation on a large scale, and the early 21st century saw large influxes of new adherents around the world. The Bahá'í Faith is currently the largest religious minority in Iran, Panama, Belize, and South Carolina; the second largest international religion in Bolivia, Zambia, and Papua New Guinea; and the third largest international religion in Chad and Kenya.\n\nAccording to \"The World Almanac and Book of Facts 2004\":\n\nThe Bahá'í Faith is a medium-sized religion and was listed in \"The Britannica Book of the Year\" (1992–present) as the second most widespread of the world's independent religions in terms of the number of countries represented. According to \"Britannica\", the Bahá'í Faith (as of 2010) is established in 221 countries and territories and has an estimated seven million adherents worldwide. Additionally, Bahá'ís have self-organized in most of the nations of the world.\n\nThe Bahá'í religion was ranked by the Foreign Policy magazine as the world's second fastest growing religion by percentage (1.7%) in 2007.\n\nThe following are a few examples from Bahá'u'lláh's teachings on personal conduct that are required or encouraged of his followers.\n\nThe following are a few examples from Bahá'u'lláh's teachings on personal conduct that are prohibited or discouraged.\n\nWhile some of the laws from the Kitáb-i-Aqdas are applicable at the present time, others are dependent upon the existence of a predominantly Bahá'í society, such as the punishments for arson or murder. The laws, when not in direct conflict with the civil laws of the country of residence, are binding on every Bahá'í, and the observance of personal laws, such as prayer or fasting, is the sole responsibility of the individual.\n\nThe purpose of marriage in the Bahá'i faith is mainly to foster spiritual harmony, fellowship and unity between a man and a woman and to provide a stable and loving environment for the rearing of children. The Bahá'í teachings on marriage call it a \"fortress for well-being and salvation\" and place marriage and the family as the foundation of the structure of human society. Bahá'u'lláh highly praised marriage, discouraged divorce, and required chastity outside of marriage; Bahá'u'lláh taught that a husband and wife should strive to improve the spiritual life of each other. Interracial marriage is also highly praised throughout Bahá'í scripture.\n\nBahá'ís intending to marry are asked to obtain a thorough understanding of the other's character before deciding to marry. Although parents should not choose partners for their children, once two individuals decide to marry, they must receive the consent of all living biological parents, whether they are Bahá'í or not. The Bahá'í marriage ceremony is simple; the only compulsory part of the wedding is the reading of the wedding vows prescribed by Bahá'u'lláh which both the groom and the bride read, in the presence of two witnesses. The vows are \"We will all, verily, abide by the Will of God.\"\n\nBahá'u'lláh prohibited a mendicant and ascetic lifestyle. Monasticism is forbidden, and Bahá'ís are taught to practice spirituality while engaging in useful work. The importance of self-exertion and service to humanity in one's spiritual life is emphasised further in Bahá'u'lláh's writings, where he states that work done in the spirit of service to humanity enjoys a rank equal to that of prayer and worship in the sight of God.\n\nMost Bahá'í meetings occur in individuals' homes, local Bahá'í centers, or rented facilities. Worldwide, there are currently seven Bahá'í Houses of Worship, with an eighth near completion in Chile, and a further seven planned as of April 2012. Bahá'í writings refer to an institution called a \"Mashriqu'l-Adhkár\" (Dawning-place of the Mention of God), which is to form the center of a complex of institutions including a hospital, university, and so on. The first ever Mashriqu'l-Adhkár in `Ishqábád, Turkmenistan, has been the most complete House of Worship.\n\nThe Bahá'í calendar is based upon the calendar established by the Báb. The year consists of 19 months, each having 19 days, with four or five intercalary days, to make a full solar year. The Bahá'í New Year corresponds to the traditional Persian New Year, called Naw Rúz, and occurs on the vernal equinox, near 21 March, at the end of the month of fasting. Bahá'í communities gather at the beginning of each month at a meeting called a Feast for worship, consultation and socializing.\n\nEach of the 19 months is given a name which is an attribute of God; some examples include Bahá’ (Splendour), ‘Ilm (Knowledge), and Jamál (Beauty). The Bahá'í week is familiar in that it consists of seven days, with each day of the week also named after an attribute of God. Bahá'ís observe 11 Holy Days throughout the year, with work suspended on 9 of these. These days commemorate important anniversaries in the history of the religion.\n\nThe symbols of the religion are derived from the Arabic word Bahá’ ( \"splendor\" or \"glory\"), with a numerical value of 9, which is why the most common symbol is the nine-pointed star. The ringstone symbol and calligraphy of the Greatest Name are also often encountered. The former consists of two five-pointed stars interspersed with a stylized Bahá’ whose shape is meant to recall the three onenesses, while the latter is a calligraphic rendering of the phrase Yá Bahá'u'l-Abhá ( \"O Glory of the Most Glorious!\").The five-pointed star is the symbol of the Bahá'í Faith. In the Bahá'í Faith, the star is known as the Haykal (), and it was initiated and established by the Báb. The Báb and Bahá'u'lláh wrote various works in the form of a pentagram.\n\nSince its inception the Bahá'í Faith has had involvement in socio-economic development beginning by giving greater freedom to women, promulgating the promotion of female education as a priority concern, and that involvement was given practical expression by creating schools, agricultural coops, and clinics.\n\nThe religion entered a new phase of activity when a message of the Universal House of Justice dated 20 October 1983 was released. Bahá'ís were urged to seek out ways, compatible with the Bahá'í teachings, in which they could become involved in the social and economic development of the communities in which they lived. Worldwide in 1979 there were 129 officially recognized Bahá'í socio-economic development projects. By 1987, the number of officially recognized development projects had increased to 1482.\n\nBahá'u'lláh wrote of the need for world government in this age of humanity's collective life. Because of this emphasis the international Bahá'í community has chosen to support efforts of improving international relations through organizations such as the League of Nations and the United Nations, with some reservations about the present structure and constitution of the UN. The Bahá'í International Community is an agency under the direction of the Universal House of Justice in Haifa, and has consultative status with the following organizations:\n\nThe Bahá'í International Community has offices at the United Nations in New York and Geneva and representations to United Nations regional commissions and other offices in Addis Ababa, Bangkok, Nairobi, Rome, Santiago, and Vienna. In recent years an Office of the Environment and an Office for the Advancement of Women were established as part of its United Nations Office. The Bahá'í Faith has also undertaken joint development programs with various other United Nations agencies. In the 2000 Millennium Forum of the United Nations a Bahá'í was invited as the only non-governmental speaker during the summit.\n\nBahá'ís continue to be persecuted in Islamic countries, as Islamic leaders do not recognize the Bahá'í Faith as an independent religion, but rather as apostasy from Islam. The most severe persecutions have occurred in Iran, where over 200 Bahá'ís were executed between 1978 and 1998, and in Egypt. The rights of Bahá'ís have been restricted to greater or lesser extents in numerous other countries, including Afghanistan, Indonesia, Iraq, Morocco, and several countries in sub-Saharan Africa.\n\nThe marginalization of the Iranian Bahá'ís by current governments is rooted in historical efforts by Muslim clergy to persecute the religious minority. When the Báb started attracting a large following, the clergy hoped to stop the movement from spreading by stating that its followers were enemies of God. These clerical directives led to mob attacks and public executions. Starting in the twentieth century, in addition to repression aimed at individual Bahá'ís, centrally directed campaigns that targeted the entire Bahá'í community and its institutions were initiated. In one case in Yazd in 1903 more than 100 Bahá'ís were killed. Bahá'í schools, such as the Tarbiyat boys' and girls' schools in Tehran, were closed in the 1930s and 1940s, Bahá'í marriages were not recognized and Bahá'í texts were censored.\n\nDuring the reign of Mohammad Reza Pahlavi, to divert attention from economic difficulties in Iran and from a growing nationalist movement, a campaign of persecution against the Bahá'ís was instituted. An approved and coordinated anti-Bahá'í campaign (to incite public passion against the Bahá'ís) started in 1955 and it included the spreading of anti-Bahá'í propaganda on national radio stations and in official newspapers. In the late 1970s the Shah's regime consistently lost legitimacy due to criticism that it was pro-Western. As the anti-Shah movement gained ground and support, revolutionary propaganda was spread which alleged that some of the Shah's advisors were Bahá'ís. Bahá'ís were portrayed as economic threats, and as supporters of Israel and the West, and societal hostility against the Bahá'ís increased.\n\nSince the Islamic Revolution of 1979 Iranian Bahá'ís have regularly had their homes ransacked or have been banned from attending university or from holding government jobs, and several hundred have received prison sentences for their religious beliefs, most recently for participating in study circles. Bahá'í cemeteries have been desecrated and property has been seized and occasionally demolished, including the House of Mírzá Buzurg, Bahá'u'lláh's father. The House of the Báb in Shiraz, one of three sites to which Bahá'ís perform pilgrimage, has been destroyed twice.\n\nAccording to a US panel, attacks on Bahá'ís in Iran increased under Mahmoud Ahmadinejad's presidency. The United Nations Commission on Human Rights revealed an October 2005 confidential letter from Command Headquarters of the Armed Forces of Iran ordering its members to identify Bahá'ís and to monitor their activities. Due to these actions, the Special Rapporteur of the United Nations Commission on Human Rights stated on 20 March 2006, that she \"also expresses concern that the information gained as a result of such monitoring will be used as a basis for the increased persecution of, and discrimination against, members of the Bahá'í faith, in violation of international standards. The Special Rapporteur is concerned that this latest development indicates that the situation with regard to religious minorities in Iran is, in fact, deteriorating.\n\nOn 14 May 2008, members of an informal body known as the \"Friends\" that oversaw the needs of the Bahá'í community in Iran were arrested and taken to Evin prison. The Friends court case has been postponed several times, but was finally underway on 12 January 2010. Other observers were not allowed in the court. Even the defence lawyers, who for two years have had minimal access to the defendants, had difficulty entering the courtroom. The chairman of the U.S. Commission on International Religious Freedom said that it seems that the government has already predetermined the outcome of the case and is violating international human rights law. Further sessions were held on 7 February 2010, 12 April 2010 and 12 June 2010. On 11 August 2010 it became known that the court sentence was 20 years imprisonment for each of the seven prisoners which was later reduced to ten years. After the sentence, they were transferred to Gohardasht prison. In March 2011 the sentences were reinstated to the original 20 years. On 3 January 2010, Iranian authorities detained ten more members of the Baha'i minority, reportedly including Leva Khanjani, granddaughter of Jamaloddin Khanjani, one of seven Baha'i leaders jailed since 2008 and in February, they arrested his son, Niki Khanjani.\n\nThe Iranian government claims that the Bahá'í Faith is not a religion, but is instead a political organization, and hence refuses to recognize it as a minority religion. However, the government has never produced convincing evidence supporting its characterization of the Bahá'í community. Also, the government's statements that Bahá'ís who recanted their religion would have their rights restored, attest to the fact that Bahá'ís are persecuted solely for their religious affiliation. The Iranian government also accuses the Bahá'í Faith of being associated with Zionism because the Bahá'í World Centre is located in Haifa, Israel. These accusations against the Bahá'ís have no basis in historical fact, and the accusations are used by the Iranian government to use the Bahá'ís as \"scapegoats\". In fact it was the Iranian leader Naser al-Din Shah Qajar who banished Bahá'u'lláh from Persia to the Ottoman Empire and Bahá'u'lláh was later exiled by the Ottoman Sultan, at the behest of the Persian Shah, to territories further away from Iran and finally to Acre in Syria, which only a century later was incorporated into the state of Israel.\n\nBahá'í institutions and community activities have been illegal under Egyptian law since 1960. All Bahá'í community properties, including Bahá'í centers, libraries, and cemeteries, have been confiscated by the government and fatwas have been issued charging Bahá'ís with apostasy.\n\nThe Egyptian identification card controversy began in the 1990s when the government modernized the electronic processing of identity documents, which introduced a de facto requirement that documents must list the person's religion as Muslim, Christian, or Jewish (the only three religions officially recognized by the government). Consequently, Bahá'ís were unable to obtain government identification documents (such as national identification cards, birth certificates, death certificates, marriage or divorce certificates, or passports) necessary to exercise their rights in their country unless they lied about their religion, which conflicts with Bahá'í religious principle. Without documents, they could not be employed, educated, treated in hospitals, travel outside of the country, or vote, among other hardships. Following a protracted legal process culminating in a court ruling favorable to the Bahá'ís, the interior minister of Egypt released a decree on 14 April 2009, amending the law to allow Egyptians who are not Muslim, Christian, or Jewish to obtain identification documents that list a dash in place of one of the three recognized religions. The first identification cards were issued to two Bahá'ís under the new decree on 8 August 2009.\n\n\n\n"}
{"id": "4257", "url": "https://en.wikipedia.org/wiki?curid=4257", "title": "Burgundians", "text": "Burgundians\n\nThe Burgundians (; ; ; ) were a large East Germanic or Vandal tribe, or group of tribes, who lived in the area of modern Poland in the time of the Roman Empire.\n\nIn the late Roman period, as the empire came under pressure from many such \"barbarian\" peoples, a powerful group of Burgundians and other Vandalic tribes moved westwards towards the Roman frontiers along the Rhine Valley, making them neighbors of the Franks who formed their kingdoms to the north, and the Suebic Alemanni who were settling to their south, also near the Rhine. They established themselves in Worms, but with Roman cooperation their descendants eventually established the Kingdom of the Burgundians much further south, and within the empire, in the western Alps region where modern Switzerland, France and Italy meet. This later became a component of the Frankish empire. The name of this Kingdom survives in the regional appellation, Burgundy, which is a region in modern France, representing only a part of that kingdom.\n\nAnother part of the Burgundians stayed in their previous homeland in the Oder-Vistula basin and formed a contingent in Attila's Hunnic army by 451.\n\nBefore clear documentary evidence begins, the Burgundians may have originally emigrated from mainland Scandinavia to the Baltic island of Bornholm, and from there to the Vistula basin, in the middle of modern Poland.\n\nThe ethnonym Burgundians is commonly used in English to refer to the \"Burgundi\" (\"Burgundionei\", \"Burgundiones\" or \"Burgunds\") who settled in Sapaudia (Savoy), in the western Alps, during the 5th Century. The original Kingdom of the Burgundians barely intersected the modern \"Bourgogne\" and more closely matched the boundaries of the Arpitan or Romand (Franco-Provencal) language area, centred on the \"Rôno-Arpes\" (Rhône-Alpes) region of France, Romandy in west Switzerland and \" Val d'Outa\" (Val d'Aosta), in north west Italy.\n\nIn modern usage, however, \"Burgundians\" can sometimes refer to later inhabitants of the geographical \"Bourgogne\" or \"Borgogne\" (Burgundy), named after the old kingdom, but not corresponding to the original boundaries of it. Between the 6th and 20th centuries, the boundaries and political connections of \"Burgundy\" have changed frequently. In modern times the only area still referred to as Burgundy is in France, which derives its name from the Duchy of Burgundy. But in the context of the middle ages the term Burgundian (or similar spellings) can refer even to the powerful political entity the Dukes controlled which included not only Burgundy itself but had actually expanded to have a strong association with areas now in modern Belgium. The parts of the old Kingdom not within the French controlled Duchy tended to come under different names, except for the County of Burgundy.\n\nThe Burgundians had a tradition of Scandinavian origin which finds support in place-name evidence and archaeological evidence (Stjerna) and many consider their tradition to be correct (e.g. Musset, p. 62). The Burgundians are believed to have then emigrated to the Baltic island of Bornholm (\"the island of the Burgundians\" in Old Norse). However, by about 250 CE, the population of Bornholm had largely disappeared from the island. Most cemeteries ceased to be used, and those that were still used had few burials (Stjerna, in Nerman 1925:176). In \"Þorsteins saga Víkingssonar\" (\"The Saga of Thorstein, Viking's Son\"), the Veseti settled in an island or holm, which was called Borgund's holm, i.e. Bornholm. Alfred the Great's translation of \"Orosius\" uses the name \"Burgenda land\" to refer to a territory next to the land of Sweons (\"Swedes\"). The poet and early mythologist Viktor Rydberg (1828–1895), (\"Our Fathers' Godsaga\") asserted from an early medieval source, \"Vita Sigismundi\", that they themselves retained oral traditions about their Scandinavian origin.\n\nEarly Roman sources, such as Tacitus and Pliny the Elder, knew little concerning the Germanic peoples east of the Elbe river, or on the Baltic Sea. Pliny (IV.28) however mentions them among the Vandalic or Eastern Germanic Germani peoples, including also the Goths. Claudius Ptolemy lists them as living between the Suevus (probably the Oder) and Vistula rivers, north of the Lugii, and south of the coast dwelling tribes. Around the mid 2nd century AD, there was a significant migration by Germanic tribes of Scandinavian origin (Rugii, Goths, Gepidae, Vandals, Burgundians, and others) towards the south-east, creating turmoil along the entire Roman frontier. These migrations culminated in the Marcomannic Wars, which resulted in widespread destruction and the first invasion of Italy in the Roman Empire period. Jordanes reports that during the 3rd century, the Burgundians living in the Vistula basin were almost annihilated by Fastida, king of the Gepids, whose kingdom was at the mouth of the Vistula.\n\nIn the late 3rd century, the Burgundians appear on the east bank of the Rhine, confronting Roman Gaul. Zosimus (1.68) reports them being defeated by the emperor Probus in 278 in Gaul. At this time, they were led by a Vandal king. A few years later, Claudius Mamertinus mentions them along with the Alamanni, a Suebic people. These two peoples had moved into the Agri Decumates on the eastern side of the Rhine, an area today referred to still as Swabia, at times attacking Roman Gaul together and sometimes fighting each other. He also mentions that the Goths had previously defeated the Burgundians.\n\nAmmianus Marcellinus, on the other hand, claimed that the Burgundians were descended from Romans. The Roman sources do not speak of any specific migration from Poland by the Burgundians (although other Vandalic peoples are more clearly mentioned as having moved west in this period), and so there have historically been some doubts about the link between the eastern and western Burgundians.\n\nIn 369/370, the Emperor Valentinian I enlisted the aid of the Burgundians in his war against the Alemanni.\n\nApproximately four decades later, the Burgundians appear again. Following Stilicho's withdrawal of troops to fight Alaric I the Visigoth in AD 406-408, the northern tribes crossed the Rhine and entered the Empire in the \"Völkerwanderung\", or Germanic migrations. Among them were the Alans, Vandals, the Suevi, and possibly some Burgundians. Some Burgundians migrated westwards and settled as \"foederati\" in the Roman province of Germania Secunda along the Middle Rhine. Other Burgundians stayed in their previous homeland in Oder-Vistula interfluvial and formed a contingent in Attila's Hunnic army by 451.\n\nIn 411, the Burgundian king Gundahar (or \"Gundicar\") set up a puppet emperor, Jovinus, in cooperation with Goar, king of the Alans. With the authority of the Gallic emperor that he controlled, Gundahar settled on the left (Roman) bank of the Rhine, between the river Lauter and the Nahe, seizing Worms, Speyer, and Strassburg. Apparently as part of a truce, the Emperor Honorius later officially \"granted\" them the land, (Prosper, a. 386) with its capital at the old Celtic Roman settlement of Borbetomagus (present Worms).\n\nDespite their new status as \"foederati\", Burgundian raids into Roman Upper Gallia Belgica became intolerable and were ruthlessly brought to an end in 436, when the Roman general Aëtius called in Hun mercenaries, who overwhelmed the Rhineland kingdom in 437. Gundahar was killed in the fighting, reportedly along with the majority of the Burgundian tribe. (Prosper; \"Chronica Gallica 452\"; Hydatius; and Sidonius Apollinaris)\n\nThe destruction of Worms and the Burgundian kingdom by the Huns became the subject of heroic legends that were afterwards incorporated in the \"Nibelungenlied\"—on which Wagner based his Ring Cycle—where King Gunther (Gundahar) and Queen Brünhild hold their court at Worms, and Siegfried comes to woo Kriemhild. (In Old Norse sources the names are \"Gunnar\", \"Brynhild\", and \"Gudrún\" as normally rendered in English.) In fact, the \"Etzel\" of the \"Nibelungenlied\" is based on Attila the Hun.\n\nFor reasons not cited in the sources, the Burgundians were granted \"foederati\" status a second time, and in 443 were resettled by Aëtius in the region of \"Sapaudia\". (\"Chronica Gallica 452\") Though the precise geography is uncertain, \"Sapaudia\" corresponds to the modern-day Savoy, and the Burgundians probably lived near \"Lugdunum\", known today as Lyon. (Wood 1994, Gregory II, 9) A new king Gundioc or \"Gunderic\", presumed to be Gundahar's son, appears to have reigned following his father's death. (Drew, p. 1) The historian Pline tells us that Gonderic reigned the areas of Saône, Dauphiny, Savoie and a part of Provence. He set up Vienne as the capital of the kingdom of Burgundy. In all, eight Burgundian kings of the house of Gundahar ruled until the kingdom was overrun by the Franks in 534.\n\nAs allies of Rome in its last decades, the Burgundians fought alongside Aëtius and a confederation of Visigoths and others in the battle against Attila at the Battle of Châlons (also called \"The Battle of the Catalaunian Fields\") in 451. The alliance between Burgundians and Visigoths seems to have been strong, as Gundioc and his brother Chilperic I accompanied Theodoric II to Spain to fight the Sueves in 455. (Jordanes, \"Getica\", 231)\n\nAlso in 455, an ambiguous reference \"infidoque tibi Burdundio ductu\" (Sidonius Apollinaris in \"Panegyr. Avit\". 442.) implicates an unnamed treacherous Burgundian leader in the murder of the emperor Petronius Maximus in the chaos preceding the sack of Rome by the Vandals. The Patrician Ricimer is also blamed; this event marks the first indication of the link between the Burgundians and Ricimer, who was probably Gundioc's brother-in-law and Gundobad's uncle, (John Malalas, 374)\n\nIn 456, the Burgundians, apparently confident in their growing power, negotiated a territorial expansion and power sharing arrangement with the local Roman senators. (Marius of Avenches)\n\nIn 457, Ricimer overthrew another emperor, Avitus, raising Majorian to the throne. This new emperor proved unhelpful to Ricimer and the Burgundians. The year after his ascension, Majorian stripped the Burgundians of the lands they had acquired two years earlier. After showing further signs of independence, he was murdered by Ricimer in 461.\n\nTen years later, in 472, Ricimer–who was by now the son-in-law of the Western Emperor Anthemius–was plotting with Gundobad to kill his father-in-law; Gundobad beheaded the emperor (apparently personally). (\"Chronica Gallica 511\"; John of Antioch, fr. 209; Jordanes, \"Getica\", 239) Ricimer then appointed Olybrius; both died, surprisingly of natural causes, within a few months. Gundobad seems then to have succeeded his uncle as Patrician and king-maker, and raised Glycerius to the throne. (Marius of Avenches; John of Antioch, fr. 209)\n\nIn 474, Burgundian influence over the empire seems to have ended. Glycerius was deposed in favor of Julius Nepos, and Gundobad returned to Burgundy, presumably at the death of his father Gundioc. At this time or shortly afterwards, the Burgundian kingdom was divided between Gundobad and his brothers, Godigisel, Chilperic II, and Gundomar I. (Gregory, II, 28)\n\nAccording to Gregory of Tours, the years following Gundobad's return to Burgundy saw a bloody consolidation of power. Gregory states that Gundobad murdered his brother Chilperic, drowning his wife and exiling their daughters (one of whom was to become the wife of Clovis the Frank, and was reputedly responsible for his conversion). This is contested by, e.g., Bury, who points out problems in much of Gregory's chronology for the events.\n\nC.500, when Gundobad and Clovis were at war, Gundobad appears to have been betrayed by his brother Godegisel, who joined the Franks; together Godegisel's and Clovis' forces \"crushed the army of Gundobad\". (Marius a. 500; Gregory, II, 32) Gundobad was temporarily holed up in Avignon, but was able to re-muster his army and sacked Vienne, where Godegisel and many of his followers were put to death. From this point, Gundobad appears to have been the sole king of Burgundy. (e.g., Gregory, II, 33) This would imply that his brother Gundomar was already dead, though there are no specific mentions of the event in the sources.\n\nEither Gundobad and Clovis reconciled their differences, or Gundobad was forced into some sort of vassalage by Clovis' earlier victory, as the Burgundian king appears to have assisted the Franks in 507 in their victory over Alaric II the Visigoth.\n\nDuring the upheaval, sometime between 483-501, Gundobad began to set forth the \"Lex Gundobada\" (see below), issuing roughly the first half, which drew upon the \"Lex Visigothorum\". (Drew, p. 1) Following his consolidation of power, between 501 and his death in 516, Gundobad issued the second half of his law, which was more originally Burgundian.\n\nThe Burgundians were extending their power over southeastern Gaul; that is, northern Italy, western Switzerland, and southeastern France. In 493, Clovis, king of the Franks, married the Burgundian princess Clotilda (daughter of Chilperic), who converted him to the Catholic faith.\n\nAt first allied with Clovis' Franks against the Visigoths in the early 6th century, the Burgundians were eventually conquered at Autun by the Franks in 532 after a first attempt in the Battle of Vézeronce. The Burgundian kingdom was made part of the Merovingian kingdoms, and the Burgundians themselves were by and large absorbed as well.\nThe 5th century Gallo-Roman poet and landowner Sidonius, who at one point lived with the Burgundians, described them as a long-haired people of immense physical size:\n\nThe Burgundian language belonged to the East Germanic language group. It appears to have become extinct during the late sixth century.\n\nLittle is known of the language. Some proper names of Burgundians are recorded, and some words used in the area in modern times are thought to be derived from the ancient Burgundian language, but it is often difficult to distinguish these from Germanic words of other origin, and in any case the modern form of the words is rarely suitable to infer much about the form in the old language.\n\nSomewhere in the east the Burgundians had converted to the Arian form of Christianity from their native Germanic polytheism. Their Arianism proved a source of suspicion and distrust between the Burgundians and the Catholic Western Roman Empire. Divisions were evidently healed or healing circa AD 500, however, as Gundobad, one of the last Burgundian kings, maintained a close personal friendship with Avitus, the bishop of Vienne. Moreover, Gundobad's son and successor, Sigismund, was himself a Catholic, and there is evidence that many of the Burgundian people had converted by this time as well, including several female members of the ruling family.\n\nThe Burgundians left three legal codes, among the earliest from any of the Germanic tribes.\n\nThe Liber Constitutionum sive Lex Gundobada (\"The Book of Constitutions or Law of Gundobad\"), also known as the \"Lex Burgundionum\", or more simply the \"Lex Gundobada\" or the \"Liber\", was issued in several parts between 483 and 516, principally by Gundobad, but also by his son, Sigismund. (Drew, p. 6–7) It was a record of Burgundian customary law and is typical of the many Germanic law codes from this period. In particular, the \"Liber\" borrowed from the \"Lex Visigothorum\" (Drew, p. 6) and influenced the later \"Lex Ribuaria\". (Rivers, p. 9) The \"Liber\" is one of the primary sources for contemporary Burgundian life, as well as the history of its kings.\n\nLike many of the Germanic tribes, the Burgundians' legal traditions allowed the application of separate laws for separate ethnicities. Thus, in addition to the \"Lex Gundobada\", Gundobad also issued (or codified) a set of laws for Roman subjects of the Burgundian kingdom, the \"Lex Romana Burgundionum\" (\"The Roman Law of the Burgundians\").\n\nIn addition to the above codes, Gundobad's son Sigismund later published the \"Prima Constitutio\".\n\n\n"}
{"id": "4260", "url": "https://en.wikipedia.org/wiki?curid=4260", "title": "Dots and Boxes", "text": "Dots and Boxes\n\nDots and Boxes is a pencil-and-paper game for two players (sometimes more). It was first published in the 19th century by Édouard Lucas, who called it la pipopipette. It has gone by many other names, including the game of dots, boxes, dot to dot grid, and pigs in a pen.\nStarting with an empty grid of dots, two players take turns adding a single horizontal or vertical line between two \"unjoined\" adjacent dots. The player who completes the fourth side of a 1×1 box earns one point and takes another turn. (A point is typically recorded by placing a mark that identifies the player in the box, such as an initial). The game ends when no more lines can be placed. The winner is the player with the most points. The board may be of any size. When short on time, a 2×2 board (a square of 9 dots) is good for beginners. A 5×5 is good for experts.\n\nThe diagram on the right shows a game being played on the 2×2 board. The second player (B) plays the mirror image of the first player's move, hoping to divide the board into two pieces and tie the game. But the first player (A) makes a \"sacrifice\" at move 7 and B accepts the sacrifice, getting one box. However, B must now add another line, and connects the center dot to the center-right dot, causing the remaining boxes to be joined together in a \"chain\" (shown at the end of move 8). With A's next move, player A gets them all and wins 3–1.\n\nFor most novice players, the game begins with a phase of more-or-less randomly connecting dots, where the only strategy is to avoid adding the third side to any box. This continues until all the remaining (potential) boxes are joined together into \"chains\" – groups of one or more adjacent boxes in which any move gives all the boxes in the chain to the opponent. At this point, players typically take all available boxes, then \"open\" the smallest available chain to their opponent. For example, a novice player faced with a situation like position 1 in the diagram on the right, in which some boxes can be captured, may take all the boxes in the chain, resulting in position 2. But, with their last move, they have to open the next, larger chain, and the novice loses the game.\n\nA more \"experienced\" player faced with position 1 will instead play the \"double-cross strategy\", taking all but 2 of the boxes in the chain and leaving position 3. The opponent will take these two boxes and then be forced to open the next chain. By achieving position 3, player A wins. The same double-cross strategy applies no matter how many long chains there are: a player using this strategy will take all but two boxes in each chain and take all the boxes in the last chain. If the chains are long enough, then this player will win.\n\nThe next level of strategic complexity, between \"experts\" who would both use the double-cross strategy (if they were allowed to), is a battle for \"control\": An expert player tries to force their opponent \"to open the first long chain\", because the player who first opens a long chain usually loses. Against a player who doesn't understand the concept of a sacrifice, the expert simply has to make the correct number of sacrifices to encourage the opponent to hand him the first chain long enough to ensure a win. \"If\" the other player also sacrifices, the expert has to additionally manipulate the number of available sacrifices through earlier play.\n\nIn combinatorial game theory, dots and boxes is an impartial game and many positions can be analyzed using Sprague–Grundy theory. However, Dots and Boxes lacks the normal play convention of most impartial games (where the last player to move wins), which complicates the analysis considerably.\n\nDots and Boxes need not be played on a rectangular grid – it can be played on a triangular grid or a hexagonal grid. There is also a variant in Bolivia where it is played in a Chakana or Inca Cross grid, which adds more complications to the game.\n\nDots and Boxes has a dual graph form called \"Strings-and-Coins\". This game is played on a network of coins (vertices) joined by strings (edges). Players take turns cutting a string. When a cut leaves a coin with no strings, the player \"pockets\" the coin and takes another turn. The winner is the player who pockets the most coins. Strings-and-Coins can be played on an arbitrary graph.\n\nA variant played in Poland allows a player to claim a region of several squares as soon as its boundary is completed. In the Netherlands, it is called \"kamertje verhuren\" (\"Rent-a-Room\") and the outer border already has lines. In analyses of Dots and Boxes, starting with outer lines is called a \"Swedish board\" while the standard version is called an \"American board\". An intermediate version with the outer left and bottom sides starting with lines is called an \"Icelandic board\".\n\n"}
{"id": "4261", "url": "https://en.wikipedia.org/wiki?curid=4261", "title": "Big Brother (Nineteen Eighty-Four)", "text": "Big Brother (Nineteen Eighty-Four)\n\nBig Brother is a fictional character and symbol in George Orwell's novel \"Nineteen Eighty-Four\". He is ostensibly the leader (most likely a symbolic figurehead) of Oceania, a totalitarian state wherein the ruling Party wields total power \"for its own sake\" over the inhabitants.\n\nIn the society that Orwell describes, every citizen is under constant surveillance by the authorities, mainly by telescreens (with the exception of the Proles). The people are constantly reminded of this by the slogan \"Big Brother is watching you\": a maxim which is ubiquitously on display. In modern culture the term \"Big Brother\" has entered the lexicon as a synonym for abuse of government power, particularly in respect to civil liberties, often specifically related to mass surveillance.\n\nIn the essay section of his novel \"1985\", Anthony Burgess states that Orwell got the idea for the name of Big Brother from advertising billboards for educational correspondence courses from a company called \"Bennett's\", current during World War II. The original posters showed J. M. Bennett himself: a kindly-looking old man offering guidance and support to would-be students with the phrase \"Let me be your father\" attached. According to Burgess, after Bennett's death, his son took over the company, and the posters were replaced with pictures of the son (who looked imposing and stern in contrast to his father's kindly demeanor) with the text \"Let me be your big brother.\"\n\nAdditional speculation from Douglas Kellner of UCLA argued that Big Brother represents Joseph Stalin.\nAnother theory is that the inspiration for Big Brother was Brendan Bracken, the Minister of Information until 1945. Orwell worked under Bracken on the BBC's Indian Service. Bracken was customarily referred to by MOI employees by his initials, B.B., the same initials as the character Big Brother. Orwell also resented the wartime censorship and need to manipulate information which he felt came from the highest levels of the MOI and from Bracken's office in particular.\n\nIn the novel, it is never made clear whether Big Brother is or had been a real person, or is a fictional personification of the Party, similar to Britannia and Uncle Sam. Big Brother is described as appearing on posters and telescreens as a handsome man in his mid-40s.\n\nIn Party propaganda, Big Brother is presented as one of the founders of the Party, along with Goldstein. At one point, Winston Smith, the protagonist of Orwell's novel, tries \"to remember in what year he had first heard mention of Big Brother. He thought it must have been at some time in the sixties, but it was impossible to be certain. In the Party histories, of course, Big Brother figured as the leader and guardian of the Revolution since its very earliest days. His exploits had been gradually pushed backwards in time until already they extended into the fabulous world of the forties and the thirties, when the capitalists in their strange cylindrical hats still rode through the streets of London . . .\"\n\nIn the book \"The Theory and Practice of Oligarchical Collectivism\", read by Winston Smith and purportedly written by Goldstein, Big Brother is referred to as infallible and all-powerful. No-one has ever seen him and there is a reasonable certainty that he will never die. He is simply \"the guise in which the Party chooses to exhibit itself to the world\", since the emotions of love, fear and reverence are more easily focused on an individual (if only a face on the hoardings and a voice on the telescreens), than an organisation. When Winston Smith is later arrested, O'Brien repeats that Big Brother will never die. When Smith asks if Big Brother exists, O'Brien describes him as \"the embodiment of the Party\" and says that he will exist as long as the Party exists. When Winston asks \"Does Big Brother exist the same way I do?\" (meaning is Big Brother an actual human being), O'Brien replies \"You do not exist\" (meaning that Smith is now an unperson; an example of doublethink).\n\nA spontaneous ritual of devotion to Big Brother (\"BB\") is illustrated at the end of the \"Two Minutes Hate\":\n\nThough Oceania's Ministry of Truth, Ministry of Plenty, and Ministry of Peace each have names with meanings deliberately opposite to their real purpose, the Ministry of Love is perhaps the most straightforward: \"rehabilitated thought criminals\" leave the Ministry as loyal subjects who have been brainwashed into adoring (loving) Big Brother, hence its name.\n\nSince the publication of \"Nineteen Eighty-Four\" the phrase \"Big Brother\" has come into common use to describe any prying or overly-controlling authority figure, and attempts by government to increase surveillance.\n\nBig Brother and other Orwellian imagery are often referenced in the type of joke known as the Russian reversal.\n\nThe magazine \"Book\" ranked Big Brother No. 59 on its 100 Best Characters in Fiction Since 1900 list. \"Wizard\" magazine rated him the 75th greatest villain of all time.\n\nThe worldwide reality television show \"Big Brother\" is based on the novel's concept of people being under constant surveillance. In 2000, after the U.S. version of the CBS program \"Big Brother\" premiered, the Estate of George Orwell sued CBS and its production company \"Orwell Productions, Inc.\" in federal court in Chicago for copyright and trademark infringement. The case was \"Estate of Orwell v. CBS\", 00-c-5034 (ND Ill). On the eve of trial, the case settled worldwide to the parties' \"mutual satisfaction\"; the amount that CBS paid to the Orwell Estate was not disclosed. CBS had not asked the Estate for permission. Under current laws the novel will remain under copyright protection until 2020 in the European Union and until 2044 in the United States.\n\nThe iconic image of Big Brother (played by David Graham) played a key role in Apple's 1984 television commercial introducing the Macintosh. The Orwell Estate viewed the Apple commercial as a copyright infringement, and sent a cease-and-desist letter to Apple and its advertising agency. The commercial was never televised again, though the date mentioned in the ad (January 24) was but two days later, making it unlikely that it would have been re-aired. Subsequent (now posthumous) ads featuring Steve Jobs (for a variety of products including audio books) have mimicked the format and appearance of that original ad campaign, with the appearance of Steve Jobs nearly identical to that of Big Brother. In 2008, the Simpsons animated television series spoofed the Apple Big Brother commercial in an episode entitled \"Mypods and Boomsticks.\"\n\nThe December 2002 issue of \"Gear\" magazine featured a story about technologies and trends that could violate personal privacy moving society closer to a \"Big Brother\" state and utilised a recreation of the movie poster from the film version of \"1984\" created by Dallmeierart.com.\n\nComputer company Microsoft patented in 2011 a product distribution system with a camera or capture device that monitors the viewers that consume the product, allowing the provider to take \"remedial action\" if the actual viewers do not match the distribution license. The system has been compared with \"1984\"'s telescreen surveillance system.\n\nA series of laws intended to implement the EU Data Retention Directive in Romania were nicknamed \"the Big Brother laws\" by Romanian media and civil society, as they would have led to blanket storage of citizens' telecommunications data for 6 months. All of these laws were struck down as unconstitutional by the Constitutional Court of Romania, and the Directive itself was ultimately invalidated by the Court of Justice of the European Union.\n\n"}
{"id": "4266", "url": "https://en.wikipedia.org/wiki?curid=4266", "title": "Binary search algorithm", "text": "Binary search algorithm\n\nIn computer science, binary search, also known as half-interval search, logarithmic search, or binary chop, is a search algorithm that finds the position of a target value within a sorted array. Binary search compares the target value to the middle element of the array; if they are unequal, the half in which the target cannot lie is eliminated and the search continues on the remaining half until it is successful. If the search ends with the remaining half being empty, the target is not in the array.\n\nBinary search runs in at worst logarithmic time, making comparisons, where is the number of elements in the array, the is Big O notation, and is the logarithm. Binary search takes constant () space, meaning that the space taken by the algorithm is the same for any number of elements in the array. Although specialized data structures designed for fast searching—such as hash tables—can be searched more efficiently, binary search applies to a wider range of problems.\n\nAlthough the idea is simple, implementing binary search correctly requires attention to some subtleties about its exit conditions and midpoint calculation.\n\nThere are numerous variations of binary search. In particular, fractional cascading speeds up binary searches for the same value in multiple arrays, efficiently solving a series of search problems in computational geometry and numerous other fields. Exponential search extends binary search to unbounded lists. The binary search tree and B-tree data structures are based on binary search.\n\nBinary search works on sorted arrays. Binary search begins by comparing the middle element of the array with the target value. If the target value matches the middle element, its position in the array is returned. If the target value is less than or greater than the middle element, the search continues in the lower or upper half of the array, respectively, eliminating the other half from consideration.\n\nGiven an array \"A\" of \"n\" elements with values or records \"A\", \"A\", ..., \"A\", sorted such that \"A\" ≤ \"A\" ≤ ... ≤ \"A\", and target value \"T\", the following subroutine uses binary search to find the index of \"T\" in \"A\".\n\nThis iterative procedure keeps track of the search boundaries with two variables. Some implementations may check whether the middle element is equal to the target at the end of the procedure. This results in a faster comparison loop, but requires one more iteration on average.\n\nThe above procedure only performs \"exact\" matches, finding the position of a target value. However, due to the ordered nature of sorted arrays, it is trivial to extend binary search to perform approximate matches. For example, binary search can be used to compute, for a given value, its rank (the number of smaller elements), predecessor (next-smallest element), successor (next-largest element), and nearest neighbor. Range queries seeking the number of elements between two values can be performed with two rank queries.\n\nThe performance of binary search can be analyzed by reducing the procedure to a binary comparison tree, where the root node is the middle element of the array; the middle element of the lower half is left of the root and the middle element of the upper half is right of the root. The rest of the tree is built in a similar fashion. This model represents binary search; starting from the root node, the left or right subtrees are traversed depending on whether the target value is less or more than the node under consideration, representing the successive elimination of elements.\n\nThe worst case is formula_1 iterations of the comparison loop, where the formula_2 notation denotes the floor function that rounds its argument to the next-smallest integer and is the binary logarithm. The worst case is reached when the search reaches the deepest level of the tree, equivalent to a binary search that has reduced to one element and, in each iteration, always eliminates the smaller subarray out of the two if they are not of equal size.\n\nOn average, assuming that each element is equally likely to be searched, the procedure will most likely find the target value the second-deepest level of the tree. This is equivalent to a binary search that completes one iteration before the worst case, reached after formula_3 iterations. However, the tree may be unbalanced, with the deepest level partially filled, and equivalently, the array may not be divided perfectly by the search in some iterations, half of the time resulting in the smaller subarray being eliminated. The actual number of average iterations is slightly higher, at formula_4 iterations. In the best case, where target value is the middle element of the array, its position is returned after one iteration. In terms of iterations, no search algorithm that works only by comparing elements can exhibit better average and worst-case performance than binary search.\n\nEach iteration of the binary search procedure defined above makes one or two comparisons, checking if the middle element is equal to the target in each iteration. Again assuming that each element is equally likely to be searched, each iteration makes 1.5 comparisons on average. A variation of the algorithm checks whether the middle element is equal to the target at the end of the search, eliminating on average half a comparison from each iteration. This slightly cuts the time taken per iteration on most computers, while guaranteeing that the search takes the maximum number of iterations, on average adding one iteration to the search. Because the comparison loop is performed only formula_1 times in the worst case, for all but enormous formula_6, the slight increase in comparison loop efficiency does not compensate for the extra iteration. gives a value of formula_7 (more than 73 quintillion) elements for this variation to be faster.\nFractional cascading can be used to speed up searches of the same value in multiple arrays. Where formula_8 is the number of arrays, searching each array for the target value takes formula_9 time; fractional cascading reduces this to formula_10.\n\nSorted arrays with binary search are a very inefficient solution when insertion and deletion operations are interleaved with retrieval, taking formula_11 time for each such operation, and complicating memory use. Other data structures support much more efficient insertion and deletion, and also fast exact matching. However, binary search applies to a wide range of search problems, usually solving them in formula_12 time regardless of the type or structure of the values themselves.\n\nFor implementing associative arrays, hash tables, a data structure that maps keys to records using a hash function, are generally faster than binary search on a sorted array of records; most implementations require only amortized constant time on average. However, hashing is not useful for approximate matches, such as computing the next-smallest, next-largest, and nearest key, as the only information given on a failed search is that the target is not present in any record. Binary search is ideal for such matches, performing them in logarithmic time. In addition, all operations possible on a sorted array can be performed—such as finding the smallest and largest key and performing range searches.\n\nA binary search tree is a binary tree data structure that works based on the principle of binary search. The records of the tree are arranged in sorted order, and each record in the tree can be searched using an algorithm similar to binary search, taking on average logarithmic time. Insertion and deletion also require on average logarithmic time in binary search trees. This can faster than the linear time insertion and deletion of sorted arrays, and binary trees retain the ability to perform all the operations possible on a sorted array, including range and approximate queries.\n\nHowever, binary search is usually more efficient for searching as binary search trees will most likely be imperfectly balanced, resulting in slightly worse performance than binary search. This applies even to balanced binary search trees, binary search trees that balance their own nodes—as they rarely produce \"optimally\"-balanced trees—but to a lesser extent. Although unlikely, the tree may be severely imbalanced with few internal nodes with two children, resulting in the average and worst-case search time approaching formula_6 comparisons. Binary search trees take more space than sorted arrays.\n\nBinary search trees lend themselves to fast searching in external memory stored in hard disks, as binary search trees can effectively be structured in filesystems. The B-tree generalizes this method of tree organization; B-trees are frequently used to organize long-term storage such as databases and filesystems.\n\nLinear search is a simple search algorithm that checks every record until it finds the target value. Linear search can be done on a linked list, which allows for faster insertion and deletion than an array. Binary search is faster than linear search for sorted arrays except if the array is short. If the array must first be sorted, that cost must be amortized over any searches. Sorting the array also enables efficient approximate matches and other operations.\n\nThe Judy array uses a combination of approaches to provide a highly efficient solution.\n\nA related problem to search is set membership. Any algorithm that does lookup, like binary search, can also be used for set membership. There are other algorithms that are more specifically suited for set membership. A bit array is the simplest, useful when the range of keys is limited; it is very fast, requiring only formula_14 time. The Judy1 type of Judy array handles 64-bit keys efficiently.\n\nFor approximate results, Bloom filters, another probabilistic data structure based on hashing, store a set of keys by encoding the keys using a bit array and multiple hash functions. Bloom filters are much more space-efficient than bitarrays in most cases and not much slower: with formula_8 hash functions, membership queries require only formula_16 time. However, Bloom filters suffer from false positives.\n\nThere exist data structures that may improve on binary search in some cases for both searching and other operations available for sorted arrays. For example, searches, approximate matches, and the operations available to sorted arrays can be performed more efficiently than binary search on specialized data structures such as van Emde Boas trees, fusion trees, tries, and bit arrays. However, while these operations can always be done at least efficiently on a sorted array regardless of the keys, such data structures are usually only faster because they exploit the properties of keys with a certain attribute (usually keys that are small integers), and thus will be time or space consuming for keys that lack that attribute.\n\nUniform binary search stores, instead of the lower and upper bounds, the index of the middle element and the number of elements around the middle element that were not eliminated yet. Each step reduces the width by about half. This variation is \"uniform\" because the difference between the indices of middle elements and the preceding middle elements chosen remains constant between searches of arrays of the same length.\n\nFibonacci search is a method similar to binary search that successively shortens the interval in which the maximum of a unimodal function lies. Given a finite interval, a unimodal function, and the maximum length of the resulting interval, Fibonacci search finds a Fibonacci number such that if the interval is divided equally into that many subintervals, the subintervals would be shorter than the maximum length. After dividing the interval, it eliminates the subintervals in which the maximum cannot lie until one or more contiguous subintervals remain.\n\nExponential search extends binary search to unbounded lists. It starts by finding the first element with an index that is both a power of two and greater than the target value. Afterwards, it sets that index as the upper bound, and switches to binary search. A search takes formula_17 iterations of the exponential search and at most formula_18 iterations of the binary search, where formula_19 is the position of the target value. Exponential search works on bounded lists, but becomes an improvement over binary search only if the target value lies near beginning of the array.\n\nInstead of merely calculating the midpoint, interpolation search estimates the position of the target value, taking into account the lowest and highest elements in the array and the length of the array. This is only possible if the array elements are numbers. It works on the basis that the midpoint is not the best guess in many cases; for example, if the target value is close to the highest element in the array, it is likely to be located near the end of the array. When the distribution of the array elements is uniform or near uniform, it makes formula_20 comparisons.\n\nIn practice, interpolation search is slower than binary search for small arrays, as interpolation search requires extra computation, and the slower growth rate of its time complexity compensates for this only for large arrays.\n\nFractional cascading is a technique that speeds up binary searches for the same element for both exact and approximate matching in \"catalogs\" (arrays of sorted elements) associated with vertices in graphs. Searching each catalog separately requires formula_9 time, where formula_8 is the number of catalogs. Fractional cascading reduces this to formula_10 by storing specific information in each catalog about other catalogs.\n\nFractional cascading was originally developed to efficiently solve various computational geometry problems, but it also has been applied elsewhere, in domains such as data mining and Internet Protocol routing.\n\nIn 1946, John Mauchly made the first mention of binary search as part of the Moore School Lectures, the first ever set of lectures regarding any computer-related topic. Every published binary search algorithm worked only for arrays whose length is one less than a power of two until 1960, when Derrick Henry Lehmer published a binary search algorithm that worked on all arrays. In 1962, Hermann Bottenbruch presented an ALGOL 60 implementation of binary search that placed the comparison for equality at the end, increasing the average number of iterations by one, but reducing to one the number of comparisons per iteration. The uniform binary search was presented to Donald Knuth in 1971 by A. K. Chandra of Stanford University and published in Knuth's The Art of Computer Programming. In 1986, Bernard Chazelle and Leonidas J. Guibas introduced fractional cascading as a method to solve numerous search problems in computational geometry.\n\nAlthough the basic idea of binary search is comparatively straightforward, the details can be surprisingly tricky ... — Donald Knuth\n\nWhen Jon Bentley assigned binary search as a problem in a course for professional programmers, he found that ninety percent failed to provide a correct solution after several hours of working on it, and another study published in 1988 shows that accurate code for it is only found in five out of twenty textbooks. Furthermore, Bentley's own implementation of binary search, published in his 1986 book \"Programming Pearls\", contained an overflow error that remained undetected for over twenty years. The Java programming language library implementation of binary search had the same overflow bug for more than nine years.\n\nIn a practical implementation, the variables used to represent the indices will often be of fixed size, and this can result in an arithmetic overflow for very large arrays. If the midpoint of the span is calculated as (\"L\" + \"R\") / 2, then the value of \"L\" + \"R\" may exceed the range of integers of the data type used to store the midpoint, even if \"L\" and \"R\" are within the range. If \"L\" and \"R\" are nonnegative, this can be avoided by calculating the midpoint as \"L\" + (\"R\" − \"L\") / 2.\n\nIf the target value is greater than the greatest value in the array, and the last index of the array is the maximum representable value of \"L\", the value of \"L\" will eventually become too large and overflow. A similar problem will occur if the target value is smaller than the least value in the array and the first index of the array is the smallest representable value of \"R\". In particular, this means that \"R\" must not be an unsigned type if the array starts with index 0.\n\nAn infinite loop may occur if the exit conditions for the loop are not defined correctly. Once \"L\" exceeds \"R\", the search has failed and must convey the failure of the search. In addition, the loop must be exited when the target element is found, or in the case of an implementation where this check is moved to the end, checks for whether the search was successful or failed at the end must be in place. Bentley found that, in his assignment of binary search, most of the programmers who implemented binary search incorrectly made an error defining the exit conditions.\n\nMany languages' standard libraries include binary search routines:\n\n\n"}
{"id": "4267", "url": "https://en.wikipedia.org/wiki?curid=4267", "title": "Belle and Sebastian", "text": "Belle and Sebastian\n\nBelle and Sebastian are a Scottish band formed in Glasgow in January 1996. Led by Stuart Murdoch, the band has released 9 albums to date. Much of their work had been released on Jeepster Records, but they are now signed to Rough Trade Records in the United Kingdom and Matador Records in the United States. Though often praised by critics, Belle and Sebastian have enjoyed only limited commercial success.\n\nBelle and Sebastian were formed in Glasgow, Scotland in 1996 by Stuart Murdoch and Stuart David. Together, with Stow College music professor Alan Rankine, they recorded some demos, which were picked up by the college's Music Business course that produces and releases one single each year on the college's label, Electric Honey. As the band had a number of songs already and the label was extremely impressed with the demos, Belle and Sebastian were allowed to record a full-length album, which was titled \"Tigermilk\". Murdoch once described the band as a \"product of botched capitalism\". The band took their name from the television adaptation of the French novel \"Belle et Sébastien\".\n\n\"Tigermilk\" was recorded in three days and originally only one thousand copies were pressed in vinyl. These original copies now sell for up to £400. The warm reception the album received inspired Murdoch and David to turn the band into a full-time project, recruiting Stevie Jackson (guitar and vocals), Isobel Campbell (cello/vocals), Chris Geddes (keys) and Richard Colburn (drums) to fill out the group.\n\nAfter the success of the debut album, Belle and Sebastian were signed to Jeepster Records in August 1996 and \"If You're Feeling Sinister\", their second album, was released on 18 November. The album was named by \"Spin\" as one of the 100 greatest albums between 1985 and 2005, and is widely considered the band's masterpiece. Just before the recording of \"Sinister\", Sarah Martin (violin/vocals) joined the band. Following this a series of EPs were released in 1997. The first of these was \"Dog on Wheels\", which contained four demo tracks recorded before the real formation of the band. In fact, the only long-term band members to play on the songs were Murdoch, David, and Mick Cooke, who played trumpet on the EP but would not officially join the band until a few years later. It charted at No. 59 in the UK singles chart.\n\nThe \"Lazy Line Painter Jane\" EP followed in July. The track was recorded in the church where Murdoch lived and features vocals from Monica Queen. The EP narrowly missed out on the UK top 40, peaking at No. 41. The last of the 1997 EPs was October's \"3.. 6.. 9 Seconds of Light\". The EP was made Single of the Week in both the \"NME\" and \"Melody Maker\" and reached No. 32 in the charts, thus becoming the band's first top 40 single.\n\nThe band released their third LP, \"The Boy with the Arab Strap\" in 1998, and it reached No. 12 in the UK charts. \"Arab Strap\" garnered positive reviews from \"Rolling Stone\" and the \"Village Voice,\" among others; however, the album has its detractors, including \"Pitchfork\", who gave the album a particularly poor review, calling it a \"parody\" of their earlier work (Pitchfork has since removed the review from their website). During the recording of the album, long-time studio trumpet-player Mick Cooke was asked to join the band as a full member. The \"This Is Just a Modern Rock Song\" EP followed later that year.\n\nIn 1999 the band was awarded with Best Newcomer (for their third album) at the BRIT Awards, upsetting better-known acts such as Steps and 5ive. That same year, the band hosted their own festival, the Bowlie Weekender. \"Tigermilk\" was also given a full release by Jeepster before the band started work on their next LP. The result was \"Fold Your Hands Child, You Walk Like a Peasant\", which became the band's first top 10 album in the UK. A stand-alone single, \"Legal Man\", reached No. 15 and gave them their first appearance on Top of the Pops.\n\nAs the band's popularity and recognition was growing worldwide, their music began appearing in films and on television. The 2000 film \"High Fidelity\" mentions the band and features a clip from the song \"Seymour Stein\" from \"The Boy with the Arab Strap\". Also, the title track from \"Arab Strap\" was played over the end credits of the UK television series \"Teachers,\" and the lyric \"Colour my life with the chaos of trouble\" from the song was quoted by one of the characters in the 2009 film \"(500) Days of Summer\".\n\nStuart David soon left the band to concentrate on his side project, Looper, and his book writing, which included his \"The Idle Thoughts of a Daydreamer\". He was replaced by Bobby Kildea of V-Twin. The \"Jonathan David\" single, sung by Stevie Jackson, was released in June 2001 and was followed by \"I'm Waking Up to Us\" in November. \"I'm Waking Up to Us\" saw the band use an outside producer (Mike Hurst) for the first time. Most of 2002 was spent touring and recording a soundtrack album, \"Storytelling\" (for \"Storytelling\" by Todd Solondz). Campbell left the band in the spring of 2002, in the middle of the band's North American tour to pursue a solo career, first as The Gentle Waves, and later under her own name. She later collaborated with singer Mark Lanegan on three albums.\n\nThe band left Jeepster in 2002, signing a four-album deal with Rough Trade Records. Their first album for Rough Trade, \"Dear Catastrophe Waitress\", was released in 2003, and was produced by Trevor Horn. The album showed a markedly more \"produced\" sound compared to their first four LPs, as the band was making a concerted effort to produce more \"radio-friendly\" music. The album was warmly received and is credited with restoring the band's \"indie cred\". The album also marked the return of Murdoch as the group's primary songwriter, following the poorly received \"Fold Your Hands Child, You Walk Like a Peasant\" and \"Storytelling\", both of which were more collaborative than the band's early work. A documentary DVD, \"Fans Only\", was released by Jeepster in October 2003, featuring promotional videos, live clips and unreleased footage. A single from the album, \"Step into My Office, Baby\" followed in November 2003; it would be their first single taken from an album.\n\nThe Thin Lizzy-inspired \"I'm a Cuckoo\" was the second single from the album. It achieved their highest chart position yet, reaching No. 14 in the UK. The \"Books\" EP followed, a double A-side single led by \"Wrapped Up in Books\" from \"Dear Catastrophe Waitress\" and the new \"Your Cover's Blown\". This EP became the band's third top 20 UK release, and the band was nominated for both the Mercury Music Prize and an Ivor Novello Award. In January 2005, B&S was voted Scotland's greatest band in a poll by The List, beating Simple Minds, Idlewild, Travis, Franz Ferdinand, and The Proclaimers, among others.\n\nIn April 2005, members of the band visited Israel and the Palestinian territories with the UK charity War on Want; the group subsequently recorded a song inspired by the trip titled \"The Eighth Station of the Cross Kebab House\", which would first appear on the digital-download version of the charity album and would later have a physical release as a B-side on 2006's \"Funny Little Frog\" single. \"Push Barman to Open Old Wounds\", a compilation of the Jeepster singles and EPs, was released in May 2005 while the band were recording their seventh album in California. The result of the sessions was \"The Life Pursuit\", produced by Tony Hoffer. The album, originally intended to be a double album, became the band's highest-charting album upon its release in February 2006, peaking at No. 8 in the UK and No. 65 on the US \"Billboard\" 200. \"Funny Little Frog\", which preceded it, also proved to be their highest-charting single, debuting at No. 13.\n\nOn 6 July 2006, the band played a historic show with the Los Angeles Philharmonic at the Hollywood Bowl. The opening act at the 18,000 seat sell-out concert was The Shins. The members of the band see this as a landmark event, with Stevie Jackson saying, \"This is the biggest thrill of my entire life\". In October 2006, members of the band helped put together a CD collection of new songs for children titled \"Colours Are Brighter\", with the involvement of major bands such as Franz Ferdinand and The Flaming Lips.\n\nOn 18 November 2008 the band released \"The BBC Sessions\", which features songs from the period of 1996–2001 (including the last recordings featuring Isobel Campbell before she left the band), along with a second disc featuring a recording of a live performance in Belfast from Christmas 2001.\n\nOn 17 July 2010, the band performed their first UK gig in almost four years to a crowd of around 30,000 at Latitude Festival in Henham Park, Southwold. They performed two new songs, \"I Didn't See It Coming\" and \"I'm Not Living in the Real World\".\n\nTheir eighth studio album, released in the UK and internationally on 25 September 2010, was titled \"Belle and Sebastian Write About Love\". The first single from the album, as well as the record's title track \"Write About Love\", was released in the US on 7 September 2010. \"Write About Love\" entered the UK albums chart in its first week of release, peaking at No. 8 as of 19 October 2010. Norah Jones is featured on the track \"Little Lou, Ugly Jack, Prophet John\".\n\nIn December 2010 Belle and Sebastian curated the sequel to the \"Bowlie Weekender\" in the form of \"Bowlie 2\" presented by All Tomorrow's Parties.\n\nIn 2013, Pitchfork TV released an hour-long documentary in February, directed by RJ Bentler which focused on the band's 1996 album If You're Feeling Sinister, as well as the formation and early releases of the band. The documentary featured interviews with every member that was present on the album, as well as several archival photos and videos from the band's early days. The band compiled a second compilation album \"The Third Eye Centre\" which included the b-sides and rarities released after \"Push Barman to Open Old Wounds\", from the albums \"Dear Catastrophe Waitress\", \"The Life Pursuit\", and \"Write About Love\". In an interview at the end of 2013, Mick Cooke confirmed he had left the band on good terms.\n\nThe band received an 'Outstanding Contribution To Music Award' at the NME Awards 2014.\n\nIn 2014, the band returned to the studio, recording in Atlanta, Georgia for their ninth studio album, along with announcing tour dates for various festivals and concerts across the world during 2014. Their ninth album \"Girls in Peacetime Want to Dance\" was released on 19 January 2015.\nThe Belle and Sebastian song 'There's Too Much Love' forms much of the soundtrack for the Brazilian film 'The Way He Looks', about a blind, gay teenage boy and his friends, released in 2014. \n\nBelle and Sebastian performed at the world-famous Glastonbury Festival on Sunday 28 June 2015, on 'The Other Stage' and at O2 Academy, Glasgow in March 2017 which was televised in the U.K. as part of the 'BBC 6 MUSIC Presents Festival'.\n\nThe Reindeer Section were a Scottish indie rock supergroup formed in 2001 by Gary Lightbody of Snow Patrol, which released albums and gigged in 2001 and 2002. It featured Richard Colburn, Mick Cooke and Bobby Kildea from Belle and Sebastian.\n\nThe Vaselines are an alternative rock band from Glasgow formed in Glasgow in 1986. Between 2008 and 2014 their lineup featured Stevie Jackson and Bobby Kildea from Belle and Sebastian and they performed at Bowlie Weekender 2 curated by Belle and Sebastian in December 2010.\n\nTired Pony is a country / Americana supergroup formed by Gary Lightbody of Snow Patrol in 2010. It features Richard Colburn from Belle and Sebastian.\n\nGod Help the Girl is a musical project by Stuart Murdoch, featuring a group of female vocalists, including Catherine Ireton, with Belle and Sebastian as the accompanying band.\n\n\n\n\n\n\n"}
{"id": "4279", "url": "https://en.wikipedia.org/wiki?curid=4279", "title": "Broadcast domain", "text": "Broadcast domain\n\nA broadcast domain is a logical division of a computer network, in which all nodes can reach each other by broadcast at the data link layer. A broadcast domain can be within the same LAN segment or it can be bridged to other LAN segments.\n\nIn terms of current popular technologies: Any computer connected to the same Ethernet repeater or switch is a member of the same broadcast domain. Further, any computer connected to the same set of inter-connected switches/repeaters is a member of the same broadcast domain. Routers and other higher-layer devices form boundaries between broadcast domains.\n\nThis is as compared to a collision domain, which would be all nodes on the same set of inter-connected repeaters, divided by switches and learning bridges. Collision domains are generally smaller than, and contained within, broadcast domains.\n\nWhile some layer two network devices are able to divide the collision domains, broadcast domains are only divided by layer 3 network devices such as routers or layer 3 switches. Separating VLANs divides broadcast domains as well.\n\nThe distinction between broadcast and collision domains comes about because simple Ethernet and similar systems use a shared transmission system. In simple Ethernet (without switches or bridges), data frames are transmitted to all other nodes on a network. Each receiving node checks the destination address of each frame, and simply ignores any frame not addressed to its own MAC.\n\nSwitches act as buffers, receiving and analyzing the frames from each connected network segment. Frames destined for nodes connected to the originating segment are not forwarded by the switch. Frames destined for a specific node on a different segment are sent only to that segment. Only broadcast frames are forwarded to all other segments. This reduces unnecessary traffic and collisions.\n\nIn such a switched network, transmitted frames may not be received by all other reachable nodes. Nominally, only broadcast frames will be received by all other nodes. Collisions are localized to the network segment they occur on. Thus, the broadcast domain is the entire inter-connected layer two network, and the segments connected to each switch/bridge port are each a collision domain.\n\nNot all network systems or media feature broadcast/collision domains. For example, PPP links.\n\nWith a sufficiently sophisticated switch, it is possible to create a network in which the normal notion of a broadcast domain is strictly controlled. One implementation of this concept is termed a \"private VLAN\". Another implementation is possible with Linux and iptables. One helpful analogy is that by creating multiple VLANs, the number of broadcast domains increases, but the size of each broadcast domain decreases. This is because a virtual LAN (or VLAN) is technically a broadcast domain.\n\nThis is achieved by designating one or more \"server\" or \"provider\" nodes, either by MAC address or switch port. Broadcast frames are allowed to originate from these sources, and are sent to all other nodes. Broadcast frames from all other sources are directed only to the server/provider nodes. Traffic from other sources not destined to the server/provider nodes (\"peer-to-peer\" traffic) is blocked.\n\nThe result is a network based on a nominally shared transmission system; like Ethernet, but in which \"client\" nodes cannot communicate with each other, only with the server/provider. A common application is Internet providers. Allowing direct data link layer communication between customer nodes exposes the network to various security attacks, such as ARP spoofing. Controlling the broadcast domain in this fashion provides many of the advantages of a point-to-point network, using commodity broadcast-based hardware.\n\n\n"}
{"id": "4282", "url": "https://en.wikipedia.org/wiki?curid=4282", "title": "Beechcraft", "text": "Beechcraft\n\nBeechcraft is a brand of Textron Aviation since 2014. Originally, it was a brand of Beech Aircraft Corporation, an American manufacturer of general aviation, commercial, and military aircraft -- ranging from light single-engined aircraft to twin-engined turboprop transports, business jets, and military trainers. Beech later became a division of Raytheon and later Hawker Beechcraft -- before a bankruptcy sale turned its assets over to Textron (parent company of Beech's cross-town Wichita rival, Cessna Aircraft Company).\n\nBeech Aircraft Company was founded in Wichita, Kansas, in 1932 by Walter Beech and his wife Olive Ann Beech. The company began operations in an idle Cessna factory. With designer Ted Wells, they developed the first aircraft under the Beechcraft name, the classic Beechcraft Model 17 Staggerwing, which first flew in November 1932. Over 750 Staggerwings were built, with 270 manufactured for the United States Army Air Forces during World War II.\n\nBeechcraft was not Beech's first company, as he had previously formed Travel Air in 1924 and the design numbers used at Beechcraft followed the sequence started at Travel Air, and were then continued at Curtiss-Wright, after Travel Air had been absorbed into the much larger company in 1929. Beech became President of the Curtiss-Wright's airplane division and VP of sales, but became dissatisfied with being so far removed from aircraft production and quit to form Beechcraft, using the original Travel Air facilities and employing many of the same people. Model numbers prior to 11/11000 were built under the Travel Air name, while Curtiss-Wright built the CW-12, 14, 15, and 16 as well as previous successful Travel Air models (mostly the model 4).\n\nIn 1942 Beech won its first Army-Navy \"E\" Award production award and became one of the elite five percent of war contracting firms in the country to win five straight awards for production efficiency, mostly for the production of the Beechcraft Model 18 which remains in widespread use worldwide. Beechcraft ranked 69th among United States corporations in the value of World War II military production contracts.\n\nAfter the war, the Staggerwing was replaced by the revolutionary Beechcraft Bonanza with a distinctive V-tail. Perhaps the best known Beech aircraft, the single-engined Bonanza has been manufactured in various models since 1947. The Bonanza has had the longest production run of any airplane, past or present, in the world. Other important Beech aircraft are the King Air/Super King Air line of twin-engined turboprops, in production since 1964, the Baron, a twin-engined variant of the Bonanza, and the Beechcraft Model 18, originally a business transport and commuter airliner from the late 1930s through the 1960s, which remains in active service as a cargo transport.\n\nIn 1950, Olive Ann Beech was installed as president and CEO of the company, after the sudden death of her husband from a heart attack on 29 November of that year. She continued as CEO until Beech was purchased by Raytheon Company on 8 February 1980. Ted Wells had been replaced as Chief Engineer by Herbert Rawdon, who remained at the post until his retirement in the early 1960s.\n\nIn 1994, Raytheon merged Beechcraft with the Hawker product line it had acquired in 1993 from British Aerospace, forming Raytheon Aircraft Company. In 2002, the Beechcraft brand was revived to again designate the Wichita-produced aircraft. In 2006, Raytheon sold Raytheon Aircraft to Goldman Sachs creating Hawker Beechcraft. Since its inception Beechcraft has resided in Wichita, Kansas, also the home of chief competitor Cessna, the birthplace of Learjet and of Stearman, whose trainers were used in large numbers during WW2.\n\nThe entry into bankruptcy of Hawker Beechcraft on May 3, 2012 ended with its emergence on February 16, 2013 as a new entity, Beechcraft Corporation, with the Hawker Beechcraft name being retired. The new and much smaller company will produce the King Air line of aircraft as well as the T-6 and AT-6 military trainer/attack aircraft, as well as the piston-powered single-engined Bonanza and twin-engined Baron aircraft. The jet line was discontinued, but the new company would continue to support the aircraft already produced with parts, plus engineering and airworthiness documentation.\n\nBy October 2013, the company, now financially turned around, was up for sale.\n\nOn December 26, 2013, Textron agreed to purchase Beechcraft, including the discontinued Hawker jet line, for $1.4 billion. The sale was concluded in the first half of 2014, with government approval. Textron CEO Scott Donnelly indicated that Beechcraft and Cessna would be combined to form a new light aircraft manufacturing concern, Textron Aviation, that will result in US$65M-$85M in annual savings over keeping the companies separate. Textron has kept both the Beechcraft and Cessna names as separate brands.\n\n\n\n\nBeech Factory Airport house Beechcraft's head office, manufacturing facility, and runway for test flights.\n\n"}
{"id": "4283", "url": "https://en.wikipedia.org/wiki?curid=4283", "title": "Battle of Peleliu", "text": "Battle of Peleliu\n\nThe Battle of Peleliu, codenamed Operation Stalemate II by the United States military, was fought between the US and the Empire of Japan during the Mariana & Palau Campaign of World War II, from September to November 1944, on the island of Peleliu.\n\nUS Marines of the 1st Marine Division, and later soldiers of the US Army's 81st Infantry Division, fought to capture an airstrip on the small coral island. This battle was part of a larger offensive campaign known as Operation Forager, which ran from June to November 1944, in the Pacific Theater.\n\nMajor General William Rupertus, (USMC commander of the 1st Marine Division) predicted the island would be secured within four days. However, after repeated Imperial Army defeats in previous island campaigns, Japan had developed new island-defense tactics and well-crafted fortifications that allowed stiff resistance, extending the battle through more than two months. In the United States, this was a controversial battle because of the island's questionable strategic value and the high casualty rate, which exceeded that of all other amphibious operations during the Pacific War. The National Museum of the Marine Corps called it \"the bitterest battle of the war for the Marines\".\n\nBy 1944, American victories in the Southwest and Central Pacific had brought the war closer to Japan, with American bombers able to strike at the Japanese main islands from air bases secured during the Mariana Islands campaign (June—August 1944). There was disagreement among the U.S. Joint Chiefs over two proposed strategies to defeat the Japanese Empire. The strategy proposed by General Douglas MacArthur called for the recapture of the Philippines, followed by the capture of Okinawa, then an attack on the Japanese mainland. Admiral Chester Nimitz favored a more direct strategy of bypassing the Philippines, but seizing Okinawa and Taiwan as staging areas to an attack on the Japanese mainland, followed by the future invasion of Japan's southernmost islands. Both strategies included the invasion of Peleliu, but for different reasons.\n\nThe 1st Marine Division had already been chosen to make the assault. President Franklin D. Roosevelt traveled to Pearl Harbor to personally meet both commanders and hear their arguments. MacArthur's strategy was chosen. However, before MacArthur could retake the Philippines, the Palau Islands, specifically Peleliu and Angaur, were to be neutralized and an airfield built to protect MacArthur's right flank.\n\nBy 1944, Peleliu Island was occupied by about 11,000 Japanese of the 14th Infantry Division with Korean and Okinawan laborers. Colonel Kunio Nakagawa, commander of the division's 2nd Regiment, led the preparations for the island's defense.\n\nAfter their losses in the Solomons, Gilberts, Marshalls and Marianas, the Imperial Army assembled a research team to develop new island-defense tactics. They chose to abandon the old strategy of stopping the enemy at the beach. The new tactics would only disrupt the landings at the water's edge and depend on an in-depth defense farther inland. Colonel Nakagawa used the rough terrain to his advantage, by constructing a system of heavily fortified bunkers, caves and underground positions all interlocked into a \"honeycomb\" system. The old \"banzai charge\" attack was also discontinued as being both wasteful of men and ineffective. These changes would force the Americans into a war of attrition requiring increasingly more resources.\nNakagawa's defenses were based at Peleliu's highest point, Umurbrogol Mountain, a collection of hills and steep ridges located at the center of Peleliu overlooking a large portion of the island, including the crucial airfield. The Umurbrogol contained some 500 limestone caves, interconnected by tunnels. Many of these were former mine shafts that were turned into defense positions. Engineers added sliding armored steel doors with multiple openings to serve both artillery and machine guns. Cave entrances were built slanted as a defense against grenade and flamethrower attacks. The caves and bunkers were connected to a vast system throughout central Peleliu, which allowed the Japanese to evacuate or reoccupy positions as needed, and to take advantage of shrinking interior lines.\n\nThe Japanese were well armed with and mortars and anti-aircraft cannons, backed by a light tank unit and an anti-aircraft detachment.\n\nThe Japanese also used the beach terrain to their advantage. The northern end of the landing beaches faced a coral promontory that overlooked the beaches from a small peninsula, a spot later known to the Marines who assaulted it simply as \"The Point\". Holes were blasted into the ridge to accommodate a gun, and six 20-mm cannons. The positions were then sealed shut, leaving just a small firing slit to assault the beaches. Similar positions were crafted along the stretch of landing beaches.\n\nThe beaches were also filled with thousands of obstacles for the landing craft, principally mines and a large number of heavy artillery shells buried with the fuses exposed to explode when they were run over. A battalion was placed along the beach to defend against the landing, but they were meant to merely delay the inevitable American advance inland.\n\nUnlike the Japanese, who drastically altered their tactics for the upcoming battle, the American invasion plan was unchanged from that of previous amphibious landings, even after suffering 3,000 casualties and two months of delaying tactics against the entrenched Japanese defenders at the Battle of Biak. On Peleliu, American planners chose to land on the southwest beaches because of their proximity to the airfield on South Peleliu. The 1st Marine Regiment, commanded by Colonel Lewis B. Puller, was to land on the northern end of the beaches. The 5th Marine Regiment, under Colonel Harold D. Harris, would land in the center, and the 7th Marine Regiment, under Col. Herman H. Hanneken, would land at the southern end.\n\nThe division's artillery regiment, the 11th Marines, would land after the infantry regiments. The plan was for the 1st and 7th Regiments to push inland, guarding the 5th Regiment's left and right flank, and allowing them to capture the airfield located directly to the center of the landing beaches. The 5th Marines were to push to the eastern shore, cutting the island in half. The 1st Marines would push north into the Umurbrogol, while the 7th Marines would clear the southern end of the island. Only one battalion was left behind in reserve, with the Army's 81st Infantry Division available for support from Angaur, just south of Peleliu.\n\nOn September 4, the Marines shipped off from their station on Pavuvu, just north of Guadalcanal, a trip across the Pacific to Peleliu. The Navy's Underwater Demolition Team went in first to clear the beaches of obstacles, while U.S. Navy warships began their pre-invasion bombardment of Peleliu on September 12.\n\nThe battleships , , , and , heavy cruisers , , and , and light cruisers , and , led by the command ship , subjected the tiny island, only in size, to a massive three-day bombardment, pausing only to permit air strikes from the three aircraft carriers, five light aircraft carriers, and eleven escort carriers with the attack force. A total of 519 rounds of shells, 1,845 rounds of shells and 1,793 bombs were dropped on the islands during this period.\n\nThe Americans believed the bombardment to be successful, as Rear Admiral Jesse Oldendorf claimed that the Navy had run out of targets. In reality, the majority of the Japanese positions were completely unharmed. Even the battalion left to defend the beaches was virtually unscathed. During the assault, the island's defenders exercised unusual firing discipline to avoid giving away their positions. The bombardment managed only to destroy Japan's aircraft on the island, as well as the buildings surrounding the airfield. The Japanese remained in their fortified positions, ready to attack the troops soon to be landing.\n\nU.S. Marines landed on Peleliu at 08:32, on September 15, the 1st Marines to the north on White Beach 1 and 2 and the 5th and 7th Marines to the center and south on Orange Beach 1, 2 and 3. As the other landing craft approached the beaches, they were caught in a crossfire when the Japanese opened the steel doors guarding their positions and fired artillery. The positions on the coral promontories guarding each flank attacked the Marines with 47-mm guns and 20-mm cannons. By 09:30, the Japanese had destroyed 60 LVTs and DUKWs.\nThe 1st Marines were quickly bogged down by heavy fire from the extreme left flank and a 30-foot-high coral ridge, \"The Point\". Colonel Chesty Puller narrowly escaped death when a dud high velocity artillery round struck his LVT. His communications section was destroyed on its way to the beach by a hit from a 47-mm round. The 7th Marines faced a cluttered Orange Beach 3, with natural and man-made obstacles, forcing the Amtracs to approach in column.\n\nThe 5th Marines made the most progress on the first day, aided by cover provided by coconut groves. They pushed toward the airfield, but were met with Nakagawa's first counterattack. His armored tank company raced across the airfield to push the Marines back, but was soon engaged by tanks, howitzers, naval guns and dive bombers. Nakagawa's tanks and escorting infantrymen were quickly destroyed.\n\nAt the end of the first day, the Americans held their stretch of landing beaches, but little else. Their biggest push in the south moved inland, but the 1st Marines to the north made very little progress because of the extremely thick resistance. The Marines had suffered 200 dead and 900 wounded. Rupertus, still unaware of his enemy's change of tactics, believed the Japanese would quickly crumble since their perimeter had been broken.\n\nThe 7th Marines were in need of stretcher bearers for the wounded and sent three men to find help. They found two units 16th Marine Field Depot and 17th(special) Seabees. They were not certain what their officers would think because both were segregated units but they knew they needed help. The 17th was assigned to the 1st Pioneers as shore party. They carried the wounded, buried the dead, reinforced the line where directed , provided a crew for a 37mm and volunteered for anything. On the first night nearly the entire battalion humped ammo to the front and it is written that they remained there several days. For their efforts they received an official a \"Well Done\". The 33rd NCB also had 202 men assigned to the shore party.\n\nOn the second day, the 5th Marines moved to capture the airfield and push toward the eastern shore. They ran across the airfield, enduring heavy artillery fire from the highlands to the north, suffering heavy casualties in the process. After capturing the airfield, they rapidly advanced to the eastern end of Peleliu, leaving the island's southern defenders to be destroyed by the 7th Marines.\n\nThis area was hotly contested by the Japanese, who still occupied numerous pillboxes. Heat indices were around , and the Marines soon suffered high casualties from heat exhaustion. Further complicating the situation, the Marines' water was distributed in empty oil drums, contaminating the water with the oil residue. Still, by the eighth day the 5th and 7th Marines had accomplished their objectives, holding the airfield and the southern portion of the island, although the airfield remained under threat of sustained Japanese fire from the heights of Umurbrogol Mountain until the end of the battle.\n\nAmerican forces put the airfield to use on the third day. L-2 Grasshoppers from VMO-1 began aerial spotting missions for Marine artillery and naval gunfire support. On September 26 (D+11), Marine F4U Corsairs from VMF-114 landed on the airstrip. The Corsairs began dive-bombing missions across Peleliu, firing rockets into open cave entrances for the infantrymen, and dropping napalm; it was only the second time the latter weapon had been used in the Pacific. Napalm proved useful, burning away the vegetation hiding spider holes and usually killing their occupants.\n\nThe time from liftoff to the target area for the Corsairs based on Peleliu Airfield was very short, sometimes only 10 to 15 seconds. Consequently, there was almost no time for pilots to raise their aircraft undercarriage; most pilots did not bother and left them down during the strike. After the strike was completed and the payload dropped, the Corsair simply turned back into the landing pattern again.\n\nThe fortress at the end of the southern landing beaches (a.k.a. “The Point”) continued to cause heavy casualties due to enfilading fire from heavy machine guns and anti-tank artillery across the landing beaches. Puller ordered Captain George P. Hunt, commander of K Company, 3rd Battalion, 1st Marines, to capture the position. He approached The Point short on supplies, having lost most of his machine guns while approaching the beaches. Hunt's second platoon was pinned down for nearly a day in an anti-tank trench between fortifications. The rest of his company was endangered when the Japanese cut a hole in their line, surrounding his company and leaving his right flank cut off.\n\nHowever, a rifle platoon began knocking out the Japanese gun positions one by one. Using smoke grenades for cover, they swept through each hole, destroying the positions with rifle grenades and close-quarters combat. After knocking out the six machine gun positions, the Marines faced the 47 mm gun cave. A lieutenant blinded the 47 mm gunner with a smoke grenade, allowing Corporal Henry W. Hahn to launch a grenade through the cave's aperture. The grenade detonated the 47 mm's shells, forcing the cave's occupants out with their bodies lit aflame as well as their ammunition belts exploding around their waists. A fire team was positioned on the flank of the cave where the former occupants were shot down.\n\nK Company had captured The Point, but Nakagawa counterattacked. The next 30 hours saw four major counterattacks against a sole company, critically low on supplies, out of water, and surrounded. The Marines soon had to resort to hand-to-hand combat to fend off the Japanese attackers. By the time reinforcements arrived, the company had successfully repulsed all Japanese attacks, but had been reduced to 18 men, suffering 157 casualties during the battle for The Point. Hunt and Hahn were both awarded the Navy Cross for their actions.\n\nThe 5th Marines—after having secured the airfield—were sent to capture Ngesebus Island, just north of Peleliu. Ngesebus was occupied by many Japanese artillery positions, and was the site of an airfield still under construction. The tiny island was connected to Peleliu by a small causeway, but 5th Marines commander Harris opted instead to make a shore-to-shore amphibious landing, predicting the causeway to be an obvious target for the island's defenders.\n\nHarris coordinated a pre-landing bombardment of the island on September 28, carried out by Army guns, naval guns, howitzers from the 11th Marines, strafing runs from VMF-114's Corsairs, and fire from the approaching LVTs. Unlike the Navy's bombardment of Peleliu, Harris' assault on Ngesebus successfully killed most of the Japanese defenders. The Marines still faced opposition in the ridges and caves, but the island fell quickly, with relatively light casualties for the 5th Marines. They had suffered 15 killed and 33 wounded, and inflicted 470 casualties on the Japanese.\n\nAfter capturing The Point, the 1st Marines moved north into the Umurbrogol pocket, named \"Bloody Nose Ridge\" by the Marines. Puller led his men in numerous assaults, but every one brought on severe casualties by the Japanese. The 1st Marines were trapped within the narrow paths between the ridges, with each ridge fortification supporting the other with deadly crossfire.\n\nThe Marines took increasingly high casualties as they slowly advanced through the ridges. The Japanese again showed unusual fire discipline, striking only when they could inflict maximum casualties. As casualties mounted, Japanese snipers began to take aim at stretcher bearers, knowing that if two stretcher bearers were injured or killed, more would have to return to replace them, and the snipers could steadily pick off more and more Marines. The Japanese infiltrated the American lines at night to attack the Marines in their foxholes. The Marines built two-man foxholes, so one could sleep while the other kept watch for infiltrators.\n\nOne particularly bloody battle on Bloody Nose came when the 1st Battalion, 1st Marines—under the command of Major Raymond Davis—attacked Hill 100. Over six days of fighting, the battalion suffered 71% casualties. Captain Everett Pope and his company penetrated deep into the ridges, leading his remaining 90 men to seize what he thought was Hill 100. It took a day's fighting to reach what he thought was the crest of the hill, which was in fact another ridge, occupied by more Japanese defenders.\n\nTrapped at the base of the ridge, Pope set up a small defense perimeter, which was attacked relentlessly by the Japanese throughout the night. The Marines soon ran out of ammunition, and had to fight the attackers with knives and fists, even resorting to throwing coral rock and empty ammunition boxes at the Japanese. Pope and his men managed to hold out until dawn came, which brought on more deadly fire. When they evacuated the position, only nine men remained. Pope later received the Medal of Honor for the action. (Picture of the Peleliu Memorial dedicated on the 50th anniversary of the landing on Peleliu with Captain Pope's name)\nThe Japanese eventually inflicted 70% casualties on Puller's 1st Marines, or 1,749 men. After six days of fighting in the ridges of Umurbrogol, General Roy Geiger, commander of the III Amphibious Corps, sent elements of 81st Infantry Division to Peleliu to relieve the regiment. The 321st Regiment Combat Team landed on the western beaches of Peleliu—at the northern end of Umurbrogol mountain—on 23 September. The 321st and the 7th Marines encircled The Pocket by 24 Sept., D+9.\n\nBy 15 October, the 7th Marines had suffered 46% casualties and General Geiger relieved them with the 5th Marines. Col. Harris adopted siege tactics, using bulldozers and flame-thrower tanks, pushing from the north. On 30 Oct., the 81st Inf. Div. took over command of Peleliu, taking another six weeks, with the same tactics, to reduce The Pocket.\n\nOn 24 November, Nakagawa proclaimed \"Our sword is broken and we have run out of spears\". He then burnt his regimental colors and performed ritual suicide. He was posthumously promoted to lieutenant general for his valor displayed on Peleliu. On 27 November, the island was declared secure, ending the 73-day-long battle.\n\nA Japanese lieutenant with twenty-six 2nd Infantry soldiers and eight 45th Guard Force sailors held out in the caves in Peleliu until April 22, 1947 and surrendered after a Japanese admiral convinced them the war was over.\n\nThe reduction of the Japanese pocket around Umurbrogol mountain has been called the most difficult fight that the U.S. military encountered in the entire war. The 1st Marine Division was severely mauled and it remained out of action until the invasion of Okinawa on 1 April 1945. In total, the 1st Marine Division suffered over 6,500 casualties during their month on Peleliu, over of their entire division. The 81st Infantry Division suffered nearly 3,300 casualties during their tenure on the island.\n\nPostwar statisticians calculated that it took US forces over 1500 rounds of ammunition to kill each Japanese defender, and that during the course of the battle, the Americans expended 13.32 million rounds of 30-calibre, 1.52 million rounds of 45-calibre, 693,657 rounds of 50-calibre bullets, 118,262 hand grenades and approximately 150,000 mortar rounds.\n\nThe battle was controversial in the United States due to the island's lack of strategic value and the high casualty rate. The defenders lacked the means to interfere with potential US operations in the Philippines, and the airfield captured on Peleliu never played a key role in subsequent operations. The high casualty rate exceeded all other amphibious operations during the Pacific War.\n\nInstead, the Ulithi Atoll in the Caroline Islands was used as a staging base for the invasion of Okinawa. In addition, few news reports were published about the battle because Rupertus' prediction of a \"three days\" victory motivated only six reporters to report from shore. The battle was also overshadowed by MacArthur's return to the Philippines and the Allies' push towards Germany in Europe.\n\nThe battles for Angaur and Peleliu showed Americans the pattern of future Japanese island defense which would be seen again at Iwo Jima and Okinawa. Naval bombardment prior to amphibious assault at Iwo Jima was only slightly more effective than at Peleliu, but at Okinawa the preliminary shelling was much improved. Frogmen performing underwater demolition at Iwo Jima confused the enemy by sweeping both coasts, but later alerted Japanese defenders to the exact assault beaches at Okinawa. American ground forces at Peleliu gained experience in assaulting heavily fortified positions such as they would find again at Okinawa.\n\nOn the recommendation of Admiral William F. Halsey, Jr., the planned occupation of Yap Island in the Caroline Islands was canceled. Halsey actually recommended that the landings on Peleliu and Angaur be canceled, too, and their Marines and soldiers be thrown into Leyte Island instead, but was overruled by Nimitz.\n\nThe Battle of Peleliu is featured in many World War II themed video games including \"\". The player takes the role of a US Marine forced to take Peleliu Airfield, repel counter-attacks, destroy machine-gun and mortar positions and eventually secure Japanese artillery emplacements at the point. In flight-simulation game \"War Thunder\", two teams of players clash to hold the southern and northern airfields. In multi-player shooter \"Red Orchestra 2: Rising Storm\", a team of American troops attack the defensive Japanese team's control points.\n\nThe battle including footage and stills are featured in the fifth episode of \"Ken Burns' The War\".\n\nThe battle features in Episodes 5, 6 and 7 of the TV mini-series \"The Pacific\".\n\nIn his book, 'With the Old Breed,' Eugene Bondurant Sledge described his experiences in the battle for Peleliu.\n\nIn 2015, the Japanese magazine \"Young Animal\" commenced serialization of \"Peleliu: Rakuen no Guernica\" by Masao Hiratsuka and artist Kazuyoshi Takeda, telling the story of the battle in manga form.\n\nOne of the final scenes in \"Parer's War\", a 2014 Australian television film, shows the Battle of Peleliu recorded by Damien Parer with his camera at the time of his death.\n\nJapan posthumously promoted Colonel Kunio Nakagawa and Kenjiro Murai to lieutenant general for their heroism in defending the island.\n\nThe United States awarded the Medal of Honor to eight Marines (five posthumously):\n\n\n\n\n"}
{"id": "4284", "url": "https://en.wikipedia.org/wiki?curid=4284", "title": "Battle of Stalingrad", "text": "Battle of Stalingrad\n\nThe Battle of Stalingrad (23 August 1942 – 2 February 1943) was a major battle of World War II in which Nazi Germany and its allies fought the Soviet Union for control of the city of Stalingrad (now Volgograd) in Southern Russia.\n\nMarked by fierce close quarters combat and direct assaults on civilians by air raids, it is often regarded as one of the single largest (nearly 2.2 million personnel) and bloodiest (1.7–2 million wounded, killed or captured) battles in the history of warfare. It was an extremely costly defeat for German forces, and the Army High Command had to withdraw vast military forces from the West to replace their losses.\n\nThe German offensive to capture Stalingrad began in August 1942, using the German 6th Army and elements of the 4th Panzer Army. The attack was supported by intensive \"Luftwaffe\" bombing that reduced much of the city to rubble. The fighting degenerated into house-to-house fighting, and both sides poured reinforcements into the city. By mid-November 1942, the Germans had pushed the Soviet defenders back at great cost into narrow zones along the west bank of the Volga River.\n\nOn 19 November 1942, the Red Army launched Operation Uranus, a two-pronged attack targeting the weaker Romanian and Hungarian armies protecting the German 6th Army's flanks. The Axis forces on the flanks were overrun and the 6th Army was cut off and surrounded in the Stalingrad area. Adolf Hitler ordered that the army stay in Stalingrad and make no attempt to break out; instead, attempts were made to supply the army by air and to break the encirclement from the outside. Heavy fighting continued for another two months. By the beginning of February 1943, the Axis forces in Stalingrad had exhausted their ammunition and food. The remaining units of the 6th Army surrendered. The battle lasted five months, one week, and three days.\n\nBy the spring of 1942, despite the failure of Operation Barbarossa to decisively defeat the Soviet Union in a single campaign, the Wehrmacht had captured vast expanses of territory, including Ukraine, Belarus, and the Baltic republics. Elsewhere, the war had been progressing well: the U-boat offensive in the Atlantic had been very successful and Rommel had just captured Tobruk. In the east, they had stabilized their front in a line running from Leningrad in the north to Rostov in the south. There were a number of salients, but these were not particularly threatening. Hitler was confident that he could master the Red Army after the winter of 1942, because even though Army Group Centre (\"Heeresgruppe Mitte\") had suffered heavy losses west of Moscow the previous winter, 65% of Army Group Centre's infantry had not been engaged and had been rested and re-equipped. Neither Army Group North nor Army Group South had been particularly hard pressed over the winter. Stalin was expecting the main thrust of the German summer attacks to be directed against Moscow again.\n\nWith the initial operations being very successful, the Germans decided that their summer campaign in 1942 would be directed at the southern parts of the Soviet Union. The initial objectives in the region around Stalingrad were the destruction of the industrial capacity of the city and the deployment of forces to block the Volga River. The river was a key route from the Caucasus and the Caspian Sea to central Russia. Its capture would disrupt commercial river traffic. The Germans cut the pipeline from the oilfields when they captured Rostov on 23 July. The capture of Stalingrad would make the delivery of Lend Lease supplies via the Persian Corridor much more difficult.\n\nOn 23 July 1942, Hitler personally rewrote the operational objectives for the 1942 campaign, greatly expanding them to include the occupation of the city of Stalingrad. Both sides began to attach propaganda value to the city based on it bearing the name of the leader of the Soviet Union. Hitler proclaimed that after Stalingrad's capture, its male citizens were to be killed and all women and children were to be deported because its population was \"thoroughly communistic\" and \"especially dangerous\". It was assumed that the fall of the city would also firmly secure the northern and western flanks of the German armies as they advanced on Baku, with the aim of securing these strategic petroleum resources for Germany. The expansion of objectives was a significant factor in Germany's failure at Stalingrad, caused by German overconfidence and an underestimation of Soviet reserves.\n\nThe Soviets realized that they were under tremendous constraints of time and resources and ordered that anyone strong enough to hold a rifle be sent to fight.\n\nArmy Group South was selected for a sprint forward through the southern Russian steppes into the Caucasus to capture the vital Soviet oil fields there. The planned summer offensive was code-named \"Fall Blau\" (Case Blue). It was to include the German 6th, 17th, 4th Panzer and 1st Panzer Armies. Army Group South had overrun the Ukrainian Soviet Socialist Republic in 1941. Poised in Eastern Ukraine, it was to spearhead the offensive.\n\nHitler intervened, however, ordering the Army Group to split in two. Army Group South (A), under the command of Wilhelm List, was to continue advancing south towards the Caucasus as planned with the 17th Army and First Panzer Army. Army Group South (B), including Friedrich Paulus's 6th Army and Hermann Hoth's 4th Panzer Army, was to move east towards the Volga and Stalingrad. Army Group B was commanded initially by Field Marshal Fedor von Bock and later by General Maximilian von Weichs.\nThe start of \"Case Blue\" had been planned for late May 1942. A number of German and Romanian units that were to take part in \"Blau\", however, were besieging Sevastopol on the Crimean Peninsula. Delays in ending the siege pushed back the start date for \"Blau\" several times, and the city did not fall until the end of June.\n\nOperation Fridericus I by the Germans against the \"Isium bulge\", pinched off the Soviet salient in the Second Battle of Kharkov, and resulted in the envelopment of a large Soviet force between 17 May and 29 May. Similarly, Operation Wilhelm attacked Voltshansk on 13 June and Operation Fridericus attacked Kupiansk on 22 June.\n\n\"Blau\" finally opened as Army Group South began its attack into southern Russia on 28 June 1942. The German offensive started well. Soviet forces offered little resistance in the vast empty steppes and started streaming eastward. Several attempts to re-establish a defensive line failed when German units outflanked them. Two major pockets were formed and destroyed: the first, northeast of Kharkov, on 2 July, and a second, around Millerovo, Rostov Oblast, a week later. Meanwhile, the Hungarian 2nd Army and the German 4th Panzer Army had launched an assault on Voronezh, capturing the city on 5 July.\nThe initial advance of the 6th Army was so successful that Hitler intervened and ordered the 4th Panzer Army to join Army Group South (A) to the south. A massive traffic jam resulted when the 4th Panzer and the 1st Panzer both required the few roads in the area. Both armies were stopped dead while they attempted to clear the resulting mess of thousands of vehicles. The delay was long, and it is thought that it cost the advance at least one week. With the advance now slowed, Hitler changed his mind and reassigned the 4th Panzer Army back to the attack on Stalingrad.\n\nBy the end of July, the Germans had pushed the Soviets across the Don River. At this point, the Don and Volga Rivers were only apart, and the Germans left their main supply depots west of the Don, which had important implications later in the course of the battle. The Germans began using the armies of their Italian, Hungarian and Romanian allies to guard their left (northern) flank. The Italians won several accolades in official German communiques. Sometimes they were held in little regard by the Germans, and were even accused of having low morale: in reality, the Italian divisions fought comparatively well, with the 3rd Mountain Infantry Division Ravenna and 5th Infantry Division Cosseria proving to have good morale, according to a German liaison officer and being forced to retreat only after a massive armoured attack in which German reinforcements had failed to arrive in time, according to a German historian. Indeed the Italians distinguished themselves in numerous battles, as in the battle of Nikolayevka.\nOn 25 July the Germans faced stiff resistance with a Soviet bridgehead west of Kalach. \"We had had to pay a high cost in men and material...left on the Kalatch battlefield were numerous burnt-out or shot-up German tanks.\"\n\nThe Germans formed bridgeheads across the Don on 20 Aug. with the 295th and 76th Infantry Divisions, enabling the XIVth Panzer Corps \"to thrust to the Volga north of Stalingrad.\" The German 6th Army was only a few dozen kilometers from Stalingrad. The 4th Panzer Army, ordered south on 13 July to block the Soviet retreat \"weakened by the 17th Army and the 1st Panzer Army\", had turned northwards to help take the city from the south.\n\nTo the south, Army Group A was pushing far into the Caucasus, but their advance slowed as supply lines grew overextended. The two German army groups were not positioned to support one another due to the great distances involved.\n\nAfter German intentions became clear in July 1942, Stalin appointed Marshal Andrey Yeryomenko as commander of the Southeastern Front on 1 August 1942. Yeryomenko and Commissar Nikita Khrushchev were tasked with planning the defense of Stalingrad. The eastern border of Stalingrad was the wide River Volga, and over the river, additional Soviet units were deployed. These units became the newly formed 62nd Army, which Yeryomenko placed under the command of Lieutenant General Vasiliy Chuikov on 11 September 1942. The situation was extremely dire. When asked how he interpreted his task, he responded \"We will defend the city or die in the attempt.\" The 62nd Army's mission was to defend Stalingrad at all costs. Chuikov's generalship during the battle earned him one of his two Hero of the Soviet Union awards.\n\nDavid Glantz indicated that four hard-fought battles – collectively known as the Kotluban Operations – north of Stalingrad, where the Soviets made their greatest stand, decided Germany's fate before the Nazis ever set foot in the city itself, and were a turning point in the war. Beginning in late August, continuing in September and into October, the Soviets committed between two and four armies in hastily coordinated and poorly controlled attacks against the German's northern flank. The actions resulted in more than 200,000 Red Army casualties but did slow the German assault.\n\nOn 23 August the 6th Army reached the outskirts of Stalingrad in pursuit of the 62nd and 64th Armies, which had fallen back into the city. Kleist later said after the war:\n\nThe Soviets had enough warning of the Germans' advance to ship grain, cattle, and railway cars across the Volga and out of harm's way but most civilian residents were not evacuated. This \"harvest victory\" left the city short of food even before the German attack began. Before the \"Heer\" reached the city itself, the \"Luftwaffe\" had rendered the River Volga, vital for bringing supplies into the city, unusable to Soviet shipping. Between 25 and 31 July, 32 Soviet ships were sunk, with another nine crippled.\nThe battle began with the heavy bombing of the city by \"Generaloberst\" Wolfram von Richthofen's \"Luftflotte 4\", which in the summer and autumn of 1942 was the single most powerful air formation in the world. Some 1,000 tons of bombs were dropped in 48 hours, more than in London at the height of the Blitz. Stalin refused to evacuate civilian population from the city, so when bombing began 400,000 civilians were trapped within city boundaries. The exact number of civilians killed during the course of the battle is unknown but was most likely very high.\nAround 40,000 were moved to Germany as slave workers, some fled the city during battle and a small number were evacuated by the Soviets. In February 1943 only between 10,000 to 60,000 civilians were still alive in Stalingrad. Much of the city was quickly turned to rubble, although some factories continued production while workers joined in the fighting. The Stalingrad Tractor Factory continued to turn out T-34 tanks literally until German troops burst into the plant. The 369th (Croatian) Reinforced Infantry Regiment was the only non-German unit selected by the \"Wehrmacht\" to enter Stalingrad city during assault operations. It fought as part of the 100th Jäger Division.\nStalin rushed all available troops to the east bank of the Volga, some from as far away as Siberia. All the regular ferries were quickly destroyed by the Luftwaffe, which then targeted troop barges being towed slowly across the river by tugs. It has been said that Stalin prevented civilians from leaving the city in the belief that their presence would encourage greater resistance from the city's defenders. Civilians, including women and children, were put to work building trenchworks and protective fortifications. A massive German air raid on 23 August caused a firestorm, killing hundreds and turning Stalingrad into a vast landscape of rubble and burnt ruins. Ninety percent of the living space in the Voroshilovskiy area was destroyed. Between 23 and 26 August, Soviet reports indicate 955 people were killed and another 1,181 wounded as a result of the bombing. Casualties of 40,000 were greatly exaggerated, and after 25 August, the Soviets did not record any civilian and military casualties as a result of air raids.\n\nThe Soviet Air Force, the \"Voyenno-Vozdushnye Sily\" (VVS), was swept aside by the Luftwaffe. The VVS bases in the immediate area lost 201 aircraft between 23 and 31 August, and despite meager reinforcements of some 100 aircraft in August, it was left with just 192 serviceable aircraft, 57 of which were fighters. The Soviets continued to pour aerial reinforcements into the Stalingrad area in late September, but continued to suffer appalling losses; the \"Luftwaffe\" had complete control of the skies.\nThe burden of the initial defense of the city fell on the 1077th Anti-Aircraft Regiment, a unit made up mainly of young female volunteers who had no training for engaging ground targets. Despite this, and with no support available from other units, the AA gunners stayed at their posts and took on the advancing panzers. The German 16th Panzer Division reportedly had to fight the 1077th's gunners \"shot for shot\" until all 37 anti-aircraft guns were destroyed or overrun. The 16th Panzer was shocked to find that, due to Soviet manpower shortages, it had been fighting female soldiers. In the early stages of the battle, the NKVD organized poorly armed \"Workers' militias\" composed of civilians not directly involved in war production for immediate use in the battle. The civilians were often sent into battle without rifles. Staff and students from the local technical university formed a \"tank destroyer\" unit. They assembled tanks from leftover parts at the tractor factory. These tanks, unpainted and lacking gunsights, were driven directly from the factory floor to the front line. They could only be aimed at point-blank range through the gun barrel.\n\nBy the end of August, Army Group South (B) had finally reached the Volga, north of Stalingrad. Another advance to the river south of the city followed, while the Soviets abandoned their Rossoshka position for the inner defensive ring west of Stalingrad. The wings of the 6th Army and the 4th Panzer Army met near Jablotchni along the Zaritza on 2 Sept. By 1 September, the Soviets could only reinforce and supply their forces in Stalingrad by perilous crossings of the Volga under constant bombardment by artillery and aircraft.\n\nOn 5 September, the Soviet 24th and 66th Armies organized a massive attack against XIV Panzer Corps. The \"Luftwaffe\" helped repulse the offensive by heavily attacking Soviet artillery positions and defensive lines. The Soviets were forced to withdraw at midday after only a few hours. Of the 120 tanks the Soviets had committed, 30 were lost to air attack.\nSoviet operations were constantly hampered by the \"Luftwaffe\". On 18 September, the Soviet 1st Guards and 24th Army launched an offensive against VIII Army Corps at Kotluban. \"VIII. Fliegerkorps\" dispatched wave after wave of Stuka dive-bombers to prevent a breakthrough. The offensive was repulsed. The Stukas claimed 41 of the 106 Soviet tanks knocked out that morning, while escorting Bf 109s destroyed 77 Soviet aircraft.\nAmid the debris of the wrecked city, the Soviet 62nd and 64th Armies, which included the Soviet 13th Guards Rifle Division, anchored their defense lines with strongpoints in houses and factories.\nFighting within the ruined city was fierce and desperate. Lieutenant General Alexander Rodimtsev was in charge of the 13th Guards Rifle Division, and received one of two Heroes of the Soviet Union awarded during the battle for his actions. Stalin's Order No. 227 of 27 July 1942 decreed that all commanders who ordered unauthorized retreat would be subject to a military tribunal. However, it was the NKVD that ordered the regular army and lectured them, on the need to show some guts. Through brutal coercion for self-sacrifice, thousands of deserters and presumed malingerers were captured or executed to discipline the troops. At Stalingrad, it is estimated that 14,000 soldiers of the Red Army were executed in order to keep the formation. \"Not a step back!\" and \"There is no land behind the Volga!\" were the slogans. The Germans pushing forward into Stalingrad suffered heavy casualties.\n\nBy 12 September, at the time of their retreat into the city, the Soviet 62nd Army had been reduced to 90 tanks, 700 mortars and just 20,000 personnel. The remaining tanks were used as immobile strongpoints within the city. The initial German attack attempted to take the city in a rush. One infantry division went after the Mamayev Kurgan hill, one attacked the central rail station and one attacked toward the central landing stage on the Volga.\nThough initially successful, the German attacks stalled in the face of Soviet reinforcements brought in from across the Volga. The Soviet 13th Guards Rifle Division, assigned to counterattack at the Mamayev Kurgan and at Railway Station No. 1 suffered particularly heavy losses. Over 30 percent of its soldiers were killed in the first 24 hours, and just 320 out of the original 10,000 survived the entire battle. Both objectives were retaken, but only temporarily. The railway station changed hands 14 times in six hours. By the following evening, the 13th Guards Rifle Division had ceased to exist.\n\nCombat raged for three days at the giant grain elevator in the south of the city. About fifty Red Army defenders, cut off from resupply, held the position for five days and fought off ten different assaults before running out of ammunition and water. Only forty dead Soviet fighters were found, though the Germans had thought there were many more due to the intensity of resistance. The Soviets burned large amounts of grain during their retreat in order to deny the enemy food. Paulus chose the grain elevator and silos as the symbol of Stalingrad for a patch he was having designed to commemorate the battle after a German victory.\nGerman military doctrine was based on the principle of combined-arms teams and close cooperation between tanks, infantry, engineers, artillery and ground-attack aircraft. Some Soviet commanders adopted the tactic of always keeping their front-line positions as close to the Germans as physically possible; Chuikov called this \"hugging\" the Germans. This slowed the German advance and reduced the effectiveness of the German advantage in supporting fire.\n\nThe Red Army gradually adopted a strategy to hold for as long as possible all the ground in the city. Thus, they converted multi-floored apartment blocks, factories, warehouses, street corner residences and office buildings into a series of well defended strongpoints with small 5–10 man units. Manpower in the city was constantly refreshed by bringing additional troops over the Volga. When a position was lost, an immediate attempt was usually made to re-take it with fresh forces.\nBitter fighting raged for every ruin, street, factory, house, basement, and staircase. Even the sewers were the sites of firefights. The Germans, calling this unseen urban warfare \"Rattenkrieg\" (\"Rat War\"), bitterly joked about capturing the kitchen but still fighting for the living room and the bedroom. Buildings had to be cleared room by room through the bombed-out debris of residential neighborhoods, office blocks, basements and apartment high-rises. Some of the taller buildings, blasted into roofless shells by earlier German aerial bombardment, saw floor-by-floor, close quarters combat, with the Germans and Soviets on alternate levels, firing at each other through holes in the floors.\n\nFighting on and around Mamayev Kurgan, a prominent hill above the city, was particularly merciless; indeed, the position changed hands many times.\n\nIn another part of the city, a Soviet platoon under the command of Sergeant Yakov Pavlov fortified a four-story building that oversaw a square 300 meters from the river bank, later called \"Pavlov's House\". The soldiers surrounded it with minefields, set up machine-gun positions at the windows and breached the walls in the basement for better communications. The soldiers found about ten Soviet civilians hiding in the basement. They were not relieved, and not significantly reinforced, for two months. The building was labeled \"Festung\" (\"Fortress\") on German maps. Sgt. Pavlov was awarded the Hero of the Soviet Union for his actions.\nThe Germans made slow but steady progress through the city. Positions were taken individually, but the Germans were never able to capture the key crossing points along the river bank. By 27 Sept. the Germans occupied the southern portion of the city, but the Soviets held the center and northern part. Most importantly, the Soviets controlled the ferries to their supplies on the east bank of the Volga.\nThe Germans used airpower, tanks and heavy artillery to clear the city with varying degrees of success. Toward the end of the battle, the gigantic railroad gun nicknamed \"Dora\" was brought into the area. The Soviets built up a large number of artillery batteries on the east bank of the Volga. This artillery was able to bombard the German positions or at least to provide counter-battery fire.\nSnipers on both sides used the ruins to inflict casualties. The most famous Soviet sniper in Stalingrad was Vasily Zaytsev, with 225 confirmed kills during the battle. Targets were often soldiers bringing up food or water to forward positions. Artillery spotters were an especially prized target for snipers.\nA significant historical debate concerns the degree of terror in the Red Army. The British historian Antony Beevor noted the \"sinister\" message from the Stalingrad Front's Political Department on 8 October 1942 that: \"The defeatist mood is almost eliminated and the number of treasonous incidents is getting lower\" as an example of the sort of coercion Red Army soldiers experienced under the Special Detachments (later to be renamed SMERSH). On the other hand, Beevor noted the often extraordinary bravery of the Soviet soldiers in a battle that was only comparable to Verdun, and argued that terror alone cannot explain such self-sacrifice. Richard Overy addresses the question of just how important the Red Army's coercive methods were to the Soviet war effort compared with other motivational factors such as hatred for the enemy. He argues that, though it is \"easy to argue that from the summer of 1942 the Soviet army fought because it was forced to fight,\" to concentrate solely on coercion is nonetheless to \"distort our view of the Soviet war effort.\" After conducting hundreds of interviews with Soviet veterans on the subject of terror on the Eastern Front – and specifically about Order No. 227 (\"Not a step back!\") at Stalingrad – Catherine Merridale notes that, seemingly paradoxically, \"their response was frequently relief.\" Infantryman Lev Lvovich's explanation, for example, is typical for these interviews; as he recalls, \"[i]t was a necessary and important step. We all knew where we stood after we had heard it. And we all – it's true – felt better. Yes, we felt better.\"\nMany women fought on the Soviet side, or were under fire. As General Chuikov acknowledged, \"Remembering the defence of Stalingrad, I can't overlook the very important question ... about the role of women in war, in the rear, but also at the front. Equally with men they bore all the burdens of combat life and together with us men, they went all the way to Berlin.\" At the beginning of the battle there were 75,000 women and girls from the Stalingrad area who had finished military or medical training, and all of whom were to serve in the battle. Women staffed a great many of the anti-aircraft batteries that fought not only the Luftwaffe but German tanks. Soviet nurses not only treated wounded personnel under fire but were involved in the highly dangerous work of bringing wounded soldiers back to the hospitals under enemy fire. Many of the Soviet wireless and telephone operators were women who often suffered heavy casualties when their command posts came under fire. Though women were not usually trained as infantry, many Soviet women fought as machine gunners, mortar operators, and scouts. Women were also snipers at Stalingrad. Three air regiments at Stalingrad were entirely female. At least three women won the title Hero of the Soviet Union while driving tanks at Stalingrad.\nFor both Stalin and Hitler, Stalingrad became a matter of prestige far beyond its strategic significance. The Soviet command moved units from the Red Army strategic reserve in the Moscow area to the lower Volga, and transferred aircraft from the entire country to the Stalingrad region.\n\nThe strain on both military commanders was immense: Paulus developed an uncontrollable tic in his eye, which eventually afflicted the left side of his face, while Chuikov experienced an outbreak of eczema that required him to have his hands completely bandaged. Troops on both sides faced the constant strain of close-range combat.\n\nDetermined to crush Soviet resistance, \"Luftflotte\" 4's \"Stukawaffe\" flew 900 individual sorties against Soviet positions at the \"Dzerzhinskiy\" Tractor Factory on 5 October. Several Soviet regiments were wiped out; the entire staff of the Soviet 339th Infantry Regiment was killed the following morning during an air raid.\n\nIn mid-October, the \"Luftwaffe\" intensified its efforts against remaining Red Army positions holding the west bank. \"Luftflotte\" 4 flew 2,000 sorties on 14 October and of bombs were dropped while German infantry surrounded the three factories. \"Stukageschwader\" 1, 2, and 77 had largely silenced Soviet artillery on the eastern bank of the Volga before turning their attention to the shipping that was once again trying to reinforce the narrowing Soviet pockets of resistance. The 62nd Army had been cut in two, and, due to intensive air attack on its supply ferries, was receiving much less material support. With the Soviets forced into a strip of land on the western bank of the Volga, over 1,208 \"Stuka\" missions were flown in an effort to eliminate them.\n\nThe \"Luftwaffe\" retained air superiority into November and Soviet daytime aerial resistance was nonexistent. However, the combination of constant air support operations on the German side and the Soviet surrender of the daytime skies began to affect the strategic balance in the air. After flying 20,000 individual sorties, the \"Luftwaffe\" original strength of 1,600 serviceable aircraft had fallen to 950. The \"Kampfwaffe\" (bomber force) had been hardest hit, having only 232 out of a force of 480 left. The \"VVS\" remained qualitatively inferior, but by the time of the Soviet counter-offensive, the \"VVS\" had reached numerical superiority.\nThe Soviet bomber force, the \"Aviatsiya Dal'nego Deystviya\" (Long Range Aviation; ADD), having taken crippling losses over the past 18 months, was restricted to flying at night. The Soviets flew 11,317 night sorties over Stalingrad and the Don-bend sector between 17 July and 19 November. These raids caused little damage and were of nuisance value only.\n\nOn 8 November, substantial units from \"Luftflotte\" 4 were withdrawn to combat the Allied landings in North Africa. The German air arm found itself spread thinly across Europe, struggling to maintain its strength in the other southern sectors of the Soviet-German front. The Soviets began receiving material assistance from the American government under the Lend-Lease program. During the last quarter of 1942, the U.S. sent the Soviet Union of explosives and of aviation fuel.\n\nAs historian Chris Bellamy notes, the Germans paid a high strategic price for the aircraft sent into Stalingrad: the \"Luftwaffe\" was forced to divert much of its air strength away from the oil-rich Caucasus, which had been Hitler's original grand-strategic objective.\n\nAfter three months of slow advance, the Germans finally reached the river banks, capturing 90% of the ruined city and splitting the remaining Soviet forces into two narrow pockets. Ice floes on the Volga now prevented boats and tugs from supplying the Soviet defenders. Nevertheless, the fighting, especially on the slopes of Mamayev Kurgan and inside the factory area in the northern part of the city, continued.\n\nRecognizing that German troops were ill prepared for offensive operations during the winter of 1942, and that most of them were redeployed elsewhere on the southern sector of the Eastern Front, the Stavka decided to conduct a number of offensive operations between 19 November 1942 and 2 February 1943. These operations opened the Winter Campaign of 1942–1943 (19 November 1942 – 3 March 1943), which involved some 15 Armies operating on several fronts.\n\nAccording to Zhukov, \"German operational blunders were aggravated by poor intelligence: they failed to spot preparations for the major counter-offensive near Stalingrad where there were 10 field, 1 tank and 4 air armies.\"\n\nDuring the siege, the German and allied Italian, Hungarian, and Romanian armies protecting Army Group B's flanks had pressed their headquarters for support. The Hungarian 2nd Army was given the task of defending a section of the front north of Stalingrad between the Italian Army and Voronezh. This resulted in a very thin line, with some sectors where stretches were being defended by a single platoon. These forces were also lacking in effective anti-tank weapons.\n\nZhukov states, \"Compared with the Germans, the troops of the satellites were not so well armed, less experienced and less efficient, even in defence.\"\n\nBecause of the total focus on the city, the Axis forces had neglected for months to consolidate their positions along the natural defensive line of the Don River. The Soviet forces were allowed to retain bridgeheads on the right bank from which offensive operations could be quickly launched. These bridgeheads in retrospect presented a serious threat to Army Group B.\n\nSimilarly, on the southern flank of the Stalingrad sector the front southwest of Kotelnikovo was held only by the Romanian 4th Army. Beyond that army, a single German division, the 16th Motorized Infantry, covered 400 km. Paulus had requested permission to \"withdraw the 6th Army behind the Don,\" but was rejected. According to Paulus' comments to Adam, \"There is still the order whereby no commander of an army group or an army has the right to relinquish a village, even a trench, without Hitler's consent.\"\n\nIn autumn, the Soviet generals Georgy Zhukov and Aleksandr Vasilevsky, responsible for strategic planning in the Stalingrad area, concentrated forces in the steppes to the north and south of the city. The northern flank was defended by Hungarian and Romanian units, often in open positions on the steppes. The natural line of defense, the Don River, had never been properly established by the German side. The armies in the area were also poorly equipped in terms of anti-tank weapons. The plan was to punch through the overstretched and weakly defended German flanks and surround the German forces in the Stalingrad region.\n\nDuring the preparations for the attack, Marshal Zhukov personally visited the front and noticing the poor organization, insisted on a one-week delay in the start date of the planned attack. The operation was code-named \"Uranus\" and launched in conjunction with Operation Mars, which was directed at Army Group Center. The plan was similar to the one Zhukov had used to achieve victory at Khalkhin Gol three years before, where he had sprung a double envelopment and destroyed the 23rd Division of the Japanese army.\n\nOn 19 November 1942, the Red Army launched Operation Uranus. The attacking Soviet units under the command of Gen. Nikolay Vatutin consisted of three complete armies, the 1st Guards Army, 5th Tank Army, and 21st Army, including a total of 18 infantry divisions, eight tank brigades, two motorized brigades, six cavalry divisions and one anti-tank brigade. The preparations for the attack could be heard by the Romanians, who continued to push for reinforcements, only to be refused again. Thinly spread, deployed in exposed positions, outnumbered and poorly equipped, the Romanian 3rd Army, which held the northern flank of the German 6th Army, was overrun.\n\nBehind the front lines, no preparations had been made to defend key points in the rear such as Kalach. The response by the \"Wehrmacht\" was both chaotic and indecisive. Poor weather prevented effective air action against the Soviet offensive.\n\nOn 20 November, a second Soviet offensive (two armies) was launched to the south of Stalingrad against points held by the Romanian 4th Army Corps. The Romanian forces, made up primarily of infantry, were overrun by large numbers of tanks. The Soviet forces raced west and met on 23 November at the town of Kalach, sealing the ring around Stalingrad. The link-up of the Soviet forces, not filmed at the time, was later re-enacted for a propaganda film which was shown worldwide.\n\nAbout 265,000 German, Romanian, and Italian soldiers, the 369th (Croatian) Reinforced Infantry Regiment, and other volunteer subsidiary troops including some 40,000 Soviet conscripts and volunteers fighting for the Germans (Beevor states that one quarter of the sixth army's frontline strength were HIWIs, as collaborationists recruited from the ranks of Soviet POWs were called) were surrounded. These Soviet HIWIs remained loyal, knowing the Soviet penalty for helping the Germans was summary execution. German strength in the pocket was about 210,000 according to strength breakdowns of the 20 field divisions (average size 9,000) and 100 battalion sized units of the Sixth Army on 19 November 1942. Inside the pocket (, literally \"cauldron\"), there were also around 10,000 Soviet civilians and several thousand Soviet soldiers the Germans had taken captive during the battle. Not all of the 6th Army was trapped; 50,000 soldiers were brushed aside outside the pocket. These belonged mostly to the other 2 divisions of the 6th Army between the Italian and Romanian Armies: the 62nd and 298th Infantry Divisions. Of the 210,000 Germans, 10,000 remained to fight on, 105,000 surrendered, 35,000 left by air and the remaining 60,000 died.\n\nArmy Group Don was formed under Field Marshal von Manstein. Under his command were the 20 German and 2 Romanian divisions encircled at Stalingrad, Adam's battle groups formed along the Chir River and on the Don bridgehead, plus the remains of the Romanian 3rd Army.\n\nThe Red Army units immediately formed two defensive fronts: a circumvallation facing inward and a contravallation facing outward. Field Marshal Erich von Manstein advised Hitler not to order the 6th Army to break out, stating that he could break through the Soviet lines and relieve the besieged 6th Army. The American historians Williamson Murray and Alan Millet wrote that it was Manstein's message to Hitler on 24 November advising him that the 6th Army should not break out, along with Göring's statements that the Luftwaffe could supply Stalingrad that \"... sealed the fate of the Sixth Army.\" After 1945, Manstein claimed that he told Hitler that the 6th Army must break out. The American historian Gerhard Weinberg wrote that Manstein distorted his record on the matter. Manstein was tasked to conduct a relief operation, named Operation Winter Storm (\"Unternehmen Wintergewitter\") against Stalingrad, which he thought was feasible if the 6th Army was temporarily supplied through the air.\n\nAdolf Hitler had declared in a public speech (in the Berlin Sportpalast) on 30 September 1942 that the German army would never leave the city. At a meeting shortly after the Soviet encirclement, German army chiefs pushed for an immediate breakout to a new line on the west of the Don, but Hitler was at his Bavarian retreat of Obersalzberg in Berchtesgaden with the head of the \"Luftwaffe\", Hermann Göring. When asked by Hitler, Göring replied, after being convinced by Hans Jeschonnek, that the Luftwaffe could supply the 6th Army with an \"air bridge.\" This would allow the Germans in the city to fight on temporarily while a relief force was assembled. A similar plan had been used a year earlier at the Demyansk Pocket, albeit on a much smaller scale: a corps at Demyansk rather than an entire army.\nThe director of \"Luftflotte\" 4, Wolfram von Richthofen, tried to get this decision overturned. The forces under the 6th Army were almost twice as large as a regular German army unit, plus there was also a corps of the 4th Panzer Army trapped in the pocket. The maximum they could deliver a day – based on the number of available aircraft and with only the airfield at Pitomnik to land at – was far less than the minimum needed. To supplement the limited number of Junkers Ju 52 transports, the Germans pressed other aircraft into the role, such as the Heinkel He 177 bomber (some bombers performed adequately – the Heinkel He 111 proved to be quite capable and was much faster than the Ju 52). General Richthofen informed Manstein on 27 November of the small transport capacity of the Luftwaffe and the impossibility of supplying 300 tons a day by air. Manstein now saw the enormous technical difficulties of a supply by air of these dimensions. The next day he made a six-page situation report to the general staff. Based on the information of the expert Richthofen, he declared that contrary to the example of the pocket of Demyansk the permanent supply by air would be impossible. If only a narrow link could be established to Sixth Army, he proposed that this should be used to pull it out from the encirclement, and said that the Luftwaffe should instead of supplies deliver only enough ammunition and fuel for a breakout attempt. He acknowledged the heavy moral sacrifice that giving up Stalingrad would mean, but this would be made easier to bear by the conserving the combat power of Sixth Army and regaining the initiative. He ignored the limited mobility of the army and the difficulties of disengaging the Soviets. Hitler reiterated that Sixth Army would stay at Stalingrad and that the air bridge would supply it until the encirclement was broken by a new German offensive.\n\nSupplying the 270,000 men trapped in the \"cauldron\" required 700 tons of supplies a day. That would mean 350 Ju52 flights a day into Pitomnik. At a minimum, 500 tons were required. However, according to Adam, \"On not one single day have the minimal essential number of tons of supplies been flown in.\" The \"Luftwaffe\" was able to deliver an average of of supplies per day out of an air transport capacity of per day. The most successful day, 19 December, delivered of supplies in 154 flights. The outcome of the airlift was the Luftwaffe's failure to provide its transport units with the tools they needed to maintain an adequate count of operational aircraft – tools that included airfield facilities, supplies, manpower, and even aircraft suited to the prevailing conditions. These factors, taken together, prevented the Luftwaffe from effectively employing the full potential of its transport forces, ensuring that they were unable to deliver the quantity of supplies needed to sustain the 6th Army.\n\nIn the early parts of the operation, fuel was shipped at a higher priority than food and ammunition because of a belief that there would be a breakout from the city. Transport aircraft also evacuated technical specialists and sick or wounded personnel from the besieged enclave. Sources differ on the number flown out: at least 25,000 to at most 35,000. Carell: 42,000, of which 5000 did not survive.\nInitially, supply flights came in from the field at Tatsinskaya, called 'Tazi' by the German pilots. On 23 December, the Soviet 24th Tank Corps, commanded by Major-General Vasily Mikhaylovich Badanov, reached nearby Skassirskaya and in the early morning of 24 December, the tanks reached Tatsinskaya. Without any soldiers to defend the airfield, it was abandoned under heavy fire; in a little under an hour, 108 Ju 52s and 16 Ju 86s took off for Novocherkassk – leaving 72 Ju 52s and many other aircraft burning on the ground. A new base was established some from Stalingrad at Salsk, the additional distance would become another obstacle to the resupply efforts. Salsk was abandoned in turn by mid-January for a rough facility at Zverevo, near Shakhty. The field at Zverevo was attacked repeatedly on 18 January and a further 50 Ju 52s were destroyed. Winter weather conditions, technical failures, heavy Soviet anti-aircraft fire and fighter interceptions eventually led to the loss of 488 German aircraft.\n\nIn spite of the failure of the German offensive to reach the 6th Army, the air supply operation continued under ever more difficult circumstances. The 6th Army slowly starved. General Zeitzler, moved by their plight, began to limit himself to their slim rations at meal times. After a few weeks on such a diet, he had \"visibly lost weight\", according to Albert Speer, and Hitler \"commanded Zeitler to resume at once taking sufficient nourishment.\" \n\nThe toll on the \"Transportgruppen\" was heavy. 160 aircraft were destroyed and 328 were heavily damaged (beyond repair). Some 266 Junkers Ju 52s were destroyed; one-third of the fleet's strength on the Eastern Front. The He 111 \"gruppen\" lost 165 aircraft in transport operations. Other losses included 42 Ju 86s, 9 Fw 200 Condors, 5 He 177 bombers and 1 Ju 290. The \"Luftwaffe\" also lost close to 1,000 highly experienced bomber crew personnel. So heavy were the \"Luftwaffe\"s losses that four of \"Luftflotte\" 4's transport units (KGrzbV 700, KGrzbV 900, I./KGrzbV 1 and II./KGzbV 1) were \"formally dissolved.\"\n\nSoviet forces consolidated their positions around Stalingrad, and fierce fighting to shrink the pocket began. Operation Winter Storm (\"Operation Wintergewitter\"), the German attempt led by Erich von Manstein to relieve the trapped army from the south, was initially successful. By 18 December, the German Army had pushed to within 48 km (30 mi) of Sixth Army's positions. The starving encircled forces at Stalingrad made no attempt to break out or link up with Manstein's advance. Some German officers requested that Paulus defy Hitler's orders to stand fast and instead attempt to break out of the Stalingrad pocket. Paulus refused, concerned about the Red Army attacks on the flank of Army Group Don and Army Group B in their advance on Rostov-on-Don, \"an early abandonment\" of Stalingrad \"would result in the destruction of Army Group A in the Caucasus,\" and the fact that his 6th Army tanks only had fuel for a 30 km advance towards Hoth's spearhead, a futile effort if they did not receive assurance of resupply by air. Of his questions to Army Group Don, Paulus was told, \"Wait, implement Operation 'Thunderclap' only on explicit orders!\" Operation Thunderclap being the code word initiating the breakout.\n\nOn 23 December, the attempt to relieve Stalingrad was abandoned and Manstein's forces switched over to the defensive to deal with new Soviet offensives. As Zhukov states, \"The military and political leadership of Nazi Germany sought not to relieve them, but to get them to fight on for as long possible so as to tie up the Soviet forces. The aim was to win as much time as possible to withdraw forces from the Caucasus and to rush troops from other Fronts to form a new front that would be able in some measure to check our counter-offensive.\"\n\nOn 16 December, the Soviets launched Operation Little Saturn, which attempted to punch through the Axis army (mainly Italians) on the Don and take Rostov. The Germans set up a \"mobile defense\" of small units that were to hold towns until supporting armor arrived. From the Soviet bridgehead at Mamon, 15 divisions – supported by at least 100 tanks – attacked the Italian Cosseria and Ravenna Divisions, and although outnumbered 9 to 1, the Italians initially fought well, with the Germans praising the quality of the Italian defenders, but on 19 December, with the Italian lines disintegrating, ARMIR headquarters ordered the battered divisions to withdraw to new lines.\n\nThe fighting forced a total revaluation of the German situation. The attempt to break through to Stalingrad was abandoned and Army Group A was ordered to pull back from the Caucasus. The 6th Army now was beyond all hope of German relief. While a motorised breakout might have been possible in the first few weeks, the 6th Army now had insufficient fuel and the German soldiers would have faced great difficulty breaking through the Soviet lines on foot in harsh winter conditions. But in its defensive position on the Volga, 6th Army continued to tie down a significant number of Soviet Armies.\n\nThe Red Army High Command sent three envoys while simultaneously aircraft and loudspeakers announced terms of capitulation on 7 Jan. 1943. The letter was signed by Colonel-General of Artillery Voronov and the commander-in-chief of the Don Front, Lieutenant-General Rokossovski. The German High Command informed Paulus, \"Every day that the army holds out longer helps the whole front and draws away the Russian divisions from it.\"\n\nThe Germans inside the pocket retreated from the suburbs of Stalingrad to the city itself. The loss of the two airfields, at Pitomnik on 16 January 1943 and Gumrak on the night of 21/22 January, meant an end to air supplies and to the evacuation of the wounded. The third and last serviceable runway was at the Stalingradskaya flight school, which reportedly had the last landings and takeoffs on 23 Jan. After 23 Jan. there were no more reported landings, just intermittent air drops of ammunition and food until the end.\nThe Germans were now not only starving, but running out of ammunition. Nevertheless, they continued to resist, in part because they believed the Soviets would execute any who surrendered. In particular, the so-called \"HiWis\", Soviet citizens fighting for the Germans, had no illusions about their fate if captured. The Soviets were initially surprised by the number of Germans they had trapped, and had to reinforce their encircling troops. Bloody urban warfare began again in Stalingrad, but this time it was the Germans who were pushed back to the banks of the Volga. The Germans adopted a simple defense of fixing wire nets over all windows to protect themselves from grenades. The Soviets responded by fixing fish hooks to the grenades so they stuck to the nets when thrown.\n\nThe Germans had no usable tanks in the city, and those that still functioned could, at best, be used as makeshift pillboxes. The Soviets did not bother employing tanks in areas where the urban destruction restricted their mobility. A low-level Soviet envoy party (comprising Major Aleksandr Smyslov, Captain Nikolay Dyatlenko and a trumpeter) carried an offer to Paulus: if he surrendered within 24 hours, he would receive a guarantee of safety for all prisoners, medical care for the sick and wounded, prisoners being allowed to keep their personal belongings, \"normal\" food rations, and repatriation to any country they wished after the war; but Paulus – ordered not to surrender by Hitler – did not respond.\n\nOn 22 January, Paulus requested that he be granted permission to surrender. Hitler rejected it on a point of honour. He telegraphed the 6th Army later that day, claiming that it had made a historic contribution to the greatest struggle in German history and that it should stand fast \"to the last soldier and the last bullet.\" Hitler told Goebbels that the plight of the 6th Army was a \"heroic drama of German history.\"\n\nOn 24 Jan., in his radio report to Hitler, Paulus reported \"18,000 wounded without the slightest aid of bandages and medicines.\"\n\nOn 26 January 1943, the German forces inside Stalingrad were split into two pockets north and south of Mamai-Kurgan. The northern pocket consisting of the VIIIth Corps, under General Walter Heitz, and the XIth Corps, was now cut off from telephone communication with Paulus in the southern pocket. Now \"each part of the cauldron came personally under Hitler.\"\n\nOn 28 Jan., the cauldron was split into three parts. The northern cauldron consisted of the XIth Corps, the central with the VIIIth and LIst Corps, and the southern with the XIVth Panzer Corps and IVth Corps \"without units\". The sick and wounded reached 40,000 to 50,000.\n\nOn 30 January 1943, the 10th anniversary of Hitler's coming to power, Goebbels read out a proclamation that included the sentence: \"The heroic struggle of our soldiers on the Volga should be a warning for everybody to do the utmost for the struggle for Germany's freedom and the future of our people, and thus in a wider sense for the maintenance of our entire continent.\" Hitler promoted Paulus to the rank of \"Generalfeldmarschall\". No German field marshal had ever surrendered, and the implication was clear: if Paulus surrendered, he would shame himself and would become the highest ranking German officer ever to be captured. Hitler believed that Paulus would either fight to the last man or commit suicide.\n\nOn the next day, the southern pocket in Stalingrad collapsed. Soviet forces reached the entrance to the German headquarters in the ruined GUM department store. General Schmidt negotiated a surrender of the headquarters while Paulus was unaware in another room. When interrogated by the Soviets, Paulus claimed that he had not surrendered. He said that he had been taken by surprise. He denied that he was the commander of the remaining northern pocket in Stalingrad and refused to issue an order in his name for them to surrender. The central pocket, under the command of Heitz, surrendered the same day while the northern pocket, under the command of Strecker, held out for two more days.\n\nFour Soviet armies were deployed against the remaining northern pocket. At four in the morning on 2 February, General Strecker was informed that one of his own officers had gone to the Soviets to negotiate surrender terms. Seeing no point in continuing, he sent a radio message saying that his command had done its duty and fought to the last man. He then surrendered. Around 91,000 exhausted, ill, wounded, and starving prisoners were taken, including 3,000 Romanians (the survivors of the 20th Infantry Division, 1st Cavalry Division and \"Col. Voicu\" Detachment). The prisoners included 22 generals. Hitler was furious and confided that Paulus \"could have freed himself from all sorrow and ascended into eternity and national immortality, but he prefers to go to Moscow.\"\n\nThe German public was not officially told of the impending disaster until the end of January 1943, though positive media reports had stopped in the weeks before the announcement. Stalingrad marked the first time that the Nazi government publicly acknowledged a failure in its war effort; it was not only the first big defeat for the German military but German losses were almost equal to those of the Soviets and this was unprecedented. Previous Soviet losses were generally three times as high as those of the Germans. On 31 January, regular programmes on German state radio were replaced by a broadcast of the somber Adagio movement from Anton Bruckner's Seventh Symphony, followed by the announcement of the defeat at Stalingrad. On 18 February, Minister of Propaganda Joseph Goebbels gave the famous \"Sportpalast\" speech in Berlin, encouraging the Germans to accept a total war that would claim all resources and efforts from the entire population.\nBased on Soviet records, over 10,000 soldiers continued to resist in isolated groups within the city for the next month. Some have presumed that they were motivated by a belief that fighting on was better than a slow death in Soviet captivity. Brown University historian Omer Bartov claims they were motivated by National Socialism. He studied 11,237 letters sent by soldiers inside of Stalingrad between 20 December 1942 and 16 January 1943 to their families in Germany. Almost every letter expressed belief in Germany's ultimate victory and their willingness to fight and die at Stalingrad to achieve that victory. Bartov reported that a great many of the soldiers were well aware that they would not be able to escape from Stalingrad but in their letters to their families boasted that they were proud to \"sacrifice themselves for the Führer\".\n\nThe remaining forces continued to resist, hiding in cellars and sewers but by early March 1943, the last small and isolated pockets of resistance had surrendered. According to Soviet intelligence documents shown in the documentary, a remarkable NKVD report from March 1943 is available showing the tenacity of some of these German groups:\n\nThe operative report of the Don Front's staff issued on 5 February 1943, 22:00 said,\n\nThe condition of the troops that surrendered was pitiful. British war correspondent Alexander Werth described the following scene in his \"Russia at War\" book, based on a first-hand account of his visit to Stalingrad from 3–5 February 1943,\n\nOut of the nearly 91,000 German prisoners captured in Stalingrad, only about 5,000 returned. Weakened by disease, starvation and lack of medical care during the encirclement, they were sent on foot marches to prisoner camps and later to labour camps all over the Soviet Union. Some 35,000 were eventually sent on transports, of which 17,000 did not survive. Most died of wounds, disease (particularly typhus), cold, overwork, mistreatment and malnutrition. Some were kept in the city to help rebuild.\n\nA handful of senior officers were taken to Moscow and used for propaganda purposes, and some of them joined the National Committee for a Free Germany. Some, including Paulus, signed anti-Hitler statements that were broadcast to German troops. Paulus testified for the prosecution during the Nuremberg Trials and assured families in Germany that those soldiers taken prisoner at Stalingrad were safe. He remained in the Soviet Union until 1952, then moved to Dresden in East Germany, where he spent the remainder of his days defending his actions at Stalingrad and was quoted as saying that Communism was the best hope for postwar Europe. General Walther von Seydlitz-Kurzbach offered to raise an anti-Hitler army from the Stalingrad survivors, but the Soviets did not accept. It was not until 1955 that the last of the 5,000–6,000 survivors were repatriated (to West Germany) after a plea to the Politburo by Konrad Adenauer.\n\nStalingrad has been described as the biggest defeat in the history of the German Army. It is often identified as the turning point on the Eastern Front, in the war against Germany overall, and the entire Second World War. Before Stalingrad, the German forces had gone from victory to victory on the Eastern Front, with a limited setback in the winter of 1941–42. After Stalingrad, they won no decisive battles, even in summer. The Red Army had the initiative, and the Wehrmacht was in retreat. A year of German gains during Case Blue had been wiped out. Germany's Sixth Army had ceased to exist, and the forces of Germany's European allies, except Finland, had been shattered. In a speech on 9 November 1944, Hitler himself blamed Stalingrad for Germany's impending doom.\n\nStalingrad's significance has been downplayed by some historians, who point either to the Battle of Moscow or the Battle of Kursk as more strategically decisive. Others maintain that the destruction of an entire army (the largest killed, captured, wounded figures for Axis soldiers, nearly 1 million, during the war) and the frustration of Germany's grand strategy made the battle a watershed moment. At the time, however, the global significance of the battle was not in doubt. Writing in his diary on 1 January 1943, British General Alan Brooke, Chief of the Imperial General Staff, reflected on the change in the position from a year before:\n\nAt this point, the British had won the Battle of El Alamein in November 1942. However, there were only about 50,000 German soldiers at El Alamein in Egypt, while at Stalingrad 200,000 Germans had been lost.\n\nRegardless of the strategic implications, there is little doubt about Stalingrad's symbolism. Germany's defeat shattered its reputation for invincibility and dealt a devastating blow to German morale. On 30 January 1943, the tenth anniversary of his coming to power, Hitler chose not to speak. Joseph Goebbels read the text of his speech for him on the radio. The speech contained an oblique reference to the battle, which suggested that Germany was now in a defensive war. The public mood was sullen, depressed, fearful, and war-weary. Germany was looking in the face of defeat.\n\nThe reverse was the case on the Soviet side. There was an overwhelming surge in confidence and belief in victory. A common saying was: \"You cannot stop an army which has done Stalingrad.\" Stalin was feted as the hero of the hour and made a Marshal of the Soviet Union. In recognition of the determination of its defenders, Stalingrad was awarded the title Hero City in 1945. A colossal monument called The Motherland Calls was erected in 1967 on Mamayev Kurgan, the hill overlooking the city where bones and rusty metal splinters can still be found. The statue forms part of a war memorial complex which includes the ruins of the Grain Silo and Pavlov's House.\n\nThe news of the battle echoed round the world, with many people now believing that Hitler's defeat was inevitable. The Turkish Consul in Moscow predicted that \"the lands which the Germans have destined for their living space will become their dying space\". Britain's conservative \"The Daily Telegraph\" proclaimed that the victory had saved European civilisation. The country celebrated \"Red Army Day\" on 23 February 1943. A ceremonial Sword of Stalingrad was forged by King George VI. After being put on public display in Britain, this was presented to Stalin by Winston Churchill at the Tehran Conference later in 1943. Soviet propaganda spared no effort and wasted no time in capitalising on the triumph, impressing a global audience. The prestige of Stalin, the Soviet Union, and the worldwide Communist movement was immense, and their political position greatly enhanced.\n\nOn 3 February 2013 Russian President Vladimir Putin flew to the city to join a military parade commemorating the 70th anniversary of the final victory.\n\nDuring the defence of Stalingrad, the Red Army deployed five armies (28th, 51st, 57th, 62nd and 64th Armies) in and around the city and an additional nine armies in the encirclement counter offensive. The nine armies amassed for the counteroffensive were the 24th Army, 65th Army, 66th Army and 16th Air Army from the north as part of the Don Front offensive and 1st Guards Army, 5th Tank, 21st Army, 2nd Air Army and 17th Air Army from the south as part of the Southwestern Front.\n\nThe calculation of casualties depends on what scope is given to the Battle of Stalingrad. The scope can vary from the fighting in the city and suburbs to the inclusion of almost all fighting on the southern wing of the Soviet–German front from the spring of 1942 to the end of the fighting in the city in the winter of 1943. Scholars have produced different estimates depending on their definition of the scope of the battle. The difference is comparing the city against the region. The Axis suffered 730,000 total casualties (wounded, killed, captured) among all branches of the German armed forces and its allies; 400,000 Germans, 109,000 Romanians of which at least 70,000 were captured or missing, 114,000 Italians and 105,000 Hungarians were killed, wounded or captured.\n\nThe Germans lost 900 aircraft (including 274 transports and 165 bombers used as transports), 500 tanks and 6,000 artillery pieces. According to a contemporary Soviet report, 5,762 guns, 1,312 mortars, 12,701 heavy machine guns, 156,987 rifles, 80,438 sub-machine guns, 10,722 trucks, 744 aircraft; 1,666 tanks, 261 other armored vehicles, 571 half-tracks and 10,679 motorcycles were captured by the Soviets. An unknown amount of Hungarian, Italian, and Romanian materiel was lost.\n\nThe USSR, according to archival figures, suffered 1,129,619 total casualties; 478,741 personnel killed or missing, and 650,878 wounded or sick. The USSR lost 4,341 tanks destroyed or damaged, 15,728 artillery pieces and 2,769 combat aircraft. 955 Soviet civilians died in Stalingrad and its suburbs from aerial bombing by \"Luftflotte\" 4 as the German 4th Panzer and 6th Armies approached the city.\n\nThe losses of transport planes were especially serious, as they destroyed the capacity for supply of the trapped 6th Army. The destruction of 72 aircraft when the airfield at Tatsinskaya was overrun meant the loss of about 10 percent of the Luftwaffe transport fleet.\n\nThese losses amounted to about 50 percent of the aircraft committed and the Luftwaffe training program was stopped and sorties in other theaters of war were significantly reduced to save fuel for use at Stalingrad.\n\nThe events of the Battle for Stalingrad have been covered in numerous media works of British, American, German, and Russian origin, for its significance as a turning point in the Second World War and for the loss of life associated with the battle. The term Stalingrad has become almost synonymous with large-scale urban battles with high casualties on both sides.\n\n\n\n"}
{"id": "4285", "url": "https://en.wikipedia.org/wiki?curid=4285", "title": "Bodhidharma", "text": "Bodhidharma\n\nBodhidharma was a Buddhist monk who lived during the 5th or 6th century. He is traditionally credited as the transmitter of Chan Buddhism to China, and regarded as its first Chinese patriarch. According to Chinese legend, he also began the physical training of the monks of Shaolin Monastery that led to the creation of Shaolin kungfu. In Japan, he is known as Daruma.\n\nLittle contemporary biographical information on Bodhidharma is extant, and subsequent accounts became layered with legend.\n\nAccording to the principal Chinese sources, Bodhidharma came from the Western Regions, which refers to Central Asia but may also include the Indian subcontinent, and was either a \"Persian Central Asian\" or a \"South Indian [...] the third son of a great Indian king.\"\n\nThroughout Buddhist art, Bodhidharma is depicted as an ill-tempered, profusely-bearded, wide-eyed non-Chinese person. He is referred as \"The Blue-Eyed Barbarian\" () in Chan texts.\n\nAside from the Chinese accounts, several popular traditions also exist regarding Bodhidharma's origins.\n\nThe accounts also differ on the date of his arrival, with one early account claiming that he arrived during the Liu Song dynasty (420–479) and later accounts dating his arrival to the Liang dynasty (502–557). Bodhidharma was primarily active in the territory of the Northern Wei (386-634). Modern scholarship dates him to about the early 5th century.\n\nBodhidharma's teachings and practice centered on meditation and the \"Laṅkāvatāra Sūtra\". The \"Anthology of the Patriarchal Hall\" (952) identifies Bodhidharma as the 28th Patriarch of Buddhism in an uninterrupted line that extends all the way back to the Gautama Buddha himself.\n\nThere are two known extant accounts written by contemporaries of Bodhidharma. According to these sources, Bodhidharma came from the Western regions, and was either a \"Persian Central Asian\" or a \"South Indian [...] the third son of a great Indian king.\" Later sources draw on these two sources, adding additional details, including a change to being descendent from a \"Brahmin\" king, which accords with the reign of the Pallavas, who were Brahmins.\n\nThe \"Western Regions\" was a historical name specified in the Chinese chronicles between the 3rd century BC to the 8th century AD that referred to the regions west of Yumen Pass, most often Central Asia or sometimes more specifically the easternmost portion of it (e.g. Altishahr or the Tarim Basin in southern Xinjiang). Sometimes it was used more generally to refer to other regions to the west of China as well, such as the Indian subcontinent (as in the novel \"Journey to the West\").\n\nThe earliest text mentioning Bodhidharma is \"The Record of the Buddhist Monasteries of Luoyang\" ( \"Luòyáng Qiélánjì\") which was compiled in 547 by Yáng Xuànzhī (楊衒之), a writer and translator of Mahayana sutras into Chinese. Yang gave the following account:\n\nThe second account was written by Tánlín (曇林; 506–574). Tánlín's brief biography of the \"Dharma Master\" is found in his preface to the \"Long Scroll of the Treatise on the Two Entrances and Four Practices\", a text traditionally attributed to Bodhidharma and the first text to identify him as South Indian:\nTánlín's account was the first to mention that Bodhidharma attracted disciples, specifically mentioning Dàoyù (道育) and Dazu Huike (慧可), the latter of whom would later figure very prominently in the Bodhidharma literature. Although Tánlín has traditionally been considered a disciple of Bodhidharma, it is more likely that he was a student of Huìkě.\n\nTanlin's preface has also been preserved in Jingjue's (683-750) \"Lengjie Shizi ji\" \"Chronicle of the \"Laṅkāvatāra\" Masters\", which dates from 713-716./ca. 715 He writes, \nIn the 7th-century historical work \"Further Biographies of Eminent Monks\" (續高僧傳 \"Xù gāosēng zhuàn\"), Dàoxuān (道宣; 596-667) possibly drew on Tanlin's preface as a basic source, but made several significant additions:\n\nFirstly, Dàoxuān adds more detail concerning Bodhidharma's origins, writing that he was of \"South Indian Brahman stock\" (南天竺婆羅門種 \"nán tiānzhú póluómén zhŏng\").\n\nSecondly, more detail is provided concerning Bodhidharma's journeys. Tanlin's original is imprecise about Bodhidharma's travels, saying only that he \"crossed distant mountains and seas\" before arriving in Wei. Dàoxuān's account, however, implies \"a specific itinerary\": \"He first arrived at Nan-yüeh during the Sung period. From there he turned north and came to the Kingdom of Wei\" This implies that Bodhidharma had travelled to China by sea and that he had crossed over the Yangtze.\n\nThirdly, Dàoxuān suggests a date for Bodhidharma's arrival in China. He writes that Bodhidharma makes landfall in the time of the Song, thus making his arrival no later than the time of the Song's fall to the Southern Qi in 479.\n\nFinally, Dàoxuān provides information concerning Bodhidharma's death. Bodhidharma, he writes, died at the banks of the Luo River, where he was interred by his disciple Dazu Huike, possibly in a cave. According to Dàoxuān's chronology, Bodhidharma's death must have occurred prior to 534, the date of the Northern Wei's fall, because Dazu Huike subsequently leaves Luoyang for Ye. Furthermore, citing the shore of the Luo River as the place of death might possibly suggest that Bodhidharma died in the mass executions at Heyin (河陰) in 528. Supporting this possibility is a report in the Chinese Buddhist canon stating that a Buddhist monk was among the victims at Héyīn.\n\nIn the \"Anthology of the Patriarchal Hall\" (祖堂集 \"Zǔtángjí\") of 952, the elements of the traditional Bodhidharma story are in place. Bodhidharma is said to have been a disciple of Prajñātāra, thus establishing the latter as the 27th patriarch in India. After a three-year journey, Bodhidharma reached China in 527, during the Liang (as opposed to the Song in Dàoxuān's text). The \"Anthology of the Patriarchal Hall\" includes Bodhidharma's encounter with Emperor Wu of Liang, which was first recorded around 758 in the appendix to a text by Shenhui (神會), a disciple of Huineng.\n\nFinally, as opposed to Daoxuan's figure of \"over 180 years,\" the \"Anthology of the Patriarchal Hall\" states that Bodhidharma died at the age of 150. He was then buried on Mount Xiong'er (熊耳山 \"Xióng'ĕr Shān\") to the west of Luoyang. However, three years after the burial, in the Pamir Mountains, Sòngyún (宋雲)—an official of one of the later Wei kingdoms—encountered Bodhidharma, who claimed to be returning to India and was carrying a single sandal. Bodhidharma predicted the death of Songyun's ruler, a prediction which was borne out upon the latter's return. Bodhidharma's tomb was then opened, and only a single sandal was found inside.\n\nAccording to the \"Anthology of the Patriarchal Hall\", Bodhidharma left the Liang court in 527 and relocated to Mount Song near Luoyang and the Shaolin Monastery, where he \"faced a wall for nine years, not speaking for the entire time\", his date of death can have been no earlier than 536. Moreover, his encounter with the Wei official indicates a date of death no later than 554, three years before the fall of the Western Wei.\n\nSubsequent to the \"Anthology of the Patriarchal Hall\", the only dated addition to the biography of Bodhidharma is in the \"Jingde Records of the Transmission of the Lamp\" (景德傳燈錄 \"Jĭngdé chuándēng lù\", published 1004 CE), by Dàoyuán (道原), in which it is stated that Bodhidharma's original name had been Bodhitāra but was changed by his master Prajñātāra. The same account is given by the Japanese master Keizan's 13th century work of the same title.\n\nSeveral contemporary popular traditions also exist regarding Bodhidharma's origins. An Indian tradition regards Bodhidharma to be the third son of a Pallava king from Kanchipuram. This is consistent with the Southeast Asian traditions which also describe Bodhidharma as a former South Indian Tamil prince who had awakened his kundalini and renounced royal life to become a monk. The Tibetan version similarly characterises him as a dark-skinned siddha from South India. Conversely, the Japanese tradition generally regards Bodhidharma as Persian.\n\nSeveral stories about Bodhidharma have become popular legends, which are still being used in the Ch'an, Seon and Zen-tradition.\n\nThe \"Anthology of the Patriarchal Hall\" says that in 527, Bodhidharma visited Emperor Wu of Liang (Xiāo Yǎn 蕭衍, posthumous name Wǔdì 武帝), a fervent patron of Buddhism:\nThis encounter was included as the first kōan of the \"Blue Cliff Record\".\n\nFailing to make a favorable impression in South China, Bodhidharma is said to have travelled to the Shaolin Monastery. After either being refused entry or being ejected after a short time, he lived in a nearby cave, where he \"faced a wall for nine years, not speaking for the entire time\".\n\nThe biographical tradition is littered with apocryphal tales about Bodhidharma's life and circumstances. In one version of the story, he is said to have fallen asleep seven years into his nine years of wall-gazing. Becoming angry with himself, he cut off his eyelids to prevent it from happening again. According to the legend, as his eyelids hit the floor the first tea plants sprang up, and thereafter tea would provide a stimulant to help keep students of Chan awake during zazen.\n\nThe most popular account relates that Bodhidharma was admitted into the Shaolin temple after nine years in the cave and taught there for some time. However, other versions report that he \"passed away, seated upright\"; or that he disappeared, leaving behind the \"Yijin Jing\"; or that his legs atrophied after nine years of sitting, which is why Daruma dolls have no legs.\n\nIn one legend, Bodhidharma refused to resume teaching until his would-be student, Dazu Huike, who had kept vigil for weeks in the deep snow outside of the monastery, cut off his own left arm to demonstrate sincerity.\n\n\"Jǐngdé Records of the Transmission of the Lamp\" (Jǐngdé chuándēng lù 景德传灯录) of Dàoyuán 道原, presented to the emperor in 1004, records that Bodhidharma wished to return to India and called together his disciples:\n\nBodhidharma passed on the symbolic robe and bowl of dharma succession to Dazu Huike and, some texts claim, a copy of the \"Laṅkāvatāra Sūtra\". Bodhidharma then either returned to India or died.\n\nSome Chinese myths and legends describe Bodhidharma as being disturbed by the poor physical shape of the Shaolin monks, after which he instructed them in techniques to maintain their physical condition as well as teaching meditation. He is said to have taught a series of external exercises called the Eighteen Arhat Hands and an internal practice called the Sinew Metamorphosis Classic. In addition, after his departure from the temple, two manuscripts by Bodhidharma were said to be discovered inside the temple: the \"Yijin Jing\" and the \"Xisui Jing\". Copies and translations of the \"Yijin Jing\" survive to the modern day. The \"Xisui Jing\" has been lost.\n\nAccording to Southeast Asian folklore, Bodhidharma travelled from Jambudvipa by sea to Palembang, Indonesia. Passing through Sumatra, Java, Bali, and Malaysia, he eventually entered China through Nanyue. In his travels through the region, Bodhidharma is said to have transmitted his knowledge of the Mahayana doctrine and the martial arts. Malay legend holds that he introduced forms to silat.\n\nVajrayana tradition links Bodhidharma with the 11th-century south Indian monk Dampa Sangye who travelled extensively to Tibet and China spreading tantric teachings.\n\nThree years after Bodhidharma's death, Ambassador Sòngyún of northern Wei is said to have seen him walking while holding a shoe at the Pamir Heights. Sòngyún asked Bodhidharma where he was going, to which Bodhidharma replied \"I am going home\". When asked why he was holding his shoe, Bodhidharma answered \"You will know when you reach Shaolin monastery. Don't mention that you saw me or you will meet with disaster\". After arriving at the palace, Sòngyún told the emperor that he met Bodhidharma on the way. The emperor said Bodhidharma was already dead and buried and had Sòngyún arrested for lying. At Shaolin Monastery, the monks informed them that Bodhidharma was dead and had been buried in a hill behind the temple. The grave was exhumed and was found to contain a single shoe. The monks then said \"Master has gone back home\" and prostrated three times: \"For nine years he had remained and nobody knew him; Carrying a shoe in hand he went home quietly, without ceremony.\"\n\nBodhidharma is traditionally seen as introducing dhyana-practice in China.\n\nOne of the fundamental Chán texts attributed to Bodhidharma is a four-line stanza whose first two verses echo the \"Laṅkāvatāra Sūtra\"s disdain for words and whose second two verses stress the importance of the insight into reality achieved through \"self-realization\":\n\nThe stanza, in fact, is not Bodhidharma's, but rather dates to the year 1108.\n\nTanlin, in the preface to \"Two Entrances and Four Acts\", and Daoxuan, in the \"Further Biographies of Eminent Monks\", mention a practice of Bodhidharma's termed \"wall-gazing\" (壁觀 \"bìguān\"). Both Tanlin and Daoxuan associate this \"wall-gazing\" with \"quieting [the] mind\" ().\n\nIn the \"Two Entrances and Four Acts\", traditionally attributed to Bodhidharma, the term \"wall-gazing\" is given as follows: Daoxuan states, \"The merits of Mahāyāna wall-gazing are the highest\".\n\nThese are the first mentions in the historical record of what may be a type of meditation being ascribed to Bodhidharma.\n\nExactly what sort of practice Bodhidharma's \"wall-gazing\" was remains uncertain. Nearly all accounts have treated it either as an undefined variety of meditation, as Daoxuan and Dumoulin, or as a variety of seated meditation akin to the zazen () that later became a defining characteristic of Chan. The latter interpretation is particularly common among those working from a Chan standpoint.\n\nThere have also, however, been interpretations of \"wall-gazing\" as a non-meditative phenomenon.\n\nThere are early texts which explicitly associate Bodhidharma with the \"Laṅkāvatāra Sūtra\". Daoxuan, for example, in a late recension of his biography of Bodhidharma's successor Huike, has the sūtra as a basic and important element of the teachings passed down by Bodhidharma:\nAnother early text, the \"Record of the Masters and Disciples of the Laṅkāvatāra Sūtra\" () of Jìngjué (淨覺; 683–750), also mentions Bodhidharma in relation to this text. Jingjue's account also makes explicit mention of \"sitting meditation\" or zazen:\nIn other early texts, the school that would later become known as Chan Buddhism is sometimes referred to as the \"Laṅkāvatāra school\" (楞伽宗 \"Léngqié zōng\").\n\nThe \"Laṅkāvatāra Sūtra\", one of the Mahayana sutras, is a highly \"difficult and obscure\" text whose basic thrust is to emphasize \"the inner enlightenment that does away with all duality and is raised above all distinctions\". It is among the first and most important texts for East Asian Yogācāra.\n\nOne of the recurrent emphases in the \"Laṅkāvatāra Sūtra\" is a lack of reliance on words to effectively express reality:\n\nIn contrast to the ineffectiveness of words, the sūtra instead stresses the importance of the \"self-realization\" that is \"attained by noble wisdom\" and occurs \"when one has an insight into reality as it is\": \"The truth is the state of self-realization and is beyond categories of discrimination\". The sūtra goes on to outline the ultimate effects of an experience of self-realization:\n\nThe idea of a patriarchal lineage in Ch'an dates back to the epitaph for Fărú (法如 638–689), a disciple of the 5th patriarch Hóngrĕn (弘忍 601–674). In the \"Long Scroll of the Treatise on the Two Entrances and Four Practices\" and the \"Continued Biographies of Eminent Monks\", Daoyu and Dazu Huike are the only explicitly identified disciples of Bodhidharma. The epitaph gives a line of descent identifying Bodhidharma as the first patriarch.\n\nIn the 6th century biographies of famous monks were collected. From this genre the typical Chan lineage was developed:\nD. T. Suzuki contends that Chan's growth in popularity during the 7th and 8th centuries attracted criticism that it had \"no authorized records of its direct transmission from the founder of Buddhism\" and that Chan historians made Bodhidharma the 28th patriarch of Buddhism in response to such attacks.\n\nThe earliest lineages described the lineage from Bodhidharma into the 5th to 7th generation of patriarchs. Various records of different authors are known, which give a variation of transmission lines:\nEventually these descriptions of the lineage evolved into a continuous lineage from Śākyamuni Buddha to Bodhidharma. The idea of a line of descent from Śākyamuni Buddha is the basis for the distinctive lineage tradition of Chan Buddhism.\n\nAccording to the \"Song of Enlightenment\" (證道歌 \"Zhèngdào gē\") by Yǒngjiā Xuánjué (665-713), one of the chief disciples of Huìnéng, was Bodhidharma, the 28th Patriarch of Buddhism in a line of descent from Gautama Buddha via his disciple Mahākāśyapa:\n<poem>Mahakashyapa was the first, leading the line of transmission;\nTwenty-eight Fathers followed him in the West;\nThe Lamp was then brought over the sea to this country;\nAnd Bodhidharma became the First Father here\nHis mantle, as we all know, passed over six Fathers,\nAnd by them many minds came to see the Light.</poem>\nThe \"Transmission of the Light\" gives 28 patriarchs in this transmission:\n\nBodhidharma has been the subject of critical scientific research, which has shed new light on the traditional stories about Bodhidharma.\n\nAccording to John McRae, Bodhidharma has been the subject of a hagiographic process which served the needs of Chan Buddhism. According to him it is not possible to write an accurate biography of Bodhidharma:\nMcRae's standpoint accords with Yanagida's standpoint: \"Yanagida ascribes great historical value to the witness of the disciple T'an-lin, but at the same time acknowledges the presence of \"many puzzles in the biography of Bodhidharma\". Given the present state of the sources, he considers it impossible to compile a reliable account of Bodhidharma's life.\n\nSeveral scholars have suggested that the composed image of Bodhidharma depended on the combination of supposed historical information on various historical figures over several centuries. Bodhidharma as a historical person may even never have actually existed.\n\nDumoulin comments on the three principal sources. The Persian heritage is doubtful, according to Dumoulin: \"In the description of the Lo-yang temple, bodhidharma is called a Persian. Given the ambiguity of geographical references in writings of this period, such a statement should not be taken too seriously.\" Dumoulin considers Tan-lin's account of Bodhidharma being \"the third son of a great Brahman king\" to be a later addition, and finds the exact meaning of \"South Indian Brahman stock\" unclear: \"And when Tao-hsuan speaks of origins from South Indian Brahman stock, it is not clear whether he is referring to roots in nobility or to India in general as the land of the Brahmans.\"\n\nThese Chinese sources lend themselves to make inferences about Bodhidharma's origins. \"The third son of a Brahman king\" has been speculated to mean \"the third son of a Pallavine king\". Based on a specific pronunciation of the Chinese characters 香至 as Kang-zhi, \"meaning fragrance extreme\", Tsutomu Kambe identifies 香至 to be Kanchipuram, an old capital town in the state Tamil Nadu, India. According to Tsutomu Kambe, \"Kanchi means 'a radiant jewel' or 'a luxury belt with jewels', and puram means a town or a state in the sense of earlier times. Thus, it is understood that the '香至-Kingdom' corresponds to the old capital 'Kanchipuram'.\"\n\nThe Pakistani scholar Ahmad Hasan Dani speculated that according to popular accounts in Pakistan's northwest, Bodhidharma may be from the region around the Peshawar valley, or possibly around modern Afghanistan's eastern border with Pakistan.\n\nIn the context of the Indian caste system the mention of \"Brahman king\" acquires a nuance. Broughton notes that \"king\" implies that Bodhidharma was of a member of the thondaiman caste, an shatriya caste of warriors and rulers. Brahman is, in western contexts, easily understood as Brahmana or Brahmin, which means \"priest\".\n\nAccording to tradition Bodhidharma was given this name by his teacher known variously as Panyatara, Prajnatara, or Prajñādhara. His name prior to monkhood is said to be Jayavarman.\n\nBodhidharma is associated with several other names, and is also known by the name Bodhitara. Faure notes that:\nTibetan sources give his name as \"Bodhidharmottāra\" or \"Dharmottara\", that is, \"Highest teaching (dharma) of enlightenment\".\n\nBuswell dates Bodhidharma abode in China approximately at the early 5th century. Broughton dates Bodhidharma's presence in Luoyang to between 516 and 526, when the temple referred to—Yǒngníngsì (永寧寺), was at the height of its glory. Starting in 526, Yǒngníngsì suffered damage from a series of events, ultimately leading to its destruction in 534.\n\nTraditionally Bodhidharma is credited as founder of the martial arts at the Shaolin Temple. However, martial arts historians have shown this legend stems from a 17th-century qigong manual known as the \"Yijin Jing\".\n\nThe authenticity of the \"Yi Jin Jing\" has been discredited by some historians including Tang Hao, Xu Zhen and Matsuda Ryuchi. This argument is summarized by modern historian Lin Boyuan in his \"Zhongguo wushu shi\":\nThe oldest available copy was published in 1827. The composition of the text itself has been dated to 1624. Even then, the association of Bodhidharma with martial arts only became widespread as a result of the 1904–1907 serialization of the novel \"The Travels of Lao Ts'an\" in \"Illustrated Fiction Magazine\":\n\n\n"}
{"id": "4286", "url": "https://en.wikipedia.org/wiki?curid=4286", "title": "Biconditional introduction", "text": "Biconditional introduction\n\nIn propositional logic, biconditional introduction is a valid rule of inference. It allows for one to infer a biconditional from two conditional statements. The rule makes it possible to introduce a biconditional statement into a logical proof. If formula_1 is true, and if formula_2 is true, then one may infer that formula_3 is true. For example, from the statements \"if I'm breathing, then I'm alive\" and \"if I'm alive, then I'm breathing\", it can be inferred that \"I'm breathing if and only if I'm alive\". Biconditional introduction is the converse of biconditional elimination. The rule can be stated formally as:\n\nwhere the rule is that wherever instances of \"formula_1\" and \"formula_2\" appear on lines of a proof, \"formula_3\" can validly be placed on a subsequent line.\n\nThe \"biconditional introduction\" rule may be written in sequent notation:\n\nwhere formula_9 is a metalogical symbol meaning that formula_3 is a syntactic consequence when formula_1 and formula_2 are both in a proof;\n\nor as the statement of a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_14, and formula_15 are propositions expressed in some formal system.\n"}
{"id": "4287", "url": "https://en.wikipedia.org/wiki?curid=4287", "title": "Biconditional elimination", "text": "Biconditional elimination\n\nBiconditional elimination is the name of two valid rules of inference of propositional logic. It allows for one to infer a conditional from a biconditional. If formula_1 is true, then one may infer that formula_2 is true, and also that formula_3 is true. For example, if it's true that I'm breathing if and only if I'm alive, then it's true that if I'm breathing, I'm alive; likewise, it's true that if I'm alive, I'm breathing. The rules can be stated formally as:\n\nand\n\nwhere the rule is that wherever an instance of \"formula_1\" appears on a line of a proof, either \"formula_2\" or \"formula_3\" can be placed on a subsequent line;\n\nThe \"biconditional elimination\" rule may be written in sequent notation:\nand\n\nwhere formula_11 is a metalogical symbol meaning that formula_2, in the first case, and formula_3 in the other are syntactic consequences of formula_1 in some logical system;\n\nor as the statement of a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_17, and formula_18 are propositions expressed in some formal system.\n\n"}
{"id": "4292", "url": "https://en.wikipedia.org/wiki?curid=4292", "title": "Base pair", "text": "Base pair\n\nA base pair (bp) is a unit consisting of two nucleobases bound to each other by hydrogen bonds. They form the building blocks of the DNA double helix, and contribute to the folded structure of both DNA and RNA. Dictated by specific hydrogen bonding patterns, Watson-Crick base pairs (guanine-cytosine and adenine-thymine) allow the DNA helix to maintain a regular helical structure that is subtly dependent on its nucleotide sequence. The complementary nature of this based-paired structure provides a backup copy of all genetic information encoded within double-stranded DNA. The regular structure and data redundancy provided by the DNA double helix make DNA well suited to the storage of genetic information, while base-pairing between DNA and incoming nucleotides provides the mechanism through which DNA polymerase replicates DNA, and RNA polymerase transcribes DNA into RNA. Many DNA-binding proteins can recognize specific base pairing patterns that identify particular regulatory regions of genes.\n\nIntramolecular base pairs can occur within single-stranded nucleic acids. This is particularly important in RNA molecules (e.g., transfer RNA), where Watson-Crick base pairs (guanine-cytosine and adenine-uracil) permit the formation of short double-stranded helices, and a wide variety of non-Watson-Crick interactions (e.g., G-U or A-A) allow RNAs to fold into a vast range of specific three-dimensional structures. In addition, base-pairing between transfer RNA (tRNA) and messenger RNA (mRNA) forms the basis for the molecular recognition events that result in the nucleotide sequence of mRNA becoming translated into the amino acid sequence of proteins via the genetic code.\n\nThe size of an individual gene or an organism's entire genome is often measured in base pairs because DNA is usually double-stranded. Hence, the number of total base pairs is equal to the number of nucleotides in one of the strands (with the exception of non-coding single-stranded regions of telomeres). The haploid human genome (23 chromosomes) is estimated to be about 3.2 billion bases long and to contain 20,000–25,000 distinct protein-coding genes. A kilobase (kb) is a unit of measurement in molecular biology equal to 1000 base pairs of DNA or RNA. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 10, and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).\n\nHydrogen bonding is the chemical interaction that underlies the base-pairing rules described above. Appropriate geometrical correspondence of hydrogen bond donors and acceptors allows only the \"right\" pairs to form stably. DNA with high GC-content is more stable than DNA with low GC-content, but, contrary to popular belief, the hydrogen bonds do not stabilize the DNA significantly, and stabilization is mainly due to stacking interactions.\n\nThe larger nucleobases, adenine and guanine, are members of a class of double-ringed chemical structures called purines; the smaller nucleobases, cytosine and thymine (and uracil), are members of a class of single-ringed chemical structures called pyrimidines. Purines are complementary only with pyrimidines: pyrimidine-pyrimidine pairings are energetically unfavorable because the molecules are too far apart for hydrogen bonding to be established; purine-purine pairings are energetically unfavorable because the molecules are too close, leading to overlap repulsion. Purine-pyrimidine base pairing of AT or GC or UA (in RNA) results in proper duplex structure. The only other purine-pyrimidine pairings would be AC and GT and UG (in RNA); these pairings are mismatches because the patterns of hydrogen donors and acceptors do not correspond. The GU pairing, with two hydrogen bonds, does occur fairly often in RNA (see wobble base pair).\n\nPaired DNA and RNA molecules are comparatively stable at room temperature but the two nucleotide strands will separate above a melting point that is determined by the length of the molecules, the extent of mispairing (if any), and the GC content. Higher GC content results in higher melting temperatures; it is, therefore, unsurprising that the genomes of extremophile organisms such as \"Thermus thermophilus\" are particularly GC-rich. On the converse, regions of a genome that need to separate frequently — for example, the promoter regions for often-transcribed genes — are comparatively GC-poor (for example, see TATA box). GC content and melting temperature must also be taken into account when designing primers for PCR reactions.\n\nThe following DNA sequences illustrate pair double-stranded patterns. By convention, the top strand is written from the 5' end to the 3' end; thus, the bottom strand is written 3' to 5'.\n\nChemical analogs of nucleotides can take the place of proper nucleotides and establish non-canonical base-pairing, leading to errors (mostly point mutations) in DNA replication and DNA transcription. This is due to their isosteric chemistry. One common mutagenic base analog is 5-bromouracil, which resembles thymine but can base-pair to guanine in its enol form.\n\nOther chemicals, known as DNA intercalators, fit into the gap between adjacent bases on a single strand and induce frameshift mutations by \"masquerading\" as a base, causing the DNA replication machinery to skip or insert additional nucleotides at the intercalated site. Most intercalators are large polyaromatic compounds and are known or suspected carcinogens. Examples include ethidium bromide and acridine.\n\nAn unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature. DNA sequences have been described which use newly created nucleobases to form a third base pair, in addition to the two base pairs found in nature, A-T (adenine - thymine) and G-C (guanine - cytosine). A few research groups have been searching for a third base pair for DNA, including teams led by Steven A. Benner, Philippe Marliere, Floyd Romesberg and Ichiro Hirao. Some new base pairs have been reported.\n\nIn 1989 Steven Benner, then at the Swiss Federal Institute of Technology in Zurich, and his team led with modified forms of cytosine and guanine into DNA molecules \"in vitro\". The nucleotides, which encoded RNA and proteins, were successfully replicated \"in vitro\". Since then, Benner's team has been trying to engineer cells that can make foreign bases from scratch, obviating the need for a feedstock.\n\nIn 2002, Ichiro Hirao’s group in Japan developed an unnatural base pair between 2-amino-8-(2-thienyl)purine (s) and pyridine-2-one (y) that functions in transcription and translation, for the site-specific incorporation of non-standard amino acids into proteins. In 2006, they created 7-(2-thienyl)imidazo[4,5-b]pyridine (Ds) and pyrrole-2-carbaldehyde (Pa) as a third base pair for replication and transcription. Afterward, Ds and 4-[3-(6-aminohexanamido)-1-propynyl]-2-nitropyrrole (Px) was discovered as a high fidelity pair in PCR amplification. In 2013, they applied the Ds-Px pair to DNA aptamer generation by \"in vitro\" selection (SELEX) and demonstrated the genetic alphabet expansion significantly augment DNA aptamer affinities to target proteins.\n\nIn 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP). The two new artificial nucleotides or \"Unnatural Base Pair\" (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS–dNaM) complex or base pair in DNA. His team designed a variety of \"in vitro\" or \"test tube\" templates containing the unnatural base pair and they confirmed that it was efficiently replicated with high fidelity in virtually all sequence contexts using the modern standard \"in vitro\" techniques, namely PCR amplification of DNA and PCR-based applications. Their results show that for PCR and PCR-based applications, the d5SICS–dNaM unnatural base pair is functionally equivalent to a natural base pair, and when combined with the other two natural base pairs used by all organisms, A–T and G–C, they provide a fully functional and expanded six-letter \"genetic alphabet\".\n\nIn 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed, and inserted it into cells of the common bacterium \"E. coli\" that successfully replicated the unnatural base pairs through multiple generations. The transfection did not hamper the growth of the \"E. coli\" cells, and showed no sign of losing its unnatural base pairs to its natural DNA repair mechanisms. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. Romesberg said he and his colleagues created 300 variants to refine the design of nucleotides that would be stable enough and would be replicated as easily as the natural ones when the cells divide. This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into \"E. coli\" bacteria. Then, the natural bacterial replication pathways use them to accurately replicate a plasmid containing d5SICS–dNaM. Other researchers were surprised that the bacteria replicated these human-made DNA subunits.\n\nThe successful incorporation of a third base pair is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, from the existing 20 amino acids to a theoretically possible 172, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses. Experts said the synthetic DNA incorporating the unnatural base pair raises the possibility of life forms based on a different DNA code.\n\nThe following abbreviations are commonly used to describe the length of a D/RNA molecule:\n\nFor case of single-stranded DNA/RNA units of nucleotides are used, abbreviated nt (or knt, Mnt, Gnt), as they are not paired.\nFor distinction between units of computer storage and bases kbp, Mbp, Gbp, etc. may be used for base pairs.\n\nThe centimorgan is also often used to imply distance along a chromosome, but the number of base pairs it corresponds to varies widely. In the Human genome, the centimorgan is about 1 million base pairs.\n\n\n\n"}
{"id": "4293", "url": "https://en.wikipedia.org/wiki?curid=4293", "title": "Baltimore Ravens", "text": "Baltimore Ravens\n\nThe Baltimore Ravens are a professional American football team based in Baltimore, Maryland. The Ravens compete in the National Football League (NFL) as a member club of the American Football Conference (AFC) North division. The team plays its home games at M&T Bank Stadium and is headquartered in Owings Mills.\n\nThe Ravens were established in 1996, when Art Modell, who was then the owner of the Cleveland Browns, announced plans to relocate the franchise from Cleveland to Baltimore. As part of a settlement between the league and the city of Cleveland, Modell was required to leave the Browns' history and records in Cleveland for a replacement team and replacement personnel that would take control in 1999. In return, he was allowed to take his own personnel and team to Baltimore, where such personnel would then form an expansion team.\n\nThe Ravens have been one of the more successful franchises since their inception, having qualified for the NFL playoffs ten times since 2000, with two Super Bowl victories (Super Bowl XXXV and Super Bowl XLVII), two AFC Championship titles (2000 and 2012), 15 playoff victories, four AFC Championship game appearances (2000, 2008, 2011 and 2012), four AFC North division titles (2003, 2006, 2011 and 2012), and are currently the only team in the NFL to hold a perfect record in multiple Super Bowl and Thanksgiving Day appearances. The Ravens organization has been led by general manager Ozzie Newsome since 1996, and has had three head coaches: Ted Marchibroda, Brian Billick, and John Harbaugh. With a record-breaking defensive unit in their 2000 season, the team established a reputation for relying on strong defensive play, led by players like middle linebacker Ray Lewis, who, until his retirement, was considered the \"face of the franchise.\" The team is owned by Steve Bisciotti and valued at $1.5 billion, making the Ravens the 24th-most valuable sports franchise in the world.\nThe name \"Ravens\" was inspired by Edgar Allan Poe's poem \"The Raven\". Chosen in a fan contest that drew 33,288 voters, the allusion honors Poe, who spent the early part of his career in Baltimore and is buried there. As the \"Baltimore Sun\" reported at the time, fans also \"liked the tie-in with the other birds in town, the Orioles, and found it easy to visualize a tough, menacing black bird.\"\n\nAfter the controversial relocation of the Colts to Indianapolis, several attempts were made to bring an NFL team back to Baltimore. In 1993, ahead of the 1995 league expansion, the city was considered a favorite, behind only St. Louis, to be granted one of two new franchises. League officials and team owners feared litigation due to conflicts between rival bidding groups if St. Louis was awarded a franchise, and in October Charlotte, North Carolina was the first city chosen. Several weeks later, Baltimore's bid for a franchise—dubbed the Baltimore Bombers, in honor of the locally produced Martin B-26 Marauder bomber—had three ownership groups in place and a state financial package which included a proposed $200 million, rent-free stadium and permission to charge up to $80 million in personal seat license fees. Baltimore, however, was unexpectedly passed over in favor of Jacksonville, Florida, despite Jacksonville's minor TV market status and that the city had withdrawn from contention in the summer, only to return with then-Commissioner Paul Tagliabue's urging. Although league officials denied that any city had been favored, it was reported that Taglibue and his longtime friend Washington Redskins owner Jack Kent Cooke had lobbied against Baltimore due to its proximity to Washington, D.C., and that Taglibue had used the initial committee voting system to prevent the entire league ownership from voting on Baltimore's bid. This led to public outrage and the \"Baltimore Sun\" describing Taglibue as having an \"Anybody But Baltimore\" policy. Maryland governor William Donald Schaefer said afterward that Taglibue had led him on, praising Baltimore and the proposed owners while working behind-the-scenes to oppose Baltimore's bid.\n\nBy May 1994, Baltimore Orioles owner Peter Angelos had gathered a new group of investors, including author Tom Clancy, to bid on teams whose owners had expressed interest in relocating. Angelos found a potential partner in Georgia Frontiere, who was open to moving the Los Angeles Rams to Baltimore. Jack Kent Cooke opposed the move, intending to build the Redskins' new stadium in Laurel, Maryland, close enough to Baltimore to cool outside interest in bringing in a new franchise. This led to heated arguments between Cooke and Angelos, who accused Cooke of being a \"carpetbagger.\" The league eventually persuaded Rams team president John Shaw to relocate to St. Louis instead, leading to a league-wide rumor that Tagliabue was again steering interest away from Baltimore, a claim which Tagliabue denied. In response to anger in Baltimore, including Governor Schaefer's threat to announce over the loudspeakers Tagliabue's exact location in Camden Yards any time he attended a Baltimore Orioles game, Tagliabue remarked of Baltimore's financial package: \"Maybe (Baltimore) can open another museum with that money.\" Following this, Angelos made an unsuccessful $200 million bid to bring the Tampa Bay Buccaneers to Baltimore.\n\nHaving failed to obtain a franchise via the expansion, the city, despite having \"misgivings,\" turned to the possibility of obtaining the Cleveland Browns, whose owner Art Modell was financially struggling and at odds with the city of Cleveland over needed improvements to the team's stadium.\n\nEnticed by Baltimore's available funds for a first-class stadium and a promised yearly operating subsidy of $25 million dollars, Modell announced on November 6, 1995 his intention to relocate the team from Cleveland to Baltimore the following year. The resulting controversy ended when representatives of Cleveland and the NFL reached a settlement on February 8, 1996. Tagliabue promised the city of Cleveland that an NFL team would be located in Cleveland, either through relocation or expansion, \"no later than 1999\". Additionally, the agreement stipulated that the Browns' name, colors, uniform design and franchise records would remain in Cleveland. The franchise history includes Browns club records and connections with Pro Football Hall of Fame players. Modell's Baltimore team, while retaining all current player contracts, would, for purposes of team history, appear as an expansion team, a new franchise. Not all players, staff or front office would make the move to Baltimore, however.\n\nAfter relocation, Modell hired Ted Marchibroda as the head coach for his new team in Baltimore. Marchibroda was already well known because of his work as head coach of the Baltimore Colts during the 1970s and the Indianapolis Colts during the early 1990s. Ozzie Newsome, the Browns' tight end for many seasons, joined Modell in Baltimore as director of football operations. He was later promoted to vice-president/general manager.\n\nThe home stadium for the Ravens first two seasons was Baltimore's Memorial Stadium, home field of the Baltimore Colts and Baltimore Stallions years before. The Ravens moved to their own new stadium next to Camden Yards in 1998. Raven Stadium would subsequently wear the names PSI Net Stadium and then M&T Bank Stadium.\n\nIn the 1996 NFL Draft, the Ravens, with two picks in the first round, drafted offensive tackle Jonathan Ogden at No. 4 overall and linebacker Ray Lewis at No. 26 overall.\nThe 1996 Ravens won their opening game against the Oakland Raiders, but finished the season 4–12 despite receiver Michael Jackson leading the league with 14 touchdown catches.\n\nThe 1997 Ravens started 3–1. Peter Boulware, a rookie defender from Florida State, recorded 11.5 sacks and was named AFC Defensive Rookie of the Year. The team finished 6–9–1. On October 26, the team made its first trip to Landover, Maryland to play their new regional rivals, the Washington Redskins, for the first time in the regular season, at the new Jack Kent Cooke Stadium (replacing the still-standing RFK Stadium in Washington, DC). The Ravens won the game 20–17.\n\nQuarterback Vinny Testaverde left for the New York Jets before the 1998 season, and was replaced by former Indianapolis Colt Jim Harbaugh, and later Eric Zeier. Cornerback Rod Woodson joined the team after a successful stint with the Pittsburgh Steelers, and Priest Holmes started getting the first playing time of his career and ran for 1,000 yards.\n\nThe Ravens finished 1998 with a 6–10 record. On November 29, the Ravens welcomed the Colts back to Baltimore for the first time in 15 years. Amidst a shower of negative cheers towards the Colts, the Ravens, with Jim Harbaugh at quarterback, won 38–31.\n\nThree consecutive losing seasons under Marchibroda led to a change in the head coach. Brian Billick took over as head coach in 1999. Billick had been offensive coordinator for the record-setting Minnesota Vikings the season before. Quarterback Tony Banks came to Baltimore from the St. Louis Rams and had the best season of his career with 17 touchdown passes and an 81.2 pass rating. He was joined by receiver Qadry Ismail, who posted a 1,000-yard season. The Ravens initially struggled with a record of 4–7 but managed to finish with an 8–8 record.\n\nDue to continual financial hardships for the organization, the NFL took an unusual move and directed Modell to initiate the sale of his franchise. On March 27, 2000, NFL owners approved the sale of 49% of the Ravens to Steve Bisciotti. In the deal, Bisciotti had an option to purchase the remaining 51% for $325 million in 2004 from Art Modell. On April 9, 2004 the NFL approved Steve Bisciotti's purchase of the majority stake in the club.\n\nBanks shared playing time in the 2000 regular season with Trent Dilfer. Both players put up decent numbers (and a 1,364-yard rushing season by rookie Jamal Lewis helped too) but the defense became the team's hallmark and bailed a struggling offense out in many instances through the season. Ray Lewis was named Defensive Player of the Year. Two of his defensive teammates, Sam Adams and Rod Woodson, made the Pro Bowl. Baltimore's season started strong with a 5–1 record. But the team struggled through mid-season, at one point going five games without scoring an offensive touchdown. The team regrouped and won each of their last seven games, finishing 12–4 and making the playoffs for the first time.\n\nDuring the 2000 season, the Ravens defense broke two notable NFL records. They held opposing teams to 165 total points, surpassing the 1985 Chicago Bears mark of 198 points for a 16-game season as well as surpassing the 1986 Chicago Bears mark of 187 points for a 16-game season, which at that time was the current NFL record.\n\nSince the divisional rival Tennessee Titans had a record of 13–3, the Ravens had to play in the wild card round. They dominated the Denver Broncos 21–3 in their first game. In the divisional playoff, they went on the road to Tennessee. With the score tied 10–10 in the fourth quarter, an Al Del Greco field goal attempt was blocked and returned for a touchdown by Anthony Mitchell, and a Ray Lewis interception return for a score put the game squarely in Baltimore's favor. The 24–10 win put the Ravens in the AFC Championship against the Oakland Raiders. The game was rarely in doubt. Shannon Sharpe's 96-yard touchdown catch early in the second quarter followed by an injury to Raiders quarterback Rich Gannon were crucial as the Ravens won easily, 16–3.\n\nBaltimore then went to Tampa for Super Bowl XXXV against the New York Giants. The game was also dominated by the Ravens. They recorded four sacks and forced five turnovers, one of which was a Kerry Collins interception returned for a touchdown by Duane Starks. The Giants' only score was a Ron Dixon kickoff return for another touchdown; however, the Ravens immediately countered with a return by Jermaine Lewis. The Ravens became champions with a 34–7 win, becoming only the third wild card team to win a Super Bowl championship.\n\nIn 2001, the Ravens attempted to defend their title with Elvis Grbac as their new starting quarterback, but a season-ending injury to Jamal Lewis on the first day of training camp and poor offensive performances stymied the team. After a 3–3 start, the Ravens defeated the Minnesota Vikings in the final week to clinch a wild card berth at 10–6. In the first round the Ravens showed flashes of their previous year with a 20–3 win over the Miami Dolphins, in which the team forced three turnovers and out-gained the Dolphins 347 yards to 151. In the divisional playoff the Ravens played the Pittsburgh Steelers. Three interceptions by Grbac ended the Ravens' season, as they lost 27–10.\n\nBaltimore ran into salary cap problems entering the 2002 season and was forced to part with a number of impact players. In the NFL Draft, the team selected Ed Reed with the 24th overall pick. Reed would go on to become one of the best safeties in NFL history, making nine Pro Bowls until leaving the Ravens for the Houston Texans in 2013. Despite low expectations, the Ravens stayed somewhat competitive in 2002 until a losing streak in December eliminated any chances of a post-season berth. Their final record that year was 7–9.\n\nIn 2003, the Ravens drafted their new quarterback, Kyle Boller, but he was injured midway through the season and was replaced by Anthony Wright. Jamal Lewis ran for 2,066 yards (including a record 295 yards in one game against the Cleveland Browns on September 14). With a 10–6 record, Baltimore won their first AFC North division title. Their first playoff game, at home against the Tennessee Titans, went back and forth, with the Ravens being held to only 54 yards total rushing. The Titans won 20–17 on a late field goal, and Baltimore's season ended early.\n\nRay Lewis was also named Defensive Player of the year for the second time in his career.\n\nIn April 2003, Art Modell sold 49% of the team to Steve Bisciotti, a local businessman who had made his fortune in the temporary staffing field. After the season, Art Modell sold his remaining 51% ownership to Bisciotti, ending over 40 years of tenure as an NFL franchise owner.\n\nThe Ravens did not make the playoffs in 2004 and finished the season with a record of 9–7 with Kyle Boller spending the season at QB. They did get good play from veteran corner Deion Sanders and third year safety Ed Reed, who won the NFL Defensive Player of the Year award. They were also the only team to defeat the 15–1 Pittsburgh Steelers in the regular season.\n\nIn the 2005 offseason the Ravens looked to augment their receiving corps (which was second-worst in the NFL in 2004) by signing Derrick Mason from the Titans and drafting star Oklahoma wide receiver Mark Clayton in the first round of the 2005 NFL Draft. However, the Ravens ended their season 6–10, but defeated the Green Bay Packers 48–3 on Monday Night Football and the Super Bowl champion Steelers.\n\nThe 2006 Baltimore Ravens season began with the team trying to improve on their 6–10 record of 2005. The Ravens, for the first time in franchise history, started 4–0, under the leadership of former Titans quarterback Steve McNair.\n\nThe Ravens lost two straight games mid-season on offensive troubles, prompting coach Billick to drop their offensive coordinator Jim Fassel in their week seven bye. After the bye, and with Billick calling the offense, Baltimore would record a five-game win streak before losing to the Cincinnati Bengals in week 13.\n\nStill ranked second overall to first-place San Diego Chargers, the Ravens continued on. They defeated the Kansas City Chiefs, and held the defending Super Bowl champion Pittsburgh Steelers to only one touchdown at Heinz Field, allowing the Ravens to clinch the AFC North.\n\nThe Ravens ended the regular season with a franchise-best 13–3 record. Baltimore had secured the AFC North title, the No. 2 AFC playoff seed, and clinched a 1st-round bye by season's end. The Ravens were slated to face the Indianapolis Colts in the second round of the playoffs, in the first meeting of the two teams in the playoffs. Many Baltimore and Indianapolis fans saw this historic meeting as a sort of \"Judgment Day\" with the new team of Baltimore facing the old team of Baltimore (the former Baltimore Colts having left Baltimore under questionable circumstances in 1984). Both Indianapolis and Baltimore were held to scoring only field goals as the two defenses slugged it out all over M&T Bank Stadium. McNair threw two costly interceptions, including one at the 1-yard line. The eventual Super Bowl champion Colts won 15–6, ending Baltimore's season.\n\nAfter a stellar 2006 season, the Ravens hoped to improve upon their 13–3 record but injuries and poor play plagued the team. The Ravens finished the 2007 season in the AFC North cellar with a disappointing 5–11 record. A humiliating 22–16 overtime loss to the previously winless Miami Dolphins on December 16 ultimately led to Billick's dismissal on New Year's Eve, one day after the end of the regular season. He was replaced by John Harbaugh, the special teams coach of the Philadelphia Eagles and the older brother of former Ravens quarterback Jim Harbaugh (1998).\n\nWith rookies at head coach (John Harbaugh) and quarterback (Joe Flacco), the Ravens entered the 2008 campaign with lots of uncertainty. Baltimore smartly recovered in 2008, winning eleven games and achieving a wild card spot in the postseason. On the strength of four interceptions, one resulting in an Ed Reed touchdown, the Ravens began its postseason run by winning a rematch over Miami 27–9 at Dolphin Stadium on January 4, 2009 in a wild-card game. Six days later, they advanced to the AFC Championship Game by avenging a Week 5 loss to the Titans 13–10 at LP Field on a Matt Stover field goal with 53 seconds left in regulation time. The Ravens fell one victory short of Super Bowl XLIII by losing to the Steelers 23–14 at Heinz Field on January 18, 2009.\n\nIn 2009, the Ravens won their first three matches, then lost the next three, including a close match in Minnesota. The rest of the season was an uneven string of wins and losses, which included a home victory over Pittsburgh in overtime followed by a Monday Night loss in Green Bay. That game was notable for the huge number of penalties committed, costing a total of 310 yards, and almost tying with the record set by Tampa Bay and Seattle in 1976. Afterwards, the Ravens easily crushed the Lions and Bears, giving up less than ten points in both games. The next match was against the Steelers, where Baltimore lost a close one before beating the Raiders to end the season. With a record of 9–7, the team finished second in the division and gained another wild card. Moving into the playoffs, they overwhelmed the Patriots; nevertheless they did not reach the AFC Championship because they were routed 20–3 by the Colts in the divisional round a week later.\n\nBaltimore managed to beat the Jets 10–9 on the 2010 opener, but then lost a poorly-played game against Cincinnati the following week. The Ravens rebounded against the other two division teams, beating Cleveland 24–17 in Week 3 and then Pittsburgh 17–14 in Week 4. The Ravens scored a fine win (31–17) at home against Denver in Week 5. After an overtime loss to New England, they narrowly avoided losing at home to the winless Bills. Next, the Ravens hosted Miami and won 26–10, breaking that team's 4–0 road streak. On Thursday Night, the team headed to Atlanta and lost 26–21 in a game that had some criticizing the officiating. The Ravens finished the season 12–4, second in the division due to a tiebreaker with Pittsburgh, and earning a wild card spot. Baltimore headed to Kansas City and crushed the unprepared Chiefs 34–7, but once again were knocked from the playoffs by Pittsburgh in a hard-fought battle.\n\nThe Ravens hosted their arch-enemy in Week 1 of the 2011 season. On a hot, humid day in M&T Bank Stadium, crowd noise and multiple Steelers mistakes allowed Baltimore to crush them with three touchdowns 35–7. The frustrated Pittsburgh players also committed several costly penalties. Thus, the Ravens had gained their first ever victory over the Steelers with Ben Roethlisberger playing and avenged themselves of repeated regular and postseason losses in the series.\n\nBut in Week 2, the Ravens collapsed in Tennessee and lost 26–13. They rebounded by routing the Rams in Week 3 and then overpowering the Jets 34–17 in Week 4.\nWeek 5, the Ravens had a bye week, following a game against the Texans. But in Week 7, Baltimore had a stunning MNF upset loss in Jacksonville as they were held to one touchdown in a 12–7 loss. Their final scoring drive failed as Joe Flacco threw an interception in the closing seconds of the game.\n\nAfter beating the Cincinnati Bengals in Week 17 of the regular season, the Ravens advanced to the playoffs as the Number 2 seed in the AFC with a record of 12-4. They gained the distinction of AFC North Champions over Pittsburgh (12-4) due to a tie breaker.\n\nRavens' Lee Evans was stripped of a 14-yard touchdown pass by the Patriots Sterling Moore with 22 seconds left and Ravens kicker Billy Cundiff pushed a 32-yard field goal attempt wide left on fourth down as the Patriots held on to beat the Ravens 23-20 during the AFC championship game and advance to Super Bowl XLVI.\n\nThe Ravens' attempt to convert Joe Flacco into a pocket passer remained a work in progress as the 2012 season began. Terrell Suggs suffered a tendon injury during an off-season basketball game and was unable to play for at least several weeks. In the opener on September 10, Baltimore routed Cincinnati 44-13. After this easy win, the team headed to Philadelphia. The Eagles had struggled during their Week 1 match in Cleveland and were not expected to win, but a bizarre game ensued thanks to the NFL facing another lockout mess, this one involving the league's referees, who were replaced by ex-college officials. The replacement officials were widely criticized throughout the league. This game featured multiple questionable calls that went against the Ravens, perhaps costing them the game 24-23.\n\nReturning home for a primetime rematch of the AFC Championship, another bizarre game ensued. New England picked apart the Baltimore defense (which was considerably weakened without Terrell Suggs and some other players lost over the off-season) for the first half. Trouble began early in the game when a streaker ran out onto the field and had to be tackled by security, and accelerated when, at 2:18 in the 4th quarter, the referees made a holding call on RG Marshal Yanda. Enraged fans repeatedly chanted an obscenity at this penalty. The Ravens finally drove downfield and on the last play of the game, Justin Tucker kicked a 27-yard field goal to win the game 31-30, capping off a second intense and controversially-officiated game in a row for the Ravens.\n\nThe Ravens would win the AFC North with a 10-6 record, but finished 4th in the AFC playoff seeding, and thus had to play a wild-card game. After defeating the Indianapolis Colts 24-9 at home (the final home game of Ray Lewis), the Ravens traveled to Denver to play against the top seeded Broncos. In a very back-and-forth contest, the Ravens pulled out a 38-35 victory in double overtime. They then won their 2nd AFC championship by coming back from a 13-7 halftime deficit to defeat the New England Patriots once again, 28-13.\n\nThe Ravens played the Super Bowl XLVII on February 3, 2013, against the San Francisco 49ers. Baltimore built a 28–6 lead early in the third quarter before a partial power outage in the Superdome suspended play for 34 minutes (earning the game the added nickname of the Blackout Bowl). After play resumed, San Francisco scored 17 unanswered third-quarter points to cut the Ravens' lead, 28–23, and continued to chip away in the fourth quarter. With the Ravens leading late in the game, 34–29, the 49ers advanced to the Baltimore 7-yard line just before the two-minute warning but turned the ball over on downs. The Ravens then took an intentional safety in the waning moments of the game to preserve the victory. Baltimore quarterback Joe Flacco, who completed 22 of 33 passes for 287 yards and three touchdowns, was named Super Bowl MVP.\n\nComing off as the defending Super Bowl champions, this was the first year in franchise history for the team without Ray Lewis. The Ravens started out 3-2, and started the 2-0 Houston Texans 14-loss streak by shutting them 30-9 in Week 3. However, the Ravens lost their next 3 games, losing to the Green Bay Packers and Pittsburgh Steelers in last-minute field goals and were shut out in an attempt to tie the game against the Cleveland Browns 18-24. \n\nAfter winning and losing their next game, the Baltimore Ravens came out 4-6, but managed winning their next four games in dominating the Jets 19-3 in Baltimore, a Steelers win 20-22 during Thanksgiving, a booming ending in Baltimore against the Vikings 29-26, and a 18-16 win at Detroit, including Justin Tucker's 61-yard game-winning field goal. The Ravens were 8-6, with the 6th seed, but after losing their next two games, and the San Diego Chargers winning their next two to clinch the 6th seed, the Ravens finished 8-8 and missed the playoffs for the first time since 2007.\n\nOn January 27, 2014, the Ravens hired former Houston Texans head coach Gary Kubiak to be their new offensive coordinator after Jim Caldwell accepted the new available head coaching job with the Detroit Lions. On February 15, 2014, star running back Ray Rice and his fiancée Janay Palmer were arrested and charged with assault after a physical altercation at Revel Casino in Atlantic City, New Jersey. Celebrity news website TMZ posted a video of Rice dragging Palmer's body out of an elevator after apparently knocking her out. For the incident, Rice was initially suspended for the first two games of the 2014 NFL season on July 25, 2014, which led to widespread criticism of the NFL.\n\nIn Week 1, on September 7, the Baltimore Ravens lost to the Cincinnati Bengals, 23-16. The next day, on September 8, 2014, TMZ released additional footage from an elevator camera showing Rice punching Palmer. The Baltimore Ravens terminated Rice's contract as a result, and was later indefinitely suspended by the NFL. Although starting out 0-1 for two straight seasons and having received unwanted media attention for the Ray Rice incident, on September 11, 2014, the Ravens rallied back and beat the Pittsburgh Steelers 26-6, to improve to 1-1. In Week 12, the Ravens traveled down for an interconference battle with the New Orleans Saints, which the Ravens won 34-27, reaching a 4-0 sweep of the NFC south. In Week 16, the Ravens traveled to Houston to take on the Texans. In one of Joe Flacco's worst performances, the offense sputtered against the Houston defense and Flacco threw three interceptions, falling to the Texans 25-13. With their playoff chances and season hanging in the balance, the Ravens took on the Browns in Week 17 at home. After three quarters had gone by and down 10-3, Joe Flacco led the Ravens on a comeback scoring 17 unanswered points, winning 20-10. With the win, and the Kansas City Chiefs defeating the San Diego Chargers, the Ravens clinched their sixth playoff berth in seven seasons, and the first since winning Super Bowl XLVII.\n\nIn the wildcard playoff game, the Ravens won 30-17 against their divisional rivals, the Pittsburgh Steelers, at Heinz Field. In the next game in the Divisional round, the Ravens faced the New England Patriots. Despite a strong offensive effort and having a 14-point lead twice in the game, the Ravens were defeated by the Patriots 35-31, ending their season.\n\nThe 2015 season marked 20 seasons of the franchise's existence, competing in the NFL which the franchise have recognized with a special badge being worn on their uniforms during the 2015 NFL season.\nAfter coming up just short against the Patriots in the playoffs, the Ravens were picked by some to win the AFC and even the Super Bowl. However, they lost key players such as Joe Flacco, Justin Forsett, Terrell Suggs, Steve Smith Sr., and Eugene Monroe to season-ending injuries. Injuries and their inability to win close games early in the season led to the first losing season in the John Harbaugh-Flacco era.\n\nThe 2016 Ravens improved on their 5–11 record from 2015, finishing 8–8, but failed to qualify the playoffs for the second straight year. They were eliminated from playoff contention after their Week 16 loss to their division rivals, the Steelers. This was the first time the Ravens missed the playoffs in consecutive seasons since 2004–2005, as well as the first in the Harbaugh/Flacco era.\n\nBy far the team's biggest rival is the Pittsburgh Steelers. Pittsburgh and Baltimore are separated by a less-than-5-hour drive along Interstate 70. Both teams are known for their hard-hitting physical style of play. They play twice a year in the AFC North, and have met four times in the playoffs. Games between these two teams usually come down to the wire as most within the last 5 years have come down to 3 points or less.\nThe rivalry is considered one of the most significant and intense in the NFL today.\n\nAlthough the Steelers rivalry is based on mutual respect and antagonism for each other, the Ravens' rivalry with the Indianapolis Colts is fueled by the fans' animosity towards the organization, not contention between the players. This is due to the fact that the then-Colts owner, Robert Irsay, under the threat of eminent domain from the city of Baltimore, was forced to sneak the Colts out of Baltimore in the middle of the night to take them to Indianapolis. During Ravens home games the scoreboard lists the away team simply as \"Away\" or \"Indy\" rather than the team name that is traditionally used for the visiting opponent. The PA announcer will also refer to the Colts as the Indianapolis Professional Football Team; although on January 6, 2013 the scoreboard at the playoff game between the Baltimore Ravens and Indianapolis Colts at M&T Bank Stadium listed the away team as \"Colts\". The Indianapolis Colts hold an all-time 9–4 advantage over the Baltimore Ravens, including a 2–1 advantage in the playoffs.\n\nThe Ravens also have divisional rivalries with the Cleveland Browns and Cincinnati Bengals.\n\nThe reactivated Cleveland Browns and their fans maintain a hatred of Baltimore's team due to its move from Cleveland. The rivalry with the Browns has been very one-sided; Baltimore holds an advantage of 27-9 against Cleveland.\n\nThe rivalry with Cincinnati has been closer, with both teams tied at 21-21 following the 2016 season.\n\nThe Ravens first met the New England Patriots in 1996, but the rivalry truly started in 2007 when the Ravens suffered a bitter 27–24 loss in the Patriots quest for perfection. The rivalry began to escalate in 2009 when the Patriots beat the Ravens 27–21 in a game that involved a confrontation between Patriots quarterback Tom Brady and Ravens linebacker Terrell Suggs. Both players would go on to take verbal shots at each other through the media after the game. The Ravens faced the Patriots in a 2009 AFC wild card playoff game and won 33–14; the Ravens ran the ball for more than 250 yards.\n\nThe Ravens faced the Patriots in Week 6 of the 2010 season; the Ravens ended up losing 23–20 in overtime; the game caused controversy due to a hit to the helmet of tight end Todd Heap by Patriots safety Brandon Meriweather.\n\nThe Ravens played the Patriots for the third consecutive season, in the 2011 AFC championship game in which the Ravens lost 23–20. The rivalry reached a new level of friction with this, the second career playoff game between the two clubs. The Ravens clawed to a 20–16 lead in the fourth quarter but Patriots quarterback Tom Brady dove into the end zone to make the score 23–20 with around 11 minutes remaining; this proved to be the winning touchdown. On the Ravens last possession of the game, quarterback Joe Flacco threw a pass to wide receiver Lee Evans in the corner of the end zone which looked to be the game-winning touchdown, before a last second strip by Sterling Moore forced the ball from the hands of Evans, forcing the game to be decided on a last minute field goal by Ravens placekicker Billy Cundiff. With eleven seconds remaining on the clock, the kicker missed the 32-yard field goal attempt by a very wide margin, allowing the Patriots to kill the clock on their way to Super Bowl XLVI.\n\nThe Ravens' first regular-season win over the Patriots came on September 23, 2012. The game was emotional as receiver Torrey Smith was competing following the death of his brother in a motorcycle accident just the night before. Smith caught two touchdowns in a back and forth game; the Ravens erased a 13–0 deficit in the first half and led 14–13, but the Patriots scored at the end of the second quarter for a 20–14 lead. The lead changed twice in the third quarter and the Patriots led 30–21 in the fourth, but the Ravens scored on Smith's second touchdown catch. The Ravens were stopped on fourth down but the Patriots had to punt; in the final two minutes a pass interference penalty on Devin McCourty put the ball at the Patriots 7-yard line; new Ravens kicker Justin Tucker booted a 27-yard field goal on the final play; the ball sailed directly over the upright and was ruled good; the quality of officiating by replacement referees caused controversy as Bill Belichick angrily reached for one of the referees as they were leaving the field, leading to a $50,000 fine later that week.\n\nThe two teams met again on January 20, 2013 in the AFC Championship, where the Ravens won 28–13. The Patriots led at halftime, 13–7, but the Ravens' defense gave up no points in the second half. It was the first time ever that Tom Brady lost a game at home after leading at halftime, and the first time a road team beat the Patriots in the AFC Championship.\n\nOn December 22, 2013 the teams met again, this rematch of the AFC championship game was a mismatch from the outset. New England took a 17-0 lead early in the second quarter and never let up behind a defense that forced four turnovers and had four sacks. New England would go on to win the game 41-7.\n\nOn January 10, 2015, the two teams would meet in the Divisional Round of the playoffs. Unlike the previous meeting, the Ravens put up a strong offensive performance, leading by 14 points twice in the game. However, Tom Brady would bring the Patriots back by attacking the Ravens vulnerable secondary and taking a 35-31 lead late in the 4th quarter. Joe Flacco would drive to the Patriots side of the field with under two minutes to play in regulation. However, a key interception by Flacco due to a misplay on the ball by Torrey Smith essentially sealed the game in the Patriots favor to send them to the AFC Championship.\n\nThe team's first helmet logo, used from 1996 through 1998, featured raven wings outspread from a shield displaying a letter \"B\" framed by the word \"Ravens\" overhead and a cross bottony underneath. The US Fourth Circuit Court of Appeals affirmed a jury verdict that the logo infringed on a copyright retained by Frederick E. Bouchat, an amateur artist and security guard in Maryland, but that he was entitled to only three dollars in damages from the NFL.\n\nBouchat had submitted his design to the Maryland Stadium Authority by fax after learning that Baltimore was to acquire an NFL team. He was not credited for the design when the logo was announced. Bouchat sued the team, claiming to be the designer of the emblem; representatives of the team asserted that the image had been designed independently. The court ruled in favor of Bouchat, noting that team owner Modell had access to Bouchat's work. Bouchat's fax had gone to John Moag, the Maryland Stadium Authority chairman, whose office was located in the same building as Modell's. Bouchat ultimately was not awarded monetary compensation in the damages phase of the case.\n\nThe \"Baltimore Sun\" ran a poll showing three designs for new helmet logos. Fans participating in the poll expressed a preference for a raven's head in profile over other designs. Art Modell announced that he would honor this preference but still wanted a letter \"B\" to appear somewhere in the design. The new Ravens logo featured a raven's head in profile with the letter superimposed. The secondary logo is a shield that honors Baltimore's history of heraldry. Alternating Calvert and Crossland emblems (seen also in the flag of Maryland and the flag of Baltimore) are interlocked with stylized letters \"B\" and \"R\".\n\nThe design of the Ravens uniform has remained essentially unchanged since the team's inaugural season in 1996. Art Modell admitted to ESPN's Roy Firestone that the Ravens' colors, introduced in early 1996, were inspired by the Northwestern Wildcats 1995 dream season. Helmets are black with purple \"talon\" stripes rising from the facemask to the crown. Players normally wear purple jerseys at home and white jerseys on the road. In 1996 the team wore black pants with a single large white stripe for all games. At home games the combination of black pants with purple jersey made the Ravens the first NFL team to wear dark colors head to calf. A number of NFL teams have since donned the look, beginning with the all-black home uniform worn in three games by the 2001 New Orleans Saints.\n\nIn 1997 the Ravens opted for a more classic NFL look with white pants sporting stripes in purple and black, along with the jerseys sporting a different font for the uniform numbers. The white pants were worn with both home and road jerseys. The road uniform (white pants with white jerseys) was worn by the Ravens in Super Bowl XXXV, at the end of the 2000 NFL season.\n\nIn the 2002 season the Ravens began the practice of wearing white jerseys for the home opener and, occasionally, other early games in the season that have a 1:00 kickoff. Since John Harbaugh became the head coach in 2008, the Ravens have also worn their white jerseys at home for preseason games.\n\nIn November 2004 the team introduced an alternate uniform design featuring black jerseys and solid black pants with black socks. The all-black uniform was first worn for a home game against the Cleveland Browns, entitled \"Pitch Black\" night, that resulted in a Ravens win. The uniform has since been worn for select prime-time national game broadcasts and other games of significance.\n\nThe Ravens began wearing black pants again with the white jersey in 2008. On December 7, 2008, during a Sunday Night Football game against the Washington Redskins, the Ravens introduced a new combination of black jersey with white pants. It was believed to be due to the fact that John Harbaugh doesn't like the \"blackout\" look. However, on December 19, 2010, the Ravens wore their black jerseys and black pants in a 30–24 victory over the New Orleans Saints.\n\nOn December 5, 2010, the Ravens reverted to the black pants with the purple jerseys versus the Pittsburgh Steelers during NBC's \"Sunday Night Football\" telecast. The Ravens lost to the Steelers 13–10. They wore the same look again for their game against the Cleveland Browns on December 24, 2011, and they won, 20–14. They wore this combination a third time against the Houston Texans on January 15, 2012 in the AFC Divisional playoff. They won 20–13. They would again wear this combination on January 6, 2013, during the AFC Wild Card playoff and what turned out to be Ray Lewis' final home game, where they defeated the Indianapolis Colts 24-9.\n\nFrom their inaugural season until 2006, the Ravens wore white cleats with their uniforms; they switched to black cleats in 2007.\n\nOn December 20, 2015, the team unexpectedly debuted gold pants for the first time, wearing them with their regular purple jerseys against the Kansas City Chiefs. Although gold is an official accent color of the Ravens, the pants got an overwhelmingly negative response on social media by both Ravens fans and fans of other NFL teams, with some comparisons being made to the rival Pittsburgh Steelers' pants.\n\nThe team marching band is called Baltimore's Marching Ravens. They began as the Colts' marching band and have operated continuously from September 7, 1947 to the present. They helped campaign for football to return to Baltimore after the Colts moved. Because they stayed in Baltimore after the Colts left, the band is nicknamed \"the band that would not die\" and were the subject of an episode of ESPN's \"30 for 30\". The Washington Redskins are the only other NFL team that currently has a marching band.\n\nThe Ravens do not officially have retired numbers. However, the number 19 is not issued out of respect for Baltimore Colts quarterback Johnny Unitas, except for quarterback Scott Mitchell in his lone season in Baltimore in 1999. In addition, numbers 75, 52, and 20, in honor of Jonathan Ogden, Ray Lewis, and Ed Reed respectively, have not been issued since those players' retirements from football. The number 3 has been in very limited circulation (offseason only) in respect to former kicker Matt Stover.\n\nThe Ravens have a \"Ring of Honor\" which is on permanent display encircling the field of M&T Bank Stadium. The ring currently honors the following, including 8 former members of the Baltimore Colts. Bold Numbers are those whose numbers have not been issued or reissued after a player's time in Baltimore:\n\n<br>\nKey/Legend\n\n<br>\n\nThe Baltimore Ravens had their first draft in 1996, where they selected offensive lineman from UCLA and current NFL Hall of Famer, and 11-time Pro-Bowler Jonathan Ogden. Along with their pick in the next year's draft, this was the highest first-round draft pick that the Ravens have had. They also selected Ray Lewis with the 26th pick. In both 1996 and 2000, the Ravens had two first-round draft picks. However, in 2004 they had none. In their history, the Ravens have drafted 4 offensive linemen, 3 linebackers, 2 wide receivers, 2 cornerbacks, 2 quarterbacks, a running back, tight end, safety, and defensive tackle. The Ravens have 56 combined Pro-Bowl appearances from their first-round draft picks.\n\n+ = min. 500 attempts, # = min. 100 attempts, ∗ = minimum 15 attempts,\n\n∗ = minimum 15 attempts, # = min. 100 attempts, + = min. 500 attempts\n\n∗ = minimum 4 receptions, # = min. 20 receptions, + = min. 200 receptions\n\n\n\"All records as of February 9, 2017 per Pro-Football Reference.com\"\n\nThe Ravens' flagship radio stations are WIYY (98 Rock) and WBAL 1090 AM, with Gerry Sandusky (WBAL-TV Sports Anchor since 1988) as the play-by-play announcer and analysts Stan White (Baltimore Colts LB 1972–1979) and Qadry Ismail (Baltimore Ravens WR 1999–2001). \n\nThe team's flagship station is WBAL-TV, which broadcasts pre-season games and team programming throughout the season. The programming is syndicated to WJLA-TV in Washington, WGAL in the Harrisburg-Lebanon-York-Lancaster, PA market, and until 2017, was carried through the remainder of the team's region by CSN Mid-Atlantic. In January 2017, the Ravens announced that it had cut ties with CSN Mid-Atlantic, as the network was cutting back on its day-to-day coverage of other teams in the region in order to focus more extensively on the Washington Capitals and Wizards—whose games are broadcast by CSN Mid-Atlantic, and whose owner holds a stake in the network. The team announced that it would seek a new partner; until 2010, these rights were held by MASN.\n\nRavens regular season games are typically broadcast by WJZ-TV as part of CBS's rights to the AFC, but games may occasionally be broadcast on WBAL (\"Sunday Night Football\" and simulcasts of games on cable) or WBFF-TV.\n\n"}

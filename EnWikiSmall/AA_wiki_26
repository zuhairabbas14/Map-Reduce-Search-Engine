{"id": "4050", "url": "https://en.wikipedia.org/wiki?curid=4050", "title": "Battle of Ramillies", "text": "Battle of Ramillies\n\nThe Battle of Ramillies (), fought on 23 May 1706, was a battle of the War of the Spanish Succession. For the Grand Alliance – Austria, England, and the Dutch Republic – the battle had followed an indecisive campaign against the Bourbon armies of King Louis XIV of France in 1705. Although the Allies had captured Barcelona that year, they had been forced to abandon their campaign on the Moselle, had stalled in the Spanish Netherlands and suffered defeat in northern Italy. Yet despite his opponents' setbacks Louis XIV was desirous of peace – but he wanted it on reasonable terms. For this end and in order to maintain their momentum, the French and their allies took the offensive in 1706.\n\nThe campaign began well for Louis XIV's generals: in Italy Marshal Vendôme had defeated the Austrians at the Battle of Calcinato in April, while in Alsace Marshal Villars had forced the Margrave of Baden back across the Rhine. Encouraged by these early gains Louis XIV urged Marshal Villeroi to go over to the offensive in the Spanish Netherlands and, with victory, gain a 'fair' peace. Accordingly, the French Marshal set off from Leuven (\"Louvain\") at the head of 60,000 men and marched towards Tienen (\"Tirlemont\"), as if to threaten Zoutleeuw (\"Léau\"). Also determined to fight a major engagement, the Duke of Marlborough, commander-in-chief of Anglo-Dutch forces, assembled his army – some 62,000 men – near Maastricht, and marched past Zoutleeuw. With both sides seeking battle, they soon encountered each other on the dry ground between the Mehaigne and Petite Gheete rivers, close to the small village of Ramillies.\n\nIn less than four hours Marlborough's Dutch, English, and Danish forces overwhelmed Villeroi's and Max Emanuel's Franco-Spanish-Bavarian army. The Duke's subtle moves and changes in emphasis during the battle – something his opponents failed to realise until it was too late – caught the French in a tactical vice. With their foe broken and routed, the Allies were able to fully exploit their victory. Town after town fell, including Brussels, Bruges and Antwerp; by the end of the campaign Villeroi's army had been driven from most of the Spanish Netherlands. With Prince Eugene's subsequent success at the Battle of Turin in northern Italy, the Allies had imposed the greatest loss of territory and resources that Louis XIV would suffer during the war. Thus, the year 1706 proved, for the Allies, to be an \"annus mirabilis\".\n\nAfter their disastrous defeat at Blenheim in 1704, the next year brought the French some respite. The Duke of Marlborough had intended the 1705 campaign – an invasion of France through the Moselle valley – to complete the work of Blenheim and persuade King Louis XIV to make peace but the plan had been thwarted by friend and foe alike. The reluctance of his Dutch allies to see their frontiers denuded of troops for another gamble in Germany had denied Marlborough the initiative but of far greater importance was the Margrave of Baden’s pronouncement that he could not join the Duke in strength for the coming offensive. This was in part due to the sudden switching of troops from the Rhine to reinforce Prince Eugene in Italy and part due to the deterioration of Baden’s health brought on by the re-opening of a severe foot wound he had received at the storming of the Schellenberg the previous year. Marlborough had to cope with the death of Emperor Leopold I in May and the accession of Joseph I, which unavoidably complicated matters for the Grand Alliance.\n\nThe resilience of the French King and the efforts of his generals, also added to Marlborough’s problems. Marshal Villeroi, exerting considerable pressure on the Dutch commander, Count Overkirk, along the Meuse, took Huy on 10 June before pressing on towards Liège. With Marshal Villars sitting strong on the Moselle, the Allied commander – whose supplies had by now become very short – was forced to call off his campaign on 16 June. \"What a disgrace for Marlborough,\" exulted Villeroi, \"to have made false movements without any result!\" With Marlborough’s departure north, the French transferred troops from the Moselle valley to reinforce Villeroi in Flanders, while Villars marched off to the Rhine.\n\nThe Anglo-Dutch forces gained minor compensation for the failed Moselle campaign with the success at Elixheim and the crossing of the Lines of Brabant in the Spanish Netherlands (Huy was also retaken on 11 July) but a chance to bring the French to a decisive engagement eluded Marlborough. The year 1705 proved almost entirely barren for the Duke, whose military disappointments were only partly compensated by efforts on the diplomatic front where, at the courts of Düsseldorf, Frankfurt, Vienna, Berlin and Hanover, Marlborough sought to bolster support for the Grand Alliance and extract promises of prompt assistance for the following year's campaign.\n\nOn 11 January 1706, Marlborough finally reached London at the end of his diplomatic tour but he had already been planning his strategy for the coming season. The first option (although it is debatable to what extent the Duke was committed to such an enterprise) was a plan to transfer his forces from the Spanish Netherlands to northern Italy; once there, he intended linking up with Prince Eugene in order to defeat the French and safeguard Savoy from being overrun. Savoy would then serve as a gateway into France by way of the mountain passes or an invasion with naval support along the Mediterranean coast via Nice and Toulon, in connexion with redoubled Allied efforts in Spain. It seems that the Duke’s favoured scheme was to return to the Moselle valley (where Marshal Marsin had recently taken command of French forces) and once more attempt an advance into the heart of France. But these decisions soon became academic. Shortly after Marlborough landed in the Dutch Republic on 14 April, news arrived of big Allied setbacks in the wider war.\n\nDetermined to show the Grand Alliance that France was still resolute, Louis XIV prepared to launch a double surprise in Alsace and northern Italy. On the latter front Marshal Vendôme defeated the Imperial army at Calcinato on 19 April, pushing the Imperialists back in confusion (French forces were now in a position to prepare for the long-anticipated siege of Turin). In Alsace, Marshal Villars took Baden by surprise and captured Haguenau, driving him back across the Rhine in some disorder, thus creating a threat on Landau. With these reverses, the Dutch refused to contemplate Marlborough's ambitious march to Italy or any plan that denuded their borders of the Duke and their army. In the interest of coalition harmony, Marlborough prepared to campaign in the Low Countries.\n\nThe Duke left The Hague on 9 May. \"God knows I go with a heavy heart,\" he wrote six days later to his friend and political ally in England, Lord Godolphin, \"for I have no hope of doing anything considerable, unless the French do what I am very confident they will not … \" – in other words, court battle. On 17 May the Duke concentrated his Dutch and English troops at Tongeren, near Maastricht. The Hanoverians, Hessians and Danes, despite earlier undertakings, found, or invented, pressing reasons for withholding their support. Marlborough wrote an appeal to the Duke of Württemberg, the commander of the Danish contingent – \"I send you this express to request your Highness to bring forward by a double march your cavalry so as to join us at the earliest moment …\" Additionally, the King \"in\" Prussia, Frederick I, had kept his troops in quarters behind the Rhine while his personal disputes with Vienna and the States General at The Hague remained unresolved. Nevertheless, the Duke could think of no circumstances why the French would leave their strong positions and attack his army, even if Villeroi was first reinforced by substantial transfers from Marsin’s command. But in this he had miscalculated. Although Louis XIV wanted peace he wanted it on reasonable terms; for that, he needed victory in the field and to convince the Allies that his resources were by no means exhausted.\n\nFollowing the successes in Italy and along the Rhine, Louis XIV was now hopeful of similar results in Flanders. Far from standing on the defensive therefore – and unbeknown to Marlborough – Louis XIV was persistently goading his marshal into action. \"[Villeroi] began to imagine,\" wrote St Simon, \"that the King doubted his courage, and resolved to stake all at once in an effort to vindicate himself.\" Accordingly, on 18 May, Villeroi set off from Leuven at the head of 70 battalions, 132 squadrons and 62 cannon – comprising an overall force of some 60,000 troops – and crossed the river Dyle to seek battle with the enemy. Spurred on by his growing confidence in his ability to out-general his opponent, and by Versailles’ determination to avenge Blenheim, Villeroi and his generals anticipated success.\n\nNeither opponent expected the clash at the exact moment or place where it occurred. The French moved first to Tienen, (as if to threaten Zoutleeuw, abandoned by the French in October 1705), before turning southwards, heading for Jodoigne – this line of march took Villeroi’s army towards the narrow aperture of dry ground between the Mehaigne and Petite Gheete rivers close to the small villages of Ramillies and Taviers; but neither commander quite appreciated how far his opponent had travelled. Villeroi still believed (on 22 May) the Allies were a full day’s march away when in fact they had camped near Corswaren waiting for the Danish squadrons to catch up; for his part, Marlborough deemed Villeroi still at Jodoigne when in reality he was now approaching the plateau of Mont St. André with the intention of pitching camp near Ramillies (see map at right). However, the Prussian infantry was not there. Marlborough wrote to Lord Raby, the English resident at Berlin: \"If it should please God to give us victory over the enemy, the Allies will be little obliged to the King [Frederick] for the success.\"\n\nThe following day, at 01:00, Marlborough dispatched Cadogan, his Quartermaster-General, with an advanced guard to reconnoitre the same dry ground that Villeroi’s army was now heading toward, country that was well known to the Duke from previous campaigns. Two hours later the Duke followed with the main body: 74 battalions, 123 squadrons, 90 pieces of artillery and 20 mortars, totalling 62,000 troops. At about 08:00, after Cadogan had just passed Merdorp, his force made brief contact with a party of French hussars gathering forage on the edge of the plateau of Jandrenouille. After a brief exchange of shots the French retired and Cadogan's dragoons pressed forward. With a short lift in the mist, Cadogan soon discovered the smartly ordered lines of Villeroi’s advance guard some off; a galloper hastened back to warn Marlborough. Two hours later the Duke, accompanied by the Dutch field commander Field Marshal Overkirk, General Daniel Dopff, and the Allied staff, rode up to Cadogan where on the horizon to the westward he could discern the massed ranks of the French army deploying for battle along the front. Marlborough later told Bishop Burnet that, ‘the French army looked the best of any he had ever seen’.\n\nThe battlefield of Ramillies is very similar to that of Blenheim, for here too there is an immense area of arable land unimpeded by woods or hedges. Villeroi’s right rested on the villages of Franquenée and Taviers, with the river Mehaigne protecting his flank. A large open plain, about wide, lay between Taviers and Ramillies, but unlike Blenheim, there was no stream to hinder the cavalry. His centre was secured by Ramillies itself, lying on a slight eminence which gave distant views to the north and east. The French left flank was protected by broken country, and by a stream, the Petite Gheete, which runs deep between steep and slippery slopes. On the French side of the stream the ground rises to Offus, the village which, together with Autre-Eglise farther north, anchored Villeroi’s left flank. To the west of the Petite Gheete rises the plateau of Mont St. André; a second plain, the plateau of Jandrenouille – upon which the Anglo-Dutch army amassed – rises to the east.\n\nAt 11:00, the Duke ordered the army to take standard battle formation. On the far right, towards Foulz, the British battalions and squadrons took up their posts in a double line near the Jeuche stream. The centre was formed by the mass of Dutch, German, Protestant Swiss and Scottish infantry – perhaps 30,000 men – facing Offus and Ramillies. Also facing Ramillies Marlborough placed a powerful battery of thirty 24-pounders, dragged into position by a team of oxen; further batteries were positioned overlooking the Petite Gheete. On their left, on the broad plain between Taviers and Ramillies – and where Marlborough thought the decisive encounter must take place – Overkirk drew the 69 squadrons of the Dutch and Danish horse, supported by 19 battalions of Dutch infantry and two artillery pieces.\n\nMeanwhile, Villeroi deployed his forces. In Taviers on his right, he placed two battalions of the Greder Suisse Régiment, with a smaller force forward in Franquenée; the whole position was protected by the boggy ground of the Mehaigne river, thus preventing an Allied flanking movement. In the open country between Taviers and Ramillies, he placed 82 squadrons under General de Guiscard supported by several interleaved brigades of French, Swiss and Bavarian infantry. Along the Ramillies–Offus–Autre Eglise ridge-line, Villeroi positioned Walloon and Bavarian infantry, supported by the Elector of Bavaria's 50 squadrons of Bavarian and Walloon cavalry placed behind on the plateau of Mont St. André. Ramillies, Offus and Autre-Eglise were all packed with troops and put in a state of defence, with alleys barricaded and walls loop-holed for muskets. Villeroi also positioned powerful batteries near Ramillies. These guns (some of which were of the three barrelled kind first seen at Elixheim the previous year) enjoyed good arcs of fire, able to fully cover the approaches of the plateau of Jandrenouille over which the Allied infantry would have to pass.\n\nMarlborough, however, noticed several important weaknesses in the French dispositions. Tactically, it was imperative for Villeroi to occupy Taviers on his right and Autre-Eglise on his left, but by adopting this posture he had been forced to over-extend his forces. Moreover, this disposition – concave in relation to the Allied army – gave Marlborough the opportunity to form a more compact line, drawn up in a shorter front between the ‘horns’ of the French crescent; when the Allied blow came it would be more concentrated and carry more weight. Additionally, the Duke’s disposition facilitated the transfer of troops across his front far more easily than his foe, a tactical advantage that would grow in importance as the events of the afternoon unfolded. Although Villeroi had the option of enveloping the flanks of the Allied army as they deployed on the plateau of Jandrenouille – threatening to encircle their army – the Duke correctly gauged that the characteristically cautious French commander was intent on a defensive battle along the ridge-line.\n\nAt 13:00 the batteries went into action; a little later two Allied columns set out from the extremities of their line and attacked the flanks of the Franco-Bavarian army. To the south the Dutch Guards, under the command of Colonel Wertmüller, came forward with their two field guns to seize the hamlet of Franquenée. The small Swiss garrison in the village, shaken by the sudden onslaught and unsupported by the battalions to their rear, were soon compelled back towards the village of Taviers. Taviers was of particular importance to the Franco-Bavarian position: it protected the otherwise unsupported flank of General de Guiscard’s cavalry on the open plain, while at the same time, it allowed the French infantry to pose a threat to the flanks of the Dutch and Danish squadrons as they came forward into position. But hardly had the retreating Swiss rejoined their comrades in that village when the Dutch Guards renewed their attack. The fighting amongst the alleys and cottages soon deteriorated into a fierce bayonet and clubbing \"mêlée\", but the superiority in Dutch firepower soon told. The accomplished French officer, Colonel de la Colonie, standing on the plain nearby remembered – \"this village was the opening of the engagement, and the fighting there was almost as murderous as the rest of the battle put together.\" By about 15:00 the Swiss had been pushed out of the village into the marshes beyond.\n\nVilleroi’s right flank fell into chaos and was now open and vulnerable. Alerted to the situation de Guiscard ordered an immediate attack with 14 squadrons of French dragoons currently stationed in the rear. Two other battalions of the Greder Suisse Régiment were also sent, but the attack was poorly co-ordinated and consequently went in piecemeal. The Anglo-Dutch commanders now sent dismounted Dutch dragoons into Taviers, which, together with the Guards and their field guns, poured concentrated musketry- and canister-fire into the advancing French troops. Colonel d’Aubigni, leading his regiment, fell mortally wounded.\n\nAs the French ranks wavered, the leading squadrons of Württemberg’s Danish horse – now unhampered by enemy fire from either village – were also sent into the attack and fell upon the exposed flank of the Franco-Swiss infantry and dragoons. De la Colonie, with his Grenadiers Rouge regiment, together with the Cologne Guards who were brigaded with them, was now ordered forward from his post south of Ramillies to support the faltering counter-attack on the village. But on his arrival, all was chaos – \"Scarcely had my troops got over when the dragoons and Swiss who had preceded us, came tumbling down upon my battalions in full flight … My own fellows turned about and fled along with them.\" De La Colonie managed to rally some of his grenadiers, together with the remnants of the French dragoons and Greder Suisse battalions, but it was an entirely peripheral operation, offering only fragile support for Villeroi’s right flank.\n\nWhile the attack on Taviers went in the Earl of Orkney launched his first line of English across the Petite Gheete in a determined attack against the barricaded villages of Offus and Autre-Eglise on the Allied right. Villeroi, posting himself near Offus, watched anxiously the redcoats' advance, mindful of the counsel he had received on 6 May from Louis XIV – \"Have particular care to that part of the line which will endure the first shock of the English troops.\" Heeding this advice the French commander began to transfer battalions from his centre to reinforce the left, drawing more foot from the already weakened right to replace them.\n\nAs the English battalions descended the gentle slope of the Petite Gheete valley, struggling through the boggy stream, they were met by Major General de la Guiche’s disciplined Walloon infantry sent forward from around Offus. After concentrated volleys, exacting heavy casualties on the redcoats, the Walloons reformed back to the ridgeline in good order. The English took some time to reform their ranks on the dry ground beyond the stream and press on up the slope towards the cottages and barricades on the ridge. The vigour of the English assault, however, was such that they threatened to break through the line of the villages and out onto the open plateau of Mont St André beyond. This was potentially dangerous for the Allied infantry who would then be at the mercy of the Elector’s Bavarian and Walloon squadrons patiently waiting on the plateau for the order to move.\n\nAlthough Henry Lumley’s English cavalry had managed to cross the marshy ground around the Petite Gheete, it was soon evident to Marlborough that sufficient cavalry support would not be practicable and that the battle could not be won on the Allied right. The Duke, therefore, called off the attack against Offus and Autre-Eglise. To make sure that Orkney obeyed his order to withdraw, Marlborough sent his Quartermaster-General in person with the command. Despite Orkney’s protestations, Cadogan insisted on compliance and, reluctantly, Orkney gave the word for his troops to fall back to their original positions on the edge of the plateau of Jandrenouille. It is still not clear how far Orkney’s advance was planned only as a feint; according to historian David Chandler it is probably more accurate to surmise that Marlborough launched Orkney in a serious probe with a view to sounding out the possibilities of the sector. Nevertheless, the attack had served its purpose. Villeroi had given his personal attention to that wing and strengthened it with large bodies of horse and foot that ought to have been taking part in the decisive struggle south of Ramillies.\n\nMeanwhile, the Dutch assault on Ramillies was gaining pace. Marlborough’s younger brother, General of Infantry, Charles Churchill, ordered four brigades of foot to attack the village. The assault consisted of 12 battalions of Dutch infantry commanded by Major Generals Schultz and Spaar; two brigades of Saxons under Count Schulenburg; a Scottish brigade in Dutch service led by the 2nd Duke of Argyle; and a small brigade of Protestant Swiss. The 20 French and Bavarian battalions in Ramillies, supported by the Irish dragoons who had left Ireland in the Flight of the Wild Geese to join Clare's Dragoons and a small brigade of Cologne and Bavarian Guards under the Marquis de Maffei, put up a determined defence, initially driving back the attackers with severe losses as commemorated in the song \"Clare's Dragoons\":\n<poem>\nWhen on Ramillies' bloody field\nThe baffled French were forced to yield,\nThe victor Saxon backward reeled\nBefore the charge of Clare's Dragoons.\n\n\"Viva là, the new brigade!\"\n\"Viva là the old one too!\"\n\"Viva là, the Rose shall fade\"\n\"The Shamrock shine forever new!\"\n</poem>\nSeeing that Schultz and Spaar were faltering, Marlborough now ordered Orkney’s second-line British and Danish battalions (who had not been used in the assault on Offus and Autre-Eglise) to move south towards Ramillies. Shielded as they were from observation by a slight fold in the land, their commander, Brigadier-General Van Pallandt, ordered the regimental colours to be left in place on the edge of the plateau to convince their opponents they were still in their initial position. Therefore, unbeknown to the French who remained oblivious to the Allies’ real strength and intentions on the opposite side of the Petite Gheete, Marlborough was throwing his full weight against Ramillies and the open plain to the south. Villeroi meanwhile, was still moving more reserves of infantry in the opposite direction towards his left flank; crucially, it would be some time before the French commander noticed the subtle change in emphasis of the Allied dispositions.\n\nAt around 15:30, Overkirk advanced his massed squadrons on the open plain in support of the infantry attack on Ramillies. Overkirk's squadrons – 48 Dutch, supported on their left by 21 Danish – steadily advanced towards the enemy (taking care not to prematurely tire the horses), before breaking into a trot to gain the impetus for their charge. The Marquis de Feuquières writing after the battle described the scene – \"They advanced in four lines … As they approached they advanced their second and fourth lines into the intervals of their first and third lines; so that when they made their advance upon us, they formed only one front, without any intermediate spaces.\"\n\nThe initial clash favoured the Dutch and Danish squadrons. The disparity of numbers – exacerbated by Villeroi stripping their ranks of infantry to reinforce his left flank – enabled Overkirk's cavalry to throw the first line of French horse back in some disorder towards their second-line squadrons. This line also came under severe pressure and, in turn, was forced back to their third-line of cavalry and the few battalions still remaining on the plain. But these French horsemen were amongst the best in Louis XIV’s army – the \"Maison du Roi\", supported by four elite squadrons of Bavarian Cuirassiers. Ably led by de Guiscard, the French cavalry rallied, thrusting back the Allied squadrons in successful local counterattacks. On Overkirk’s right flank, close to Ramillies, ten of his squadrons suddenly broke ranks and were scattered, riding headlong to the rear to recover their order, leaving the left flank of the Allied assault on Ramillies dangerously exposed. Notwithstanding the lack of infantry support, de Guiscard threw his cavalry forward in an attempt to split the Allied army in two.\n\nA crisis threatened the centre, but from his vantage point Marlborough was at once aware of the situation. The Allied commander now summoned the cavalry on the right wing to reinforce his centre, leaving only the English squadrons in support of Orkney. Thanks to a combination of battle-smoke and favourable terrain, his redeployment went unnoticed by Villeroi who made no attempt to transfer any of his own 50 unused squadrons. While he waited for the fresh reinforcements to arrive, Marlborough flung himself into the \"mêlée\", rallying some of the Dutch cavalry who were in confusion. But his personal involvement nearly led to his undoing. A number of French horsemen, recognising the Duke, came surging towards his party. Marlborough’s horse tumbled and the Duke was thrown – \"Milord Marlborough was rid over,\" wrote Orkney some time later. It was a critical moment of the battle. \"Major-General Murray,\" recalled one eye witness, \" … seeing him fall, marched up in all haste with two Swiss battalions to save him and stop the enemy who were hewing all down in their way.\" Fortunately Marlborough’s newly appointed aide-de-camp, Richard Molesworth, galloped to the rescue, mounted the Duke on his horse and made good their escape, before Murray’s disciplined ranks threw back the pursuing French troopers.\n\nAfter a brief pause, Marlborough’s equerry, Colonel Bringfield (or Bingfield), led up another of the Duke’s spare horses; but while assisting him onto his mount, the unfortunate Bringfield was hit by an errant cannonball that sheared off his head. One account has it that the cannonball flew between the Captain-General’s legs before hitting the unfortunate colonel, whose torso fell at Marlborough’s feet – a moment subsequently depicted in a lurid set of contemporary playing cards. Nevertheless, the danger passed, enabling the Duke to attend to the positioning of the cavalry reinforcements feeding down from his right flank – a change of which Villeroi remained blissfully unaware.\n\nThe time was about 16:30, and the two armies were in close contact across the whole front, from the skirmishing in the marshes in the south, through the vast cavalry battle on the open plain; to the fierce struggle for Ramillies at the centre, and to the north, where, around the cottages of Offus and Autre-Eglise, Orkney and de la Guiche faced each other across the Petite Gheete ready to renew hostilities.\n\nThe arrival of the transferring squadrons now began to tip the balance in favour of the Allies. Tired, and suffering a growing list of casualties, the numerical inferiority of Guiscard’s squadrons battling on the plain at last began to tell. After earlier failing to hold or retake Franquenée and Taviers, Guiscard’s right flank had become dangerously exposed and a fatal gap had opened on the right of their line. Taking advantage of this breach, Württemberg’s Danish cavalry now swept forward, wheeling to penetrate the flank of the Maison du Roi whose attention was almost entirely fixed on holding back the Dutch. Sweeping forwards, virtually without resistance, the 21 Danish squadrons reformed behind the French around the area of the Tomb of Ottomond, facing north across the plateau of Mont St André towards the exposed flank of Villeroi’s army.\n\nThe final Allied reinforcements for the cavalry contest to the south were at last in position; Marlborough’s superiority on the left could no longer be denied, and his fast-moving plan took hold of the battlefield. Now, far too late, Villeroi tried to redeploy his 50 unused squadrons, but a desperate attempt to form line facing south, stretching from Offus to Mont St André, floundered amongst the baggage and tents of the French camp carelessly left there after the initial deployment. The Allied commander ordered his cavalry forward against the now heavily outnumbered French and Bavarian horsemen. De Guiscard’s right flank, without proper infantry support, could no longer resist the onslaught and, turning their horses northwards, they broke and fled in complete disorder. Even the squadrons currently being scrambled together by Villeroi behind Ramillies could not withstand the onslaught. \"We had not got forty yards on our retreat,\" remembered Captain Peter Drake, an Irishmen serving with the French – \"when the words \"sauve qui peut\" went through the great part, if not the whole army, and put all to confusion\"\n\nIn Ramillies the Allied infantry, now reinforced by the English troops brought down from the north, at last broke through. The Régiment de Picardie stood their ground but were caught between Colonel Borthwick’s Scots-Dutch regiment and the English reinforcements. Borthwick was killed, as was Charles O’Brien, the Irish Viscount Clare in French service, fighting at the head of his regiment. The Marquis de Maffei attempted one last stand with his Bavarian and Cologne Guards, but it proved in vain. Noticing a rush of horsemen fast approaching from the south, he later recalled – \" … I went towards the nearest of these squadrons to instruct their officer, but instead of being listened to [I] was immediately surrounded and called upon to ask for quarter.\"\n\nThe roads leading north and west were choked with fugitives. Orkney now sent his English troops back across the Petite Gheete stream to once again storm Offus where de la Guiche’s infantry had begun to drift away in the confusion. To the right of the infantry Lord John Hay’s ‘Scots Greys’ also picked their way across the stream and charged the Régiment du Roi within Autre-Eglise. \"Our dragoons,\" wrote John Deane, \"pushing into the village … made terrible slaughter of the enemy.\" The Bavarian Horse Grenadiers and the Electoral Guards withdrew and formed a shield about Villeroi and the Elector but were scattered by Lumley’s cavalry. Stuck in the mass of fugitives fleeing the battlefield, the French and Bavarian commanders narrowly escaped capture by General Cornelius Wood who, unaware of their identity, had to content himself with the seizure of two Bavarian Lieutenant-Generals. Far to the south, the remnants of de la Colonie’s brigade headed in the opposite direction towards the French held fortress of Namur.\"\n\nThe retreat became a rout. Individual Allied commanders drove their troops forward in pursuit, allowing their beaten enemy no chance to recover. Soon the Allied infantry could no longer keep up, but their cavalry were off the leash, heading through the gathering night for the crossings on the Dyle river. At last, however, Marlborough called a halt to the pursuit shortly after midnight near Meldert, from the field. \"It was indeed a truly shocking sight to see the miserable remains of this mighty army,\" wrote Captain Drake, \"… reduced to a handful.\"\n\nWhat was left of Villeroi’s army was now broken in spirit; the imbalance of the casualty figures amply demonstrates the extent of the disaster for Louis XIV’s army: (\"see below\"). In addition, hundreds of French soldiers were fugitives, many of whom would never remuster to the colours. Villeroi also lost 52 artillery pieces and his entire engineer pontoon train. In the words of Marshal Villars, the French defeat at Ramillies was – \"The most shameful, humiliating and disastrous of routs.\"\n\nTown after town now succumbed to the Allies. Leuven fell on 25 May 1706; three days later, the Allies entered Brussels, the capital of the Spanish Netherlands. Marlborough realised the great opportunity created by the early victory of Ramillies: \"We now have the whole summer before us,\" wrote the Duke from Brussels to Robert Harley, \"and with the blessing of God I shall make the best use of it.\" Malines, Lierre, Ghent, Alost, Damme, Oudenaarde, Bruges, and on 6 June Antwerp, all subsequently fell to Marlborough’s victorious army and, like Brussels, proclaimed the Austrian candidate for the Spanish throne, the Archduke Charles, as their sovereign. Villeroi was helpless to arrest the process of collapse. When Louis XIV learnt of the disaster he recalled Marshal Vendôme from northern Italy to take command in Flanders; but it would be weeks before the command changed hands.\n\nAs news spread of the Allies’ triumph, the Prussians, Hessians and Hanoverian contingents, long delayed by their respective rulers, eagerly joined the pursuit of the broken French and Bavarian forces. \"This,\" wrote Marlborough wearily, \"I take to be owing to our late success.\" Meanwhile, Overkirk took the port of Ostend on 4 July thus opening a direct route to the English Channel for communication and supply, but the Allies were making scant progress against Dendermonde whose governor, the Marquis de Valée, was stubbornly resisting. Only later when Cadogan and Churchill went to take charge did the town’s defences begin to fail.\n\nVendôme formally took over command in Flanders on 4 August; Villeroi would never again receive a major command – \"I cannot foresee a happy day in my life save only that of my death.\" Louis XIV was more forgiving to his old friend – \"At our age, Marshal, we must no longer expect good fortune.\" In the mean time, Marlborough invested the elaborate fortress of Menin which, after a costly siege, capitulated on 22 August. Dendermonde finally succumbed on 6 September followed by Ath – the last conquest of 1706 – on 2 October. By the time Marlborough had closed down the Ramillies campaign he had denied the French most of the Spanish Netherlands west of the Meuse and north of the Sambre – it was an unsurpassed operational triumph for the English Duke but once again it was not decisive as these gains did not defeat France.\n\nThe immediate question for the Allies now was how to deal with the Spanish Netherlands, a subject which the Austrians and the Dutch were diametrically opposed. Emperor Joseph I, acting on behalf of his younger brother King ’Charles III’, absent in Spain, claimed that reconquered Brabant and Flanders should be put under immediate possession of a governor named by himself. The Dutch, however, who had supplied the major share of the troops and money to secure the victory (the Austrians had produced nothing of either) claimed the government of the region till the war was over, and that after the peace they should continue to garrison Barrier Fortresses stronger than those which had fallen so easily to Louis XIV’s forces in 1701. Marlborough mediated between the two parties but favoured the Dutch position. To sway the Duke’s opinion, the Emperor offered Marlborough the governorship of the Spanish Netherlands. It was a tempting offer, but in the name of Allied unity, it was one he refused. In the end England and the Dutch Republic took control of the newly won territory for the duration of the war; after which it was to be handed over to the direct rule of ‘Charles III’, subject to the reservation of a Dutch Barrier, the extent and nature of which had yet to be settled.\n\nMeanwhile, on the Upper Rhine, Villars had been forced onto the defensive as battalion after battalion had been sent north to bolster collapsing French forces in Flanders; there was now no possibility of his undertaking the re-capture of Landau. Further good news for the Allies arrived from northern Italy where, on 7 September, Prince Eugene had routed a French army before the Piedmontese capital, Turin, driving the Franco-Spanish forces from northern Italy. Only from Spain did Louis XIV receive any good news where Das Minas and Galway had been forced to retreat from Madrid towards Valencia, allowing Philip V to re-enter his capital on 4 October. All in all though, the situation had changed considerably and Louis XIV began to look for ways to end what was fast becoming a ruinous war for France. For Queen Anne also, the Ramillies campaign had one overriding significance – \"Now we have God be thanked so hopeful a prospect of peace.\" Instead of continuing the momentum of victory, however, cracks in Allied unity would enable Louis XIV to reverse some of the major setbacks suffered at Turin and Ramillies.\n\nThe total number of French casualties cannot be calculated precisely, so complete was the collapse of the Franco-Bavarian army that day. David G. Chandler’s \"Marlborough as Military Commander\" and \"A Guide to the Battlefields of Europe\" are consistent with regards to French casualty figures i.e., 12,000 dead and wounded plus some 7,000 taken prisoner. James Falkner, in \"Ramillies 1706: Year of Miracles,\" also notes 12,000 dead and wounded and states ‘up to 10,000’ taken prisoner. In \"The Collins Encyclopaedia of Military History\", Dupuy puts Villeroi’s dead and wounded at 8,000, with a further 7,000 captured. John Millner’s memoirs – \"Compendious Journal\" (1733) – is more specific, recording 12,087 of Villeroi’s army were killed or wounded, with another 9,729 taken prisoner. In \"Marlborough\", however, Correlli Barnett puts the total casualty figure as high as 30,000 – 15,000 dead and wounded with an additional 15,000 taken captive. Trevelyan estimates Villeroi’s casualties at 13,000, but adds, ‘his losses by desertion may have doubled that number’. La Colonie omits a casualty figure in his \"Chronicles of an old Campaigner\"; but Saint-Simon in his \"Memoirs\" states 4,000 killed, adding 'many others were wounded and many important persons were taken prisoner'. Voltaire, however, in \"Histoire du siècle du Louis XIV\" records, 'the French lost there twenty thousand men'.\n\n\n\n"}
{"id": "4051", "url": "https://en.wikipedia.org/wiki?curid=4051", "title": "Brian Kernighan", "text": "Brian Kernighan\n\nBrian Wilson Kernighan (; born January 1, 1942) is a Canadian computer scientist who worked at Bell Labs alongside Unix creators Ken Thompson and Dennis Ritchie and contributed to the development of Unix. He is also coauthor of the AWK and AMPL programming languages. The \"K\" of K&R C and the \"K\" in AWK both stand for \"Kernighan\". Since 2000 Brian Kernighan has been a Professor at the Computer Science Department of Princeton University, where he is also the Undergraduate Department Representative.\n\nBorn in Toronto, Kernighan attended the University of Toronto between 1960 and 1964, earning his Bachelor's degree in engineering physics. He received his PhD in electrical engineering from Princeton University in 1969 for research supervised by Peter Weiner.\n\nKernighan has held a professorship in the department of computer science at Princeton since 2000. Each fall he teaches a course called \"Computers in Our World\", which introduces the fundamentals of computing to non-majors. Kernighan's name became widely known through co-authorship of the first book on the C programming language with Dennis Ritchie. Kernighan affirmed that he had no part in the design of the C language (\"it's entirely Dennis Ritchie's work\"). He authored many Unix programs, including ditroff.\n\nIn collaboration with Shen Lin he devised well-known heuristics for two NP-complete optimization problems: graph partitioning and the travelling salesman problem. In a display of authorial equity, the former is usually called the Kernighan–Lin algorithm, while the latter is known as the Lin–Kernighan heuristic.\n\nKernighan was the software editor for Prentice Hall International. His \"Software Tools\" series spread the essence of \"C/Unix thinking\" with makeovers for BASIC, FORTRAN, and Pascal, and most notably his \"Ratfor\" (rational FORTRAN) was put in the public domain.\n\nHe has said that if stranded on an island with only one programming language it would have to be C.\n\nKernighan coined the term Unix and helped popularize Thompson's Unix philosophy. Kernighan is also known as a coiner of the expression \"What You See Is All You Get\" (WYSIAYG), which is a sarcastic variant of the original \"What You See Is What You Get\" (WYSIWYG). Kernighan's term is used to indicate that WYSIWYG systems might throw away information in a document that could be useful in other contexts.\n\nKernighan's original 1978 implementation of Hello, World! was sold at The Algorithm Auction, the world’s first auction of computer algorithms.\n\nIn 1996, Kernighan taught CS50 which is the Harvard University introductory course in Computer Science. His students on CS50 include David J. Malan who now runs the course.\n\nOther achievements during his career include:\n\n"}
{"id": "4052", "url": "https://en.wikipedia.org/wiki?curid=4052", "title": "BCPL", "text": "BCPL\n\nBCPL (\"Basic Combined Programming Language\") is a procedural, imperative, and structured computer programming language designed by Martin Richards of the University of Cambridge in 1966.\n\nOriginally intended for writing compilers for other languages, BCPL is no longer in common use. However, its influence is still felt because a stripped down and syntactically changed version of BCPL, called B, was the language on which the C programming language was based. This led many C programmers to give BCPL the humorous backronym Before C Programming Language.\n\nBCPL was the first brace programming language, and the braces survived the syntactical changes and have become a common means of denoting program source code statements. In practice, on limited keyboards of the day, source programs often used the sequences codice_1 and codice_2 in place of the symbols codice_3 and codice_4.\nThe single-line codice_5 comments of BCPL, which were not adopted by C, reappeared in C++ and later in C99.\n\nBCPL was a response to difficulties with its predecessor Combined Programming Language (CPL), which was designed during the early 1960s. Richards created BCPL by \"removing those features of the full language which make compilation difficult\". The first compiler implementation, for the IBM 7094 under Compatible Time-Sharing System (CTSS), was written while Richards was visiting Project MAC at the Massachusetts Institute of Technology (MIT) in the spring of 1967. The language was first described in a paper presented to the 1969 Spring Joint Computer Conference.\n\nIt was designed so that small and simple compilers could be written for it; reputedly some compilers could be run in 16 kilobytes. Further, the Richards compiler, itself written in BCPL, was easily portable. BCPL was thus a popular choice for bootstrapping a system.\n\nA major reason for the compiler's portability lay in its structure. It was split into two parts: the front end parsed the source and generated O-code for a virtual machine, and the back end took the O-code and translated it into the code for the target machine. Only 1/5 of the compiler's code needed to be rewritten to support a new machine, a task that usually took between 2 and 5 man-months. This approach became common practice later, e.g., Pascal or Java, but the Richards BCPL compiler was the first to define a virtual machine for this purpose.\n\nThe language is unusual in having only one data type: a word, a fixed number of bits, usually chosen to align with the architecture's machine word and of adequate capacity to represent any valid storage address. For many machines of the time, this data type was a 16-bit word. This choice later proved to be a significant problem when BCPL was used on machines in which the smallest addressable item was not a word but a byte, or on machines with larger word sizes such as 32-bit or 64-bit.\n\nThe interpretation of any value was determined by the operators used to process the values. (For example, + added two values together treating them as integers; ! indirected through a value, effectively treating it as a pointer.) In order for this to work, the implementation provided no type checking. The Hungarian notation was developed to help programmers avoid inadvertent type errors.\n\nThe mismatch between BCPL's word orientation and byte-oriented hardware was addressed in several ways. One was providing standard library routines for packing and unpacking words into byte strings. Later, two language features were added: the bit-field selection operator and the infix byte indirection operator (denoted by the '%' character).\n\nBCPL handles bindings spanning separate compilation units in a unique way. There are no user-declarable global variables; instead there is a global vector, which is similar to \"blank common\" in Fortran. All data shared between different compilation units comprises scalars and pointers to vectors stored in a pre-arranged place in the global vector. Thus the header files (files included during compilation using the \"GET\" directive) become the primary means of synchronizing global data between compilation units, containing \"GLOBAL\" directives that present lists of symbolic names, each paired with a number that associates the name with the corresponding numerically addressed word in the global vector. As well as variables, the global vector also contains bindings for external procedures. This makes dynamic loading of compilation units very simple to achieve. Instead of relying on the link loader of the underlying implementation, effectively BCPL gives the programmer control of the linking process.\n\nThe global vector also made it very simple to replace or augment standard library routines. A program could save the pointer from the global vector to the original routine and replace it with a pointer to an alternative version. The alternative might call the original as part of its processing. This could be used as a quick \"ad hoc\" debugging aid.\n\nThe book \"BCPL: The language and its compiler\" describes the philosophy of BCPL as follows:\nBoth the design and philosophy of BCPL strongly influenced B, which in turn influenced C. Programmers at the time debated whether an eventual successor to C would be called \"D\" (the next letter in the alphabet) or \"P\" (the next letter in the parent language name). As it happened, the actual name turned out to be \"C++\" (a pun on the C language increment operator).\n\nBCPL has been rumored to have originally stood for \"Bootstrap Cambridge Programming Language\", but CPL was never created since development stopped at BCPL, and the acronym was later reinterpreted for the BCPL book.\n\nBCPL is the language in which the original hello world program was written. The first MUD was also written in BCPL (MUD1).\n\nSeveral operating systems were written partially or wholly in BCPL (for example, TRIPOS and the earliest versions of AmigaDOS). BCPL was also the initial language used in the seminal Xerox PARC Alto project, the first modern personal computer; among other projects, the Bravo document preparation system was written in BCPL.\n\nAn early compiler, bootstrapped in 1969, by starting with a paper tape of the O-code of Martin Richards's Atlas 2 compiler, targeted the ICT 1900 series. The two machines had different word-lengths (48 vs 24 bits), different character encodings, and different packed string representations—and the successful bootstrapping increased confidence in the practicality of the method.\n\nBy late 1970, implementations existed for the Honeywell 635 and Honeywell 645, the IBM 360, the PDP-10, the TX-2, the CDC 6400, the UNIVAC 1108, the PDP-9, the KDF 9 and the Atlas 2. In 1974 a dialect of BCPL was implemented at BBN without using the intermediate O-code. The initial implementation was a cross-compiler hosted on BBN's Tenex PDP-10s, and directly targeted the PDP-11s used in BBN's implementation of the second generation IMPs used in the Arpanet.\n\nThere was also a version produced for the BBC Micro in the mid-1980s, by Richards Computer Products, a company started by John Richards, the brother of Dr. Martin Richards. The BBC Domesday Project made use of the language. Versions of BCPL for the Amstrad CPC and Amstrad PCW computers were also released in 1986 by UK software house Arnor Ltd. MacBCPL was released for the Apple Macintosh in 1985 by Topexpress Ltd, of Kensington, England.\n\nIn 1979, implementations of BCPL existed for at least 25 architectures; the language gradually fell out of favour as C became popular on non-Unix systems.\n\nThese complete and compilable examples are from Martin Richards’ BCPL distribution.\n\nPrint factorials:\nCount solutions to the N queens problem:\n\n"}
{"id": "4054", "url": "https://en.wikipedia.org/wiki?curid=4054", "title": "Battleship", "text": "Battleship\n\nA battleship is a large armored warship with a main battery consisting of large caliber guns. During the late 19th and early 20th centuries the battleship was the most powerful type of warship, and a fleet of battleships was vital for any nation that desired to maintain command of the sea.\n\nThe word \"battleship\" was coined around 1794 and is a contraction of the phrase \"line-of-battle ship\", the dominant wooden warship during the Age of Sail. The term came into formal use in the late 1880s to describe a type of ironclad warship, now referred to by historians as pre-dreadnought battleships. In 1906, the commissioning of heralded a revolution in battleship design. Subsequent battleship designs, influenced by HMS \"Dreadnought\", were referred to as \"dreadnoughts\".\n\nBattleships were a symbol of naval dominance and national might, and for decades the battleship was a major factor in both diplomacy and military strategy. A global arms race in battleship construction began in Europe in the 1890s and culminated at the decisive Battle of Tsushima in 1905; the outcome of which significantly influenced the design of HMS \"Dreadnought\". The launch of \"Dreadnought\" in 1906 commenced a new naval arms race. Three major fleet actions between steel battleships took place: the decisive battles of the Yellow Sea (1904) and Tsushima (1905) during the Russo-Japanese War, and the inconclusive Battle of Jutland (1916) during the First World War. Jutland was the largest naval battle and the only full-scale clash of battleships in the war, and it was the last major battle fought primarily by battleships in world history.\n\nThe Naval Treaties of the 1920s and 1930s limited the number of battleships, though technical innovation in battleship design continued. Both the Allied and Axis powers built battleships during World War II, though the increasing importance of the aircraft carrier meant that the battleship played a less important role than had been expected.\n\nThe value of the battleship has been questioned, even during their heyday. There were few of the decisive fleet battles that battleship proponents expected, and used to justify the vast resources spent on building battlefleets. Even in spite of their huge firepower and protection, battleships were increasingly vulnerable to much smaller, cheaper weapons: initially the torpedo and the naval mine, and later aircraft and the guided missile. The growing range of naval engagements led to the aircraft carrier replacing the battleship as the leading capital ship during World War II, with the last battleship to be launched being in 1944. Battleships were retained by the United States Navy into the Cold War for fire support purposes before being stricken from the U.S. Naval Vessel Register in the 2000s.\n\nA ship of the line was a large, unarmored wooden sailing ship which mounted a battery of up to 120 smoothbore guns and carronades. The ship of the line developed gradually over centuries and, apart from growing in size, it changed little between the adoption of line of battle tactics in the early 17th century and the end of the sailing battleship's heyday in the 1830s. From 1794, the alternative term 'line of battle ship' was contracted (informally at first) to 'battle ship' or 'battleship'.\n\nThe sheer number of guns fired broadside meant a sail battleship could wreck any wooden enemy, holing her hull, knocking down masts, wrecking her rigging, and killing her crew. However, the effective range of the guns was as little as a few hundred yards, so the battle tactics of sailing ships depended in part on the wind.\n\nThe first major change to the ship of the line concept was the introduction of steam power as an auxiliary propulsion system. Steam power was gradually introduced to the navy in the first half of the 19th century, initially for small craft and later for frigates. The French Navy introduced steam to the line of battle with the 90-gun in 1850—the first true steam battleship. \"Napoléon\" was armed as a conventional ship-of-the-line, but her steam engines could give her a speed of , regardless of the wind condition. This was a potentially decisive advantage in a naval engagement. The introduction of steam accelerated the growth in size of battleships. France and the United Kingdom were the only countries to develop fleets of wooden steam screw battleships although several other navies operated small numbers of screw battleships, including Russia (9), Turkey (3), Sweden (2), Naples (1), Denmark (1) and Austria (1).\n\nThe adoption of steam power was only one of a number of technological advances which revolutionized warship design in the 19th century. The ship of the line was overtaken by the ironclad: powered by steam, protected by metal armor, and armed with guns firing high-explosive shells.\n\nGuns that fired explosive or incendiary shells were a major threat to wooden ships, and these weapons quickly became widespread after the introduction of 8 inch shell guns as part of the standard armament of French and American line-of-battle ships in 1841. In the Crimean War, six line-of-battle ships and two frigates of the Russian Black Sea Fleet destroyed seven Turkish frigates and three corvettes with explosive shells at the Battle of Sinop in 1853. Later in the war, French ironclad floating batteries used similar weapons against the defenses at the Battle of Kinburn.\n\nNevertheless, wooden-hulled ships stood up comparatively well to shells, as shown in the 1866 Battle of Lissa, where the modern Austrian steam two-decker ranged across a confused battlefield, rammed an Italian ironclad and took 80 hits from Italian ironclads, many of which were shells, but including at least one 300 pound shot at point blank range. Despite losing her bowsprit and her foremast, and being set on fire, she was ready for action again the very next day.\n\nThe development of high-explosive shells made the use of iron armor plate on warships necessary. In 1859 France launched , the first ocean-going ironclad warship. She had the profile of a ship of the line, cut to one deck due to weight considerations. Although made of wood and reliant on sail for most journeys, \"Gloire\" was fitted with a propeller, and her wooden hull was protected by a layer of thick iron armor. \"Gloire\" prompted further innovation from the Royal Navy, anxious to prevent France from gaining a technological lead.\n\nThe superior armored frigate followed \"Gloire\" by only 14 months, and both nations embarked on a program of building new ironclads and converting existing screw ships of the line to armored frigates. Within two years, Italy, Austria, Spain and Russia had all ordered ironclad warships, and by the time of the famous clash of the and the at the Battle of Hampton Roads at least eight navies possessed ironclad ships.\nNavies experimented with the positioning of guns, in turrets (like the USS \"Monitor\"), central-batteries or barbettes, or with the ram as the principal weapon. As steam technology developed, masts were gradually removed from battleship designs. By the mid-1870s steel was used as a construction material alongside iron and wood. The French Navy's , laid down in 1873 and launched in 1876, was a central battery and barbette warship which became the first battleship in the world to use steel as the principal building material.\n\nThe term \"battleship\" was officially adopted by the Royal Navy in the re-classification of 1892. By the 1890s, there was an increasing similarity between battleship designs, and the type that later became known as the 'pre-dreadnought battleship' emerged. These were heavily armored ships, mounting a mixed battery of guns in turrets, and without sails. The typical first-class battleship of the pre-dreadnought era displaced 15,000 to 17,000 tons, had a speed of , and an armament of four guns in two turrets fore and aft with a mixed-caliber secondary battery amidships around the superstructure. An early design with superficial similarity to the pre-dreadnought is the British of 1871.\n\nThe slow-firing main guns were the principal weapons for battleship-to-battleship combat. The intermediate and secondary batteries had two roles. Against major ships, it was thought a 'hail of fire' from quick-firing secondary weapons could distract enemy gun crews by inflicting damage to the superstructure, and they would be more effective against smaller ships such as cruisers. Smaller guns (12-pounders and smaller) were reserved for protecting the battleship against the threat of torpedo attack from destroyers and torpedo boats.\n\nThe beginning of the pre-dreadnought era coincided with Britain reasserting her naval dominance. For many years previously, Britain had taken naval supremacy for granted. Expensive naval projects were criticised by political leaders of all inclinations. However, in 1888 a war scare with France and the build-up of the Russian navy gave added impetus to naval construction, and the British Naval Defence Act of 1889 laid down a new fleet including eight new battleships. The principle that Britain's navy should be more powerful than the two next most powerful fleets combined was established. This policy was designed to deter France and Russia from building more battleships, but both nations nevertheless expanded their fleets with more and better pre-dreadnoughts in the 1890s.\nIn the last years of the 19th century and the first years of the 20th, the escalation in the building of battleships became an arms race between Britain and Germany. The German naval laws of 1890 and 1898 authorised a fleet of 38 battleships, a vital threat to the balance of naval power. Britain answered with further shipbuilding, but by the end of the pre-dreadnought era, British supremacy at sea had markedly weakened. In 1883, the United Kingdom had 38 battleships, twice as many as France and almost as many as the rest of the world put together. By 1897, Britain's lead was far smaller due to competition from France, Germany, and Russia, as well as the development of pre-dreadnought fleets in Italy, the United States and Japan. Turkey, Spain, Sweden, Denmark, Norway, the Netherlands, Chile and Brazil all had second-rate fleets led by armored cruisers, coastal defence ships or monitors.\n\nPre-dreadnoughts continued the technical innovations of the ironclad. Turrets, armor plate, and steam engines were all improved over the years, and torpedo tubes were introduced. A small number of designs, including the American and es, experimented with all or part of the 8-inch intermediate battery superimposed over the 12-inch primary. Results were poor: recoil factors and blast effects resulted in the 8-inch battery being completely unusable, and the inability to train the primary and intermediate armaments on different targets led to significant tactical limitations. Even though such innovative designs saved weight (a key reason for their inception), they proved too cumbersome in practice.\n\nIn 1906, the British Royal Navy launched the revolutionary . Created as a result of pressure from Admiral Sir John (\"Jackie\") Fisher, HMS \"Dreadnought\" made existing battleships obsolete. Combining an \"all-big-gun\" armament of ten 12-inch (305 mm) guns with unprecedented speed (from steam turbine engines) and protection, she prompted navies worldwide to re-evaluate their battleship building programs. While the Japanese had laid down an all-big-gun battleship, , in 1904 and the concept of an all-big-gun ship had been in circulation for several years, it had yet to be validated in combat. \"Dreadnought\" sparked a new arms race, principally between Britain and Germany but reflected worldwide, as the new class of warships became a crucial element of national power.\n\nTechnical development continued rapidly through the dreadnought era, with steep changes in armament, armor and propulsion. Ten years after \"Dreadnought\"s commissioning, much more powerful ships, the super-dreadnoughts, were being built.\n\nIn the first years of the 20th century, several navies worldwide experimented with the idea of a new type of battleship with a uniform armament of very heavy guns.\n\nAdmiral Vittorio Cuniberti, the Italian Navy's chief naval architect, articulated the concept of an all-big-gun battleship in 1903. When the \"Regia Marina\" did not pursue his ideas, Cuniberti wrote an article in \"Janes\" proposing an \"ideal\" future British battleship, a large armored warship of 17,000 tons, armed solely with a single calibre main battery (twelve 12-inch {305 mm} guns), carrying belt armor, and capable of 24 knots (44 km/h).\n\nThe Russo-Japanese War provided operational experience to validate the 'all-big-gun' concept. At the Yellow Sea and Tsushima, pre-dreadnoughts exchanged volleys at ranges of 7,600–12,000 yd (7 to 11 km), beyond the range of the secondary batteries. It is often held that these engagements demonstrated the importance of the gun over its smaller counterparts, though some historians take the view that secondary batteries were just as important as the larger weapons.\n\nIn Japan, the two battleships of the 1903-4 Programme were the first to be laid down as all-big-gun designs, with eight 12-inch guns. However, the design had armor which was considered too thin, demanding a substantial redesign. The financial pressures of the Russo-Japanese War and the short supply of 12-inch guns which had to be imported from Britain meant these ships were completed with a mixed 10- and 12-inch armament. The 1903-4 design also retained traditional triple-expansion steam engines.\nAs early as 1904, Jackie Fisher had been convinced of the need for fast, powerful ships with an all-big-gun armament. If Tsushima influenced his thinking, it was to persuade him of the need to standardise on guns. Fisher's concerns were submarines and destroyers equipped with torpedoes, then threatening to outrange battleship guns, making speed imperative for capital ships. Fisher's preferred option was his brainchild, the battlecruiser: lightly armored but heavily armed with eight 12-inch guns and propelled to by steam turbines.\n\nIt was to prove this revolutionary technology that \"Dreadnought\" was designed in January 1905, laid down in October 1905 and sped to completion by 1906. She carried ten 12-inch guns, had an 11-inch armor belt, and was the first large ship powered by turbines. She mounted her guns in five turrets; three on the centerline (one forward, two aft) and two on the wings, giving her at her launch twice the broadside of any other warship. She retained a number of 12-pound (3-inch, 76 mm) quick-firing guns for use against destroyers and torpedo-boats. Her armor was heavy enough for her to go head-to-head with any other ship in a gun battle, and conceivably win.\n\"Dreadnought\" was to have been followed by three s, their construction delayed to allow lessons from \"Dreadnought\" to be used in their design. While Fisher may have intended \"Dreadnought\" to be the last Royal Navy battleship, the design was so successful he found little support for his plan to switch to a battlecruiser navy. Although there were some problems with the ship (the wing turrets had limited arcs of fire and strained the hull when firing a full broadside, and the top of the thickest armor belt lay below the waterline at full load), the Royal Navy promptly commissioned another six ships to a similar design in the and es.\n\nAn American design, , authorized in 1905 and laid down in December 1906, was another of the first dreadnoughts, but she and her sister, , were not launched until 1908. Both used triple-expansion engines and had a superior layout of the main battery, dispensing with \"Dreadnought\"s wing turrets. They thus retained the same broadside, despite having two fewer guns.\n\nIn 1897, before the revolution in design brought about by HMS \"Dreadnought\", the Royal Navy had 62 battleships in commission or building, a lead of 26 over France and 50 over Germany. In 1906, the Royal Navy owned the field with \"Dreadnought\". The new class of ship prompted an arms race with major strategic consequences. Major naval powers raced to build their own dreadnoughts. Possession of modern battleships was not only vital to naval power, but also, as with nuclear weapons today, represented a nation's standing in the world. Germany, France, Japan, Italy, Austria, and the United States all began dreadnought programmes; while Ottoman Turkey, Argentina, Russia, Brazil, and Chile commissioned dreadnoughts to be built in British and American yards.\n\nThe battleship, particularly the dreadnought, was the dominant naval weapon of the World War I era. There were few serious challenges at that time. The most significant naval battles of World War I, such as Jutland (May 31, 1916 – June 1, 1916), were fought by battleships and their battlecruiser cousins.\nBy virtue of geography, the Royal Navy was able to use her imposing battleship and battlecruiser fleet to impose a strict and successful naval blockade of Germany and kept Germany's smaller battleship fleet bottled up in the North Sea: only narrow channels led to the Atlantic Ocean and these were guarded by British forces. Both sides were aware that, because of the greater number of British dreadnoughts, a full fleet engagement would be likely to result in a British victory. The German strategy was therefore to try to provoke an engagement on their terms: either to induce a part of the Grand Fleet to enter battle alone, or to fight a pitched battle near the German coastline, where friendly minefields, torpedo-boats and submarines could be used to even the odds. Germany's submarines were able to break out and raid commerce, but even though they sank many merchant ships, they could not successfully blockade Great Britain – in contrast to Britain's successful battleship blockade of Germany, which was a major cause of Germany's economic collapse in 1918. The Royal Navy on the other hand, successfully adopted convoy tactics to combat Germany's submarine blockade and eventually defeated it.\nThe first two years of war saw the Royal Navy's battleships and battlecruisers regularly \"sweep\" the North Sea making sure that no German ships could get in or out. Only a few German surface ships that were already at sea, such as the famous light cruiser , were able to raid commerce. Even some of those that did manage to get out were hunted down by battlecruisers, as in the Battle of the Falklands, December 7, 1914. The results of sweeping actions in the North Sea were battles such as the Heligoland Bight and Dogger Bank and German raids on the English coast, all of which were attempts by the Germans to lure out portions of the Grand Fleet in an attempt to defeat the Royal Navy in detail. On May 31, 1916, a further attempt to draw British ships into battle on German terms resulted in a clash of the battlefleets in the Battle of Jutland. The German fleet withdrew to port after two short encounters with the British fleet. Less than two months later, the Germans once again attempted to draw portions of the Grand Fleet into battle. The resulting Action of 19 August 1916 proved inconclusive. This reinforced German determination not to engage in a fleet to fleet battle.\nIn the other naval theatres there were no decisive pitched battles. In the Black Sea, engagement between Russian and Turkish battleships was restricted to skirmishes. In the Baltic Sea, action was largely limited to the raiding of convoys, and the laying of defensive minefields; the only significant clash of battleship squadrons there was the Battle of Moon Sound at which one Russian pre-dreadnought was lost. The Adriatic was in a sense the mirror of the North Sea: the Austro-Hungarian dreadnought fleet remained bottled up by the British and French blockade. And in the Mediterranean, the most important use of battleships was in support of the amphibious assault on Gallipoli.\n\nIn September 1914, the threat posed to surface ships by German U-boats was confirmed by successful attacks on British cruisers, including the sinking of three British armored cruisers by the German submarine in less than an hour. The British Super-dreadnought HMS \"Audacious\" soon followed suit as she struck a mine laid by a German U-boat in October 1914 and sank. The threat that German U-boats posed to British dreadnoughts was enough to cause the Royal Navy to change their strategy and tactics in the North Sea to reduce the risk of U-boat attack. Further near-misses from submarine attacks on battleships and casualties amongst cruisers led to growing concern in the Royal Navy about the vulnerability of battleships.\n\nAs the war wore on however, it turned out that whilst submarines did prove to be an incredibly dangerous threat to older pre-dreadnought battleships, as shown by examples such as the sinking of the , which was caught in the Dardanelles by a British submarine and the and were torpedoed by \"U-21\" as well as , , etc., the threat posed to dreadnought battleships proved to have been largely a false alarm. HMS \"Audacious\" turned out to have been the only dreadnought sunk by a submarine in World War I. While battleships were never intended for anti-submarine warfare, there was one instance of a submarine being sunk by a dreadnought battleship. HMS \"Dreadnought\" rammed and sank the German U-29 on March 18, 1915 off Moray Firth.\nWhilst the escape of the German fleet from the superior British firepower at Jutland was effected by the German cruisers and destroyers successfully turning away the British battleships, the German attempt to rely on U-boat attacks on the British fleet failed.\n\nTorpedo boats did have some successes against battleships in World War I, as demonstrated by the sinking of the British pre-dreadnought by during the Dardanelles Campaign and the destruction of the Austro-Hungarian dreadnought by Italian motor torpedo boats in June 1918. In large fleet actions, however, destroyers and torpedo boats were usually unable to get close enough to the battleships to damage them. The only battleship sunk in a fleet action by either torpedo boats or destroyers was the obsolescent German pre-dreadnought . She was sunk by destroyers during the night phase of the Battle of Jutland.\n\nThe German High Seas Fleet, for their part, were determined not to engage the British without the assistance of submarines; and since the submarines were needed more for raiding commercial traffic, the fleet stayed in port for much of the war.\n\nFor many years, Germany simply had no battleships. The Armistice with Germany required that most of the High Seas Fleet be disarmed and interned in a neutral port; largely because no neutral port could be found, the ships remained in British custody in Scapa Flow, Scotland. The Treaty of Versailles specified that the ships should be handed over to the British. Instead, most of them were scuttled by their German crews on June 21, 1919 just before the signature of the peace treaty. The treaty also limited the German Navy, and prevented Germany from building or possessing any capital ships.\nThe inter-war period saw the battleship subjected to strict international limitations to prevent a costly arms race breaking out.\nWhile the victors were not limited by the Treaty of Versailles, many of the major naval powers were crippled after the war. Faced with the prospect of a naval arms race against the United Kingdom and Japan, which would in turn have led to a possible Pacific war, the United States was keen to conclude the Washington Naval Treaty of 1922. This treaty limited the number and size of battleships that each major nation could possess, and required Britain to accept parity with the U.S. and to abandon the British alliance with Japan. The Washington treaty was followed by a series of other naval treaties, including the First Geneva Naval Conference (1927), the First London Naval Treaty (1930), the Second Geneva Naval Conference (1932), and finally the Second London Naval Treaty (1936), which all set limits on major warships. These treaties became effectively obsolete on September 1, 1939 at the beginning of World War II, but the ship classifications that had been agreed upon still apply. The treaty limitations meant that fewer new battleships were launched in 1919–39 than in 1905–14. The treaties also inhibited development by putting maximum limits on the weights of ships. Designs like the projected British , the first American , and the Japanese —all of which continued the trend to larger ships with bigger guns and thicker armor—never got off the drawing board. Those designs which were commissioned during this period were referred to as treaty battleships.\n\nAs early as 1914, the British Admiral Percy Scott predicted that battleships would soon be made irrelevant by aircraft. By the end of World War I, aircraft had successfully adopted the torpedo as a weapon. In 1921 the Italian general and air theorist Giulio Douhet completed a hugely influential treatise on strategic bombing titled \"The Command of the Air\", which foresaw the dominance of air power over naval units.\n\nIn the 1920s, General Billy Mitchell of the United States Army Air Corps, believing that air forces had rendered navies around the world obsolete, testified in front of Congress that \"1,000 bombardment airplanes can be built and operated for about the price of one battleship\" and that a squadron of these bombers could sink a battleship, making for more efficient use of government funds. This infuriated the U.S. Navy, but Mitchell was nevertheless allowed to conduct a careful series of bombing tests alongside Navy and Marine bombers. In 1921, he bombed and sank numerous ships, including the \"unsinkable\" German World War I battleship and the American pre-dreadnought .\n\nAlthough Mitchell had required \"war-time conditions\", the ships sunk were obsolete, stationary, defenseless and had no damage control. The sinking of \"Ostfriesland\" was accomplished by violating an agreement that would have allowed Navy engineers to examine the effects of various munitions: Mitchell's airmen disregarded the rules, and sank the ship within minutes in a coordinated attack. The stunt made headlines, and Mitchell declared, \"No surface vessels can exist wherever air forces acting from land bases are able to attack them.\" While far from conclusive, Mitchell's test was significant because it put proponents of the battleship against naval aviation on the back foot. Rear Admiral William A. Moffett used public relations against Mitchell to make headway toward expansion of the U.S. Navy's nascent aircraft carrier program.\n\nThe Royal Navy, United States Navy, and Imperial Japanese Navy extensively upgraded and modernized their World War I–era battleships during the 1930s. Among the new features were an increased tower height and stability for the optical rangefinder equipment (for gunnery control), more armor (especially around turrets) to protect against plunging fire and aerial bombing, and additional anti-aircraft weapons. Some British ships received a large block superstructure nicknamed the \"Queen Anne's castle\", such as in the and , which would be used in the new conning towers of the fast battleships. External bulges were added to improve both buoyancy to counteract weight increase and provide underwater protection against mines and torpedoes. The Japanese rebuilt all of their battleships, plus their battlecruisers, with distinctive \"pagoda\" structures, though the received a more modern bridge tower that would influence the new . Bulges were fitted, including steel tube arrays to improve both underwater and vertical protection along the waterline. The U.S. experimented with cage masts and later tripod masts, though after the Japanese attack on Pearl Harbor some of the most severely damaged ships (such as and ) were rebuilt with tower masts, for an appearance similar to their contemporaries. Radar, which was effective beyond visual range and effective in complete darkness or adverse weather, was introduced to supplement optical fire control.\n\nEven when war threatened again in the late 1930s, battleship construction did not regain the level of importance it had held in the years before World War I. The \"building holiday\" imposed by the naval treaties meant the capacity of dockyards worldwide had shrunk, and the strategic position had changed.\n\nIn Germany, the ambitious Plan Z for naval rearmament was abandoned in favor of a strategy of submarine warfare supplemented by the use of battlecruisers and commerce raiding (in particular by s). In Britain, the most pressing need was for air defenses and convoy escorts to safeguard the civilian population from bombing or starvation, and re-armament construction plans consisted of five ships of the . It was in the Mediterranean that navies remained most committed to battleship warfare. France intended to build six battleships of the and es, and the Italians four ships. Neither navy built significant aircraft carriers. The U.S. preferred to spend limited funds on aircraft carriers until the . Japan, also prioritising aircraft carriers, nevertheless began work on three mammoth \"Yamato\"s (although the third, , was later completed as a carrier) and a planned fourth was cancelled.\n\nAt the outbreak of the Spanish Civil War, the Spanish navy consisted of only two small dreadnought battleships, and . \"España\" (originally named \"Alfonso XIII\"), by then in reserve at the northwestern naval base of El Ferrol, fell into Nationalist hands in July 1936. The crew aboard \"Jaime I\" remained loyal to the Republic, killed their officers, who apparently supported Franco's attempted coup, and joined the Republican Navy. Thus each side had one battleship; however, the Republican Navy generally lacked experienced officers. The Spanish battleships mainly restricted themselves to mutual blockades, convoy escort duties, and shore bombardment, rarely in direct fighting against other surface units. In April 1937, \"España\" ran into a mine laid by friendly forces, and sank with little loss of life. In May 1937, \"Jaime I\" was damaged by Nationalist air attacks and a grounding incident. The ship was forced to go back to port to be repaired. There she was again hit by several aerial bombs. It was then decided to tow the battleship to a more secure port, but during the transport she suffered an internal explosion that caused 300 deaths and her total loss. Several Italian and German capital ships participated in the non-intervention blockade. On May 29, 1937, two Republican aircraft managed to bomb the German pocket battleship outside Ibiza, causing severe damage and loss of life. retaliated two days later by bombarding Almería, causing much destruction, and the resulting \"Deutschland\" incident meant the end of German and Italian support for non-intervention.\n\nThe —an obsolete pre-dreadnought—fired the first shots of World War II with the bombardment of the Polish garrison at Westerplatte; and the final surrender of the Japanese Empire took place aboard a United States Navy battleship, . Between those two events, it had become clear that aircraft carriers were the new principal ships of the fleet and that battleships now performed a secondary role.\n\nBattleships played a part in major engagements in Atlantic, Pacific and Mediterranean theaters; in the Atlantic, the Germans used their battleships as independent commerce raiders. However, clashes between battleships were of little strategic importance. The Battle of the Atlantic was fought between destroyers and submarines, and most of the decisive fleet clashes of the Pacific war were determined by aircraft carriers.\n\nIn the first year of the war, armored warships defied predictions that aircraft would dominate naval warfare. and surprised and sank the aircraft carrier off western Norway in June 1940. This engagement marked the last time a fleet carrier was sunk by surface gunnery. In the attack on Mers-el-Kébir, British battleships opened fire on the French battleships in the harbor near Oran in Algeria with their heavy guns, and later pursued fleeing French ships with planes from aircraft carriers.\n\nThe subsequent years of the war saw many demonstrations of the maturity of the aircraft carrier as a strategic naval weapon and its potential against battleships. The British air attack on the Italian naval base at Taranto sank one Italian battleship and damaged two more. The same Swordfish torpedo bombers played a crucial role in sinking the German commerce-raider .\nOn December 7, 1941, the Japanese launched a surprise attack on Pearl Harbor. Within a short time five of eight U.S. battleships were sunk or sinking, with the rest damaged. The American aircraft carriers were out to sea, however, and evaded detection. They took up the fight, and eventually turned the tide of the war in the Pacific. The sinking of the British battleship and her escort, the battlecruiser , demonstrated the vulnerability of a battleship to air attack while at sea without sufficient air cover, settling the argument begun by Mitchell in 1921. Both warships were under way and en route to attack the Japanese amphibious force that had invaded Malaya when they were caught by Japanese land-based bombers and torpedo bombers on December 10, 1941.\n\nAt many of the early crucial battles of the Pacific, for instance Coral Sea and Midway, battleships were either absent or overshadowed as carriers launched wave after wave of planes into the attack at a range of hundreds of miles. In later battles in the Pacific, battleships primarily performed shore bombardment in support of amphibious landings and provided anti-aircraft defense as escort for the carriers. Even the largest battleships ever constructed, Japan's , which carried a main battery of nine 18-inch (46 cm) guns and were designed as a principal strategic weapon, were never given a chance to show their potential in the decisive battleship action that figured in Japanese pre-war planning.\n\nThe last battleship confrontation in history was the Battle of Surigao Strait, on October 25, 1944, in which a numerically and technically superior American battleship group destroyed a lesser Japanese battleship group by gunfire after it had already been devastated by destroyer torpedo attacks. All but one of the American battleships in this confrontation had previously been sunk during the attack on Pearl Harbor and subsequently raised and repaired. When fired the last salvo of this battle, the last salvo fired by a battleship against another heavy ship, she was \"firing a funeral salute to a finished era of naval warfare.\" In April 1945, during the battle for Okinawa, the world's most powerful battleship, the \"Yamato\", was sent out on a suicide mission against a massive U.S. force and sunk by overwhelming pressure from carrier aircraft with nearly all hands lost.\n\nAfter World War II, several navies retained their existing battleships, but they were no longer strategically dominant military assets. Indeed, it soon became apparent that they were no longer worth the considerable cost of construction and maintenance and only one new battleship was commissioned after the war, . During the war it had been demonstrated that battleship-on-battleship engagements like Leyte Gulf or the sinking of were the exception and not the rule, and with the growing role of aircraft engagement ranges were becoming longer and longer, making heavy gun armament irrelevant. The armor of a battleship was equally irrelevant in the face of a nuclear attack as tactical missiles with a range of or more could be mounted on the Soviet and s. By the end of the 1950s, smaller vessel classes such as destroyers, which formerly offered no noteworthy opposition to battleships, now were capable of eliminating battleships from outside the range of the ship's heavy guns.\n\nThe remaining battleships met a variety of ends. and were sunk during the testing of nuclear weapons in Operation Crossroads in 1946. Both battleships proved resistant to nuclear air burst but vulnerable to underwater nuclear explosions. The was taken by the Soviets as reparations and renamed \"Novorossiysk\"; she was sunk by a leftover German mine in the Black Sea on October 29, 1955. The two ships were scrapped in 1956. The French was scrapped in 1954, in 1968, and in 1970.\n\nThe United Kingdom's four surviving ships were scrapped in 1957, and followed in 1960. All other surviving British battleships had been sold or broken up by 1949. The Soviet Union's was scrapped in 1953, in 1957 and (back under her original name, , since 1942) in 1956-7. Brazil's was scrapped in Genoa in 1953, and her sister ship sank during a storm in the Atlantic \"en route\" to the breakers in Italy in 1951.\n\nArgentina kept its two ships until 1956 and Chile kept (formerly ) until 1959. The Turkish battlecruiser (formerly , launched in 1911) was scrapped in 1976 after an offer to sell her back to Germany was refused. Sweden had several small coastal-defense battleships, one of which, , survived until 1970. The Soviets scrapped four large incomplete cruisers in the late 1950s, whilst plans to build a number of new s were abandoned following the death of Joseph Stalin in 1953. The three old German battleships , , and all met similar ends. \"Hessen\" was taken over by the Soviet Union and renamed \"Tsel\". She was scrapped in 1960. \"Schleswig-Holstein\" was renamed \"Borodino\", and was used as a target ship until 1960. \"Schlesien\", too, was used as a target ship. She was broken up between 1952 and 1957.\nThe s gained a new lease of life in the U.S. Navy as fire support ships. Radar and computer-controlled gunfire could be aimed with pinpoint accuracy to target. The U.S. recommissioned all four \"Iowa\"-class battleships for the Korean War and the for the Vietnam War. These were primarily used for shore bombardment, \"New Jersey\" firing nearly 6,000 rounds of 16 inch shells and over 14,000 rounds of 5 inch projectiles during her tour on the gunline, seven times more rounds against shore targets in Vietnam than she had fired in the Second World War.\n\nAs part of Navy Secretary John F. Lehman's effort to build a 600-ship Navy in the 1980s, and in response to the commissioning of \"Kirov\" by the Soviet Union, the United States recommissioned all four \"Iowa\"-class battleships. On several occasions, battleships were support ships in carrier battle groups, or led their own battleship battle group. These were modernized to carry Tomahawk missiles, with \"New Jersey\" seeing action bombarding Lebanon in 1983 and 1984, while and fired their 16-inch (406 mm) guns at land targets and launched missiles during Operation Desert Storm in 1991. \"Wisconsin\" served as the TLAM strike commander for the Persian Gulf, directing the sequence of launches that marked the opening of \"Desert Storm\", firing a total of 24 TLAMs during the first two days of the campaign. The primary threat to the battleships were Iraqi shore based surface-to-surface missiles; \"Missouri\" was targeted by two Iraqi Silkworm missiles, with one missing and another being intercepted by the British destroyer .\n\nAll four \"Iowa\" ships were decommissioned in the early 1990s, making them the last battleships to see active service. and were maintained to a standard where they could be rapidly returned to service as fire support vessels, pending the development of a superior fire support vessel. These last two battleships were finally stricken from the U.S. Naval Vessel Register in 2006. The Military Balance and Russian states the U.S. Navy listed one battleship in the reserve (Naval Inactive Fleet/Reserve 2nd Turn) in 2010. The Military Balance states the U.S. Navy listed no battleships in the reserve in 2014. The U.S. Marine Corps believes that the current naval surface fire support gun and missile programs will not be able to provide adequate fire support for an amphibious assault or onshore operations.\n\nWith the decommissioning of the last \"Iowa\"-class ships, no battleships remain in service or in reserve with any navy worldwide. A number are preserved as museum ships, either afloat or in drydock. The U.S. has eight battleships on display: , , , , , , and . \"Missouri\" and \"New Jersey\" are museums at Pearl Harbor and Camden, New Jersey, respectively. \"Iowa\" is on display as an educational attraction at the Los Angeles Waterfront in San Pedro, California. \"Wisconsin\" now serves as a museum ship in Norfolk, Virginia. \"Massachusetts\", which has the distinction of never having lost a man during service, is on display at the Battleship Cove naval museum in Fall River, Massachusetts. \"Texas\", the first battleship turned into a museum, is on display at the San Jacinto Battleground State Historic Site, near Houston. \"North Carolina\" is on display in Wilmington, North Carolina. \"Alabama\" is on display in Mobile, Alabama. The wreck of the , sunk during the Pearl Harbor attack in 1941, is designated a historical landmark and national gravesite.\n\nThe only other 20th-century battleship on display is the Japanese pre-dreadnought . A replica of the Chinese ironclad Dingyuan was built by the Weihai Port Bureau in 2003 and is on display in Weihai, China.\n\nBattleships were the embodiment of sea power. For Alfred Thayer Mahan and his followers, a strong navy was vital to the success of a nation, and control of the seas was vital for the projection of force on land and overseas. Mahan's theory, proposed in \"The Influence of Sea Power Upon History, 1660–1783\" of 1890, dictated the role of the battleship was to sweep the enemy from the seas. While the work of escorting, blockading, and raiding might be done by cruisers or smaller vessels, the presence of the battleship was a potential threat to any convoy escorted by any vessels other than capital ships. This concept of \"potential threat\" can be further generalized to the mere existence (as opposed to presence) of a powerful fleet tying the opposing fleet down. This concept came to be known as a \"fleet in being\" – an idle yet mighty fleet forcing others to spend time, resource and effort to actively guard against it.\n\nMahan went on to say victory could only be achieved by engagements between battleships, which came to be known as the \"decisive battle\" doctrine in some navies, while targeting merchant ships (commerce raiding or \"guerre de course\", as posited by the \"Jeune École\") could never succeed.\n\nMahan was highly influential in naval and political circles throughout the age of the battleship, calling for a large fleet of the most powerful battleships possible. Mahan's work developed in the late 1880s, and by the end of the 1890s it had a massive international influence, in the end adopted by many major navies (notably the British, American, German, and Japanese). The strength of Mahanian opinion was important in the development of the battleships arms races, and equally important in the agreement of the Powers to limit battleship numbers in the interwar era.\n\nThe \"fleet in being\" suggested battleships could simply by their existence tie down superior enemy resources. This in turn was believed to be able to tip the balance of a conflict even without a battle. This suggested even for inferior naval powers a battleship fleet could have important strategic effect.\n\nWhile the role of battleships in both World Wars reflected Mahanian doctrine, the details of battleship deployment were more complex. Unlike ships of the line, the battleships of the late 19th and early 20th centuries had significant vulnerability to torpedoes and mines —because efficient mines & torpedoes didn't exist before that— which could be used by relatively small and inexpensive craft. The \"Jeune École\" doctrine of the 1870s and 1880s recommended placing torpedo boats alongside battleships; these would hide behind the larger ships until gun-smoke obscured visibility enough for them to dart out and fire their torpedoes. While this tactic was vitiated by the development of smokeless propellant, the threat from more capable torpedo craft (later including submarines) remained. By the 1890s, the Royal Navy had developed the first destroyers, which were initially designed to intercept and drive off any attacking torpedo boats. During the First World War and subsequently, battleships were rarely deployed without a protective screen of destroyers.\n\nBattleship doctrine emphasised the concentration of the battlegroup. In order for this concentrated force to be able to bring its power to bear on a reluctant opponent (or to avoid an encounter with a stronger enemy fleet), battlefleets needed some means of locating enemy ships beyond horizon range. This was provided by scouting forces; at various stages battlecruisers, cruisers, destroyers, airships, submarines and aircraft were all used. (With the development of radio, direction finding and traffic analysis would come into play, as well, so even shore stations, broadly speaking, joined the battlegroup.) So for most of their history, battleships operated surrounded by squadrons of destroyers and cruisers. The North Sea campaign of the First World War illustrates how, despite this support, the threat of mine and torpedo attack, and the failure to integrate or appreciate the capabilities of new techniques, seriously inhibited the operations of the Royal Navy Grand Fleet, the greatest battleship fleet of its time.\n\nThe presence of battleships had a great psychological and diplomatic impact. Similar to possessing nuclear weapons today, the ownership of battleships served to enhance a nation's force projection.\n\nEven during the Cold War, the psychological impact of a battleship was significant. In 1946, USS \"Missouri\" was dispatched to deliver the remains of the ambassador from Turkey, and her presence in Turkish and Greek waters staved off a possible Soviet thrust into the Balkan region. In September 1983, when Druze militia in Lebanon's Shouf Mountains fired upon U.S. Marine peacekeepers, the arrival of USS \"New Jersey\" stopped the firing. Gunfire from \"New Jersey\" later killed militia leaders.\n\nBattleships were the largest and most complex, and hence the most expensive warships of their time; as a result, the value of investment in battleships has always been contested. As the French politician Etienne Lamy wrote in 1879, \"The construction of battleships is so costly, their effectiveness so uncertain and of such short duration, that the enterprise of creating an armored fleet seems to leave fruitless the perseverance of a people\". The \"Jeune École\" school of thought of the 1870s and 1880s sought alternatives to the crippling expense and debatable utility of a conventional battlefleet. It proposed what would nowadays be termed a sea denial strategy, based on fast, long-ranged cruisers for commerce raiding and torpedo boat flotillas to attack enemy ships attempting to blockade French ports. The ideas of the \"Jeune École\" were ahead of their time; it was not until the 20th century that efficient mines, torpedoes, submarines, and aircraft were available that allowed similar ideas to be effectively implemented. The determination of powers such as Germany to build battlefleets with which to confront much stronger rivals has been criticised by historians, who emphasise the futility of investment in a battlefleet that has no chance of matching its opponent in an actual battle.\n\n\n\n\n"}
{"id": "4055", "url": "https://en.wikipedia.org/wiki?curid=4055", "title": "Bifröst", "text": "Bifröst\n\nIn Norse mythology, Bifröst ( or sometimes Bilröst or Bivrost) is a burning rainbow bridge that reaches between Midgard (Earth) and Asgard, the realm of the gods. The bridge is attested as \"Bilröst\" in the \"Poetic Edda\"; compiled in the 13th century from earlier traditional sources, and as \"Bifröst\" in the \"Prose Edda\"; written in the 13th century by Snorri Sturluson, and in the poetry of skalds. Both the \"Poetic Edda\" and the \"Prose Edda\" alternately refer to the bridge as Ásbrú (Old Norse \"Æsir's bridge\").\n\nAccording to the \"Prose Edda\", the bridge ends in heaven at Himinbjörg, the residence of the god Heimdallr, who guards it from the jötnar. The bridge's destruction during Ragnarök by the forces of Muspell is foretold. Scholars have proposed that the bridge may have originally represented the Milky Way and have noted parallels between the bridge and another bridge in Norse mythology, Gjallarbrú.\n\nScholar Andy Orchard posits that \"Bifröst\" may mean \"shimmering path.\" He notes that the first element of \"Bilröst\"—\"bil\" (meaning \"a moment\")—\"suggests the fleeting nature of the rainbow,\" which he connects to the first element of \"Bifröst\"—the Old Norse verb \"bifa\" (meaning \"to shimmer\" or \"to shake\")—noting that the element evokes notions of the \"lustrous sheen\" of the bridge. Austrian Germanist Rudolf Simek says that \"Bifröst\" either means \"the swaying road to heaven\" (also citing \"bifa\") or, if \"Bilröst\" is the original form of the two (which Simek says is likely), \"the fleetingly glimpsed rainbow\" (possibly connected to \"bil\", perhaps meaning \"moment, weak point\").\n\nTwo poems in the \"Poetic Edda\" and two books in the \"Prose Edda\" provide information about the bridge:\n\nIn the \"Poetic Edda\", the bridge is mentioned in the poems \"Grímnismál\" and \"Fáfnismál\", where it is referred to as \"Bilröst\". In one of two stanzas in the poem \"Grímnismál\" that mentions the bridge, Grímnir (the god Odin in disguise) provides the young Agnarr with cosmological knowledge, including that Bilröst is the best of bridges. Later in \"Grímnismál\", Grímnir notes that Asbrú \"burns all with flames\" and that, every day, the god Thor wades through the waters of Körmt and Örmt and the two Kerlaugar:\n\nIn \"Fáfnismál\", the dying wyrm Fafnir tells the hero Sigurd that, during the events of Ragnarok, bearing spears, gods will meet at Óskópnir. From there, the gods will cross Bilröst, which will break apart as they cross over it, causing their horses to dredge through an immense river.\n\nThe bridge is mentioned in the \"Prose Edda\" books \"Gylfaginning\" and \"Skáldskaparmál\", where it is referred to as \"Bifröst\". In chapter 13 of \"Gylfaginning\", Gangleri (King Gylfi in disguise) asks the enthroned figure of High what way exists between heaven and earth. Laughing, High replies that the question isn't an intelligent one, and goes on to explain that the gods built a bridge from heaven and earth. He incredulously asks Gangleri if he has not heard the story before. High says that Gangleri must have seen it, and notes that Gangleri may call it a rainbow. High says that the bridge consists of three colors, has great strength, \"and is built with art and skill to a greater extent than other constructions.\"\n\nHigh notes that, although the bridge is strong, it will break when \"Muspell's lads\" attempt to cross it, and their horses will have to make do with swimming over \"great rivers.\" Gangleri says that it doesn't seem that the gods \"built the bridge in good faith if it is liable to break, considering that they can do as they please.\" High responds that the gods do not deserve blame for the breaking of the bridge, for \"there is nothing in this world that will be secure when Muspell's sons attack.\"\n\nIn chapter 15 of \"Gylfaginning\", Just-As-High says that Bifröst is also called \"Asbrú\", and that every day the gods ride their horses across it (with the exception of Thor, who instead wades through the boiling waters of the rivers Körmt and Örmt) to reach Urðarbrunnr, a holy well where the gods have their court. As a reference, Just-As-High quotes the second of the two stanzas in \"Grímnismál\" that mention the bridge (see above). Gangleri asks if fire burns over Bifröst. High says that the red in the bridge is burning fire, and, without it, the frost jotnar and mountain jotnar would \"go up into heaven\" if anyone who wanted could cross Bifröst. High adds that, in heaven, \"there are many beautiful places\" and that \"everywhere there has divine protection around it.\"\n\nIn chapter 17, High tells Gangleri that the location of Himinbjörg \"stands at the edge of heaven where Bifrost reaches heaven.\" While describing the god Heimdallr in chapter 27, High says that Heimdallr lives in Himinbjörg by Bifröst, and guards the bridge from mountain jotnar while sitting at the edge of heaven. In chapter 34, High quotes the first of the two \"Grímnismál\" stanzas that mention the bridge. In chapter 51, High foretells the events of Ragnarök. High says that, during Ragnarök, the sky will split open, and from the split will ride forth the \"sons of Muspell\". When the \"sons of Muspell\" ride over Bifröst it will break, \"as was said above.\"\n\nIn the \"Prose Edda\" book \"Skáldskaparmál\", the bridge receives a single mention. In chapter 16, a work by the 10th century skald Úlfr Uggason is provided, where Bifröst is referred to as \"the powers' way.\"\n\nIn his translation of the \"Prose Edda\", Henry Adams Bellows comments that the \"Grímnismál\" stanza mentioning Thor and the bridge stanza may mean that \"Thor has to go on foot in the last days of the destruction, when the bridge is burning. Another interpretation, however, is that when Thor leaves the heavens (i.e., when a thunder-storm is over) the rainbow-bridge becomes hot in the sun.\"\n\nJohn Lindow points to a parallel between Bifröst, which he notes is \"a bridge between earth and heaven, or earth and the world of the gods\", and the bridge Gjallarbrú, \"a bridge between earth and the underworld, or earth and the world of the dead.\" Several scholars have proposed that Bifröst may represent the Milky Way.\n\n"}
{"id": "4057", "url": "https://en.wikipedia.org/wiki?curid=4057", "title": "Battlecruiser", "text": "Battlecruiser\n\nThe battlecruiser, or battle cruiser, was a type of capital ship of the first half of the 20th century. They were similar in size, cost, and armament to battleships, but they generally carried less armour in order to obtain faster speeds. The first battlecruisers were designed in the United Kingdom in the first decade of the century, as a development of the armoured cruiser, at the same time as the dreadnought succeeded the pre-dreadnought battleship. The goal of the design was to outrun any ship with similar armament, and chase down any ship with lesser armament; they were intended to hunt down slower, older armoured cruisers and destroy them with heavy gunfire while avoiding combat with the more powerful but slower battleships. However, as more and more battlecruisers were built, they were increasingly used alongside the better-protected battleships.\n\nBattlecruisers served in the navies of the UK, Germany, the Ottoman Empire, Australia and Japan during World War I, most notably at the Battle of the Falkland Islands and in the several raids and skirmishes in the North Sea which culminated in a pitched fleet battle, the Battle of Jutland. British battlecruisers in particular suffered heavy losses at Jutland, where their light armour made them very vulnerable to large-caliber shells. By the end of the war, capital ship design had developed with battleships becoming faster and battlecruisers becoming more heavily armoured, blurring the distinction between a battlecruiser and a fast battleship. The Washington Naval Treaty, which limited capital ship construction from 1922 onwards, treated battleships and battlecruisers identically, and the new generation of battlecruisers planned was scrapped under the terms of the treaty.\n\nImprovements in armor design and propulsion created the 1930s \"fast battleship\" with the speed of a battlecruiser and armor of a battleship, making the battlecruiser in the traditional sense effectively an obsolete concept. Thus from the 1930s on, only the Royal Navy continued to use \"battlecruiser\" as a classification for the World War I–era capital ships that remained in the fleet; while Japan's battlecruisers remained in service, they had been significantly reconstructed and were re-rated as full-fledged fast battleships.\n\nBattlecruisers were put into action again during World War II, and only one survived to the end. There was also renewed interest in large \"cruiser-killer\" type warships, but few were ever begun, as construction of battleships and battlecruisers were curtailed in favor of more-needed convoy escorts, aircraft carriers, and cargo ships. In the post–Cold War era, the Soviet of large guided missile cruisers have also been termed \"battlecruisers\".\n\nThe battlecruiser was developed by the Royal Navy in the first years of the 20th century as an evolution of the armoured cruiser.\n\nThe first armoured cruisers had been built in the 1870s, as an attempt to give armour protection to ships fulfilling the typical cruiser roles of patrol, trade protection and power projection. However, the results were rarely satisfactory, as the weight of armour required for any meaningful protection usually meant that the ship became almost as slow as a battleship. As a result, navies preferred to build protected cruisers with an armoured deck protecting their engines, or simply no armour at all.\n\nIn the 1890s, technology began to change this balance. New Krupp steel armour meant that it was now possible to give a cruiser side armour which would protect it against the quick-firing guns of enemy battleships and cruisers alike. In 1896–97 France and Russia, who were regarded as likely allies in the event of war, started to build large, fast armoured cruisers taking advantage of this. In the event of a war between Britain and France or Russia, or both, these cruisers threatened to cause serious difficulties for the British Empire's worldwide trade.\n\nBritain, which had concluded in 1892 that it needed twice as many cruisers as any potential enemy to adequately protect its empire's sea lanes, responded to the perceived threat by laying down its own large armoured cruisers. Between 1899 and 1905, it completed or laid down seven classes of this type, a total of 35 ships. This building program, in turn, prompted the French and Russians to increase their own construction. The Imperial German Navy began to build large armoured cruisers for use on their overseas stations, laying down eight between 1897 and 1906.\n\nThe cost of this cruiser arms race was significant. In the period 1889–96, the Royal Navy spent £7.3 million on new large cruisers. From 1897–1904, it spent £26.9 million. Many armoured cruisers of the new kind were just as large and expensive as the equivalent battleship.\n\nThe increasing size and power of the armoured cruiser led to suggestions in British naval circles that cruisers should displace battleships entirely. The battleship's main advantage was its 12-inch heavy guns, and heavier armour designed to protect from shells of similar size. However, for a few years after 1900 it seemed that those advantages were of little practical value. The torpedo now had a range of 2,000 yards, and it seemed unlikely that a battleship would engage within torpedo range. However, at ranges of more than 2,000 yards it became increasingly unlikely that the heavy guns of a battleship would score any hits, as the heavy guns relied on primitive aiming techniques. The secondary batteries of 6-inch quick-firing guns, firing more plentiful shells, were more likely to hit the enemy. As naval expert Fred T. Jane wrote in June 1902,Is there anything outside of 2,000 yards that the big gun in its hundreds of tons of medieval castle can effect, that its weight in 6-inch guns without the castle could not effect equally well? And inside 2,000, what, in these days of gyros, is there that the torpedo cannot effect with far more certainty?\n\nIn 1904, Admiral John \"Jacky\" Fisher became First Sea Lord, the senior officer of the Royal Navy. He had for some time thought about the development of a new fast armoured ship. He was very fond of the \"second-class battleship\" , a faster, more lightly armoured battleship. As early as 1901, there is confusion in Fisher's writing about whether he saw the battleship or the cruiser as the model for future developments. This did not stop him from commissioning designs from naval architect W. H. Gard for an armoured cruiser with the heaviest possible armament for use with the fleet. The design Gard submitted was for a ship between , capable of , armed with four 9.2-inch and twelve guns in twin gun turrets and protected with six inches of armour along her belt and 9.2-inch turrets, on her 7.5-inch turrets, 10 inches on her conning tower and up to on her decks. However, mainstream British naval thinking between 1902 and 1904 was clearly in favour of heavily armoured battleships, rather than the fast ships that Fisher favoured.\n\nThe Battle of Tsushima proved conclusively the effectiveness of heavy guns over intermediate ones and the need for a uniform main caliber on a ship for fire control. Even before this, the Royal Navy had begun to consider a shift away from the mixed-calibre armament of the 1890s pre-dreadnought to an \"all-big-gun\" design, and preliminary designs circulated for battleships with all 12-inch or all 10-inch guns and armoured cruisers with all 9.2-inch guns. In late 1904, not long after the Royal Navy had decided to use 12-inch guns for its next generation of battleships because of their superior performance at long range, Fisher began to argue that big-gun cruisers could replace battleships altogether. The continuing improvement of the torpedo meant that submarines and destroyers would be able to destroy battleships; this in Fisher's view heralded the end of the battleship or at least compromised the validity of heavy armour protection. Nevertheless, armoured cruisers would remain vital for commerce protection.\n\nFisher's views were very controversial within the Royal Navy, and even given his position as First Sea Lord, he was not in a position to insist on his own approach. Thus he assembled a \"Committee on Designs\", consisting of a mixture of civilian and naval experts, to determine the approach to both battleship and armoured cruiser construction in the future. While the stated purpose of the Committee was to investigate and report on future requirements of ships, Fisher and his associates had already made key decisions. The terms of reference for the Committee were for a battleship capable of with 12-inch guns and no intermediate calibres, capable of docking in existing drydocks; and a cruiser capable of , also with 12-inch guns and no intermediate armament, armoured like , the most recent armoured cruiser, and also capable of using existing docks.\n\nUnder the Selborne plan of 1902, the Royal Navy intended to start three new battleships and four armoured cruisers each year. However, in late 1904 it became clear that the 1905–06 programme would have to be considerably smaller, because of lower than expected tax revenue and the need to buy out two Chilean battleships under construction in British yards, lest they be purchased by the Russians for use against the Japanese, Britain's ally. These economies meant that the 1905–06 programme consisted only of one battleship, but three armoured cruisers. The battleship became the revolutionary battleship , and the cruisers became the three ships of the . Fisher later claimed, however, that he had argued during the Committee for the cancellation of the remaining battleship.\n\nThe construction of the new class were begun in 1906 and completed in 1908, delayed perhaps to allow their designers to learn from any problems with \"Dreadnought\". The ships fulfilled the design requirement quite closely. On a displacement similar to \"Dreadnought\", the \"Invincible\"s were longer to accommodate additional boilers and more powerful turbines to propel them at . Moreover, the new ships could maintain this speed for days, whereas pre-dreadnought battleships could not generally do so for more than an hour. Armed with eight 12-inch Mk X guns, compared to ten on \"Dreadnought\", they had of armour protecting the hull and the gun turrets. (\"Dreadnought\"s armour, by comparison, was at its thickest.) The class had a very marked increase in speed, displacement and firepower compared to the most recent armoured cruisers but no more armour.\n\nWhile the \"Invincible\"s were to fill the same role as the armoured cruisers they succeeded, they were expected to do so more effectively. Specifically their roles were:\nConfusion about how to refer to these new battleship-size armoured cruisers set in almost immediately. Even in late 1905, before work was begun on the \"Invincible\"s, a Royal Navy memorandum refers to \"large armoured ships\" meaning both battleships and large cruisers. In October 1906, the Admiralty began to classify all post-Dreadnought battleships and armoured cruisers as \"capital ships\", while Fisher used the term \"dreadnought\" to refer either to his new battleships or the battleships and armoured cruisers together. At the same time, the \"Invincible\" class themselves were referred to as \"cruiser-battleships\", \"dreadnought cruisers\"; the term \"battlecruiser\" was first used by Fisher in 1908. Finally, on 24 November 1911, Admiralty Weekly Order No. 351 laid down that \"All cruisers of the “Invincible” and later types are for the future to be described and classified as “battle cruisers” to distinguish them from the armoured cruisers of earlier date.\"\n\nAlong with questions over the new ships' nomenclature came uncertainty about their actual role due to their lack of protection. If they were primarily to act as scouts for the battle fleet and hunter-killers of enemy cruisers and commerce raiders, then the seven inches of belt armour with which they had been equipped would be adequate. If, on the other hand, they were expected to reinforce a battle line of dreadnoughts with their own heavy guns, they were too thin-skinned to be safe from an enemy's heavy guns. The \"Invincible\"s were essentially extremely large, heavily armed, fast armoured cruisers. However, the viability of the armoured cruiser was already in doubt. A cruiser that could have worked with the Fleet might have been a more viable option for taking over that role.\n\nBecause of the \"Invincible\"s size and armament, naval authorities considered them capital ships almost from their inception—an assumption that might have been inevitable. Complicating matters further was that many naval authorities, including Lord Fisher, had made overoptimistic assessments from the Battle of Tsushima in 1905 about the armoured cruiser's ability to survive in a battle line against enemy capital ships due to their superior speed. These assumptions had been made without taking into account the Russian Baltic Fleet's inefficiency and tactical ineptitude. By the time the term \"battlecruiser\" had been given to the \"Invincible\"s, the idea of their parity with battleships had been fixed in many people's minds.\n\nNot everyone was so convinced. \"Brasseys Naval Annual\", for instance, stated that with vessels as large and expensive as the \"Invincible\"s, an admiral \"will be certain to put them in the line of battle where their comparatively light protection will be a disadvantage and their high speed of no value.\" Those in favor of the battlecruiser countered with two points—first, since all capital ships were vulnerable to new weapons such as the torpedo, armour had lost some of its validity; and second, because of its greater speed, the battlecruiser could control the range at which it engaged an enemy.\n\nBetween the launching of the \"Invincible\"s to just after the outbreak of the First World War, the battlecruiser played a junior role in the developing dreadnought arms race, as it was never wholeheartedly adopted as the key weapon in British imperial defence, as Fisher had presumably desired. The biggest factor for this lack of acceptance was the marked change in Britain's strategic circumstances between their conception and the commissioning of the first ships. The prospective enemy for Britain had shifted from a Franco-Russian alliance with many armoured cruisers to a resurgent and increasingly belligerent Germany. Diplomatically, Britain had entered the Entente cordiale in 1904 and the Anglo-Russian Entente. Neither France nor Russia posed a particular naval threat; the Russian navy had largely been sunk or captured in the Russo-Japanese War of 1904–5, while the French were in no hurry to adopt the new dreadnought-type design. Britain also boasted very cordial relations with two of the significant new naval powers, Japan (bolstered by the Anglo-Japanese Alliance, signed in 1902 and renewed in 1905), and the USA. These changed strategic circumstances, and the great success of the \"Dreadnought\", ensured that she rather than the \"Invincible\" became the new model capital ship. Nevertheless, battlecruiser construction played a part in the renewed naval arms-race sparked by the \"Dreadnought\".\nFor their first few years of service, the \"Invincible\"s entirely fulfilled Fisher's vision of being able to sink any ship fast enough to catch them, and run from any ship capable of sinking them. An \"Invincible\" would also, in many circumstances, be able to take on an enemy pre-dreadnought battleship. Naval circles concurred that the armoured cruiser in its current form had come to the logical end of its development and the \"Invincible\"s were so far ahead of any enemy armoured cruiser in firepower and speed that it proved difficult to justify building more or bigger cruisers. This lead was extended by the surprise both \"Dreadnought\" and \"Invincible\" produced by having been built in secret; this prompted most other navies to delay their building programmes and radically revise their designs. This was particularly true for cruisers, because the details of the \"Invincible\" class were kept secret for longer; this meant that the last German armoured cruiser, , was armed with only guns, and was no match for the new battlecruisers.\n\nThe Royal Navy's early superiority in capital ships led to the rejection of a 1905–06 design that would, essentially, have fused the battlecruiser and battleship concepts into what would eventually become the fast battleship. The 'X4' design combined the full armour and armament of \"Dreadnought\" with the 25 knot speed of \"Invincible\". The additional cost could not be justified given the existing British lead and the new Liberal government's need for economy; the slower and cheaper , a relatively close copy of \"Dreadnought\", was adopted instead. The X4 concept would eventually be fulfilled in the and later by other navies.\n\nThe next British battlecruisers were the three , slightly improved \"Invincible\"s built to fundamentally the same specification, partly due to political pressure to limit costs and partly due to the secrecy surrounding German battlecruiser construction, particularly about the heavy armour of . This class came to be widely seen as a mistake and the next generation of British battlecruisers were markedly more powerful. By 1909–10 a sense of national crisis about rivalry with Germany outweighed cost-cutting, and a naval panic resulted in the approval of a total of eight capital ships in 1909–10. Fisher pressed for all eight to be battlecruisers, but was unable to have his way; he had to settle for six battleships and two battlecruisers of the . The \"Lion\"s carried eight 13.5-inch guns, the now-standard caliber of the British \"super-dreadnought\" battleships. Speed increased to and armour protection, while not as good as in German designs, was better than in previous British battlecruisers, with armour belt and barbettes. The two \"Lion\"s were followed by the very similar .\nBy 1911 Germany had built battlecruisers of her own, and the superiority of the British ships could no longer be assured. Moreover, the German Navy did not share Fisher's view of the battlecruiser. In contrast to the British focus on increasing speed and firepower, Germany progressively improved the armour and staying power of their ships to better the British battlecruisers. \"Von der Tann\", begun in 1908 and completed in 1910, carried eight 11.1-inch guns, but with 11.1-inch (283 mm) armour she was far better protected than the \"Invincible\"s. The two s were quite similar but carried ten 11.1-inch guns of an improved design. , designed in 1909 and finished in 1913, was a modified \"Moltke\"; speed increased by one knot to , while her armour had a maximum thickness of 12 inches, equivalent to the s of a few years earlier. \"Seydlitz\" was Germany's last battlecruiser completed before World War I.\n\nThe next step in battlecruiser design came from Japan. The Imperial Japanese Navy had been planning the ships from 1909, and was determined that, since the Japanese economy could support relatively few ships, each would be more powerful than its likely competitors. Initially the class was planned with the \"Invincible\"s as the benchmark. On learning of the British plans for \"Lion\", and the likelihood that new U.S. Navy battleships would be armed with guns, the Japanese decided to radically revise their plans and go one better. A new plan was drawn up, carrying eight 14-inch guns, and capable of , thus marginally having the edge over the \"Lion\"s in speed and firepower. The heavy guns were also better-positioned, being superfiring both fore and aft with no turret amidships. The armour scheme was also marginally improved over the \"Lion\"s, with nine inches of armour on the turrets and on the barbettes. The first ship in the class was built in Britain, and a further three constructed in Japan. The Japanese also re-classified their powerful armoured cruisers of the \"Tsukuba\" and \"Ibuki\" classes, carrying four 12-inch guns, as battlecruisers; nonetheless, their armament was weaker and they were slower than any battlecruiser.\nThe next British battlecruiser, , was intended initially as the fourth ship in the \"Lion\" class, but was substantially redesigned. She retained the eight 13.5-inch guns of her predecessors, but they were positioned like those of \"Kongō\" for better fields of fire. She was faster (making on sea trials), and carried a heavier secondary armament. \"Tiger\" was also more heavily armoured on the whole; while the maximum thickness of armour was the same at nine inches, the height of the main armour belt was increased. Not all the desired improvements for this ship were approved, however. Her designer, Sir Eustace Tennyson d'Eyncourt, had wanted small-bore water-tube boilers and geared turbines to give her a speed of , but he received no support from the authorities and the engine makers refused his request.\n\n1912 saw work begin on three more German battlecruisers of the , the first German battlecruisers to mount 12-inch guns. These ships, like \"Tiger\" and the \"Kongō\"s, had their guns arranged in superfiring turrets for greater efficiency. Their armour and speed was similar to the previous \"Seydlitz\" class. In 1913, the Russian Empire also began the construction of the four-ship , which were designed for service in the Baltic Sea. These ships were designed to carry twelve 14-inch guns, with armour up to 12 inches thick, and a speed of . The heavy armour and relatively slow speed of these ships made them more similar to German designs than to British ships; construction of the \"Borodino\"s was halted by the First World War and all were scrapped after the end of the Russian Civil War.\n\nFor most of the combatants, capital ship construction was very limited during the war. Germany finished the \"Derfflinger\" class and began work on the . The \"Mackensen\"s were a development of the \"Derfflinger\" class, with 13.8-inch guns and a broadly similar armour scheme, designed for .\n\nIn Britain, Jackie Fisher returned to the office of First Sea Lord in October 1914. His enthusiasm for big, fast ships was unabated, and he set designers to producing a design for a battlecruiser with 15-inch guns. Because Fisher expected the next German battlecruiser to steam at 28 knots, he required the new British design to be capable of 32 knots. He planned to reorder two s, which had been approved but not yet laid down, to a new design. Fisher finally received approval for this project on 28 December 1914 and they became the . With six 15-inch guns but only 6-inch armour they were a further step forward from \"Tiger\" in firepower and speed, but returned to the level of protection of the first British battlecruisers.\n\nAt the same time, Fisher resorted to subterfuge to obtain another three fast, lightly armoured ships that could use several spare gun turrets left over from battleship construction. These ships were essentially light battlecruisers, and Fisher occasionally referred to them as such, but officially they were classified as \"large light cruisers\". This unusual designation was required because construction of new capital ships had been placed on hold, while there were no limits on light cruiser construction. They became and her sisters and , and there was a bizarre imbalance between their main guns of 15 inches (or in \"Furious\") and their armour, which at thickness was on the scale of a light cruiser. The design was generally regarded as a failure (nicknamed in the Fleet \"Outrageous\", \"Uproarious\" and \"Spurious\"), though the later conversion of the ships to aircraft carriers was very successful. Fisher also speculated about a new mammoth, but lightly built battlecruiser, that would carry guns, which he termed ; this never got beyond the concept stage.\n\nIt is often held that the \"Renown\" and \"Courageous\" classes were designed for Fisher's plan to land troops (possibly Russian) on the German Baltic coast. Specifically, they were designed with a reduced draught, which might be important in the shallow Baltic. This is not clear-cut evidence that the ships were designed for the Baltic: it was considered that earlier ships had too much draught and not enough freeboard under operational conditions. Roberts argues that the focus on the Baltic was probably unimportant at the time the ships were designed, but was inflated later, after the disastrous Dardanelles Campaign.\n\nThe final British battlecruiser design of the war was the , which was born from a requirement for an improved version of the \"Queen Elizabeth\" battleship. The project began at the end of 1915, after Fisher's final departure from the Admiralty. While initially envisaged as a battleship, senior sea officers felt that Britain had enough battleships, but that new battlecruisers might be required to combat German ships being built (the British overestimated German progress on the \"Mackensen\" class as well as their likely capabilities). A battlecruiser design with eight 15-inch guns, 8 inches of armour and capable of 32 knots was decided on. The experience of battlecruisers at the Battle of Jutland meant that the design was radically revised and transformed again into a fast battleship with armour up to 12 inches thick, but still capable of . The first ship in the class, , was built according to this design to counter the possible completion of any of the Mackensen-class ship. The plans for her three sisters, on which little work had been done, were revised once more later in 1916 and in 1917 to improve protection.\n\nThe Admiral class would have been the only British ships capable of taking on the German \"Mackensen\" class; nevertheless, German shipbuilding was drastically slowed by the war, and while two \"Mackensen\"s were launched, none were ever completed. The Germans also worked briefly on a further three ships, of the , which were modified versions of the \"Mackensen\"s with 15-inch guns. Work on the three additional Admirals was suspended in March 1917 to enable more escorts and merchant ships to be built to deal with the new threat from U-boats to trade. They were finally cancelled in February 1919.\n\nThe first combat involving battlecruisers during World War I was the Battle of Heligoland Bight in August 1914. A force of British light cruisers and destroyers entered the Heligoland Bight (the part of the North Sea closest to Hamburg) to attack German destroyer patrols. When they met opposition from light cruisers, Vice Admiral David Beatty took his squadron of five battlecruisers into the Bight and turned the tide of the battle, ultimately sinking three German light cruisers and killing their commander, Rear Admiral Leberecht Maass.\n\nThe German battlecruiser perhaps made the most impact early in the war. Stationed in the Mediterranean, she and the escorting light cruiser evaded British and French ships on the outbreak of war, and steamed to Constantinople (Istanbul) with two British battlecruisers in hot pursuit. The two German ships were handed over to the Ottoman Navy, and this was instrumental in bringing the Ottoman Empire into the war as one of the Central Powers. \"Goeben\" herself, renamed \"Yavuz Sultan Selim\", fought engagements against the Imperial Russian Navy in the Black Sea and against the British in the Aegean Sea.\n\nThe original battlecruiser concept proved successful in December 1914 at the Battle of the Falkland Islands. The British battlecruisers and did precisely the job for which they were intended when they chased down and annihilated the German East Asia Squadron, centered on the armoured cruisers and , along with three light cruisers, commanded by Admiral Maximilian Graf Von Spee, in the South Atlantic Ocean. Prior to the battle, the Australian battlecruiser had unsuccessfully searched for the German ships in the Pacific.\nDuring the Battle of Dogger Bank in 1915, the aftermost barbette of the German flagship \"Seydlitz\" was struck by a British 13.5-inch shell from \"HMS Lion\". The shell did not penetrate the barbette, but it dislodged a piece of the barbette armour that allowed the flame from the shell's detonation to enter the barbette. The propellant charges being hoisted upwards were ignited, and the fireball flashed up into the turret and down into the magazine, setting fire to charges removed from their brass cartridge cases. The gun crew tried to escape into the next turret, which allowed the flash to spread into that turret as well, killing the crews of both turrets. \"Seydlitz\" was saved from near-certain destruction only by emergency flooding of her after magazines, which had been effected by Wilhelm Heidkamp. This near-disaster was due to the way that ammunition handling was arranged and was common to both German and British battleships and battlecruisers, but the lighter protection on the latter made them more vulnerable to the turret or barbette being penetrated. The Germans learned from investigating the damaged \"Seydlitz\" and instituted measures to ensure that ammunition handling minimised any possible exposure to flash.\n\nApart from the cordite handling, the battle was mostly inconclusive, though both the British flagship \"Lion\" and \"Seydlitz\" were severely damaged. \"Lion\" lost speed, causing her to fall behind the rest of the battleline, and Beatty was unable to effectively command his ships for the remainder of the engagement. A British signalling error allowed the German battlecruisers to withdraw, as most of Beatty's squadron mistakenly concentrated on the crippled armoured cruiser \"Blücher\", sinking her with great loss of life. The British blamed their failure to win a decisive victory on their poor gunnery and attempted to increase their rate of fire by stockpiling unprotected cordite charges in their ammunition hoists and barbettes.\nAt the Battle of Jutland on 31 May 1916, both British and German battlecruisers were employed as fleet units. The British battlecruisers became engaged with both their German counterparts, the battlecruisers, and then German battleships before the arrival of the battleships of the British Grand Fleet. The result was a disaster for the Royal Navy's battlecruiser squadrons: \"Invincible\", \"Queen Mary\", and exploded with the loss of all but a handful of their crews. The exact reason why the ships' magazines detonated is not known, but the plethora of exposed cordite charges stored in their turrets, ammunition hoists and working chambers in the quest to increase their rate of fire undoubtedly contributed to their loss. Beatty's flagship \"Lion\" herself was almost lost in a similar manner, save for the heroic actions of Major Francis Harvey.\n\nThe better-armoured German battlecruisers fared better, in part due to the poor performance of British fuzes (the British shells tended to explode or break up on impact with the German armour). —the only German battlecruiser lost at Jutland—had only 128 killed, for instance, despite receiving more than thirty hits. The other German battlecruisers, , \"Von der Tann\", \"Seydlitz\", and , were all heavily damaged and required extensive repairs after the battle, \"Seydlitz\" barely making it home, for they had been the focus of British fire for much of the battle.\n\nIn the years immediately after World War I, Britain, Japan and the US all began design work on a new generation of ever more powerful battleships and battlecruisers. The new burst of shipbuilding that each nation's navy desired was politically controversial and potentially economically crippling. This nascent arms race was prevented by the Washington Naval Treaty of 1922, where the major naval powers agreed to limits on capital ship numbers. The German navy was not represented at the talks; under the terms of the Treaty of Versailles, Germany was not allowed any modern capital ships at all.\n\nThrough the 1920s and 1930s only Britain and Japan retained battlecruisers, often modified and rebuilt from their original designs. The line between the battlecruiser and the modern fast battleship became blurred; indeed, the Japanese \"Kongō\"s were formally redesignated as battleships.\n\n\"Hood\", launched in 1918, was the last World War I battlecruiser to be completed. Owing to lessons from Jutland, the ship was modified during construction; the thickness of her belt armour was increased by an average of 50 percent and extended substantially, she was given heavier deck armour, and the protection of her magazines was improved to guard against the ignition of ammunition. This was hoped to be capable of resisting her own weapons—the classic measure of a \"balanced\" battleship. \"Hood\" was the largest ship in the Royal Navy when completed; thanks to her great displacement, in theory she combined the firepower and armour of a battleship with the speed of a battlecruiser, causing some to refer to her as a fast battleship. However her protection was markedly less than that of the British battleships built immediately after World War I, the .\nThe navies of Japan and the United States, not being affected immediately by the war, had time to develop new heavy guns for their latest designs and to refine their battlecruiser designs in light of combat experience in Europe. The Imperial Japanese Navy began four s. These vessels would have been of unprecedented size and power, as fast and well armoured as \"Hood\" whilst carrying a main battery of ten 16-inch guns, the most powerful armament ever proposed for a battlecruiser. They were, for all intents and purposes, fast battleships—the only differences between them and the s which were to precede them were less side armour and a increase in speed. The United States Navy, which had worked on its battlecruiser designs since 1913 and watched the latest developments in this class with great care, responded with the . If completed as planned, they would have been exceptionally fast and well armed with eight 16-inch guns, but carried armour little better than the \"Invincible\"s—this after an increase in protection following Jutland. The final stage in the post-war battlecruiser race came with the British response to the \"Amagi\" and \"Lexington\" types: four G3 battlecruisers. Royal Navy documents of the period often described any battleship with a speed of over about as a battlecruiser, regardless of the amount of protective armour, although the G3 was considered by most to be a well-balanced fast battleship.\n\nThe Washington Naval Treaty meant that none of these designs came to fruition. Ships that had been started were either broken up on the slipway or converted to aircraft carriers. In Japan, \"Amagi\" and were selected for conversion. \"Amagi\" was damaged beyond repair by the 1923 Great Kantō earthquake and was broken up for scrap; the hull of one of the proposed \"Tosa\"-class battleships, , was converted in her stead. The United States Navy also converted two battlecruiser hulls into aircraft carriers in the wake of the Washington Treaty: and , although this was only considered marginally preferable to scrapping the hulls outright (the remaining four: \"Constellation\", \"Ranger\", \"Constitution\" and \"United States\" were scrapped). In Britain, Fisher's \"large light cruisers,\" were converted to carriers. \"Furious\" had already been partially converted during the war and \"Glorious\" and \"Courageous\" were similarly converted.\n\nIn total, nine battlecruisers survived the Washington Naval Treaty, although HMS \"Tiger\" later became a victim of the London Naval Conference of 1930 and was scrapped. Because their high speed made them valuable surface units in spite of their weaknesses, most of these ships were significantly updated before World War II. and were modernized significantly in the 1920s and 1930s. Between 1934 and 1936, \"Repulse\" was partially modernized and had her bridge modified, an aircraft hangar, catapult and new gunnery equipment added and her anti-aircraft armament increased. \"Renown\" underwent a more thorough reconstruction between 1937 and 1939. Her deck armour was increased, new turbines and boilers were fitted, an aircraft hangar and catapult added and she was completely rearmed aside from the main guns which had their elevation increased to +30 degrees. The bridge structure was also removed and a large bridge similar to that used in the battleships installed in its place. While conversions of this kind generally added weight to the vessel, \"Renown\"s tonnage actually decreased due to a substantially lighter power plant. Similar thorough rebuildings planned for \"Repulse\" and \"Hood\" were cancelled due to the advent of World War II.\n\nUnable to build new ships, the Imperial Japanese Navy also chose to improve its existing battlecruisers of the \"Kongō\" class (initially the , , and —the only later as it had been disarmed under the terms of the Washington treaty) in two substantial reconstructions (one for \"Hiei\"). During the first of these, elevation of their main guns was increased to +40 degrees, anti-torpedo bulges and of horizontal armour added, and a \"pagoda\" mast with additional command positions built up. This reduced the ships' speed to . The second reconstruction focused on speed as they had been selected as fast escorts for aircraft carrier task forces. Completely new main engines, a reduced number of boilers and an increase in hull length by allowed them to reach up to 30 knots once again. They were reclassified as \"fast battleships,\" although their armour and guns still fell short compared to surviving World War I–era battleships in the American or the British navies, with dire consequences during the Pacific War, when \"Hiei\" and \"Kirishima\" were easily crippled by US gunfire during actions off Guadalcanal, forcing their scuttling shortly afterwards. Perhaps most tellingly, \"Hiei\" was crippled by medium-caliber gunfire from heavy and light cruisers in a close-range night engagement.\n\nThere were two exceptions: Turkey's \"Yavuz Sultan Selim\" and the Royal Navy's \"Hood\". The Turkish Navy made only minor improvements to the ship in the interwar period, which primarily focused on repairing wartime damage and the installation of new fire control systems and anti-aircraft batteries. \"Hood\" was in constant service with the fleet and could not be withdrawn for an extended reconstruction. She received minor improvements over the course of the 1930s, including modern fire control systems, increased numbers of anti-aircraft guns, and in March 1941, radar.\n\nIn the late 1930s navies began to build capital ships again, and during this period a number of large commerce raiders and small, fast battleships were built that are sometimes referred to as battlecruisers. Germany and Russia designed new battlecruisers during this period, though only the latter laid down two of the 35,000-ton . They were still on the slipways when the Germans invaded in 1941 and construction was suspended. Both ships were scrapped after the war.\n\nThe Germans planned three battlecruisers of the as part of the expansion of the Kriegsmarine (Plan Z). With six 15-inch guns, high speed, excellent range, but very thin armour, they were intended as commerce raiders. Only one was ordered shortly before World War II; no work was ever done on it. No names were assigned, and they were known by their contract names: 'O', 'P', and 'Q'. The new class was not universally welcomed in the Kriegsmarine. Their abnormally-light protection gained it the derogatory nickname \"Ohne Panzer Quatsch\" (without armour nonsense) within certain circles of the Navy.\n\nThe Royal Navy deployed some of its battlecruisers during the Norwegian Campaign in April 1940. The and the were engaged during the Action off Lofoten by \"Renown\" in very bad weather and disengaged after \"Gneisenau\" was damaged. One of \"Renown\"s 15-inch shells passed through \"Gneisenau\"s director-control tower without exploding, severing electrical and communication cables as it went and destroyed the rangefinders for the forward 150 mm (5.9 in) turrets. Main-battery fire control had to be shifted aft due to the loss of electrical power. Another shell from \"Renown\" knocked out \"Gneisenau\"s aft turret. The British ship was struck twice by German shells that failed to inflict any significant damage. She was the only pre-war battlecruiser to survive the war.\n\nIn the early years of the war various German ships had a measure of success hunting merchant ships in the Atlantic. Allied battlecruisers such as \"Renown\", \"Repulse\", and the fast battleships \"Dunkerque\" and were employed on operations to hunt down the commerce-raiding German ships, but they never got close to their targets. The one stand-up fight occurred when the battleship and the heavy cruiser sortied into the North Atlantic to attack British shipping and were intercepted by \"Hood\" and the battleship in May 1941 in the Battle of the Denmark Strait. The elderly British battlecruiser was no match for the modern German battleship: within minutes, the \"Bismarck\"s 15-inch shells caused a magazine explosion in \"Hood\" reminiscent of the Battle of Jutland. Only three men survived.\n\nThe first battlecruiser to see action in the Pacific War was \"Repulse\" when she was sunk by Japanese torpedo bombers north of Singapore on 10 December 1941 whilst in company with \"Prince of Wales\". She was lightly damaged by a single bomb and near-missed by two others in the first Japanese attack. Her speed and agility enabled her to avoid the other attacks by level bombers and dodge 33 torpedoes. The last group of torpedo bombers attacked from multiple directions and \"Repulse\" was struck by five torpedoes. She quickly capsized with the loss of 27 officers and 486 crewmen; 42 officers and 754 enlisted men were rescued by the escorting destroyers. The loss of \"Repulse\" and \"Prince of Wales\" conclusively proved the vulnerability of capital ships to aircraft without air cover of their own.\n\nThe Japanese \"Kongō\"-class battlecruisers were extensively used as carrier escorts for most of their wartime career due to their high speed. Their World War I–era armament was weaker and their upgraded armour was still thin compared to contemporary battleships. On 13 November 1942, during the First Naval Battle of Guadalcanal, \"Hiei\" stumbled across American cruisers and destroyers at point-blank range. The ship was badly damaged in the encounter and had to be towed by her sister ship \"Kirishima\". Both were spotted by American aircraft the following morning and \"Kirishima\" was forced to cast off her tow because of repeated aerial attacks. \"Hiei\"s captain ordered her crew to abandon ship after further damage and scuttled \"Hiei\" in the early evening of 14 November. On the night of 14/15 November during the Second Naval Battle of Guadalcanal, \"Kirishima\" returned to Ironbottom Sound, but encountered the American battleships and . While failing to detect \"Washington\", \"Kirishima\" engaged \"South Dakota\" with some effect. \"Washington\" opened fire a few minutes later at short range and badly damaged \"Kirishima\", knocking out her aft turrets, jamming her rudder, and hitting the ship below the waterline. The flooding proved to be uncontrollable and \"Kirishima\" capsized three and a half hours later.\n\nReturning to Japan after the Battle of Leyte Gulf, \"Kongō\" was torpedoed and sunk by the American submarine on 21 November 1944. \"Haruna\" was moored at Kure, Japan when the naval base was attacked by American carrier aircraft on 24 and 28 July. The ship was only lightly damaged by a single bomb hit on 24 July, but was hit a dozen more times on 28 July and sank at her pier. She was refloated after the war and scrapped in early 1946.\n\nA late renaissance in popularity of ships between battleships and cruisers in size occurred on the eve of World War II. Described by some as battlecruisers, but never classified as capital ships, they were variously described as \"super cruisers\", \"large cruisers\" or even \"unrestricted cruisers\". The Dutch, American, and Japanese navies all planned these new classes specifically to counter the heavy cruisers, or their counterparts, being built by their naval rivals.\n\nThe first such battlecruisers were the Dutch Design 1047, designed to protect their colonies in the East Indies in the face of Japanese aggression. Never officially assigned names, these ships were designed with German and Italian assistance. While they broadly resembled the German \"Scharnhorst\" class and had the same main battery, they would have been more lightly armoured and only protected against eight-inch gunfire. Although the design was mostly completed, work on the vessels never commenced as the Germans overran the Netherlands in May 1940. The first ship would have been laid down in June of that year.\n\nThe only class of these late battlecruisers actually built were the United States Navy's \"large cruisers\". Two of them were completed, and ; a third, , was cancelled while under construction and three others, to be named \"Philippines\", \"Puerto Rico\" and \"Samoa\", were cancelled before they were laid down. They were classified as \"large cruisers\" instead of battlecruisers, and their status as non-capital ships evidenced by their being named for territories or protectorates. (Battleships, in contrast, were named after states and cruisers after cities.) With a main armament of nine 12-inch guns in three triple turrets and a displacement of , the \"Alaska\"s were twice the size of s and had guns some 50% larger in diameter. They lacked the thick armoured belt and intricate torpedo defence system of true capital ships. However, unlike most battlecruisers, they were considered a balanced design according to cruiser standards as their protection could withstand fire from their own caliber of gun, albeit only in a very narrow range band. They were designed to hunt down Japanese heavy cruisers, though by the time they entered service most Japanese cruisers had been sunk by American aircraft or submarines. Like the contemporary fast battleships, their speed ultimately made them more useful as carrier escorts and bombardment ships than as the surface combatants they were developed to be.\n\nThe Japanese started designing the B64 class, which was similar to the \"Alaska\" but with guns. News of the \"Alaska\"s led them to upgrade the design, creating Design B-65. Armed with 356 mm guns, the B65s would have been the best armed of the new breed of battlecruisers, but they still would have had only sufficient protection to keep out eight-inch shells. Much like the Dutch, the Japanese got as far as completing the design for the B65s, but never laid them down. By the time the designs were ready the Japanese Navy recognized that they had little use for the vessels and that their priority for construction should lie with aircraft carriers. Like the \"Alaska\"s, the Japanese did not call these ships battlecruisers, referring to them instead as super-heavy cruisers.\n\nIn spite of the fact that most navies abandoned the battleship and battlecruiser concepts after World War II, Joseph Stalin's fondness for big-gun-armed warships caused the Soviet Union to plan a large cruiser class in the late 1940s. In the Soviet Union they were termed \"heavy cruisers\" (\"tjazholyj krejser\"). The fruits of this program were the Project 82 (\"Stalingrad\") cruisers, of standard load, nine 305 mm guns and a speed of . Three ships were laid down in 1951–52, but they were cancelled in April 1953 after Stalin's death. Only the central armoured hull section of the first ship, \"Stalingrad\", was launched in 1954 and then used as a target.\n\nThe Soviet is sometimes referred to as a battlecruiser. This description arises from their over displacement, which is roughly equal to that of a First World War battleship and more than twice the displacement of contemporary cruisers; upon entry into service, \"Kirov\" was the largest surface ship (aside from aircraft carriers and amphibious assault ships) to be built since World War II. The \"Kirov\" class lacks the armour that distinguishes battlecruisers from ordinary cruisers and they are classified as heavy nuclear-powered missile cruisers (\"tyazholyy atomnyy raketny kreyser\") by Russia, with their primary surface armament consisting of twenty P-700 Granit surface to surface missiles. Four members of the class were completed during the 1980s and 1990s, but due to budget constraints only the is operational with the Russian Navy, though plans were announced in 2010 to return the other three ships to service. As of 2012 one ship was being refitted, but the other two ships are reportedly beyond economical repair.\n\n\n\n"}
{"id": "4059", "url": "https://en.wikipedia.org/wiki?curid=4059", "title": "Bob Hawke", "text": "Bob Hawke\n\nRobert James Lee Hawke (born 9 December 1929) is an Australian politician who was the Prime Minister of Australia and the Leader of the Labor Party from 1983 to 1991.\n\nHawke was born in South Australia but moved to Western Australia as a child. He attended the University of Western Australia and then went on to Oxford University as a Rhodes Scholar. In 1956, Hawke joined the Australian Council of Trade Unions (ACTU) as a research officer. Having risen to become responsible for wage arbitration, he was elected President of the ACTU in 1969, where he achieved an unprecedented level of popularity. After a decade as ACTU President, Hawke announced his intention to enter politics, and was immediately elected to the House of Representatives as the Labor MP for Wills.\n\nThree years later, he led Labor to a landslide election victory at the 1983 election and was sworn in as Prime Minister. He led Labor to victory at three more elections in 1984, 1987 and 1990, thus making him the most electorally successful Labor Party Leader in history. The Hawke Government created Medicare and Landcare, brokered the Prices and Incomes Accord, formed APEC, floated the Australian dollar, deregulated the financial sector, introduced the Family Assistance Scheme, announced \"Advance Australia Fair\" as the official national anthem and initiated superannuation pension schemes for all workers.\n\nHawke was eventually replaced by Paul Keating at the end of 1991, who would go on to deliver the Labor government a record fifth consecutive victory and a record thirteen years in government at the 1993 election. He remains to date Labor's longest-serving Prime Minister, Australia's third-longest-serving Prime Minister, and at the age of , Hawke is currently the longest living former Australian Prime Minister. To date, he is the only Australian Prime Minister to be born in South Australia, as well as being the only one raised and educated in Western Australia.\n\nHawke was born in Bordertown, South Australia, the second child of Arthur Hawke (1898-1989) (known as Clem), a Congregationalist minister, and his wife Edith (known as Ellie), a schoolteacher. \nHis uncle, Albert, was the Labor Premier of Western Australia between 1953–59, and was also a close friend of Prime Minister John Curtin, who was in many ways Bob Hawke's role model.\n\nHawke's elder brother Neil, who was seven years his senior, died at the age of seventeen after contracting meningitis, for which there was no cure at the time. Ellie Hawke subsequently developed an almost messianic belief in her son's destiny, and this contributed to Hawke's supreme self-confidence throughout his career. At the age of fifteen, he presciently boasted to friends that he would one day become the Prime Minister of Australia.\n\nAt the age of seventeen, the same age that his brother Neil had died, Hawke had a serious accident while riding his Panther motorcycle that left him in a critical condition for several days. This near-death experience acted as his catharsis, driving him to make the most of his talents and not let his abilities go to waste. He joined the Labor Party in 1947 at the age of eighteen, and successfully applied for a Rhodes Scholarship at the end of 1952.\n\nHawke was educated at Perth Modern School and the University of Western Australia, graduating in 1952 with a Bachelor of Arts and Bachelor of Laws. He was also president of the university's guild during the same year. The following year, Hawke won a Rhodes Scholarship to attend University College, Oxford, where he undertook a Bachelor of Arts in Philosophy, Politics and Economics (PPE). He soon found he was covering much the same ground as he did in his education at the University of Western Australia, and transferred to a Bachelor of Letters, writing his thesis on wage-fixing in Australia which was successfully presented in January 1956.\n\nHis academic achievements were complemented by setting a new world record for beer drinking; he downed – equivalent to a yard of ale – from a sconce pot in 11 seconds as part of a college penalty. In his memoirs, Hawke suggested that this single feat may have contributed to his political success more than any other, by endearing him to an electorate with a strong beer culture.\n\nIn 1956, Hawke accepted a scholarship to undertake doctoral studies in the area of arbitration law in the law department at the Australian National University in Canberra. Soon after his arrival at ANU, Hawke became the students' representative on the University Council. A year later, Hawke was recommended to the President of the Australian Council of Trade Unions (ACTU) to become a research officer, replacing Harold Souter who had become ACTU Secretary. The recommendation was made by Hawke's mentor at ANU, H.P. Brown, who for a number of years had assisted the ACTU in national wage cases. Hawke decided to abandon his doctoral studies and accept the offer, moving to Melbourne with his wife Hazel.\n\nNot long after Hawke began work at the ACTU, he became responsible for the presentation of its annual case for higher wages to the national wages tribunal, the Conciliation and Arbitration Commission. He was first appointed as an ACTU advocate in 1959. The 1958 case, under previous advocate R.L. Eggleston, had yielded only a five-shilling increase. The 1959 case found for a fifteen-shilling increase, and was regarded as a personal triumph for Hawke. He went on to attain such success and prominence in his role as an ACTU advocate that, in 1969, he was encouraged to run for the position of ACTU President, despite the fact that he had never held elected office in a trade union.\n\nHe was elected ACTU President in 1969 on a modernising platform by the narrow margin of 399 to 350, with the support of the left of the union movement, including some associated with the Communist Party. He later credited Ray Gietzelt, General Secretary of the FMWU, as the single most significant union figure in helping him achieve this outcome.\n\nHawke declared publicly that \"socialist is not a word I would use to describe myself\", and his approach to government was pragmatic. He concerned himself with making improvements to workers' lives from within the traditional institutions of government, rather than by using any ideological theory. He opposed the Vietnam War, but was a strong supporter of the US-Australian alliance, and also an emotional supporter of Israel. It was his commitment to the cause of Jewish Refuseniks which purportedly led to a planned assassination attempt on Hawke by the Popular Front for the Liberation of Palestine, and its Australian operative Munif Mohammed Abou Rish.\n\nIn 1971, Hawke along with other members of the ACTU requested that South Africa send a non-racially biased team for the Rugby Union tour, with the intention of unions agreeing not to serve the team in Australia. Prior to arrival, the Western Australian branch of the Transport Workers Union, and the Barmaids' and Barmens' Union, announced that they would serve the team, which allowed the Springboks to land in Perth. The tour commenced on 26 June and riots occurred as anti-apartheid protesters disrupted games. Hawke and his family started to receive malicious mail and phone calls from people who thought that sport and politics should not mix. Hawke remained committed to the ban on apartheid teams and later that year, the South African cricket team was successfully denied and no apartheid team was to ever come to Australia again. It was this ongoing dedication to racial equality in South Africa that would later earn Hawke the respect and friendship of Nelson Mandela.\n\nIn industrial matters, Hawke continued to demonstrate a preference for, and considerable skill at, negotiation, and was generally liked and respected by employers as well as the unions he advocated for. As early as 1972, speculation began that he would seek to enter Parliament and eventually run to become the Leader of the Labor Party. But while his professional career continued successfully, his heavy drinking and his notorious womanising placed considerable strains on his family life.\n\nIn 1973, Hawke was elected as the Federal President of the Labor Party. Two years later, when the Whitlam Government was controversially dismissed by the Governor-General, Hawke showed an initial keenness to enter Parliament at the ensuing election. Harry Jenkins, the MP for Scullin, came under pressure to step down to allow Hawke to stand in his place, but he strongly resisted this push. Hawke eventually decided not to attempt to enter Parliament at that time, a decision he soon regretted. After Labor was defeated at the election, Whitlam initially offered the leadership to Hawke, although it was not within Whitlam's power to decide who would succeed him. Despite not taking on the offer, Hawke remained influential, playing a key role in averting national strike action.\n\nThe strain of this period, serving as both ACTU President and Labor Party President, took its toll on Hawke and in 1979 he suffered a physical collapse. This shock led Hawke to publicly announce his alcoholism in a television interview, and that he would make a concerted – and ultimately successful – effort to overcome it. He was helped through this period by the relationship that he had established with writer Blanche d'Alpuget, who, in 1982, published a biography of Hawke. His popularity with the public was, if anything, enhanced by this period of rehabilitation, and opinion polling suggested that he was a far more popular public figure than either Labor Leader Bill Hayden or Liberal Prime Minister Malcolm Fraser.\n\nHawke's first attempt to enter Parliament came during the 1963 federal election. He stood in the seat of Corio in Geelong and managed to achieve a 3.1% swing against the national trend, although he fell short of ousting longtime Liberal incumbent Hubert Opperman. \n\nHawke passed up several opportunities to enter Parliament throughout the 1970s, something he later wrote that he \"regretted\". He eventually stood for election to the House of Representatives at the 1980 election for the safe Melbourne seat of Wills, winning it comfortably. Immediately upon his election to Parliament, Hawke was appointed to the Shadow Cabinet by Labor Leader Bill Hayden as Shadow Minister for Industrial Relations. Following his entry to Parliament, opinion polls continually indicated that, in contrast to Hayden, Hawke was regarded as \"a certain election winner\". After losing the 1980 election, Hayden's leadership had become insecure. In order to quell speculation over his position, Hayden eventually called a leadership ballot for 16 July 1982, believing that if he won he would be able to lead Labor into the next election. Hawke duly challenged Hayden, but Hayden was able to defeat him and remain in position, although his five-vote victory over the former ACTU President was not large enough to dispel doubts that he could lead the Labor Party to victory at an election.\n\nDespite being defeated, Hawke continued to agitate behind the scenes for a change in leadership, with opinion polls continuing to show that Hawke was a far more popular figure than both Hayden and Prime Minister Malcolm Fraser. Hayden's leadership position was thrown into further doubt after Labor performed poorly at a by-election in December 1982 for the Victorian seat of Flinders, following the resignation of the former Liberal Minister Sir Phillip Lynch. Labor needed a swing of 5.5% to win the seat, and had been predicted by the media to win, but could only achieve a swing of 3%. This convinced many Labor MPs that only Hawke would be able to lead Labor to victory at the upcoming election. Labor Party power-brokers, such as Graham Richardson and Barrie Unsworth, now openly switched their allegiance from Hayden to Hawke. More significantly, Hayden's staunch friend and political ally, Labor's Senate Leader John Button, had become convinced that Hawke's chances of victory at an election were greater than Hayden's. Having initially believed that he could carry on, Button's defection proved to be the final straw in convincing Hayden that he would have to resign as Labor Leader. \n\nLess than two months after the disastrous result at the Flinders by-election, Hayden announced his resignation as Leader of the Labor Party to the caucus on 3 February 1983. Hawke was subsequently named as leader—and hence became Leader of the Opposition—pending a party-room ballot at which he was elected unopposed. By a remarkable coincidence, on the same day that Hawke became Leader, Fraser called a snap election for 5 March 1983, hoping to capitalise on Labor's feuding before it could replace Hayden with Hawke. Fraser initially believed that he had caught Labor out, thinking that they would be forced to fight the election with Hayden as Leader. However, he was surprised to find out upon his return from seeing the Governor-General that Hayden had already resigned that morning, just hours before the writs were issued. In the election held a month later, Hawke led Labor to a landslide election victory, achieving a 24-seat swing—still the worst defeat that a sitting non-Labor Government has ever suffered—and ending seven years of Liberal Party rule.\n\nAfter Labor's landslide victory, Hawke was sworn in as the 23rd Prime Minister of Australia by the Governor-General on 11 March 1983. The inaugural days of the Hawke Government were distinctly different from those of the Whitlam Government. Rather than immediately initiating extensive reform programmes as Whitlam had, Hawke announced that Malcolm Fraser's pre-election concealment of the budget deficit meant that many of Labor's election commitments would have to be deferred. As part of his internal reforms package, Hawke divided the Government into two tiers, with only the most senior ministers sitting in the Cabinet. The Labor caucus was still given the authority to determine who would make up the Ministry, but gave Hawke unprecedented powers for a Labor Prime Minister to select which individual ministers would comprise the 13-strong Cabinet. \n\nHawke said that he did this in order to avoid what he viewed as the unwieldy nature of the Whitlam Cabinet, which had 27 members. Caucus under Hawke also exhibited a much more formalised system of parliamentary factions, which significantly altered the dynamics of caucus operations.\n\nUnlike his predecessor, Hawke's authority within the Labor Party was absolute. This enabled him to persuade his MPs to support a substantial set of policy changes. Individual accounts from ministers indicate that while Hawke was not usually the driving force behind individual reforms, he took on the role of achieving consensus and providing political guidance on what was electorally feasible and how best to sell it to the public, tasks at which he proved highly successful. Hawke took on a very public role as Prime Minister, proving to be incredibly popular with the Australian electorate; to this date he still holds the highest ever AC Nielsen approval rating.\n\nThe political partnership between Hawke and his Treasurer, Paul Keating, proved essential to Labor's success in government. The two men proved a study in contrasts: Hawke was a Rhodes Scholar; Keating left high school early. Hawke's enthusiasms were cigars, horse racing and all forms of sport; Keating preferred classical architecture, Mahler symphonies and collecting British Regency and French Empire antiques. Hawke was consensus-driven; Keating revelled in aggressive debate. Hawke was a lapsed Protestant; Keating was a practising Catholic. These differences, however, seemed only to increase the effectiveness of their partnership, as they oversaw sweeping economic and social changes throughout Australia.\n\nAccording to political commentator Paul Kelly, \"the most influential economic decisions of the 1980s were the floating of the Australian dollar and the deregulation of the financial system\". Although the Fraser Government had played a part in the process of financial deregulation by commissioning the 1981 Campbell Report, opposition from Fraser himself had stalled the deregulation process. When the Hawke Government implemented a comprehensive program of financial deregulation and reform, it \"transformed economics and politics in Australia\". The Australian economy became significantly more integrated with the global economy as a result, which completely transformed its relationship with Asia, Europe and the United States. Both Hawke and Keating would claim the credit for being the driving force behind the success of the Australian Dollar float.\n\nAmong other reforms, the Hawke Government dismantled the tariff system, privatised state sector industries, ended the subsidisation of loss-making industries, and sold off the state-owned Commonwealth Bank of Australia, Qantas and CSL Limited. The tax system was reformed, with the introduction of a fringe benefits tax and a capital gains tax, reforms strongly opposed by the Liberal Party at the time, but not ones that they reversed when they eventually returned to office. Partially offsetting these imposts upon the business community – the \"main loser\" from the 1985 Tax Summit according to Paul Kelly – was the introduction of full dividend imputation, a reform insisted upon by Keating. Funding for schools was also considerably increased, while financial assistance was provided for students to enable them to stay at school longer. Considerable progress was also made in directing assistance \"to the most disadvantaged recipients over the whole range of welfare benefits.\"\n\nHawke benefited greatly from the disarray into which the Liberal Party fell after the resignation of Malcolm Fraser. The Liberals were divided between supporters of the dour, socially conservative John Howard and the more liberal, urbane Andrew Peacock. The arch-conservative Premier of Queensland, Joh Bjelke-Petersen, added to the Liberals' problems with his \"Joh for Canberra\" campaign, which proved highly damaging. Exploiting these divisions, Hawke led the Labor Party to landslide election victories in a snap 1984 election and the 1987 election.\n\nHawke's tenure as Prime Minister saw considerable friction develop between himself and the grassroots of the Labor Party, who were unhappy at what they viewed as Hawke's iconoclasm and willingness to cooperate with business interests. All Labor Prime Ministers have at times engendered the hostility of the organisational wing of the Party, but none more so than Hawke, who regularly expressed his willingness to cull Labor's \"sacred cows\". The Socialist Left faction, as well as prominent Labor figure Barry Jones, offered severe criticism of a number of government decisions. He also received criticism for his \"confrontationalist style\" in siding with the airlines in the 1989 Australian pilots' strike.\n\nIn spite of the criticisms levelled against the Hawke Government, it succeeded in enacting a wide range of social reforms during its time in office. Deflecting arguments that the Hawke Government had failed as a reform government, Neville Wran, John Dawkins, Bill Hayden and Paul Keating made a number of speeches throughout the 1980s arguing that the Hawke Government had been a recognisably reformist government, drawing attention to Hawke's achievements as Prime Minister during his first five years in office. As well as the reintroduction of Medibank, under the new name Medicare, these included the doubling of the number of childcare places, the introduction of occupational superannuation, a boost in school retention rates, a focus on young people's job skills, a doubling of subsidised homecare services, the elimination of poverty traps in the welfare system, a 50% increase in public housing funds, an increase in the real value of the old-age pension, the development of a new youth support program, the reintroduction of six-monthly indexation of single-person unemployment benefits, and significant improvements in social security provisions. As pointed out by John Dawkins, the proportion of total government outlays allocated to families, the sick, single parents, widows, the handicapped, and veterans was significantly higher under the Hawke Government than under the Whitlam Government.\n\nAnother notable success for which Hawke's response is given considerable credit was Australia's public health campaign regarding AIDS. In the later years of the Hawke Government, Aboriginal affairs also saw considerable attention, with an investigation of the idea of a treaty between Aborigines and the Government, although this idea would be overtaken by events, notably the Mabo court decision.\n\nThe Hawke Government also made some notable environmental decisions. In its first months in office, it halted the construction of the Franklin Dam in Tasmania, responding to a groundswell of protest about the issue. In 1990, with an election looming, tough political operator Graham Richardson was appointed Environment Minister, and was given the task of attracting second-preference votes from the Australian Democrats and other environmental parties. Richardson claimed this as a major factor in the government's narrow re-election at the 1990 election.\n\nRichardson felt that the importance of his contribution to Labor's victory would automatically entitle him to the ministerial portfolio of his choice, which was Transport and Communications. He was shocked, however, at what he perceived as Hawke's ingratitude in allocating him Social Security instead. He later vowed in a telephone conversation with Peter Barron, a former Hawke staffer, to do \"whatever it takes\" to \"get\" Hawke. He immediately transferred his allegiance to Paul Keating, who after seven years as Treasurer was openly coveting the leadership.\n\nThe late 1980s recession and accompanying high interest rates had seen the government in considerable electoral trouble, with many doubting if Hawke could win in 1990. Although Keating was the main architect of the government's economic policies, he took advantage of Hawke's declining popularity to plan a leadership challenge. In 1988, in the wake of poorer opinion polls, Keating put pressure on Hawke to step down immediately. Hawke responded by agreeing a secret deal with Keating, the so-called \"Kirribilli agreement\", that he would stand down in Keating's favour shortly after the 1990 election, which he convinced Keating he could win. Hawke duly won the 1990 election, albeit by a very tight margin, and subsequently appointed Keating as Deputy Prime Minister to replace the retiring Lionel Bowen, and to prepare Keating to assume the leadership.\n\nNot long after becoming Deputy Prime Minister, frustrated at the lack of any indication from Hawke as to when he might step down, Keating made a provocative speech to the Federal Parliamentary Press Gallery. Hawke considered the speech extremely disloyal, and subsequently indicated to Keating that he would renege on the Kirribilli Agreement as a result. After this disagreement, tensions between the two men reached an all-time high, and after a turbulent year, Keating finally resigned as Deputy Prime Minister and Treasurer in June 1991, to challenge Hawke for the leadership. Hawke comfortably defeated Keating, and in a press conference after the result Keating declared that with regards the leadership, he had fired his \"one shot\". Hawke appointed John Kerin to replace Keating as Treasurer, but Kerin quickly proved to be unfit for the job. \n\nDespite his convincing victory over Keating, Hawke was seen after the result as a \"wounded\" leader; he had now lost his long-term political partner, his rating in opinion polls began to decrease, and after nearly nine years as Prime Minister, many were openly speculating that he was \"tired\", and that it was time for somebody new.\n\nHawke's leadership was finally irrevocably damaged towards the end of 1991, as new Liberal Leader John Hewson released 'Fightback!', a detailed proposal for sweeping economic change, including the introduction of a goods and services tax and deep cuts to government spending and personal income tax. The package appeared to take Hawke by complete surprise, and his response to it was judged to be extremely ineffective. Many within the Labor Party appeared to lose faith in him over this, and Keating duly challenged for the leadership a second time on 19 December 1991, this time narrowly defeating Hawke by 56 votes to 51. \n\nIn a speech to the House of Representatives the following day, Hawke declared that his nine years as Prime Minister had left Australia a better country than he found, and he was given a standing ovation by those present. He subsequently tendered his resignation as Prime Minister to the Governor-General. Hawke briefly returned to the backbenches before resigning from Parliament on 20 February 1992, sparking a by-election which was won by the independent candidate Phil Cleary from a record field of 22 candidates.\n\nHawke wrote that he had very few regrets over his time in office; although his bitterness towards Keating surfaced in his earlier memoirs, by 2008, Hawke claimed that he and Keating had long since buried their differences, and that they regularly dined together and considered each other friends. However, in 2010, the publication of the book \"Hawke: The Prime Minister,\" by Hawke's second wife, Blanche d'Alpuget, reignited conflict between the two. In an open letter to Hawke published in Australian newspapers, Keating bitterly accused Hawke and D'alpuget of spreading falsehoods about his role in Hawke's premiership. He declared that \"in hindsight, it is obvious yours and Blanche's expressions of friendship towards me over the last few years have been completely insincere.\"\n\nAfter leaving Parliament, Hawke entered the business world, taking on a number of directorships and consultancy positions which enabled him to achieve considerable financial success. He deliberately had little involvement with the Labor Party during Keating's tenure as Prime Minister, not wanting to overshadow his successor, although he did occasionally criticise some of Keating's policies publicly.\n\nAfter Keating's defeat and the election of the Howard Government at the 1996 election, he began to be more involved with Labor, regularly appearing at a number of Labor election launches and campaigns, often alongside Keating. In 2002, Hawke was named an honorary member of South Australia's Economic Development Board during Rann's Labor government.\n\nIn the run up to the 2007 election, Hawke made a considerable personal effort to support Kevin Rudd, making speeches at a large number of campaign office openings across Australia. As well as campaigning against WorkChoices, Hawke also attacked John Howard's record as Treasurer, stating \"it was the judgement of every economist and international financial institution that it was the restructuring reforms undertaken by my government, with the full cooperation of the trade union movement, which created the strength of the Australian economy today\".\n\nSimilarly, in the 2010 and 2013 campaigns, Hawke lent considerable support to Julia Gillard and Kevin Rudd respectively. Hawke also maintained an involvement in Labor politics at a state level; in 2011, Hawke publicly supported New South Wales Premier Kristina Keneally, who was facing almost certain defeat, in her campaign against Liberal Barry O'Farrell, describing her campaign as \"gutsy\".\n\nIn February 2008, Hawke joined former Prime Ministers Gough Whitlam, Malcolm Fraser and Paul Keating in Parliament House to witness Prime Minister Kevin Rudd deliver the long anticipated apology to the Stolen Generations.\n\nIn 2009, Hawke helped establish the Centre for Muslim and Non-Muslim Understanding at the University of South Australia. Interfaith dialogue was an important issue for Hawke, who told the \"Adelaide Review\" that he is \"convinced that one of the great potential dangers confronting the world is the lack of understanding in regard to the Muslim world. Fanatics have misrepresented what Islam is. They give a false impression of the essential nature of Islam.\"\n\nIn 2016, after taking part in Andrew Denton's Better Off Dead podcast, Hawke added his voice to calls for voluntary euthanasia to be legalised.\n\nHawke labelled as 'absurd' the lack of political will to fix the problem. He revealed that he had such an arrangement with his wife Blanche should such a devastating medical situation occur.\n\nHawke married Hazel Masterson in 1956 at Perth Trinity Church. They had three children; Susan (born 1957), Stephen (born 1959) and Roslyn (born 1960). Their fourth child, Robert Jr, died in his early infancy in 1963. Hawke was named Victorian Father of the Year in 1971. The couple divorced in 1995. Hawke subsequently married the writer Blanche d'Alpuget, and the two currently live together in Northbridge, a suburb of the North Shore of Sydney.\n\nOn the subject of his religion, Hawke previously wrote, while attending the 1952 World Christian Youth Conference in India, that \"there were all these poverty stricken kids at the gate of this palatial place where we were feeding our face and I just had this struck by this enormous sense of irrelevance of religion to the needs of people\". He subsequently abandoned his Christian beliefs. By the time he entered politics he was a self-described agnostic. Hawke told Andrew Denton in 2008 that his father's Christian faith had continued to influence his outlook, saying \"My father said if you believe in the fatherhood of God you must necessarily believe in the brotherhood of man, it follows necessarily, and even though I left the church and was not religious, that truth remained with me.\"\n\nOrders\n\nForeign honours\n\nOrganisations\n\nFellowships\n\nHonorary degrees\n\n\nA biographical television film, \"Hawke\", premiered on the Ten Network in Australia on 18 July 2010, with Richard Roxburgh playing the title character. Rachael Blake and Felix Williamson portrayed Hazel Hawke and Paul Keating respectively.\n\n\n\n"}
{"id": "4060", "url": "https://en.wikipedia.org/wiki?curid=4060", "title": "Baldr", "text": "Baldr\n\nBaldr (also Balder, Baldur) is a god in Norse mythology, and a son of the god Odin and the goddess Frigg. He has numerous brothers, such as Thor and Váli.\n\nIn the 12th century, Danish accounts by Saxo Grammaticus and other Danish Latin chroniclers recorded a euhemerized account of his story. Compiled in Iceland in the 13th century, but based on much older Old Norse poetry, the Poetic Edda and the Prose Edda contain numerous references to the death of Baldr as both a great tragedy to the Æsir and a harbinger of Ragnarök.\n\nAccording to \"Gylfaginning\", a book of Snorri Sturluson's Prose Edda, Baldr's wife is Nanna and their son is Forseti. In \"Gylfaginning\", Snorri relates that Baldr had the greatest ship ever built, named Hringhorni, and that there is no place more beautiful than his hall, Breidablik.\n\nJacob Grimm in his \"Teutonic Mythology\" (ch. 11) identifies Old Norse \"Baldr\" with the Old High German \"Baldere\" (2nd Merseburg Charm, Thuringia), \"Palter\" (theonym, Bavaria), \"Paltar\" (personal name) and with Old English \"bealdor, baldor\" \"lord, prince, king\" (used always with a genitive plural, as in \"gumena baldor\" \"lord of men\", \"wigena baldor\" \"lord of warriors\", et cetera). Old Norse shows this usage of the word as an honorific in a few cases, as in \"baldur î brynju\" (Sæm. 272b) and \"herbaldr\" (Sæm. 218b), both epithets of heroes in general.\n\nGrimm traces the etymology of the name to *\"balþaz\", whence Gothic \"balþs\", Old English \"bald\", Old High German \"pald\", all meaning \"bold, brave\".\n\nBut the interpretation of Baldr as \"the brave god\" may be secondary. Baltic (cf. Lithuanian \"baltas\", Latvian \"balts\") has a word meaning \"the white, the good\", and Grimm speculates that the name may originate as a Baltic loan into Proto-Germanic.\nIn continental Saxon and Anglo-Saxon tradition, the son of Woden is called not \"Bealdor\" but \"Baldag\" (Saxon) and \"Bældæg, Beldeg\" (Anglo-Saxon), which shows association with \"day\", possibly with Day personified as a deity. This, as Grimm points out, would agree with the meaning \"shining one, white one, a god\" derived from the meaning of Baltic \"baltas\", further adducing Slavic \"Belobog\" and German \"Berhta\".\n\nGrimm's etymology is endorsed by modern research. According to Rudolf Simek, the original name for Baldr must be understood as 'shining day'.\n\nOne of the two Merseburg Incantations names \"Baldere\", but also mentions a figure named \"Phol\", considered to be a byname for Baldr (as in Scandinavian \"Falr\", \"Fjalarr\"; (in Saxo) \"Balderus\" : \"Fjallerus\").\n\nIn the Poetic Edda the tale of Baldr's death is referred to rather than recounted at length. Among the visions which the Völva sees and describes in the prophecy known as the \"Völuspá\" is one of the fatal mistletoe, the birth of Váli and the weeping of Frigg (stanzas 31–33). Yet looking far into the future the Völva sees a brighter vision of a new world, when both Höðr and Baldr will come back (stanza 62). The Eddic poem \"Baldr's Dreams\" mentions that Baldr has bad dreams which the gods then discuss. Odin rides to Hel and awakens a seeress, who tells him Höðr will kill Baldr but Vali will avenge him (stanzas 9, 11).\n\nIn \"Gylfaginning\", Baldur is described as follows:\n\nApart from this description Baldr is known primarily for the story of his death. His death is seen as the first in the chain of events which will ultimately lead to the destruction of the gods at Ragnarök. Baldr will be reborn in the new world, according to \"Völuspá\".\n\nHe had a dream of his own death and his mother had the same dreams. Since dreams were usually prophetic, this depressed him, so his mother Frigg made every object on earth vow never to hurt Baldr. All objects made this vow except mistletoe—a detail which has traditionally been explained with the idea that it was too unimportant and nonthreatening to bother asking it to make the vow, but which Merrill Kaplan has instead argued echoes the fact that young people were not eligible to swear legal oaths, which could make them a threat later in life.\n\nWhen Loki, the mischief-maker, heard of this, he made a magical spear from this plant (in some later versions, an arrow). He hurried to the place where the gods were indulging in their new pastime of hurling objects at Baldr, which would bounce off without harming him. Loki gave the spear to Baldr's brother, the blind god Höðr, who then inadvertently killed his brother with it (other versions suggest that Loki guided the arrow himself). For this act, Odin and the giantess Rindr gave birth to Váli who grew to adulthood within a day and slew Höðr.\n\nBaldr was ceremonially burnt upon his ship, Hringhorni, the largest of all ships. As he was carried to the ship, Odin whispered in his ear. This was to be a key riddle asked by Odin (in disguise) of the giant Vafthrudnir (and which was, of course, unanswerable) in the poem \"Vafthrudnismal\". The riddle also appears in the riddles of Gestumblindi in \"Hervarar saga\".\n\nThe dwarf Litr was kicked by Thor into the funeral fire and burnt alive. Nanna, Baldr's wife, also threw herself on the funeral fire to await Ragnarök when she would be reunited with her husband (alternatively, she died of grief). Baldr's horse with all its trappings was also burned on the pyre. The ship was set to sea by Hyrrokin, a giantess, who came riding on a wolf and gave the ship such a push that fire flashed from the rollers and all the earth shook.\n\nUpon Frigg's entreaties, delivered through the messenger Hermod, Hel promised to release Baldr from the underworld if all objects alive and dead would weep for him. All did, except a giantess, Þökk often presumed to be the god Loki in disguise, who refused to mourn the slain god. Thus Baldr had to remain in the underworld, not to emerge until after Ragnarök, when he and his brother Höðr would be reconciled and rule the new earth together with Thor's sons.\n\nWriting at about the end of the 12th century, the Danish historian Saxo Grammaticus tells the story of Baldr (recorded as \"Balderus\") in a form which professes to be historical. According to him, Balderus and Høtherus were rival suitors for the hand of Nanna, daughter of Gewar, King of Norway. Balderus was a demigod and common steel could not wound his sacred body. The two rivals encountered each other in a terrific battle. Though Odin and Thor and the rest of the gods fought for Balderus, he was defeated and fled away, and Høtherus married the princess.\n\nNevertheless Balderus took heart of grace and again met Høtherus in a stricken field. But he fared even worse than before. Høtherus dealt him a deadly wound with a magic sword, named Mistletoe, which he had received from Miming, the satyr of the woods; after lingering three days in pain Balderus died of his injury and was buried with royal honours in a barrow.\n\nThere are also two lesser known Danish Latin chronicles, the \"Chronicon Lethrense\" and the \"Annales Lundenses\" of which the latter is included in the former. These two sources provide a second euhemerized account of Höðr's slaying of Baldr.\n\nIt relates that Hother was the king of the Saxons and son of Hothbrod and the daughter of Hadding. Hother first slew Othen's (i.e. Odin) son Balder in battle and then chased Othen and Thor. Finally, Othen's son Both killed Hother. Hother, Balder, Othen and Thor were incorrectly considered to be gods.\n\nA Latin votive inscription from Utrecht, from the 3rd or 4th century C.E., has been theorized as containing the dative form \"Baldruo\", pointing to a Latin nominative singular *\"Baldruus\", which some have identified with the Norse/Germanic god, although both the reading and this interpretation have been questioned.\n\nAs referenced in \"Gylfaginning\", in Sweden and Norway, the scentless mayweed (\"Matricaria perforata\") and the similar sea mayweed (\"Matricaria maritima\") are both called \"baldursbrá\" \"Balder's brow\" and regionally in northern England (\"baldeyebrow\"). In Iceland only the former is found. In Germany lily-of-the-valley is known as \"weisser Baldrian\"; variations using or influenced by reflexes of \"Phol\" include \"Faltrian\" (upper Austria), Villum\"fallum\" (Salzburg), and \"Fildron\" or \"Faldron\" (Tyrol).\n\nThere are a few old place names in Scandinavia that contain the name \"Baldr\". The most certain and notable one is the (former) parish name Balleshol in Hedmark county, Norway: \"a Balldrshole\" 1356 (where the last element is \"hóll\" m \"mound; small hill\"). Others may be (in Norse forms) \"Baldrsberg\" in Vestfold county, \"Baldrsheimr\" in Hordaland county \"Baldrsnes\" in Sør-Trøndelag county—and (very uncertain) the Balsfjorden fjord and Balsfjord municipality in Troms county.\n\nIn Copenhagen, there is also a Baldersgade, or \"Balder's Street.\" A street in downtown Reykjavík is called Baldursgata (Baldur's Street).\n\nIn Belgium, the name \"Balder\" is also used in dialect for a village called Berlaar and in another village (Tielen), the \"Balderij\" is a street and a swampy area next to it.\n\nIn Yorkshire there are Baldersby and Pule Hill (from Phol).\n\nIn Nottinghamshire there is a village called Balderton, originally a vineyard. This is also mentioned also in the Doomsday book.\n\nBaldur, Manitoba is a village in southern Manitoba, Canada. About 1890, Sigurdur Christopherson could not find a suitable flower in the district to name the town after, so he suggested the name of a beautiful Nordic God, namely Baldur, son of Odin.\n\nEarlier reflexes of Phol, especially in Baldr's role as opener of wells, appear as Pholesbrunnen (Thuringia), Phulsborn (village, near Saale river), Falsbrunn (Steigerwald, Franconia), and the village Pfalsau (OHG \"pholesauwa\", \"pholesouwa\"). Also, there are two Baldersbrunno (Eifel mountains; Rhine Palatinate) and a Baldur's Brönd on the road between Copenhagen and Roeskilde (Saxo Grammaticus).\n\n\n"}
{"id": "4061", "url": "https://en.wikipedia.org/wiki?curid=4061", "title": "Breidablik", "text": "Breidablik\n\nIn Norse mythology, Breiðablik (\"Broad-gleaming\") is the home of Baldr. It is briefly described in Snorri Sturluson's \"Gylfaginning\" as one of the halls of Asgard:\n\nLater in the work, when Snorri describes Baldr, he gives a longer description, citing \"Grímnismál\", though he does not name the poem:\n\nBreiðablik is not otherwise mentioned in the Eddic sources.\n\n\n\n"}
{"id": "4062", "url": "https://en.wikipedia.org/wiki?curid=4062", "title": "Bilskirnir", "text": "Bilskirnir\n\nBilskirnir (Old Norse \"lightning-crack\") is the hall of the god Thor in Norse mythology. Here he lives with his wife Sif and their children. According to \"Grímnismál\", the hall is the greatest of buildings and contains 540 rooms, located in Asgard, as are all the dwellings of the gods, in the kingdom of Þrúðheimr (or Þrúðvangar according to \"Gylfaginning\" and \"Ynglinga saga\").\n\n\n"}
{"id": "4063", "url": "https://en.wikipedia.org/wiki?curid=4063", "title": "Brísingamen", "text": "Brísingamen\n\nIn Norse mythology, Brísingamen (or Brísinga men) is the torc or necklace of the goddess Freyja. The name is an Old Norse compound \"brísinga-men\" whose second element is \"men\" \"(ornamental) neck-ring (of precious metal), torc\". The etymology of the first element is uncertain. It has been derived from Old Norse \"brísingr\", a poetic term for \"fire\" or \"amber\" mentioned in the anonymous versified word-lists (\"þulur\") appended to many manuscripts of the Prose Edda, making Brísingamen \"gleaming torc\", \"sunny torc\", or the like. However, \"Brísingr\" can also be an ethnonym, in which case \"Brísinga men\" is \"torque of the Brísings\"; the Old English parallel in \"Beowulf\" supports this derivation, though who the Brísings (Old Norse \"Brísingar\") may have been remains unknown.\n\nBrísingamen is referred to in the Anglo-Saxon epic \"Beowulf\" as \"Brosinga mene\". The brief mention in \"Beowulf\" is as follows (trans. by Howell Chickering, 1977):\n\nThis seems to confuse two different stories as the \"Beowulf\" poet is clearly referring to the legends about Theoderic the Great. The \"Þiðrekssaga\" tells that the warrior Heime (\"Háma\" in Old English) takes sides against Ermanaric (\"Eormanric\"), king of the Goths, and has to flee his kingdom after robbing him; later in life, Hama enters a monastery and gives them all his stolen treasure. However, this saga makes no mention of the great necklace. Possibly the \"Beowulf\" poet was confused, or invented the addition of the necklace to give him an excuse to drag in a mention of Eormanric. In any case, the necklace given to Beowulf in the story is not the Brísingamen itself; it is only being compared to it.\n\nIn the poem \"Þrymskviða\" of the \"Poetic Edda\", Þrymr, the king of the jǫtnar, steals Thor's hammer, Mjölnir. Freyja lends Loki her falcon cloak to search for it; but upon returning, Loki tells Freyja that Þrymr has hidden the hammer and demanded to marry her in return. Freyja is so wrathful that all the Æsir’s halls beneath her are shaken and the necklace Brísingamen breaks off from her neck. Later Thor borrows Brísingamen when he dresses up as Freyja to go to the wedding at Jǫtunheimr.\n\nThis myth is also recorded in an 18th-century Swedish folksong called \"Hammar-Hemtningen\" (the taking of the hammer), where Freyja is called Miss Frojenborg, \"den väna solen\" (the fair sun).\n\n\"Húsdrápa\", a skaldic poem partially preserved in the \"Prose Edda\", relates the story of the theft of Brísingamen by Loki. One day when Freyja wakes up and finds Brísingamen missing, she enlists the help of Heimdallr to help her search for it. Eventually they find the thief, who turns out to be Loki who has transformed himself into a seal. Heimdallr turns into a seal as well and fights Loki. After a lengthy battle at Singasteinn, Heimdallr wins and returns Brísingamen to Freyja.\n\nSnorri Sturluson quoted this old poem in \"Skáldskaparmál\", saying that because of this legend Heimdall is called \"Seeker of Freyja's Necklace\" (\"Skáldskaparmál\", section 8) and Loki is called \"Thief of Brísingamen\" (\"Skáldskaparmál\", section 16). A similar story appears in the later \"Sörla þáttr\", where Heimdallr does not appear.\n\nSörla þáttr is a short story in the later and extended version of the \"Saga of Olaf Tryggvason\" in the manuscript of the \"Flateyjarbók\", which was written and compiled by two Christian priests, Jon Thordson and Magnus Thorhalson, in the late 14th century. In the end of the story, the arrival of Christianity dissolves the old curse that traditionally was to endure until Ragnarök.\nThe battle of Högni and Heðinn is recorded in several medieval sources, including the skaldic poem \"Ragnarsdrápa\", \"Skáldskaparmál\" (section 49), and \"Gesta Danorum\": king Högni's daughter, Hildr, is kidnapped by king Heðinn. When Högni comes to fight Heðinn on an island, Hildr comes to offer her father a necklace on behalf of Heðinn for peace; but the two kings still battle, and Hildr resurrects the fallen to make them fight until Ragnarök. None of these earlier sources mentions Freyja or king Olaf Tryggvason, the historical figure who Christianized Norway and Iceland in the 10th Century.\n\nA pagan völva was buried with considerable splendour in Hagebyhöga in Östergötland. In addition to being buried with her wand, she had received great riches which included horses, a wagon and an Arabian bronze pitcher. There was also a silver pendant, which represents a woman with a broad necklace around her neck. This kind of necklace was only worn by the most prominent women during the Iron Age and some have interpreted it as Freyja's necklace Brísingamen. The pendant may represent Freyja herself.\n\nAlan Garner wrote a children's fantasy novel called \"The Weirdstone of Brisingamen\" about an enchanted teardrop bracelet.\n\nDiana Paxson's novel \"Brisingamen\" features Freyja and her bracelet.\n\nBlack Phoenix Alchemy Lab has a perfumed oil scent named Brisingamen.\n\nFreyja's necklace Brisingamen features prominently in Betsy Tobin's novel \"Iceland\", where the necklace is seen to have significant protective powers.\n\nJ. R. R. Tolkien's \"The Silmarillion\" includes a treasure called the Nauglamír, which was made by the dwarves of Eriador for the elvish king Finrod Felagund. However, the necklace was brought out a dragon's hoard by Túrin Turambar and given to King Thingol of Doriath. This king asks a group of dwarves to set a Silmaril into the necklace for his wife Melian to wear. The dwarves fall under the spell of the Silmaril and they claim the Nauglamir as their own – with the Silmaril attached. They kill Thingol and make off with the necklace. It is eventually recovered and is an heirloom of Thingol's descendants, eventually leading Eärendil to Valinor and resulting in the return of the Valar into the affairs of Middle-earth. This is clearly intended to be the equivalent in his mythology to the Brísingamen.\n\nIn Christopher Paolini's \"Inheritance Cycle\", the word \"brisingr\" means fire. This is probably a distillation of the word \"brisinga\".\n\nUrsula Le Guin's short story \"Semley's Necklace\", the first part of her novel \"Rocannon's World\", is a retelling of the Brisingamen story on an alien planet. \n\nBrisingamen is represented as a card in the Yu-Gi-Oh! Trading Card Game, \"Nordic Relic Brisingamen\".\n\nBrisingamen was part of MMORPG Ragnarok Online lore, which is ranked as \"God item\". The game is heavily based from Norse mythology.\n\nIn the Firefly Online Game, one of the planets of the Himinbjörg system (which features planets named after figures from Germanic mythology) is named Brisingamen. It is third from the star, and has moons named Freya, Beowulf, and Alberich.\n"}
{"id": "4064", "url": "https://en.wikipedia.org/wiki?curid=4064", "title": "Borsuk–Ulam theorem", "text": "Borsuk–Ulam theorem\n\nIn mathematics, the Borsuk–Ulam theorem (BUT), states that every continuous function from an \"n\"-sphere into Euclidean \"n\"-space maps some pair of antipodal points to the same point. Here, two points on a sphere are called antipodal if they are in exactly opposite directions from the sphere's center.\n\nFormally: if formula_1 is continuous then there exists an formula_2 such that: formula_3.\n\nThe case formula_4 can be illustrated by saying that there always exist a pair of opposite points on the earth's equator with the same temperature. The same is true for any circle. This assumes the temperature varies continuously.\n\nThe case formula_5 is often illustrated by saying that at any moment, there is always a pair of antipodal points on the Earth's surface with equal temperatures and equal barometric pressures.\n\nBUT has several equivalent statements in terms of odd functions. Recall that formula_6 is the \"n\"-sphere and formula_7 is the \"n\"-ball:\n\nAccording to , the first historical mention of the statement of BUT appears in . The first proof was given by , where the formulation of the problem was attributed to Stanislaw Ulam. Since then, many alternative proofs have been found by various authors, as collected by .\n\nThe following statements are equivalent to BUT.\n\nA function formula_16 is called \"odd\" (aka \"antipodal\" or \"antipode-preserving\") if for every formula_17: formula_18.\n\nBUT is equivalent to the following statement: A continuous odd function from an \"n\"-sphere into Euclidean \"n\"-space has a zero. PROOF: \n\nDefine a \"retraction\" as a function formula_25.\n\nBUT is equivalent to the following claim: there is no continuous odd retraction.\n\nPROOF: If BUT is correct, then every continuous odd function from formula_6 must include 0 in its range. However, formula_27 so there cannot be a continuous odd function whose range is formula_12.\n\nConversely, if BUT is incorrect, then there is a continuous odd function formula_29 with no zeroes. Then we can construct another odd function formula_25 by:\n\nsince formula_16 has no zeroes, formula_33 is well-defined and continuous. Thus we have a continuous odd retraction.\n\nThe 1-dimensional case can easily be proved using the intermediate value theorem (IVT).\n\nLet formula_16 be an odd real-valued continuous function on a circle. Pick an arbitrary formula_17. If formula_10 then we are done. Otherwise, w.l.o.g. formula_37. But formula_38. Hence, by the IVT there is a point formula_39 between formula_17 and formula_41 on which formula_42.\n\nAssume that formula_25 is an odd continuous function with formula_44 (the case formula_45 is treated above, the case formula_46 can be handled using basic covering theory). By passing to orbits under the antipodal action, we then get an induced function formula_47, which induces an isomorphism on fundamental groups. By the Hurewicz theorem, the induced map on cohomology with formula_48 coefficients, formula_49, sends formula_50 to formula_51. But then we get that formula_52 is send to formula_53, a contradiction.\n\nOne can also show the stronger statement that any odd map formula_54 has odd degree and then deduce BUT from this result.\n\nBUT can be proved from Tucker's lemma.\n\nLet formula_8 be a continuous odd function. Because \"g\" is continuous on a compact domain, it is uniformly continuous. Therefore, for every formula_56, there is a formula_57 such that, for every two points of formula_58 which are within formula_59 of each other, their images under \"g\" are within formula_60 of each other.\n\nDefine a triangulation of formula_58 with edges of length at most formula_59. Label each vertex formula_63 of the triangulation with a label formula_64 in the following way:\nBecause \"g\" is odd, the labeling is also odd: formula_67. Hence, by Tucker's lemma, there are two adjacent vertices formula_68 with opposite labels. Assume w.l.o.g. that the labels are formula_69. By definition of \"l\", this means that in both formula_70 and formula_71, coordinate #1 is the largest coordinate; in formula_70 this coordinate is positive while in formula_71 it is negative. By the construction of the triangulation, the distance between formula_70 and formula_71 is at most formula_60; this means that both formula_77 and formula_78 are bounded by formula_60.\n\nThe above is true for every formula_60; hence there must be a point \"u\" in which formula_81.\n\n\nAbove we showed how to prove BUT from Tucker's lemma. The converse is also true: it is possible to prove Tucker's lemma from BUT. Therefore, these two theorems are equivalent.\n1. In the original BUT, the domain of the function \"f\" is the unit \"n\"-sphere (the boundary of the unit \"n\"-ball). In general, it is true also when the domain of \"f\" is the boundary of any open bounded symmetric subset of \"R\" containing the origin (Here, symmetric means that if \"x\" is in the subset then -\"x\" is also in the subset).\n\n2. Consider the function \"A\" which maps a point to its antipodal point: \"A(x)=-x\". Note that \"A(A(x))=x\". The original BUT claims that there is a point \"x\" in which \"f(A(x))=f(x)\". In general, this is true also for every function \"A\" for which \"A(A(x))=x\". However, in general this is not true for other functions \"A\".\n\n\n"}
{"id": "4067", "url": "https://en.wikipedia.org/wiki?curid=4067", "title": "Bragi", "text": "Bragi\n\nBragi is the skaldic god of poetry in Norse mythology.\n\n\"Bragi\" is generally associated with \"bragr\", the Norse word for poetry. The name of the god may have been derived from \"bragr\", or the term \"bragr\" may have been formed to describe 'what Bragi does'. A connection between the name Bragi and Old English \"brego\" 'chieftain' has been suggested but is generally now discounted. A connection between Bragi and the \"bragarfull\" 'promise cup' is sometimes suggested, as \"bragafull\", an alternate form of the word, might be translated as 'Bragi's cup'. See Bragarfull.\n\nSnorri Sturluson writes in the \"Gylfaginning\" after describing Odin, Thor, and Baldr:\nIn \"Skáldskaparmál\" Snorri writes:\nThat Bragi is Odin's son is clearly mentioned only here and in some versions of a list of the sons of Odin (see Sons of Odin). But \"wish-son\" in stanza 16 of the \"Lokasenna\" could mean \"Odin's son\" and is translated by Hollander as \"Odin's kin\". Bragi's mother is possibly the giantess Gunnlod. If Bragi's mother is Frigg, then Frigg is somewhat dismissive of Bragi in the \"Lokasenna\" in stanza 27 when Frigg complains that if she had a son in Ægir's hall as brave as Baldr then Loki would have to fight for his life.\n\nIn that poem Bragi at first forbids Loki to enter the hall but is overruled by Odin. Loki then gives a greeting to all gods and goddesses who are in the hall save to Bragi. Bragi generously offers his sword, horse, and an arm ring as peace gift but Loki only responds by accusing Bragi of cowardice, of being the most afraid to fight of any of the Æsir and Elves within the hall. Bragi responds that if they were outside the hall, he would have Loki's head, but Loki only repeats the accusation. When Bragi's wife Iðunn attempts to calm Bragi, Loki accuses her of embracing her brother's slayer, a reference to matters that have not survived. It may be that Bragi had slain Iðunn's brother.\n\nA passage in the \"Poetic Edda\" poem \"Sigrdrífumál\" describes runes being graven on the sun, on the ear of one of the sun-horses and on the hoofs of the other, on Sleipnir's teeth, on bear's paw, on eagle's beak, on wolf's claw, and on several other things including on Bragi's tongue. Then the runes are shaved off and the shavings are mixed with mead and sent abroad so that Æsir have some, Elves have some, Vanir have some, and Men have some, these being beech runes and birth runes, ale runes, and magic runes. The meaning of this is obscure.\n\nThe first part of Snorri Sturluson's \"Skáldskaparmál\" is a dialogue between Ægir and Bragi about the nature of poetry, particularly skaldic poetry. Bragi tells the origin of the mead of poetry from the blood of Kvasir and how Odin obtained this mead. He then goes on to discuss various poetic metaphors known as \"kennings\".\n\nSnorri Sturluson clearly distinguishes the god Bragi from the mortal skald Bragi Boddason, whom he often mentions separately. The appearance of Bragi in the \"Lokasenna\" indicates that if these two Bragis were originally the same, they have become separated for that author also, or that chronology has become very muddled and Bragi Boddason has been relocated to mythological time. Compare the appearance of the Welsh Taliesin in the second branch of the Mabinogi. Legendary chronology sometimes does become muddled. Whether Bragi the god originally arose as a deified version of Bragi Boddason was much debated in the 19th century, especially by the scholars Eugen Mogk and Sophus Bugge. The debate remains undecided.\n\nIn the poem \"Eiríksmál\" Odin, in Valhalla, hears the coming of the dead Norwegian king Eric Bloodaxe and his host, and bids the heroes Sigmund and Sinfjötli rise to greet him. Bragi is then mentioned, questioning how Odin knows that it is Eric and why Odin has let such a king die. In the poem \"Hákonarmál\", Hákon the Good is taken to Valhalla by the valkyrie Göndul and Odin sends Hermóðr and Bragi to greet him. In these poems Bragi could be either a god or a dead hero in Valhalla. Attempting to decide is further confused because \"Hermóðr\" also seems to be sometimes the name of a god and sometimes the name of a hero. That Bragi was also the first to speak to Loki in the \"Lokasenna\" as Loki attempted to enter the hall might be a parallel. It might have been useful and customary that a man of great eloquence and versed in poetry should greet those entering a hall.\n\nIn the \"Prose Edda\" Snorri Sturluson quotes many stanzas attributed to Bragi Boddason the old (\"Bragi Boddason inn gamli\"), a Norwegian court poet who served several Swedish kings, Ragnar Lodbrok, Östen Beli and Björn at Hauge who reigned in the first half of the 9th century. This Bragi was reckoned as the first skaldic poet, and was certainly the earliest skaldic poet then remembered by name whose verse survived in memory.\n\nSnorri especially quotes passages from Bragi's \"Ragnarsdrápa\", a poem supposedly composed in honor of the famous legendary Viking Ragnar Lodbrók ('Hairy-breeches') describing the images on a decorated shield which Ragnar had given to Bragi. The images included Thor's fishing for Jörmungandr, Gefjun's ploughing of Zealand from the soil of Sweden, the attack of Hamdir and Sorli against King Jörmunrekk, and the never-ending battle between Hedin and Högni.\n\nBragi son of Hálfdan the Old is mentioned only in the \"Skjáldskaparmál\". This Bragi is the sixth of the second of two groups of nine sons fathered by King Hálfdan the Old on Alvig the Wise, daughter of King Eymund of Hólmgard. This second group of sons are all eponymous ancestors of legendary families of the north. Snorri says:\n\nBragi, from whom the Bragnings are sprung (that is the race of Hálfdan the Generous).\n\nOf the Bragnings as a race and of Hálfdan the Generous nothing else is known. However, \"Bragning\" is often, like some others of these dynastic names, used in poetry as a general word for 'king' or 'ruler'.\n\nIn the eddic poem \"Helgakviða Hundingsbana II\", Bragi Högnason, his brother Dag, and his sister Sigrún were children of Högne, the king of East Götaland. The poem relates how Sigmund's son Helgi Hundingsbane agreed to take Sigrún daughter of Högni as his wife against her unwilling betrothal to Hodbrodd son of Granmar the king of Södermanland. In the subsequent battle of Frekastein (probably one of the 300 hill forts of Södermanland, as \"stein\" meant \"hill fort\") against Högni and Grammar, all the chieftains on Granmar's side are slain, including Bragi, except for Bragi's brother Dag.\n\n"}
{"id": "4068", "url": "https://en.wikipedia.org/wiki?curid=4068", "title": "Blaise Pascal", "text": "Blaise Pascal\n\nBlaise Pascal (; ; 19 June 1623 – 19 August 1662) was a French mathematician, physicist, inventor, writer and Catholic theologian. He was a child prodigy who was educated by his father, a tax collector in Rouen. Pascal's earliest work was in the natural and applied sciences where he made important contributions to the study of fluids, and clarified the concepts of pressure and vacuum by generalising the work of Evangelista Torricelli. Pascal also wrote in defence of the scientific method.\n\nIn 1642, while still a teenager, he started some pioneering work on calculating machines. After three years of effort and 50 prototypes, he built 20 finished machines (called Pascal's calculators and later Pascalines) over the following 10 years, establishing him as one of the first two inventors of the mechanical calculator.\n\nPascal was an important mathematician, helping create two major new areas of research: he wrote a significant treatise on the subject of projective geometry at the age of 16, and later corresponded with Pierre de Fermat on probability theory, strongly influencing the development of modern economics and social science. Following Galileo Galilei and Torricelli, in 1647, he rebutted Aristotle's followers who insisted that nature abhors a vacuum. Pascal's results caused many disputes before being accepted.\n\nIn 1646, he and his sister Jacqueline identified with the religious movement within Catholicism known by its detractors as Jansenism. His father died in 1651. Following a religious experience in late 1654, he began writing influential works on philosophy and theology. His two most famous works date from this period: the \"Lettres provinciales\" and the \"Pensées\", the former set in the conflict between Jansenists and Jesuits. In that year, he also wrote an important treatise on the arithmetical triangle. Between 1658 and 1659 he wrote on the cycloid and its use in calculating the volume of solids.\n\nPascal had poor health, especially after the age of 18, and he died just two months after his 39th birthday.\n\nPascal was born in Clermont-Ferrand, which is in France's Auvergne region. He lost his mother, Antoinette Begon, at the age of three. His father, Étienne Pascal (1588–1651), who also had an interest in science and mathematics, was a local judge and member of the \"Noblesse de Robe\". Pascal had two sisters, the younger Jacqueline and the elder Gilberte.\n\nIn 1631, five years after the death of his wife, Étienne Pascal moved with his children to Paris. The newly arrived family soon hired Louise Delfault, a maid who eventually became an instrumental member of the family. Étienne, who never remarried, decided that he alone would educate his children, for they all showed extraordinary intellectual ability, particularly his son Blaise. The young Pascal showed an amazing aptitude for mathematics and science.\n\nParticularly of interest to Pascal was a work of Desargues on conic sections. Following Desargues' thinking, the 16-year-old Pascal produced, as a means of proof, a short treatise on what was called the \"Mystic Hexagram\", \"Essai pour les coniques\" (\"Essay on Conics\") and sent it—his first serious work of mathematics—to Père Mersenne in Paris; it is known still today as Pascal's theorem. It states that if a hexagon is inscribed in a circle (or conic) then the three intersection points of opposite sides lie on a line (called the Pascal line).\n\nPascal's work was so precocious that Descartes was convinced that Pascal's father had written it. When assured by Mersenne that it was, indeed, the product of the son and not the father, Descartes dismissed it with a sniff: \"I do not find it strange that he has offered demonstrations about conics more appropriate than those of the ancients,\" adding, \"but other matters related to this subject can be proposed that would scarcely occur to a 16-year-old child.\"\n\nIn France at that time offices and positions could be—and were—bought and sold. In 1631 Étienne sold his position as second president of the \"Cour des Aides\" for 65,665 livres. The money was invested in a government bond which provided, if not a lavish, then certainly a comfortable income which allowed the Pascal family to move to, and enjoy, Paris. But in 1638 Richelieu, desperate for money to carry on the Thirty Years' War, defaulted on the government's bonds. Suddenly Étienne Pascal's worth had dropped from nearly 66,000 livres to less than 7,300.\nLike so many others, Étienne was eventually forced to flee Paris because of his opposition to the fiscal policies of Cardinal Richelieu, leaving his three children in the care of his neighbour Madame Sainctot, a great beauty with an infamous past who kept one of the most glittering and intellectual salons in all France. It was only when Jacqueline performed well in a children's play with Richelieu in attendance that Étienne was pardoned. In time, Étienne was back in good graces with the cardinal and in 1639 had been appointed the king's commissioner of taxes in the city of Rouen—a city whose tax records, thanks to uprisings, were in utter chaos.\n\nIn 1642, in an effort to ease his father's endless, exhausting calculations, and recalculations, of taxes owed and paid (into which work the young Pascal had been recruited), Pascal, not yet 19, constructed a mechanical calculator capable of addition and subtraction, called \"Pascal's calculator\" or the \"Pascaline\". Of the eight Pascalines known to have survived, four are held by the Musée des Arts et Métiers in Paris and one more by the Zwinger museum in Dresden, Germany, exhibit two of his original mechanical calculators. Though these machines are pioneering forerunners to a further 400 years of development of mechanical methods of calculation, and in a sense to the later field of computer engineering, the calculator failed to be a great commercial success. Partly because it was still quite cumbersome to use in practice, but probably primarily because it was extraordinarily expensive, the Pascaline became little more than a toy, and a status symbol, for the very rich both in France and elsewhere in Europe. Pascal continued to make improvements to his design through the next decade, and he refers to some 50 machines that were built to his design.\n\nPascal continued to influence mathematics throughout his life. His \"Traité du triangle arithmétique\" (\"Treatise on the Arithmetical Triangle\") of 1653 described a convenient tabular presentation for binomial coefficients, now called Pascal's triangle. The triangle can also be represented:\nHe defines the numbers in the triangle by recursion: Call the number in the (\"m\" + 1)th row and (\"n\" + 1)th column \"t\". Then \"t\" = \"t\" + \"t\", for \"m\" = 0, 1, 2, ... and \"n\" = 0, 1, 2, ... The boundary conditions are \"t\" = 0, \"t\" = 0 for \"m\" = 1, 2, 3, ... and \"n\" = 1, 2, 3, ... The generator \"t\" = 1. Pascal concludes with the proof,\n\nIn 1654 he proved \"Pascal's identity\" relating the sums of the \"p\"-th powers of the first \"n\" positive integers for \"p\" = 0, 1, 2, …, \"k\".\n\nIn 1654, prompted by his friend the Chevalier de Méré, he corresponded with Pierre de Fermat on the subject of gambling problems, and from that collaboration was born the mathematical theory of probabilities. The specific problem was that of two players who want to finish a game early and, given the current circumstances of the game, want to divide the stakes fairly, based on the chance each has of winning the game from that point. From this discussion, the notion of expected value was introduced. Pascal later (in the \"Pensées\") used a probabilistic argument, Pascal's Wager, to justify belief in God and a virtuous life. The work done by Fermat and Pascal into the calculus of probabilities laid important groundwork for Leibniz' formulation of the calculus.\n\nAfter a religious experience in 1654, Pascal mostly gave up work in mathematics.\n\nPascal's major contribution to the philosophy of mathematics came with his \"De l'Esprit géométrique\" (\"Of the Geometrical Spirit\"), originally written as a preface to a geometry textbook for one of the famous \"\"Petites-Ecoles de Port-Royal\" (\"Little Schools of Port-Royal\")\". The work was unpublished until over a century after his death. Here, Pascal looked into the issue of discovering truths, arguing that the ideal of such a method would be to found all propositions on already established truths. At the same time, however, he claimed this was impossible because such established truths would require other truths to back them up—first principles, therefore, cannot be reached. Based on this, Pascal argued that the procedure used in geometry was as perfect as possible, with certain principles assumed and other propositions developed from them. Nevertheless, there was no way to know the assumed principles to be true.\n\nPascal also used \"De l'Esprit géométrique\" to develop a theory of definition. He distinguished between definitions which are conventional labels defined by the writer and definitions which are within the language and understood by everyone because they naturally designate their referent. The second type would be characteristic of the philosophy of essentialism. Pascal claimed that only definitions of the first type were important to science and mathematics, arguing that those fields should adopt the philosophy of formalism as formulated by Descartes.\n\nIn \"De l'Art de persuader\" (\"On the Art of Persuasion\"), Pascal looked deeper into geometry's axiomatic method, specifically the question of how people come to be convinced of the axioms upon which later conclusions are based. Pascal agreed with Montaigne that achieving certainty in these axioms and conclusions through human methods is impossible. He asserted that these principles can be grasped only through intuition, and that this fact underscored the necessity for submission to God in searching out truths.\n\nPascal's work in the fields of the study of hydrodynamics and hydrostatics centered on the principles of hydraulic fluids. His inventions include the hydraulic press (using hydraulic pressure to multiply force) and the syringe. He proved that hydrostatic pressure depends not on the weight of the fluid but on the elevation difference. He demonstrated this principle by attaching a thin tube to a barrel full of water and filling the tube with water up to the level of the third floor of a building. This caused the barrel to leak, in what became known as Pascal's barrel experiment.\n\nBy 1647, Pascal had learned of Evangelista Torricelli's experimentation with barometers. Having replicated an experiment that involved placing a tube filled with mercury upside down in a bowl of mercury, Pascal questioned what force kept some mercury in the tube and what filled the space above the mercury in the tube. At the time, most scientists contended that, rather than a vacuum, some invisible matter was present. This was based on the Aristotelian notion that creation was a thing of substance, whether visible or invisible; and that this substance was forever in motion. Furthermore, \"Everything that is in motion must be moved by something,\" Aristotle declared. Therefore, to the Aristotelian trained scientists of Pascal's time, a vacuum was an impossibility. How so? As proof it was pointed out:\n\nFollowing more experimentation in this vein, in 1647 Pascal produced \"Experiences nouvelles touchant le vide\" (\"New experiments with the vacuum\"), which detailed basic rules describing to what degree various liquids could be supported by air pressure. It also provided reasons why it was indeed a vacuum above the column of liquid in a barometer tube. This work was followed by \"Récit de la grande expérience de l’équilibre des liqueurs\" (\"Account of the great experiment on equilibrium in liquids\") published in 1648.\n\nOn 19 September 1648, after many months of Pascal's friendly but insistent prodding, Florin Périer, husband of Pascal's elder sister Gilberte, was finally able to carry out the fact-finding mission vital to Pascal's theory. The account, written by Périer, reads:\n\nPascal replicated the experiment in Paris by carrying a barometer up to the top of the bell tower at the church of Saint-Jacques-de-la-Boucherie, a height of about 50 metres. The mercury dropped two lines.\n\nIn the face of criticism that some invisible matter must exist in Pascal's empty space, Pascal, in his reply to Estienne Noel, gave one of the 17th century's major statements on the scientific method, which is a striking anticipation of the idea popularised by Karl Popper that scientific theories are characterised by their falsifiability: \"In order to show that a hypothesis is evident, it does not suffice that all the phenomena follow from it; instead, if it leads to something contrary to a single one of the phenomena, that suffices to establish its falsity.\" His insistence on the existence of the vacuum also led to conflict with other prominent scientists, including Descartes.\n\nPascal introduced a primitive form of roulette and the roulette wheel in his search for a perpetual motion machine.\n\nIn the winter of 1646, Pascal's 58-year-old father broke his hip when he slipped and fell on an icy street of Rouen; given the man's age and the state of medicine in the 17th century, a broken hip could be a very serious condition, perhaps even fatal. Rouen was home to two of the finest doctors in France: Monsieur Doctor Deslandes and Monsieur Doctor de La Bouteillerie. The elder Pascal \"would not let anyone other than these men attend him...It was a good choice, for the old man survived and was able to walk again...\" But treatment and rehabilitation took three months, during which time La Bouteillerie and Deslandes had become household guests.\n\nBoth men were followers of Jean Guillebert, proponent of a splinter group from Catholic teaching known as Jansenism. This still fairly small sect was making surprising inroads into the French Catholic community at that time. It espoused rigorous Augustinism. Blaise spoke with the doctors frequently, and upon his successful treatment of Étienne, borrowed from them works by Jansenist authors. In this period, Pascal experienced a sort of \"first conversion\" and began to write on theological subjects in the course of the following year.\n\nPascal fell away from this initial religious engagement and experienced a few years of what some biographers have called his \"worldly period\" (1648–54). His father died in 1651 and left his inheritance to Pascal and Jacqueline, for whom Pascal acted as her conservator. Jacqueline announced that she would soon become a postulant in the Jansenist convent of Port-Royal. Pascal was deeply affected and very sad, not because of her choice, but because of his chronic poor health; he too needed her.\n\nBy the end of October in 1651, a truce had been reached between brother and sister. In return for a healthy annual stipend, Jacqueline signed over her part of the inheritance to her brother. Gilberte had already been given her inheritance in the form of a dowry. In early January, Jacqueline left for Port-Royal. On that day, according to Gilberte concerning her brother, \"He retired very sadly to his rooms without seeing Jacqueline, who was waiting in the little parlor...\"\nIn early June 1653, after what must have seemed like endless badgering from Jacqueline,\nPascal formally signed over the whole of his sister's inheritance to Port-Royal, which, to him, \"had begun to smell like a cult.\" With two thirds of his father's estate now gone, the 29-year-old Pascal was now consigned to genteel poverty.\n\nFor a while, Pascal pursued the life of a bachelor. During visits to his sister at Port-Royal in 1654, he displayed contempt for affairs of the world but was not drawn to God.\n\nOn 23 November 1654, between 10:30 and 12:30 at night, Pascal had an intense religious vision and immediately recorded the experience in a brief note to himself which began: \"Fire. God of Abraham, God of Isaac, God of Jacob, not of the philosophers and the scholars...\" and concluded by quoting Psalm 119:16: \"I will not forget thy word. Amen.\" He seems to have carefully sewn this document into his coat and always transferred it when he changed clothes; a servant discovered it only by chance after his death. This piece is now known as the \"Memorial\". The story of the carriage accident as having led to the experience described in the \"Memorial\" is disputed by some scholars.\nHis belief and religious commitment revitalized, Pascal visited the older of two convents at Port-Royal for a two-week retreat in January 1655. For the next four years, he regularly travelled between Port-Royal and Paris. It was at this point immediately after his conversion when he began writing his first major literary work on religion, the \"Provincial Letters\".\n\nBeginning in 1656–57, Pascal published his memorable attack on casuistry, a popular ethical method used by Catholic thinkers in the early modern period (especially the Jesuits, and in particular Antonio Escobar). Pascal denounced casuistry as the mere use of complex reasoning to justify moral laxity and all sorts of sins. The 18-letter series was published between 1656 and 1657 under the pseudonym Louis de Montalte and incensed Louis XIV. The king ordered that the book be shredded and burnt in 1660. In 1661, in the midsts of the formulary controversy, the Jansenist school at Port-Royal was condemned and closed down; those involved with the school had to sign a 1656 papal bull condemning the teachings of Jansen as heretical. The final letter from Pascal, in 1657, had defied Alexander VII himself. Even Pope Alexander, while publicly opposing them, nonetheless was persuaded by Pascal's arguments.\n\nAside from their religious influence, the \"Provincial Letters\" were popular as a literary work. Pascal's use of humor, mockery, and vicious satire in his arguments made the letters ripe for public consumption, and influenced the prose of later French writers like Voltaire and Jean-Jacques Rousseau.\n\nCharles Perrault wrote of the \"Letters\": \"Everything is there—purity of language, nobility of thought, solidity in reasoning, finesse in raillery, and throughout an \"agrément\" not to be found anywhere else.\"\n\nPascal's most influential theological work, referred to posthumously as the \"Pensées\" (\"Thoughts\"), was not completed before his death. It was to have been a sustained and coherent examination and defense of the Christian faith, with the original title \"Apologie de la religion Chrétienne\" (\"Defense of the Christian Religion\"). The first version of the numerous scraps of paper found after his death appeared in print as a book in 1669 titled \"Pensées de M. Pascal sur la religion, et sur quelques autres sujets\" (\"Thoughts of M. Pascal on religion, and on some other subjects\") and soon thereafter became a classic. One of the \"Apologie\"s main strategies was to use the contradictory philosophies of skepticism and stoicism, personalized by Montaigne on one hand, and Epictetus on the other, in order to bring the unbeliever to such despair and confusion that he would embrace God.\n\nPascal's \"Pensées\" is widely considered to be a masterpiece, and a landmark in French prose. When commenting on one particular section (Thought #72), Sainte-Beuve praised it as the finest pages in the French language. Will Durant hailed it as \"the most eloquent book in French prose\". In \"Pensées\", Pascal surveys several philosophical paradoxes: infinity and nothing, faith and reason, soul and matter, death and life, meaning and vanity – seemingly arriving at no definitive conclusions besides humility, ignorance, and grace. Rolling these into one he develops Pascal's Wager.\n\nT. S. Eliot described him during this phase of his life as \"a man of the world among ascetics, and an ascetic among men of the world.\" Pascal's ascetic lifestyle derived from a belief that it was natural and necessary for a person to suffer. In 1659, Pascal fell seriously ill. During his last years, he frequently tried to reject the ministrations of his doctors, saying, \"Sickness is the natural state of Christians.\"\n\nLouis XIV suppressed the Jansenist movement at Port-Royal in 1661. In response, Pascal wrote one of his final works, \"Écrit sur la signature du formulaire\" (\"Writ on the Signing of the Form\"), exhorting the Jansenists not to give in. Later that year, his sister Jacqueline died, which convinced Pascal to cease his polemics on Jansenism. Pascal's last major achievement, returning to his mechanical genius, was inaugurating perhaps the first bus line, moving passengers within Paris in a carriage with many seats.\n\nIn 1662, Pascal's illness became more violent, and his emotional condition had severely worsened since his sister's death. Aware that his health was fading quickly, he sought a move to the hospital for incurable diseases, but his doctors declared that he was too unstable to be carried. In Paris on 18 August 1662, Pascal went into convulsions and received extreme unction. He died the next morning, his last words being \"May God never abandon me,\" and was buried in the cemetery of Saint-Étienne-du-Mont.\n\nAn autopsy performed after his death revealed grave problems with his stomach and other organs of his abdomen, along with damage to his brain. Despite the autopsy, the cause of his poor health was never precisely determined, though speculation focuses on tuberculosis, stomach cancer, or a combination of the two. The headaches which afflicted Pascal are generally attributed to his brain lesion.\n\nIn honour of his scientific contributions, the name \"Pascal\" has been given to the SI unit of pressure, to a programming language, and Pascal's law (an important principle of hydrostatics), and as mentioned above, Pascal's triangle and Pascal's wager still bear his name.\n\nPascal's development of probability theory was his most influential contribution to mathematics. Originally applied to gambling, today it is extremely important in economics, especially in actuarial science. John Ross writes, \"Probability theory and the discoveries following it changed the way we regard uncertainty, risk, decision-making, and an individual's and society's ability to influence the course of future events.\" However, it should be noted that Pascal and Fermat, though doing important early work in probability theory, did not develop the field very far. Christiaan Huygens, learning of the subject from the correspondence of Pascal and Fermat, wrote the first book on the subject. Later figures who continued the development of the theory include Abraham de Moivre and Pierre-Simon Laplace.\n\nIn literature, Pascal is regarded as one of the most important authors of the French Classical Period and is read today as one of the greatest masters of French prose. His use of satire and wit influenced later polemicists. The content of his literary work is best remembered for its strong opposition to the rationalism of René Descartes and simultaneous assertion that the main countervailing philosophy, empiricism, was also insufficient for determining major truths.\n\nIn France, prestigious annual awards, Blaise Pascal Chairs are given to outstanding international scientists to conduct their research in the Ile de France region. One of the Universities of Clermont-Ferrand, France – Université Blaise Pascal – is named after him. The University of Waterloo, Ontario, Canada, holds an annual math contest named in his honour.\n\nPascalian theology has grown out of his perspective that we are, according to Wood, \"born into a duplicitous world that shapes us into duplicitous subjects and so we find it easy to reject God continually and deceive ourselves about our own sinfulness\".\n\nRoberto Rossellini directed a filmed biopic, \"Blaise Pascal\", which originally aired on Italian television in 1971. Pascal was a subject of the first edition of the 1984 BBC Two documentary, \"Sea of Faith\", presented by Don Cupitt.\n\nIn 2014 Nvidia announced its new Pascal microarchitecture, which is named for Pascal. The first graphics cards featuring Pascal were released in 2016.\n\n\n\n\n"}
{"id": "4069", "url": "https://en.wikipedia.org/wiki?curid=4069", "title": "Brittonic languages", "text": "Brittonic languages\n\nThe Brittonic, Brythonic or British Celtic languages (, , ) form one of the two branches of the Insular Celtic language family; the other is Goidelic. The name \"Brythonic\" was derived by Welsh Celticist John Rhys from the Welsh word \"Brython\", meaning an indigenous Briton as opposed to an Anglo-Saxon or Gael. The name \"Brittonic\" derives ultimately from the name \"Prettanike\", recorded by Greek authors for the British Isles. Some authors reserve the term \"Brittonic\" for the modified later Brittonic languages after about AD 600.\n\nThe Brittonic languages derive from the Common Brittonic language, spoken throughout Great Britain south of the Firth of Forth during the Iron Age and Roman period. In addition, North of the Forth, the Pictish language is considered to be related; it is possible it was a Brittonic language, but it may have been a sister language. In the 5th and 6th centuries emigrating Britons also took Brittonic speech to the continent, most significantly in Brittany and Britonia. During the next few centuries the language began to split into several dialects, eventually evolving into Welsh, Cornish, Breton, and Cumbric. Welsh and Breton continue to be spoken as native languages, while a revival in Cornish has led to an increase in speakers of that language. Cumbric is extinct, having been replaced by Goidelic and English speech. The Isle of Man and Orkney may also have originally spoken a Brittonic language, later replaced with a Goidelic one. Due to emigration, there are also communities of Brittonic language speakers in England, France, and Y Wladfa (the Welsh settlement in Patagonia).\n\nThe names \"Brittonic\" and \"Brythonic\" are scholarly conventions referring to the Celtic languages of Britain and to the ancestral language they originated from, designated Common Brittonic, in contrast to the Goidelic languages originating in Ireland. Both were created in the 19th century to avoid the ambiguity of earlier terms such as \"British\" and \"Cymric\". \"Brythonic\" was coined in 1879 by the Celticist John Rhys from the Welsh word \"Brython\". \"Brittonic\", derived from \"Briton\" and also earlier spelled \"Britonic\" and \"Britonnic\", emerged later in the 19th century. It became more prominent through the 20th century, and was used in Kenneth H. Jackson's highly influential 1953 work on the topic, \"Language and History in Early Britain\". Jackson noted that by that time \"Brythonic\" had become a dated term, and that \"of late there has been an increasing tendency to use Brittonic instead.\" Today, \"Brittonic\" often replaces \"Brythonic\" in the literature. Rudolf Thurneysen used \"Britannic\" in his influential \"A Grammar of Old Irish\", though this never became popular among subsequent scholars.\n\nComparable historical terms include the Medieval Latin \"lingua Britannica\" and \"sermo Britannicus\" and the Welsh \"Brythoneg\". Some writers use \"British\" for the language and its descendants, though due to the risk of confusion, others avoid it or use it only in a restricted sense. Jackson, and later John T. Koch, use \"British\" only for the early phase of the Common Brittonic language.\n\nPrior to Jackson's work, \"Brittonic\" (and \"Brythonic\") were often used for all the P-Celtic languages, including not just the varieties in Britain but those Continental Celtic languages that similarly experienced the evolution of the Proto-Celtic language element to . However, subsequent writers have tended to follow Jackson's scheme, rendering this use obsolete.\n\nKnowledge of the Brittonic languages comes from a variety of sources. For the early languages information is obtained from coins, inscriptions and comments by classical writers as well as place names and personal names recorded by them. For later languages there is information from medieval writers and modern native speakers, together with place names. The names recorded in the Roman period are given in Rivet and Smith.\n\nThe Brittonic branch is also referred to as P-Celtic because linguistic reconstruction of the Brittonic reflex of the Proto-Indo-European phoneme *\"kʷ\" is \"p\" as opposed to Goidelic \"c\". Such nomenclature usually implies an acceptance of the P-Celtic and Q-Celtic hypothesis rather than the Insular Celtic hypothesis because the term includes certain Continental Celtic languages as well. (For a discussion, see Celtic languages.)\n\nOther major characteristics include:\nInitial \"s-\":\nLenition:\nVoiceless spirants:\nNasal assimilation:\n\nThe family tree of the Brittonic languages is as follows:\n\n\nBrittonic languages in use today are Welsh, Cornish and Breton. Welsh and Breton have been spoken continuously since they formed. For all practical purposes Cornish died out during the 18th or 19th centuries, but a revival movement has more recently created small numbers of new speakers. Also notable are the extinct language Cumbric, and possibly the extinct Pictish although this may be best considered to be a sister of the Brittonic languages. The late Kenneth H. Jackson argued during the 1950s, from some of the few remaining examples of stone inscriptions, that the Picts may have also used a non-Indo-European language, but some modern scholars of Pictish do not agree.\n\nThe modern Brittonic languages are generally considered to all derive from a common ancestral language termed \"Brittonic\", \"British\", \"Common Brittonic\", \"Old Brittonic\" or \"Proto-Brittonic\", which is thought to have developed from Proto-Celtic or early Insular Celtic by the 6th century BC.\n\nBrittonic languages were probably spoken prior to the Roman invasion at least in the majority of Great Britain south of the rivers Forth and Clyde, though the Isle of Man later had a Goidelic language, Manx. Northern Scotland mainly spoke Pritennic, which became the Pictish language, which may have been a Brittonic language like that of its neighbors. The theory has been advanced (notably by T. F. O'Rahilly) that part of Ireland spoke a Brittonic language, usually termed \"Ivernic\", before it was displaced by Primitive Irish, although the authors Dillon and Chadwick reject this theory as being implausible.\n\nDuring the period of the Roman occupation of what are now England and Wales (AD 43 to c. 410), Common Brittonic borrowed a large stock of Latin words, both for concepts unfamiliar in the pre-urban society of Celtic Britain such as urbanisation and new tactics of warfare as well as for rather more mundane words which displaced native terms (most notably, the word for \"fish\" in all the Brittonic languages derives from the Latin \"piscis\" rather than the native *\"ēskos\" - which may survive, however, in the Welsh name of the River Usk, ). Approximately 800 of these Latin loan-words have survived in the three modern Brittonic languages.\n\nIt is probable that at the start of the Post-Roman period \"Common Brittonic\" was differentiated into at least two major dialect groups – Southwestern and Western (in addition we may posit additional dialects, such as Eastern Brittonic, spoken in what is now the East of England, which have left little or no evidence). Between the end of the Roman occupation and the mid 6th century the two dialects began to diverge into recognisably separate languages, the Western into Cumbric and Welsh and the Southwestern into Cornish and its closely related sister language Breton, which was carried to continental Armorica. Jackson showed that a few of the dialect distinctions between West and Southwest Brittonic go back a long way. New divergencies began around AD 500 but other changes which were shared occurred in the 6th century. Other common changes occurred in the 7th century onward and are possibly due to inherent tendencies. Thus the concept of a common Brittonic language ends by AD 600. Substantial numbers of Britons certainly remained in the expanding area controlled by Anglo-Saxons, but over the fifth and sixth centuries they mostly adopted the English language.\n\nThe Brittonic languages spoken in what is now Scotland, the Isle of Man and what is now England began to be displaced in the 5th century through the settlement of Irish-speaking Gaels and Germanic peoples. The displacement of the languages of Brittonic descent was probably complete in all of Britain except Cornwall and Wales and the English counties bordering these areas such as Devon by the 11th century. Western Herefordshire continued to speak Welsh until the late nineteenth century, and isolated pockets of Shropshire speak Welsh today.\n\nThe regular consonantal sound changes from Proto-Celtic to Welsh, Cornish and Breton are summarised in the following table. Where the graphemes have a different value from the corresponding IPA symbols, the IPA equivalent is indicated between slashes. V represents a vowel; C represents a consonant.\n\nThe principal legacy left behind in those territories from which the Brittonic languages were displaced is that of toponyms (place names) and hydronyms (river names). There are many Brittonic place names in lowland Scotland and in the parts of England where it is agreed that substantial Brittonic speakers remained (Brittonic names, apart from those of the former Romano-British towns, are scarce over most of England). Names derived (sometimes indirectly) from Brittonic include London, Penicuik, Perth, Aberdeen, York, Dorchester, Dover and Colchester. Brittonic elements found in England include \"bre-\" and \"bal-\" for hills, while some such as combe or coomb(e) for a small deep valley and tor for a hill are examples of Brittonic words that were borrowed into English. Others reflect the presence of Britons such as Dumbarton – from the Scottish Gaelic \"Dùn Breatainn\" meaning \"Fort of the Britons\", or Walton meaning a \"tun\" or settlement where the \"Wealh\" \"Britons\" still lived.\n\nThe number of Celtic river names in England generally increases from east to west, a map showing these being given by Jackson. These names include ones such as Avon, Chew, Frome, Axe, Brue and Exe, but also river names containing the elements \"der-/dar-/dur-\" and \"-went\" e.g. \"Derwent, Darwen, Deer, Adur, Dour, Darent, Went\". In fact these names exhibit multiple different Celtic roots. One is *dubri- \"water\" [Bret. \"dour\", C. \"dowr\", W. \"dŵr\"], also found in the place-name \"Dover\" (attested in the Roman period as \"Dubrīs\"); this is the original source of rivers named \"Dour\". Another is *deru̯o- \"oak\" or \"true\" [Bret. \"derv\", C. \"derow\", W. \"derw\"), coupled with two agent suffixes, *-ent- and *-iū; this is the origin of \"Derwent\", \" Darent\" and \"Darwen\" (attested in the Roman period as \"Deru̯entiō\"). The final root to be examined is \"went\". In Roman Britain there were three tribal capitals named \"U̯entā\" (modern Winchester, Caerwent and Caistor St Edmunds), whose meaning was 'place, town'.\n\nSome, including J. R. R. Tolkien have argued that Celtic has acted as a substrate to English for both the lexicon and syntax. It is generally accepted that linguistic effects on English were lexically rather poor aside from toponyms, consisting of a few domestic words, which may include hubbub, dad, peat, bucket, crock, crumpet (cf. Br. \"krampouz\"), noggin, gob (cf. Gaelic \"gob\"), nook; and the dialectal term for a badger, i.e. \"brock\" (cf. Welsh \"broch\", C. \"brogh\" and Gaelic \"broc\"). Another legacy may be the sheep-counting system Yan Tan Tethera in the west, in the traditionally Celtic areas of England such as Cumbria. Several Cornish mining words are still in use in English language mining terminology, such as costean, gunnies, and vug.\n\nThose who argue against the theory of a Brittonic substratum and heavy influence point out that many toponyms have no semantic continuation from the Brittonic language. A notable example is \"Avon\" which comes from the Celtic term for river \"abona\" or the Welsh term for river \"afon\" but was used by the English as a personal name. Likewise the River Ouse, Yorkshire contains the word \"usa\" which merely means water and the name of the river Trent simply comes from the Welsh word for a trepasser (\"an over-flowing river\")\nIt has been argued that the use of periphrastic constructions (using auxiliary verbs like \"do\" and \"be\" in the continuous/progressive) in the English verb, which is more widespread than in the other Germanic languages, is traceable to Brittonic influence. Some however find this very unlikely and claim a native English development rather than Celtic influence, though Roberts postulates Northern Germanic influence, despite such constructions not existing in Norse. Literary Welsh has the simple present \"Caraf\" = \"I love\" and the present stative (al. continuous/progressive) \"Yr wyf yn caru\" = \"I am loving\", where the Brittonic syntax is partly mirrored in English (Note that \"I am loving\" comes from older \"I am a-loving\", from still older \"ich am on luvende\" \"I am in the process of loving\"). In the Germanic sister languages of English there is only one form, for example \"ich liebe\" in German, though in \"colloquial\" German, a progressive aspect form has evolved which is formally similar to those found in Celtic languages, and somewhat less similar to the Modern English form, e.g. \"I am working\" is \"ich bin am Arbeiten\", literally: \"I am on the working\". The same structure is also found in modern Dutch (\"ik ben aan het werk\"), alongside other structures (e.g. \"ik zit te werken\", lit. \"I work sitting\"). These parallel developments suggest that the English progressive is not necessarily due to Celtic influence; moreover, the native English development of the structure can be traced over the over 1000 years of English literature.\n\nSome researchers (Filppula \"et al.\", 2001) argue that English syntax reflects more extensive Brittonic influences. For instance, in English tag questions, the form of the tag depends on the verb form in the main statement (\"aren't I?\", \"isn't he?\", \"won't we?\" etc.). The German \"nicht wahr?\" and the French \"n'est-ce pas?\", by contrast, are fixed forms which can be used with almost any main statement. It has been claimed that the English system has been borrowed from Brittonic, since Welsh tag questions vary in almost exactly the same way. However, as these are fairly late developments in English and even later in Welsh and Gaelic, it is more probable that the Celtic languages borrowed the structure from English.\n\nFar more notable, but less well known, are Brittonic influences on Scottish Gaelic, though Scottish and Irish Gaelic, with their wider range of preposition-based periphastic constructions, suggest that such constructions descend from their common Celtic heritage. Scottish Gaelic contains a number of apparently P-Celtic loanwords, but as there is a far greater overlap in terms of Celtic vocabulary, than with English, it is not always possible to disentangle P- and Q-Celtic words. However some common words such as \"monadh\" = Welsh \"mynydd\" Cumbric \"*monidh\" are particularly evident.\n\nOften the Brittonic influence on Scots Gaelic is indicated by considering Irish language usage, which is not likely to have been influenced so much by Brittonic. In particular, the word \"srath\" (Anglicised as \"Strath\") is a native Goidelic word, but its usage appears to have been modified by the Brittonic cognate \"ystrad\" whose meaning is slightly different. The effect on Irish has been the loan from British of many Latin-derived words. This has been associated with the Christianisation of Ireland from Britain.\n\n"}
{"id": "4071", "url": "https://en.wikipedia.org/wiki?curid=4071", "title": "Bronski Beat", "text": "Bronski Beat\n\nBronski Beat were a popular British synthpop trio who achieved success in the mid-1980s, particularly with the 1984 chart hit \"Smalltown Boy\". All members of the group were openly gay and their songs reflected this, often containing political commentary on gay-related issues. The initial line-up, which recorded the majority of the band's hits, consisted of Jimmy Somerville (vocals), Steve Bronski (keyboards, percussion) and Larry Steinbachek (keyboards, percussion). Somerville left Bronski Beat in 1985, and went on to have success as lead singer of The Communards and as a solo artist. He was replaced by new vocalist John Foster, with whom the band continued to have hits in the UK and Europe through 1986. Foster left Bronski Beat after their second album, and the band used a series of vocalists before dissolving in 1996.\n\nSteve Bronski, the only original member, now leads a revived touring version of the band, and is recording new material for release in 2017 alongside '90s member Ian Donaldson.\n\nBronski Beat formed in 1983 when Somerville, Bronski (both from Glasgow), and Steinbachek (from Southend) shared a three-bedroom flat at Lancaster House in Brixton. Steinbachek had heard Somerville singing during the making of \"\" and suggested they make some music. They first performed publicly at an arts festival \"September in the Pink\". The trio were unhappy with the inoffensive nature of contemporary gay performers and sought to be more outspoken and political.\n\nBronski Beat signed a recording contract with London Records in 1984 after doing only nine live gigs. The band's debut single, \"Smalltown Boy\" (about a gay teenager leaving his family and fleeing his hometown) was a hit, peaking at No 3 in the UK Singles Chart, and topping charts in Belgium and the Netherlands. The single was accompanied by a promotional video directed by Bernard Rose, showing Somerville trying to befriend an attractive diver at a swimming pool, then being attacked by the diver's homophobic associates, being returned to his family by the police and having to leave home. (The police officer was played by Colin Bell, then the marketing manager of London Records). \"Smalltown Boy\" reached #48 in the U.S. chart and peaked at #7 in Australia. \n\nThe follow-up single, \"Why?\", adopted a Hi-NRG sound and was more lyrically focused on anti-gay prejudice. It also achieved Top 10 status in the UK, reaching #6, and was another Top 10 hit for the band in Australia, Switzerland, Germany, France and the Netherlands.\n\nAt the end of 1984, the trio released an album titled \"The Age of Consent\". The inner sleeve listed the varying ages of consent for consensual gay sex in different nations around the world. At the time, the age of consent for sexual acts between men in the UK was 21 compared with 16 for heterosexual acts, with several other countries having more liberal laws on gay sex. The album peaked at #4 in the UK Albums Chart, #36 in the U.S., and #12 in Australia.\n\nAround the same time, the band headlined \"Pits and Perverts\", a concert at the Electric Ballroom in London to raise funds for the Lesbians and Gays Support the Miners campaign. This event is featured in the film \"Pride\".\n\nThe third single, released before Christmas 1984, was a revival of \"It Ain't Necessarily So\", the George and Ira Gershwin classic (from \"Porgy and Bess\"). The song questions the authenticity of biblical tales. It also reached the UK Top 20.\n\nIn 1985, the trio joined up with Marc Almond to record a version of Donna Summer's \"I Feel Love\". The full version was actually a medley that also incorporated snippets of Summer's \"Love to Love You Baby\" and John Leyton's \"Johnny Remember Me\". It was a big success, reaching #3 in the UK and equalling the chart achievement of \"Smalltown Boy\". Although the original had been one of Marc Almond’s all-time favourite songs, he had never read the lyrics and thus incorrectly sang \"What’ll it be, what’ll it be, you and me\" instead of \"Falling free, falling free, falling free\" on the finished record.\n\nThe band and their producer Mike Thorne had gone back into the studio in early 1985 to record a new single, \"Run From Love\", and PolyGram (London Records' parent company at that time) had pressed a number of promo singles and 12\" versions of the song and sent them to radio and record stores in the UK. However, the single was shelved as tensions in the band, both personal and political, resulted in Somerville leaving Bronski Beat in the summer of that year.\n\n\"Run From Love\" was subsequently released in a remix form on the Bronski Beat album \"Hundreds & Thousands\", a collection of mostly remixes (LP) and B-sides (as bonus tracks on the CD version) as well as the hit \"I Feel Love\". Somerville went on to form The Communards with Richard Coles while the remaining members of Bronski Beat searched for a new vocalist.\n\nBronski Beat recruited John Foster as Somerville's replacement (Foster is credited as \"Jon Jon\"). A single, \"Hit That Perfect Beat\", was released in November 1985, reaching #3 in the UK. It repeated this success on the Australian chart and was also featured in the film \"Letter to Brezhnev\". A second single, \"C'mon C'mon\", also charted in the UK Top 20 and an album, \"Truthdare Doubledare\", released in May 1986, peaked at #18. The film \"Parting Glances\" (1986) included Bronski Beat songs \"Love and Money\", \"Smalltown Boy\" and \"Why?\". During this period, the band teamed up with producer Mark Cunningham on the first-ever BBC Children In Need single, a cover of David Bowie's \"Heroes\", released in 1986 under the name of The County Line.\n\nFoster left the band in 1987. Following Foster's departure, Bronski Beat began work on their next album, \"Out and About\". The tracks were recorded at Berry Street studios in London with engineer Brian Pugsley. Some of the song titles were \"The Final Spin\" and \"Peace And Love\". The latter track featured Strawberry Switchblade vocalist Rose McDowall and appeared on several internet sites in 2006. One of the other songs from the project called \"European Boy\" was recorded in 1987 by disco group Splash. The lead singer of Splash was former Tight Fit singer Steve Grant. Steinbachek and Bronski toured extensively with the new material and got great reviews, however the project was abandoned as the group were dropped by London Records. Also in 1987, Bronski Beat and Somerville performed at a reunion concert for \"International AIDS Day\", supported by New Order, at the Brixton Academy, London.\n\nIn 1989, Jonathan Hellyer became lead singer, and the band extensively toured the U.S. and Europe with back-up vocalist Annie Conway. They achieved one minor hit with the song \"Cha Cha Heels\", a one-off collaboration sung by American actress and singer Eartha Kitt, which peaked at #32 in the UK. The song was originally written for movie and recording star Divine, who was unable to record the song before his death in 1988. 1990–91 saw Bronski Beat release three further singles on the Zomba record label, \"I'm Gonna Run Away\", \"One More Chance\" and \"What More Can I Say\". The singles were produced by Mike Thorne.\n\nFoster and Bronski Beat teamed up again in 1994, and released a techno \"Tell Me Why '94\" and an acoustic \"Smalltown Boy '94\" on the German record label, ZYX Music. The album \"Rainbow Nation\" was released the following year with Hellyer returning as lead vocalist, as Foster had dropped out of the project and Ian Donaldson was brought onboard to do keyboards and programming. After a few years of touring the world Bronski Beat then dissolved, with Steve Bronski going on to become a producer for other artists and Ian Donaldson becoming a successful DJ (Sordid Soundz). Larry Steinbachek became the musical director for Michael Laub's theatre company, 'Remote Control Productions'.\n\nIn 2007, Bronski remixed the song \"Stranger to None\" by the UK alternative rock band, All Living Fear. Four different mixes were done, with one appearing on their retrospective album, \"Fifteen Years After\". Bronski also remixed the track \"Flowers in the Morning\" by Northern Irish electronic band Electrobronze in 2007, changing the style of the song from classical to Hi-NRG disco.\n\nIn 2016, Steve Bronski teamed up once again with Ian Donaldson to bring Bronski Beat back and enlisted a new singer, Stephen Granville. They are currently reworking the old songs as well as new material, to be released in 2017.\n\nOn 12 January 2017, Larry Steinbachek's sister Louise Jones told BBC News he had died the previous month after a short battle with cancer, with his family and friends at his bedside.\n\nThe original member set of Bronski Beat was Jimmy Somerville (vocals), Steve Bronski and Larry Steinbachek (keyboards). Following Somerville leaving to form pop group The Communards with Richard Coles, he was replaced by John Foster and later by Jonathan Hellyer. The band set-up has seen a number of changes.\n\n\n\n"}
{"id": "4074", "url": "https://en.wikipedia.org/wiki?curid=4074", "title": "Barrel (disambiguation)", "text": "Barrel (disambiguation)\n\nA barrel is a cylindrical container, traditionally made with wooden material.\n\nBarrel may also refer to:\n\n\n"}
{"id": "4077", "url": "https://en.wikipedia.org/wiki?curid=4077", "title": "Binary prefix", "text": "Binary prefix\n\nA binary prefix is a unit prefix for multiples of units in data processing, data transmission, and digital information, notably the bit and the byte, to indicate multiplication by a power of 2.\n\nThe computer industry has historically used the units \"kilobyte\", \"megabyte\", and \"gigabyte\", and the corresponding symbols KB, MB, and GB, in at least two slightly different measurement systems. In citations of main memory (RAM) capacity, \"gigabyte\" customarily means bytes. As this is a power of 1024, and 1024 is a power of two (2), this usage is referred to as a binary measurement.\n\nIn most other contexts, the industry uses the multipliers \"kilo\", \"mega\", \"giga\", etc., in a manner consistent with their meaning in the International System of Units (SI), namely as powers of 1000. For example, a 500 gigabyte hard disk holds bytes, and a 1 Gbit/s (gigabit-per-second) Ethernet connection transfers data at bit/s. In contrast with the \"binary prefix\" usage, this use is described as a \"decimal prefix\", as 1000 is a power of 10 (10).\n\nThe use of the same unit prefixes with two different meanings has caused confusion. Starting around 1998, the International Electrotechnical Commission (IEC) and several other standards and trade organizations addressed the ambiguity by publishing standards and recommendations for a set of binary prefixes that refer exclusively to powers of 1024. Accordingly, the US National Institute of Standards and Technology (NIST) requires that SI prefixes only be used in the decimal sense: kilobyte and megabyte denote one thousand bytes and one million bytes respectively (consistent with SI), while new terms such as kibibyte, mebibyte and gibibyte, having the symbols KiB, MiB, and GiB, denote 1024 bytes, bytes, and bytes, respectively. In 2008, the IEC prefixes were incorporated into the ISO/IEC 80000 standard.\n\nEarly computers used one of two addressing methods to access the system memory; binary (base 2) or decimal (base 10).\nFor example, the IBM 701 (1952) used binary and could address 2048 words of 36 bits each, while the IBM 702 (1953) used decimal and could address ten thousand 7-bit words.\n\nBy the mid-1960s, binary addressing had become the standard architecture in most computer designs, and main memory sizes were most commonly powers of two. This is the most natural configuration for memory, as all combinations of their address lines map to a valid address, allowing easy aggregation into a larger block of memory with contiguous addresses.\n\nEarly computer system documentation would specify the memory size with an exact number such as 4096, 8192, or 16384 words of storage. These are all powers of two, and furthermore are small multiples of 2, or 1024. As storage capacities increased, several different methods were developed to abbreviate these quantities.\n\nThe method most commonly used today uses prefixes such as kilo, mega, giga, and corresponding symbols K, M, and G, which the computer industry originally adopted from the metric system. The prefixes \"kilo-\" and \"mega-\", meaning 1000 and respectively, were commonly used in the electronics industry before World War II. \nAlong with \"giga-\" or G-, meaning , they are now known as SI prefixes after the International System of Units (SI), introduced in 1960 to formalize aspects of the metric system. \n\nThe International System of Units does not define units for digital information but notes that the SI prefixes may be applied outside the contexts where base units or derived units would be used. But as computer main memory in a \nbinary-addressed system is manufactured in sizes that were easily expressed as multiples of 1024, \"kilobyte\", when applied to computer memory, came to be used to mean 1024 bytes instead of 1000. This usage is not consistent with the SI. Compliance with the SI requires that the prefixes take their 1000-based meaning, and that they are not to be used as placeholders for other numbers, like 1024.\n\nThe use of K in the binary sense as in a \"32K core\" meaning words, i.e., words, can be found as early as 1959.\nGene Amdahl's seminal 1964 article on IBM System/360 used \"1K\" to mean 1024.\nThis style was used by other computer vendors, the CDC 7600 \"System Description\" (1968) made extensive use of K as 1024.\nThus the first binary prefix was born.\n\nAnother style was to truncate the last three digits and append K, essentially using K as a decimal prefix similar to SI, but always truncating to the next lower whole number instead of rounding to the nearest. The exact values words, words and words would then be described as \"32K\", \"65K\" and \"131K\".\nThis style was used from about 1965 to 1975.\n\nThese two styles (K = 1024 and truncation) were used loosely around the same time, sometimes by the same company. In discussions of binary-addressed memories, the exact size was evident from context. (For memory sizes of \"41K\" and below, there is no difference between the two styles.) The HP 21MX real-time computer (1974) denoted (which is 192×1024) as \"196K\" and as \"1M\",\nwhile the HP 3000 business computer (1973) could have \"64K\", \"96K\", or \"128K\" bytes of memory.\n\nThe \"truncation\" method gradually waned. Capitalization of the letter K became the \"de facto\" standard for binary notation, although this could not be extended to higher powers, and use of the lowercase k did persist. Nevertheless, the practice of using the SI-inspired \"kilo\" to indicate 1024 was later extended to \"megabyte\" meaning 1024 () bytes, and later \"gigabyte\" for 1024 () bytes. For example, a \"512 megabyte\" RAM module is 512×1024 bytes (512 × , or ), rather than .\n\nThe symbols Kbit, Kbyte, Mbit and Mbyte started to be used as \"binary units\"—\"bit\" or \"byte\" with a multiplier that is a power of 1024—in the early 1970s.\nFor a time, memory capacities were often expressed in K, even when M could have been used: The IBM System/370 Model 158 brochure (1972) had the following: \"Real storage capacity is available in 512K increments ranging from 512K to 2,048K bytes.\"\n\nMegabyte was used to describe the 22-bit addressing of DEC PDP-11/70 (1975)\nand gigabyte the 30-bit addressing DEC VAX-11/780 (1977).\n\nIn 1998, the International Electrotechnical Commission IEC introduced the binary prefixes kibi, mebi, gibi ... to mean 1024, 1024, 1024 etc., so that 1048576 bytes could be referred to unambiguously as 1 mebibyte. The IEC prefixes were defined for use alongside the International System of Quantities (ISQ) in 2009.\n\nThe disk drive industry followed a different pattern. Industry practice, more thoroughly documented at Timeline of binary prefixes and continuing today, is generally to specify hard drives using prefixes and symbols with decimal meaning as described by SI. Unlike computer main memory, there is nothing in a disk drive that causes, or even influences, the capacity to be an integer multiple of a power of 1024. For example, the first commercially sold disk drive, the IBM 350, had 50 (not 32 or 64) physical disk \"platters\" containing a total of 50,000 sectors of 100 characters each, for a total quoted capacity of \"5 million characters.\" It was introduced in September 1956. \n\nIn the 1960s most disk drives used IBM's variable block length format (called Count Key Data or \"CKD\").\nAny block size could be specified up to the maximum track length. Since the block headers occupied space, the usable capacity of the drive was dependent on the block size. Blocks (\"records\" in IBM's terminology) of 88, 96, 880 and 960 were often used because they related to the fixed block size of 80- and 96-character punch cards. The drive capacity was usually stated under conditions of full track record blocking. For example, the 100-megabyte 3336 disk pack only achieved that capacity with a full track block size of 13,030 bytes.\n\nFloppy disks for the IBM PC and compatibles quickly standardized on 512-byte sectors, so two sectors were easily referred to as \"1K\". The 3.5-inch \"360 KB\" and \"720 KB\" had 720 (single-sided) and 1440 sectors (double-sided) respectively. When the High Density \"1.44 MB\" floppies came along, with 2880 of these 512-byte sectors, that terminology represented a hybrid binary-decimal definition of \"1 MB\" = 2 x 10 =1 024 000 bytes.\n\nIn contrast, hard disk drive manufacturers used \"megabytes\" or \"MB\", meaning 10 bytes, to characterize their products as early as 1974. By 1977, in its first edition, Disk/Trend, a leading hard disk drive industry marketing consultancy segmented the industry according to MBs (decimal sense) of capacity.\n\nOne of the earliest hard disk drives in personal computing history,\nthe Seagate ST-412, was specified as \"Formatted: 10.0 Megabytes\". More precisely, the drive contains 4 heads or active surfaces (tracks per cylinder), 306 cylinders, and when formatted with a sector size of 256 bytes and 32 sectors/track results in a capacity of . This drive was one of several types installed into the IBM PC/XT and extensively advertised and reported as a \"10 MB\" (formatted) hard disk drive.\nThe factor of 306 cylinders (rather than 256 or 512, both of which are powers of two) in the calculation causes the capacity to be not conveniently close to a power of 1024; operating systems and programs using the customary binary prefixes show this as \"9.5625 MB\". Many later drives in the personal computer market used 17 sectors per track; still later, zone bit recording was introduced, causing the number of sectors per track to vary from the outer track to the inner. Nor are drives required to have a number of active surfaces that is a power of, or even divisible by, two; drives with e.g. three active surfaces are not uncommon. All of these factors reduce the utility of the customary binary prefixes for expressing drive capacity. \n\nToday, the hard drive industry continues to use decimal prefixes for drive capacity (as well as for transfer rate). For example, a \"300 GB\" hard drive offers slightly more than 300×10, or , bytes, not (which would be about ). Operating systems such as Microsoft Windows that display hard drive sizes using the customary binary prefix \"GB\" (as it is used for RAM) would display this as \"279.4 GB\" (meaning bytes, or ). On the other hand, Mac OS X has since version 10.6 shown hard drive size using decimal prefixes (thus matching the drive makers' packaging). (Previous versions of Mac OS used binary prefixes.)\n\nHowever, other usages still occur. For example, in one document, Seagate specifies data transfer rates of some of its hard drives in \"both\" IEC and decimal units. \n\"Advanced Format\" drives using 4096-byte sectors are described as having \"4K sectors.\"\n\nComputer clock frequencies are always quoted using SI prefixes in their decimal sense. For example, the internal clock frequency of the original IBM PC was 4.77 MHz, that is .\nSimilarly, digital information transfer rates are quoted using decimal prefixes:\n\nBy the mid-1970s it was common to see K meaning 1024 and the occasional M meaning for words or bytes of main memory (RAM) while K and M were commonly used with their decimal meaning for disk storage. In the 1980s, as capacities of both types of devices increased, the SI prefix G, with SI meaning, was commonly applied to disk storage, while M in its binary meaning, became common for computer memory. In the 1990s, the prefix G, in its binary meaning, became commonly used for computer memory capacity. The first terabyte (SI prefix, bytes) hard disk drive was introduced in 2007.\n\nThe dual usage of the kilo (K), mega (M), and giga (G) prefixes as both powers of 1000 and powers of 1024 has been recorded in standards and dictionaries. For example, the 1986 ANSI/IEEE Std 1084-1986\ndefined dual uses for kilo and mega.\n\nMany dictionaries have noted the practice of using traditional prefixes to indicate binary multiples.\nOxford online dictionary defines, for example, megabyte as: \"Computing: a unit of information equal to one million or (strictly) bytes.\"\n\nThe units Kbyte, Mbyte, and Gbyte are found in the trade press and in IEEE journals. Gigabyte was formally defined in IEEE Std 610.10-1994 as either or 2 bytes.\nKilobyte, Kbyte, and KB are equivalent units and all are defined in the obsolete standard, IEEE 100-2000.\n\nThe hardware industry measures system memory (RAM) using the binary meaning while magnetic disk storage uses the SI definition. However, many exceptions exist. Labeling of diskettes uses the megabyte to denote 1024×1000 bytes. In the optical disks market, Compact Disks use \"MB\" to mean 1024 bytes while DVDs use \"GB\" to mean 1000 bytes.\n\nComputer storage has become cheaper per unit and thereby larger, by many orders of magnitude since \"K\" was first used to mean 1024. \nBecause both the SI and \"binary\" meanings of kilo, mega, etc., are based on powers of 1000 or 1024 rather than simple multiples, the difference between 1M \"binary\" and 1M \"decimal\" is proportionally larger than that between 1K \"binary\" and 1k \"decimal,\" and so on up the scale.\nThe relative difference between the values in the binary and decimal interpretations increases, when using the SI prefixes as the base, from 2.4% for kilo to nearly 21% for the yotta prefix.\n\nIn the early days of computers (roughly, prior to the advent of personal computers) there was little or no consumer confusion because of the technical sophistication of the buyers and their familiarity with the products. In addition, it was common for computer manufacturers to specify their products with capacities in full precision.\n\nIn the personal computing era, one source of consumer confusion is the difference in the way many operating systems display hard drive sizes, compared to the way hard drive manufacturers describe them. Hard drives are specified and sold using \"GB\" and \"TB\" in their decimal meaning: one billion and one trillion bytes. Many operating systems and other software, however, display hard drive and file sizes using \"MB\", \"GB\" or other SI-looking prefixes in their binary sense, just as they do for displays of RAM capacity. For example, many such systems display a hard drive marketed as \"160 GB\" as \"149.05 GB\". The earliest known presentation of hard disk drive capacity by an operating system using \"KB\" or \"MB\" in a binary sense is 1984; earlier operating systems generally presented the hard disk drive capacity as an exact number of bytes, with no prefix of any sort, for example, in the output of the MS-DOS or PC DOS CHKDSK command.\n\nThe different interpretations of disk size prefixes has led to three significant class action lawsuits against digital storage manufacturers.\nOne case involved flash memory and the other two involved hard disk drives.\nTwo of these were settled with the manufacturers admitting no wrongdoing but agreeing to clarify the storage capacity of their products on the consumer packaging.\nFlash memory and hard disk manufacturers now have disclaimers on their packaging and web sites clarifying the formatted capacity of the devices\nor defining MB as 1 million bytes and 1 GB as 1 billion bytes.\n\nOn 20 February 2004, Willem Vroegh filed a lawsuit against Lexar Media, Dane–Elec Memory, Fuji Photo Film USA, Eastman Kodak Company, Kingston Technology Company, Inc., Memorex Products, Inc.; PNY Technologies Inc., SanDisk Corporation, Verbatim Corporation, and Viking Interworks alleging that their descriptions of the capacity of their flash memory cards were false and misleading.\n\nVroegh claimed that a 256 MB Flash Memory Device had only 244 MB of accessible memory. \"Plaintiffs allege that Defendants marketed the memory capacity of their products by assuming that one megabyte equals one million bytes and one gigabyte equals one billion bytes.\"\nThe plaintiffs wanted the defendants to use the traditional values of 1024 for megabyte and 1024 for gigabyte.\nThe plaintiffs acknowledged that the IEC and IEEE standards define a MB as one million bytes but stated that the industry has largely ignored the IEC standards.\n\nThe manufacturers agreed to clarify the flash memory card capacity on the packaging and web sites. The consumers could apply for \"a discount of ten percent off a future online purchase from Defendants' Online Stores Flash Memory Device\".\n\nOn 7 July 2005, an action entitled \"Orin Safier v. Western Digital Corporation, et al.\" was filed in the Superior Court for the City and County of San Francisco, Case No. CGC-05-442812.\nThe case was subsequently moved to the Northern District of California, Case No. 05-03353 BZ.\n\nAlthough Western Digital maintained that their usage of units is consistent with \"the indisputably correct industry standard for measuring and describing storage capacity\", and that they \"cannot be expected to reform the software industry\", they agreed to settle in March 2006 with 14 June 2006 as the Final Approval hearing date.\n\nWestern Digital offered to compensate customers with a free download of backup and recovery software valued at US$30. They also paid $500,000 in fees and expenses to San Francisco lawyers Adam Gutride and Seth Safier, who filed the suit.\nThe settlement called for Western Digital to add a disclaimer to their later packaging and advertising.\n\nA lawsuit (Cho v. Seagate Technology (US) Holdings, Inc., San Francisco Superior Court, Case No. CGC-06-453195) was filed against Seagate Technology, alleging that Seagate overrepresented the amount of usable storage by 7% on hard drives sold between March 22, 2001 and September 26, 2007. The case was settled without Seagate admitting wrongdoing, but agreeing to supply those purchasers with free backup software or a 5% refund on the cost of the drives.\n\nWhile early computer scientists typically used k to mean 1000, some recognized the convenience that would result from working with multiples of 1024 and the confusion that resulted from using the same prefixes for two different meanings.\n\nSeveral proposals for unique binary prefixes were made in 1968. Donald Morrison proposed to use the Greek letter kappa (κ) to denote 1024, κ to denote 1024, and so on.\nWallace Givens responded with a proposal to use bK as an abbreviation for 1024 and bK2 or bK for 1024, though he noted that neither the Greek letter nor lowercase letter b would be easy to reproduce on computer printers of the day.\nBruce Alan Martin of Brookhaven National Laboratory further proposed that the prefixes be abandoned altogether, and the letter B be used for base-2 exponents, similar to E in decimal scientific notation, to create shorthands like 3B20 for 3×2, a convention still used on some calculators to present binary floating point-numbers today.\n\nNone of these gained much acceptance, and capitalization of the letter K became the \"de facto\" standard for indicating a factor of 1024 instead of 1000, although this could not be extended to higher powers.\n\nAs the discrepancy between the two systems increased in the higher-order powers, more proposals for unique prefixes were made.\nIn 1996, Markus Kuhn proposed a system with \"di\" prefixes, like the \"dikilobyte\" (K₂B or K2B). Donald Knuth, who uses decimal notation like 1 MB = 1000 kB, expressed \"astonishment\" that the IEC proposal was adopted, calling them \"funny-sounding\" and opining that proponents were assuming \"that standards are automatically adopted just because they are there.\" Knuth proposed that the powers of 1024 be designated as \"large kilobytes\" and \"large megabytes\" (abbreviated KKB and MMB, as \"doubling the letter connotes both binary-ness and large-ness\"). Double prefixes were already abolished from SI, however, having a multiplicative meaning (\"MMB\" would be equivalent to \"TB\"), and this proposed usage never gained any traction.\n\nThe set of binary prefixes that were eventually adopted, now referred to as the \"IEC prefixes\", were first proposed by the International Union of Pure and Applied Chemistry's (IUPAC) Interdivisional Committee on Nomenclature and Symbols (IDCNS) in 1995. At that time, it was proposed that the terms kilobyte and megabyte be used only for 10 bytes and 10 bytes, respectively. The new prefixes \"kibi\" (kilobinary), \"mebi\" (megabinary), \"gibi\" (gigabinary) and \"tebi\" (terabinary) were also proposed at the time, and the proposed symbols for the prefixes were kb, Mb, Gb and Tb respectively, rather than Ki, Mi, Gi and Ti. The proposal was not accepted at the time.\n\nThe Institute of Electrical and Electronic Engineers (IEEE) began to collaborate with the International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC) to find acceptable names for binary prefixes. IEC proposed \"kibi\", \"mebi\", \"gibi\" and \"tebi\", with the symbols Ki, Mi, Gi and Ti respectively, in 1996.\n\nThe names for the new prefixes are derived from the original SI prefixes combined with the term \"binary\", but contracted, by taking the first two letters of the SI prefix and \"bi\" from binary. The first letter of each such prefix is therefore identical to the corresponding SI prefixes, except for \"K\", which is used interchangeably with \"k\", whereas in SI, only the lower-case k represents 1000.\n\nThe IEEE decided that their standards would use the prefixes \"kilo\", etc. with their metric definitions, but allowed the binary definitions to be used in an interim period as long as such usage was explicitly pointed out on a case-by-case basis.\n\nIn January 1999, the IEC published the first international standard (IEC 60027-2 Amendment 2) with the new prefixes, extended up to \"pebi\" (Pi) and \"exbi\" (Ei).\n\nThe IEC 60027-2 Amendment 2 also states that the IEC position is the same as that of BIPM (the body that regulates the SI system); the SI prefixes retain their definitions in powers of 1000 and are never used to mean a power of 1024.\n\nIn usage, products and concepts typically described using powers of 1024 would continue to be, but with the new IEC prefixes. For example, a memory module of bytes () would be referred to as 512 MiB or 512 mebibytes instead of 512 MB or 512 megabytes. Conversely, since hard drives have historically been marketed using the SI convention that \"giga\" means , a \"500 GB\" hard drive would still be labeled as such. According to these recommendations, operating systems and other software would also use binary and SI prefixes in the same way, so the purchaser of a \"500 GB\" hard drive would find the operating system reporting either \"500 GB\" or \"466 GiB\", while bytes of RAM would be displayed as \"512 MiB\".\n\nThe second edition of the standard, published in 2000, defined them only up to \"exbi\", but in 2005, the third edition added prefixes \"zebi\" and \"yobi\", thus matching all SI prefixes with binary counterparts.\n\nThe harmonized ISO/IEC IEC 80000-13:2008 standard cancels and replaces subclauses 3.8 and 3.9 of IEC 60027-2:2005 (those defining prefixes for binary multiples). The only significant change is the addition of explicit definitions for some quantities. In 2009, the prefixes kibi-, mebi-, etc. were defined by ISO 80000-1 in their own right, independently of the kibibyte, mebibyte, and so on.\n\nThe BIPM standard JCGM 200:2012 \"International vocabulary of metrology – Basic and general concepts and associated terms (VIM), 3rd edition\" lists the IEC binary prefixes and states \"SI prefixes refer strictly to powers of 10, and should not be used for powers of 2. For example, 1 kilobit should not be used to represent bits (2 bits), which is 1 kibibit.\" \n\nThe IEC standard binary prefixes are now supported by other standardization bodies and technical organizations.\n\nThe United States National Institute of Standards and Technology (NIST) supports the ISO/IEC standards for\n\"Prefixes for binary multiples\" and has a web site documenting them, describing and justifying their use. NIST suggests that in English, the first syllable of the name of the binary-multiple prefix should be pronounced in the same way as the first syllable of the name of the corresponding SI prefix, and that the second syllable should be pronounced as \"bee\". NIST has stated the SI prefixes \"refer strictly to powers of 10\" and that the binary definitions \"should not be used\" for them.\n\nThe microelectronics industry standards body JEDEC describes the IEC prefixes in its online dictionary. The JEDEC standards for semiconductor memory use the customary prefix symbols K, M, G and T in the binary sense.\n\nOn 19 March 2005, the IEEE standard IEEE 1541-2002 (\"Prefixes for Binary Multiples\") was elevated to a full-use standard by the IEEE Standards Association after a two-year trial period. However, , the IEEE Publications division does not require the use of IEC prefixes in its major magazines such as \"Spectrum\" or \"Computer\". \n\nThe International Bureau of Weights and Measures (BIPM), which maintains the International System of Units (SI), expressly prohibits the use of SI prefixes to denote binary multiples, and recommends the use of the IEC prefixes as an alternative since units of information are not included in SI.\n\nThe Society of Automotive Engineers (SAE) prohibits the use of SI prefixes with anything but a power-of-1000 meaning, but does not recommend or otherwise cite the IEC binary prefixes.\n\nThe European Committee for Electrotechnical Standardization (CENELEC) adopted the IEC-recommended binary prefixes via the harmonization document HD 60027-2:2003-03.\nThe European Union (EU) has required the use of the IEC binary prefixes since 2007.\n\nMost computer hardware uses SI prefixes to state capacity and define other performance parameters such as data rate. Main and cache memories are notable exceptions.\n\nCapacities of main memory and cache memory are usually expressed with customary binary prefixes\nOn the other hand, flash memory, like that found in solid state drives, mostly uses SI prefixes to state capacity.\n\nSome operating systems and other software continue to use the customary binary prefixes in displays of memory, disk storage capacity, and file size, but SI prefixes in other areas such as network communication speeds and processor speeds.\n\nIn the following subsections, unless otherwise noted, examples are first given using the common prefixes used in each case, and then followed by interpretation using other notation where appropriate.\n\nPrior to the release of Macintosh System Software (1984), file sizes were typically reported by the operating system without any prefixes. Today, most operating systems report file sizes with prefixes.\n\n\n, most software does not distinguish symbols for binary and decimal prefixes.\nThe IEC binary naming convention has been adopted by a few, but this is not used universally.\n\nOne of the stated goals of the introduction of the IEC prefixes was \"to preserve the SI prefixes as unambiguous decimal multipliers.\" Programs such as fdisk/cfdisk, parted, and apt-get use SI prefixes with their decimal meaning.\n\nExample of the use of IEC binary prefixes in the Linux operating system displaying traffic volume on a network interface in kibibytes (KiB) and mebibytes (MiB), as obtained with the ifconfig utility:\nSoftware that uses standard SI prefixes for powers of 1000, but \"not\" IEC binary prefixes for powers of 1024, includes:\n\n\nSoftware that supports decimal prefixes for powers of 1000 \"and\" binary prefixes for powers of 1024 (but does not follow SI or IEC nomenclature for this) includes:\n\n\nSoftware that uses IEC binary prefixes for powers of 1024 \"and\" uses standard SI prefixes for powers of 1000 includes:\n\n\n\nHardware types that use powers-of-1024 multipliers, such as memory, continue to be marketed with customary binary prefixes.\n\nMeasurements of most types of electronic memory such as RAM and ROM are given using customary binary prefixes (kilo, mega, and giga). This includes some flash memory, like EEPROMs. For example, a \"512-megabyte\" memory module is 512×2 bytes (512 × , or ).\n\nJEDEC Solid State Technology Association, the semiconductor engineering standardization body of the Electronic Industries Alliance (EIA), continues to include the customary binary definitions of kilo, mega and giga in their \"Terms, Definitions, and Letter Symbols\" document,\nand uses those definitions in later memory standards\n\nMany computer programming tasks reference memory in terms of powers of two because of the inherent binary design of current hardware addressing systems. For example, a 16-bit processor register can reference at most 65,536 items (bytes, words, or other objects); this is conveniently expressed as \"64K\" items. An operating system might map memory as 4096-byte pages, in which case exactly 8192 pages could be allocated within bytes of memory: \"8K\" (8192) pages of \"4 kilobytes\" (4096 bytes) each within \"32 megabytes\" (32 MiB) of memory.\n\nAll hard disk drive manufacturers state capacity using SI prefixes.\n\nUSB flash drives, flash-based memory cards like CompactFlash or Secure Digital, and flash-based SSDs use SI prefixes;\nfor example, a \"256 MB\" flash card provides at least 256 million bytes (), not 256×1024×1024 ().\nThe flash memory chips inside these devices contain considerably more than the quoted capacities, but much like a traditional hard drive, some space is reserved for internal functions of the flash drive. These include wear leveling, error correction, sparing, and metadata needed by the device's internal firmware.\n\nFloppy disks have existed in numerous physical and logical formats, and have been sized inconsistently. In part, this is because the end user capacity of a particular disk is a function of the controller hardware, so that the same disk could be formatted to a variety of capacities. In many cases, the media are marketed without any indication of the end user capacity, as for example, DSDD, meaning double-sided double-density.\n\nThe last widely adopted diskette was the 3½-inch high density. This has a formatted capacity of bytes or 1440 KB (1440 × 1024, using \"KB\" in the customary binary sense). These are marketed as \"HD\", or \"1.44 MB\" or both. This usage creates a third definition of \"megabyte\" as 1000×1024 bytes.\n\nMost operating systems display the capacity using \"MB\" in the customary binary sense, resulting in a display of \"1.4 MB\" (). Some users have noticed the missing 0.04 MB and both Apple and Microsoft have support bulletins referring to them as 1.4 MB.\n\nThe earlier \"1200 KB\" (1200×1024 bytes) 5¼-inch diskette sold with the IBM PC AT was marketed as \"1.2 MB\" (). The largest 8-inch diskette formats could contain more than a megabyte, and the capacities of those devices were often irregularly specified in megabytes, also without controversy.\n\nOlder and smaller diskette formats were usually identified as an accurate number of (binary) KB, for example the Apple Disk II described as \"140KB\" had a 140×1024-byte capacity, and the original \"360KB\" double sided, double density disk drive used on the IBM PC had a 360×1024-byte capacity.\n\nIn many cases diskette hardware was marketed based on unformatted capacity, and the overhead required to format sectors on the media would reduce the nominal capacity as well (and this overhead typically varied based on the size of the formatted sectors), leading to more irregularities.\n\nThe capacities of most optical disc storage media like DVD, Blu-ray Disc, HD DVD and magneto-optical (MO) are given using SI decimal prefixes.\nA \"4.7 GB\" DVD has a nominal capacity of about 4.38 GiB. However, CD capacities are always given using customary binary prefixes. Thus a \"700-MB\" (or \"80-minute\") CD has a nominal capacity of about 700 MiB (approx 730 MB).\n\nTape drive and media manufacturers use SI decimal prefixes to identify capacity.\n\nCertain units are always used with SI decimal prefixes even in computing contexts.\nTwo examples are hertz (Hz), which is used to measure the clock rates of electronic components, and bit/s, used to measure data transmission speed.\n\nBus clock speeds and therefore bandwidths are both quoted using SI decimal prefixes.\n\n\nIEC prefixes are used by Toshiba, IBM, HP to advertise or describe some of their products. According to one HP brochure, \"[t]o reduce confusion, vendors are pursuing one of two remedies: they are changing SI prefixes to the new binary prefixes, or they are recalculating the numbers as powers of ten.\" The IBM Data Center also uses IEC prefixes to reduce confusion. The IBM Style Guide reads \"To help avoid inaccuracy (especially with the larger prefixes) and potential ambiguity, the International Electrotechnical Commission (IEC) in 2000 adopted a set of prefixes specifically for binary multipliers (See IEC 60027-2). Their use is now supported by the United States National Institute of Standards and Technology (NIST) and incorporated into ISO 80000. They are also required by EU law and in certain contexts in the US.\n\nHowever, most documentation and products in the industry continue to use SI prefixes when referring to binary multipliers. In product documentation, follow the same standard that is used in the product itself (for example, in the interface or firmware). Whether you choose to use IEC prefixes for powers of 2 and SI prefixes for powers of 10, or use SI prefixes for a dual purpose ... be consistent in your usage and explain to the user your adopted system.\" \n\n\n\n"}
{"id": "4078", "url": "https://en.wikipedia.org/wiki?curid=4078", "title": "National Baseball Hall of Fame and Museum", "text": "National Baseball Hall of Fame and Museum\n\nThe National Baseball Hall of Fame and Museum is an American history museum and hall of fame, located in Cooperstown, New York, and operated by private interests. It serves as the central point for the study of the history of baseball in the United States and beyond, displays baseball-related artifacts and exhibits, and honors those who have excelled in playing, managing, and serving the sport. The Hall's motto is \"Preserving History, Honoring Excellence, Connecting Generations.\"\n\nThe word Cooperstown is often used as shorthand (or a metonym) for the National Baseball Hall of Fame and Museum.\n\nThe Hall of Fame was established in 1939 by Stephen Carlton Clark, the owner of a local hotel. Clark had sought to bring tourists to a city hurt by the Great Depression, which reduced the local tourist trade, and Prohibition, which devastated the local hops industry. A new building was constructed, and the Hall of Fame was dedicated on June 12, 1939. (Clark's granddaughter, Jane Forbes Clark, is the current chairman of the Board of Directors.)\n\nThe erroneous claim that U.S. Civil War hero Abner Doubleday invented baseball in Cooperstown was instrumental in the early marketing of the Hall.\n\nAn expanded library and research facility opened in 1994. Dale Petroskey became the organization's president in 1999.\n\nIn 2002, the Hall launched \"Baseball As America\", a traveling exhibit that toured ten American museums over six years. The Hall of Fame has since also sponsored educational programming on the Internet to bring the Hall of Fame to schoolchildren who might not visit. The Hall and Museum completed a series of renovations in spring 2005. The Hall of Fame also presents an annual exhibit at FanFest at the Major League Baseball All-Star Game.\n\nJeff Idelson replaced Petroskey as president on April 16, 2008. He had been acting as president since March 25, 2008, when Petroskey was forced to resign for having \"failed to exercise proper fiduciary responsibility\" and making \"judgments that were not in the best interest of the National Baseball Hall of Fame and Museum.\"\n\nIn 2012, Congress passed and President Barack Obama signed a law ordering the United States Mint to produce and sell commemorative, non-circulating coins to benefit the private, non-profit Hall. The bill, , was introduced in the United States House of Representatives by Rep. Richard Hanna, a Republican from New York, and passed the House on October 26, 2011. The coins, which depict baseball gloves and balls, are the first concave designs produced by the Mint. The mintage included 50,000 gold coins, 400,000 silver coins, and 750,000 clad (Nickel-Copper) coins. The Mint released them on March 27, 2014, and the gold and silver editions quickly sold out. The Hall receives money from surcharges included in the sale price: a total of $9.5 million if all the coins are sold.\n\nAmong baseball fans, \"Hall of Fame\" means not only the museum and facility in Cooperstown, New York, but the pantheon of players, managers, umpires, executives, and pioneers who have been enshrined in the Hall. The first five men elected were Ty Cobb, Babe Ruth, Honus Wagner, Christy Mathewson and Walter Johnson, chosen in 1936; roughly 20 more were selected before the entire group was inducted at the Hall's 1939 opening. , 317 people had been elected to the Hall of Fame, including 220 former Major League Baseball players, 35 Negro league baseball players and executives, 22 managers, 10 umpires, and 30 pioneers, executives, and organizers. 114 members of the Hall of Fame have been inducted posthumously, including four who died after their selection was announced. Of the 35 Negro league members, 29 were inducted posthumously, including all 24 selected since the 1990s. The Hall of Fame includes one female member, Effa Manley.\n\nThe newest inductees, enshrined on July 30, , are players Jeff Bagwell, Tim Raines, and Iván Rodríguez, plus executives John Schuerholz and Bud Selig. In addition to honoring Hall of Fame inductees, the National Baseball Hall of Fame has presented 41 men with the Ford C. Frick Award for excellence in broadcasting, 67 men and one woman with the J. G. Taylor Spink Award for excellence in baseball writing, and three men and one woman with the Buck O'Neil Lifetime Achievement Award for contributions to baseball. While Frick and Spink Award honorees are not members of the Hall of Fame, they are recognized in an exhibit in the Hall of Fame's library. O'Neil Award honorees are also not Hall of Fame members, but are listed alongside a permanent statue of the award's namesake and first recipient, Buck O'Neil, that stands at the Hall.\n\nPlayers are currently inducted into the Hall of Fame through election by either the Baseball Writers' Association of America (or BBWAA), or the Veterans Committee, which now consists of four subcommittees, each of which considers and votes for candidates from a separate era of baseball. Five years after retirement, any player with 10 years of major league experience who passes a screening committee (which removes from consideration players of clearly lesser qualification) is eligible to be elected by BBWAA members with 10 years' membership or more who also have been actively covering MLB at any time in the 10 years preceding the election (the latter requirement was added for the 2016 election). From a final ballot typically including 25–40 candidates, each writer may vote for up to 10 players; until the late 1950s, voters were advised to cast votes for the maximum 10 candidates. Any player named on 75% or more of all ballots cast is elected. A player who is named on fewer than 5% of ballots is dropped from future elections. In some instances, the screening committee had restored their names to later ballots, but in the mid-1990s, dropped players were made permanently ineligible for Hall of Fame consideration, even by the Veterans Committee. A 2001 change in the election procedures restored the eligibility of these dropped players; while their names will not appear on future BBWAA ballots, they may be considered by the Veterans Committee. Players receiving 5% or more of the votes but fewer than 75% are reconsidered annually until a maximum of ten years of eligibility (lowered from fifteen years for the 2015 election).\n\nUnder special circumstances, certain players may be deemed eligible for induction even though they have not met all requirements. Addie Joss was elected in 1978, despite only playing nine seasons before he died of meningitis. Additionally, if an otherwise eligible player dies before his fifth year of retirement, then that player may be placed on the ballot at the first election at least six months after his death. Roberto Clemente's induction in 1973 set the precedent when the writers chose to put him up for consideration after his death on New Year's Eve, 1972.\n\nThe five-year waiting period was established in 1954 after an evolutionary process. In 1936 all players were eligible, including active ones. From the 1937 election until the 1945 election, there was no waiting period, so any retired player was eligible, but writers were discouraged from voting for current major leaguers. Since there was no formal rule preventing a writer from casting a ballot for an active player, the scribes did not always comply with the informal guideline; Joe DiMaggio received a vote in 1945, for example. From the 1946 election until the 1954 election, an official one-year waiting period was in effect. (DiMaggio, for example, retired after the 1951 season and was first eligible in the 1953 election.) The modern rule establishing a wait of five years was passed in 1954, although an exception was made for Joe DiMaggio because of his high level of previous support, thus permitting him to be elected within four years of his retirement.\n\nContrary to popular belief, no formal exception was made for Lou Gehrig (other than to hold a special one-man election for him): there was no waiting period at that time, and Gehrig met all other qualifications, so he would have been eligible for the next regular election after he retired during the 1939 season. However, the BBWAA decided to hold a special election at the 1939 Winter Meetings in Cincinnati, specifically to elect Gehrig (most likely because it was known that he was terminally ill, making it uncertain that he would live long enough to see another election). Nobody else was on that ballot, and the numerical results have never been made public. Since no elections were held in 1940 or 1941, the special election permitted Gehrig to enter the Hall while still alive.\n\nIf a player fails to be elected by the BBWAA within 10 years of his retirement from active play, he may be selected by the Veterans Committee. Following changes to the election process for that body made in 2010 and 2016, it is now responsible for electing all otherwise eligible candidates who are not eligible for the BBWAA ballot—both long-retired players and non-playing personnel (managers, umpires, and executives). From 2011 through 2016, each candidate could be considered once every three years; now, the frequency depends on the era in which an individual made his greatest contributions. A more complete discussion of the new process is available below.\n\nFrom 2008 to 2010, following changes made by the Hall in July 2007, the main Veterans Committee, then made up of living Hall of Famers, voted only on players whose careers began in 1943 or later. These changes also established three separate committees to select other figures:\n\nPlayers of the Negro Leagues have also been considered at various times, beginning in 1971. In 2005 the Hall completed a study on African American players between the late 19th century and the integration of the major leagues in 1947, and conducted a special election for such players in February 2006; seventeen figures from the Negro Leagues were chosen in that election, in addition to the eighteen previously selected. Following the 2010 changes, Negro Leagues figures were primarily considered for induction alongside other figures from the 1871–1946 era, called the \"Pre-Integration Era\" by the Hall; since 2016, Negro Leagues figures are primarily considered alongside other figures from what the Hall calls the \"Early Baseball\" era (1871–1949).\n\nPredictably, the selection process catalyzes endless debate among baseball fans over the merits of various candidates. Even players elected years ago remain the subjects of discussions as to whether they deserved election. For example, Bill James' book \"Whatever Happened to the Hall of Fame?\" goes into detail about who he believes does and does not belong in the Hall of Fame.\n\nThe actions and composition of the Veterans Committee have been at times controversial, with occasional selections of contemporaries and teammates of the committee members over seemingly more worthy candidates.\n\nIn 2001, the Veterans Committee was reformed to comprise the living Hall of Fame members and other honorees. The revamped Committee held three election, in 2003 and 2007, for both players and non-players, and in 2005 for players only. No individual was elected in that time, sparking criticism among some observers who expressed doubt whether the new Veterans Committee would ever elect a player. The Committee members, most of whom were Hall members, were accused of being reluctant to elect new candidates in the hope of heightening the value of their own selection. After no one was selected for the third consecutive election in 2007, Hall of Famer Mike Schmidt noted, \"The same thing happens every year. The current members want to preserve the prestige as much as possible, and are unwilling to open the doors.\" In 2007, the committee and its selection processes were again reorganized; the main committee then included all living members of the Hall, and voted on a reduced number of candidates from among players whose careers began in 1943 or later. Separate committees, including sportswriters and broadcasters, would select umpires, managers and executives, as well as players from earlier eras.\n\nIn the first election to be held under the 2007 revisions, two managers and three executives were elected in December 2007 as part of the 2008 election process. The next Veterans Committee elections for players were held in December 2008 as part of the 2009 election process; the main committee did not select a player, while the panel for pre–World War II players elected Joe Gordon in its first and ultimately only vote. The main committee voted as part of the election process for inductions in odd-numbered years, while the pre-World War II panel would vote every five years, and the panel for umpires, managers, and executives voted as part of the election process for inductions in even-numbered years.\n\nFurther changes to the Veterans Committee process were announced by the Hall on July 26, 2010, effective with the 2011 election.\n\nAll individuals eligible for induction but not eligible for BBWAA consideration were considered on a single ballot, grouped by the following eras in which they made their greatest contributions:\n\nThe Hall used the BBWAA's Historical Overview Committee to formulate the ballots for each era, consisting of 12 individuals for the Expansion Era and 10 for the other eras. The Hall's board of directors selected a committee of 16 voters for each era, made up of Hall of Famers, executives, baseball historians, and media members. Each committee met and voted at the Baseball Winter Meetings once every three years. The Expansion Era committee held its first vote in 2010 for 2011 induction, with longtime general manager Pat Gillick becoming the first individual elected under the new procedure. The Golden Era committee voted in 2011 for the induction class of 2012, with Ron Santo becoming the first player elected under the new procedure. The Pre-Integration Era committee voted in 2012 for the induction class of 2013, electing three figures. Subsequent elections rotated among the three committees in that order through the 2016 election.\n\nIn July 2016, however, the Hall of Fame announced a restructuring of the timeframes to be considered, with a much greater emphasis on modern eras. Four new committees were established:\n\nAll committees' ballots now include 10 candidates. While there was previously a one-year waiting period after elimination from annual BBWAA consideration, there is now no waiting period. At least one committee convenes each December as part of the election process for the following calendar year's induction ceremony. The Early Baseball committee convenes only in years ending in 0 (2020, 2030). The Golden Days committee convenes only in years ending in 0 and 5 (2020, 2025). The remaining two committees convene twice every 5 years. More specifically, the Today's Game and Modern Baseball committees alternate their meetings in that order, skipping years in which either the Early Baseball or Golden Days committee meets. This means that in the next two 5-year cycles, the Today's Game committee (having met in 2016) will meet in 2018, 2021, and 2023, while the Modern Baseball committee will meet in 2017, 2019, 2022 and 2024. Additionally, the Hall of Fame has modified the criteria under which active executives can be considered for induction. All active executives age 70 or older may now have their careers reviewed as part of the Era Committee balloting process, regardless of the position they hold in an organization, and regardless of whether their body of work has been completed. Previously, active executives 65 years or older were eligible for consideration.\n\nWhile the text on a player's or manager's plaque lists all teams for which the inductee was a member in that specific role, inductees are usually depicted wearing the cap of a specific team, though in a few cases, like umpires, they wear caps without logos. (Executives are not depicted wearing caps.) Additionally, as of 2015, inductee biographies on the Hall's website for all players and managers, and executives who were associated with specific teams, list a \"primary team\", which does not necessarily match the cap logo. The Hall selects the logo \"based on where that player makes his most indelible mark.\"\n\nAlthough the Hall always made the final decision on which logo was shown, until 2001 the Hall deferred to the wishes of players or managers whose careers were linked with multiple teams. Some examples of inductees associated with multiple teams are the following:\n\nIn all of the above cases, the \"primary team\" is the team for which the inductee spent the largest portion of his career except for Ryan, whose primary team is listed as the Angels despite playing one fewer season for that team than for the Astros.\n\nIn 2001, the Hall of Fame decided to change the policy on cap logo selection, as a result of rumors that some teams were offering compensation, such as number retirement, money, or organizational jobs, in exchange for the cap designation. (For example, though Wade Boggs denied the claims, some media reports had said that his contract with the Tampa Bay Devil Rays required him to request depiction in the Hall of Fame as a Devil Ray.) The Hall decided that it would no longer defer to the inductee, though the player's wishes would be considered, when deciding on the logo to appear on the plaque. Newly elected members affected by the change include the following:\n\nAccording to the Hall of Fame, approximately 300,000 visitors enter the museum each year, and the running total has surpassed 14 million. These visitors see only a fraction of its 40,000 artifacts, 3 million library items (such as newspaper clippings and photos) and 140,000 baseball cards.\n\nThe Hall has seen a noticeable decrease in attendance in recent years. A 2013 story on \"ESPN.com\" about the village of Cooperstown and its relation to the game partially linked the reduced attendance with Cooperstown Dreams Park, a youth baseball complex about 5 miles (8 km) away in the town of Hartwick. The 22 fields at Dreams Park currently draw 17,000 players each summer for a week of intensive play; while the complex includes housing for the players, their parents and grandparents must stay elsewhere. According to the story,Prior to Dreams Park, a room might be filled for a week by several sets of tourists. Now, that room will be taken by just one family for the week, and that family may only go into Cooperstown and the Hall of Fame once. While there are other contributing factors (the recession and high gas prices among them), the Hall's attendance has tumbled since Dreams Park opened. The Hall drew 383,000 visitors in 1999. It drew 262,000 last year.\n\n\n\n\nA controversy erupted in 1982, when it emerged that some historic items given to the Hall had been sold on the collectibles market. The items had been lent to the Baseball Commissioner's office, gotten mixed up with other property owned by the Commissioner's office and employees of the office, and moved to the garage of Joe Reichler, an assistant to Commissioner Bowie Kuhn, who sold the items to resolve his personal financial difficulties. Under pressure from the New York Attorney General, the Commissioner's Office made reparations, but the negative publicity damaged the Hall of Fame's reputation, and made it more difficult for it to solicit donations.\n\nFollowing the banning of Pete Rose from MLB, the selection rules for the Baseball Hall of Fame were modified to prevent the induction of anyone on Baseball's permanent suspension list, such as Rose or Shoeless Joe Jackson. Many others have been barred from participation in MLB, but none have Hall of Fame qualifications on the level of Jackson or Rose.\n\nJackson and Rose were both banned from MLB for life for actions related to gambling on their own teams—Jackson was determined to have cooperated with those who conspired to lose the 1919 World Series intentionally, and Rose voluntarily accepted a permanent spot on the ineligible list in return for MLB's promise to make no official finding in relation to alleged betting on the Cincinnati Reds when he was their manager in the 1980s. (Baseball's Rule 21, prominently posted in every clubhouse locker room, mandates permanent banishment from the MLB for having a gambling interest of any sort on a game in which a player or manager is directly involved.) Rose later admitted that he bet on the Reds in his 2004 autobiography. Baseball fans are deeply split on the issue of whether these two should remain banned or have their punishment revoked. Writer Bill James, though he advocates Rose eventually making it into the Hall of Fame, compared the people who want to put Jackson in the Hall of Fame to \"those women who show up at murder trials wanting to marry the cute murderer\".\n\n\nNotes\n\n"}
{"id": "4079", "url": "https://en.wikipedia.org/wiki?curid=4079", "title": "BPP (complexity)", "text": "BPP (complexity)\n\nIn computational complexity theory, BPP, which stands for bounded-error probabilistic polynomial time is the class of decision problems solvable by a probabilistic Turing machine in polynomial time with an error probability bounded away from 1/2 for all instances.\nBPP is one of the largest \"practical\" classes of problems, meaning most problems of interest in BPP have efficient probabilistic algorithms that can be run quickly on real modern machines. BPP also contains P, the class of problems solvable in polynomial time with a deterministic machine, since a deterministic machine is a special case of a probabilistic machine.\n\nInformally, a problem is in BPP if there is an algorithm for it that has the following properties:\n\nA language \"L\" is in BPP if and only if there exists a probabilistic Turing machine \"M\", such that\nUnlike the complexity class ZPP, the machine \"M\" is required to run for polynomial time on all inputs, regardless of the outcome of the random coin flips.\n\nAlternatively, BPP can be defined using only deterministic Turing machines. A language \"L\" is in BPP if and only if there exists a polynomial \"p\" and deterministic Turing machine \"M\", such that\nIn this definition, the string \"y\" corresponds to the output of the random coin flips that the probabilistic Turing machine would have made. For some applications this definition is preferable since it does not mention probabilistic Turing machines.\n\nIn practice, an error probability of might not be acceptable, however, the choice of in the definition is arbitrary. It can be any constant between 0 and (exclusive) and the set BPP will be unchanged. It does not even have to be constant: the same class of problems is defined by allowing error as high as − \"n\" on the one hand, or requiring error as small as 2 on the other hand, where \"c\" is any positive constant, and \"n\" is the length of input. The idea is that there is a probability of error, but if the algorithm is run many times, the chance that the majority of the runs are wrong drops off exponentially as a consequence of the Chernoff bound. This makes it possible to create a highly accurate algorithm by merely running the algorithm several times and taking a \"majority vote\" of the answers. For example, if one defined the class with the restriction that the algorithm can be wrong with probability at most , this would result in the same class of problems.\n\nBesides the problems in P, which are obviously in BPP, many problems were known to be in BPP but not known to be in P. The number of such problems is decreasing, and it is conjectured that P = BPP.\n\nFor a long time, one of the most famous problems that was known to be in BPP but not known to be in P was the problem of determining whether a given number is prime. However, in the 2002 paper \"PRIMES is in P\", Manindra Agrawal and his students Neeraj Kayal and Nitin Saxena found a deterministic polynomial-time algorithm for this problem, thus showing that it is in P.\n\nAn important example of a problem in BPP (in fact in co-RP) still not known to be in P is polynomial identity testing, the problem of determining whether a polynomial is identically equal to the zero polynomial, when you have access to the value of the polynomial for any given input, but not to the coefficients. In other words, is there an assignment of values to the variables such that when a nonzero polynomial is evaluated on these values, the result is nonzero? It suffices to choose each variable's value uniformly at random from a finite subset of at least \"d\" values to achieve bounded error probability, where \"d\" is the total degree of the polynomial.\n\nIf the access to randomness is removed from the definition of BPP, we get the complexity class P. In the definition of the class, if we replace the ordinary Turing machine with a quantum computer, we get the class BQP.\n\nAdding postselection to BPP, or allowing computation paths to have different lengths, gives the class BPP. BPP is known to contain NP, and it is contained in its quantum counterpart PostBQP.\n\nA Monte Carlo algorithm is a randomized algorithm which is likely to be correct. Problems in the class BPP have Monte Carlo algorithms with polynomial bounded running time. This is compared to a Las Vegas algorithm which is a randomized algorithm which either outputs the correct answer, or outputs \"fail\" with low probability. Las Vegas algorithms with polynomial bound running times are used to define the class ZPP. Alternatively, ZPP contains probabilistic algorithms that are always correct and have expected polynomial running time. This is weaker than saying it is a polynomial time algorithm, since it may run for super-polynomial time, but with very low probability.\n\nIt is known that BPP is closed under complement; that is, BPP = co-BPP. BPP is low for itself, meaning that a BPP machine with the power to solve BPP problems instantly (a BPP oracle machine) is not any more powerful than the machine without this extra power. In symbols, BPP = BPP.\n\nThe relationship between BPP and NP is unknown: it is not known whether BPP is a subset of NP, NP is a subset of BPP or neither. If NP is contained in BPP, which is considered unlikely since it would imply practical solutions for NP-complete problems, then NP = RP and PH ⊆ BPP.\n\nIt is known that RP is a subset of BPP, and BPP is a subset of PP. It is not known whether those two are strict subsets, since we don't even know if P is a strict subset of PSPACE. BPP is contained in the second level of the polynomial hierarchy and therefore it is contained in PH. More precisely, the Sipser–Lautemann theorem states that formula_1. As a result, P = NP leads to P = BPP since PH collapses to P in this case. Thus either P = BPP or P ≠ NP or both.\n\nAdleman's theorem states that membership in any language in BPP can be determined by a family of polynomial-size Boolean circuits, which means BPP is contained in P/poly. Indeed, as a consequence of the proof of this fact, every BPP algorithm operating on inputs of bounded length can be derandomized into a deterministic algorithm using a fixed string of random bits. Finding this string may be expensive, however.\nSome weak separation results for Monte Carlo time classes were proven by , see also \n\nThe class BPP is closed under complementation, union and intersection.\n\nRelative to oracles, we know that there exist oracles A and B, such that P = BPP and P ≠ BPP. Moreover, relative to a random oracle with probability 1, P = BPP and BPP is strictly contained in NP and co-NP.\n\nThere is even an oracle in which BPP=EXP (and hence P<NP<BPP=EXP=NEXP), which can be iteratively constructed as follows. For a fixed E (relativized) complete problem, the oracle will give correct answers with high probability if queried with the problem instance followed by a random string of length \"kn\" (\"n\" is instance length; \"k\" is an appropriate small constant). Start with \"n\"=1. For every instance of the problem of length \"n\" fix oracle answers (see lemma below) to fix the instance output. Next, provide the instance outputs for queries consisting of the instance followed by \"kn\"-length string, and then treat output for queries of length ≤(\"k\"+1)\"n\" as fixed, and proceed with instances of length \"n\"+1.\n\nLemma: Given a problem (specifically, an oracle machine code and time constraint) in relativized E, for every partially constructed oracle and input of length \"n\", the output can be fixed by specifying 2 oracle answers.\nProof: The machine is simulated, and the oracle answers (that are not already fixed) are fixed step-by-step. There is at most one oracle query per deterministic computation step. For the relativized NP oracle, if possible fix the output to be yes by choosing a computation path and fixing the answers of the base oracle; otherwise no fixing is necessary, and either way there is at most 1 answer of the base oracle per step. Since there are 2 steps, the lemma follows.\n\nThe lemma ensures that (for a large enough \"k\"), it is possible to do the construction while leaving enough strings for the relativized E answers. Also, we can ensure that for the relativized E, linear time suffices, even for function problems (if given a function oracle and linear output size) and with exponentially small (with linear exponent) error probability. Also, this construction is effective in that given an arbitrary oracle A we can arrange the oracle B to have P≤P and EXP=EXP=BPP. Also, for a ZPP=EXP oracle (and hence ZPP=BPP=EXP<NEXP), one would fix the answers in the relativized E computation to a special nonanswer, thus ensuring that no fake answers are given.\n\nThe existence of certain strong pseudorandom number generators is conjectured by most experts of the field. This conjecture implies that randomness does not give additional computational power to polynomial time computation, that is, P = RP = BPP. Note that ordinary generators are not sufficient to show this result; any probabilistic algorithm implemented using a typical random number generator will always produce incorrect results on certain inputs irrespective of the seed (though these inputs might be rare).\n\nLászló Babai, Lance Fortnow, Noam Nisan, and Avi Wigderson showed that unless EXPTIME collapses to MA, BPP is contained in \nThe class i.o.-SUBEXP, which stands for infinitely often SUBEXP, contains problems which have sub-exponential time algorithms for infinitely many input sizes. They also showed that P = BPP if the exponential-time hierarchy, which is defined in terms of the polynomial hierarchy and E as E, collapses to E; however, note that the exponential-time hierarchy is usually conjectured \"not\" to collapse.\n\nRussell Impagliazzo and Avi Wigderson showed that if any problem in E, where \nhas circuit complexity 2 then P = BPP.\n\n\n\n"}
{"id": "4080", "url": "https://en.wikipedia.org/wiki?curid=4080", "title": "BQP", "text": "BQP\n\nIn computational complexity theory, BQP (bounded-error quantum polynomial time) is the class of decision problems solvable by a quantum computer in polynomial time, with an error probability of at most 1/3 for all instances. It is the quantum analogue of the complexity class BPP.\n\nA decision problem is a member of BQP if there exists an algorithm for a quantum computer (a quantum algorithm) that solves the decision problem with \"high\" probability and is guaranteed to run in polynomial time. A run of the algorithm will correctly solve the decision problem with a probability of at least 2/3.\n\nSimilarly to other \"bounded error\" probabilistic classes the choice of 1/3 in the definition is arbitrary. We can run the algorithm a constant number of times and take a majority vote to achieve any desired probability of correctness less than 1, using the Chernoff bound. Detailed analysis shows that the complexity class is unchanged by allowing error as high as 1/2 − \"n\" on the one hand, or requiring error as small as 2 on the other hand, where \"c\" is any positive constant, and \"n\" is the length of input.\n\nBQP can also be viewed as the languages associated with certain bounded-error uniform families of quantum circuits. A language \"L\" is in BQP if and only if there exists a polynomial-time uniform family of quantum circuits formula_1, such that\n\nThe number of qubits in the computer is allowed to be a polynomial function of the instance size. For example, algorithms are known for factoring an \"n\"-bit integer using just over 2\"n\" qubits (Shor's algorithm).\n\nUsually, computation on a quantum computer ends with a measurement. This leads to a collapse of quantum state to one of the basis states. It can be said that the quantum state is measured to be in the correct state with high probability.\n\nQuantum computers have gained widespread interest because some problems of practical interest are known to be in BQP, but suspected to be outside P. Some prominent examples are:\n\nThis class is defined for a quantum computer and its natural corresponding class for an ordinary computer (or a Turing machine plus a source of randomness) is . Just like and , is low for itself, which means = . Informally, this is true because polynomial time algorithms are closed under composition. If a polynomial time algorithm calls as a subroutine polynomially many polynomial time algorithms, the resulting algorithm is still polynomial time.\n\nIn fact, is low for , meaning that a machine achieves no benefit from being able to solve problems instantly, an indication of the possible difference in power between these similar classes.\n\nAs the problem of has not yet been solved, the proof of inequality between and classes mentioned above is supposed to be difficult. The relation between and is not known.\n\nAdding postselection to results in the complexity class which is equal to .\n\n"}
{"id": "4081", "url": "https://en.wikipedia.org/wiki?curid=4081", "title": "Blade Runner 3: Replicant Night", "text": "Blade Runner 3: Replicant Night\n\nBlade Runner 3: Replicant Night is a science fiction novel by American writer K. W. Jeter, published in 1996, which continues the story of Rick Deckard. It is the sequel to \"\", which was a sequel to Ridley Scott's 1982 film \"Blade Runner\", and the book on which the film was based, \"Do Androids Dream of Electric Sheep?\".\n\nLiving on Mars, Deckard is acting as a consultant to a movie crew filming the story of his Blade Runner days. He finds himself drawn into a mission on behalf of the replicants he was once assigned to kill. Meanwhile, the mystery surrounding the beginnings of the Tyrell Corporation is being dragged out into the light.\n\n"}
{"id": "4082", "url": "https://en.wikipedia.org/wiki?curid=4082", "title": "Blade Runner 2: The Edge of Human", "text": "Blade Runner 2: The Edge of Human\n\nBlade Runner 2: The Edge of Human (1995) is a science fiction novel by K. W. Jeter, and a continuation of both the film \"Blade Runner\", and the novel upon which it was based, Philip K. Dick's \"Do Androids Dream of Electric Sheep?\"\n\nBeginning several months after the events in \"Blade Runner\", Deckard has retired to an isolated shack outside the city, taking the replicant Rachael with him in a Tyrell transport container, which slows down the replicant aging process. He is approached by a woman who explains she is Sarah Tyrell, niece of Eldon Tyrell, heiress to the entire Tyrell Corporation and the human template (\"templant\") for the Rachael replicant. She asks Deckard to hunt down the \"missing\" sixth replicant. At the same time, the human template for Roy Batty hires Dave Holden, the blade runner attacked by Leon, to help him hunt down the man he believes is the sixth replicant - Deckard.\n\nDeckard and Holden's investigations lead them to re-visit Sebastian, Bryant, and John Isidore (from the book \"Do Androids Dream Of Electric Sheep?\"), learning more about the nature of the blade runners and the replicants.\n\nWhen Deckard, Batty, and Holden finally clash, Batty's inhuman fighting prowess leads Holden to believe he has been duped all along and that Batty is the sixth replicant; he shoots him. Deckard returns to Sarah with his suspicion: there is \"no\" sixth replicant. Sarah, speaking via a remote camera, confesses that she created and maintained the rumor herself, to deliberately discredit and eventually destroy the Tyrell Corporation, after her uncle Eldon created Rachael based on her and then abandoned the real Sarah. Sarah brings Rachael back to the Corporation building to meet with Deckard, and he escapes with her.\n\nHowever, Holden - recovering from his injuries during the fight - later finds the truth: Rachael has been killed by Tyrell agents, and the \"Rachael\" who escaped with Deckard was actually Sarah. She has completed her revenge by both destroying Tyrell, and taking back Rachael's place.\n\n\nThe book's plot draws from other material related to \"Blade Runner\" in a number of ways:\n\nHowever, it also contradicts material in some ways:\n\nMichael Giltz of \"Entertainment Weekly\" gave the book a \"C-\", feeling that \"only hardcore fans will be satisfied by this tale\" and saying that Jeter's \"habit of echoing dialogue and scenes from the film is annoying and begs comparisons he would do well to avoid.\" Tal Cohen of \"Tal Cohen's Bookshelf\" called \"The Edge of Human\" \"a good book\", praising Jeter's \"further, and deeper, investigation of the questions Philip K. Dick originally asked\", but criticized the book for its \"needless grandioseness\" and for \"rel[ying] on \"Blade Runner\" too heavily, [as] the number of new characters introduced is extremely small...\"\n\nIan Kaplan of \"BearCave.com\" gave the book three stars out of five, saying that while he was \"not entirely satisfied\" and felt that the \"story tends to be shallow\", \"Jeter does deal with the moral dilemma of the Blade Runners who hunt down beings that are virtually human in every way.\" J. Patton of \"The Bent Cover\" praised Jeter for \"[not] try[ing] to emulate Philip K. Dick\", adding, \"This book also has all the grittiness and dark edges that the movie showed off so well, along with a very fast pace that will keep you reading into the wee hours of the night.\"\n\n"}
{"id": "4086", "url": "https://en.wikipedia.org/wiki?curid=4086", "title": "Brainfuck", "text": "Brainfuck\n\nBrainfuck is an esoteric programming language created in 1993 by Urban Müller, and notable for its extreme minimalism. \n\nThe language consists of only eight simple commands and an instruction pointer. While it is fully Turing-complete, it is not intended for practical use, but to challenge and amuse programmers. Brainfuck simply requires one to break commands into microscopic steps. \n\nThe language's name is a reference to the slang term \"brainfuck\", which refers to things so complicated or unusual that they exceed the limits of one's understanding.\nIn 1992, Urban Müller, a Swiss physics student, took over a small online archive for Amiga software. The archive grew more popular, and was soon mirrored around the world. Today, it is the world's largest Amiga archive, known as Aminet. \n\nMüller designed Brainfuck with the goal of implementing it with the smallest possible compiler, inspired by the 1024-byte compiler for the FALSE programming language. Müller's original compiler was implemented in machine language and compiled to a binary with a size of 296 bytes. He uploaded the first Brainfuck compiler to Aminet in 1993. The program came with a \"Readme\" file, which briefly described the language, and challenged the reader \"Who can program anything useful with it? :)\". Müller also included an interpreter and some quite elaborate examples. A second version of the compiler used only 240 bytes.\n\nAs Aminet grew, the compiler became popular among the Amiga community, and in time it was implemented for other platforms. Several brainfuck compilers have been made smaller than 200 bytes, and one is only 100 bytes.\n\nThe language consists of eight commands, listed below. A brainfuck program is a sequence of these commands, possibly interspersed with other characters (which are ignored). The commands are executed sequentially, with some exceptions: an instruction pointer begins at the first command, and each command it points to is executed, after which it normally moves forward to the next command. The program terminates when the instruction pointer moves past the last command.\n\nThe brainfuck language uses a simple machine model consisting of the program and instruction pointer, as well as an array of at least 30,000 byte cells initialized to zero; a movable data pointer (initialized to point to the leftmost byte of the array); and two streams of bytes for input and output (most often connected to a keyboard and a monitor respectively, and using the ASCII character encoding).\n\nThe eight language commands each consist of a single character:\n\ncodice_2 and codice_1 match as parentheses usually do: each codice_2 matches exactly one codice_1 and vice versa, the codice_2 comes first, and there can be no unmatched codice_2 or codice_1 between the two.\n\nBrainfuck programs can be translated into C using the following substitutions, assuming codice_10 is of type codice_11 and has been initialized to point to an array of zeroed bytes:\nAs the name suggests, brainfuck programs tend to be difficult to comprehend. This is partly because any mildly complex task requires a long sequence of commands; partly it is because the program's text gives no direct indications of the program's state. These, as well as brainfuck's inefficiency and its limited input/output capabilities, are some of the reasons it is not used for serious programming. Nonetheless, like any Turing-complete language, brainfuck is theoretically capable of computing any computable function or simulating any other computational model, if given access to an unlimited amount of memory. A variety of brainfuck programs have been written. Although brainfuck programs, especially complicated ones, are difficult to write, it is quite trivial to write an interpreter for brainfuck in a more typical language such as C due to its simplicity. There even exists a brainfuck interpreter written in the brainfuck language itself.\n\nBrainfuck is an example of a so-called Turing tarpit: It can be used to write \"any\" program, but it is not practical to do so, because Brainfuck provides so little abstraction that the programs get very long or complicated.\n\nExcept for its two I/O commands, brainfuck is a minor variation of the formal programming language P′′ created by Corrado Böhm in 1964. In fact, using six symbols equivalent to the respective brainfuck commands codice_12, codice_13, codice_14, codice_15, codice_2, codice_1, Böhm provided an explicit program for each of the basic functions that together serve to compute any computable function. So the first \"brainfuck\" programs appear in Böhm's 1964 paper – and they were programs sufficient to prove Turing-completeness.\n\nAs a first, simple example, the following code snippet will add the current cell's value to the next cell: Each time the loop is executed, the current cell is decremented, the data pointer moves to the right, that next cell is incremented, and the data pointer moves left again. This sequence is repeated until the starting cell is 0.\n\nThis can be incorporated into a simple addition program as follows:\n\nThe following program prints \"Hello World!\" and a newline to the screen:\n\nFor \"readability\", this code has been spread across many lines and blanks and comments have been added. Brainfuck ignores all characters except the eight commands codice_18 so no special syntax for comments is needed (as long as the comments do not contain the command characters). The code could just as well have been written as:\nThis program enciphers its input with the ROT13 cipher. To do this, it must map characters A-M (ASCII 65-77) to N-Z (78-90), and vice versa. Also it must map a-m (97-109) to n-z (110-122) and vice versa. It must map all other characters to themselves; it reads characters one at a time and outputs their enciphered equivalents until it reads an EOF (here assumed to be represented as either -1 or \"no change\"), at which point the program terminates.\n\nThe basic approach used is as follows. Calling the input character \"x\", divide \"x\"-1 by 32, keeping quotient and remainder. Unless the quotient is 2 or 3, just output \"x\", having kept a copy of it during the division. If the quotient is 2 or 3, divide the remainder ((\"x\"-1) modulo 32) by 13; if the quotient here is 0, output \"x\"+13; if 1, output \"x\"-13; if 2, output \"x\".\n\nRegarding the division algorithm, when dividing \"y\" by \"z\" to get a quotient \"q\" and remainder \"r\", there is an outer loop which sets \"q\" and \"r\" first to the quotient and remainder of 1/\"z\", then to those of 2/\"z\", and so on; after it has executed \"y\" times, this outer loop terminates, leaving \"q\" and \"r\" set to the quotient and remainder of \"y\"/\"z\". (The dividend \"y\" is used as a diminishing counter that controls how many times this loop is executed.) Within the loop, there is code to increment \"r\" and decrement \"y\", which is usually sufficient; however, every \"z\"th time through the outer loop, it is necessary to zero \"r\" and increment \"q\". This is done with a diminishing counter set to the divisor \"z\"; each time through the outer loop, this counter is decremented, and when it reaches zero, it is refilled by moving the value from \"r\" back into it.\n\nPartly because Urban Müller did not write a thorough language specification, the many subsequent brainfuck interpreters and compilers have come to use slightly different dialects of brainfuck.\n\nIn the classic distribution, the cells are of 8-bit size (cells are bytes), and this is still the most common size. However, to read non-textual data, a brainfuck program may need to distinguish an end-of-file condition from any possible byte value; thus 16-bit cells have also been used. Some implementations have used 32-bit cells, 64-bit cells, or bignum cells with practically unlimited range, but programs that use this extra range are likely to be slow, since storing the value \"n\" into a cell requires Ω(\"n\") time as a cell's value may only be changed by incrementing and decrementing.\n\nIn all these variants, the codice_19 and codice_20 commands still read and write data in bytes. In most of them, the cells wrap around, i.e. incrementing a cell which holds its maximal value (with the codice_12 command) will bring it to its minimal value and vice versa. The exceptions are implementations which are distant from the underlying hardware, implementations that use bignums, and implementations that try to enforce portability.\n\nFortunately, it is usually easy to write brainfuck programs that do not ever cause integer wraparound or overflow, and therefore don't depend on cell size. Generally this means avoiding increment of +255 (unsigned 8-bit wraparound), or avoiding overstepping the boundaries of [-128, +127] (signed 8-bit wraparound) (since there are no comparison operators, a program cannot distinguish between a signed and unsigned two's complement fixed-bit-size cell and negativeness of numbers is a matter of interpretation). For more details on integer wraparound, see the Integer overflow article.\n\nIn the classic distribution, the array has 30,000 cells, and the pointer begins at the leftmost cell. Even more cells are needed to store things like the millionth Fibonacci number, and the easiest way to make the language Turing-complete is to make the array unlimited on the right.\n\nA few implementations extend the array to the left as well; this is an uncommon feature, and therefore portable brainfuck programs do not depend on it.\n\nWhen the pointer moves outside the bounds of the array, some implementations will give an error message, some will try to extend the array dynamically, some will not notice and will produce undefined behavior, and a few will move the pointer to the opposite end of the array. Some tradeoffs are involved: expanding the array dynamically to the right is the most user-friendly approach and is good for memory-hungry programs, but it carries a speed penalty. If a fixed-size array is used it is helpful to make it very large, or better yet let the user set the size. Giving an error message for bounds violations is very useful for debugging but even that carries a speed penalty unless it can be handled by the operating system's memory protections.\n\nDifferent operating systems (and sometimes different programming environments) use subtly different versions of ASCII. The most important difference is in the code used for the end of a line of text. MS-DOS and Microsoft Windows use a CRLF, i.e. a 13 followed by a 10, in most contexts. UNIX and its descendants (including GNU/Linux and Mac OS X) and Amigas use just 10, and older Macs use just 13. It would be unfortunate if brainfuck programs had to be rewritten for different operating systems. Fortunately, a unified standard is easy to find. Urban Müller's compiler and his example programs use 10, on both input and output; so do a large majority of existing brainfuck programs; and 10 is also more convenient to use than CRLF. Thus, brainfuck implementations should make sure that brainfuck programs that assume newline=10 will run properly; many do so, but some do not.\n\nThis assumption is also consistent with most of the world's sample code for C and other languages, in that they use '\\n', or 10, for their newlines. On systems that use CRLF line endings, the C standard library transparently remaps \"\\n\" to \"\\r\\n\" on output and \"\\r\\n\" to \"\\n\" on input for streams not opened in binary mode.\n\nThe behavior of the \"codice_19\" command when an end-of-file condition has been encountered varies. Some implementations set the cell at the pointer to 0, some set it to the C constant EOF (in practice this is usually -1), some leave the cell's value unchanged. There is no real consensus; arguments for the three behaviors are as follows.\n\nSetting the cell to 0 avoids the use of negative numbers, and makes it marginally more concise to write a loop that reads characters until EOF occurs. This is a language extension devised by Panu Kalliokoski.\n\nSetting the cell to -1 allows EOF to be distinguished from any byte value (if the cells are larger than bytes), which is necessary for reading non-textual data; also, it is the behavior of the C translation of \"codice_19\" given in Müller's readme file. However, it is not obvious that those C translations are to be taken as normative.\n\nLeaving the cell's value unchanged is the behavior of Urban Müller's brainfuck compiler. This behavior can easily coexist with either of the others; for instance, a program that assumes EOF=0 can set the cell to 0 before each \"codice_19\" command, and will then work correctly on implementations that do either EOF=0 or EOF=\"no change\". It is so easy to accommodate the \"no change\" behavior that any brainfuck programmer interested in portability should do so.\n\nMany people have created brainfuck equivalents (languages with commands that directly map to brainfuck) or brainfuck derivatives (languages that extend its behavior or map it into new semantic territory).\n\nSome examples:\nHowever, there are also unnamed minor variants (or dialects), possibly formed as a result of inattention, of which some of the more common are:\n\n\n"}
{"id": "4091", "url": "https://en.wikipedia.org/wiki?curid=4091", "title": "Bartolomeo Ammannati", "text": "Bartolomeo Ammannati\n\nBartolomeo Ammannati (18 June 151113 April 1592) was an Italian architect and sculptor, born at Settignano, near Florence. He studied under Baccio Bandinelli and Jacopo Sansovino (assisting on the design of the Library of St. Mark's, the \"Biblioteca Marciana\", Venice) and closely imitated the style of Michelangelo.\n\nHe was more distinguished in architecture than in sculpture. He worked in Rome in collaboration with Vignola and Vasari), including designs for the Villa Giulia, but also for works and at Lucca. He labored during 1558–1570, in the refurbishment and enlargement of Pitti Palace, creating the courtyard consisting of three wings with rusticated facades, and one lower portico leading to the amphitheatre in the Boboli Gardens. His design mirrored the appearance of the main external façade of Pitti. He was also named \"Consul\" of Accademia delle Arti del Disegno of Florence, which had been founded by the Duke Cosimo I in 1563.\nIn 1569, Ammanati was commissioned to build the Ponte Santa Trinita, a bridge over the Arno River. The three arches are elliptic, and though very light and elegant, has survived, when floods had damaged other Arno bridges at different times. Santa Trinita was destroyed in 1944, during World War II, and rebuilt in 1957.\n\nAmmannati designed what is considered a prototypic mannerist sculptural ensemble in the \"Fountain of Neptune\" (\"Fontana del Nettuno\"), prominently located in the Piazza della Signoria in the center of Florence. The assignment was originally given to the aged Bartolommeo Bandinelli; however when Bandinelli died, Ammannati's design, bested the submissions of Benvenuto Cellini and Vincenzo Danti, to gain the commission. From 1563 and 1565, Ammannati and his assistants, among them Giambologna, sculpted the block of marble that had been chosen by Bandinelli. He took Grand Duke Cosimo I as model for Neptune's face. The statue was meant to highlight Cosimo's goal of establishing a Florentine Naval force. When the work on the ungainly sea god was finished, and sited at the other corner of the Palazzo Vecchio of Michelangelo David statue, the then 87-year-old sculptor, is said to have scoffed at Ammannati that he had ruined a beautiful piece of marble, with the ditty: \"Ammannati, Ammanato, che bel marmo hai rovinato!\" Ammannati continued work on this fountain for a decade, adding around the perimeter a cornucopia of demigod figures: bronze reclining river gods, laughing satyrs and marble sea horses emerging from the water. \n\nIn 1550 Ammannati married Laura Battiferri, an elegant poet and an accomplished woman. Later in his life he had a religious crisis, influenced by Counter-Reformation piety, which resulted in condemning his own works depicting nudity, and he left all his possessions to the Jesuits.\n\nHe died in Florence in 1592.\n\n\n"}
{"id": "4092", "url": "https://en.wikipedia.org/wiki?curid=4092", "title": "Bishop", "text": "Bishop\n"}
{"id": "4093", "url": "https://en.wikipedia.org/wiki?curid=4093", "title": "Bertrand Andrieu", "text": "Bertrand Andrieu\n\nBertrand Andrieu (24 November 1761 – 6 December 1822) was a French engraver of medals. He was born in Bordeaux. In France, he was considered as the restorer of the art, which had declined after the time of Louis XIV. During the last twenty years of his life, the French government commissioned him to undertake every major work of importance.\n\n"}
{"id": "4097", "url": "https://en.wikipedia.org/wiki?curid=4097", "title": "Bordeaux", "text": "Bordeaux\n\nBordeaux (; Gascon Occitan: \"\") is a port city on the Garonne River in the Gironde department in southwestern France.\n\nThe municipality (commune) of Bordeaux proper has a population of 243,626 (2012). Together with its suburbs and satellite towns, Bordeaux is the centre of the Bordeaux Métropole. With 749,595 inhabitants () and 1,178,335 in the metropolitan area, it is the sixth largest in France, after Paris, Marseille, Lyon, Toulouse and Lille. It is the capital of the Nouvelle-Aquitaine region, as well as the prefecture of the Gironde department. Its inhabitants are called \"\"Bordelais\"\" (for men) or \"\"Bordelaises\"\" (women). The term \"Bordelais\" may also refer to the city and its surrounding region.\n\nBordeaux is the world's major wine industry capital. It is home to the world's main wine fair, Vinexpo, and the wine economy in the metro area takes in 14.5 billion euros each year. Bordeaux wine has been produced in the region since the 8th century. The historic part of the city is on the UNESCO World Heritage List as \"an outstanding urban and architectural ensemble\" of the 18th century. After Paris, Bordeaux has the highest number of preserved historical buildings of any city in France.\n\nIn historical times, around 300 BC it was the settlement of a Celtic tribe, the Bituriges Vivisci, who named the town Burdigala, probably of Aquitanian origin. The name Bourde is still the name of a river south of the city.\n\nIn 107 BC, the Battle of Burdigala was fought by the Romans who were defending the Allobroges, a Gallic tribe allied to Rome, and the Tigurini led by Divico. The Romans were defeated and their commander, the consul Lucius Cassius Longinus, was killed in the action.\n\nThe city fell under Roman rule around 60 BC, its importance lying in the commerce of tin and lead towards Rome. Later it became capital of Roman Aquitaine, flourishing especially during the Severan dynasty (3rd century). In 276 it was sacked by the Vandals. Further ravage was brought by the same Vandals in 409, the Visigoths in 414 and the Franks in 498, beginning a period of obscurity for the city.\nIn the late 6th century, the city re-emerged as the seat of a county and an archdiocese within the Merovingian kingdom of the Franks, but royal Frankish power was never strong. The city started to play a regional role as a major urban center on the fringes of the newly founded Frankish Duchy of Vasconia. Around 585, a certain Gallactorius is cited as count of Bordeaux and fighting the Basques.\n\nThe city was plundered by the troops of Abd er Rahman in 732 after storming the fortified city and overwhelming the Aquitanian garrison. Duke Eudes mustered a force ready to engage the Umayyads outside Bordeaux, eventually taking on them in the Battle of the River Garonne somewhere near the river Dordogne, described as taking a heavy death toll. After Duke Eudes's defeat, the Aquitanian duke could still save part of its troops and keep his grip on Aquitaine after the Battle of Poitiers.\n\nIn 735, the Aquitanian duke Hunald led a rebellion after his father Eudes's death, at which Charles responded by sending an expedition that captured and plundered Bordeaux again, but it was not retained for long. The following year, the Frankish commander descended again over Aquitaine, but clashed in battle with the Aquitanians and left to take on hostile Burgundian authorities and magnates. In 745, Aquitaine faced yet another expedition by Charles' sons Pepin and Carloman against Hunald, the Aquitanian \"princeps\" (or duke) strong in Bordeaux. Hunald was defeated, and his son Waifer replaced him, who in turn confirmed Bordeaux as the capital city (along with Bourges in the north).\n\nDuring the last stage of the war against Aquitaine (760–768), it was one of Waifer's last important strongholds to fall to King Pepin the Short's troops. Next to Bordeaux, Charlemagne built the fortress of Fronsac (\"Frontiacus\", \"Franciacus\") on a hill across the border with the Basques (\"Wascones\"), where Basque commanders came over to vow loyalty to him (769).\n\nIn 778, Seguin (or Sihimin) was appointed count of Bordeaux, probably undermining the power of the Duke Lupo, and possibly leading to the Battle of Roncevaux Pass that very year. In 814, Seguin was made Duke of Vasconia, but he was deposed in 816 for failing to suppress or sympathise with a Basque rebellion. Under the Carolingians, sometimes the Counts of Bordeaux held the title concomitantly with that of Duke of Vasconia. They were meant to keep the Basques in check and defend the mouth of the Garonne from the Vikings when the latter appeared c. 844 in the region of Bordeaux. In Autumn 845, count Seguin II marched on the Vikings assaulting Bordeaux and Saintes, but was captured and put to death. No bishops were mentioned during the whole 8th century and part of the 9th in Bordeaux.\n\nFrom the 12th to the 15th century, Bordeaux regained importance following the marriage of Duchess Eléonore of Aquitaine with the French-speaking Count Henri Plantagenet, born in Le Mans, who became, within months of their wedding, King Henry II of England. The city flourished, primarily due to the wine trade, and the cathedral of St. André was built. It was also the capital of an independent state under Edward, the Black Prince (1362–1372), but in the end, after the Battle of Castillon (1453), it was annexed by France which extended its territory. The \"Château Trompette\" (Trumpet Castle) and the \"Fort du Hâ\", built by Charles VII of France, were the symbols of the new domination, which however deprived the city of its wealth by halting the wine commerce with England.\n\nIn 1462, Bordeaux obtained a parliament, but regained importance only in the 16th century when it became the centre of the distribution of sugar and slaves from the West Indies along with the traditional wine.\n\nBordeaux adhered to the Fronde, being effectively annexed to the Kingdom of France only in 1653, when the army of Louis XIV entered the city.\n\nThe 18th century was the golden age of Bordeaux. Many downtown buildings (about 5,000), including those on the quays, are from this period. Victor Hugo found the town so beautiful he once said: \"Take Versailles, add Antwerp, and you have Bordeaux\". Baron Haussmann, a long-time prefect of Bordeaux, used Bordeaux's 18th-century large-scale rebuilding as a model when he was asked by Emperor Napoleon III to transform a then still quasi-medieval Paris into a \"modern\" capital that would make France proud.\n\nIn 1814, towards the end of the Peninsula war, the Duke of Wellington sent William Beresford with two divisions, who seized Bordeaux without much resistance on 12 March. Bordeaux was largely anti-Bonapartist and had a majority that supported the Bourbons, so the British troops were treated as liberators.\n\nIn 1870, at the beginning of the Franco-Prussian war against Prussia, the French government temporarily relocated to Bordeaux from Paris. This happened again during the First World War and again very briefly during the Second World War, when it became clear that Paris would soon fall into German hands. However, on the last of these occasions the French capital was soon moved again to Vichy. In May and June 1940, Bordeaux was the site of the life-saving actions of the Portuguese consul-general, Aristides de Sousa Mendes, who illegally granted thousands of Portuguese visas, which were needed to pass the Spanish border, to refugees fleeing the German Occupation.\n\nFrom 1940 to 1943, the Italian Royal Navy (\"Regia Marina Italiana\") established BETASOM, a submarine base at Bordeaux. Italian submarines participated in the Battle of the Atlantic from this base, which was also a major base for German U-boats as headquarters of 12th U-boat Flotilla. The massive, reinforced concrete U-boat pens have proved impractical to demolish and are now partly used as a cultural center for exhibitions.\n\nBordeaux is located close to the European Atlantic coast, in the southwest of France and in the north of the Aquitaine region. It is around southwest of Paris. The city is built on a bend of the river Garonne, and is divided into two parts: the right bank to the east and left bank in the west. Historically the left bank is more developed because when flowing outside the bend, the water makes a furrow of the required depth to allow the passing of merchant ships, which used to offload on this side of the river. In Bordeaux, the Garonne River is accessible to ocean liners. The right bank of the Garonne is a low-lying, often marshy plain.\n\nBordeaux's climate is usually classified as an oceanic climate (Köppen climate classification \"Cfb\"); however, the summers tend to be warmer and the winters milder than most areas of similar classification. Substantial summer rainfall prevents its climate from being classified as Mediterranean.\n\nWinters are cool because of the prevalence of westerly winds from the Atlantic. Summers are warm and long due to the influence from the Bay of Biscay (surface temperature reaches . The average seasonal winter temperature is , but recent winters have been warmer than this. Frosts in the winter are commonplace, occurring several times during a winter, but snowfall is very rare, occurring only once every three years. The average summer seasonal temperature is . The summer of 2003 set a record with an average temperature of .\n\nBordeaux is a major centre for business in France as it has the fifth largest metropolitan population in France.\n\n, the GDP of Bordeaux is €32.7 Billion.\n\nThe vine was introduced to the Bordeaux region by the Romans, probably in the mid-first century, to provide wine for local consumption, and wine production has been continuous in the region since.\n\nBordeaux now has about of vineyards, 57 appellations, 10,000 wine-producing châteaux and 13,000 grape growers. With an annual production of approximately 960 million bottles, Bordeaux produces large quantities of everyday wine as well as some of the most expensive wines in the world. Included among the latter are the area's five \"premier cru\" (first growth) red wines (four from Médoc and one, Château Haut-Brion, from Graves), established by the Bordeaux Wine Official Classification of 1855:\nThe first growths are:\n\n\nBoth red and white wines are made in Bordeaux. Red Bordeaux is called claret in the United Kingdom. Red wines are generally made from a blend of grapes, and may be made from Cabernet Sauvignon, Merlot, Cabernet Franc, Petit verdot, Malbec, and, less commonly in recent years, Carménère.\n\nWhite Bordeaux is made from Sauvignon blanc, Sémillon, and Muscadelle. Sauternes is a sub-region of Graves known for its intensely sweet, white, dessert wines such as Château d'Yquem.\n\nBecause of a wine glut (wine lake) in the generic production, the price squeeze induced by an increasingly strong international competition, and vine pull schemes, the number of growers has recently dropped from 14,000 and the area under vine has also decreased significantly. In the meantime, the global demand for first growths and the most famous labels markedly increased and their prices skyrocketed.\n\nThe Cité du Vin, a museum as well as a place of exhibitions, shows, movie projections and academic seminars on the theme of wine opened its doors in June 2016.\n\nThe Laser Mégajoule will be one of the most powerful lasers in the world, allowing fundamental research and the development of the laser and plasma technologies. This project, carried by the French Ministry of Defence, involves an investment of 2 billion euros. The \"Road of the lasers\", a major project of regional planning, promotes regional investment in optical and laser related industries leading to the Bordeaux area having the most important concentration of optical and laser expertise in Europe.\n\nSome 20,000 people work for the aeronautic industry in Bordeaux. The city has some of the biggest companies including Dassault, EADS Sogerma, Snecma, Thales, SNPE, and others. The Dassault Falcon private jets are built there as well as the military aircraft Rafale and Mirage 2000, the Airbus A380 cockpit, the boosters of Ariane 5, and the M51 SLBM missile.\n\nTourism, especially wine tourism, is a major industry. Globelink.co.uk mentioned Bordeaux as the best tourist destination in Europe in 2015.\n\nAccess to the port from the Atlantic is via the Gironde estuary. Almost nine million tonnes of goods arrive and leave each year.\n\nAt the January 2011 census, there were 239,399 inhabitants in the city proper (commune) of Bordeaux. Bordeaux in its hey day had a population of 262,662 in 1968. The majority of the population is French, but there are sizable groups of Italians, Spaniards (Up to 20% of the Bordeaux population claim some degree of Spanish heritage), Portuguese, Turks, Germans..\n\nThe built-up area has grown for more than a century beyond the municipal borders of Bordeaux due to urban sprawl, so that by the January 2011 census there were 1,140,668 people living in the overall metropolitan area of Bordeaux, only a fifth of whom lived in the city proper.\n\nLargest communities of foreigners : \n\nBordeaux is multiracial and multicultural city .\n\nAt the 2007 presidential election, the Bordelais gave 31.37% of their votes to Ségolène Royal of the Socialist Party against 30.84% to Nicolas Sarkozy, president of the UMP. Then came Francois Bayrou with 22.01%, followed by Jean-Marie Le Pen who recorded 5.42%. None of the other candidates exceeded the 5% mark. Nationally, Nicolas Sarkozy led with 31.18%, then Ségolène Royal with 25.87%, followed by François Bayrou with 18.57%. After these came Jean-Marie Le Pen with 10.44%, none of the other candidates exceeded the 5% mark. In the second round, the city of Bordeaux gave Ségolène Royal 52.44% against 47.56% for Nicolas Sarkozy, the latter being elected President of the Republic with 53.06% against 46.94% for Ségolène Royal. The abstention rates for Bordeaux were 14.52% in the first round and 15.90% in the second round. This is an earthquake in Bordeaux, a city deeply rooted right traditions.\n\nIn the parliamentary elections of 2007, the left won eight constituencies against only three for the right. It should be added that after the partial 2008 elections, the eighth district of Gironde switched to the left, bringing the count to nine. In Bordeaux, the left was for the first time in its history the majority as it held two of three constituencies following the elections. In the first division of the Gironde, the outgoing UMP MP Chantal Bourragué was well ahead with 44.81% against 25.39% for the Socialist candidate Beatrice Desaigues. In the second round, it was Chantal Bourragué who was re-elected with 54.45% against 45.55% for his socialist opponent. In the second district of Gironde the UMP mayor and all new Minister of Ecology, Energy, Sustainable Development and the Sea Alain Juppé confronted the General Counsel PS Michèle Delaunay. In the first round, Alain Juppé was well ahead with 43.73% against 31.36% for Michèle Delaunay. In the second round, it was finally Michèle Delaunay who won the election with 50.93% of the votes against 49.07% for Alain Juppé, the margin being only 670 votes. The defeat of the so-called constituency \"Mayor\" showed that Bordeaux was rocking increasingly left. Finally, in the third constituency of the Gironde, Noël Mamère was well ahead with 39.82% against 28.42% for the UMP candidate Elizabeth Vine. In the second round, Noël Mamère was re-elected with 62.82% against 37.18% for his right-wing rival.\n\nIn 2008 municipal elections saw the clash between mayor of Bordeaux, Alain Juppé and the President of the Regional Council of Aquitaine Socialist Alain Rousset. The PS had put up a Socialist heavyweight in the Gironde and had put great hopes in this election after the victory of Ségolène Royal and Michèle Delaunay in 2007. However, after a rather exciting campaign it was Alain Juppé who was widely elected in the first round with 56.62%, far ahead of Alain Rousset who has managed to get 34.14%. At present, of the eight cantons that has Bordeaux, five are held by the PS and three by the UMP, the left eating a little each time into the right's numbers.\n\nIn the European elections of 2009, Bordeaux voters largely voted for the UMP candidate Dominique Baudis, who won 31.54% against 15.00% for PS candidate Kader Arif. The candidate of Europe Ecology José Bové came second with 22.34%. None of the other candidates reached the 10% mark. The 2009 European elections were like the previous ones in eight constituencies. Bordeaux is located in the district \"Southwest\", here are the results:\n\nUMP candidate Dominique Baudis: 26.89%. His party gained four seats. PS candidate Kader Arif: 17.79%, gaining two seats in the European Parliament. Europe Ecology candidate Bove: 15.83%, obtaining two seats. MoDem candidate Robert Rochefort: 8.61%, winning a seat. Left Front candidate Jean-Luc Mélenchon: 8.16%, gaining the last seat. At regional elections in 2010, the Socialist incumbent president Alain Rousset won the first round by totaling 35.19% in Bordeaux, but this score was lower than the plan for Gironde and Aquitaine. Xavier Darcos, Minister of Labour followed with 28.40% of the votes, scoring above the regional and departmental average. Then came Monique De Marco, Green candidate with 13.40%, followed by the member of Pyrenees-Atlantiques and candidate of the MoDem Jean Lassalle who registered a low 6.78% while qualifying to the second round on the whole Aquitaine, closely followed by Jacques Colombier, candidate of the National Front, who gained 6.48%. Finally the candidate of the Left Front Gérard Boulanger with 5.64%, no other candidate above the 5% mark. In the second round, Alain Rousset had a tidal wave win as national totals rose to 55.83%. If Xavier Darcos largely lost the election, he nevertheless achieved a score above the regional and departmental average obtaining 33.40%. Jean Lassalle, who qualified for the second round, passed the 10% mark by totaling 10.77%. The ballot was marked by abstention amounting to 55.51% in the first round and 53.59% in the second round.\n\nThe Mayor of the city is Alain Juppé.\n\nBordeaux is the capital of five cantons and the Prefecture of the Gironde and Aquitaine.\n\nThe town is divided into three districts, the first three of Gironde. The headquarters of Urban Community of Bordeaux Mériadeck is located in the neighborhood and the city is at the head of the Chamber of Commerce and Industry that bears his name.\n\nThe number of inhabitants of Bordeaux is greater than 199,999 and less than 250,000 and so the number of municipal councilors is 61. They are divided according to the following composition:\nSince 1947, there have been 3 mayors of Bordeaux:\n\n\nThe university was created by the archbishop Pey Berland in 1441 and was abolished in 1793, during the French Revolution, before reappearing in 1808 with Napoleon I. Bordeaux accommodates approximately 70,000 students on one of the largest campuses of Europe (235 ha).\nThe University of Bordeaux is divided into four:\n\n\nBordeaux has numerous public and private schools offering undergraduate and postgraduate programs.\n\nEngineering schools:\n\nBusiness and management schools:\n\n\nOther:\n\nThe \"École Compleméntaire Japonaise de Bordeaux\" (ボルドー日本語補習授業校 \"Borudō Nihongo Hoshū Jugyō Kō\"), a part-time Japanese supplementary school, is held in the \"Salle de L'Athenee Municipal\" in Bordeaux.\n\nBordeaux is classified \"City of Art and History\". The city is home to 362 \"monuments historiques\" (only Paris has more in France) with some buildings dating back to Roman times. Bordeaux has been inscribed on UNESCO World Heritage List as \"\"an outstanding urban and architectural ensemble\"\".\n\nBordeaux is home to one of Europe's biggest 18th-century architectural urban areas, making it a sought-after destination for tourists and cinema production crews. It stands out as one of the first French cities, after Nancy, to have entered an era of urbanism and metropolitan big scale projects, with the team Gabriel father and son, architects for King Louis XV, under the supervision of two intendants (Governors), first Nicolas-François Dupré de Saint-Maur then the Marquis de Tourny.\n\nMain sights include:\n\nSaint-André Cathedral, Saint-Michel Basilica and Saint-Seurin Basilica are part of the World Heritage Sites of the Routes of Santiago de Compostela in France.\n\n\n\n\"Le Jardin Public\" is a park in the heart of the city.\n\nEurope’s longest-span vertical-lift bridge, the Pont Jacques Chaban-Delmas, was opened in 2013 in Bordeaux, spanning the River Garonne. The central lift span is and can be lifted vertically up to to let tall ships pass underneath. The €160 million bridge was inaugurated by President François Hollande and Mayor Alain Juppé on 16 March 2013. The bridge was named after the late Jacques Chaban-Delmas, who was a former Prime Minister and Mayor of Bordeaux.\n\nBordeaux has many shopping options. In the heart of Bordeaux is \"Rue Sainte-Catherine\". This pedestrian-only shopping street has of shops, restaurants and cafés; it is also one of the longest shopping streets in Europe. \"Rue Sainte-Catherine\" starts at \"Place de la Victoire\" and ends at \"Place de la Comédie\" by the \"Grand Théâtre\". The shops become progressively more upmarket as one moves towards \"Place de la Comédie\" and the nearby \"Cours de l'Intendance\" is where one finds the more exclusive shops and boutiques.\n\nBordeaux is also the first city in France to have created, in the 1980s, an architecture exhibition and research centre, \"Arc en rêve\". Bordeaux offers a large number of cinemas, theatres and is the home of the Opéra national de Bordeaux. There are many music venues of varying capacity. The city also offers several festivals throughout the year.\n\nBordeaux is an important road and motorway junction. The city is connected to Paris by the A10 motorway, with Lyon by the A89, with Toulouse by the A62, and with Spain by the A63. There is a ring road called the \"Rocade\" which is often very busy. Another ring road is under consideration.\n\nBordeaux has five road bridges that cross the Garonne, the Pont de pierre built in the 1820s and three modern bridges built after 1960: the Pont Saint Jean, just south of the Pont de pierre (both located downtown), the Pont d'Aquitaine, a suspended bridge downstream from downtown, and the Pont François Mitterrand, located upstream of downtown. These two bridges are part of the ring road around Bordeaux. A fifth bridge, the Pont Jacques-Chaban-Delmas, was constructed in 2009–2012 and opened to traffic in March 2013. Located halfway between the Pont de pierre and the Pont d'Aquitaine and serving downtown rather than highway traffic, it is a vertical-lift bridge with a height comparable to the Pont de pierre in closed position, and to the Pont d'Aquitaine in open position. All five road bridges, including the two highway bridges, are open to cyclists and pedestrians as well.\nAnother bridge, the Pont Jean-Jacques Bosc, is to be built in 2018.\n\nLacking any steep hills, Bordeaux is relatively friendly to cyclists. Cycle paths (separate from the roadways) exist on the highway bridges, along the riverfront, on the university campuses, and incidentally elsewhere in the city. Cycle lanes and bus lanes that explicitly allow cyclists exist on many of the city's boulevards. A paid Bicycle sharing system with automated stations has been established in 2010.\n\nThe main railway station, Gare de Bordeaux Saint-Jean, near the center of the city, has 4 million passengers a year. It is served by the French national (SNCF) railway's high speed train, the TGV, that gets to Paris in three hours, with connections to major European centers such as Lille, Brussels, Amsterdam, Cologne, Geneva and London. The TGV also serves Toulouse and Irun from Bordeaux. A regular train service is provided to Nantes, Nice, Marseille and Lyon. The Gare Saint-Jean is the major hub for regional trains (TER) operated by the SNCF to Arcachon, Limoges, Agen, Périgueux, Pau, Le Médoc, Angoulême and Bayonne.\n\nHistorically the train line used to terminate at a station on the right bank of the river Garonne near the Pont de Pierre, and passengers crossed the bridge to get into the city. Subsequently, a double-track steel railway bridge was constructed in the 1850s, by Gustave Eiffel, to bring trains across the river direct into Gare de Bordeaux Saint-Jean. The old station was later converted and in 2010 comprised a cinema and restaurants.\n\nThe two-track Eiffel bridge with a speed limit of became a bottleneck and a new bridge was built, opening in 2009. The new bridge has 4 tracks and allows trains to pass at . During the planning there was much lobbying by the Eiffel family and other supporters to preserve the old bridge as a footbridge across the Garonne, with possibly a museum to document the history of the bridge and Gustave Eiffel's contribution. The decision was taken to save the bridge, but by early 2010 no plans had been announced as to its future use. The bridge remains intact, but unused and without any means of access.\n\nSince July 2017, the LGV Sud Europe Atlantique is fully operational and makes Bordeaux city 2h04 from Paris.\n\nBordeaux is served by an international airport, Aéroport de Bordeaux Mérignac, located from the city center in the suburban city of Mérignac.\n\nBordeaux has an important public transport system called Transports Bordeaux Métropole (TBM). This company is run by the Keolis group. The network consists of:\nThis network is operated from 5 am to 2 am.\n\nThere had been several plans for a subway network to be set up, but they stalled for both geological and financial reasons. Work on the Tramway de Bordeaux system was started in the autumn of 2000, and services started in December 2003 connecting Bordeaux with its suburban areas. The tram system uses ground-level power supply technology (APS), a new cable-free technology developed by French company Alstom and designed to preserve the aesthetic environment by eliminating overhead cables in the historic city. Conventional overhead cables are used outside the city. The system was controversial for its considerable cost of installation, maintenance and also for the numerous initial technical problems that paralysed the network. Many streets and squares along the tramway route became pedestrian areas, with limited access for cars.\n\nThere are more than 400 taxicabs in Bordeaux.\n\nThe 42,155-capacity Nouveau Stade de Bordeaux is the largest stadium in Bordeaux. The stadium was opened in 2015 and replaced the Stade Chaban-Delmas, which was a venue for the FIFA World Cup in 1938 and 1998, as well as the 2007 Rugby World Cup. In the 1938 FIFA World Cup, it hosted a violent quarter-final known as the Battle of Bordeaux. The ground was formerly known as the \"Stade du Parc Lescure\" until 2001, when it was renamed in honour of the city's long-time mayor, Jacques Chaban-Delmas.\n\nThere are two major sport teams in Bordeaux, both playing at the Stade Chaban-Delmas. Girondins de Bordeaux is the football team, currently playing in Ligue 1 in the French football championship. Union Bordeaux Bègles is a rugby team in the Top 14 in the Ligue Nationale de Rugby.\nSkateboarding, rollerblading, and BMX biking are activities enjoyed by many young inhabitants of the city. Bordeaux is home to a beautiful quay which runs along the Gironde river. On the quay there is a skate-park divided into three sections. One section is for Vert tricks, one for street style tricks, and one for little action sports athletes with easier features and softer materials. The skate-park is very well maintained by the municipality.\n\nBordeaux is also the home to one of the strongest cricket teams in France and are the current champions of the South West League.\n\nThere is a wooden velodrome, Vélodrome du Lac, in Bordeaux which hosts international cycling competition in the form of UCI Track Cycling World Cup events.\n\nThe 2015 Trophee Eric Bompard was in Bordeaux. But the Free Skate was cancelled in all of the divisions due to the Paris bombing(s) and aftermath. The Short Program occurred hours before the bombing. French skaters Chafik Besseghier (68.36) in 10th place, Romain Ponsart (62.86) in 11th. Mae-Berenice-Meite (46.82) in 11th and Laurine Lecavelier (46.53) in 12th. Vanessa James/Morgan Cipres (65.75) in 2nd.\n\nBordeaux was the birthplace of:\n\nGerald Causse, Presiding Bishop of The Church of Jesus Christ of Latter Day Saints \n\nBordeaux is twinned with:\n\n\n"}
{"id": "4098", "url": "https://en.wikipedia.org/wiki?curid=4098", "title": "Puzzle Bobble", "text": "Puzzle Bobble\n\n, also known as Bust-a-Move in North America, is a 1994 tile-matching arcade puzzle video game for one or two players created by Taito Corporation. It is based on Taito's popular 1986 arcade game \"Bubble Bobble\", featuring characters and themes from that game. Its characteristically cute Japanese animation and music, along with its play mechanics and level designs, made it successful as an arcade title and spawned several sequels and ports to home gaming systems.\n\nTwo different versions of the original game were released. \"Puzzle Bobble\" was originally released in Japan only in June 1994 by Taito Corporation, running on Taito's B System hardware (with the preliminary title \"Bubble Buster\"). Then, 6 months later in December, the international Neo Geo version of \"Puzzle Bobble\" was released. It was almost identical aside from being in stereo and having some different sound effects and translated text.\n\nWhen set to the US region, the Neo Geo version displays the alternative title \"Bust a Move\" and features anti-drugs and anti-littering messages in the title sequence. The Bust-a-Move title was used for all subsequent games in the series in the United States and Canada, as well as for some (non-Taito published) console releases in Europe.\n\nAt the start of each round, the rectangular playing arena contains a prearranged pattern of colored \"bubbles\". (These are actually referred to in the translation as \"balls\"; however, they were clearly intended to be bubbles, since they pop, and are taken from \"Bubble Bobble\".) At the bottom of the screen, the player controls a device called a \"pointer\", which aims and fires bubbles up the screen. The color of bubbles fired is randomly generated and chosen from the colors of bubbles still left on the screen.\n\nThe fired bubbles travel in straight lines (possibly bouncing off the side walls of the arena), stopping when they touch other bubbles or reach the top of the arena. If a bubble touches identically-colored bubbles, forming a group of three or more, those bubbles—as well as any bubbles hanging from them—are removed from the field of play, and points are awarded.\n\nAfter every few shots, the \"ceiling\" of the playing arena drops downwards slightly, along with all the bubbles stuck to it. The number of shots between each drop of the ceiling is influenced by the number of bubble colors remaining. The closer the bubbles get to the bottom of the screen, the faster the music plays and if they cross the line at the bottom then the game is over.\n\nThe objective of the game is to clear all the bubbles from the arena without any bubble crossing the bottom line. Bubbles will fire automatically if the player remains idle. After clearing the arena, the next round begins with a new pattern of bubbles to clear. The game consists of 32 levels.\n\nAs with many popular arcade games, experienced players (who can complete the game relatively easily) become much more interested in the secondary challenge of obtaining a high score (which involves a lot more skill and strategy). \"Puzzle Bobble\" caters to this interest very well, featuring an exponential scoring system which allows extremely high scores to be achieved.\n\n\"Popped\" bubbles (that is, bubbles of the same color which disappear) are worth 10 points each. However, \"dropped\" bubbles (that is, bubbles that were hanging from popped bubbles), are worth far more: one dropped bubble scores 20 points; two score 40; three score 80. This figure continues doubling for each bubble dropped, up to 17 or more bubbles which scores 1,310,720 points. It is possible to achieve this maximum on most rounds (sometimes twice or more), resulting in a potential total score of 30 million and beyond.\n\nBonus points are also awarded for completing a round quickly. The maximum 50,000-point bonus is awarded for clearing a round in 5 seconds or less; this bonus then drops down to zero over the next minute, after which no bonus is awarded.\n\nThere are no rounds in the two player game. Both players have an arena each (both visible on screen) and an identical arrangement of colored bubbles in each arena. When a player removes a large group (four bubbles or more) some of those removed are transferred to the opponent's arena, usually delaying their efforts to remove all the bubbles from their individual arena. In some versions, the two player game can also be played by one player against a computer opponent.\n\nThe characters and theme of the game are based on the 1986 platform arcade game \"Bubble Bobble\". An arrangement of the original \"Bubble Bobble\" background music is played in the game's end credits.\n\nThe two dinosaurs operating the pointer are called \"Bub\" and \"Bob\" (or \"Bubblun\" and \"Bobblun\" in Japan). Their graphics and animation are based directly on the original \"Bubble Bobble\", only larger (very similar to Bubble Symphony which was released less than a month later). Less obvious is the fact that \"Puzzle Bobble\" also features all the enemies from \"Bubble Bobble\", which are trapped inside the bubbles and fly out when the bubbles pop. Inspecting the bubbles closely, one can see the enemies twitching inside the bubbles.\n\nReviewing the Super NES version, Mike Weigand of \"Electronic Gaming Monthly\" called it \"a thoroughly enjoyable and incredibly addicting puzzle game\". He considered the two player mode the highlight, but also said that the one player mode provides a solid challenge. \"GamePro\" gave it a generally negative review, saying it \"starts out fun but ultimately lacks intricacy and longevity.\" They elaborated that in one player mode all the levels feel the same, and that two player matches are over too quickly to build up any excitement. They also criticized the lack of any 3D effects in the graphics.\n\nA reviewer for \"Next Generation\", while questioning the continued viability of the action puzzle genre, admitted that the game is \"very simple and \"very\" addictive\". He remarked that though the 3DO version makes no significant additions, none are called for by a game with such simple enjoyment. \"GamePro\"s brief review of the 3DO version commented, \"The move-and-shoot controls are very responsive and the simple visuals and music are well done. This is one puzzler that isn't a bust.\"\n\nPuzzle Bobble is played at both Penny Arcade Expos, PAX East and PAX Prime. After being the 9-time (7 consecutive years at PAX East, 2010-2016, and the 2015-2016 PAX Prime) undefeated champion, Matthew A. Tom has declared that he is \"retiring from competitive Puzzle Bobble playing.\"\n"}
{"id": "4099", "url": "https://en.wikipedia.org/wiki?curid=4099", "title": "Bone", "text": "Bone\n\nA bone is a rigid organ that constitutes part of the vertebrate skeleton. Bones support and protect the various organs of the body, produce red and white blood cells, store minerals, provide structure and support for the body, and enable mobility. Bones come in a variety of shapes and sizes and have a complex internal and external structure. They are lightweight yet strong and hard, and serve multiple functions.\n\nBone tissue is a hard tissue, a type of dense connective tissue. It has a honeycomb-like matrix internally, which helps to give the bone rigidity. Bone tissue is made up of different types of bone cells. Osteoblasts and osteocytes are involved in the formation and mineralization of bone; osteoclasts are involved in the resorption of bone tissue. Modified (flattened) osteoblasts become the lining cells that form a protective layer on the bone surface. The mineralised matrix of bone tissue has an organic component of mainly collagen called \"ossein\" and an inorganic component of bone mineral made up of various salts. Bone tissue is a mineralized tissue of two types, cortical and cancellous bone. Other types of tissue found in bones include bone marrow, endosteum, periosteum, nerves, blood vessels and cartilage.\n\nIn the human body at birth, there are over 270 bones, but many of these fuse together during development, leaving a total of 206 separate bones in the adult, not counting numerous small sesamoid bones. The largest bone in the body is the femur or thigh-bone, and the smallest is the stapes in the middle ear.\n\nThe Latin word for bone is \"os\", hence the many terms that use it as a prefix – such as osseous and osteopathy.\n\nBone is not uniformly solid, but includes a tough matrix. This matrix makes up about 30% of the bone and the other 70% is of salts that give strength to it. The matrix is made up of between 90 and 95% collagen fibers, and the remainder is ground substance. The primary tissue of bone, bone tissue (osseous tissue), is relatively hard and lightweight. Its matrix is mostly made up of a composite material incorporating the inorganic mineral calcium phosphate in the chemical arrangement termed calcium hydroxylapatite (this is the bone mineral that gives bones their rigidity) and collagen, an elastic protein which improves fracture resistance. The collagen of bone is known as ossein. Bone is formed by the hardening of this matrix around entrapped cells. When these cells become entrapped from osteoblasts they become osteocytes.\n\nThe hard outer layer of bones is composed of cortical bone also called compact bone. Cortical referring to the outer (cortex) layer. The hard outer layer gives bone its smooth, white, and solid appearance, and accounts for 80% of the total bone mass of an adult human skeleton. Cortical bone consists of multiple microscopic columns, each called an osteon. Each column is multiple layers of osteoblasts and osteocytes around a central canal called the haversian canal. Volkmann's canals at right angles connect the osteons together. The columns are metabolically active, and as bone is reabsorbed and created the nature and location of the cells within the osteon will change. Cortical bone is covered by a periosteum on its outer surface, and an endosteum on its inner surface. The endosteum is the boundary between the cortical bone and the cancellous bone. \n\nCancellous bone also known as trabecular or spongy bone tissue is the internal tissue of the skeletal bone and is an open cell porous network. Thin formations of osteoblasts covered in endosteum create an irregular network of spaces, known as trabeculae. Within these spaces are bone marrow and hematopoietic stem cells that give rise to platelets, red blood cells and white blood cells. Trabecular marrow is composed of a network of rod- and plate-like elements that make the overall organ lighter and allow room for blood vessels and marrow. Trabecular bone accounts for the remaining 20% of total bone mass but has nearly ten times the surface area of compact bone.\n\nBone marrow, also known as myeloid tissue in red bone marrow, can be found in almost any bone that holds cancellous tissue. In newborns, all such bones are filled exclusively with red marrow or hematopoietic marrow, but as the child ages the hematopoietic fraction decreases in quantity and the fatty/ yellow fraction called marrow adipose tissue (MAT) increases in quantity. In adults, red marrow is mostly found in the bone marrow of the femur, the ribs, the vertebrae and pelvic bones.\n\nBone is a metabolically active tissue composed of several types of cells. These cells include osteoblasts, which are involved in the creation and mineralization of bone tissue, osteocytes, and osteoclasts, which are involved in the reabsorption of bone tissue. Osteoblasts and osteocytes are derived from osteoprogenitor cells, but osteoclasts are derived from the same cells that differentiate to form macrophages and monocytes. Within the marrow of the bone there are also hematopoietic stem cells. These cells give rise to other cells, including white blood cells, red blood cells, and platelets.\n\nBones consist of living cells embedded in a mineralized organic matrix. This matrix consists of organic components, mainly Type I collagen – \"organic\" referring to materials produced as a result of the human body – and inorganic components, primarily hydroxyapatite and other salts of calcium and phosphate. Above 30% of the acellular part of bone consists of the organic components, and 70% of salts. The collagen fibers give bone its tensile strength, and the interspersed crystals of hydroxyapatite give bone its compressive strength. These effects are synergistic.\n\nThe inorganic composition of bone (bone mineral) is primarily formed from salts of calcium and phosphate, the major salt being hydroxyapatite (Ca(PO)(OH)). The exact composition of the matrix may change over time and with nutrition, with the ratio of calcium to phosphate varying between 1.3 and 2.0 (per weight), and trace minerals such as magnesium, sodium, potassium and carbonate also being found.\nType I collagen composes 90–95% of the organic matrix, with remainder of the matrix being a homogenous liquid called ground substance consisting of proteoglycans such as hyaluronic acid and chondroitin sulfate , as well as non-collagenous proteins such as osteocalcin, osteopontin or bone sialoprotein . Collagen consists of strands of repeating units, which give bone tensile strength, and are arranged in an overlapping fashion that prevents shear stress. The function of ground substance is not fully known. Two types of bone can be identified microscopically according to the arrangement of collagen: woven and lamellar.\nWoven bone is produced when osteoblasts produce osteoid rapidly, which occurs initially in all fetal bones, but is later replaced by more resilient lamellar bone. In adults woven bone is created after fractures or in Paget's disease. Woven bone is weaker, with a smaller number of randomly oriented collagen fibers, but forms quickly; it is for this appearance of the fibrous matrix that the bone is termed \"woven\". It is soon replaced by lamellar bone, which is highly organized in concentric sheets with a much lower proportion of osteocytes to surrounding tissue. Lamellar bone, which makes its first appearance in humans in the fetus during the third trimester, is stronger and filled with many collagen fibers parallel to other fibers in the same layer (these parallel columns are called osteons). In cross-section, the fibers run in opposite directions in alternating layers, much like in plywood, assisting in the bone's ability to resist torsion forces. After a fracture, woven bone forms initially and is gradually replaced by lamellar bone during a process known as \"bony substitution.\" Compared to woven bone, lamellar bone formation takes place more slowly. The orderly deposition of collagen fibers restricts the formation of osteoid to about 1 to 2 µm per day. Lamellar bone also requires a relatively flat surface to lay the collagen fibers in parallel or concentric layers.\nThe extracellular matrix of bone is laid down by osteoblasts, which secrete both collagen and ground substance. These synthesise collagen within the cell, and then secrete collagen fibrils. The collagen fibres rapidly polymerise to form collagen strands. At this stage they are not yet mineralised, and are called \"osteoid\". Around the strands calcium and phosphate precipitate on the surface of these strands, within a days to weeks becoming crystals of hydroxyapatite.\n\nIn order to mineralise the bone, the osteoblasts secrete vesicles containing alkaline phosphatase. This cleaves the phosphate groups and acts as the foci for calcium and phosphate deposition. The vesicles then rupture and act as a centre for crystals to grow on. More particularly, bone mineral is formed from globular and plate structures.\n\nThere are five types of bones in the human body: long, short, flat, irregular, and sesamoid.\n\nIn the study of anatomy, anatomists use a number of anatomical terms to describe the appearance, shape and function of bones. Other anatomical terms are also used to describe the location of bones. Like other anatomical terms, many of these derive from Latin and Greek. Some anatomists still use Latin to refer to bones. The term \"osseous\", and the prefix \"osteo-\", referring to things related to bone, are still used commonly today.\n\nSome examples of terms used to describe bones include the term \"foramen\" to describe a hole through which something passes, and a \"canal\" or \"meatus\" to describe a tunnel-like structure. A protrusion from a bone can be called a number of terms, including a \"condyle\", \"crest\", \"spine\", \"eminence\", \"tubercle\" or \"tuberosity\", depending on the protrusion's shape and location. In general, long bones are said to have a \"head\", \"neck\", and \"body\".\n\nWhen two bones join together, they are said to \"articulate\". If the two bones have a fibrous connection and are relatively immobile, then the joint is called a \"suture\".\n\nThe formation of bone is called ossification. During the fetal stage of development this occurs by two processes: intramembranous ossification and endochondral ossification. Intramembranous ossification involves the formation of bone from connective tissue whereas endochondral ossification involves the formation of bone from cartilage.\n\nIntramembranous ossification mainly occurs during formation of the flat bones of the skull but also the mandible, maxilla, and clavicles; the bone is formed from connective tissue such as mesenchyme tissue rather than from cartilage. The process includes: the development of the ossification center, calcification, trabeculae formation and the development of the periosteum.\n\nEndochondral ossification occurs in long bones and most other bones in the body; it involves the development of bone from cartilage. This process includes the development of a cartilage model, its growth and development, development of the primary and secondary ossification centers, and the formation of articular cartilage and the epiphyseal plates.\n\nEndochondral ossification begins with points in the cartilage called \"primary ossification centers.\" They mostly appear during fetal development, though a few short bones begin their primary ossification after birth. They are responsible for the formation of the diaphyses of long bones, short bones and certain parts of irregular bones. Secondary ossification occurs after birth, and forms the epiphyses of long bones and the extremities of irregular and flat bones. The diaphysis and both epiphyses of a long bone are separated by a growing zone of cartilage (the epiphyseal plate). At skeletal maturity (18 to 25 years of age), all of the cartilage is replaced by bone, fusing the diaphysis and both epiphyses together (epiphyseal closure). In the upper limbs, only the diaphyses of the long bones and scapula are ossified. The epiphyses, carpal bones, coracoid process, medial border of the scapula, and acromion are still cartilaginous.\n\nThe following steps are followed in the conversion of cartilage to bone:\n\nBones have a variety of functions:\n\nBones serve a variety of mechanical functions. Together the bones in the body form the skeleton. They provide a frame to keep the body supported, and an attachment point for skeletal muscles, tendons, ligaments and joints, which function together to generate and transfer forces so that individual body parts or the whole body can be manipulated in three-dimensional space (the interaction between bone and muscle is studied in biomechanics).\n\nBones protect internal organs, such as the skull protecting the brain or the ribs protecting the heart and lungs. Because of the way that bone is formed, bone has a high compressive strength of about 170 MPa (1800 kgf/cm²), poor tensile strength of 104–121 MPa, and a very low shear stress strength (51.6 MPa). This means that bone resists pushing(compressional) stress well, resist pulling(tensional) stress less well, but only poorly resists shear stress (such as due to torsional loads). While bone is essentially brittle, bone does have a significant degree of elasticity, contributed chiefly by collagen. The macroscopic yield strength of cancellous bone has been investigated using high resolution computer models.\n\nMechanically, bones also have a special role in hearing. The ossicles are three small bones in the middle ear which are involved in sound transduction.\n\nThe cancellous part of bones contain bone marrow. Bone marrow produces blood cells in a process called hematopoiesis. Blood cells that are created in bone marrow include red blood cells, platelets and white blood cells. Progenitor cells such as the hematopoietic stem cell divide in a process called mitosis to produce precursor cells. These include precursors which eventually give rise to white blood cells, and erythroblasts which give rise to red blood cells. Unlike red and white blood cells, created by mitosis, platelets are shed from very large cells called megakaryocytes. This process of progressive differentiation occurs within the bone marrow. After the cells are matured, they enter the circulation. Every day, over 2.5 billion red blood cells and platelets, and 50–100 billion granulocytes are produced in this way.\n\nAs well as creating cells, bone marrow is also one of the major sites where defective or aged red blood cells are destroyed.\n\n\nBone is constantly being created and replaced in a process known as remodeling. This ongoing turnover of bone is a process of resorption followed by replacement of bone with little change in shape. This is accomplished through osteoblasts and osteoclasts. Cells are stimulated by a variety of signals, and together referred to as a remodeling unit. Approximately 10% of the skeletal mass of an adult is remodelled each year. The purpose of remodeling is to regulate calcium homeostasis, repair microdamaged bones from everyday stress, and to shape the skeleton during growth.. Repeated stress, such as weight-bearing exercise or bone healing, results in the bone thickening at the points of maximum stress (Wolff's law). It has been hypothesized that this is a result of bone's piezoelectric properties, which cause bone to generate small electrical potentials under stress.\n\nThe action of osteoblasts and osteoclasts are controlled by a number of chemical enzymes that either promote or inhibit the activity of the bone remodeling cells, controlling the rate at which bone is made, destroyed, or changed in shape. The cells also use paracrine signalling to control the activity of each other. For example, the rate at which osteoclasts resorb bone is inhibited by calcitonin and osteoprotegerin. Calcitonin is produced by parafollicular cells in the thyroid gland, and can bind to receptors on osteoclasts to directly inhibit osteoclast activity. Osteoprotegerin is secreted by osteoblasts and is able to bind RANK-L, inhibiting osteoclast stimulation.\n\nOsteoblasts can also be stimulated to increase bone mass through increased secretion of osteoid and by inhibiting the ability of osteoclasts to break down osseous tissue. Increased secretion of osteoid is stimulated by the secretion of growth hormone by the pituitary, thyroid hormone and the sex hormones (estrogens and androgens). These hormones also promote increased secretion of osteoprotegerin. Osteoblasts can also be induced to secrete a number of cytokines that promote reabsorbtion of bone by stimulating osteoclast activity and differentiation from progenitor cells. Vitamin D, parathyroid hormone and stimulation from osteocytes induce osteoblasts to increase secretion of RANK-ligand and interleukin 6, which cytokines then stimulate increased reabsorption of bone by osteoclasts. These same compounds also increase secretion of macrophage colony-stimulating factor by osteoblasts, which promotes the differentiation of progenitor cells into osteoclasts, and decrease secretion of osteoprotegerin.\n\nBone volume is determined by the rates of bone formation and bone resorption. Recent research has suggested that certain growth factors may work to locally alter bone formation by increasing osteoblast activity. Numerous bone-derived growth factors have been isolated and classified via bone cultures. These factors include insulin-like growth factors I and II, transforming growth factor-beta, fibroblast growth factor, platelet-derived growth factor, and bone morphogenetic proteins. Evidence suggests that bone cells produce growth factors for extracellular storage in the bone matrix. The release of these growth factors from the bone matrix could cause the proliferation of osteoblast precursors. Essentially, bone growth factors may act as potential determinants of local bone formation. Research has suggested that cancellous bone volume in postemenopausal osteoporosis may be determined by the relationship between the total bone forming surface and the percent of surface resorption.\n\nA number of diseases can affect bone, including arthritis, fractures, infections, osteoporosis and tumours. Conditions relating to bone can be managed by a variety of doctors, including rheumatologists for joints, and orthopedic surgeons, who may conduct surgery to fix broken bones. Other doctors, such as rehabilitation specialists may be involved in recovery, radiologists in interpreting the findings on imaging, and pathologists in investigating the cause of the disease, and family doctors may play a role in preventing complications of bone disease such as osteoporosis.\n\nWhen a doctor sees a patient, a history and exam will be taken. Bones are then often imaged, called radiography. This might include ultrasound X-ray, CT scan, MRI scan and other imaging such as a Bone scan, which may be used to investigate cancer. Other tests such as a blood test for autoimmune markers may be taken, or a synovial fluid aspirate may be taken.\n\nIn normal bone, fractures occur when there is significant force applied, or repetitive trauma over a long time. Fractures can also occur when a bone is weakened, such as with osteoporosis, or when there is a structural problem, such as when the bone remodels excessively (such as Paget's disease) or is the site of the growth of cancer. Common fractures include wrist fractures and hip fractures, associated with osteoporosis, vertebral fractures associated with high-energy trauma and cancer, and fractures of long-bones. Not all fractures are painful. When serious, depending on the fractures type and location, complications may include flail chest, compartment syndromes or fat embolism.\nCompound fractures involve the bone's penetration through the skin. Some complex fractures can be treated by the use of bone grafting procedures that replace missing bone portions.\n\nFractures and their underlying causes can be investigated by X-rays, CT scans and MRIs. Fractures are described by their location and shape, and several classification systems exist, depending on the location of the fracture. A common long bone fracture in children is a Salter–Harris fracture. When fractures are managed, pain relief is often given, and the fractured area is often immobilised. This is to promote bone healing. In addition, surgical measures such as internal fixation may be used. Because of the immobilisation, people with fractures are often advised to undergo rehabilitation.\n\nThere are several types of tumour that can affect bone; examples of benign bone tumours include osteoma, osteoid osteoma, osteochondroma, osteoblastoma, enchondroma, giant cell tumor of bone, and aneurysmal bone cyst.\n\nCancer can arise in bone tissue, and bones are also a common site for other cancers to spread (metastasise) to. Cancers that arise in bone are called \"primary\" cancers, although such cancers are rare. Metastases within bone are \"secondary\" cancers, with the most common being breast cancer, lung cancer, prostate cancer, thyroid cancer, and kidney cancer. Secondary cancers that affect bone can either destroy bone (called a \"lytic\" cancer) or create bone (a \"sclerotic\" cancer). Cancers of the bone marrow inside the bone can also affect bone tissue, examples including leukemia and multiple myeloma. Bone may also be affected by cancers in other parts of the body. Cancers in other parts of the body may release parathyroid hormone or parathyroid hormone-related peptide. This increases bone reabsorption, and can lead to bone fractures.\n\nBone tissue that is destroyed or altered as a result of cancers is distorted, weakened, and more prone to fracture. This may lead to compression of the spinal cord, destruction of the marrow resulting in bruising, bleeding and immunosuppression, and is one cause of bone pain. If the cancer is metastatic, then there might be other symptoms depending on the site of the original cancer. Some bone cancers can also be felt.\n\nCancers of the bone are managed according to their type, their stage, prognosis, and what symptoms they cause. Many primary cancers of bone are treated with radiotherapy. Cancers of bone marrow may be treated with chemotherapy, and other forms of targeted therapy such as immunotherapy may be used. Palliative care, which focuses on maximising a person's quality of life, may play a role in management, particularly if the likelihood of survival within five years is poor.\n\n\nOsteoporosis is a disease of bone where there is reduced bone mineral density, increasing the likelihood of fractures. Osteoporosis is defined in women by the World Health Organization as a bone mineral density of 2.5 standard deviations below peak bone mass, relative to the age and sex-matched average. This density is measured using dual energy X-ray absorptiometry (DEXA), with the term \"established osteoporosis\" including the presence of a fragility fracture. Osteoporosis is most common in women after menopause, when it is called \"postmenopausal osteoporosis\", but may develop in men and premenopausal women in the presence of particular hormonal disorders and other chronic diseases or as a result of smoking and medications, specifically glucocorticoids. Osteoporosis usually has no symptoms until a fracture occurs. For this reason, DEXA scans are often done in people with one or more risk factors, who have developed osteoporosis and be at risk of fracture.\n\nOsteoporosis treatment includes advice to stop smoking, decrease alcohol consumption, exercise regularly, and have a healthy diet. Calcium supplements may also be advised, as may Vitamin D. When medication is used, it may include bisphosphonates, Strontium ranelate, and hormone replacement therapy.\n\nOsteopathic medicine is a school of medical thought originally developed based on the idea of the link between the musculoskeletal system and overall health, but now very similar to mainstream medicine. , over 77,000 physicians in the United States are trained in Osteopathic medicine colleges.\n\nThe study of bones and teeth is referred to as osteology. It is frequently used in anthropology, archeology and forensic science for a variety of tasks. This can include determining the nutritional, health, age or injury status of the individual the bones were taken from. Preparing fleshed bones for these types of studies can involve the process of maceration.\n\nTypically anthropologists and archeologists study bone tools made by \"Homo sapiens\" and \"Homo neanderthalensis\". Bones can serve a number of uses such as projectile points or artistic pigments, and can also be made from external bones such as antlers.\n\nBird skeletons are very lightweight. Their bones are smaller and thinner, to aid flight. Among mammals, bats come closest to birds in terms of bone density, suggesting that small dense bones are a flight adaptation. Many bird bones have little marrow due to their being hollow.\n\nA bird's beak is primarily made of bone as projections of the mandibles which are covered in keratin.\n\nA deer's antlers are composed of bone which is an unusual example of bone being outside the skin of the animal once the velvet is shed.\n\nThe extinct predatory fish \"Dunkleosteus\" had sharp edges of hard exposed bone along its jaws.\n\nMany animals possess an exoskeleton that is not made of bone. These include insects and crustaceans.\n\nThe proportion of cortical bone that is 80% in the human skeleton may be much lower in other animals, especially in marine mammals and marine turtles, or in various Mesozoic marine reptiles, such as ichthyosaurs, among others.\n\nMany animals particularly herbivores practice osteophagy – the eating of bones. This is presumably carried out in order to replenish lacking phosphate.\n\nMany bone diseases that affect humans also affect other vertebrates – an example of one disorder is skeletal flurosis.\nBones from slaughtered animals have a number of uses. In prehistoric times, they have been used for making bone tools. They have further been used in bone carving, already important in prehistoric art, and also in modern time as crafting materials for buttons, beads, handles, bobbins, calculation aids, head nuts, dice, poker chips, pick-up sticks, ornaments, etc. A special genre is scrimshaw.\n\nBone glue can be made by prolonged boiling of ground or cracked bones, followed by filtering and evaporation to thicken the resulting fluid. Historically once important, bone glue and other animal glues today have only a few specialized uses, such as in antiques restoration. Essentially the same process, with further refinement, thickening and drying, is used to make gelatin.\n\nBroth is made by simmering several ingredients for a long time, traditionally including bones.\n\nGround bones are used as an organic phosphorus-nitrogen fertilizer and as additive in animal feed. Bones, in particular after calcination to bone ash, are used as source of calcium phosphate for the production of bone china and previously also phosphorus chemicals.\n\nBone char, a porous, black, granular material primarily used for filtration and also as a black pigment, is produced by charring mammal bones.\n\nOracle bone script was a writing system used in Ancient china based on inscriptions in bones.\n\nTo point the bone at someone is considered bad luck in some cultures, such as Australian aborigines, such as by the Kurdaitcha.\n\nThe wishbones of fowl have been used for divination, and are still customarily used in a tradition to determine which one of two people pulling on either prong of the bone may make a wish.\n\nVarious cultures throughout history have adopted the custom of shaping an infant's head by the practice of artificial cranial deformation. A widely practised\ncustom in China was that of foot binding to limit the normal growth of the foot.\n\n\n\n"}
{"id": "4100", "url": "https://en.wikipedia.org/wiki?curid=4100", "title": "Bretwalda", "text": "Bretwalda\n\nBretwalda (also brytenwalda and bretenanwealda, sometimes capitalized) is an Old English word. The first record comes from the late 9th-century \"Anglo-Saxon Chronicle\". It is given to some of the rulers of Anglo-Saxon kingdoms from the 5th century onwards who had achieved overlordship of some or all of the other Anglo-Saxon kingdoms. It is unclear whether the word dates back to the 5th century and was used by the kings themselves or whether it is a later, 9th-century, invention. The term \"bretwalda\" also appears in a 10th-century charter of Æthelstan. The literal meaning of the word is disputed and may translate to either 'wide-ruler' or 'Britain-ruler'.\n\nThe rulers of Mercia were generally the most powerful of the Anglo-Saxon kings from the mid 7th century to the early 9th century but are not accorded the title of \"bretwalda\" by the \"Chronicle\", which had an anti-Mercian bias. The \"Annals of Wales\" continued to recognise the kings of Northumbria as \"Kings of the Saxons\" until the death of Osred I of Northumbria in 716.\n\n\n\n\n\nThe first syllable of the term \"bretwalda\" may be related to \"Briton\" or \"Britain\". The second element is taken to mean 'ruler' or 'sovereign', though is more literally 'wielder'. Thus, this interpretation would mean 'sovereign of Britain' or 'wielder of Britain'. The word may be a compound containing the Old English adjective \"brytten\" (from the verb \"breotan\" meaning 'to break' or 'to disperse'), an element also found in the terms \"bryten rice\" ('kingdom'), \"bryten-grund\" ('the wide expanse of the earth') and \"bryten cyning\" ('king whose authority was widely extended'). Though the origin is ambiguous, the draughtsman of the charter issued by Æthelstan used the term in a way that can only mean 'wide-ruler'.\n\nThe latter etymology was first suggested by John Mitchell Kemble who alluded that \"of six manuscripts in which this passage occurs, one only reads \"Bretwalda\": of the remaining five, four have \"Bryten-walda\" or \"-wealda\", and one \"Breten-anweald\", which is precisely synonymous with Brytenwealda\"; that Æthelstan was called \"brytenwealda ealles ðyses ealondes\", which Kemble translates as 'ruler of all these islands'; and that \"bryten-\" is a common prefix to words meaning 'wide or general dispersion' and that the similarity to the word \"bretwealh\" ('Briton') is \"merely accidental\".\n\nThe first recorded use of the term \"Bretwalda\" comes from a West Saxon chronicle of the late 9th century that applied the term to Ecgberht, who ruled Wessex from 802 to 839. The chronicler also wrote down the names of seven kings that Bede listed in his \"Historia ecclesiastica gentis Anglorum\" in 731. All subsequent manuscripts of the \"Chronicle\" use the term \"Brytenwalda\", which may have represented the original term or derived from a common error.\n\nThere is no evidence that the term was a title that had any practical use, with implications of formal rights, powers and office, or even that it had any existence before the 9th-century. Bede wrote in Latin and never used the term and his list of kings holding \"imperium\" should be treated with caution, not least in that he overlooks kings such as Penda of Mercia, who clearly held some kind of dominance during his reign. Similarly, in his list of bretwaldas, the West Saxon chronicler ignored such Mercian kings as Offa.\n\nThe use of the term \"Bretwalda\" was the attempt by a West Saxon chronicler to make some claim of West Saxon kings to the whole of Great Britain. The concept of the overlordship of the whole of Britain was at least recognised in the period, whatever was meant by the term. Quite possibly it was a survival of a Roman concept of \"Britain\": it is significant that, while the hyperbolic inscriptions on coins and titles in charters often included the title \"rex Britanniae\", when England was unified the title used was \"rex Angulsaxonum\", ('king of the Anglo-Saxons'.)\n\nFor some time, the existence of the word \"bretwalda\" in the \"Anglo-Saxon Chronicle\", which was based in part on the list given by Bede in his \"Historia Ecclesiastica\", led historians to think that there was perhaps a \"title\" held by Anglo-Saxon overlords. This was particularly attractive as it would lay the foundations for the establishment of an English monarchy. The 20th-century historian Frank Stenton said of the Anglo-Saxon chronicler that \"his inaccuracy is more than compensated by his preservation of the English title applied to these outstanding kings\". He argued that the term \"bretwalda\" \"falls into line with the other evidence which points to the Germanic origin of the earliest English institutions\".\n\nOver the later 20th century, this assumption was increasingly challenged. Patrick Wormald interpreted it as \"less an objectively realized office than a subjectively perceived status\" and emphasised the partiality of its usage in favour of Southumbrian rulers. In 1991, Steven Fanning argued that \"it is unlikely that the term ever existed as a title or was in common usage in Anglo-Saxon England\". The fact that Bede never mentioned a special title for the kings in his list implies that he was unaware of one. In 1995, Simon Keynes observed that \"if Bede's concept of the Southumbrian overlord, and the chronicler's concept of the 'Bretwalda', are to be regarded as artificial constructs, which have no validity outside the context of the literary works in which they appear, we are released from the assumptions about political development which they seem to involve... we might ask whether kings in the eighth and ninth centuries were quite so obsessed with the establishment of a pan-Southumbrian state\".\n\nModern interpretations view the concept of \"bretwalda\" overlordship as complex and an important indicator of how a 9th-century chronicler interpreted history and attempted to insert the increasingly powerful Saxon kings into that history.\n\nA complex array of dominance and subservience existed during the Anglo-Saxon period. A king who used charters to grant land in another kingdom indicated such a relationship. If a king held sway over a large kingdom, such as when the Mercians dominated the East Anglians, the relationship would have been more equal than in the case of the Mercian dominance of the Hwicce, which was a comparatively small kingdom. Mercia was arguably the most powerful Anglo-Saxon kingdom for much of the late 7th though 8th centuries, though Mercian kings are missing from the two main \"lists\". For Bede, Mercia was a traditional enemy of his native Northumbria and he regarded powerful kings such as the pagan Penda as standing in the way of the Christian conversion of the Anglo-Saxons. Bede omits them from his list, even though it is evident that Penda held a considerable degree of power. Similarly powerful Mercia kings such as Offa are missed out of the West Saxon \"Anglo-Saxon Chronicle\", which sought to demonstrate the legitimacy of their kings to rule over other Anglo-Saxon peoples.\n\n\n\n"}
{"id": "4101", "url": "https://en.wikipedia.org/wiki?curid=4101", "title": "Brouwer fixed-point theorem", "text": "Brouwer fixed-point theorem\n\nBrouwer's fixed-point theorem is a fixed-point theorem in topology, named after Luitzen Brouwer. It states that for any continuous function formula_1 mapping a compact convex set into itself there is a point formula_2 such that formula_3. The simplest forms of Brouwer's theorem are for continuous functions formula_1 from a closed interval formula_5 in the real numbers to itself or from a closed disk formula_6 to itself. A more general form than the latter is for continuous functions from a convex compact subset formula_7 of Euclidean space to itself.\n\nAmong hundreds of fixed-point theorems, Brouwer's is particularly well known, due in part to its use across numerous fields of mathematics.\nIn its original field, this result is one of the key theorems characterizing the topology of Euclidean spaces, along with the Jordan curve theorem, the hairy ball theorem and the Borsuk–Ulam theorem.\nThis gives it a place among the fundamental theorems of topology. The theorem is also used for proving deep results about differential equations and is covered in most introductory courses on differential geometry.\nIt appears in unlikely fields such as game theory. In economics, Brouwer's fixed-point theorem and its extension, the Kakutani fixed-point theorem, play a central role in the proof of existence of general equilibrium in market economies as developed in the 1950s by economics Nobel prize winners Kenneth Arrow and Gérard Debreu.\n\nThe theorem was first studied in view of work on differential equations by the French mathematicians around Poincaré and Picard.\nProving results such as the Poincaré–Bendixson theorem requires the use of topological methods.\nThis work at the end of the 19th century opened into several successive versions of the theorem. The general case was first proved in 1910 by Jacques Hadamard and by Luitzen Egbertus Jan Brouwer.\n\nThe theorem has several formulations, depending on the context in which it is used and its degree of generalization.\nThe simplest is sometimes given as follows:\n\nThis can be generalized to an arbitrary finite dimension:\n\nA slightly more general version is as follows:\n\nAn even more general form is better known under a different name:\n\nThe theorem holds only for sets that are \"compact\" (thus, in particular, bounded and closed) and \"convex\". The following examples show why these requirements are important.\n\nConsider the function\nwhich is a continuous function from formula_9 to itself. As it shifts every point to the right, it cannot have a fixed point. Note that formula_9 is convex and closed, but not bounded.\n\nConsider the function\nwhich is a continuous function from the open interval (−1,1) to itself. In this interval, it shifts every point to the right, so it cannot have a fixed point. Note that (−1,1) is convex and bounded, but not closed. The function \"f\" \"does\" have a fixed point for the closed interval [−1,1], namely \"f\"(1) = 1.\n\nNote that convexity is not strictly necessary for BFPT. Because the properties involved (continuity, being a fixed point) are invariant under homeomorphisms, BFPT is equivalent to forms in which the domain is required to be a closed unit ball formula_12. For the same reason it holds for every set that is homeomorphic to a closed ball (and therefore also closed, bounded, connected, without holes, etc.).\n\nThe following example shows that BFPT doesn't work for domains with holes. Consider the following function, defined in polar coordinates:\nwhich is a continuous function from the unit circle to itself. It rotates every point on the unit circle 45 degrees counterclockwise, so it cannot have a fixed point. Note that the unit circle is closed and bounded, but it has a hole (and so it is not convex). The function \"f\" \"does\" have a fixed point for the unit disc, since it takes the origin to itself.\n\nA formal generalization of BFPT for \"hole-free\" domains can be derived from the Lefschetz fixed-point theorem.\n\nThe continuous function in this theorem is not required to be bijective or even surjective.\n\nThe theorem has several \"real world\" illustrations. Here are some examples.\n\n1. Take two sheets of graph paper of equal size with coordinate systems on them, lay one flat on the table and crumple up (without ripping or tearing) the other one and place it, in any fashion, on top of the first so that the crumpled paper does not reach outside the flat one. There will then be at least one point of the crumpled sheet that lies directly above its corresponding point (i.e. the point with the same coordinates) of the flat sheet. This is a consequence of the \"n\" = 2 case of Brouwer's theorem applied to the continuous map that assigns to the coordinates of every point of the crumpled sheet the coordinates of the point of the flat sheet immediately beneath it.\n\n2. Take an ordinary map of a country, and suppose that that map is laid out on a table inside that country. There will always be a \"You are Here\" point on the map which represents that same point in the country.\n\n3. In three dimensions the consequence of the Brouwer fixed-point theorem is that, no matter how much you stir a cocktail in a glass (or think about milk shake), when the liquid has come to rest, some point in the liquid will end up in exactly the same place in the glass as before you took any action, assuming that the final position of each point is a continuous function of its original position, that the liquid after stirring is contained within the space originally taken up by it, and that the glass (and stirred surface shape) maintain a convex volume. Ordering a cocktail shaken, not stirred defeats the convexity condition (\"shaking\" being defined as a dynamic series of non-convex inertial containment states in the vacant headspace under a lid). In that case, the theorem would not apply, and thus all points of the liquid disposition are potentially displaced from the original state.\n\nThe theorem is supposed to have originated from Brouwer's observation of a cup of coffee.\nIf one stirs to dissolve a lump of sugar, it appears there is always a point without motion.\nHe drew the conclusion that at any moment, there is a point on the surface that is not moving.\nThe fixed point is not necessarily the point that seems to be motionless, since the centre of the turbulence moves a little bit.\nThe result is not intuitive, since the original fixed point may become mobile when another fixed point appears.\n\nBrouwer is said to have added: \"I can formulate this splendid result different, I take a horizontal sheet, and another identical one which I crumple, flatten and place on the other. Then a point of the crumpled sheet is in the same place as on the other sheet.\"\nBrouwer \"flattens\" his sheet as with a flat iron, without removing the folds and wrinkles. This example is better than the coffee cup one as it shows that uniqueness of the fixed point may fail. This distinguishes Brouwer's result from other fixed-point theorems, such as Banach's, that guarantee uniqueness.\n\nIn one dimension, the result is intuitive and easy to prove. The continuous function \"f\" is defined on a closed interval [\"a\", \"b\"] and takes values in the same interval. Saying that this function has a fixed point amounts to saying that its graph (dark green in the figure on the right) intersects that of the function defined on the same interval [\"a\", \"b\"] which maps \"x\" to \"x\" (light green).\n\nIntuitively, any continuous line from the left edge of the square to the right edge must necessarily intersect the green diagonal. To prove this, consider the function \"g\" which maps \"x\" to \"f\"(\"x\") - \"x\". It is ≥ 0 on \"a\" and ≤ 0 on \"b\". By the intermediate value theorem, \"g\" has a zero in [\"a\", \"b\"]; this zero is a fixed point.\n\nBrouwer is said to have expressed this as follows: \"Instead of examining a surface, we will prove the theorem about a piece of string. Let us begin with the string in an unfolded state, then refold it. Let us flatten the refolded string. Again a point of the string has not changed its position with respect to its original position on the unfolded string.\"\n\nThe Brouwer fixed point theorem was one of the early achievements of algebraic topology, and is the basis of more general fixed point theorems which are important in functional analysis. The case \"n\" = 3 first was proved by Piers Bohl in 1904 (published in \"Journal für die reine und angewandte Mathematik\"). It was later proved by L. E. J. Brouwer in 1909. Jacques Hadamard proved the general case in 1910, and Brouwer found a different proof in the same year. Since these early proofs were all non-constructive indirect proofs, they ran contrary to Brouwer's intuitionist ideals. Methods to construct (approximations to) fixed points guaranteed by Brouwer's theorem are now known.\n\nTo understand the prehistory of Brouwer's fixed point theorem one needs to pass through differential equations. At the end of the 19th century, the old problem of the stability of the solar system returned into the focus of the mathematical community.\nIts solution required new methods. As noted by Henri Poincaré, who worked on the three-body problem, there is no hope to find an exact solution: \"Nothing is more proper to give us an idea of the hardness of the three-body problem, and generally of all problems of Dynamics where there is no uniform integral and the Bohlin series diverge.\"\nHe also noted that the search for an approximate solution is no more efficient: \"the more we seek to obtain precise approximations, the more the result will diverge towards an increasing imprecision\".\n\nHe studied a question analogous to that of the surface movement in a cup of coffee. What can we say, in general, about the trajectories on a surface animated by a constant flow? Poincaré discovered that the answer can be found in what we now call the topological properties in the area containing the trajectory. If this area is compact, i.e. both closed and bounded, then the trajectory either becomes stationary, or it approaches a limit cycle. Poincaré went further; if the area is of the same kind as a disk, as is the case for the cup of coffee, there must necessarily be a fixed point. This fixed point is invariant under all functions which associate to each point of the original surface its position after a short time interval \"t\". If the area is a circular band, or if it is not closed, then this is not necessarily the case.\n\nTo understand differential equations better, a new branch of mathematics was born. Poincaré called it \"analysis situs\". The French Encyclopædia Universalis defines it as the branch which \"treats the properties of an object that are invariant if it is deformed in any continuous way, without tearing\". In 1886, Poincaré proved a result that is equivalent to Brouwer's fixed-point theorem, although the connection with the subject of this article was not yet apparent. A little later, he developed one of the fundamental tools for better understanding the analysis situs, now known as the fundamental group or sometimes the Poincaré group. This method can be used for a very compact proof of the theorem under discussion.\n\nPoincaré's method was analogous to that of Émile Picard, a contemporary mathematician who generalized the Cauchy–Lipschitz theorem. Picard's approach is based on a result that would later be formalised by another fixed-point theorem, named after Banach. Instead of the topological properties of the domain, this theorem uses the fact that the function in question is a contraction.\n\nAt the dawn of the 20th century, the interest in analysis situs did not stay unnoticed. However, the necessity of a theorem equivalent to the one discussed in this article was not yet evident. Piers Bohl, a Latvian mathematician, applied topological methods to the study of differential equations. In 1904 he proved the three-dimensional case of our theorem, but his publication was not noticed.\n\nIt was Brouwer, finally, who gave the theorem its first patent of nobility. His goals were different from those of Poincaré. This mathematician was inspired by the foundations of mathematics, especially mathematical logic and topology. His initial interest lay in an attempt to solve Hilbert's fifth problem. In 1909, during a voyage to Paris, he met Poincaré, Hadamard, and Borel. The ensuing discussions convinced Brouwer of the importance of a better understanding of Euclidean spaces, and were the origin of a fruitful exchange of letters with Hadamard. For the next four years, he concentrated on the proof of certain great theorems on this question. In 1912 he proved the hairy ball theorem for the two-dimensional sphere, as well as the fact that every continuous map from the two-dimensional ball to itself has a fixed point. These two results in themselves were not really new. As Hadamard observed, Poincaré had shown a theorem equivalent to the hairy ball theorem. The revolutionary aspect of Brouwer's approach was his systematic use of recently developed tools such as homotopy, the underlying concept of the Poincaré group. In the following year, Hadamard generalised the theorem under discussion to an arbitrary finite dimension, but he employed different methods. Hans Freudenthal comments on the respective roles as follows: \"Compared to Brouwer's revolutionary methods, those of Hadamard were very traditional, but Hadamard's participation in the birth of Brouwer's ideas resembles that of a midwife more than that of a mere spectator.\"\n\nBrouwer's approach yielded its fruits, and in 1910 he also found a proof that was valid for any finite dimension, as well as other key theorems such as the invariance of dimension. In the context of this work, Brouwer also generalized the Jordan curve theorem to arbitrary dimension and established the properties connected with the degree of a continuous mapping. This branch of mathematics, originally envisioned by Poincaré and developed by Brouwer, changed its name. In the 1930s, analysis situs became algebraic topology.\n\nThe theorem proved its worth in more than one way. During the 20th century numerous fixed-point theorems were developed, and even a branch of mathematics called fixed-point theory.\nBrouwer's theorem is probably the most important. It is also among the foundational theorems on the topology of topological manifolds and is often used to prove other important results such as the Jordan curve theorem.\n\nBesides the fixed-point theorems for more or less contracting functions, there are many that have emerged directly or indirectly from the result under discussion. A continuous map from a closed ball of Euclidean space to its boundary cannot be the identity on the boundary. Similarly, the Borsuk–Ulam theorem says that a continuous map from the \"n\"-dimensional sphere to R has a pair of antipodal points that are mapped to the same point. In the finite-dimensional case, the Lefschetz fixed-point theorem provided from 1926 a method for counting fixed points. In 1930, Brouwer's fixed-point theorem was generalized to Banach spaces. This generalization is known as Schauder's fixed-point theorem, a result generalized further by S. Kakutani to multivalued functions. One also meets the theorem and its variants outside topology. It can be used to prove the Hartman-Grobman theorem, which describes the qualitative behaviour of certain differential equations near certain equilibria. Similarly, Brouwer's theorem is used for the proof of the Central Limit Theorem. The theorem can also be found in existence proofs for the solutions of certain partial differential equations.\n\nOther areas are also touched. In game theory, John Nash used the theorem to prove that in the game of Hex there is a winning strategy for white. In economics, P. Bich explains that certain generalizations of the theorem show that its use is helpful for certain classical problems in game theory and generally for equilibria (Hotelling's law), financial equilibria and incomplete markets.\n\nBrouwer's celebrity is not exclusively due to his topological work. The proofs of his great topological theorems are not constructive, and Brouwer's dissatisfaction with this is partly what led him to articulate the idea of constructivity. He became the originator and zealous defender of a way of formalising mathematics that is known as intuitionism, which at the time made a stand against set theory. The fixed-point theorem is, as he originally stated it, \"false\" in intuitionism, and Brouwer disavowed it, proposing instead alternative versions to be constructively proven.\n\nBrouwer's original 1911 proof relied on the notion of the degree of a continuous mapping. Modern accounts of the proof can also be found in the literature.\n\nLet formula_14 denote the closed unit ball in formula_15 centered at the origin. Suppose for simplicitly that formula_16 is continuously differentiable. A regular value of formula_1 is a point formula_18 such that the Jacobian of formula_1 is non-singular at every point of the preimage of formula_20. In particular, by the inverse function theorem, every point of the preimage of formula_1 lies in formula_22 (the interior of formula_23). The degree of formula_1 at a regular value formula_18 is defined as the sum of the signs of the Jacobian determinant of formula_1 over the preimages of formula_20 under formula_1:\n\nThe degree is, roughly speaking, the number of \"sheets\" of the preimage \"f\" lying over a small open set around \"p\", with sheets counted oppositely if they are oppositely oriented. This is thus a generalization of winding number to higher dimensions.\n\nThe degree satisfies the property of \"homotopy invariance\": let formula_1 and formula_31 be two continuously differentiable functions, and formula_32 for formula_33. Suppose that the point formula_20 is a regular value of formula_35 for all \"t\". Then formula_36.\n\nIf there is no fixed point of the boundary of formula_23, then the function \nis a homotopy from formula_39 to the identity function. The identity function has degree one at every point. In particular, the identity function has degree one at the origin, so formula_31 also has degree one at the origin. As a consequence, the preimage formula_41 is not empty. The elements of formula_41 are precisely the fixed points of the original function \"f\".\n\nThis requires some work to make fully general. The definition of degree must be extended to singular values of \"f\", and then to continuous functions. The more modern advent of homology theory simplifies the construction of the degree, and so has become a standard proof in the literature.\n\nThe proof uses the observation that the boundary of \"D\" is \"S\", the (\"n\" − 1)-sphere.\nThe argument proceeds by contradiction, supposing that a continuous function \"f\" : \"D\" → \"D\" has \"no\" fixed point, and then attempting to derive an inconsistency, which proves that the function must in fact have a fixed point. For each \"x\" in \"D\", there is only one straight line that passes through \"f\"(\"x\") and \"x\", because it must be the case that \"f\"(\"x\") and \"x\" are distinct by hypothesis (recall that \"f\" having no fixed points means that \"f\"(\"x\") ≠ \"x\"). Following this line from \"f\"(\"x\") through \"x\" leads to a point on \"S\", denoted by \"F\"(\"x\"). This defines a continuous function \"F\" : \"D\" → \"S\", which is a special type of continuous function known as a retraction: every point of the codomain (in this case \"S\") is a fixed point of the function.\n\nIntuitively it seems unlikely that there could be a retraction of \"D\" onto \"S\", and in the case \"n\" = 1 it is obviously impossible because \"S\" (i.e., the endpoints of the closed interval \"D\") is not even connected. The case \"n\" = 2 is less obvious, but can be proven by using basic arguments involving the fundamental groups of the respective spaces: the retraction would induce an injective group homomorphism from the fundamental group of \"S\" to that of \"D\", but the first group is isomorphic to Z while the latter group is trivial, so this is impossible. The case \"n\" = 2 can also be proven by contradiction based on a theorem about non-vanishing vector fields.\n\nFor \"n\" > 2, however, proving the impossibility of the retraction is more difficult. One way is to make use of homology groups: the homology \"H\"(\"D\") is trivial, while \"H\"(\"S\") is infinite cyclic. This shows that the retraction is impossible, because again the retraction would induce an injective group homomorphism from the latter to the former group.\n\nTo prove that a map has fixed points, one can assume that it is smooth, because if a map has no fixed points then convolving it with a smooth function of sufficiently small support and integral equal to one produces a smooth function with no fixed points. As in the proof using homology, one is reduced to proving that there is no smooth retraction \"f\" from the ball \"B\" onto its boundary \"∂B\". If ω is a volume form on the boundary then by Stokes Theorem,\ngiving a contradiction.\n\nMore generally, this shows that there is no smooth retraction from any non-empty smooth orientable compact manifold onto its boundary. The proof using Stokes's theorem is closely related to the proof using homology (or rather cohomology), because the form ω generates the de Rham cohomology group \"H\"(\"∂B\") used in the cohomology proof.\n\nThe BFPT can be proved based on Sperner's lemma. We now give an outline of the proof for the special case in which \"f\" is a function from the unit \"n\"-simplex to itself, i.e.: \nWhere:\n\nFor every point formula_46, also formula_47. Hence the sum of their coordinates is equal:\n\nHence, by the pigeonhole principle, for every formula_46 there must be an index formula_50 such that the formula_51-th coordinate of formula_52 is weakly larger than the formula_51-th coordinate of its image under \"f\":\n\nMoreover, if formula_52 lies on a \"k\"-dimensional sub-face of formula_56, then by the same argument, the index formula_51 can be selected from among the (\"k\"+1) coordinates which are not zero on this sub-face.\n\nWe now use this fact to construct a Sperner coloring. For every triangulation of formula_56, the color of every vertex formula_52 is an index formula_51 such that formula_54.\n\nBy construction, this is a Sperner coloring. Hence, by Sperner's lemma, there is an \"n\"-dimensional simplex whose vertices are colored with the entire set of (\"n\"+1) available colors.\n\nBecause \"f\" is continuous, this simplex can be made arbitrarily small by choosing an arbitrarily fine triangulation. Hence, there must be a point formula_52 which satisfies the labeling condition in all coordinates, i.e.:\n\nBecause the sum of the coordinates of formula_52 and formula_65 must be equal, all these inequalities must actually be equalities. But this means that:\n\nI.e, formula_52 is a fixed point of formula_1.\n\nThe proof can be extended to every object which is homeomorphic to formula_56.\n\nThere is also a quick proof, by Morris Hirsch, based on the impossibility of a differentiable retraction. The indirect proof starts by noting that the map \"f\" can be approximated by a smooth map retaining the property of not fixing a point; this can be done by using the Weierstrass approximation theorem, for example. One then defines a retraction as above which must now be differentiable. Such a retraction must have a non-singular value, by Sard's theorem, which is also non-singular for the restriction to the boundary (which is just the identity). Thus the inverse image would be a 1-manifold with boundary. The boundary would have to contain at least two end points, both of which would have to lie on the boundary of the original ball—which is impossible in a retraction.\n\nKellogg, Li, and Yorke turned Hirsch's proof into a constructive proof by observing that the retract is in fact defined everywhere except at the fixed points. For almost any point, q, on the boundary, (assuming it is not a fixed point) the one manifold with boundary mentioned above does exist and the only possibility is that it leads from q to a fixed point. It is an easy numerical task to follow such a path from q to the fixed point so the method is essentially constructive. Chow, Mallet-Paret, and Yorke gave a conceptually similar path-following version of the homotopy proof which extends to a wide variety of related problems.\n\nA variation of the preceding proof does not employ the Sard's theorem, and goes as follows. If \"r\" : \"B\"→∂\"B\"   is a smooth retraction, one considers the smooth deformation \"g(x) := t r(x) + (1-t)x,\" and the smooth function\nDifferentiating under the sign of integral it is not difficult to check that \"φ′(t)=0\" for all \"t\", so \"φ\" is a constant function, which is a contradiction because \"φ(0)\" is the \"n\"-dimensional volume of the ball, while \"φ(1)\" is zero. The geometric idea is that \"φ(t)\" is the oriented area of \"g(B)\" (that is, the Lebesgue measure of the image of the ball via \"g\", taking into account multiplicity and orientation), and should remain constant (as it is very clear in the one-dimensional case). On the other hand, as the parameter \"t\" passes form \"0\" to \"1\" the map \"g\" transforms continuously from the identity map of the ball, to the retraction \"r\", which is a contradiction since the oriented area of the identity coincides with the volume of the ball, while the oriented area of \"r\" is necessarily \"0\", as its image is the boundary of the ball, a set of null measure.\n\nA quite different proof given by David Gale is based on the game of Hex. The basic theorem about Hex is that no game can end in a draw. This is equivalent to the Brouwer fixed-point theorem for dimension 2. By considering \"n\"-dimensional versions of Hex, one can prove in general that Brouwer's theorem is equivalent to the determinacy theorem for Hex.\n\nThe Lefschetz fixed-point theorem says that if a continuous map \"f\" from a finite simplicial complex \"B\" to itself has only isolated fixed points, then the number of fixed points counted with multiplicities (which may be negative) is equal to the Lefschetz number\nand in particular if the Lefschetz number is nonzero then \"f\" must have a fixed point. If \"B\" is a ball (or more generally is contractible) then the Lefschetz number is one because the only non-zero homology group is :formula_72 and \"f\" acts as the identity on this group, so \"f\" has a fixed point.\n\nIn reverse mathematics, Brouwer's theorem can be proved in the system WKL, and conversely over the base system RCA Brouwer's theorem for a square implies the weak König's lemma, so this gives a precise description of the strength of Brouwer's theorem.\n\nThe Brouwer fixed-point theorem forms the starting point of a number of more general fixed-point theorems.\n\nThe straightforward generalization to infinite dimensions, i.e. using the unit ball of an arbitrary Hilbert space instead of Euclidean space, is not true. The main problem here is that the unit balls of infinite-dimensional Hilbert spaces are not compact. For example, in the Hilbert space ℓ of square-summable real (or complex) sequences, consider the map \"f\" : ℓ → ℓ which sends a sequence (\"x\") from the closed unit ball of ℓ to the sequence (\"y\") defined by\nIt is not difficult to check that this map is continuous, has its image in the unit sphere of ℓ, but does not have a fixed point.\n\nThe generalizations of the Brouwer fixed-point theorem to infinite dimensional spaces therefore all include a compactness assumption of some sort, and in addition also often an assumption of convexity. See fixed-point theorems in infinite-dimensional spaces for a discussion of these theorems.\n\nThere is also finite-dimensional generalization to a larger class of spaces: If formula_74 is a product of finitely many chainable continua, then every continuous function formula_75 has a fixed point, where a chainable continuum is a (usually but in this case not necessarily metric) compact Hausdorff space of which every open cover has a finite open refinement formula_76, such that formula_77 if and only if formula_78. Examples of chainable continua include compact connected linearly ordered spaces and in particular closed intervals of real numbers.\n\nThe Kakutani fixed point theorem generalizes the Brouwer fixed-point theorem in a different direction: it stays in R, but considers upper hemi-continuous correspondences (functions that assign to each point of the set a subset of the set). It also requires compactness and convexity of the set.\n\nThe Lefschetz fixed-point theorem applies to (almost) arbitrary compact topological spaces, and gives a condition in terms of singular homology that guarantees the existence of fixed points; this condition is trivially satisfied for any map in the case of \"D\".\n\n\n\n"}
{"id": "4106", "url": "https://en.wikipedia.org/wiki?curid=4106", "title": "Benzoic acid", "text": "Benzoic acid\n\nBenzoic acid , CHO (or CHCOOH), is a colorless crystalline solid and a simple aromatic carboxylic acid. The name is derived from gum benzoin, which was for a long time its only known source. Benzoic acid occurs naturally in many plants and it serves as an intermediate in the biosynthesis of many secondary metabolites. Salts of benzoic acid are used as food preservatives and benzoic acid is an important precursor for the industrial synthesis of many other organic substances. The salts and esters of benzoic acid are known as benzoates .\n\nBenzoic acid was discovered in the sixteenth century. The dry distillation of gum benzoin was first described by Nostradamus (1556), and then by Alexius Pedemontanus (1560) and Blaise de Vigenère (1596).\n\nPioneer work in 1830 through a variety of experiences based on amygdalin, obtained from bitter almonds (the fruit of \"Prunus dulcis\") oil by Pierre Robiquet and Antoine Boutron-Charlard, two French chemists, had produced benzaldehyde but they failed in working out a proper interpretation of the structure of amygdalin that would account for it, and thus missed the identification of the benzoyl radical CHO.\nThis last step was achieved some few months later (1832) by Justus von Liebig and Friedrich Wöhler, who determined the composition of benzoic acid. These latter also investigated how hippuric acid is related to benzoic acid.\n\nIn 1875 Salkowski discovered the antifungal abilities of benzoic acid, which was used for a long time in the preservation of benzoate-containing cloudberry fruits.\n\nIt is also one of the chemical compounds found in castoreum. This compound is gathered from the castor sacs of the North American beaver.\n\nBenzoic acid is produced commercially by partial oxidation of toluene with oxygen. The process is catalyzed by cobalt or manganese naphthenates. The process uses cheap raw materials, and proceeds in high yield.\n\nBenzoic acid is cheap and readily available, so the laboratory synthesis of benzoic acid is mainly practiced for its pedagogical value. It is a common undergraduate preparation.\n\nBenzoic acid can be purified by recrystallization from water because of its high solubility in hot water and poor solubility in cold water. The avoidance of organic solvents for the recrystallization makes this experiment particularly safe. This process usually gives a yield of around 65% The solubility of benzoic acid in over 40 solvents with references to original sources can be found as part of the Open Notebook Science Challenge.\n\nLike other nitriles and amides, benzonitrile and benzamide can be hydrolyzed to benzoic acid or its conjugate base in acid or basic conditions.\n\nThe base-induced disproportionation of benzaldehyde, the Cannizzaro reaction, affords equal amounts of benzoate and benzyl alcohol; the latter can be removed by distillation.\n\nBromobenzene can be converted to benzoic acid by \"carboxylation\" of the intermediate phenylmagnesium bromide. This synthesis offers a convenient exercise for students to carry out a Grignard reaction, an important class of carbon–carbon bond forming reaction in organic chemistry.\n\nBenzyl alcohol is refluxed with potassium permanganate or other oxidizing reagents in water. The mixture is hot filtered to remove manganese dioxide and then allowed to cool to afford benzoic acid.\n\nBenzoic acid can be prepared by oxidation of benzyl chloride in the presence of alkaline KMnO:\n\nThe first industrial process involved the reaction of benzotrichloride (trichloromethyl benzene) with calcium hydroxide in water, using iron or iron salts as catalyst. The resulting calcium benzoate is converted to benzoic acid with hydrochloric acid. The product contains significant amounts of chlorinated benzoic acid derivatives. For this reason, benzoic acid for human consumption was obtained by dry distillation of gum benzoin. Food-grade benzoic acid is now produced synthetically.\n\nBenzoic acid is mainly consumed in the production of phenol by oxidative decarboxylation at 300−400 °C:\nThe temperature required can be lowered to 200 °C by the addition of catalytic amounts of copper(II) salts. The phenol can be converted to cyclohexanol, which is a starting material for nylon synthesis.\n\nBenzoate plasticizers, such as the glycol-, diethyleneglycol-, and triethyleneglycol esters, are obtained by transesterification of methyl benzoate with the corresponding diol. Alternatively these species arise by treatment of benzoylchloride with the diol. These plasticizers are used similarly to those derived from terephthalic acid ester.\n\nBenzoic acid and its salts are used as a food preservatives, represented by the E-numbers E210, E211, E212, and E213. Benzoic acid inhibits the growth of mold, yeast and some bacteria. It is either added directly or created from reactions with its sodium, potassium, or calcium salt. The mechanism starts with the absorption of benzoic acid into the cell. If the intracellular pH changes to 5 or lower, the anaerobic fermentation of glucose through phosphofructokinase is decreased by 95%. The efficacy of benzoic acid and benzoate is thus dependent on the pH of the food. Acidic food and beverage like fruit juice (citric acid), sparkling drinks (carbon dioxide), soft drinks (phosphoric acid), pickles (vinegar) or other acidified food are preserved with benzoic acid and benzoates.\n\nTypical levels of use for benzoic acid as a preservative in food are between 0.05–0.1%. Foods in which benzoic acid may be used and maximum levels for its application are controlled by international food law.\n\nConcern has been expressed that benzoic acid and its salts may react with ascorbic acid (vitamin C) in some soft drinks, forming small quantities of benzene.\nBenzoic acid is a constituent of Whitfield's ointment which is used for the treatment of fungal skin diseases such as tinea, ringworm, and athlete's foot. As the principal component of gum benzoin, benzoic acid is also a major ingredient in both tincture of benzoin and Friar's balsam. Such products have a long history of use as topical antiseptics and inhalant decongestants.\n\nBenzoic acid was used as an expectorant, analgesic, and antiseptic in the early 20th century.\n\nBenzoic acid is a precursor to benzoyl chloride, CHC(O)Cl by treatment with thionyl chloride, phosgene or one of the chlorides of phosphorus. Benzoyl chloride is an important starting material for several benzoic acid derivates like benzyl benzoate, which is used in artificial flavours and insect repellents.\n\nIn teaching laboratories, benzoic acid is a common standard for calibrating a bomb calorimeter.\n\nBenzoic acid is relatively nontoxic. It is excreted as hippuric acid. Benzoic acid is metabolized by butyrate-CoA ligase into an intermediate product, benzoyl-CoA, which is then metabolized by glycine \"N\"-acyltransferase into hippuric acid.\n\nBenzoic acid occurs naturally as do its esters in many plant and animal species. Appreciable amounts have been found in most berries (around 0.05%). Ripe fruits of several \"Vaccinium\" species (e.g., cranberry, \"V. vitis macrocarpon\"; bilberry, \"V. myrtillus\") contain as much as 0.03–0.13% free benzoic acid. Benzoic acid is also formed in apples after infection with the fungus \"Nectria galligena\". Among animals, benzoic acid has been identified primarily in omnivorous or phytophageous species, e.g., in viscera and muscles of the rock ptarmigan (\"Lagopus muta\") as well as in gland secretions of male muskoxen (\"Ovibos moschatus\") or Asian bull elephants (\"Elephas maximus\").\n\nGum benzoin contains up to 20% of benzoic acid and 40% benzoic acid esters.\n\n\"Cryptanaerobacter phenolicus\" is a bacterium species that produces benzoate from phenol via 4-hydroxybenzoate\n\nBenzoic acid is present as part of hippuric acid (\"N\"-benzoylglycine) in urine of mammals, especially herbivores (Gr. \"hippos\" = horse; \"ouron\" = urine). Humans produce about 0.44 g/L hippuric acid in their urine, and if the person is exposed to toluene or benzoic acid, it can rise above that level.\n\nFor humans, the World Health Organization's International Programme on Chemical Safety (IPCS) suggests a provisional tolerable intake would be 5 mg/kg body weight per day. Cats have a significantly lower tolerance against benzoic acid and its salts than rats and mice. Lethal dose for cats can be as low as 300 mg/kg body weight. The oral for rats is 3040 mg/kg, for mice it is 1940–2263 mg/kg.\n\nIn Taipei, Taiwan, a city health survey in 2010 found that 30% of dried and pickled food products had benzoic acid.\n\nReactions of benzoic acid can occur at either the aromatic ring or at the carboxyl group:\n\nElectrophilic aromatic substitution reaction will take place mainly in 3-position due to the electron-withdrawing carboxylic group; i.e. benzoic acid is \"meta\" directing.\n\nThe second substitution reaction (on the right) is slower because the first nitro group is deactivating. Conversely, if an activating group (electron-donating) was introduced (e.g., alkyl), a second substitution reaction would occur more readily than the first and the disubstituted product might accumulate to a significant extent.\n\nAll the reactions mentioned for carboxylic acids are also possible for benzoic acid.\n\n\n"}
{"id": "4107", "url": "https://en.wikipedia.org/wiki?curid=4107", "title": "Boltzmann distribution", "text": "Boltzmann distribution\n\nIn statistical mechanics and mathematics, a Boltzmann distribution (also called Gibbs distribution) is a probability distribution, probability measure, or frequency distribution of particles in a system over various possible states. The distribution is expressed in the form\n\nformula_1\n\nwhere formula_2 is state energy (which varies from state to state), and formula_3 (a constant of the distribution) is the product of Boltzmann's constant and thermodynamic temperature.\n\nIn statistical mechanics, the Boltzmann distribution is a probability distribution that gives the probability that a system will be in a certain state as a function of that state’s energy and the temperature of the system. It is given as\n\nformula_4\n\nwhere \"p\" is the probability of state i, \"ε\" the energy of state i, \"k\" the Boltzmann constant, \"T\" the temperature of the system and \"M\" is the number of states accessible to the system. The sum is over all states accessible to the system of interest. The term system here has a very wide meaning; it can range from a single atom to a macroscopic system such as a natural gas storage tank. Because of this the Boltzmann distribution can be used to solve a very wide variety of problems. The distribution shows that states with lower energy will always have a higher probability of being occupied than the states with higher energy.\n\nThe \"ratio\" of a Boltzmann distribution computed for two states is known as the Boltzmann factor and characteristically only depends on the states' energy difference.\n\nformula_5\n\nThe Boltzmann distribution is named after Ludwig Boltzmann who first formulated it in 1868 during his studies of the statistical mechanics of gases in thermal equilibrium. Boltzmann's statistical work is borne out in his paper “On the Relationship between the Second Fundamental Theorem of the Mechanical Theory of Heat and Probability Calculations Regarding the Conditions for Thermal Equilibrium.\" \nThe distribution was later investigated extensively, in its modern generic form, by Josiah Willard Gibbs in 1902.\n\nThe Boltzmann distribution should not be confused with the Maxwell-Boltzmann distribution. The former gives the probability that a system will be in a certain state as a function of that state's energy. Whereas, the latter is used to describe particle speeds in idealized gases.\n\nThe Boltzmann distribution is a probability distribution that gives the probability of a certain state as a function of that state’s energy and temperature of the system to which the distribution is applied. It is given as\n\nformula_4\n\nwhere \"p\" is the probability of state i, \"ε\" the energy of state i, \"k\" the Boltzmann constant, \"T\" the temperature of the system and \"M\" is the number of all states accessible to the system. The sum is over all states accessible to the system of interest. The right hand side denominator of the equation above is also known as the canonical partition function, commonly denoted by Q (or by some authors by Z).\n\nformula_7\n\nTherefore, the Boltzmann distribution can also be written as\n\nformula_8\n\nThe partition function can be calculated if we know the energies of the levels accessible to the system of interest. For atoms the partition function values can be found in the NIST Atomic Spectra Database.\n\nThe distribution shows that states with lower energy will always have a higher probability of being occupied than the states with higher energy. It can also give us the quantitative relationship between the probabilities of the two states being occupied. The ratio of probabilities for states i and j is given as\n\nformula_9\n\nwhere \"p\" is the probability of state i, \"p\" the probability of state j, and \"ε\" and \"ε\" are the energies of states i and j, respectively.\n\nThe Boltzmann distribution is often used to describe the distribution of particles, such as atoms or molecules, over energy states accessible to them. If we have a system consisting of many particles, the probability of a particle being in state i is practically the probability that, if we pick a random particle from that system and check what state it is in, we will find it is in state i. This probability is equal to the number of particles in state i divided by the total number of particles in the system, that is the fraction of particles that occupy state i.\n\nformula_10\n\nwhere \"N\" is the number of particles in state i and \"N\" is the total number of particles in the system. We may use the Boltzmann distribution to find this probability that is, as we have seen, equal to the fraction of particles that are in state i. So the equation that gives the fraction of particles in state i as a function of the energy of that state is \n\nformula_11\n\nThis equation is of great importance to spectroscopy. In spectroscopy we observe a spectral line if atoms or molecules that we are interested in going from one state to another. In order for this to be possible, there must be some particles in the first state to undergo the transition. We may find that this condition is fulfilled by finding the fraction of particles in the first state. If it is negligible, the transition is very likely not be observed at the temperature for which the calculation was done. In general, a larger fraction of molecules in the first state means a higher number of transitions to the second state. This gives a stronger spectral line. However, there are other factors that influence the intensity of a spectral line, such as whether it is caused by an allowed or a forbidden transition.\n\nThe Boltzmann distribution appears in statistical mechanics when considering isolated (or nearly-isolated) systems of fixed composition that are in thermal equilibrium (equilibrium with respect to energy exchange). The most general case is the probability distribution for the canonical ensemble, but also some special cases (derivable from the canonical ensemble) also show the Boltzmann distribution in different aspects:\n\n\nAlthough these cases have strong similarities, it is helpful to distinguish them as they generalize in different ways when the crucial assumptions are changed:\n\nIn more general mathematical settings, the Boltzmann distribution is also known as the Gibbs measure. In statistics and machine learning it is called a log-linear model. In deep learning the Boltzmann distribution is used in the sampling distribution of stochastic neural networks such as the Boltzmann machine.\n\nThe Boltzmann distribution can be introduced to allocate permits in emissions trading. The new allocation method using the Boltzmann distribution can describe the most probable, natural, and unbiased distribution of emissions permits among multiple countries. Simple and versatile, this new method holds potential for many economic and environmental applications.\n\n"}
{"id": "4109", "url": "https://en.wikipedia.org/wiki?curid=4109", "title": "Leg theory", "text": "Leg theory\n\nLeg theory is a bowling tactic in the sport of cricket. The term \"leg theory\" is somewhat archaic and seldom used any longer, but the basic tactic remains a play in modern cricket.\n\nSimply put, leg theory involves concentrating the bowling attack at or near the line of leg stump. This may or may not be accompanied by a concentration of fielders on the leg side. The line of attack aims to cramp the batsman, making him play the ball with the bat close to the body. This makes it difficult to hit the ball freely and score runs, especially on the off side. Since a leg theory attack means the batsman is more likely to hit the ball on the leg side, additional fielders on that side of the field can be effective in preventing runs and taking catches.\n\nStifling the batsman in this manner can lead to impatience and frustration, resulting in rash play by the batsman which in turn can lead to a quick dismissal.\n\nLeg theory can be a moderately successful tactic when used with both fast bowling and spin bowling, particularly leg spin to right-handed batsmen or off spin to left-handed batsmen. However, because it relies on lack of concentration or discipline by the batsman, it can be risky against patient and skilled players, especially batsmen who are strong on the leg side. The English opening bowlers Sydney Barnes and Frank Foster used leg theory with some success in Australia in 1911-12. In England, at around the same time Fred Root was one of the main proponents of the same tactic.\n\nConcentrating attack on the leg stump is considered by many cricket fans and commentators to lead to boring play, as it stifles run scoring and encourages batsmen to play conservatively.\n\nIn 1930, England captain Douglas Jardine, together with Nottinghamshire's captain Arthur Carr and his bowlers Harold Larwood and Bill Voce, developed a variant of leg theory in which the bowlers bowled fast, short-pitched balls that would rise into the batsman's body, together with a heavily stacked ring of close fielders on the leg side. The idea was that when the batsman defended against the ball, he would be likely to deflect the ball into the air for a catch.\n\nJardine called this modified form of the tactic \"fast leg theory\". On the 1932-33 English tour of Australia, Larwood and Voce bowled fast leg theory at the Australian batsmen. It turned out to be extremely dangerous, and most Australian players sustained injuries from being hit by the ball. Wicket-keeper Bert Oldfield's skull was fractured by a ball hitting his head (although the ball had first glanced off the bat and Larwood had an orthodox field), almost precipitating a riot by the Australian crowd.\n\nThe Australian press dubbed the tactic \"Bodyline\", and claimed it was a deliberate attempt by the English team to intimidate and injure the Australian players. Reports of the controversy reaching England at the time described the bowling as \"fast leg theory\", which sounded to many people to be a harmless and well-established tactic. This led to a serious misunderstanding amongst the English public and the Marylebone Cricket Club - the administrators of English cricket - of the dangers posed by Bodyline. The English press and cricket authorities declared the Australian protests to be a case of sore losing and \"squealing\".\n\nIt was only with the return of the English team and the subsequent use of Bodyline against English players in England by the touring West Indian cricket team in 1933 that demonstrated to the country the dangers it posed. The MCC subsequently revised the Laws of Cricket to prevent the use of \"fast leg theory\" tactics in future, also limiting the traditional tactic.\n\n"}
{"id": "4110", "url": "https://en.wikipedia.org/wiki?curid=4110", "title": "Blythe Danner", "text": "Blythe Danner\n\nBlythe Katherine Danner (born February 3, 1943) is an American actress. She won two Primetime Emmy Awards for Best Supporting Actress in a Drama Series for her role as Izzy Huffstodt on \"Huff\" (2004–2006), and a Tony Award for Best Featured Actress in a Play for her performance in \"Butterflies Are Free\" (1969–1972). Danner was nominated for two Primetime Emmy Awards for Outstanding Guest Actress in a Comedy Series for portraying Marilyn Truman on \"Will & Grace\" (2001–2006), and the Primetime Emmy Award for Outstanding Lead Actress in a Miniseries or a Movie for her roles in \"We Were the Mulvaneys\" (2002) and \"Back When We Were Grownups\" (2004). For the latter, she was also nominated for the Golden Globe Award for Best Actress – Miniseries or Television Film.\n\nDanner is best known for her roles as Martha Jefferson in the film \"1776\" (1972), and \"Meet the Parents\" (2000) and its sequels \"Meet the Fockers\" (2004) and \"Little Fockers\" (2010). She has also appeared in the films \"The Great Santini\" (1979), \"Mr. and Mrs. Bridge\" (1990), \"The Prince of Tides\" (1991), \"Husbands and Wives\" (1992), and \"I'll See You in My Dreams\" (2015). She is the widow of Bruce Paltrow and the mother of actress Gwyneth Paltrow and director Jake Paltrow.\n\nDanner was born in Philadelphia, Pennsylvania, the daughter of Katharine (née Kile; 1909–2006) and Harry Earl Danner, a bank executive. She has a brother, opera singer and actor Harry Danner; a sister-in-law, performer-turned-director Dorothy \"Dottie\" Danner; and a half-brother, violin maker William Moennig. Danner has Pennsylvania Dutch (German), and some English and Irish, ancestry; her maternal grandmother was a German immigrant, and one of her paternal great-grandmothers was born in Barbados (to a family of European descent).\n\nDanner graduated from George School, a Quaker high school located near Newtown, Bucks County, Pennsylvania in 1960.\n\nA graduate of Bard College, Danner's first roles included the 1967 musical \"Mata Hari\" (closed out of town), and the 1968 Off-Broadway production of \"Summertree\". Her early Broadway appearances included \"Cyrano de Bergerac\" (1968) and her Theatre World Award-winning performance in \"The Miser\" (1969). She won the Tony Award for Best Featured Actress in a Play for portraying a free-spirited divorcée in \"Butterflies Are Free\" (1969–1972).\n\nIn 1972, Danner portrayed Martha Jefferson in the film version of \"1776\". That same year, she played a wife whose husband has been unfaithful, opposite Peter Falk and John Cassavetes, in the \"Columbo\" episode \"Etude in Black\".\n\nHer earliest starring film role was opposite Alan Alda in \"To Kill a Clown\" (1972). Danner appeared in the episode of \"M*A*S*H\" entitled \"The More I See You\", playing the love interest of Alda's character Hawkeye Pierce. She played lawyer Amanda Bonner in television's \"Adam's Rib\", also opposite Ken Howard as Adam Bonner. She played Zelda Fitzgerald in \"F. Scott Fitzgerald and 'The Last of the Belles'\" (1974). She was the eponymous heroine in the film \"Lovin' Molly\" (1974) (directed by Sidney Lumet). She appeared in \"Futureworld\", playing Tracy Ballard with co-star Peter Fonda (1976). In the 1982 TV movie \"Inside the Third Reich\", she played the wife of Albert Speer. In the film version of Neil Simon's semi-autobiographical play \"Brighton Beach Memoirs\" (1986), she portrayed a middle-aged Jewish mother. She has appeared in two films based on the novels of Pat Conroy, \"The Great Santini\" (1979) and \"The Prince of Tides\" (1991), as well as two television movies adapted from books by Anne Tyler, \"Saint Maybe\" and \"Back When We Were Grownups\", both for the Hallmark Hall of Fame.\nDanner appeared opposite Robert De Niro in the 2000 comedy hit \"Meet the Parents\", and its sequels, \"Meet the Fockers\" (2004) and \"Little Fockers\" (2010).\n\nFrom 2001 to 2006, she regularly appeared on NBC's sitcom \"Will & Grace\" as Will Truman's mother Marilyn. From 2004 to 2006, she starred in the main cast of the comedy-drama series \"Huff\". In 2005, she was nominated for three Primetime Emmy Awards for her work on \"Will & Grace\", \"Huff\", and the television film \"Back When We Were Grownups\", winning for her role in \"Huff\". The following year, she won a second consecutive Emmy Award for \"Huff\". For 25 years, she has been a regular performer at the Williamstown Summer Theater Festival, where she also serves on the Board of Directors.\n\nIn 2006, Danner was awarded an inaugural Katharine Hepburn Medal by Bryn Mawr College's Katharine Houghton Hepburn Center. In 2015, Danner was inducted into the American Theater Hall of Fame.\n\nDanner has been involved in environmental issues such as recycling and conservation for over 30 years. She has been active with INFORM, Inc., is on the Board of Environmental Advocates of New York and the Board of Directors of the Environmental Media Association, and won the 2002 EMA Board of Directors Ongoing Commitment Award. In 2011, Danner joined Moms Clean Air Force, to help call on parents to join in the fight against toxic air pollution.\n\nAfter the death of her husband Bruce Paltrow from oral cancer, she became involved with the Oral Cancer Foundation, a national 501(c)3 nonprofit charity. In 2005, she filmed a public service announcement that played on TV stations around the country about the risks associated with oral cancer, and through that shared the personal pain associated with the loss of her husband publicly to further awareness of the disease and the need for early detection. She continues to donate her time to the foundation, and has appeared on morning talk shows, and has done interviews in high-profile magazines such as \"People\" to further public awareness of the disease and its risk factors. Through the Bruce Paltrow Oral Cancer Fund, administered by the Oral Cancer Foundation, she continues to raise awareness and funding for oral cancer issues, particularly those involving communities in which disparities in health care exist. She appeared in commercials for Prolia, a brand of denosumab for injection.\n\nDanner is the widow of producer/director Bruce Paltrow, who died from complications of pneumonia while battling oral cancer in 2002, and the mother of actress Gwyneth Paltrow and director Jake Paltrow. Danner first co-starred with her daughter in 1992 in the television film \"Cruel Doubt\", and then again in the 2003 film \"Sylvia\". Danner portrayed Aurelia Plath, the mother to Gwyneth's title role of Sylvia Plath.\n\nRegarding meditation practice, Danner said, \"I have found transcendental meditation very helpful and comforting. It centers me.\"\n\n\n"}
{"id": "4111", "url": "https://en.wikipedia.org/wiki?curid=4111", "title": "Bioleaching", "text": "Bioleaching\n\nBioleaching is the extraction of metals from their ores through the use of living organisms. This is much cleaner than the traditional heap leaching using cyanide. Bioleaching is one of several applications within biohydrometallurgy and several methods are used to recover copper, zinc, lead, arsenic, antimony, nickel, molybdenum, gold, silver, and cobalt.\n\nBioleaching can involve numerous ferrous iron and sulfur oxidizing bacteria, including \"Acidithiobacillus ferrooxidans\" (formerly known as \"Thiobacillus ferrooxidans\") and \"Acidithiobacillus thiooxidans \" (formerly known as \"Thiobacillus thiooxidans\"). As a general principle, Fe ions are used to oxidize the ore. This step is entirely independent of microbes. The role of the bacteria is the further oxidation of the ore, but also the regeneration of the chemical oxidant Fe from Fe. For example, bacteria catalyse the breakdown of the mineral pyrite (FeS) by oxidising the sulfur and metal (in this case ferrous iron, (Fe)) using oxygen. This yields soluble products that can be further purified and refined to yield the desired metal.\n\nPyrite leaching (FeS):\nIn the first step, disulfide is spontaneously oxidized to thiosulfate by ferric ion (Fe), which in turn is reduced to give ferrous ion (Fe):\n\nThe ferrous ion is then oxidized by bacteria using oxygen:\n\nThiosulfate is also oxidized by bacteria to give sulfate:\n\nThe ferric ion produced in reaction (2) oxidized more sulfide as in reaction (1), closing the cycle and given the net reaction:\n\nThe net products of the reaction are soluble ferrous sulfate and sulfuric acid.\n\nThe microbial oxidation process occurs at the cell membrane of the bacteria. The electrons pass into the cells and are used in biochemical processes to produce energy for the bacteria while reducing oxygen to water. The critical reaction is the oxidation of sulfide by ferric iron. The main role of the bacterial step is the regeneration of this reactant.\n\nThe process for copper is very similar, but the efficiency and kinetics depend on the copper mineralogy. The most efficient minerals are supergene minerals such as chalcocite, CuS and covellite, CuS. The main copper mineral chalcopyrite (CuFeS) is not leached very efficiently, which is why the dominant copper-producing technology remains flotation, followed by smelting and refining. The leaching of CuFeS follows the two stages of being dissolved and then further oxidised, with Cu ions being left in solution.\n\nChalcopyrite leaching:\nnet reaction:\n\nIn general, sulfides are first oxidized to elemental sulfur, whereas disulfides are oxidized to give thiosulfate, and the processes above can be applied to other sulfidic ores. Bioleaching of non-sulfidic ores such as pitchblende also uses ferric iron as an oxidant (e.g., UO + 2 Fe ==> UO + 2 Fe). In this case, the sole purpose of the bacterial step is the regeneration of Fe. Sulfidic iron ores can be added to speed up the process and provide a source of iron. Bioleaching of non-sulfidic ores by layering of waste sulfides and elemental sulfur, colonized by \"Acidithiobacillus\" spp., has been accomplished, which provides a strategy for accelerated leaching of materials that do not contain sulfide minerals.\n\nThe dissolved copper (Cu) ions are removed from the solution by ligand exchange solvent extraction, which leaves other ions in the solution. The copper is removed by bonding to a ligand, which is a large molecule consisting of a number of smaller groups, each possessing a lone electron pair. The ligand-copper complex is extracted from the solution using an organic solvent such as kerosene:\n\nThe ligand donates electrons to the copper, producing a complex - a central metal atom (copper) bonded to the ligand. Because this complex has no charge, it is no longer attracted to polar water molecules and dissolves in the kerosene, which is then easily separated from the solution. Because the initial reaction is reversible, it is determined by pH. Adding concentrated acid reverses the equation, and the copper ions go back into an aqueous solution.\n\nThen the copper is passed through an electro-winning process to increase its purity: An electric current is passed through the resulting solution of copper ions. Because copper ions have a 2+ charge, they are attracted to the negative cathodes and collect there.\n\nThe copper can also be concentrated and separated by displacing the copper with Fe from scrap iron:\n\nThe electrons lost by the iron are taken up by the copper. Copper is the oxidising agent (it accepts electrons), and iron is the reducing agent (it loses electrons).\n\nTraces of precious metals such as gold may be left in the original solution. Treating the mixture with sodium cyanide in the presence of free oxygen dissolves the gold. The gold is removed from the solution by adsorbing (taking it up on the surface) to charcoal.\n\nSeveral species of fungi can be used for bioleaching. Fungi can be grown on many different substrates, such as electronic scrap, catalytic converters, and fly ash from municipal waste incineration. Experiments have shown that two fungal strains (\"Aspergillus niger, Penicillium simplicissimum\") were able to mobilize Cu and Sn by 65%, and Al, Ni, Pb, and Zn by more than 95%. \"Aspergillus niger\" can produce some organic acids such as citric acid. This form of leaching does not rely on microbial oxidation of metal but rather uses microbial metabolism as source of acids that directly dissolve the metal.\n\nExtractions involve many expensive steps such as roasting and smelting, which require sufficient concentrations of elements in ores and are environmentally unfriendly. Low concentrations are not a problem for bacteria because they simply ignore the waste that surrounds the metals, attaining extraction yields of over 90% in some cases. These microorganisms actually gain energy by breaking down minerals into their constituent elements. The company simply collects the ions out of the solution after the bacteria have finished. There is a limited amount of ores.\n\n\n\nAt the current time, it is more economical to smelt copper ore rather than to use bioleaching, since the concentration of copper in its ore is in general quite high. The profit obtained from the speed and yield of smelting justifies its cost. Nonetheless, at the largest copper mine of the world, Escondida in Chile the process seems to be favorable.\n\nHowever, the concentration of gold in its ore is in general very low. In this case, the lower cost of bacterial leaching outweighs the time it takes to extract the metal.\nEconomically it is also very expensive and many companies once started can not keep up with the demand and end up in debt. Projects like Finnish Talvivaara proved to be environmentally and economically disastrous.\n\n\n"}
{"id": "4113", "url": "https://en.wikipedia.org/wiki?curid=4113", "title": "Bouldering", "text": "Bouldering\n\nBouldering is a form of rock climbing that is performed on large boulders or other small rock formations, without the use of ropes or harnesses. While it can be done without any equipment whatsoever, most climbers use climbing shoes to help secure footholds, chalk to keep their hands dry, and bouldering mats to prevent injuries from falls. Unlike free solo climbing, which is also performed without ropes, bouldering problems (the path that a climber takes in order to complete the climb) are usually less than 6 meters (20 ft.) tall. Artificial climbing walls allow boulderers to train indoors in areas without natural boulders. Bouldering competitions, which employ a variety of formats, take place in both indoor and outdoor settings.\n\nThe sport originated as a method of training for roped climbs and mountaineering. Bouldering enabled climbers to practice specific moves at a safe distance from the ground. Additionally, the sport served to build stamina and increase finger strength. Throughout the 1900s, bouldering evolved into a separate discipline. Individual problems are assigned ratings based on their difficulty. There have been many different rating systems used throughout the history of the sport, but modern problems usually use either the V-scale or the Fontainebleau scale.\n\nThe growing popularity of the sport has caused several environmental concerns, including soil erosion and trampled vegetation as climbers hike off-trail to reach bouldering sites. This has caused some landowners to restrict access or prohibit bouldering altogether.\n\nBouldering is a form of rock climbing which takes place on large boulders and other small rock formations, usually measuring less than from ground to top, but in some cases can measure up to 30+ ft. Unlike top rope climbing and lead climbing, no ropes are used to protect or aid the climber. Bouldering routes or \"problems\" require the climber to reach the top of a boulder, usually from a specified start position. Some boulder problems, known as \"traverses,\" require the climber to climb horizontally from one position to another.\n\nThe characteristics of boulder problems depend largely on the type of rock being climbed. Granite, for example, often features long cracks and slabs. Sandstone rocks are known for their steep overhangs and frequent horizontal breaks. Other common bouldering rocks include limestone and volcanic rock.\n\nThere are many prominent bouldering areas throughout the United States, including Hueco Tanks in Texas, Mount Evans in Colorado, and The Buttermilks in Bishop, California. Squamish, British Columbia is one of the most popular bouldering areas in Canada. Europe also hosts a number of bouldering sites, such as Fontainebleau in France, Albarracín in Spain, and various mountains throughout Switzerland.\n\nArtificial climbing walls are used to simulate boulder problems in an indoor environment, usually at climbing gyms. These walls are constructed with wooden panels, polymer cement panels, concrete shells, or precast molds of actual rock walls. Holds, usually made of plastic, are then bolted onto the wall to create problems. The walls often feature steep overhanging surfaces, forcing the climber to employ highly technical movements while supporting much of their weight with their upper body strength.\n\nClimbing gyms often feature multiple problems within the same section of wall. In the US the most common method Routesetters use to designate the intended route for a particular problem is by placing colored tape next to each hold—for example, holds with red tape would indicate one bouldering problem, while green tape would be used to set off a different problem in the same area. Across much of the rest of the world problems and grades are usually designated by using a set color of plastic hold to indicate a particular problem. For example, green may be v0-v1, blue may be v2-v3 and so on. Setting via color has certain advantages, the most notable of which are that it makes it more obvious where the holds for a problem are, and that there is no chance of tape being accidentally kicked off of footholds. Smaller, resource-poor climbing gyms may prefer taped problems because large, expensive holds can be used in multiple routes simply by marking them with more than one color of tape.\n\nBouldering competitions occur in both indoor and outdoor settings. The International Federation of Sport Climbing (IFSC) employs an indoor format that breaks the competition into three rounds: qualifications, semi-finals, and finals. The rounds feature different sets of four or five boulder problems, and each competitor has a fixed amount of time to attempt each problem. At the end of each round, competitors are ranked by the number of completed problems, with ties settled by the total number of attempts taken to solve the problems.\n\nThere are several other formats used for bouldering competitions. Some competitions give climbers a fixed number of attempts at each problem with a timed rest period in between each attempt, unlike the IFSC format, in which competitors can use their allotted time however they choose. In an open-format competition, all climbers compete simultaneously, and are given a fixed amount of time to complete as many problems as possible. More points are awarded for more difficult problems, while points are deducted for multiple attempts on the same problem.\n\nIn 2012, the IFSC submitted a proposal to the International Olympic Committee (IOC) to include lead climbing in the 2020 Summer Olympics. The proposal was later revised to an \"overall\" competition, which would feature bouldering, lead climbing, and speed climbing. In May 2013, the IOC announced that climbing would not be added to the 2020 Olympic program.\n\nIn 2016, the International Olympic Committee (IOC) officially approved climbing as an Olympic sport \"in order to appeal to younger audiences.\" The Olympics will feature the earlier proposed overall competition. Medalists will be competing in all three categories for a best overall score.\n\nRock climbing first known as a sport in the mid-1800s. Early records describe climbers engaging in what is now referred to as bouldering, not as a separate discipline, but as a form of training for larger ascents. In the early 20th century, the Fontainebleau area of France established itself as a prominent climbing area, where some of the first dedicated \"bleausards\" (or \"boulderers\") emerged. The specialized rock climbing shoe was invented by one such athlete, Pierre Allain.\n\nIn the 1960s, the sport was pushed forward by American mathematician John Gill, who contributed several important innovations. Gill's previous athletic pursuit was gymnastics, a sport which had an established scale of difficulty for particular movements and body positions. He applied this idea to bouldering, which shifted the focus from reaching a summit to navigating a specific sequence of holds. Gill developed a closed-ended rating system: B1 problems were as difficult as the most challenging roped routes of the time, B2 problems were more difficult, and B3 problems were those that had only been completed once.\n\nGill introduced chalk as a method of keeping the climber's hands dry. He also promoted a dynamic climbing style and emphasized the importance of strength training to complement technical skill. These practices had not been popular among climbers, but as Gill's ability level and influence grew, his ideas became the norm.\n\nTwo important training tools emerged in the 1980s: Bouldering mats and artificial climbing walls. The former, also referred to as \"crash pads\", prevented injuries from falling, and enabled boulderers to climb in areas that would have been too dangerous to attempt otherwise. Indoor climbing walls helped spread the sport to areas without outdoor climbing, and allowed serious climbers to train year-round regardless of weather conditions.\n\nAs the sport grew in popularity, new bouldering areas were developed throughout Europe and the United States, and more athletes began participating in bouldering competitions. The visibility of the sport greatly increased in the early 2000s, as YouTube videos and climbing blogs helped boulderers around the world to quickly learn techniques, find hard problems, and announce newly completed projects.\n\nUnlike other climbing sports, bouldering can be performed safely and effectively with very little equipment, an aspect which makes the discipline highly appealing to many climbers. Bouldering pioneer John Sherman asserted that \"The only gear really needed to go bouldering is boulders\". Others suggest the use of climbing shoes and a chalkbag as the bare minimum, while more experienced boulderers typically bring multiple pairs of shoes, chalk, brushes, crash pads, and a skincare kit.\n\nOf the aforementioned equipment, climbing shoes have the most direct impact on performance. Besides protecting the climber's feet from rough surfaces, climbing shoes are designed to help the climber secure and maintain footholds. Climbing shoes typically fit much tighter than other athletic footwear, and often curl the toes downwards to enable precise footwork. They are manufactured in a variety of different styles in order to perform well in different situations: High-top shoes, for example, provide better protection for the ankle, while low-top shoes provide greater flexibility and freedom of movement. Stiffer shoes excel at securing small edges, whereas softer shoes provide greater sensitivity. The front of the shoe, called the \"toe box\", can be asymmetric, which performs well on overhanging rocks, or symmetric, which is better suited for vertical problems and slabs.  \n\nMost boulderers use gymnastics chalk on their hands to absorb sweat. It is stored in a small chalkbag which can be tied around the waist, allowing the climber to reapply chalk during the climb. Brushes are used to remove excess chalk and other debris from boulders in between climbs; they are often attached to the end of a stick, pipe, or other straight object in order to reach higher holds. Crash pads, also referred to as bouldering mats, are foam cushions placed on the ground to protect climbers from falls.\n\nBoulder problems are generally shorter than from ground to top. This makes the sport significantly safer than free solo climbing, which is also performed without ropes, but with no upper limit on the height of the climb. However, minor injuries are common in bouldering, particularly sprained ankles and wrists. Two factors contribute to the frequency of injuries in bouldering: first, boulder problems typically feature more difficult moves than other climbing disciplines, making falls more common. Second, without ropes to arrest the climber's descent, every fall will cause the climber to hit the ground.\n\nTo prevent injuries, boulderers position crash pads near the boulder to provide a softer landing, as well as one or more spotters to help redirect the climber towards the pads. Upon landing, boulderers employ falling techniques similar to those used in gymnastics: spreading the impact across the entire body to avoid bone fractures, and positioning limbs to allow joints to move freely throughout the impact.\n\nAs with other forms of climbing, bouldering technique is largely centered on proper footwork. Leg muscles are significantly stronger than arm muscles; thus, proficient boulderers use their arms primarily to maintain balance and body positioning, relying on their legs to push them up the boulder. Boulderers also keep their arms straight whenever possible, allowing their bones to support their body weight rather than their muscles.\n\nBouldering movements are described as either \"static\" or \"dynamic\". Static movements are those that are performed slowly, with the climber's position controlled by maintaining contact on the boulder with the other three limbs. Dynamic movements use the climber's momentum to reach holds that would be difficult or impossible to secure statically, with an increased risk of falling if the movement is not performed accurately.\n\nBouldering problems are assigned numerical difficulty ratings by routesetters and climbers. The two most widely used rating systems are the V-scale and the Fontainebleau system.\n\nThe V-scale, which originated in the United States, is an open-ended rating system with higher numbers indicating a higher degree of difficulty. The V1 rating indicates that a problem can be completed by a novice climber in good physical condition after several attempts. The scale begins at V0, and as of 2013, the highest V rating that has been assigned to a bouldering problem is V17. Some climbing gyms also use a VB grade to indicate beginner problems.\n\nThe Fontainebleau scale follows a similar system, with each numerical grade divided into three ratings with the letters \"a\", \"b\", and \"c\". For example, Fontainebleau 7A roughly corresponds with V6, while Fontainebleau 7C+ is equivalent to V10. In both systems, grades are further differentiated by appending \"+\" to indicate a small increase in difficulty. Despite this level of specificity, ratings of individual problems are often controversial, as ability level is not the only factor that affects how difficult a problem will be for a particular climber. Height, arm length, flexibility, and other body characteristics can also be relevant.\n\nBouldering can damage vegetation that grows on rocks, such as mosses and lichens. This can occur as a result of the climber intentionally cleaning the boulder, or unintentionally from repeated use of handholds and footholds. Vegetation on the ground surrounding the boulder can also be damaged from overuse, particularly by climbers laying down crash pads. Soil erosion can occur when boulderers trample vegetation while hiking off of established trails, or when they unearth small rocks near the boulder in an effort to make the landing zone safer. Other environmental concerns include littering, improperly disposed feces, and graffiti. These issues have caused some land managers to prohibit bouldering, as was the case in Tea Garden, a popular bouldering area in Rocklands, South Africa.\n"}
{"id": "4115", "url": "https://en.wikipedia.org/wiki?curid=4115", "title": "Boiling point", "text": "Boiling point\n\nThe boiling point of a substance is the temperature at which the vapor pressure of the liquid equals the pressure surrounding the liquid and the liquid changes into a vapor.\n\nThe boiling point of a liquid varies depending upon the surrounding environmental pressure. A liquid in a partial vacuum has a lower boiling point than when that liquid is at atmospheric pressure. A liquid at high pressure has a higher boiling point than when that liquid is at atmospheric pressure. For a given pressure, different liquids boil at different temperatures. For example, water boils at at sea level, but at at altitude.\n\nThe normal boiling point (also called the atmospheric boiling point or the atmospheric pressure boiling point) of a liquid is the special case in which the vapor pressure of the liquid equals the defined atmospheric pressure at sea level, 1 atmosphere. At that temperature, the vapor pressure of the liquid becomes sufficient to overcome atmospheric pressure and allow bubbles of vapor to form inside the bulk of the liquid. The standard boiling point has been defined by IUPAC since 1982 as the temperature at which boiling occurs under a pressure of 1 bar.\n\nThe heat of vaporization is the energy required to transform a given quantity (a mol, kg, pound, etc.) of a substance from a liquid into a gas at a given pressure (often atmospheric pressure).\n\nLiquids may change to a vapor at temperatures below their boiling points through the process of evaporation. Evaporation is a surface phenomenon in which molecules located near the liquid's edge, not contained by enough liquid pressure on that side, escape into the surroundings as vapor. On the other hand, boiling is a process in which molecules anywhere in the liquid escape, resulting in the formation of vapor bubbles within the liquid.\n\nA \"saturated liquid\" contains as much thermal energy as it can without boiling (or conversely a \"saturated vapor\" contains as little thermal energy as it can without condensing).\n\nSaturation temperature means \"boiling point\". The saturation temperature is the temperature for a corresponding saturation pressure at which a liquid boils into its vapor phase. The liquid can be said to be saturated with thermal energy. Any addition of thermal energy results in a phase transition.\n\nIf the pressure in a system remains constant (isobaric), a vapor at saturation temperature will begin to condense into its liquid phase as thermal energy (heat) is removed. Similarly, a liquid at saturation temperature and pressure will boil into its vapor phase as additional thermal energy is applied.\n\nThe boiling point corresponds to the temperature at which the vapor pressure of the liquid equals the surrounding environmental pressure. Thus, the boiling point is dependent on the pressure. Boiling points may be published with respect to the NIST, USA standard pressure of 101.325 kPa (or 1 atm), or the IUPAC standard pressure of 100.000 kPa. At higher elevations, where the atmospheric pressure is much lower, the boiling point is also lower. The boiling point increases with increased pressure up to the critical point, where the gas and liquid properties become identical. The boiling point cannot be increased beyond the critical point. Likewise, the boiling point decreases with decreasing pressure until the triple point is reached. The boiling point cannot be reduced below the triple point.\n\nIf the heat of vaporization and the vapor pressure of a liquid at a certain temperature are known, the boiling point can be calculated by using the Clausius–Clapeyron equation, thus:\n\nwhere:\n\nSaturation pressure is the pressure for a corresponding saturation temperature at which a liquid boils into its vapor phase. Saturation pressure and saturation temperature have a direct relationship: as saturation pressure is increased, so is saturation temperature.\n\nIf the temperature in a system remains constant (an \"isothermal\" system), vapor at saturation pressure and temperature will begin to condense into its liquid phase as the system pressure is increased. Similarly, a liquid at saturation pressure and temperature will tend to flash into its vapor phase as system pressure is decreased.\n\nThere are two conventions regarding the \"standard boiling point of water\": The \"normal boiling point\" is at a pressure of 1 atm (i.e., 101.325 kPa). The IUPAC recommended \"standard boiling point of water\" at a standard pressure of 100 kPa (1 bar) is . For comparison, on top of Mount Everest, at elevation, the pressure is about and the boiling point of water is .\nThe Celsius temperature scale was defined until 1954 by two points: 0 °C being defined by the water freezing point and 100 °C being defined by the water boiling point at standard atmospheric pressure.\n\nThe higher the vapor pressure of a liquid at a given temperature, the lower the normal boiling point (i.e., the boiling point at atmospheric pressure) of the liquid.\n\nThe vapor pressure chart to the right has graphs of the vapor pressures versus temperatures for a variety of liquids. As can be seen in the chart, the liquids with the highest vapor pressures have the lowest normal boiling points.\n\nFor example, at any given temperature, methyl chloride has the highest vapor pressure of any of the liquids in the chart. It also has the lowest normal boiling point (−24.2 °C), which is where the vapor pressure curve of methyl chloride (the blue line) intersects the horizontal pressure line of one atmosphere (atm) of absolute vapor pressure.\n\nThe critical point of a liquid is the highest temperature (and pressure) it will actually boil at.\n\nSee also Vapour pressure of water.\n\nThe element with the lowest boiling point is helium. Both the boiling points of rhenium and tungsten exceed 5000 K at standard pressure; because it is difficult to measure extreme temperatures precisely without bias, both have been cited in the literature as having the higher boiling point.\n\nAs can be seen from the above plot of the logarithm of the vapor pressure vs. the temperature for any given pure chemical compound, its normal boiling point can serve as an indication of that compound's overall volatility. A given pure compound has only one normal boiling point, if any, and a compound's normal boiling point and melting point can serve as characteristic physical properties for that compound, listed in reference books. The higher a compound's normal boiling point, the less volatile that compound is overall, and conversely, the lower a compound's normal boiling point, the more volatile that compound is overall. Some compounds decompose at higher temperatures before reaching their normal boiling point, or sometimes even their melting point. For a stable compound, the boiling point ranges from its triple point to its critical point, depending on the external pressure. Beyond its triple point, a compound's normal boiling point, if any, is higher than its melting point. Beyond the critical point, a compound's liquid and vapor phases merge into one phase, which may be called a superheated gas. At any given temperature, if a compound's normal boiling point is lower, then that compound will generally exist as a gas at atmospheric external pressure. If the compound's normal boiling point is higher, then that compound can exist as a liquid or solid at that given temperature at atmospheric external pressure, and will so exist in equilibrium with its vapor (if volatile) if its vapors are contained. If a compound's vapors are not contained, then some volatile compounds can eventually evaporate away in spite of their higher boiling points.\nIn general, compounds with ionic bonds have high normal boiling points, if they do not decompose before reaching such high temperatures. Many metals have high boiling points, but not all. Very generally—with other factors being equal—in compounds with covalently bonded molecules, as the size of the molecule (or molecular mass) increases, the normal boiling point increases. When the molecular size becomes that of a macromolecule, polymer, or otherwise very large, the compound often decomposes at high temperature before the boiling point is reached. Another factor that affects the normal boiling point of a compound is the polarity of its molecules. As the polarity of a compound's molecules increases, its normal boiling point increases, other factors being equal. Closely related is the ability of a molecule to form hydrogen bonds (in the liquid state), which makes it harder for molecules to leave the liquid state and thus increases the normal boiling point of the compound. Simple carboxylic acids dimerize by forming hydrogen bonds between molecules. A minor factor affecting boiling points is the shape of a molecule. Making the shape of a molecule more compact tends to lower the normal boiling point slightly compared to an equivalent molecule with more surface area.\n\nMost volatile compounds (anywhere near ambient temperatures) go through an intermediate liquid phase while warming up from a solid phase to eventually transform to a vapor phase. By comparison to boiling, a sublimation is a physical transformation in which a solid turns directly into vapor, which happens in a few select cases such as with carbon dioxide at atmospheric pressure. For such compounds, a sublimation point is a temperature at which a solid turning directly into vapor has a vapor pressure equal to the external pressure.\n\nIn the preceding section, boiling points of pure compounds were covered. Vapor pressures and boiling points of substances can be affected by the presence of dissolved impurities (solutes) or other miscible compounds, the degree of effect depending on the concentration of the impurities or other compounds. The presence of non-volatile impurities such as salts or compounds of a volatility far lower than the main component compound decreases its mole fraction and the solution's volatility, and thus raises the normal boiling point in proportion to the concentration of the solutes. This effect is called boiling point elevation. As a common example, salt water boils at a higher temperature than pure water.\n\nIn other mixtures of miscible compounds (components), there may be two or more components of varying volatility, each having its own pure component boiling point at any given pressure. The presence of other volatile components in a mixture affects the vapor pressures and thus boiling points and dew points of all the components in the mixture. The dew point is a temperature at which a vapor condenses into a liquid. Furthermore, at any given temperature, the composition of the vapor is different from the composition of the liquid in most such cases. In order to illustrate these effects between the volatile components in a mixture, a boiling point diagram is commonly used. Distillation is a process of boiling and [usually] condensation which takes advantage of these differences in composition between liquid and vapor phases.\n\n"}
{"id": "4116", "url": "https://en.wikipedia.org/wiki?curid=4116", "title": "Big Bang", "text": "Big Bang\n\nThe Big Bang theory is the prevailing cosmological model for the universe from the earliest known periods through its subsequent large-scale evolution. The model describes how the universe expanded from a very high density and high temperature state, and offers a comprehensive explanation for a broad range of phenomena, including the abundance of light elements, the cosmic microwave background (CMB), large scale structure and Hubble's law. If the known laws of physics are extrapolated to the highest density regime, the result is a singularity which is typically associated with the Big Bang. Detailed measurements of the expansion rate of the universe place this moment at approximately 13.8 billion years ago, which is thus considered the age of the universe. After the initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, and later simple atoms. Giant clouds of these primordial elements later coalesced through gravity in halos of dark matter, eventually forming the stars and galaxies visible today.\n\nSince Georges Lemaître first noted in 1927 that an expanding universe could be traced back in time to an originating single point, scientists have built on his idea of cosmic expansion. While the scientific community was once divided between supporters of two different expanding universe theories, the Big Bang and the Steady State theory, empirical evidence provides strong support for the former. In 1929, from analysis of galactic redshifts, Edwin Hubble concluded that galaxies are drifting apart; this is important observational evidence consistent with the hypothesis of an expanding universe. In 1964, the cosmic microwave background radiation was discovered, which was crucial evidence in favor of the Big Bang model, since that theory predicted the existence of background radiation throughout the universe before it was discovered. More recently, measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to dark energy's existence. The known physical laws of nature can be used to calculate the characteristics of the universe in detail back in time to an initial state of extreme density and temperature.\n\nAmerican astronomer Edwin Hubble observed that the distances to faraway galaxies were strongly correlated with their redshifts. This was interpreted to mean that all distant galaxies and clusters are receding away from our vantage point with an apparent velocity proportional to their distance: that is, the farther they are, the faster they move away from us, regardless of direction. Assuming the Copernican principle (that the Earth is not the center of the universe), the only remaining interpretation is that all observable regions of the universe are receding from all others. Since we know that the distance between galaxies increases today, it must mean that in the past galaxies were closer together. The continuous expansion of the universe implies that the universe was denser and hotter in the past.\n\nLarge particle accelerators can replicate the conditions that prevailed after the early moments of the universe, resulting in confirmation and refinement of the details of the Big Bang model. However, these accelerators can only probe so far into high energy regimes. Consequently, the state of the universe in the earliest instants of the Big Bang expansion is still poorly understood and an area of open investigation and speculation.\n\nThe first subatomic particles to be formed included protons, neutrons, and electrons. Though simple atomic nuclei formed within the first three minutes after the Big Bang, thousands of years passed before the first electrically neutral atoms formed. The majority of atoms produced by the Big Bang were hydrogen, along with helium and traces of lithium. Giant clouds of these primordial elements later coalesced through gravity to form stars and galaxies, and the heavier elements were synthesized either within stars or during supernovae.\n\nThe Big Bang theory offers a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the CMB, large scale structure, and Hubble's Law. The framework for the Big Bang model relies on Albert Einstein's theory of general relativity and on simplifying assumptions such as homogeneity and isotropy of space. The governing equations were formulated by Alexander Friedmann, and similar solutions were worked on by Willem de Sitter. Since then, astrophysicists have incorporated observational and theoretical additions into the Big Bang model, and its parametrization as the Lambda-CDM model serves as the framework for current investigations of theoretical cosmology. The Lambda-CDM model is the current \"standard model\" of Big Bang cosmology, consensus is that it is the simplest model that can account for the various measurements and observations relevant to cosmology.\n\nExtrapolation of the expansion of the universe backwards in time using general relativity yields an infinite density and temperature at a finite time in the past. This singularity indicates that general relativity is not an adequate description of the laws of physics in this regime. It is debated 'how closely' models based on general relativity alone can be used to extrapolate toward the singularity—certainly no closer than the end of the Planck epoch.\n\nThis primordial singularity is itself sometimes called \"the Big Bang\", but the term can also refer to a more generic early hot, dense phase of the universe. In either case, \"the Big Bang\" as an event is also colloquially referred to as the \"birth\" of our universe since it represents the point in history where the universe can be verified to have entered into a regime where the laws of physics as we understand them (specifically general relativity and the standard model of particle physics) work. Based on measurements of the expansion using Type Ia supernovae and measurements of temperature fluctuations in the cosmic microwave background, the time that has passed since that event — otherwise known as the \"age of the universe\" — is 13.799 ± 0.021 billion years. The agreement of independent measurements of this age supports the ΛCDM model that describes in detail the characteristics of the universe.\n\nThe earliest phases of the Big Bang are subject to much speculation. In the most common models the universe was filled homogeneously and isotropically with a very high energy density and huge temperatures and pressures and was very rapidly expanding and cooling. Approximately 10 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially during which time density fluctuations that occurred because of the uncertainty principle were amplified into the seeds that would later form the large-scale structure of the universe. After inflation stopped, reheating occurred until the universe obtained the temperatures required for the production of a quark–gluon plasma as well as all other elementary particles. Temperatures were so high that the random motions of particles were at relativistic speeds, and particle–antiparticle pairs of all kinds were being continuously created and destroyed in collisions. At some point, an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptons—of the order of one part in 30 million. This resulted in the predominance of matter over antimatter in the present universe.\n\nThe universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing. Symmetry breaking phase transitions put the fundamental forces of physics and the parameters of elementary particles into their present form. After about 10 seconds, the picture becomes less speculative, since particle energies drop to values that can be attained in particle accelerators. At about 10 seconds, quarks and gluons combined to form baryons such as protons and neutrons. The small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. The temperature was now no longer high enough to create new proton–antiproton pairs (similarly for neutrons–antineutrons), so a mass annihilation immediately followed, leaving just one in 10 of the original protons and neutrons, and none of their antiparticles. A similar process happened at about 1 second for electrons and positrons.\nAfter these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated by photons (with a minor contribution from neutrinos).\n\nA few minutes into the expansion, when the temperature was about a billion (one thousand million) kelvin and the density was about that of air, neutrons combined with protons to form the universe's deuterium and helium nuclei in a process called Big Bang nucleosynthesis. Most protons remained uncombined as hydrogen nuclei.\n\nAs the universe cooled, the rest mass energy density of matter came to gravitationally dominate that of the photon radiation. After about 379,000 years, the electrons and nuclei combined into atoms (mostly hydrogen); hence the radiation decoupled from matter and continued through space largely unimpeded. This relic radiation is known as the cosmic microwave background radiation. The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the universe was only 10–17 million years old.\n\nOver a long period of time, the slightly denser regions of the nearly uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today. The details of this process depend on the amount and type of matter in the universe. The four possible types of matter are known as cold dark matter, warm dark matter, hot dark matter, and baryonic matter. The best measurements available, from Wilkinson Microwave Anisotropy Probe (WMAP), show that the data is well-fit by a Lambda-CDM model in which dark matter is assumed to be cold (warm dark matter is ruled out by early reionization), and is estimated to make up about 23% of the matter/energy of the universe, while baryonic matter makes up about 4.6%. In an \"extended model\" which includes hot dark matter in the form of neutrinos, then if the \"physical baryon density\" Ωh is estimated at about 0.023 (this is different from the 'baryon density' Ω expressed as a fraction of the total matter/energy density, which as noted above is about 0.046), and the corresponding cold dark matter density Ωh is about 0.11, the corresponding neutrino density Ωh is estimated to be less than 0.0062.\n\nIndependent lines of evidence from Type Ia supernovae and the CMB imply that the universe today is dominated by a mysterious form of energy known as dark energy, which apparently permeates all of space. The observations suggest 73% of the total energy density of today's universe is in this form. When the universe was very young, it was likely infused with dark energy, but with less space and everything closer together, gravity predominated, and it was slowly braking the expansion. But eventually, after numerous billion years of expansion, the growing abundance of dark energy caused the expansion of the universe to slowly begin to accelerate.\n\nDark energy in its simplest formulation takes the form of the cosmological constant term in Einstein's field equations of general relativity, but its composition and mechanism are unknown and, more generally, the details of its equation of state and relationship with the Standard Model of particle physics continue to be investigated both through observation and theoretically.\n\nAll of this cosmic evolution after the inflationary epoch can be rigorously described and modeled by the ΛCDM model of cosmology, which uses the independent frameworks of quantum mechanics and Einstein's General Relativity. There is no well-supported model describing the action prior to 10 seconds or so. Apparently a new unified theory of quantum gravitation is needed to break this barrier. Understanding this earliest of eras in the history of the universe is currently one of the greatest unsolved problems in physics.\n\nThe Big Bang theory depends on two major assumptions: the universality of physical laws and the cosmological principle. The cosmological principle states that on large scales the universe is homogeneous and isotropic.\n\nThese ideas were initially taken as postulates, but today there are efforts to test each of them. For example, the first assumption has been tested by observations showing that largest possible deviation of the fine structure constant over much of the age of the universe is of order 10. Also, general relativity has passed stringent tests on the scale of the Solar System and binary stars.\n\nIf the large-scale universe appears isotropic as viewed from Earth, the cosmological principle can be derived from the simpler Copernican principle, which states that there is no preferred (or special) observer or vantage point. To this end, the cosmological principle has been confirmed to a level of 10 via observations of the CMB. The universe has been measured to be homogeneous on the largest scales at the 10% level.\n\nGeneral relativity describes spacetime by a metric, which determines the distances that separate nearby points. The points, which can be galaxies, stars, or other objects, are themselves specified using a coordinate chart or \"grid\" that is laid down over all spacetime. The cosmological principle implies that the metric should be homogeneous and isotropic on large scales, which uniquely singles out the Friedmann–Lemaître–Robertson–Walker metric (FLRW metric). \nThis metric contains a scale factor, which describes how the size of the universe changes with time. This enables a convenient choice of a coordinate system to be made, called comoving coordinates. In this coordinate system, the grid expands along with the universe, and objects that are moving only because of the expansion of the universe, remain at fixed points on the grid. While their \"coordinate\" distance (comoving distance) remains constant, the \"physical\" distance between two such co-moving points expands proportionally with the scale factor of the universe.\n\nThe Big Bang is not an explosion of matter moving outward to fill an empty universe. Instead, space itself expands with time everywhere and increases the physical distance between two comoving points. In other words, the Big Bang is not an explosion \"in space\", but rather an expansion \"of space\". Because the FLRW metric assumes a uniform distribution of mass and energy, it applies to our universe only on large scales—local concentrations of matter such as our galaxy are gravitationally bound and as such do not experience the large-scale expansion of space.\n\nAn important feature of the Big Bang spacetime is the presence of particle horizons. Since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not had time to reach us. This places a limit or a \"past horizon\" on the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never \"catch up\" to very distant objects. This defines a \"future horizon\", which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of the FLRW model that describes our universe.\n\nOur understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.\n\nEnglish astronomer Fred Hoyle is credited with coining the term \"Big Bang\" during a 1949 BBC radio broadcast. It is popularly reported that Hoyle, who favored an alternative \"steady state\" cosmological model, intended this to be pejorative, but Hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models.\n\nThe Big Bang theory developed from observations of the structure of the universe and from theoretical considerations. In 1912 Vesto Slipher measured the first Doppler shift of a \"spiral nebula\" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were \"island universes\" outside our Milky Way. Ten years later, Alexander Friedmann, a Russian cosmologist and mathematician, derived the Friedmann equations from Albert Einstein's equations of general relativity, showing that the universe might be expanding in contrast to the static universe model advocated by Einstein at that time. In 1924 Edwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Independently deriving Friedmann's equations in 1927, Georges Lemaître, a Belgian physicist and Roman Catholic priest, proposed that the inferred recession of the nebulae was due to the expansion of the universe.\n\nIn 1931 Lemaître went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a \"primeval atom\" where and when the fabric of time and space came into existence.\n\nStarting in 1924, Hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the Hooker telescope at Mount Wilson Observatory. This allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by Slipher. In 1929 Hubble discovered a correlation between distance and recession velocity—now known as Hubble's law. Lemaître had already shown that this was expected, given the cosmological principle.\n\nIn the 1920s and 1930s almost every major cosmologist preferred an eternal steady state universe, and several complained that the beginning of time implied by the Big Bang imported religious concepts into physics; this objection was later repeated by supporters of the steady state theory. This perception was enhanced by the fact that the originator of the Big Bang theory, Monsignor Georges Lemaître, was a Roman Catholic priest. Arthur Eddington agreed with Aristotle that the universe did not have a beginning in time, viz., that matter is eternal. A beginning in time was \"repugnant\" to him. Lemaître, however, thought thatIf the world has begun with a single quantum, the notions of space and time would altogether fail to have any meaning at the beginning; they would only begin to have a sensible meaning when the original quantum had been divided into a sufficient number of quanta. If this suggestion is correct, the beginning of the world happened a little before the beginning of space and time.\n\nDuring the 1930s other ideas were proposed as non-standard cosmologies to explain Hubble's observations, including the Milne model, the oscillatory universe (originally suggested by Friedmann, but advocated by Albert Einstein and Richard Tolman) and Fritz Zwicky's tired light hypothesis.\n\nAfter World War II, two distinct possibilities emerged. One was Fred Hoyle's steady state model, whereby new matter would be created as the universe seemed to expand. In this model the universe is roughly the same at any point in time. The other was Lemaître's Big Bang theory, advocated and developed by George Gamow, who introduced big bang nucleosynthesis (BBN) and whose associates, Ralph Alpher and Robert Herman, predicted the CMB. Ironically, it was Hoyle who coined the phrase that came to be applied to Lemaître's theory, referring to it as \"this \"big bang\" idea\" during a BBC Radio broadcast in March 1949. For a while, support was split between these two theories. Eventually, the observational evidence, most notably from radio source counts, began to favor Big Bang over Steady State. The discovery and confirmation of the CMB in 1964 secured the Big Bang as the best theory of the origin and evolution of the universe. Much of the current work in cosmology includes understanding how galaxies form in the context of the Big Bang, understanding the physics of the universe at earlier and earlier times, and reconciling observations with the basic theory.\n\nIn 1968 and 1970 Roger Penrose, Stephen Hawking, and George F. R. Ellis published papers where they showed that mathematical singularities were an inevitable initial condition of general relativistic models of the Big Bang. Then, from the 1970s to the 1990s, cosmologists worked on characterizing the features of the Big Bang universe and resolving outstanding problems. In 1981, Alan Guth made a breakthrough in theoretical work on resolving certain outstanding theoretical problems in the Big Bang theory with the introduction of an epoch of rapid expansion in the early universe he called \"inflation\". Meanwhile, during these decades, two questions in observational cosmology that generated much discussion and disagreement were over the precise values of the Hubble Constant and the matter-density of the universe (before the discovery of dark energy, thought to be the key predictor for the eventual fate of the universe).\n\nIn the mid-1990s, observations of certain globular clusters appeared to indicate, that they were about 15 billion years old, which conflicted with most then-current estimates of the age of the universe (and indeed with the age measured today). This issue was later resolved when new computer simulations, which included the effects of mass loss due to stellar winds, indicated a much younger age for globular clusters. While there still remain some questions as to how accurately the ages of the clusters are measured, globular clusters are of interest to cosmology as some of the oldest objects in the universe.\n\nSignificant progress in Big Bang cosmology has been made since the late 1990s as a result of advances in telescope technology as well as the analysis of data from satellites such as COBE, the Hubble Space Telescope and WMAP. Cosmologists now have fairly precise and accurate measurements of many of the parameters of the Big Bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.\n\nThe earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by Big Bang nucleosynthesis. More recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures, These are sometimes called the \"four pillars\" of the Big Bang theory.\n\nPrecise modern models of the Big Bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the Standard Model of particle physics. Of these features, dark matter is currently subjected to the most active laboratory investigations. Remaining issues include the cuspy halo problem and the dwarf galaxy problem of cold dark matter. Dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible. Inflation and baryogenesis remain more speculative features of current Big Bang models. Viable, quantitative explanations for such phenomena are still being sought. These are currently unsolved problems in physics.\n\nObservations of distant galaxies and quasars show that these objects are redshifted—the light emitted from them has been shifted to longer wavelengths. This can be seen by taking a frequency spectrum of an object and matching the spectroscopic pattern of emission lines or absorption lines corresponding to atoms of the chemical elements interacting with the light. These redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. If the redshift is interpreted as a Doppler shift, the recessional velocity of the object can be calculated. For some galaxies, it is possible to estimate distances via the cosmic distance ladder. When the recessional velocities are plotted against these distances, a linear relationship known as Hubble's law is observed:\nwhere\n\nHubble's law has two possible explanations. Either we are at the center of an explosion of galaxies—which is untenable given the Copernican principle—or the universe is uniformly expanding everywhere. This universal expansion was predicted from general relativity by Alexander Friedmann in 1922 and Georges Lemaître in 1927, well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang theory as developed by Friedmann, Lemaître, Robertson, and Walker.\n\nThe theory requires the relation \"v\" = \"HD\" to hold at all times, where \"D\" is the comoving distance, \"v\" is the recessional velocity, and \"v\", \"H\", and \"D\" vary as the universe expands (hence we write \"H\" to denote the present-day Hubble \"constant\"). For distances much smaller than the size of the observable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocity \"v\". However, the redshift is not a true Doppler shift, but rather the result of the expansion of the universe between the time the light was emitted and the time that it was detected.\n\nThat space is undergoing metric expansion is shown by direct observational evidence of the Cosmological principle and the Copernican principle, which together with Hubble's law have no other explanation. Astronomical redshifts are extremely isotropic and homogeneous, supporting the Cosmological principle that the universe looks the same in all directions, along with much other evidence. If the redshifts were the result of an explosion from a center distant from us, they would not be so similar in different directions.\n\nMeasurements of the effects of the cosmic microwave background radiation on the dynamics of distant astrophysical systems in 2000 proved the Copernican principle, that, on a cosmological scale, the Earth is not in a central position. Radiation from the Big Bang was demonstrably warmer at earlier times throughout the universe. Uniform cooling of the CMB over billions of years is explainable only if the universe is experiencing a metric expansion, and excludes the possibility that we are near the unique center of an explosion.\n\nIn 1964 Arno Penzias and Robert Wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band. Their discovery provided substantial confirmation of the big-bang predictions by Alpher, Herman and Gamow around 1950. Through the 1970s the radiation was found to be approximately consistent with a black body spectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725 K. This tipped the balance of evidence in favor of the Big Bang model, and Penzias and Wilson were awarded a Nobel Prize in 1978.\nThe \"surface of last scattering\" corresponding to emission of the CMB occurs shortly after \"recombination\", the epoch when neutral hydrogen becomes stable. Prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quickly scattered from free charged particles. Peaking at around , the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent.\n\nIn 1989, NASA launched the Cosmic Background Explorer satellite (COBE), which made two major advances: in 1990, high-precision spectrum measurements showed that the CMB frequency spectrum is an almost perfect blackbody with no deviations at a level of 1 part in 10, and measured a residual temperature of 2.726 K (more recent measurements have revised this figure down slightly to 2.7255 K); then in 1992, further COBE measurements discovered tiny fluctuations (anisotropies) in the CMB temperature across the sky, at a level of about one part in 10. John C. Mather and George Smoot were awarded the 2006 Nobel Prize in Physics for their leadership in these results.\n\nDuring the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. In 2000–2001 several experiments, most notably BOOMERanG, found the shape of the universe to be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.\n\nIn early 2003, the first results of the Wilkinson Microwave Anisotropy Probe (WMAP) were released, yielding what were at the time the most accurate values for some of the cosmological parameters. The results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general. The Planck space probe was launched in May 2009. Other ground and balloon based cosmic microwave background experiments are ongoing.\n\nUsing the Big Bang model it is possible to calculate the concentration of helium-4, helium-3, deuterium, and lithium-7 in the universe as ratios to the amount of ordinary hydrogen. The relative abundances depend on a single parameter, the ratio of photons to baryons. This value can be calculated independently from the detailed structure of CMB fluctuations. The ratios predicted (by mass, not by number) are about 0.25 for /, about 10 for /, about 10 for / and about 10 for /.\n\nThe measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. The agreement is excellent for deuterium, close but formally discrepant for , and off by a factor of two for ; in the latter two cases there are substantial systematic uncertainties. Nonetheless, the general consistency with abundances predicted by Big Bang nucleosynthesis is strong evidence for the Big Bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to \"tune\" the Big Bang to produce much more or less than 20–30% helium. Indeed, there is no obvious reason outside of the Big Bang that, for example, the young universe (i.e., before star formation, as determined by studying matter supposedly free of stellar nucleosynthesis products) should have more helium than deuterium or more deuterium than , and in constant ratios, too.\n\nDetailed observations of the morphology and distribution of galaxies and quasars are in agreement with the current state of the Big Bang theory. A combination of observations and theory suggest that the first quasars and galaxies formed about a billion years after the Big Bang, and since then, larger structures have been forming, such as galaxy clusters and superclusters.\n\nPopulations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). Moreover, galaxies that formed relatively recently, appear markedly different from galaxies formed at similar distances but shortly after the Big Bang. These observations are strong arguments against the steady-state model. Observations of star formation, galaxy and quasar distributions and larger structures, agree well with Big Bang simulations of the formation of structure in the universe, and are helping to complete details of the theory.\n\nIn 2011, astronomers found what they believe to be pristine clouds of primordial gas by analyzing absorption lines in the spectra of distant quasars. Before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. These two clouds of gas contain no elements heavier than hydrogen and deuterium. Since the clouds of gas have no heavy elements, they likely formed in the first few minutes after the Big Bang, during Big Bang nucleosynthesis.\n\nThe age of the universe as estimated from the Hubble expansion and the CMB is now in good agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory of stellar evolution to globular clusters and through radiometric dating of individual Population II stars.\n\nThe prediction that the CMB temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift. This prediction also implies that the amplitude of the Sunyaev–Zel'dovich effect in clusters of galaxies does not depend directly on redshift. Observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.\n\nFuture gravitational waves observatories might be able to detect primordial gravitational waves, relics of the early universe, up to less than a second after the Big Bang.\n\nAs with any theory, a number of mysteries and problems have arisen as a result of the development of the Big Bang theory. Some of these mysteries and problems have been resolved while others are still outstanding. Proposed solutions to some of the problems in the Big Bang model have revealed new mysteries of their own. For example, the horizon problem, the magnetic monopole problem, and the flatness problem are most commonly resolved with inflationary theory, but the details of the inflationary universe are still left unresolved and many, including some founders of the theory, say it has been disproven. What follows are a list of the mysterious aspects of the Big Bang theory still under intense investigation by cosmologists and astrophysicists.\n\nIt is not yet understood why the universe has more matter than antimatter. It is generally assumed that when the universe was young and very hot it was in statistical equilibrium and contained equal numbers of baryons and antibaryons. However, observations suggest that the universe, including its most distant parts, is made almost entirely of matter. A process called baryogenesis was hypothesized to account for the asymmetry. For baryogenesis to occur, the Sakharov conditions must be satisfied. These require that baryon number is not conserved, that C-symmetry and CP-symmetry are violated and that the universe depart from thermodynamic equilibrium. All these conditions occur in the Standard Model, but the effects are not strong enough to explain the present baryon asymmetry.\n\nMeasurements of the redshift–magnitude relation for type Ia supernovae indicate that the expansion of the universe has been accelerating since the universe was about half its present age. To explain this acceleration, general relativity requires that much of the energy in the universe consists of a component with large negative pressure, dubbed \"dark energy\".\n\nDark energy, though speculative, solves numerous problems. Measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly the critical density of mass/energy. But the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30% of the critical density. Since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the \"missing\" energy density. Dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency of gravitational lenses, and the other using the characteristic pattern of the large-scale structure as a cosmic ruler.\n\nNegative pressure is believed to be a property of vacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the Big Bang. Results from the WMAP team in 2008 are in accordance with a universe that consists of 73% dark energy, 23% dark matter, 4.6% regular matter and less than 1% neutrinos. According to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. Therefore, matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in the far future as dark energy becomes even more dominant.\n\nThe dark energy component of the universe has been explained by theorists using a variety of competing theories including Einstein's cosmological constant but also extending to more exotic forms of quintessence or other modified gravity schemes. A cosmological constant problem, sometimes called the \"most embarrassing problem in physics\", results from the apparent discrepancy between the measured energy density of dark energy, and the one naively predicted from Planck units.\n\nDuring the 1970s and the 1980s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. This led to the idea that up to 90% of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. In addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. In particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. While dark matter has always been controversial, it is inferred by various observations: the anisotropies in the CMB, galaxy cluster velocity dispersions, large-scale structure distributions, gravitational lensing studies, and X-ray measurements of galaxy clusters.\n\nIndirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. Many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.\n\nAdditionally, there are outstanding problems associated with the currently favored cold dark matter model which include the dwarf galaxy problem and the cuspy halo problem. Alternative theories have been proposed that do not require a large amount of undetected matter, but instead modify the laws of gravity established by Newton and Einstein; yet no alternative theory has been as successful as the cold dark matter proposal in explaining all extant observations.\n\nThe horizon problem results from the premise that information cannot travel faster than light. In a universe of finite age this sets a limit—the particle horizon—on the separation of any two regions of space that are in causal contact. The observed isotropy of the CMB is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. There would then be no mechanism to cause wider regions to have the same temperature.\n\nA resolution to this apparent inconsistency is offered by inflationary theory in which a homogeneous and isotropic scalar energy field dominates the universe at some very early period (before baryogenesis). During inflation, the universe undergoes exponential expansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each other's particle horizon. The observed isotropy of the CMB then follows from the fact that this larger region was in causal contact before the beginning of inflation.\n\nHeisenberg's uncertainty principle predicts that during the inflationary phase there would be quantum thermal fluctuations, which would be magnified to cosmic scale. These fluctuations serve as the seeds of all current structure in the universe. Inflation predicts that the primordial fluctuations are nearly scale invariant and Gaussian, which has been accurately confirmed by measurements of the CMB.\n\nIf inflation occurred, exponential expansion would push large regions of space well beyond our observable horizon.\n\nA related issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well before electroweak symmetry breaking occurs, so inflation should not be able to prevent large-scale discontinuities in the electroweak vacuum since distant parts of the observable universe were causally separate when the electroweak epoch ended.\n\nThe magnetic monopole objection was raised in the late 1970s. Grand unified theories predicted topological defects in space that would manifest as magnetic monopoles. These objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. This problem is also resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.\n\nThe flatness problem (also known as the oldness problem) is an observational problem associated with a Friedmann–Lemaître–Robertson–Walker metric (FLRW). The universe may have positive, negative, or zero spatial curvature depending on its total energy density. Curvature is negative, if its density is less than the critical density; positive, if greater; and zero at the critical density, in which case space is said to be \"flat\".\n\nThe problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat. Given that a natural timescale for departure from flatness might be the Planck time, 10 seconds, the fact that the universe has reached neither a heat death nor a Big Crunch after billions of years requires an explanation. For instance, even at the relatively late age of a few minutes (the time of nucleosynthesis), the density of the universe must have been within one part in 10 of its critical value, or it would not exist as it does today.\n\nGottfried Wilhelm Leibniz wrote: \"\"Why is there something rather than nothing? The sufficient reason [...] is found in a substance which [...] is a necessary being bearing the reason for its existence within itself.\"\" Philosopher of physics Dean Rickles has argued that numbers and mathematics (or their underlying laws) may necessarily exist. Physics may conclude that time did not exist before 'Big Bang', but 'started' with the Big Bang and hence there might be no 'beginning', 'before' or potentially 'cause' and instead always existed. Some also argue that nothing cannot exist or that non-existence might never have been an option. Quantum fluctuations, or other laws of physics that may have existed at the start of the Big Bang could then create the conditions for matter to occur.\n\nBefore observations of dark energy, cosmologists considered two scenarios for the future of the universe. If the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. It would become denser and hotter again, ending with a state similar to that in which it started—a Big Crunch.\n\nAlternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. Star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out, leaving white dwarfs, neutron stars, and black holes. Very gradually, collisions between these would result in mass accumulating into larger and larger black holes. The average temperature of the universe would asymptotically approach absolute zero—a Big Freeze. Moreover, if the proton were unstable, then baryonic matter would disappear, leaving only radiation and black holes. Eventually, black holes would evaporate by emitting Hawking radiation. The entropy of the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.\n\nModern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond our event horizon and out of contact with us. The eventual result is not known. The ΛCDM model of the universe contains dark energy in the form of a cosmological constant. This theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. Other explanations of dark energy, called phantom energy theories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called Big Rip.\n\nThe following is a partial list of the popular misconceptions about the Big Bang model:\n\n\"The Big Bang as the origin of the universe:\" One of the common misconceptions about the Big Bang model is the belief that it was the origin of the universe. However, the Big Bang model does not comment about how the universe came into being. Current conception of the Big Bang model assumes the existence of energy, time, and space, and does not comment about their origin or the cause of the dense and high temperature initial state of the universe.\n\n\"The Big Bang was \"small\"\": It is misleading to visualize the Big Bang by comparing its size to everyday objects. When the size of the universe at Big Bang is described, it refers to the size of the observable universe, and not the entire universe.\n\n\"Hubble's law violates special theory of relativity\": Hubble's law predicts that galaxies that are beyond Hubble Distance recede faster than the speed of light. However, special relativity does not apply beyond motion through space. Hubble's law describes velocity that results from expansion \"of\" space, rather than \"through\" space.\n\n\"Doppler redshift vs cosmological red-shift\": Astronomers often refer to the cosmological red-shift as a normal Doppler shift, which is a misconception. Although similar, the cosmological red-shift is not identical to the Doppler redshift. The Doppler redshift is based on special relativity, which does not consider the expansion of space. On the contrary, the cosmological red-shift is based on general relativity, in which the expansion of space is considered. Although they may appear identical for nearby galaxies, it may cause confusion if the behavior of distant galaxies is understood through the Doppler redshift.\n\nWhile the Big Bang model is well established in cosmology, it is likely to be refined. The Big Bang theory, built upon the equations of classical general relativity, indicates a singularity at the origin of cosmic time; this infinite energy density is regarded as impossible in physics. Still, it is known that the equations are not applicable before the time when the universe cooled down to the Planck temperature, and this conclusion depends on various assumptions, of which some could never be experimentally verified. \"(Also see Planck epoch.)\"\n\nOne proposed refinement to avoid this would-be singularity is to develop a correct treatment of quantum gravity.\n\nIt is not known what could have preceded the hot dense state of the early universe or how and why it originated, though speculation abounds in the field of cosmogony.\n\nSome proposals, each of which entails untested hypotheses, are:\n\nProposals in the last two categories, see the Big Bang as an event in either a much larger and older universe or in a multiverse.\n\nAs a description of the origin of the universe, the Big Bang has significant bearing on religion and philosophy. As a result, it has become one of the liveliest areas in the discourse between science and religion. Some believe the Big Bang implies a creator, and some see its mention in their holy books, while others argue that Big Bang cosmology makes the notion of a creator superfluous.\n\n\n\n\n"}
{"id": "4119", "url": "https://en.wikipedia.org/wiki?curid=4119", "title": "Bock", "text": "Bock\n\nBock is a strong lager of German origin. Several substyles exist, including maibock (helles bock, heller bock), a paler, more hopped version generally made for consumption at spring festivals; doppelbock (double bock), a stronger and maltier version; and eisbock, a much stronger version made by partially freezing the beer and removing the ice that forms.\n\nOriginally a dark beer, a modern bock can range from light copper to brown in colour. The style is very popular, with many examples brewed internationally.\n\nThe style known now as \"bock\" was a dark, malty, lightly hopped ale first brewed in the 14th century by German brewers in the Hanseatic town of Einbeck. The style from Einbeck was later adopted by Munich brewers in the 17th century and adapted to the new lager style of brewing. Due to their Bavarian accent, citizens of Munich pronounced \"Einbeck\" as \"ein Bock\" (\"a billy goat\"), and thus the beer became known as \"bock\". To this day, as a visual pun, a goat often appears on bock labels.\n\nBock is historically associated with special occasions, often religious festivals such as Christmas, Easter or Lent (the latter as \"\"). Bocks have a long history of being brewed and consumed by Bavarian monks as a source of nutrition during times of fasting.\n\nBockfest, in Cincinnati, Ohio, is the longest running and largest bock beer festival in the world. The festival celebrates bock beer, the Cincinnati neighborhood of Over-the-Rhine, the city's German heritage, and the coming of spring.\n\nTraditional bock is a sweet, relatively strong (6.3%–7.2% by volume), lightly hopped (20-27 IBUs) lager. The beer should be clear, and colour can range from light copper to brown, with a bountiful and persistent off-white head. The aroma should be malty and toasty, possibly with hints of alcohol, but no detectable hops or fruitiness. The mouthfeel is smooth, with low to moderate carbonation and no astringency. The taste is rich and toasty, sometimes with a bit of caramel. Again, hop presence is low to undetectable, providing just enough bitterness so that the sweetness is not cloying and the aftertaste is muted. The following commercial products are indicative of the style: Einbecker Ur-Bock Dunkel, Pennsylvania Brewing St. Nick Bock, Aass Bock, Great Lakes Rockefeller Bock, Stegmaier Brewhouse Bock.\n\nThe maibock style, also known as helles bock or heller bock, is a helles lager brewed to bock strength; therefore, still as strong as traditional bock, but lighter in colour and with more hop presence. It is a fairly recent development compared to other styles of bock beers, frequently associated with springtime and the month of May. Colour can range from deep gold to light amber with a large, creamy, persistent white head, and moderate to moderately high carbonation, while alcohol content ranges from 6.3% to 7.4% by volume. The flavour is typically less malty than a traditional bock, and may be drier, hoppier, and more bitter, but still with a relatively low hop flavour, with a mild spicy or peppery quality from the hops, increased carbonation and alcohol content. The following commercial products are indicative of the style: Ayinger Maibock, Mahr’s Bock, Hacker-Pschorr Hubertus Bock, Capital Maibock, Einbecker Mai-Urbock, Hofbräu Maibock, Victory St. Boisterous, Gordon Biersch Blonde Bock, Smuttynose Maibock, Old Dominion Brewing Company Big Thaw Bock, [Brewery 85's Quittin' Time], Rogue Dead Guy Ale, Franconia Brewing Company Maibock Ale, Church Street maibock, and Tröegs Cultivator.\n\n\"Doppelbock\" or \"double bock\" is a stronger version of traditional bock that was first brewed in Munich by the Paulaner Friars, a Franciscan order founded by St. Francis of Paula. Historically, doppelbock was high in alcohol and sweet, thus serving as \"liquid bread\" for the Friars during times of fasting, when solid food was not permitted. Today, doppelbock is still strong—ranging from 7%–12% or more by volume. It isn't clear, with color ranging from dark gold, for the paler version, to dark brown with ruby highlights for darker version. It has a large, creamy, persistent head (although head retention may be impaired by alcohol in the stronger versions). The aroma is intensely malty, with some toasty notes, and possibly some alcohol presence as well; darker versions may have a chocolate-like or fruity aroma. The flavor is very rich and malty, with toasty notes and noticeable alcoholic strength, and little or no detectable hops (16–26 IBUs). Paler versions may have a drier finish. The monks who originally brewed doppelbock named their beer \"Salvator\" (\"Savior\"), which today is trademarked by Paulaner. Brewers of modern doppelbocks often add \"-ator\" to their beer's name as a signpost of the style; there are 200 \"-ator\" doppelbock names registered with the German patent office. The following are representative examples of the style: Predator, Paulaner Salvator, Ayinger Celebrator, Weihenstephaner Korbinian, Andechser Doppelbock Dunkel, Spaten Optimator, Augustiner Maximator, Tucher Bajuvator, Weltenburger Kloster Asam-Bock, Capital Autumnal Fire, EKU 28, Eggenberg Urbock 23º, Bell's Consecrator, Moretti La Rossa, Samuel Adams Double Bock, Tröegs Tröegenator Double Bock, Wasatch Brewery Devastator, Great Lakes Doppelrock, Abita Andygator, and Wolverine State Brewing Company Predator.\n\nEisbock is a traditional specialty beer of the Kulmbach district of Germany that is made by partially freezing a doppelbock and removing the water ice to concentrate the flavour and alcohol content, which ranges from 9% to 13% by volume. It is clear, with a colour ranging from deep copper to dark brown in colour, often with ruby highlights. Although it can pour with a thin off-white head, head retention is frequently impaired by the higher alcohol content. The aroma is intense, with no hop presence, but frequently can contain fruity notes, especially of prunes, raisins, and plums. Mouthfeel is full and smooth, with significant alcohol, although this should not be hot or sharp. The flavour is rich and sweet, often with toasty notes, and sometimes hints of chocolate, always balanced by a significant alcohol presence. The following are representative examples of the style: Kulmbacher Reichelbräu Eisbock, Eggenberg, Schneider Aventinus Eisbock, Urbock Dunkel Eisbock, Franconia Brewing Company Ice Bock 17%.\n\nThe strongest ice-beer is being produced by a Franconian company as well - it is called Schorschbräu and is 57% (current world record).\n\nWeizenbock is a style of bock brewed using wheat instead of barley. It was first produced in Bavaria in 1907 by G. Schneider & Sohn and was named \"Aventinus\" after a Bavarian historian. The style combines darker Munich malts and top-fermenting wheat beer yeast, brewed at the strength of a doppelbock.\n\nIn Austria, bockbier is traditionally brewed only around Christmas and Easter, when nearly every brewery brews its own bock.\n\nA number of bock beers are produced, including Brasserie d'Achouffe Bok and \"Leute Bok\" from the Van Steenberge brewer, brewed since 1927. Belgium-based InBev produces Artois Bock, which is exported internationally and can be found in areas where bock is not traditionally available.\n\nZagorka Brewery produces \"Stolichno Bock Beer\", a 6.5% abv beer.\n\nOnce a year Budweiser Budvar brew the 7.5% Imperial with Saaz wet/green hops fresh from the farm and matured for 200 days for release in late spring.\n\n\nBock is an unusual style in the UK, but a few examples exist. The Robert Cains brewery in Liverpool brews Cains Double Bock beer at 8% abv, and the Dark Star Brewery, West Sussex, produce a 5.6% abv Maibock.\n\nDreher Brewery sells a rather strong (7.3% ABV) bock beer. It is called Bak, the name for billy goat in Hungarian.\n\nA variation of bock called 'bokbier' is also brewed extensively in the Netherlands and occasionally in Belgium. Most larger Dutch breweries, such as Heineken International, Grolsch, Amstel, Alfa Brouwerij, Brand and Dommelsch, market at least one variety. Most bokbiers tend to be seasonal beers (traditionally autumn, although there are currently also spring, summer and winter boks). Microbreweries may prefer to seasonally brew a bokbier, such as the eco-beer biobok, made in autumn by Brouwerij 't IJ in Amsterdam. The consumers' organization PINT holds a bok festival every autumn at the Beurs van Berlage in Amsterdam.\n\nBocks are also brewed in Norway, where they are known as \"bokkøl\" (bockbeers) and available during the whole year. Notable examples of bock brands are Aass, Borg, Frydenlund and Mack.\n\nBocks are also brewed in Poland, where they are known as \"Koźlak\" and available during the whole year. Notable examples of bock brands are Koźlak Amber, Miłosław Koźlak, Cornelius Kożlak, Perła Kożlak.\n\nOne of the main beer brands in Portugal is Super Bock.\n\nA Spanish brewery Damm produces Bock Damm (5,4%)\n\nThe countries biggest brewer, Laško Brewery, made double bock named Striptis. It is the part of the Special line, which is a response to the rising interest of craft beers.\n\nOnly one Bock beer is brewed in Sweden; Mariestad's Old Ox, with an alcoholic percentage of 6,9%.\n\nIn Iceland, Einstök Olgerd brews an Icelandic Dopplebock. It is 6,7% ABV.\n\nThere are several beers brewed in Argentina which are termed bock, including \"Araucana Negra Bock\", \"Quilmes Bock\", and \"Blest Bock\".\n\nIn Bolivia, the Cerveceria Boliviana Nacional brews a beer called simply \"Cerveza Bock,\" advertised primarily for its 7% alcohol by volume strength.\n\nIn Brazil, Kaiser is one of the breweries that sells bock beer, called \"Kaiser Bock\". This beer is available only in the months of fall and winter (April to September). Usually Brazilian bocks are produced by local breweries or craft breweries, especially in the cities of German settlement in Paraná/Santa Catarina States and also in Petrópolis, state of Rio de Janeiro. Kaiser Bock is made in Ponta Grossa, from Curitiba, capital of Paraná.\n\nKunstmann Brewery from Valdivia produces a dark, bittersweet version of bock. Kross brewery from Curacavi is producing a maibock (6.5% abv)\n\nInducerv S.A.S. brews a Bock (6.0% vol) under their Apóstol brand with German ingredients. The brewery is located in the city of Sabaneta, near Medellín.\n\n\nCervecería Centro Americana brews \"Moza\", a bock beer with 5% ABV.\n\n\nDomingo 7 brews an Amber Bock called the \"Búho\".\n\nBock is a popular style, made by breweries across the country, including:\n\nCayman Island Brewing (George Town, Cayman Islands): Ironshore Bock (7.5% abv)\n\nBock beer is produced in Mexico around Christmas season, under the Noche Buena label with 5.9% abv\n\nBock and its substyles are popular in all parts of the country. Shiner Bock was first brewed in Texas in 1913 and became a year-round brew in 1973. Coors Brewing Company in Golden, Colorado was well known for its bock beers that it formerly brewed every spring. The city of Cincinnati, Ohio has hosted a celebration called Bockfest since 1992 that promotes its German-style brewing history and the German culture of its Over-the-Rhine neighbourhood. A short list of American bocks include:\n\n\n\nBock beer is produced and distributed under the Urbock label by Namibian Breweries. Like other Namibian Breweries beers, it is available in some of the neighbouring countries in Southern Africa, especially South Africa. The brewery also produces a maibock sporadically.\n\n\"Forefathers Willie Simpson Doppelbock Lager\" by Stone & Wood Brewing Co.\n\n2016 small batch \"Hoppy Doppelbock\" 7.5% by 4 Pines Brewing Company - Manly NSW\n\n\"Original Bock\" 5.5% by Balmain Brewing Company - Balmain NSW\n\n\"Smokin' Bishop\"; a bock-style beer that is brewed at the Invercargill Brewery\n\nDoppel-bock by Monteith's; 6% alcohol\n\n"}
{"id": "4124", "url": "https://en.wikipedia.org/wiki?curid=4124", "title": "Bantu languages", "text": "Bantu languages\n\nThe Bantu languages (), technically the Narrow Bantu languages as opposed to \"Wide Bantu\", a loosely defined categorization which includes other Bantoid languages, constitute a traditional branch of the Niger–Congo languages. There are about 250 Bantu languages by the criterion of mutual intelligibility, though the distinction between language and dialect is often unclear, and \"Ethnologue\" counts 535 languages. Bantu languages are spoken largely east and south of present-day Cameroon, that is, in the regions commonly known as Central Africa, Southeast Africa, and Southern Africa. Parts of the Bantu area include languages from other language families (see map).\n\nEstimates of number of speakers of most languages vary widely, due both to the lack of accurate statistics in most developing countries and the difficulty in defining exactly where the boundaries of a language lie, particularly in the presence of a dialect continuum.\n\nThe Bantu language with the largest total number of speakers is Swahili; however, the majority of its speakers know it as a second language. According to Ethnologue, there are over 180 million L2 (second-language) speakers, but only about 2 million native speakers.\n\nOther major languages include Zulu with 27 million speakers (15.7 million L2) and Shona with about 11 million speakers (if Manyika and Ndau are included). Ethnologue separates the largely mutually intelligible Kinyarwanda and Kirundi, which, if grouped together, have 12.4 million speakers.\n\nThe Bantu languages descend from a common Proto-Bantu language, which is believed to have been spoken in what is now Cameroon in Central Africa. \nAn estimated 2,500–3,000 years ago (1000 BC to 500 BC), although other sources put the start of the Bantu Expansion closer to 3000 BC, speakers of the Proto-Bantu language began a series of migrations eastward and southward, carrying agriculture with them. This Bantu expansion came to dominate Sub-Saharan Africa east of Cameroon, an area where Bantu peoples now constitute nearly the entire population.\n\nThe most widely used classification is an alphanumeric coding system developed by Malcolm Guthrie in his 1948 classification of the Bantu languages. is mainly geographic. The term 'narrow Bantu' was coined by the \"Benue–Congo Working Group\" to distinguish Bantu as recognized by Guthrie, from the Bantoid languages not recognized as Bantu by Guthrie.\n\nIn recent times, the distinctiveness of Narrow Bantu as opposed to the other Southern Bantoid groups has been called into doubt (cf. Piron 1995, Williamson & Blench 2000, Blench 2011), but the term is still widely used. \nA coherent classification of Narrow Bantu will likely need to exclude many of the Zone A and perhaps Zone B languages. \n\nThere is no true genealogical classification of the (Narrow) Bantu languages. Until recently most attempted classifications only considered languages that happen to fall within traditional Narrow Bantu, but there seems to be a continuum with the related languages of South Bantoid. \n\nAt a broader level, the family is commonly split in two depending on the reflexes of proto-Bantu tone patterns: Many Bantuists group together parts of zones A through D (the extent depending on the author) as \"Northwest Bantu\" or \"Forest Bantu\", and the remainder as \"Central Bantu\" or \"Savanna Bantu\". The two groups have been described as having mirror-image tone systems: where Northwest Bantu has a high tone in a cognate, Central Bantu languages generally have a low tone, and vice versa. \n\nNorthwest Bantu is more divergent internally than Central Bantu, and perhaps less conservative due to contact with non-Bantu Niger–Congo languages; Central Bantu is likely the innovative line cladistically. Northwest Bantu is clearly not a coherent family, but even for Central Bantu the evidence is lexical, with little evidence that it is a historically valid group.\n\nAnother attempt at a detailed genetic classification to replace the Guthrie system is the 1999 \"Tervuren\" proposal of Bastin, Coupez, and Mann. However, it relies on lexicostatistics, which, because of its reliance on similarity rather than shared innovations, may predict spurious groups of conservative languages that are not closely related. Meanwhile, \"Ethnologue\" has added languages to the Guthrie classification which Guthrie overlooked, while removing the Mbam languages (much of zone A), and shifting some languages between groups (much of zones D and E to a new zone J, for example, and part of zone L to K, and part of M to F) in an apparent effort at a semi-genetic, or at least semi-areal, classification. This has been criticized for sowing confusion in one of the few unambiguous ways to distinguish Bantu languages. Nurse & Philippson (2006) evaluate many proposals for low-level groups of Bantu languages, but the result is not a complete portrayal of the family. \"Glottolog\" has incorporated many of these into their classification.\n\nThe languages that share Dahl's Law may also form a valid group, Northeast Bantu. The infobox at right lists these together with various low-level groups that are fairly uncontroversial, though they continue to be revised. The development of a rigorous genealogical classification of many branches of Niger–Congo, not just Bantu, is hampered by insufficient data.\n\nGuthrie reconstructed both the phonemic inventory and the vocabulary of Proto-Bantu.\n\nThe most prominent grammatical characteristic of Bantu languages is the extensive use of affixes (see Sotho grammar and Ganda noun classes for detailed discussions of these affixes). Each noun belongs to a class, and each language may have several numbered classes, somewhat like grammatical gender in European languages. The class is indicated by a prefix that is part of the noun, as well as agreement markers on verb and qualificative roots connected with the noun. Plural is indicated by a change of class, with a resulting change of prefix.\n\nThe verb has a number of prefixes, though in the western languages these are often treated as independent words. In Swahili, for example, \"Mtoto mdogo amekisoma\" (for comparison, \"Kamwana kadoko karikuverenga\" in Shona language) means 'The small child has read it [a book]'. \"Mtoto\" 'child' governs the adjective prefix \"m-\" and the verb subject prefix \"a-\". Then comes perfect tense \"-me-\" and an object marker \"-ki-\" agreeing with implicit \"kitabu\" 'book' (from Arabic \"kitab\"). Pluralizing to 'children' gives \"Watoto wadogo wamekisoma\" (\"Vana vadoko vakaverenga\" in Shona), and pluralizing to 'books' (\"vitabu\") gives \"Watoto wadogo wamevisoma\".\n\nBantu words are typically made up of open syllables of the type CV (consonant-vowel) with most languages having syllables exclusively of this type. The Bushong language recorded by Vansina, however, has final consonants, while slurring of the final syllable (though written) is reported as common among the Tonga of Malawi. The morphological shape of Bantu words is typically CV, VCV, CVCV, VCVCV, etc.; that is, any combination of CV (with possibly a V- syllable at the start). In other words, a strong claim for this language family is that almost all words end in a vowel, precisely because closed syllables (CVC) are not permissible in most of the documented languages, as far as is understood. \n\nThis tendency to avoid consonant clusters in some positions is important when words are imported from English or other non-Bantu languages. An example from Chewa: the word \"school\", borrowed from English, and then transformed to fit the sound patterns of this language, is \"sukulu\". That is, \"sk-\" has been broken up by inserting an epenthetic \"-u-\"; \"-u\" has also been added at the end of the word. Another example is \"buledi\" for \"bread\". Similar effects are seen in loanwords for other non-African CV languages like Japanese. However, a clustering of sounds at the beginning of a syllable can be readily observed in such languages as Shona, and the Makua languages.\n\nReduplication is a common morphological phenomenon in Bantu languages and is usually used to indicate frequency or intensity of the action signalled by the (unreduplicated) verb stem.\n\n\nWell-known words and names that have reduplication include\nRepetition emphasizes the repeated word in the context that it is used. For instance, \"Mwenda pole hajikwai,\" while, \"Pole pole ndio mwendo,\" has two to emphasize the consistency of slowness of the pace. The meaning of the former in translation is, \"He who goes slowly doesn't trip,\" and that of the latter is, \"A slow but steady pace wins the race.\" Haraka haraka would mean hurrying just for the sake of hurrying, reckless hurry, as in \"Njoo! Haraka haraka\" [come here! Hurry, hurry].\n\nIn contrast, there are some words in some of the languages in which reduplication has the opposite meaning. It usually denotes short durations, and or lower intensity of the action and also means a few repetitions or a little bit more.\n\n\nThe following is a list of nominal classes in Bantu:\nFollowing is an incomplete list of the principal Bantu languages of each country. Included are those languages that constitute at least 1% of the population and have at least 10% the number of speakers of the largest Bantu language in the country.\nAn attempt at a full list of Bantu languages (with various conflations and a puzzlingly diverse nomenclature) can be found in \"The Bantu Languages of Africa\", 1959.\n\nMost languages are best known in English without the class prefix (\"Swahili\", \"Tswana\", \"Ndebele\"), but are sometimes seen with the (language-specific) prefix (\"Kiswahili\", \"Setswana\", \"Sindebele\"). In a few cases prefixes are used to distinguish languages with the same root in their name, such as Tshiluba and Kiluba (both \"Luba\"), Umbundu and Kimbundu (both \"Mbundu\"). The bare (prefixless) form typically does not occur in the language itself, but is the basis for other words based on the ethnicity. So, in the country of Botswana the people are the \"Batswana\", one person is a \"Motswana\", and the language is \"Setswana\"; and in Uganda, centred on the kingdom of \"Buganda\", the dominant ethnicity are the \"Baganda\" (sg. \"Muganda\"), whose language is \"Luganda\".\n\nLingua franca\n\nAngola\n\nBotswana\n\nBurundi\n\nCameroon\n\nCentral African Republic\n\nDemocratic Republic of the Congo\n\nEquatorial Guinea\n\nGabon\n\nKenya\n\nLesotho\n\nMalawi\n\nMozambique\n\nNamibia\nRepublic of the Congo (Congo-Brazzaville)\n\nRwanda\n\nSouth Africa\nAccording to the South African National Census of 2011\nTOTAL Nguni: 22,406,O49 (61.98%)\nTOTAL Sotho-Tswana: 13,744,775 (38.02%)\nTOTAL OFFICIAL INDIGENOUS LANGUAGE SPEAKERS: 36,150,824 (69.83%)\n\nSwaziland\n\nTanzania\n\nUganda\n\nZambia\n\nZimbabwe\nMap 1 shows Bantu languages in Africa and map 2 a magnification of the Benin, Nigeria and Cameroon area, as of July 2017.\nA case has been made out for borrowings of many place-names and even misremembered rhymes – chiefly from one of the Luba varieties – in the USA.\n\nSome words from various Bantu languages have been borrowed into western languages. These include: \n\nAlong with the Latin script and Arabic script orthographies, there are also some modern indigenous writing systems used for Bantu languages:\n\n\n\n"}
{"id": "4127", "url": "https://en.wikipedia.org/wiki?curid=4127", "title": "Bearing", "text": "Bearing\n\nBearing may refer to:\n"}
{"id": "4130", "url": "https://en.wikipedia.org/wiki?curid=4130", "title": "CIM-10 Bomarc", "text": "CIM-10 Bomarc\n\nThe Boeing CIM-10 Bomarc (IM-99 Weapon System prior to September 1962) was a supersonic long-range surface-to-air missile (SAM) used during the Cold War for the air defense of North America. In addition to being the first operational long-range SAM, it was the only SAM deployed by the United States Air Force.\n\nStored horizontally in a launcher shelter with movable roof, the missile was erected, fired vertically using rocket boosters to high altitude, and then tipped over into a horizontal Mach 2.5 cruise powered by ramjet engines. This lofted trajectory allowed the missile to operate at a maximum range as great as . Controlled from the ground for most of its flight, when it reached the target area it was commanded to begin a dive, activating an onboard active radar homing seeker for terminal guidance. A radar proximity fuse detonated the warhead, either a large conventional explosive or the W40 nuclear warhead.\n\nThe Air Force originally planned for a total of 52 sites covering most of the major cities and industrial regions in the US. The US Army was deploying their own systems at the same time, and the two services fought constantly both in political circles and in the press. Development dragged on, and by the time it was ready for deployment in the late 1950s, the nuclear threat had moved from manned bombers to the intercontinental ballistic missile (ICBM), while the Army had successfully deployed their own system that filled any possible role in the 1960s, in spite of Air Force claims to the contrary.\n\nAs testing continued, the Air Force reduced its plans to sixteen sites, and then again to eight with an additional two sites in Canada. The first US site was declared operational in 1959, but with only a single working missile. Bringing the rest of the missiles into service took years, by which time the system was totally obsolete. Deactivations began in 1969 and by 1972 all Bomarc sites had been shut down. A small number were used as target drones, and only a few remain on display today.\n\nIn 1946, Boeing started to study surface-to-air guided missiles under the United States Army Air Forces project MX-606. By 1950, Boeing had launched more than 100 test rockets in various configurations, all under the designator XSAM-A-1 GAPA (Ground-to-Air Pilotless Aircraft). Because these tests were very promising, Boeing received a USAF contract in 1949 to develop a pilotless interceptor (a term then used by the USAF for air-defense guided missiles) under project MX-1599. The MX-1599 missile was to be a ramjet-powered, nuclear-armed long-range surface-to-air missile to defend the Continental United States from high-flying bombers. The Michigan Aerospace Research Center (MARC) was added to the project soon afterward, and this gave the new missile its name Bomarc (for Boeing and MARC). In 1951, the USAF decided to emphasize its point of view that missiles were nothing else than pilotless aircraft by assigning aircraft designators to its missile projects, and anti-aircraft missiles received F-for-Fighter designations. The Bomarc became the F-99.\n\nTest flights of XF-99 test vehicles began in September 1952 and continued through early 1955. The XF-99 tested only the liquid-fueled booster rocket, which would accelerate the missile to ramjet ignition speed. In February 1955, tests of the XF-99A propulsion test vehicles began. These included live ramjets, but still had no guidance system or warhead. The designation YF-99A had been reserved for the operational test vehicles. In August 1955, the USAF discontinued the use of aircraft-like type designators for missiles, and the XF-99A and YF-99A became XIM-99A and YIM-99A, respectively. Originally the USAF had allocated the designation IM-69, but this was changed (possibly at Boeing's request to keep number 99) to IM-99 in October 1955. In October 1957, the first YIM-99A production-representative prototype flew with full guidance, and succeeded to pass the target within destructive range. In late 1957, Boeing received the production contract for the IM-99A Bomarc A interceptor missile, and in September 1959, the first IM-99A squadron became operational.\n\nThe IM-99A had an operational radius of and was designed to fly at Mach 2.5–2.8 at a cruising altitude of . It was long and weighed . Its armament was either a conventional warhead or a W40 nuclear warhead (7–10 kiloton yield). A liquid-fuel rocket engine boosted the Bomarc to Mach 2, when its Marquardt RJ43-MA-3 ramjet engines, fueled by 80-octane gasoline, would take over for the remainder of the flight. This was the same model of engine used to power both the Lockheed X-7, the Lockheed AQM-60 Kingfisher drone used to test air defenses, and the Lockheed D-21 launched from the back of an SR-71 .\n\nThe operational IM-99A missiles were based horizontally in semi-hardened shelters, nicknamed \"coffins\". After the launch order, the shelter's roof would slide open, and the missile raised to the vertical. After the missile was supplied with fuel for the booster rocket, it would be launched by the Aerojet General LR59-AJ-13 booster. After sufficient speed was reached, the Marquardt RJ43-MA-3 ramjets would ignite and propel the missile to its cruise speed of Mach 2.8 at an altitude of .\n\nWhen the Bomarc was within of the target, its own Westinghouse AN/DPN-34 radar guided the missile to the interception point. The maximum range of the IM-99A was , and it was fitted with either a conventional high-explosive or a 10 kiloton W-40 nuclear fission warhead.\n\nThe Bomarc relied on the Semi-Automatic Ground Environment (SAGE), an automated control system used by NORAD for detecting, tracking and intercepting enemy bomber aircraft. SAGE allowed for remote launching of the Bomarc missiles, which were housed in a constant combat-ready basis in individual launch shelters in remote areas. At the height of the program, there were 14 Bomarc sites located in the US and two in Canada.\n\nThe liquid-fuel booster of the Bomarc A was no optimal solution. It took two minutes to fuel before launch, which could be a long time in high-speed intercepts, and its hypergolic propellants (hydrazine and nitric acid) were very dangerous to handle, leading to several serious accidents.\n\nAs soon as high-thrust solid-fuel rockets became a reality in the mid-1950s, the USAF began to develop a new solid-fueled Bomarc variant, the IM-99B Bomarc B. It used a Thiokol XM51 booster, and also had improved Marquardt RJ43-MA-7 (and finally the RJ43-MA-11) ramjets. The first IM-99B was launched in May 1959, but problems with the new propulsion system delayed the first fully successful flight until July 1960, when a supersonic KD2U-1/MQM-15A Regulus II drone was intercepted. Because the new booster took up less space in the missile, more ramjet fuel could be carried, increasing the range to . The terminal homing system was also improved, using the world's first pulse Doppler search radar, the Westinghouse AN/DPN-53. All Bomarc Bs were equipped with the W-40 nuclear warhead. In June 1961, the first IM-99B squadron became operational, and Bomarc B quickly replaced most Bomarc A missiles. On 23 March 1961, a Bomarc B successfully intercepted a Regulus II cruise missile flying at 100,000 ft, thus achieving the highest interception in the world up to that date.\n\nBoeing built 570 Bomarc missiles between 1957 and 1964, 269 CIM-10A, 301 CIM-10B.\n\nIn September 1958 Air Research & Development Command decided to transfer the Bomarc program from its testing at Cape Canaveral Air Force Station to a new facility on Santa Rosa Island, immediately south of Eglin AFB Hurlburt Field on the Gulf of Mexico. To operate the facility and to provide training and operational evaluation in the missile program, Air Defense Command established the 4751st Air Defense Wing (Missile) (4751st ADW) on 15 January 1958. The first launch from Santa Rosa took place on 15 January 1959.\n\nThe first USAF operational Bomarc squadron was the 46th Air Defense Missile Squadron (ADMS), organized on 1 January 1959 and activated on 25 March. The 46th ADMS was assigned to the New York Air Defense Sector at McGuire Air Force Base, New Jersey. The training program, under the 4751st ADW used technicians acting as instructors and was established for a four-month duration. Training included missile maintenance; SAGE operations and launch procedures, including the launch of an unarmed missile at Eglin. In September 1959 the squadron assembled at their permanent station, the Bomarc site near McGuire AFB, and trained for operational readiness. The first Bomarc-A were used at McGuire on 19 September 1959 with Kincheloe AFB getting the first operational IM-99Bs. While several of the squadrons replicated earlier fighter interceptor unit numbers, they were all new organizations with no previous historical counterpart.\n\nADC's initial plans called for some 52 Bomarc sites around the United States with 120 missiles each but as defense budgets decreased during the 1950s the number of sites dropped substantially. Ongoing development and reliability problems didn't help, nor did Congressional debate over the missile's usefulness and necessity. In June 1959, the Air Force authorized 16 Bomarc sites with 56 missiles each; the initial five would get the IM-99A with the remainder getting the IM-99B. However, in March 1960, HQ USAF cut deployment to eight sites in the United States and two in Canada.\n\nWithin a year of operations, a Bomarc-A with a nuclear warhead caught fire at McGuire AFB on 7 June 1960 after its on-board helium tank exploded. While the missile's explosives did not detonate, the heat melted the warhead and released plutonium, which the fire crews spread. The Air Force and the Atomic Energy Commission cleaned up the site and covered it with concrete. This was the only major incident involving the weapon system. The site remained in operation for several years following the fire. Since its closure in 1972, the area has remained off limits, primarily due to low levels of plutonium contamination. In 2002, the concrete at the site was removed and transported to Lakehurst Naval Air Station for transport by rail to a site for proper disposal.\n\nIn 1962, the US Air Force started using modified A-models as drones; following the October 1962 tri-service redesignation of aircraft and weapons systems they became CQM-10As. Otherwise the air defense missile squadrons maintained alert while making regular trips to Santa Rosa Island for training and firing practice. After the inactivation of the 4751st ADW(M) on 1 July 1962 and transfer of Hurlburt to Tactical Air Command for air commando operations the 4751st Air Defense Squadron (Missile) remained at Hurlburt and Santa Rosa Island for training purposes.\n\nIn 1964, the liquid-fueled Bomarc-A sites and squadrons began to be deactivated. The sites at Dow and Suffolk County closed first. The remainder continued to be operational for several more years while the government started dismantling the air defense missile network. Niagara Falls was the first BOMARC B installation to close, in December 1969; the others remained on alert through 1972. In April 1972, the last Bomarc B in U.S. Air Force service was retired at McGuire and the 46th ADMS inactivated and the base was deactivated.\n\nIn the era of the intercontinental ballistic missiles the Bomarc, designed to intercept relatively slow manned bombers, had become a useless asset. The remaining Bomarc missiles were used by all armed services as high-speed target drones for tests of other air-defense missiles. The Bomarc A and Bomarc B targets were designated as CQM-10A and CQM-10B, respectively.\n\nFollowing the accident, the McGuire complex has never been sold or converted to other uses and remains in Air Force ownership, making it the most intact site of the eight in the US. It has been nominated to the National Register of Historic Sites. Although a number of IM-99/CIM-10 Bomarcs have been placed on public display, because of concerns about the possible environmental hazards of the thoriated magnesium structure of the airframe several have been removed from public view.\n\nRuss Sneddon, director of the Air Force Armament Museum, Eglin Air Force Base, Florida provided information about missing CIM-10 exhibit airframe serial 59-2016, one of the museum's original artifacts from its founding in 1975 and donated by the 4751st Air Defense Squadron at Hurlburt Field, Eglin Auxiliary Field 9, Eglin AFB. As of December 2006, the suspect missile was stored in a secure compound behind the Armaments Museum. In December 2010, the airframe was still on premises, but partly dismantled.\n\nThe Bomarc Missile Program was highly controversial in Canada. The Progressive Conservative government of Prime Minister John Diefenbaker initially agreed to deploy the missiles, and shortly thereafter controversially scrapped the Avro Arrow, a supersonic manned interceptor aircraft, arguing that the missile program made the Arrow unnecessary.\n\nInitially, it was unclear whether the missiles would be equipped with nuclear warheads. By 1960 it became known that the missiles were to have a nuclear payload, and a debate ensued about whether Canada should accept nuclear weapons. Ultimately, the Diefenbaker government decided that the Bomarcs should not be equipped with nuclear warheads. The dispute split the Diefenbaker Cabinet, and led to the collapse of the government in 1963. The Official Opposition and Liberal Party leader Lester \"Mike\" Pearson originally was against nuclear missiles, but reversed his personal position and argued in favor of accepting nuclear warheads. He won the 1963 election, largely on the basis of this issue, and his new Liberal government proceeded to accept nuclear-armed Bomarcs, with the first being deployed on 31 December 1963. When the nuclear warheads were deployed, Pearson's wife, Maryon, resigned her honorary membership in the anti-nuclear weapons group, Voice of Women.\n\nCanadian operational deployment of the Bomarc involved the formation of two specialized Surface/Air Missile squadrons. The first to begin operations was No. 446 SAM Squadron at RCAF Station North Bay, Ontario which was the command and control center for both squadrons. With construction of the compound and related facilities completed in 1961, the squadron received its Bomarcs in 1961, without nuclear warheads. The squadron became fully operational from 31 December 1963, when the nuclear warheads arrived, until disbanding on 31 March 1972. All the warheads were stored separately and under control of Detachment 1 of the USAF 425th Munitions Maintenance Squadron. During operational service, the Bomarcs were maintained on stand-by, on a 24-hour basis, but were never fired, although the squadron test-fired the missiles at Eglin AFB, Florida on annual winter retreats.\n\nNo. 447 SAM Squadron operating out of RCAF Station La Macaza, Quebec was activated on 15 September 1962 although warheads were not delivered until late 1963. The squadron followed the same operational procedures as No. 446, its sister squadron. With the passage of time the operational capability of the 1950s-era Bomarc system no longer met modern requirements; the Department of National Defence deemed that the Bomarc missile defense was no longer a viable system, and ordered both squadrons to be stood down in 1972. The bunkers and ancillary facilities remain at both former sites.\n\n\n\n\nLocations under construction but not activated. Each site was programmed for 28 IM-99B missiles:\n\nBelow is a list of museums or sites which have a Bomarc missile on display:\n\n"}
{"id": "4132", "url": "https://en.wikipedia.org/wiki?curid=4132", "title": "Branco River", "text": "Branco River\n\nThe Branco River (; Engl: \"White River\") is the principal affluent of the Rio Negro from the north. \n\nThe river drains the Guayanan Highlands moist forests ecoregion.\nIt is enriched by many streams from the Tepui highlands which separate Venezuela and Guyana from Brazil. Its two upper main tributaries are the Uraricoera and the Takutu. The latter almost links its sources with those of the Essequibo.\n\nThe Branco flows nearly south, and finds its way into the Negro through several channels and a chain of lagoons similar to those of the latter river. It is long, up to its Uraricoera confluence. It has numerous islands, and, above its mouth, it is broken by a bad series of rapids.\n\nThe river is referred to as \"branco\" (white) because of inorganic sediments carried in the water. Alfred Russel Wallace mentions this peculiar coloration in \"On the Rio Negro,\" a paper read at the 13 June 1853 meeting of the Royal Geographical Society, in which he says: \"[The Rio Branco] is white to a remarkable degree, its waters being actually milky in appearance.\" Alexander von Humboldt attributed the color to the presence of silicates in the water, principally mica and talc. There is a visible contrast with the waters of the Rio Negro at the confluence of the two rivers, the Rio Negro's waters being darkened by suspended organic debris containing tannin and humic acid.\n\n"}
{"id": "4146", "url": "https://en.wikipedia.org/wiki?curid=4146", "title": "Bus", "text": "Bus\n\nA bus (archaically also omnibus, multibus, motorbus, autobus) is a road vehicle designed to carry many passengers. Buses can have a capacity as high as 300 passengers. The most common type of bus is the single-decker rigid bus, with larger loads carried by double-decker and articulated buses, and smaller loads carried by midibuses and minibuses; coaches are used for longer-distance services. Many types of buses, such as city transit buses and inter-city coaches, charge a fare. Other types, such as elementary or secondary school buses or shuttle buses within a post-secondary education campus do not charge a fare. In many jurisdictions, bus drivers require a special licence above and beyond a regular driver's licence.\n\nBuses may be used for scheduled bus transport, scheduled coach transport, school transport, private hire, or tourism; promotional buses may be used for political campaigns and others are privately operated for a wide range of purposes, including rock and pop band tour vehicles.\n\nHorse-drawn buses were used from the 1820s, followed by steam buses in the 1830s, and electric trolleybuses in 1882. The first internal combustion engine buses, or motor buses, were used in 1895. Recently, interest has been growing in hybrid electric buses, fuel cell buses, and electric buses, as well as ones powered by compressed natural gas or biodiesel. As of the 2010s, bus manufacturing is increasingly globalised, with the same designs appearing around the world.\n\nBus is a clipped form of the Latin word \"omnibus\". The first horse-drawn omnibus service was started by a businessman named Stanislas Baudry in the French city of Nantes in 1823. The first vehicles stopped in front of the shop of a hatter named Omnés, which had a large sign reading \"Omnes Omnibus\", a pun on the Latin-sounding name of that hatter; \"omnes\" means \"all\" and \"omnibus\" means (among other things) \"for all\" in Latin. Nantes citizens soon gave the nickname omnibus to the vehicle. The omnibus in Nantes was a success and Baudry moved to Paris and launched the first omnibus service there in April 1828. A similar service was introduced in London in 1829.\n\nRegular intercity bus services by steam-powered buses were pioneered in England in the 1830s by Walter Hancock and by associates of Sir Goldsworthy Gurney, among others, running reliable services over road conditions which were too hazardous for horse-drawn transportation.\n\nThe first mechanically propelled omnibus appeared on the streets of London on 22 April 1833. Steam carriages were much less likely to overturn, they travelled faster than horse-drawn carriages, they were much cheaper to run, and caused much less damage to the road surface due to their wide tyres.\n\nHowever, the heavy road tolls imposed by the turnpike trusts discouraged steam road vehicles and left the way clear for the horse bus companies, and from 1861 onwards, harsh legislation virtually eliminated mechanically propelled vehicles from the roads of Great Britain for 30 years, the Locomotive Act of that year imposing restrictive speed limits on \"road locomotives\" of 5 mph in towns and cities, and 10 mph in the country.\n\nIn parallel to the development of the bus was the invention of the electric trolleybus, typically fed through trolley poles by overhead wires. The Siemens brothers, William in England and Ernst Werner in Germany, collaborated on the development of the trolleybus concept. Sir William first proposed the idea in an article to the \"Journal of the Society of Arts\" in 1881 as an \"...arrangement by which an ordinary omnibus...would have a suspender thrown at intervals from one side of the street to the other, and two wires hanging from these suspenders; allowing contact rollers to run on these two wires, the current could be conveyed to the tram-car, and back again to the dynamo machine at the station, without the necessity of running upon rails at all.\"\n\nThe first such vehicle, the Electromote, was made by his brother Dr. Ernst Werner von Siemens and presented to the public in 1882 in Halensee, Germany. Although this experimental vehicle fulfilled all the technical criteria of a typical trolleybus, it was dismantled in the same year after the demonstration.\n\nMax Schiemann opened a passenger-carrying trolleybus in 1901 near Dresden, in Germany. Although this system operated only until 1904, Schiemann had developed what is now the standard trolleybus current collection system. In the early days, a few other methods of current collection were used. Leeds and Bradford became the first cities to put trolleybuses into service in Great Britain on 20 June 1911.\n\nIn Siegerland, Germany, two passenger bus lines ran briefly, but unprofitably, in 1895 using a six-passenger motor carriage developed from the 1893 Benz Viktoria. Another commercial bus line using the same model Benz omnibuses ran for a short time in 1898 in the rural area around Llandudno, Wales.\n\nDaimler also produced one of the earliest motor-bus models in 1898, selling a double-decker bus to the Motor Traction Company which was first used on the streets of London on 23 April 1898. The vehicle had a maximum speed of 18 kph and accommodated up to 20 passengers, in an enclosed area below and on an open-air platform above. With the success and popularity of this bus, Daimler expanded production, selling more buses to companies in London and, in 1899, to Stockholm and Speyer.Daimler also entered into a partnership with the British company Milnes and developed a new double-decker in 1902 that became the market standard.\nThe first mass-produced bus model was the B-type double-decker bus, designed by Frank Searle and operated by the London General Omnibus Company – it entered service in 1910, and almost 3,000 had been built by the end of the decade. Hundreds saw military service on the Western Front during the First World War.\n\nThe Yellow Coach Manufacturing Company, which rapidly became a major manufacturer of buses in the US, was founded in Chicago in 1923 by John D. Hertz. General Motors purchased a majority stake in 1925 and changed its name to the Yellow Truck and Coach Manufacturing Company. They then purchased the balance of the shares in 1943 to form the GM Truck and Coach Division.\n\nModels expanded in the 20th century, leading to the widespread introduction of the contemporary recognizable form of full-sized buses from the 1950s. The AEC Routemaster, developed in the 1950s, was a pioneering design and remains an icon of London to this day. The innovative design used lightweight aluminium and techniques developed in aircraft production during World War II. As well as a novel weight-saving integral design, it also introduced for the first time on a bus independent front suspension, power steering, a fully automatic gearbox, and power-hydraulic braking.\n\nFormats include single-decker bus, double-decker bus (both usually with a rigid chassis) and articulated bus (or 'bendy-bus') the prevalence of which varies from country to country. Bi-articulated buses are also manufactured, and passenger-carrying trailers—either towed behind a rigid bus (a bus trailer) or hauled as a trailer by a truck (a trailer bus). Smaller midibuses have a lower capacity and open-top buses are typically used for leisure purposes. In many new fleets, particularly in local transit systems, a shift to low-floor buses is occurring, primarily for easier accessibility. Coaches are designed for longer-distance travel and are typically fitted with individual high-backed reclining seats, seat belts, toilets, and audio-visual entertainment systems, and can operate at higher speeds with more capacity for luggage. Coaches may be single- or double-deckers, articulated, and often include a separate luggage compartment under the passenger floor. Guided buses are fitted with technology to allow them to run in designated guideways, allowing the controlled alignment at bus stops and less space taken up by guided lanes than conventional roads or bus lanes.\n\nBus manufacturing may be by a single company (an integral manufacturer), or by one manufacturer's building a bus body over a chassis produced by another manufacturer.\n\nTransit buses used to be mainly high-floor vehicles. However, they are now increasingly of low-floor design and optionally also 'kneel' air suspension and have electrically or hydraulically extended under-floor ramps to provide level access for wheelchair users and people with baby carriages. Prior to more general use of such technology, these wheelchair users could only use specialist paratransit mobility buses.\n\nAccessible vehicles also have wider entrances and interior gangways and space for wheelchairs. Interior fittings and destination displays may also be designed to be usable by the visually impaired. Coaches generally use wheelchair lifts instead of low-floor designs. In some countries, vehicles are required to have these features by disability discrimination laws.\n\nBuses were initially configured with an engine in the front and an entrance at the rear. With the transition to one-man operation, many manufacturers moved to mid- or rear-engined designs, with a single door at the front or multiple doors. The move to the low-floor design has all but eliminated the mid-engined design, although some coaches still have mid-mounted engines. Front-engined buses still persist for niche markets such as American school buses, some minibuses, and buses in less developed countries, which may be derived from truck chassis, rather than purpose-built bus designs. Most buses have two axles, articulated buses have three.\n\nGuided buses are fitted with technology to allow them to run in designated guideways, allowing the controlled alignment at bus stops and less space taken up by guided lanes than conventional roads or bus lanes. Guidance can be mechanical, optical, or electromagnetic. Extensions of the guided technology include the Guided Light Transit and Translohr systems, although these are more often termed 'rubber-tyred trams' as they have limited or no mobility away from their guideways.\n\nTransit buses are normally painted to identify the operator or a route, function, or to demarcate low-cost or premium service buses. Liveries may be painted onto the vehicle, applied using adhesive vinyl technologies, or using decals. Vehicles often also carry bus advertising or part or all of their visible surfaces (as mobile billboard). Campaign buses may be decorated with key campaign messages; these can be to promote an event or initiative.\n\nThe most common power source since the 1920s has been the diesel engine. Early buses, known as trolleybuses, were powered by electricity supplied from overhead lines. Nowadays, electric buses often carry their own battery, which is sometimes recharged on stops/stations to keep the size of the battery small/lightweight. Currently, interest exists in hybrid electric buses, fuel cell buses, electric buses, and ones powered by compressed natural gas or biodiesel. Gyrobuses, which are powered by the momentum stored by a flywheel, were tried in the 1940s.\n\nUnited Kingdom:\n\nMaximum Length: Single rear axle 13.5 meters. Twin rear axle 15 meters.<br>\nMaximum Width: 2.55 meters\n\nUnited States:\n\nMaximum Length: None <br>\nMaximum Width: 2.6 meters\n\nEarly bus manufacturing grew out of carriage coachbuilding, and later out of automobile or truck manufacturers. Early buses were merely a bus body fitted to a truck chassis. This body+chassis approach has continued with modern specialist manufacturers, although there also exist integral designs such as the Leyland National where the two are practically inseparable. Specialist builders also exist and concentrate on building buses for special uses or modifying standard buses into specialised products.\n\nIntegral designs have the advantages that they are have been well-tested for strength and stability, and also are off-the-shelf. However, two incentives cause use of the chassis+body model. First, it allows the buyer and manufacturer both to shop for the best deal for their needs, rather than having to settle on one fixed design—the buyer can choose the body and the chassis separately. Second, over the lifetime of a vehicle (in constant service and heavy traffic), it will likely get minor damage now and again, and being able easily to replace a body panel or window etc. can vastly increase its service life and save the cost and inconvenience of removing it from service.\n\nAs with the rest of the automotive industry, into the 20th century, bus manufacturing increasingly became globalized, with manufacturers producing buses far from their intended market to exploit labour and material cost advantages. As with the cars, new models are often exhibited by manufacturers at prestigious industry shows to gain new orders. A typical city bus costs almost US$450,000.\n\nTransit buses, used on public transport bus services, have utilitarian fittings designed for efficient movement of large numbers of people, and often have multiple doors. Coaches are used for longer-distance routes. High-capacity bus rapid transit services may use the bi-articulated bus or tram-style buses such as the Wright StreetCar and the Irisbus Civis.\n\nBuses and coach services often operate to a predetermined published public transport timetable defining the route and the timing, but smaller vehicles may be used on more flexible demand responsive transport services.\n\nBuses play a major part in the tourism industry. Tour buses around the world allow tourists to view local attractions or scenery. These are often open-top buses, but can also be by regular bus or coach.\n\nIn local sightseeing, City Sightseeing is the largest operator of local tour buses, operating on a franchised basis all over the world. Specialist tour buses are also often owned and operated by safari parks and other theme parks or resorts. Longer-distance tours are also carried out by bus, either on a turn up and go basis or through a tour operator, and usually allow disembarkation from the bus to allow touring of sites of interest on foot. These may be day trips or longer excursions incorporating hotel stays. Tour buses often carry a tour guide, although the driver or a recorded audio commentary may also perform this function. The tour operator may itself be a subsidiary of a company that operates buses and coaches for other uses or an independent company that charters buses or coaches. Commuter transport operators may also use their coaches to conduct tours within the target city between the morning and evening commuter transport journey.\n\nBuses and coaches are also a common component of the wider package holiday industry, providing private airport transfers (in addition to general airport buses) and organised tours and day trips for holidaymakers on the package.\n\nTour buses can also be hired as chartered buses by groups as part of sightseeing at popular holiday destinations. These private tour buses may offer specific stops like all the historical sights or specifically casinos or allow the customers the comfort to make their own itineraries as per the places and activities they want to cover while on their tour. Tour buses come with professional and informed staff, insurance and maintain state governed safety standards. Not only this to make the experience for the tourists more comfortable provide facilities like entertainment units, luxurious reclining seats, large scenic windows, and even lavatories if needed.\n\nPublic long-distance coach networks are also often used as a low-cost method of travel by students or young people travelling the world. Some companies such as Topdeck Travel were set up to specifically use buses to drive the hippie trail or travel to places such as North Africa.\n\nIn many tourist or travel destinations, a bus is part of the tourist attraction, such as the North American tourist trolleys, London's AEC Routemaster heritage routes, or the customised buses of Malta, Asia, and the Americas.\n\nIn some countries, particularly the USA and Canada, buses used to transport school children have evolved into a specific design with specified mandatory features. American states have also adopted laws regarding motorist conduct around school buses, including serious fines and the possibility of prison time for passing a stopped school bus in the process of offloading children passengers. These school buses feature things such as the school bus yellow livery and crossing guards. Other countries may mandate the use of seat belts. As a minimum, many countries require a bus carrying students to display a , and may also adopt yellow liveries. Student transport often uses older buses cascaded from service use, retrofitted with more seats and/or seatbelts. Student transport may be operated by local authorities or private contractors. Schools may also own and operate their own buses for other transport needs, such as class field trips, or transport to associated sports, music, or other school events.\n\nDue to the costs involved in owning, operating, and driving buses and coaches, many bus and coach use a private hire of vehicles from charter bus companies, either for a day or two or a longer contract basis, where the charter company provides the vehicles and qualified drivers.\nCharter bus operators may be completely independent businesses, or charter hire may be a subsidiary business of a public transport operator that might maintain a separate fleet or use surplus buses, coaches, and dual-purpose coach-seated buses. Many private taxicab companies also operate larger minibus vehicles to cater for group fares. Companies, private groups, and social clubs may hire buses or coaches as a cost-effective method of transporting a group to an event or site, such as a group meeting, racing event, or organised recreational activity such as a summer camp. Schools often hire charter bus services on regular basis for transportation of children to and from their homes. Chartered buses are also used by education institutes for transport to conventions, exhibitions, and field trips. Entertainment or event companies may also hire temporary shuttles buses for transport at events such as festivals or conferences. Party buses are used by companies in a similar manner to limousine hire, for luxury private transport to social events or as a touring experience. Sleeper buses are used by bands or other organisations that tour between entertainment venues and require mobile rest and recreation facilities. Some couples hire preserved buses for their wedding transport, instead of the traditional car. Buses are often hired for parades or processions. Victory parades are often held for triumphant sports teams, who often tour their home town or city in an open-top bus. Sports teams may also contract out their transport to a team bus, for travel to away games, to a competition or to a final event. These buses are often specially decorated in a livery matching the team colours. Private companies often contract out private shuttle bus services, for transport of their customers or patrons, such as hotels, amusement parks, university campuses, or private airport transfer services. This shuttle usage can be as transport between locations, or to and from parking lots. High specification luxury coaches are often chartered by companies for executive or VIP transport. Charter buses may also be used in tourism and for promotion (See Tourism and Promotion sections).\n\nMany organisations, including the police, not for profit, social or charitable groups with a regular need for group transport may find it practical or cost-effective to own and operate a bus for their own needs. These are often minibuses for practical, tax and driver licensing reasons, although they can also be full-size buses. Cadet or scout groups or other youth organizations may also own buses. Specific charities may exist to fund and operate bus transport, usually using specially modified mobility buses or otherwise accessible buses (See Accessibility section). Some use their contributions to buy vehicles and provide volunteer drivers.\n\nAirport operators make use of special airside airport buses for crew and passenger transport in the secure airside parts of an airport. Some public authorities, police forces, and military forces make use of armoured buses where there is a special need to provide increased passenger protection. The United States Secret Service acquired two in 2010 for transporting dignitaries needing special protection. Police departments make use of police buses for a variety of reasons, such as prisoner transport, officer transport, temporary detention facilities, and as command and control vehicles. Some fire departments also use a converted bus as a command post while those in cold climates might retain a bus as a heated shelter at fire scenes. Many are drawn from retired school or service buses.\n\nBuses are often used for advertising, political campaigning, , public relations, or promotional purposes. These may take the form of temporary charter hire of service buses, or the temporary or permanent conversion and operation of buses, usually of second-hand buses. Extreme examples include converting the bus with displays and decorations or awnings and fittings. Interiors may be fitted out for exhibition or information purposes with special equipment and/or audio visual devices.\n\nBus advertising takes many forms, often as interior and exterior adverts and all-over advertising liveries. The practice often extends into the exclusive private hire and use of a bus to promote a brand or product, appearing at large public events, or touring busy streets. The bus is sometimes staffed by promotions personnel, giving out free gifts. Campaign buses are often specially decorated for a political campaign or other social awareness information campaign, designed to bring a specific message to different areas, and/or used to transport campaign personnel to local areas/meetings. Exhibition buses are often sent to public events such as fairs and festivals for purposes such as recruitment campaigns, for example by private companies or the armed forces. Complex urban planning proposals may be organised into a mobile exhibition bus for the purposes of public consultation.\n\nIn some sparesly populated areas, it's common to use Brucks, buses with a cargo area to transport both passengers and cargo at the same time. They are especially common in the Nordic countries.\n\nHistorically, the types and features of buses have developed according to local needs. Buses were fitted with technology appropriate to the local climate or passenger needs, such as air conditioning in Asia, or cycle mounts on North American buses. The bus types in use around the world where there was little mass production were often sourced second hand from other countries, such as the Malta bus, and buses in use in Africa. Other countries such as Cuba required novel solutions to import restrictions, with the creation of the \"camellos\" (camel bus), a specially manufactured trailer bus.\n\nAfter the Second World War, manufacturers in Europe and the Far East, such as Mercedes-Benz buses and Mitsubishi Fuso expanded into other continents influencing the use of buses previously served by local types. Use of buses around the world has also been influenced by colonial associations or political alliances between countries. Several of the Commonwealth nations followed the British lead and sourced buses from British manufacturers, leading to a prevalence of double-decker buses. Several Eastern Bloc countries adopted trolleybus systems, and their manufacturers such as Trolza exported trolleybuses to other friendly states. In the 1930s, Italy designed the world's only triple decker bus for the busy route between Rome and Tivoli that could carry eighty-eight passengers. It was unique not only in being a triple decker but having a separate smoking compartment on the third level.\n\nThe buses to be found in countries around the world often reflect the quality of the local road network, with high floor resilient truck-based designs prevalent in several less developed countries where buses are subject to tough operating conditions. Population density also has a major impact, where dense urbanisation such as in Japan and the far east has led to the adoption of high capacity long multi-axle buses, often double-deckers while South America and China are implementing large numbers of articulated buses for bus rapid transit schemes.\n\nEuro Bus Expo is a trade show, which is held bi-ennially at the UK's National Exhibition Centre in Birmingham. As the official show of the Confederation of Passenger Transport, the UK's trade association for the bus, coach and light rail industry, the three-day event offers visitors from Europe and beyond the chance to see and experience, at first hand, the very latest vehicles and product and service innovations right across the industry. The next show will be held in November 2016.\n\nBusworld Kortrijk in Kortrijk, Belgium, is the leading bus trade fair in Europe. It is held bi-ennially, last time October 2013 and next time October 2015.\n\nMost public or private buses and coaches, once they have reached the end of their service with one or more operators, are sent to the wrecking yard for breaking up for scrap and spare parts. Some buses, while not economical to keep running as service buses, are often converted in some way for use by the operator, or another user, for purposes other than revenue-earning transport. Much like old cars and trucks, buses often pass through a dealership where they can be bought for a price or at auction.\n\nBus operators will often find it economical to convert retired buses to use as permanent training buses for driver training, rather than taking a regular service bus out of use. Some large operators also converted retired buses into tow bus vehicles, to act as tow trucks. With the outsourcing of maintenance staff and facilities, the increase in company health and safety regulations, and the increasing curb weights of buses, many operators now contract their towing needs to a professional vehicle recovery company.\n\nSome retired buses have been converted to static or mobile cafés, often using historic buses as a tourist attraction. Food is also provided from a catering bus, in which a bus is converted into a mobile canteen and break room. These are commonly seen at external filming locations to feed the cast and crew, and at other large events to feed staff. Another use is as an emergency vehicle, such as high-capacity ambulance bus or mobile command center.\n\nSome organisations adapt and operate playbuses or learning buses to provide a playground or learning environments to children who might not have access to proper play areas. An ex-London AEC Routemaster bus has been converted to a mobile theatre and catwalk fashion show.\n\nSome buses meet a destructive end by being entered in banger races or at demolition derbys. A larger number of old retired buses have also been converted into mobile holiday homes and campers.\n\nRather than being scrapped or converted for other uses, sometimes retired buses are saved for preservation. This can be done by individuals, volunteer preservation groups or charitable trusts, museums, or sometimes by the operators themselves as part of a heritage fleet. These buses often need to undergo a degree of vehicle restoration to restore them to their original condition and will have their livery and other details such as internal notices and rollsigns restored to be authentic to a specific time in the bus's actual history. Some buses that undergo preservation are rescued from a state of great disrepair, but others enter preservation with very little wrong with them. As with other historic vehicles, many preserved buses either in a working or static state form part of the collections of transport museums. Working buses will often be exhibited at rallies and events, and they are also used as charter buses. While many preserved buses are quite old or even vintage, in some cases, relatively new examples of a bus type can enter restoration. In-service examples are still in use by other operators. This often happens when a change in design or operating practice, such as the switch to one person operation or low floor technology, renders some buses redundant while still relatively new.\n\n\n"}
{"id": "4147", "url": "https://en.wikipedia.org/wiki?curid=4147", "title": "Bali", "text": "Bali\n\nBali (Balinese: , Indonesian: \"Pulau Bali\", \"Provinsi Bali\") is an island and province of Indonesia. The province includes the island of Bali and a few smaller neighbouring islands, notably Nusa Penida, Nusa Lembongan, and Nusa Ceningan. It is located at the westernmost end of the Lesser Sunda Islands, between Java to the west and Lombok to the east. Its capital, Denpasar, is located in the southern part of the island. \n\nWith a population of 3,890,757 in the 2010 census, and 4,225,000 as of January 2014, the island is home to most of Indonesia's Hindu minority. According to the 2010 Census, 83.5% of Bali's population adhered to Balinese Hinduism, followed by 13.4% Muslim, Christianity at 2.5%, and Buddhism 0.5%.\n\nBali is a popular tourist destination, which has seen a significant rise in tourists since the 1980s. Tourism-related business makes up 80% of its economy. It is renowned for its highly developed arts, including traditional and modern dance, sculpture, painting, leather, metalworking, and music. The Indonesian International Film Festival is held every year in Bali. In March 2017, TripAdvisor named the island the world's top destination in its Traveler's choice award.\n\nBali is part of the Coral Triangle, the area with the highest biodiversity of marine species. In this area alone over 500 reef building coral species can be found. For comparison, this is about 7 times as many as in the entire Caribbean. Most recently, Bali was the host of the 2011 ASEAN Summit, 2013 APEC and Miss World 2013. Bali is the home of the Subak Irrigation System, a UNESCO World Heritage Site. It is also home to a unified confederation of kingdoms, composed of 10 traditional royal Balinese houses, where each house rules a specific geographic area. the confederation is the successor of the Bali Kingdom. The royal houses are not recognized by the government of Indonesia, however, they have been operational since their establishment prior to Dutch colonization.\n\nBali was inhabited around 2000 BC by Austronesian people who migrated originally from Southeast Asia and Oceania through Maritime Southeast Asia. Culturally and linguistically, the Balinese are closely related to the people of the Indonesian archipelago, Malaysia, the Philippines and Oceania. Stone tools dating from this time have been found near the village of Cekik in the island's west.\n\nIn ancient Bali, nine Hindu sects existed, namely Pasupata, Bhairawa, Siwa Shidanta, Waisnawa, Bodha, Brahma, Resi, Sora and Ganapatya. Each sect revered a specific deity as its personal Godhead.\n\nInscriptions from 896 and 911 don't mention a king, until 914, when Sri Kesarivarma is mentioned. They also reveal an independent Bali, with a distinct dialect, where Buddhism and Sivaism were practiced simultaneously. Mpu Sindok's great-granddaughter, Mahendradatta (Gunapriyadharmapatni), married the Bali king Udayana Warmadewa (Dharmodayanavarmadeva) around 989, giving birth to Airlangga around 1001. This marriage also brought more Hinduism and Javanese culture to Bali. Princess Sakalendukirana appeared in 1098. Suradhipa reigned from 1115 to 1119, and Jayasakti from 1146 until 1150. Jayapangus appears on inscriptions between 1178 and 1181, while Adikuntiketana and his son Paramesvara in 1204.\n\nBalinese culture was strongly influenced by Indian, Chinese, and particularly Hindu culture, beginning around the 1st century AD. The name \"Bali dwipa\" (\"Bali island\") has been discovered from various inscriptions, including the Blanjong pillar inscription written by Sri Kesari Warmadewa in 914 AD and mentioning \"Walidwipa\". It was during this time that the people developed their complex irrigation system \"subak\" to grow rice in wet-field cultivation. Some religious and cultural traditions still practiced today can be traced to this period.\n\nThe Hindu Majapahit Empire (1293–1520 AD) on eastern Java founded a Balinese colony in 1343. The uncle of Hayam Wuruk is mentioned in the charters of 1384–86. A mass Javanese immigration to Bali occurred in the next century when the Majapahit Empire fell in 1520. Bali's government then became an independent collection of Hindu kingdoms which led to a Balinese national identity and major enhancements in culture, arts, and economy. The nation with various kingdoms became independent for up to 386 years until 1906, when the Dutch subjugated and repulsed the natives for economic control and took it over.\n\nThe first known European contact with Bali is thought to have been made in 1512, when a Portuguese expedition led by Antonio Abreu and Francisco Serrão sighted its northern shores. It was the first expedition of a series of bi-annual fleets to the Moluccas, that throughout the 16th century usually traveled along the coasts of the Sunda Islands. Bali was also mapped in 1512, in the chart of Francisco Rodrigues, aboard the expedition. In 1585, a ship foundered off the Bukit Peninsula and left a few Portuguese in the service of Dewa Agung.\n\nIn 1597 the Dutch explorer Cornelis de Houtman arrived at Bali, and the Dutch East India Company was established in 1602. The Dutch government expanded its control across the Indonesian archipelago during the second half of the 19th century (see Dutch East Indies). Dutch political and economic control over Bali began in the 1840s on the island's north coast, when the Dutch pitted various competing Balinese realms against each other. In the late 1890s, struggles between Balinese kingdoms in the island's south were exploited by the Dutch to increase their control.\n\nIn June 1860 the famous Welsh naturalist, Alfred Russel Wallace, travelled to Bali from Singapore, landing at Buleleng on the north coast of the island. Wallace's trip to Bali was instrumental in helping him devise his Wallace Line theory. The Wallace Line is a faunal boundary that runs through the strait between Bali and Lombok. It has been found to be a boundary between species. In his travel memoir \"The Malay Archipelago,\" Wallace wrote of his experience in Bali, of which has strong mention of the unique Balinese irrigation methods:\n\nI was both astonished and delighted; for as my visit to Java was some years later, I had never beheld so beautiful and well-cultivated a district out of Europe. A slightly undulating plain extends from the seacoast about inland, where it is bounded by a fine range of wooded and cultivated hills. Houses and villages, marked out by dense clumps of coconut palms, tamarind and other fruit trees, are dotted about in every direction; while between them extend luxurious rice-grounds, watered by an elaborate system of irrigation that would be the pride of the best cultivated parts of Europe. \n\nThe Dutch mounted large naval and ground assaults at the Sanur region in 1906 and were met by the thousands of members of the royal family and their followers who rather than yield to the superior Dutch force committed ritual suicide (\"puputan\") to avoid the humiliation of surrender. Despite Dutch demands for surrender, an estimated 200 Balinese killed themselves rather than surrender. In the Dutch intervention in Bali, a similar mass suicide occurred in the face of a Dutch assault in Klungkung. Afterward the Dutch governors exercised administrative control over the island, but local control over religion and culture generally remained intact. Dutch rule over Bali came later and was never as well established as in other parts of Indonesia such as Java and Maluku.\n\nIn the 1930s, anthropologists Margaret Mead and Gregory Bateson, artists Miguel Covarrubias and Walter Spies, and musicologist Colin McPhee all spent time here. Their accounts of the island and its peoples created a western image of Bali as \"an enchanted land of aesthetes at peace with themselves and nature.\" Western tourists began to visit the island. The sensuous image of Bali was enhanced in the west by a quasi-pornographic 1932 documentary \"Virgins of Bali\" about a day in the lives of two teenage Balinese girls whom the film's narrator Deane Dickason notes in the first scene \"bathe their shamelessly nude bronze bodies\". Under the looser version of the Haynes code that existed up to 1934, nudity involving \"civilized\" (i.e. white) women was banned, but permitted with \"uncivilized\" (i.e. all non-white women), a loophole that was exploited by the producers of \"Virgins of Bali\". The film, which mostly consisted of scenes of topless Balinese women was a great success in 1932, and almost single-handily made Bali into a popular spot for tourists.\n\nImperial Japan occupied Bali during World War II. It was not originally a target in their Netherlands East Indies Campaign, but as the airfields on Borneo were inoperative due to heavy rains, the Imperial Japanese Army decided to occupy Bali, which did not suffer from comparable weather. The island had no regular Royal Netherlands East Indies Army (KNIL) troops. There was only a Native Auxiliary Corps \"Prajoda\" (Korps Prajoda) consisting of about 600 native soldiers and several Dutch KNIL officers under the command of KNIL Lieutenant Colonel W.P. Roodenburg. On 19 February 1942 the Japanese forces landed near the town of Senoer [Senur]. The island was quickly captured.\n\nDuring the Japanese occupation, a Balinese military officer, Gusti Ngurah Rai, formed a Balinese 'freedom army'. The harshness of Japanese occupation forces made them more resented than the Dutch colonial rulers.\n\nIn 1946, the Dutch constituted Bali as one of the 13 administrative districts of the newly proclaimed State of East Indonesia, a rival state to the Republic of Indonesia, which was proclaimed and headed by Sukarno and Hatta. Bali was included in the \"Republic of the United States of Indonesia\" when the Netherlands recognised Indonesian independence on 29 December 1949. The first governor of Bali, Anak Agung Bagus Suteja, was appointed by President Sukarno in 1958, when Bali became a province.\n\nThe 1963 eruption of Mount Agung killed thousands, created economic havoc and forced many displaced Balinese to be \"transmigrated\" to other parts of Indonesia. Mirroring the widening of social divisions across Indonesia in the 1950s and early 1960s, Bali saw conflict between supporters of the traditional caste system, and those rejecting this system. Politically, the opposition was represented by supporters of the Indonesian Communist Party (PKI) and the Indonesian Nationalist Party (PNI), with tensions and ill-feeling further increased by the PKI's land reform programs. An attempted coup in Jakarta was put down by forces led by General Suharto.\n\nThe army became the dominant power as it instigated a violent anti-communist purge, in which the army blamed the PKI for the coup. Most estimates suggest that at least 500,000 people were killed across Indonesia, with an estimated 80,000 killed in Bali, equivalent to 5% of the island's population. With no Islamic forces involved as in Java and Sumatra, upper-caste PNI landlords led the extermination of PKI members.\n\nAs a result of the 1965-66 upheavals, Suharto was able to manoeuvre Sukarno out of the presidency. His \"New Order\" government reestablished relations with western countries. The pre-War Bali as \"paradise\" was revived in a modern form. The resulting large growth in tourism has led to a dramatic increase in Balinese standards of living and significant foreign exchange earned for the country. A bombing in 2002 by militant Islamists in the tourist area of Kuta killed 202 people, mostly foreigners. This attack, and another in 2005, severely reduced tourism, producing much economic hardship to the island.\n\nThe island of Bali lies 3.2 km (2 mi) east of Java, and is approximately 8 degrees south of the equator. Bali and Java are separated by the Bali Strait. East to west, the island is approximately 153 km (95 mi) wide and spans approximately 112 km (69 mi) north to south; administratively it covers 5,780 km, or 5,577 km without Nusa Penida District, its population density is roughly 750 people/km.\n\nBali's central mountains include several peaks over in elevation. The highest is Mount Agung (), known as the \"mother mountain\" which is an active volcano rated as one of the world's most likely sites for a massive eruption within the next 100 years. Mountains range from centre to the eastern side, with Mount Agung the easternmost peak. Bali's volcanic nature has contributed to its exceptional fertility and its tall mountain ranges provide the high rainfall that supports the highly productive agriculture sector. South of the mountains is a broad, steadily descending area where most of Bali's large rice crop is grown. The northern side of the mountains slopes more steeply to the sea and is the main coffee producing area of the island, along with rice, vegetables and cattle. The longest river, Ayung River, flows approximately 75 km.\n\nThe island is surrounded by coral reefs. Beaches in the south tend to have white sand while those in the north and west have black sand. Bali has no major waterways, although the Ho River is navigable by small \"sampan\" boats. Black sand beaches between Pasut and Klatingdukuh are being developed for tourism, but apart from the seaside temple of Tanah Lot, they are not yet used for significant tourism.\n\nThe largest city is the provincial capital, Denpasar, near the southern coast. Its population is around 491,500 (2002). Bali's second-largest city is the old colonial capital, Singaraja, which is located on the north coast and is home to around 100,000 people. Other important cities include the beach resort, Kuta, which is practically part of Denpasar's urban area, and Ubud, situated at the north of Denpasar, is the island's cultural centre.\n\nThree small islands lie to the immediate south east and all are administratively part of the Klungkung regency of Bali: Nusa Penida, Nusa Lembongan and Nusa Ceningan. These islands are separated from Bali by the Badung Strait.\n\nTo the east, the Lombok Strait separates Bali from Lombok and marks the biogeographical division between the fauna of the Indomalayan ecozone and the distinctly different fauna of Australasia. The transition is known as the Wallace Line, named after Alfred Russel Wallace, who first proposed a transition zone between these two major biomes. When sea levels dropped during the Pleistocene ice age, Bali was connected to Java and Sumatra and to the mainland of Asia and shared the Asian fauna, but the deep water of the Lombok Strait continued to keep Lombok Island and the Lesser Sunda archipelago isolated.\n\nBeing just 8 degrees south of the equator, Bali has a fairly even climate year round. Average year-round temperature stands at around 30 °C with a humidity level of about 85%.\n\nDay time temperatures at low elevations vary between 20–33⁰ C (68–91⁰ F), although it can be much cooler than that in the mountains. The west monsoon is in place from approximately October to April, and this can bring significant rain, particularly from December to March. Outside of the monsoon period, humidity is relatively low and any rain is unlikely in lowland areas.\n\nThe high season in Bali is during the \"dry season\" in July and August, as well as during the Easter and Christmas holidays, when the weather is very unpredictable.\n\nBali lies just to the west of the Wallace Line, and thus has a fauna that is Asian in character, with very little Australasian influence, and has more in common with Java than with Lombok. An exception is the yellow-crested cockatoo, a member of a primarily Australasian family. There are around 280 species of birds, including the critically endangered Bali myna, which is endemic. Others include barn swallow, black-naped oriole, black racket-tailed treepie, crested serpent-eagle, crested treeswift, dollarbird, Java sparrow, lesser adjutant, long-tailed shrike, milky stork, Pacific swallow, red-rumped swallow, sacred kingfisher, sea eagle, woodswallow, savanna nightjar, stork-billed kingfisher, yellow-vented bulbul and great egret.\n\nUntil the early 20th century, Bali was home to several large mammals: the wild banteng, leopard and the endemic Bali tiger. The banteng still occurs in its domestic form, whereas leopards are found only in neighbouring Java, and the Bali tiger is extinct. The last definite record of a tiger on Bali dates from 1937, when one was shot, though the subspecies may have survived until the 1940s or 1950s. \nSquirrels are quite commonly encountered, less often is the Asian palm civet, which is also kept in coffee farms to produce Kopi Luwak. Bats are well represented, perhaps the most famous place to encounter them remaining is the Goa Lawah (Temple of the Bats) where they are worshipped by the locals and also constitute a tourist attraction. They also occur in other cave temples, for instance at Gangga Beach. Two species of monkey occur. The crab-eating macaque, known locally as \"kera\", is quite common around human settlements and temples, where it becomes accustomed to being fed by humans, particularly in any of the three \"monkey forest\" temples, such as the popular one in the Ubud area. They are also quite often kept as pets by locals. The second monkey, endemic to Java and some surrounding islands such as Bali, is far rarer and more elusive and is the Javan langur, locally known as \"lutung\". They occur in few places apart from the Bali Barat National Park. They are born an orange colour, though by their first year they would have already changed to a more blackish colouration. In Java however, there is more of a tendency for this species to retain its juvenile orange colour into adulthood, and so you can see a mixture of black and orange monkeys together as a family. Other rarer mammals include the leopard cat, Sunda pangolin and black giant squirrel.\n\nSnakes include the king cobra and reticulated python. The water monitor can grow to at least in length and and can move quickly.\n\nThe rich coral reefs around the coast, particularly around popular diving spots such as Tulamben, Amed, Menjangan or neighbouring Nusa Penida, host a wide range of marine life, for instance hawksbill turtle, giant sunfish, giant manta ray, giant moray eel, bumphead parrotfish, hammerhead shark, reef shark, barracuda, and sea snakes. Dolphins are commonly encountered on the north coast near Singaraja and Lovina.\n\nA team of scientists conducted a survey from 29 April 2011 to 11 May 2011 at 33 sea sites around Bali. They discovered 952 species of reef fish of which 8 were new discoveries at Pemuteran, Gilimanuk, Nusa Dua, Tulamben and Candidasa, and 393 coral species, including two new ones at Padangbai and between Padangbai and Amed.\nThe average coverage level of healthy coral was 36% (better than in Raja Ampat and Halmahera by 29% or in Fakfak and Kaimana by 25%) with the highest coverage found in Gili Selang and Gili Mimpang in Candidasa, Karangasem regency.\n\nAmong the larger trees the most common are: banyan trees, jackfruit, coconuts, bamboo species, acacia trees and also endless rows of coconuts and banana species. Numerous flowers can be seen: hibiscus, frangipani, bougainvillea, poinsettia, oleander, jasmine, water lily, lotus, roses, begonias, orchids and hydrangeas exist. On higher grounds that receive more moisture, for instance around Kintamani, certain species of fern trees, mushrooms and even pine trees thrive well. Rice comes in many varieties. Other plants with agricultural value include: salak, mangosteen, corn, kintamani orange, coffee and water spinach.\n\nSome of the worst erosion has occurred in Lebih Beach, where up to of land is lost every year. Decades ago, this beach was used for holy pilgrimages with more than 10,000 people, but they have now moved to Masceti Beach.\n\nFrom ranked third in previous review, in 2010 Bali got score 99.65 of Indonesia's environmental quality index and the highest of all the 33 provinces. The score measured 3 water quality parameters: the level of total suspended solids (TSS), dissolved oxygen (DO) and chemical oxygen demand (COD).\n\nBecause of over-exploitation by the tourist industry which covers a massive land area, 200 out of 400 rivers on the island have dried up and based on research, the southern part of Bali would face a water shortage up to 2,500 litres of clean water per second by 2015.\nTo ease the shortage, the central government plans to build a water catchment and processing facility at Petanu River in Gianyar. The 300 litres capacity of water per second will be channelled to Denpasar, Badung and Gianyar in 2013.\n\nThe province is divided into eight regencies (\"kabupaten\") and one city (\"kota\"). These are:\n\nThree decades ago, the Balinese economy was largely agriculture-based in terms of both output and employment. Tourism is now the largest single industry in terms of income, and as a result, Bali is one of Indonesia's wealthiest regions. In 2003, around 80% of Bali's economy was tourism related. By end of June 2011, non-performing loan of all banks in Bali were 2.23%, lower than the average of Indonesian banking industry non-performing loan (about 5%). The economy, however, suffered significantly as a result of the terrorist bombings 2002 and 2005. The tourism industry has since recovered from these events.\n\nAlthough tourism produces the GDP's largest output, agriculture is still the island's biggest employer; most notably rice cultivation. Crops grown in smaller amounts include fruit, vegetables, \"Coffea arabica\" and other cash and subsistence crops. Fishing also provides a significant number of jobs. Bali is also famous for its artisans who produce a vast array of handicrafts, including batik and ikat cloth and clothing, wooden carvings, stone carvings, painted art and silverware. Notably, individual villages typically adopt a single product, such as wind chimes or wooden furniture.\n\nThe Arabica coffee production region is the highland region of Kintamani near Mount Batur. Generally, Balinese coffee is processed using the wet method. This results in a sweet, soft coffee with good consistency. Typical flavours include lemon and other citrus notes. Many coffee farmers in Kintamani are members of a traditional farming system called Subak Abian, which is based on the Hindu philosophy of \"Tri Hita Karana\". According to this philosophy, the three causes of happiness are good relations with God, other people, and the environment. The Subak Abian system is ideally suited to the production of fair trade and organic coffee production. Arabica coffee from Kintamani is the first product in Indonesia to request a geographical indication.\n\nIn 1963 the Bali Beach Hotel in Sanur was built by Sukarno, and boosted tourism in Bali. Prior to it, only three hotels existed on the island. Construction of hotels and restaurants began to spread throughout Bali. Tourism further increased on Bali after the Ngurah Rai International Airport opened in 1970. The Buleleng regency government encouraged the tourism sector as one of the mainstays for economic progress and social welfare.\n\nThe tourism industry is primarily focused in the south, while significant in the other parts of the island as well. The main tourist locations are the town of Kuta (with its beach), and its outer suburbs of Legian and Seminyak (which were once independent townships), the east coast town of Sanur (once the only tourist hub), Ubud towards the center of the island, to the south of the Ngurah Rai International Airport, Jimbaran, and the newer developments of Nusa Dua and Pecatu.\n\nThe United States government lifted its travel warnings in 2008. The Australian government issued an advisory on Friday, 4 May 2012, with the overall level of this advisory lowered to 'Exercise a high degree of caution'. The Swedish government issued a new warning on Sunday, 10 June 2012 because of one tourist who died from methanol poisoning. Australia last issued an advisory on Monday, 5 January 2015 due to new terrorist threats.\n\nAn offshoot of tourism is the growing real estate industry. Bali's real estate has been rapidly developing in the main tourist areas of Kuta, Legian, Seminyak and Oberoi. Most recently, high-end 5-star projects are under development on the Bukit peninsula, on the south side of the island. Million dollar villas are being developed along the cliff sides of south Bali, with commanding panoramic ocean views. Foreign and domestic (many Jakarta individuals and companies are fairly active) investment into other areas of the island also continues to grow. Land prices, despite the worldwide economic crisis, have remained stable.\n\nIn the last half of 2008, Indonesia's currency had dropped approximately 30% against the US dollar, providing many overseas visitors value for their currencies. Visitor arrivals for 2009 were forecast to drop 8% (which would be higher than 2007 levels), mainly due to the worldwide economic crisis which has also affected the global tourist industry.\n\nBali's tourism economy survived the terrorist bombings of 2002 and 2005, and the tourism industry has in fact slowly recovered and surpassed its pre-terrorist bombing levels; the longterm trend has been a steady increase of visitor arrivals. In 2010, Bali received 2.57 million foreign tourists, which surpassed the target of 2.0–2.3 million tourists. The average occupancy of starred hotels achieved 65%, so the island still should be able to accommodate tourists for some years without any addition of new rooms/hotels, although at the peak season some of them are fully booked.\n\nBali received the Best Island award from Travel and Leisure in 2010. Bali won because of its attractive surroundings (both mountain and coastal areas), diverse tourist attractions, excellent international and local restaurants, and the friendliness of the local people. According to BBC Travel released in 2011, Bali is one of the World's Best Islands, ranking second after Santorini, Greece.\n\nIn August 2010, the film \"Eat Pray Love\" was released in theatres. The movie was based on Elizabeth Gilbert's best-selling memoir \"Eat, Pray, Love\". It took place at Ubud and Padang-Padang Beach at Bali. The 2006 book, which spent 57 weeks at the No. 1 spot on the \"New York Times\" paperback nonfiction best-seller list, had already fuelled a boom in \"Eat, Pray, Love\"-related tourism in Ubud, the hill town and cultural and tourist center that was the focus of Gilbert's quest for balance through traditional spirituality and healing that leads to love.\n\nIn January 2016, after music icon David Bowie died, it was revealed that in his will, Bowie asked for his ashes to be scattered in Bali, conforming to Buddhist rituals. He had visited and performed in a number of Southeast Asian cities early in his career, including Bangkok and Singapore.\n\nSince 2011, China has displaced Japan as the second-largest supplier of tourists to Bali, while Australia still tops the list. Chinese tourists increased by 17% from last year due to the impact of ACFTA and new direct flights to Bali.\nIn January 2012, Chinese tourists year on year (yoy) increased by 222.18% compared to January 2011, while Japanese tourists declined by 23.54% yoy.\n\nBali reported that it welcomed 2.88 million foreign tourists and 5 million domestic tourists in 2012, marginally surpassing the expectations of 2.8 million foreign tourists.\n\nBased on a Bank Indonesia survey in May 2013, 34.39 percent of tourists are upper-middle class, spending between $1,286 to $5,592, and are dominated by Australia, France, China, Germany and the US. Some Chinese tourists have increased their levels of spending from previous years. 30.26 percent of tourists are middle class, spending between $662 to $1,285.\n\nThe Ngurah Rai International Airport is located near Jimbaran, on the isthmus at the southernmost part of the island. Lt.Col. Wisnu Airfield is found in north-west Bali.\n\nA coastal road circles the island, and three major two-lane arteries cross the central mountains at passes reaching to 1,750m in height (at Penelokan). The Ngurah Rai Bypass is a four-lane expressway that partly encircles Denpasar. Bali has no railway lines.\n\nIn December 2010 the Government of Indonesia invited investors to build a new Tanah Ampo Cruise Terminal at Karangasem, Bali with a projected worth of $30 million. On 17 July 2011 the first cruise ship (Sun Princess) anchored about away from the wharf of Tanah Ampo harbour. The current pier is only but will eventually be extended to to accommodate international cruise ships. The harbour here is safer than the existing facility at Benoa and has a scenic backdrop of east Bali mountains and green rice fields. The tender for improvement was subject to delays, and as of July 2013 the situation remained unclear with cruise line operators complaining and even refusing to use the existing facility at Tanah Ampo.\n\nA Memorandum of Understanding has been signed by two ministers, Bali's Governor and Indonesian Train Company to build of railway along the coast around the island. As of July 2015, no details of this proposed railways have been released.\n\nOn 16 March 2011 (Tanjung) Benoa port received the \"Best Port Welcome 2010\" award from London's \"Dream World Cruise Destination\" magazine. Government plans to expand the role of Benoa port as export-import port to boost Bali's trade and industry sector. The Tourism and Creative Economy Ministry has confirmed that 306 cruise liners are heading for Indonesia in 2013 – an increase of 43 percent compared to the previous year.\n\nIn May 2011, an integrated Aerial Traffic Control System (ATCS) was implemented to reduce traffic jams at four crossing points: Ngurah Rai statue, Dewa Ruci Kuta crossing, Jimbaran crossing and Sanur crossing. ATCS is an integrated system connecting all traffic lights, CCTVs and other traffic signals with a monitoring office at the police headquarters. It has successfully been implemented in other ASEAN countries and will be implemented at other crossings in Bali.\n\nOn 21 December 2011 construction started on the Nusa Dua-Benoa-Ngurah Rai International Airport toll road which will also provide a special lane for motorcycles. This has been done by seven state-owned enterprises led by PT Jasa Marga with 60% of shares. PT Jasa Marga Bali Tol will construct the toll road (totally with access road). The construction is estimated to cost Rp.2.49 trillion ($273.9 million). The project goes through of mangrove forest and through of beach, both within area. The elevated toll road is built over the mangrove forest on 18,000 concrete pillars which occupied 2 hectares of mangroves forest. This was compensated by the planting of 300,000 mangrove trees along the road. On 21 December 2011 the Dewa Ruci underpass has also started on the busy Dewa Ruci junction near Bali Kuta Galeria with an estimated cost of Rp136 billion ($14.9 million) from the state budget. On 23 September 2013, the Bali Mandara Toll Road was opened, with the Dewa Ruci Junction (Simpang Siur) underpass being opened previously.\n\nTo solve chronic traffic problems, the province will also build a toll road connecting Serangan with Tohpati, a toll road connecting Kuta, Denpasar and Tohpati and a flyover connecting Kuta and Ngurah Rai Airport.\n\nThe population of Bali was 3,890,757 as of the 2010 Census; the latest estimate (for January 2014) is 4,225,384. There are an estimated 30,000 expatriates living in Bali.\n\nA DNA study in 2005 by Karafet et al. found that 12% of Balinese Y-chromosomes are of likely Indian origin, while 84% are of likely Austronesian origin, and 2% of likely Melanesian origin. The study does not correlate the DNA samples to the Balinese caste system.\n\nPre-modern Bali has had four castes, state Jeff Lewis and Belinda Lewis, but with a \"very strong tradition of communal decision-making and interdependence\". The four castes have been classified as Soedra (Shudra), Wesia (Vaishyas), Satrias (Kshatriyas) and Brahmana (Brahmin).\n\nThe 19th-century scholars such as Crawfurd and Friederich suggested that Balinese caste had Indian origins, but states Helen Creese, scholars such as Brumund who visited and stayed the island of Bali suggested that his field observations conflicted with the \"received understandings concerning its Indian origins\". In Bali, the Shudra (locally spelled \"Soedra\") have typically been the temple priests, though depending on the demographics, a temple priest may also be from the other three castes. In most regions, it has been the Shudra who typically make offerings to the gods on behalf of the Hindu devotees, chant prayers, recite \"meweda\" (Vedas), and set the course of Balinese temple festivals.\n\nUnlike most of Muslim-majority Indonesia, about 83.5% of Bali's population adheres to Balinese Hinduism, formed as a combination of existing local beliefs and Hindu influences from mainland Southeast Asia and South Asia. Minority religions include Islam (13.37%), Christianity (2.47%), and Buddhism (0.5%). These figures do not include immigrants from other parts of Indonesia.\n\nThe general beliefs and practices of \"Agama Hindu Dharma\" are a mixture of ancient traditions and contemporary pressures placed by Indonesian laws that permit only monotheist belief under the national ideology of \"panca sila\". Traditionally, Hinduism in Indonesia had a pantheon of deities and that tradition of belief continues in practice; further, Hinduism in Indonesia granted freedom and flexibility to Hindus as to when, how and where to pray. However, officially, Indonesian government considers and advertises Indonesian Hinduism as a monotheistic religion with certain officially recognized beliefs that comply with its national ideology. Indonesian school text books describe Hinduism as having one supreme being, Hindus offering three daily mandatory prayers, and Hinduism as having certain common beliefs that in part parallel those of Islam. Scholars contest whether these Indonesian government recognized and assigned beliefs reflect the traditional beliefs and practices of Hindus in Indonesia before Indonesia gained independence from Dutch colonial rule.\n\nBalinese Hinduism has roots in Indian Hinduism and Buddhism, that arrived through Java. Hindu influences reached the Indonesian Archipelago as early as the first century. Historical evidence is unclear about the diffusion process of cultural and spiritual ideas from India. Java legends refer to Saka-era, traced to 78 AD. Stories from the Mahabharata Epic have been traced in Indonesian islands to the 1st century; however, the versions mirror those found in southeast Indian peninsular region (now Tamil Nadu and southern Andhra Pradesh).\n\nThe Bali tradition adopted the pre-existing animistic traditions of the indigenous people. This influence strengthened the belief that the gods and goddesses are present in all things. Every element of nature, therefore, possesses its own power, which reflects the power of the gods. A rock, tree, dagger, or woven cloth is a potential home for spirits whose energy can be directed for good or evil. Balinese Hinduism is deeply interwoven with art and ritual. Ritualizing states of self-control are a notable feature of religious expression among the people, who for this reason have become famous for their graceful and decorous behaviour.\n\nApart from the majority of Balinese Hindus, there also exist Chinese immigrants whose traditions have melded with that of the locals. As a result, these Sino-Balinese not only embrace their original religion, which is a mixture of Buddhism, Christianity, Taoism and Confucianism, but also find a way to harmonise it with the local traditions. Hence, it is not uncommon to find local Sino-Balinese during the local temple's \"odalan\". Moreover, Balinese Hindu priests are invited to perform rites alongside a Chinese priest in the event of the death of a Sino-Balinese. Nevertheless, the Sino-Balinese claim to embrace Buddhism for administrative purposes, such as their Identity Cards.\n\nBalinese and Indonesian are the most widely spoken languages in Bali, and the vast majority of Balinese people are bilingual or trilingual. The most common spoken language around the tourist areas is Indonesian, as many people in the tourist sector are not solely Balinese, but migrants from Java, Lombok, Sumatra, and other parts of Indonesia. There are several indigenous Balinese languages, but most Balinese can also use the most widely spoken option: modern common Balinese. The usage of different Balinese languages was traditionally determined by the Balinese caste system and by clan membership, but this tradition is diminishing. Kawi and Sanskrit are also commonly used by some Hindu priests in Bali, for Hinduism literature was mostly written in Sanskrit.\n\nEnglish and Chinese are the next most common languages (and the primary foreign languages) of many Balinese, owing to the requirements of the tourism industry, as well as the English-speaking community and huge Chinese-Indonesian population. Other foreign languages, such as Japanese, Korean, French, Russian or German are often used in multilingual signs for foreign tourists.\n\nBali is renowned for its diverse and sophisticated art forms, such as painting, sculpture, woodcarving, handcrafts, and performing arts. Balinese cuisine is also distinctive. Balinese percussion orchestra music, known as \"gamelan\", is highly developed and varied. Balinese performing arts often portray stories from Hindu epics such as the Ramayana but with heavy Balinese influence. Famous Balinese dances include \"pendet\", \"legong\", \"baris\", \"topeng\", \"barong\", \"gong keybar\", and \"kecak\" (the monkey dance). Bali boasts one of the most diverse and innovative performing arts cultures in the world, with paid performances at thousands of temple festivals, private ceremonies, or public shows.\nThe Hindu New Year, \"Nyepi\", is celebrated in the spring by a day of silence. On this day everyone stays at home and tourists are encouraged to remain in their hotels. On the day before New Year, large and colourful sculptures of \"ogoh-ogoh\" monsters are paraded and finally burned in the evening to drive away evil spirits. Other festivals throughout the year are specified by the Balinese \"pawukon\" calendrical system.\n\nCelebrations are held for many occasions such as a tooth-filing (coming-of-age ritual), cremation or \"odalan\" (temple festival). One of the most important concepts that Balinese ceremonies have in common is that of \"désa kala patra\", which refers to how ritual performances must be appropriate in both the specific and general social context. Many of the ceremonial art forms such as \"wayang kulit\" and \"topeng\" are highly improvisatory, providing flexibility for the performer to adapt the performance to the current situation. Many celebrations call for a loud, boisterous atmosphere with lots of activity and the resulting aesthetic, \"ramé\", is distinctively Balinese. Often two or more \"gamelan\" ensembles will be performing well within earshot, and sometimes compete with each other to be heard. Likewise, the audience members talk amongst themselves, get up and walk around, or even cheer on the performance, which adds to the many layers of activity and the liveliness typical of \"ramé\".\n\"Kaja\" and \"kelod\" are the Balinese equivalents of North and South, which refer to ones orientation between the island's largest mountain Gunung Agung (\"kaja\"), and the sea (\"kelod\"). In addition to spatial orientation, \"kaja\" and \"kelod\" have the connotation of good and evil; gods and ancestors are believed to live on the mountain whereas demons live in the sea. Buildings such as temples and residential homes are spatially oriented by having the most sacred spaces closest to the mountain and the unclean places nearest to the sea.\n\nMost temples have an inner courtyard and an outer courtyard which are arranged with the inner courtyard furthest \"kaja\". These spaces serve as performance venues since most Balinese rituals are accompanied by any combination of music, dance and drama. The performances that take place in the inner courtyard are classified as \"wali\", the most sacred rituals which are offerings exclusively for the gods, while the outer courtyard is where \"bebali\" ceremonies are held, which are intended for gods and people. Lastly, performances meant solely for the entertainment of humans take place outside the walls of the temple and are called \"bali-balihan\". This three-tiered system of classification was standardised in 1971 by a committee of Balinese officials and artists to better protect the sanctity of the oldest and most sacred Balinese rituals from being performed for a paying audience.\n\nTourism, Bali's chief industry, has provided the island with a foreign audience that is eager to pay for entertainment, thus creating new performance opportunities and more demand for performers. The impact of tourism is controversial since before it became integrated into the economy, the Balinese performing arts did not exist as a capitalist venture, and were not performed for entertainment outside of their respective ritual context. Since the 1930s sacred rituals such as the \"barong\" dance have been performed both in their original contexts, as well as exclusively for paying tourists. This has led to new versions of many of these performances which have developed according to the preferences of foreign audiences; some villages have a \"barong\" mask specifically for non-ritual performances as well as an older mask which is only used for sacred performances.\n\nBalinese society continues to revolve around each family's ancestral village, to which the cycle of life and religion is closely tied. Coercive aspects of traditional society, such as customary law sanctions imposed by traditional authorities such as village councils (including \"kasepekang\", or shunning) have risen in importance as a consequence of the democratisation and decentralisation of Indonesia since 1998.\n\nBali is a major world surfing destination with popular breaks dotted across the southern coastline and around the offshore island of Nusa Lembongan.\n\nAs part of the Coral Triangle, Bali, including Nusa Penida, offers a wide range of dive sites with varying types of reefs.\n\nBali was the host of 2008 Asian Beach Games. It was the second time Indonesia hosted an Asia-level multi-sport event, after Jakarta held the 1962 Asian Games.\n\nIn football, Bali is home to the football club Bali United, which plays in the Liga 1.\nThe team was relocated from Samarinda, East Kalimantan to Gianyar, Bali. Harbiansyah Hanafiah, the main commissioner of Bali United explained that he did the name change and moved the homebase to Bali because there were no representative from Bali in the highest football tier in Indonesia. Another reason was due to local fans in Samarinda prefer to support Pusamania Borneo F.C. more than Persisam.\n\nIn June 2012, Subak, the irrigation system for paddy fields in Bali was enlisted as a Natural UNESCO world heritage site.\n\nBali was the host of Miss World 2013 (63rd edition of the Miss World pageant). It was the first time Indonesia hosted an international beauty pageant.\n\n\n\n\n\n"}
{"id": "4149", "url": "https://en.wikipedia.org/wiki?curid=4149", "title": "Bulgarian language", "text": "Bulgarian language\n\nBulgarian , ( \"bǎlgarski\", ) is an Indo-European language, a member of the Southern branch of the Slavic language family.\n\nBulgarian, along with the closely related Macedonian language (collectively forming the East South Slavic languages), has several characteristics that set it apart from all other Slavic languages: changes include the elimination of case declension, the development of a suffixed definite article (see Balkan language area), and the lack of a verb infinitive, but it retains and has further developed the Proto-Slavic verb system. Various evidential verb forms exist to express unwitnessed, retold, and doubtful action.\n\nWith the accession of Bulgaria to the European Union on 1 January 2007, Bulgarian became one of the official languages of the European Union.\n\nDevelopment of the Bulgarian language may be divided into several periods.\n\n\n\"Bulgarian\" was the first \"Slavic\" language attested in writing. As Slavic linguistic unity lasted into late antiquity, in the oldest manuscripts this language was initially referred to as языкъ словяньскъ, \"the Slavic language\". In the Middle Bulgarian period this name was gradually replaced by the name , the \"Bulgarian language\". In some cases, the name was used not only with regard to the contemporary Middle Bulgarian language of the copyist but also to the period of Old Bulgarian. A most notable example of anachronism is the Service of St. Cyril from Skopje (Скопски миней), a 13th-century Middle Bulgarian manuscript from northern Macedonia according to which St. Cyril preached with \"Bulgarian\" books among the Moravian Slavs. The first mention of the language as the \"Bulgarian language\" instead of the \"Slavonic language\" comes in the work of the Greek clergy of the Bulgarian Archbishopric of Ohrid in the 11th century, for example in the Greek hagiography of Saint Clement of Ohrid by Theophylact of Ohrid (late 11th century).\n\nDuring the Middle Bulgarian period, the language underwent dramatic changes, losing the Slavonic case system, but preserving the rich verb system (while the development was exactly the opposite in other Slavic languages) and developing a definite article. It was influenced by its non-Slavic neighbors in the Balkan language area (mostly grammatically) and later also by Turkish, which was the official language of the Ottoman Empire, in the form of the Ottoman Turkish language, mostly lexically. As a national revival occurred toward the end of the period of Ottoman rule (mostly during the 19th century), a modern Bulgarian literary language gradually emerged that drew heavily on Church Slavonic/Old Bulgarian (and to some extent on literary Russian, which had preserved many lexical items from Church Slavonic) and later reduced the number of Turkish and other Balkan loans. Today one difference between Bulgarian dialects in the country and literary spoken Bulgarian is the significant presence of Old Bulgarian words and even word forms in the latter. Russian loans are distinguished from Old Bulgarian ones on the basis of the presence of specifically Russian phonetic changes, as in оборот (turnover, rev), непонятен (incomprehensible), ядро (nucleus) and others. As usual in such cases, many other loans from French, English and the classical languages have subsequently entered the language as well.\n\nModern Bulgarian was based essentially on the Eastern dialects of the language, but its pronunciation is in many respects a compromise between East and West Bulgarian (see especially the phonetic sections below). Following the efforts of some figures of the National awakening of Bulgaria (the most notable among them being Neofit Rilski and Ivan Bogorov), there had been many attempts to codify a standard Bulgarian language; however, there was much argument surrounding the choice of norms. Between 1835–1878 more than 25 proposals were put forward and \"linguistic chaos\" ensued. Eventually the eastern dialects prevailed, and in 1899 the Ministry of Education officially codified a standard Bulgarian language based on the Drinov-Ivanchev orthography.\n\nThe language is mainly split into two broad dialect areas, based on the different reflexes of the Common Slavic yat vowel (Ѣ). This split, which occurred at some point during the Middle Ages, led to the development of Bulgaria's:\n\nThe literary language norm, which is generally based on the Eastern dialects, also has the Eastern alternating reflex of \"yat\". However, it has not incorporated the general Eastern umlaut of \"all\" synchronic or even historic \"ya\" sounds into \"e\" before front vowels – e.g. поляна (\"polyana\") vs. полени (\"poleni\") \"meadow – meadows\" or even жаба (\"zhaba\") vs. жеби (\"zhebi\") \"frog – frogs\", even though it co-occurs with the yat alternation in almost all Eastern dialects that have it (except a few dialects along the yat border, e.g. in the Pleven region).\n\nMore examples of the \"yat\" umlaut in the literary language are:\n\nUntil 1945, Bulgarian orthography did not reveal this alternation and used the original Old Slavic Cyrillic letter \"yat\" (Ѣ), which was commonly called двойно е (\"dvoyno e\") at the time, to express the historical \"yat\" vowel or at least root vowels displaying the \"ya – e\" alternation. The letter was used in each occurrence of such a root, regardless of the actual pronunciation of the vowel: thus, both \"mlyako\" and \"mlekar\" were spelled with (Ѣ). Among other things, this was seen as a way to \"reconcile\" the Western and the Eastern dialects and maintain language unity at a time when much of Bulgaria's Western dialect area was controlled by Serbia and Greece, but there were still hopes and occasional attempts to recover it. With the 1945 orthographic reform, this letter was abolished and the present spelling was introduced, reflecting the alternation in pronunciation.\n\nThis had implications for some grammatical constructions:\n\nSometimes, with the changes, words began to be spelled as other words with different meanings, e.g.:\n\nIn spite of the literary norm regarding the yat vowel, many people living in Western Bulgaria, including the capital Sofia, will fail to observe its rules. While the norm requires the realizations \"vidyal\" vs. \"videli\" (he has seen; they have seen), some natives of Western Bulgaria will preserve their local dialect pronunciation with \"e\" for all instances of \"yat\" (e.g. \"videl\", \"videli\"). Others, attempting to adhere to the norm, will actually use the \"ya\" sound even in cases where the standard language has \"e\" (e.g. \"vidyal\", \"vidyali\"). The latter hypercorrection is called свръхякане (\"svrah-yakane\" ≈\"over-\"ya\"-ing\").\n\nBulgarian is the only Slavic language whose literary standard does not naturally contain the iotated sound (or its palatalized variant , except in non-Slavic foreign-loaned words). The sound is common in all modern Slavic languages (e.g. Czech \"medvěd\" \"bear\", Polish \"pięć\" \"five\", Serbo-Croatian jelen\" \"deer\", Ukrainian \"немає \"there is not...\", Macedonian \"пишување\" \"writing\", etc.), as well as some Western Bulgarian dialectal forms – e.g. \"ора̀н’е\" (standard Bulgarian: \"оране\" , \"ploughing\"), however it is not represented in standard Bulgarian speech or writing. Even where occurs in other Slavic words, in Standard Bulgarian it is usually transcribed and pronounced as pure – e.g. Boris Yeltsin is \"Eltsin\" (), Yekaterinburg is \"Ekaterinburg\" () and Sarajevo is \"Saraevo\" (), although - because the sound is contained in a stressed syllable at the beginning of the word - Jelena Janković is \"Yelena\" – .\n\nUntil the period immediately following the Second World War, all Bulgarian and the majority of foreign linguists referred to the South Slavic dialect continuum spanning the area of modern Bulgaria, the Republic of Macedonia and parts of Northern Greece as a group of Bulgarian dialects. In contrast, Serbian sources tended to label them \"south Serbian\" dialects. Some local naming conventions included \"bolgarski\", \"bugarski\" and so forth. The codifiers of the standard Bulgarian language, however, did not wish to make any allowances for a pluricentric \"Bulgaro-Macedonian\" compromise. After 1944 the People's Republic of Bulgaria and the Socialist Federal Republic of Yugoslavia began a policy of making Macedonia into the connecting link for the establishment of a new Balkan Federative Republic and stimulating here a development of distinct Slav Macedonian consciousness. With the proclamation of the Socialist Republic of Macedonia as part of the Yugoslav federation, the new authorities also started measures that would overcome the pro-Bulgarian feeling among parts of its population and in 1945 a separate Macedonian language was codified. After 1958, when the pressure from Moscow decreased, Sofia reverted to the view that the Macedonian language did not exist as a separate language. Nowadays, Bulgarian and Greek linguists as well as some linguists from other countries still consider Macedonian dialects as Bulgarian. Outside Bulgaria and Greece, Macedonian is generally considered an autonomous language within the South Slavic dialect continuum. Sociolinguists agree that the question whether Macedonian is a dialect of Bulgarian or a language is a political one and cannot be resolved on a purely linguistic basis, because dialect continua do not allow for either/or judgments.\n\nIn 886 AD, the Bulgarian Empire introduced the Glagolitic alphabet which was devised by the Saints Cyril and Methodius in the 850s. The Glagolitic alphabet was gradually superseded in later centuries by the Cyrillic script, developed around the Preslav Literary School, Bulgaria in the beginning of the 10th century.\n\nSeveral Cyrillic alphabets with 28 to 44 letters were used in the beginning and the middle of the 19th century during the efforts on the codification of Modern Bulgarian until an alphabet with 32 letters, proposed by Marin Drinov, gained prominence in the 1870s. The alphabet of Marin Drinov was used until the orthographic reform of 1945, when the letters Ѣ, ѣ (called \"ят\" 'yat' or \"двойно е\"/\"е-двойно\" 'double e'), and Ѫ, ѫ (called \"Голям юс\" 'big yus', \"голяма носовка\" 'big nasal sign', \"ъ кръстато\" 'crossed yer' or \"широко ъ\" 'long yer'), were removed from its alphabet, reducing the number of letters to 30.\n\nWith the accession of Bulgaria to the European Union on 1 January 2007, Cyrillic became the third official script of the European Union, following the Latin and Greek scripts.\n\nBulgarian possesses a phonology similar to that of the rest of the South Slavic languages, notably lacking Serbo-Croatian's phonemic vowel length and tones and alveo-palatal affricates. Macedonian on the other side exhibits a phonology very similar to that of Bulgarian, which has spurred controversial debates regarding its status as a separate language. An interesting geographic pattern of dialectal distribution shows a tendency of western dialects to approach Serbo-Croatian's \"hard\" sound in contrast to the eastern dialect's \"soft\" sound due to pre-palatalization and rising of (similar to Russian) and ikanye (a merger of the two front vowels and ).\n\nBulgarian is typically analyzed as having six vowels, but at least two more reduced vowels can be encountered in everyday speech.\n\nThe parts of speech in Bulgarian are divided in 10 types, which are categorized in two broad classes: mutable and immutable. The difference is that mutable parts of speech vary grammatically, whereas the immutable ones do not change, regardless of their use. The five classes of mutables are: \"nouns\", \"adjectives\", \"numerals\", \"pronouns\" and \"verbs\". Syntactically, the first four of these form the group of the noun or the nominal group. The immutables are: \"adverbs\", \"prepositions\", \"conjunctions\", \"particles\" and \"interjections\". Verbs and adverbs form the group of the verb or the verbal group.\n\nNouns and adjectives have the categories grammatical gender, number, case (only vocative) and definiteness in Bulgarian. Adjectives and adjectival pronouns agree with nouns in number and gender. Pronouns have gender and number and retain (as in nearly all Indo-European languages) a more significant part of the case system.\n\nThere are three grammatical genders in Bulgarian: \"masculine\", \"feminine\" and \"neuter\". The gender of the noun can largely be inferred from its ending: nouns ending in a consonant (\"zero ending\") are generally masculine (for example, град 'city', син 'son', мъж 'man'; those ending in –а/–я (-a/-ya) (жена 'woman', дъщеря 'daughter', улица 'street') are normally feminine; and nouns ending in –е, –о are almost always neuter (дете 'child', езеро 'lake'), as are those rare words (usually loanwords) that end in –и, –у, and –ю (цунами 'tsunami', табу 'taboo', меню 'menu'). Perhaps the most significant exception from the above are the relatively numerous nouns that end in a consonant and yet are feminine: these comprise, firstly, a large group of nouns with zero ending expressing quality, degree or an abstraction, including all nouns ending on –ост/–ест -{ost/est} (мъдрост 'wisdom', низост 'vileness', прелест 'loveliness', болест 'sickness', любов 'love'), and secondly, a much smaller group of irregular nouns with zero ending which define tangible objects or concepts (кръв 'blood', кост 'bone', вечер 'evening', нощ 'night'). There are also some commonly used words that end in a vowel and yet are masculine: баща 'father', дядо 'grandfather', чичо / вуйчо 'uncle', and others.\n\nThe plural forms of the nouns do not express their gender as clearly as the singular ones, but may also provide some clues to it: the ending –и (-i) is more likely to be used with a masculine or feminine noun (факти 'facts', болести 'sicknesses'), while one in –а/–я belongs more often to a neuter noun (езера 'lakes'). Also, the plural ending –ове occurs only in masculine nouns.\n\nTwo numbers are distinguished in Bulgarian – singular and plural. A variety of plural suffixes is used, and the choice between them is partly determined by their ending in singular and partly influenced by gender; in addition, irregular declension and alternative plural forms are common. Words ending in –а/–я (which are usually feminine) generally have the plural ending –и, upon dropping of the singular ending. Of nouns ending in a consonant, the feminine ones also use –и, whereas the masculine ones usually have –и for polysyllables and –ове for monosyllables (however, exceptions are especially common in this group). Nouns ending in –о/–е (most of which are neuter) mostly use the suffixes –а, –я (both of which require the dropping of the singular endings) and –та.\n\nWith cardinal numbers and related words such as няколко ('several'), masculine nouns use a special count form in –а/–я, which stems from the Proto-Slavonic dual: два/три стола ('two/three chairs') versus тези столове ('these chairs'); cf. feminine две/три/тези книги ('two/three/these books') and neuter две/три/тези легла ('two/three/these beds'). However, a recently developed language norm requires that count forms should only be used with masculine nouns that do not denote persons. Thus, двама/трима ученици ('two/three students') is perceived as more correct than двама/трима ученика, while the distinction is retained in cases such as два/три молива ('two/three pencils') versus тези моливи ('these pencils').\n\nCases exist only in the personal and some other pronouns (as they do in many other modern Indo-European languages), with nominative, accusative, dative and vocative forms. Vestiges are present in a number of phraseological units and sayings. The major exception are vocative forms, which are still in use for masculine (with the endings -е, -о and -ю) and feminine nouns (-[ь/й]о and -е) in the singular.\n\nIn modern Bulgarian, definiteness is expressed by a definite article which is postfixed to the noun, much like in the Scandinavian languages or Romanian (indefinite: човек, 'person'; definite: човекът, \"\"the\" person\") or to the first nominal constituent of definite noun phrases (indefinite: добър човек, 'a good person'; definite: добрият човек, \"\"the\" good person\"). There are four singular definite articles. Again, the choice between them is largely determined by the noun's ending in the singular. Nouns that end in a consonant and are masculine use –ът/–ят, when they are grammatical subjects, and –а/–я elsewhere. Nouns that end in a consonant and are feminine, as well as nouns that end in –а/–я (most of which are feminine, too) use –та. Nouns that end in –е/–о use –то.\n\nThe plural definite article is –те for all nouns except for those, whose plural form ends in –а/–я; these get –та instead. When postfixed to adjectives the definite articles are –ят/–я for masculine gender (again, with the longer form being reserved for grammatical subjects), –та for feminine gender, –то for neuter gender, and –те for plural.\n\n\nIn Bulgarian adjective-noun phrases, only the adjective takes a definite article ending –\nMany of the English loanwords which have been adopted into the language since the end of communism, however, do not readily lend themselves to taking adjectival endings. This has caused an unprecedented shift in the language whereby, in certain cases, the adjective remains uninflected, while the noun following it takes the grammatical ending. Examples include –\n\nThis type of combination is sometimes favoured even when the possibility of a traditional phrase structure exists, e.g. –\nIn this case, the brand name \"btv\" cannot be inflected and, being a brand, remains in Roman script within the sentence.\n\nBoth groups agree in gender and number with the noun they are appended to. They may also take the definite article as explained above.\n\nPronouns may vary in gender, number, and definiteness, and are the only parts of speech that have retained case inflections. Three cases are exhibited by some groups of pronouns – nominative, accusative and dative. The distinguishable types of pronouns include the following: personal, relative, reflexive, interrogative, negative, indefinitive, summative and possessive.\n\nThe Bulgarian verb can take up to 3,000 distinct forms, as it varies in person, number, voice, aspect, mood, tense and even gender.\n\nFinite verbal forms are \"simple\" or \"compound\" and agree with subjects in person (first, second and third) and number (singular, plural) in Bulgarian. In addition to that, past compound forms using participles vary in gender (masculine, feminine, neuter) and voice (active and passive) as well as aspect (perfective/aorist and imperfective).\n\nBulgarian verbs express lexical aspect: perfective verbs signify the completion of the action of the verb and form past perfective (aorist) forms; imperfective ones are neutral with regard to it and form past imperfective forms. Most Bulgarian verbs can be grouped in perfective-imperfective pairs (imperfective/perfective: идвам/дойда \"come\", пристигам/пристигна \"arrive\"). Perfective verbs can be usually formed from imperfective ones by suffixation or prefixation, but the resultant verb often deviates in meaning from the original. In the pair examples above, aspect is stem-specific and therefore there is no difference in meaning.\n\nIn Bulgarian, there is also grammatical aspect. Three grammatical aspects are distinguishable: neutral, perfect and pluperfect. The neutral aspect comprises the three simple tenses and the future tense. The pluperfect is manifest in tenses that use double or triple auxiliary \"be\" participles like the past pluperfect subjunctive. Perfect constructions use a single auxiliary \"be\".\n\nThe traditional interpretation is that in addition to the four moods (наклонения ) shared by most other European languages – indicative (изявително, ) imperative (повелително ), subjunctive (подчинително ) and conditional (условно, ) – in Bulgarian there is one more to describe a general category of unwitnessed events – the inferential (преизказно ) mood. However, most contemporary Bulgarian linguists usually exclude the subjunctive mood and the inferential mood from the list of Bulgarian moods (thus placing the number of Bulgarian moods at a total of 3: indicative, imperative and conditional) and don't consider them to be moods but view them as verbial morphosyntactic constructs or separate gramemes of the verb class. The possible existence of a few other moods has been discussed in the literature. Most Bulgarian school grammars teach the traditional view of 4 Bulgarian moods (as described above, but excluding the subjunctive and including the inferential).\n\nThere are three grammatically distinctive positions in time – present, past and future – which combine with aspect and mood to produce a number of formations. Normally, in grammar books these formations are viewed as separate tenses – i. e. \"past imperfect\" would mean that the verb is in past tense, in the imperfective aspect, and in the indicative mood (since no other mood is shown). There are more than 40 different tenses across Bulgarian's two aspects and five moods.\n\nIn the indicative mood, there are three simple tenses:\n\nIn the indicative there are also the following compound tenses:\n\nThe four perfect constructions above can vary in aspect depending on the aspect of the main-verb participle; they are in fact pairs of imperfective and perfective aspects. Verbs in forms using past participles also vary in voice and gender.\n\nThere is only one simple tense in the imperative mood, the present, and there are simple forms only for the second-person singular, -и/-й (-i, -y/i), and plural, -ете/-йте (-ete, -yte), e.g. уча ('to study'): учи , sg., учете , pl.; играя 'to play': играй , играйте . There are compound imperative forms for all persons and numbers in the present compound imperative (да играе, ), the present perfect compound imperative (да е играл, ) and the rarely used present pluperfect compound imperative (да е бил играл, ).\n\nThe conditional mood consists of five compound tenses, most of which are not grammatically distinguishable. The present, future and past conditional use a special past form of the stem би- (bi – \"be\") and the past participle (бих учил, , 'I would study'). The past future conditional and the past future perfect conditional coincide in form with the respective indicative tenses.\n\nThe subjunctive mood is rarely documented as a separate verb form in Bulgarian, (being, morphologically, a sub-instance of the quasi-infinitive construction with the particle да and a normal finite verb form), but nevertheless it is used regularly. The most common form, often mistaken for the present tense, is the present subjunctive ([по-добре] да отида , 'I had better go'). The difference between the present indicative and the present subjunctive tense is that the subjunctive can be formed by \"both\" perfective and imperfective verbs. It has completely replaced the infinitive and the supine from complex expressions (see below). It is also employed to express opinion about \"possible\" future events. The past perfect subjunctive ([по-добре] да бях отишъл , 'I'd had better be gone') refers to \"possible\" events in the past, which \"did not\" take place, and the present pluperfect subjunctive (да съм бил отишъл ), which may be used about both past and future events arousing feelings of incontinence, suspicion, etc. and has no perfect to English translation.\n\nThe inferential mood has five pure tenses. Two of them are simple – \"past aorist inferential\" and \"past imperfect inferential\" – and are formed by the past participles of perfective and imperfective verbs, respectively. There are also three compound tenses – \"past future inferential\", \"past future perfect inferential\" and \"past perfect inferential\". All these tenses' forms are gender-specific in the singular. There are also conditional and compound-imperative crossovers. The existence of inferential forms has been attributed to Turkic influences by most Bulgarian linguists. Morphologically, they are derived from the perfect.\n\nBulgarian has the following participles:\n\nThe participles are inflected by gender, number, and definiteness, and are coordinated with the subject when forming compound tenses (see tenses above). When used in attributive role the inflection attributes are coordinated with the noun that is being attributed.\n\nBulgarian uses reflexive verbal forms (i.e. actions which are performed by the agent onto him- or herself) which behave in a similar way as they do in many other Indo-European languages, such as French and Spanish. The reflexive is expressed by the invariable particle se, originally a clitic form of the accusative reflexive pronoun. Thus –\nWhen the action is performed on others, other particles are used, just like in any normal verb, e.g. –\nSometimes, the reflexive verb form has a similar but not necessarily identical meaning to the non-reflexive verb –\nIn other cases, the reflexive verb has a completely different meaning from its non-reflexive counterpart –\nWhen the action is performed on an indirect object, the particles change to si and its derivatives –\nIn some cases, the particle \"si\" is ambiguous between the indirect object and the possessive meaning –\nThe difference between transitive and intransitive verbs can lead to significant differences in meaning with minimal change, e.g. –\nThe particle \"si\" is often used to indicate a more personal relationship to the action, e.g. –\n\nThe most productive way to form adverbs is to derive them from the neuter singular form of the corresponding adjective—e.g. бързо (fast), силно (hard), странно (strange)—but adjectives ending in -ки use the masculine singular form (i.e. ending in -ки), instead—e.g. юнашки (heroically), мъжки (bravely, like a man), майсторски (skillfully). The same pattern is used to form adverbs from the (adjective-like) ordinal numerals, e.g. първо (firstly), второ (secondly), трето (thirdly), and in some cases from (adjective-like) cardinal numerals, e.g. двойно (twice as/double), тройно (three times as), петорно (five times as).\n\nThe remaining adverbs are formed in ways that are no longer productive in the language. A small number are original (not derived from other words), for example: тук (here), там (there), вътре (inside), вън (outside), много (very/much) etc. The rest are mostly fossilized case forms, such as:\n\nAdverbs can sometimes be reduplicated to emphasize the qualitative or quantitative properties of actions, moods or relations as performed by the subject of the sentence: \"бавно-бавно\" (\"rather slowly\"), \"едва-едва\" (\"with great difficulty\"), \"съвсем-съвсем\" (\"quite\", \"thoroughly\").\n\nBulgarian employs clitic doubling, mostly for emphatic purposes. For example, the following constructions are common in colloquial Bulgarian:\n\nThe phenomenon is practically obligatory in the spoken language in the case of inversion signalling information structure (in writing, clitic doubling may be skipped in such instances, with a somewhat bookish effect):\n\nSometimes, the doubling signals syntactic relations, thus:\n\nThis is contrasted with:\n\nIn this case, clitic doubling can be a colloquial alternative of the more formal or bookish passive voice, which would be constructed as follows:\n\nClitic doubling is also fully obligatory, both in the spoken and in the written norm, in clauses including several special expressions that use the short accusative and dative pronouns such as играе ми се (I feel like playing), студено ми е (I am cold), and боли ме ръката (my arm hurts):\n\nExcept the above examples, clitic doubling is considered inappropriate in a formal context. Bulgarian grammars usually do not treat this phenomenon extensively.\n\nQuestions in Bulgarian which do not use a question word (such as who? what? etc.) are formed with the particle ли after the verb; a subject is not necessary, as the verbal conjugation suggests who is performing the action:\n\nWhile the particle \"ли\" generally goes after the verb, it can go after a noun or adjective if a contrast is needed:\nA verb is not always necessary, e.g. when presenting a choice:\n\nRhetorical questions can be formed by adding ли to a question word, thus forming a \"double interrogative\" –\nThe same construction +не ('no') is an emphasised positive –\n\nСъм\n\nThe verb съм – 'to be' is also used as an auxiliary for forming the perfect, the passive and the conditional:\n\nTwo alternate forms of съм exist:\n\nЩе\n\nThe impersonal verb ще (lit. 'it wants') is used to for forming the (positive) future tense:\nThe negative future is formed with the invariable construction няма да (see няма below):\nThe past tense of this verb – щях is conjugated to form the past conditional ('would have' – again, with да, since it is \"irrealis\"):\n\nИмам and нямам\n\nThe verbs имам ('to have') and нямам ('to not have'):\n\nDiminutive\nUsually done by adding -че, -це or -(ч)ка. The first two of these change the gender to the neuter:\n\nAffectionate Form\n\nSometimes proper nouns and words referring to friends or family members can have a diminutive ending added to show affection. These constructions are all referred to as \"\"na galeno\"\" (lit. \"caressing\" form):\nSuch words can be used \"both\" from parent to child, and vice versa, as can:\n\nPersonal names are shortened:\n\nThere is an interesting trend (which is comparatively modern, although it might well have deeper, dormant roots) where the feminine ending \"-ka\" and the definite suffix \"-ta\" (\"the\") are added to male names – note that this is affectionate and not at all insulting (in fact, the endings are not even really considered as being \"feminine\"):\n\nThe female equivalent would be to add the neuter ending \"-to\" to the diminutive form:\n\nAugmentative\n\nThis is to present words to sound larger – usually by adding \"-shte\":\nSome words only exist in an augmentative form – e.g.\n\n\"But\"\n\nIn Bulgarian, there are several conjunctions all translating into English as \"but\", which are all used in distinct situations. They are но (no), ама (amà), а (a), ами (amì)\", and ала (alà)\" (and \"обаче (obache)\" – \"however\", identical in use to \"но\").\n\nWhile there is some overlapping between their uses, in many cases they are specific. For example, ami is used for a choice – \"ne tova, ami onova\" – \"not this one, but that one\" (comp. Spanish \"sino\"), while ama is often used to provide extra information or an opinion – \"kazah go, ama sgreshih\" – \"I said it, but I was wrong\". Meanwhile, a provides contrast between two situations, and in some sentences can even be translated as \"although\", \"while\" or even \"and\" – \"az rabotya, a toy blee\" – \"I'm working, and he's daydreaming\".\n\nVery often, different words can be used to alter the emphasis of a sentence – e.g. while \"\"pusha, no ne tryabva\"\" and \"\"pusha, a ne tryabva\"\" both mean \"I smoke, but I shouldn't\", the first sounds more like a statement of fact (\"...but I mustn't\"), while the second feels more like a \"judgement\" (\"...but I oughtn't\"). Similarly, \"az ne iskam, ama toy iska\" and \"az ne iskam, a toy iska\" both mean \"I don't want to, but he does\", however the first emphasises the fact that \"he\" wants to, while the second emphasises the \"wanting\" rather than the person.\n\n\"Ala\" is interesting in that, while it feels archaic, it is often used in poetry and frequently in children's stories, since it has quite a moral/ominous feel to it.\n\nSome common expressions use these words, and some can be used alone as interjections:\n\nBulgarian has several abstract particles which are used to strengthen a statement. These have no precise translation in English. The particles are strictly informal and can even be considered rude by some people and in some situations. They are mostly used at the end of questions or instructions.\n\nModal Particles\n\nThese are \"tagged\" on to the beginning or end of a sentence to express the mood of the speaker in relation to the situation. They are mostly interrogative or slightly imperative in nature. There is no change in the grammatical mood when these are used (although they may be expressed through different grammatical moods in other languages).\n\nThese express intent or desire, perhaps even pleading. They can be seen as a sort of cohortative side to the language. (Since they can be used by themselves, they could even be considered as verbs in their own right.) They are also highly informal.\n\nThese particles can be combined with the vocative particles for greater effect, e.g. \"ya da vidya, be\" (let me see), or even exclusively in combinations with them, with no other elements, e.g. \"hayde, de!\" (come on!); \"nedey, de!\" (I told you not to!).\n\nBulgarian has several pronouns of quality which have no direct parallels in English – \"kakav\" (what sort of); \"takuv\" (this sort of); \"onakuv\" (that sort of – colloq.); \"nyakakav\" (some sort of); \"nikakav\" (no sort of); \"vsyakakav\" (every sort of); and the relative pronoun \"kakavto\" (the sort of...that...). The adjective \"ednakuv\" (\"the same\") derives from the same radical.\n\nExample phrases include:\n\nAn interesting phenomenon is that these can be strung along one after another in quite long constructions, e.g.\n\nAn extreme (colloquial) sentence, with almost no \"physical\" meaning in it whatsoever – yet which \"does\" have perfect meaning to the Bulgarian ear – would be :\n—Note: the subject of the sentence is simply the pronoun \"taya\" (lit. \"this one here\"; colloq. \"she\").\n\nAnother interesting phenomenon that is observed in colloquial speech is the use of \"takova\" (neuter of \"takyv\") not only as a substitute for an adjective, but also as a substitute for a verb. In that case the base form \"takova\" is used as the third person singular int the present indicative and all other forms are formed by analogy to other verbs in the language. Sometimes the \"verb\" may even acquire a derivational prefix that changes its meaning. Examples:\nAnother use of \"takova\" in colloquial speech is the word \"takovata\", which can be used as a substitution for a noun, but also, if the speaker doesn't remember or is not sure how to say something, they might say \"takovata\" and then pause to think about it:\nSimilar \"meaningless\" expressions are extremely common in spoken Bulgarian, especially when the speaker is finding it difficult to describe something.\n\nBulgarian has a rich set of inflectional and derivational processes. In the simplest terms, this can be seen in the way that most nouns and verbs are formed – namely by adding prefixes and suffixes to a rather limited number of roots, which creates almost a dozen new words, along with a couple of dozen derivatives thereof. Here are some examples using the root word \"klyuch\" (ключ) \"key/switch\":\n\nNouns:\nAdjectives:\nVerbs:\nAn extreme example using this root might be:\n\nAdjectives can also take up to three endings that are added to the masculine root, for example:\n\nVerbs can take several prefixes, thus expressing increasingly complex ideas. For example, the \"bol–\" root, which has to do with ailments (\"bol-ka\" – pain; \"bol-est\" – illness; \"bol-i\" – it hurts, etc.), can be used to express various different stages of falling ill:\n\nSimilarly, the root kri–, referring to hiding/discovery:\n\n\nMost of the vocabulary of modern Bulgarian consists of derivations of some 2,000 words inherited from proto-Slavic through the mediation of Old and Middle Bulgarian. Thus, the native lexical terms in Bulgarian account for 70% to 75% of the lexicon.\n\nThe remaining 25% to 30% are loanwords from a number of languages, as well as derivations of such words. In ancient times, Bulgarian adopted words of Thracian and Bulgar origin, as Bulgaria was inhabited by Thracian tribes, contributing to the ethnogenesis of the Bulgarian people (along with Slavs and Bulgars), and the Bulgars settled in the plains south of the Danube. The languages which have contributed most to Bulgarian are Russian, French and to a lesser extent Turkish and English. Also Latin and Greek are the source of many words, used mostly in international terminology. Many Latin terms entered the language through Romanian, Aromanian, and Megleno-Romanian during Bulgarian Empires (present-day Bulgaria was part of Roman Empire), loanwords of Greek origin in Bulgarian are a product of the influence of the liturgical language of the Orthodox Church. Many of the numerous loanwords from another Turkic language, Turkish (and, via Turkish, from Arabic and Persian) which were adopted into Bulgarian during the long period of Ottoman rule, have been replaced with native terms. In addition, both specialized (usually coming from the field of science) and commonplace English words (notably abstract, commodity/service-related or technical terms) have also penetrated Bulgarian since the second half of the 20th century, especially since 1989. A noteworthy portion of this English-derived terminology has attained some unique features in the process of its introduction to native speakers, and this has resulted in peculiar derivations that set the newly formed loanwords apart from the original words (mainly in pronunciation), although many loanwords are completely identical to the source words. A growing number of international neologisms are also being widely adopted, causing controversy between younger generations who, in general, are raised in the era of digital globalization, and the older, more conservative educated purists.\n\nSome very frequent expressions have been borrowed from other languages. Most of them are somewhat informal.\n\n(In the above two examples, the formal expression uses a plural verb but a singular pronoun, which allows speakers to distinguish the two grammatical forms.)\n\n\n\nLinguistic reports\n\nDictionaries\n\nCourses\n"}
{"id": "4153", "url": "https://en.wikipedia.org/wiki?curid=4153", "title": "Bipyramid", "text": "Bipyramid\n\nAn \"n\"-gonal bipyramid or dipyramid is a polyhedron formed by joining an \"n\"-gonal pyramid and its mirror image base-to-base. An \"n\"-gonal bipyramid has 2\"n\" triangle faces, 3\"n\" edges, and 2 + \"n\" vertices.\n\nThe referenced \"n\"-gon in the name of the bipyramids is not an external face but an internal one, existing on the primary symmetry plane which connects the two pyramid halves.\n\nA right bipyramid has two points above and below the centroid of its base. Nonright bipyramids are called oblique bipyramids. A regular bipyramid has a regular polygon internal face and is usually implied to be a \"right bipyramid\". A right bipyramid can be represented as for internal polygon P, and a regular \"n\"-bipyramid \n\nA concave bipyramid has a concave interior polygon.\n\nThe face-transitive regular bipyramids are the dual polyhedra of the uniform prisms and will generally have isosceles triangle faces.\n\nA bipyramid can be projected on a sphere or globe as \"n\" equally spaced lines of longitude going from pole to pole, and bisected by a line around the equator.\n\nBipyramid faces, projected as spherical triangles, represent the fundamental domains in the dihedral symmetry D.\n\nThe volume of a bipyramid is \"V\" =\"Bh\" where \"B\" is the area of the base and \"h\" the height from the base to the apex. This works for any location of the apex, provided that \"h\" is measured as the perpendicular distance from the plane which contains the base.\n\nThe volume of a bipyramid whose base is a regular \"n\"-sided polygon with side length \"s\" and whose height is \"h\" is therefore:\n\nOnly three kinds of bipyramids can have all edges of the same length (which implies that all faces are equilateral triangles, and thus the bipyramid is a deltahedron): the triangular, tetragonal, and pentagonal bipyramids. The tetragonal bipyramid with identical edges, or regular octahedron, counts among the Platonic solids, while the triangular and pentagonal bipyramids with identical edges count among the Johnson solids (J and J).\nIf the base is regular and the line through the apexes intersects the base at its center, the symmetry group of the \"n\"-gonal bipyramid has dihedral symmetry D of order 4\"n\", except in the case of a regular octahedron, which has the larger octahedral symmetry group O of order 48, which has three versions of D as subgroups. The rotation group is D of order 2\"n\", except in the case of a regular octahedron, which has the larger symmetry group O of order 24, which has three versions of D as subgroups.\n\nThe digonal faces of a spherical 2\"n\"-bipyramid represents the fundamental domains of dihedral symmetry in three dimensions: D, [\"n\",2], (*\"n\"22), order 4\"n\". The reflection domains can be shown as alternately colored triangles as mirror images.\n\nA scalenohedron is topologically identical to a 2\"n\"-bipyramid, but contains congruent scalene triangles.\n\nThere are two types. In one type the 2\"n\" vertices around the center alternate in rings above and below the center. In the other type, the 2\"n\" vertices are on the same plane, but alternate in two radii.\n\nThe first has 2-fold rotation axes mid-edge around the sides, reflection planes through the vertices, and n-fold rotation symmetry on its axis, representing symmetry D, [2,2\"n\"], (2*\"n\"), order 2\"n\". In crystallography, 8-sided and 12-sided scalenohedra exist. All of these forms are isohedra.\n\nThe second has symmetry D, [2,\"n\"], (*\"nn\"2), order 2\"n\".\n\nThe smallest scalenohedron has 8 faces and is topologically identical to the regular octahedron. The second type is a \"rhombic bipyramid\". The first type has 6 vertices can be represented as (0,0,±1), (±1,0,\"z\"), (0,±1,−\"z\"), where \"z\" is a parameter between 0 and 1, creating a regular octahedron at \"z\" = 0, and becoming a disphenoid with merged coplanar faces at \"z\" = 1. For \"z\" > 1, it becomes concave.\n\nSelf-intersecting bipyramids exist with a star polygon central figure, defined by triangular faces connecting each polygon edge to these two points. A {p/q} bipyramid has Coxeter diagram .\nisohedral even-sided stars can also be made with zig-zag offplane vertices, in-out isotoxal forms, or both, like this {8/3} form:\n\nThe dual of the rectification of each convex regular 4-polytopes is a cell-transitive 4-polytope with bipyramidal cells. In the following, the apex vertex of the bipyramid is A and an equator vertex is E. The distance between adjacent vertices on the equator EE = 1, the apex to equator edge is AE and the distance between the apices is AA. The bipyramid 4-polytope will have \"V\" vertices where the apices of \"N\" bipyramids meet. It will have \"V\" vertices where the type E vertices of \"N\" bipyramids meet. \"N\" bipyramids meet along each type AE edge. \"N\" bipyramids meet along each type EE edge. \"C\" is the cosine of the dihedral angle along an AE edge. \"C\" is the cosine of the dihedral angle along an EE edge. As cells must fit around an edge, \n\nIn general, a \"bipyramid\" can be seen as an \"n\"-polytope constructed with a (\"n\" − 1)-polytope in a hyperplane with two points in opposite directions, equal distance perpendicular from the hyperplane. If the (\"n\" − 1)-polytope is a regular polytope, it will have identical pyramids facets. An example is the 16-cell, which is an octahedral bipyramid, and more generally an \"n\"-orthoplex is an (\"n\" − 1)-orthoplex bypyramid.\n\n\n\n"}
{"id": "4154", "url": "https://en.wikipedia.org/wiki?curid=4154", "title": "Beast of Bodmin", "text": "Beast of Bodmin\n\nThe Beast of Bodmin, also known as the Beast of Bodmin Moor () is a phantom wild cat purported to live in Cornwall, England. Bodmin Moor became a centre of these sightings with occasional reports of mutilated slain livestock; the alleged panther-like cats of the same region came to be popularly known as the Beast of Bodmin Moor.\n\nIn general, scientists reject such claims because of the improbably large numbers necessary to maintain a breeding population and because climate and food supply issues would make such purported creatures' survival in reported habitats unlikely.\n\nA long-held hypothesis suggests the possibility that alien big cats at large in the United Kingdom could have been imported as part of private collections or zoos, later escaped or set free. An escaped big cat would not be reported to the authorities due to the illegality of owning and importing the animals. It has been claimed by that animal trainer Mary Chipperfield released three pumas into the wild following the closure of her Plymouth zoo in 1978, and that subsequent sightings of the animals gave rise to rumors of the Beast.\n\nThe Ministry of Agriculture, Fisheries and Food conducted an official investigation in 1995. The study found that there was \"no verifiable evidence\" of exotic felines loose in Britain, and that the mauled farm animals could have been attacked by common indigenous species. The report stated that, \"No verifiable evidence for the presence of a 'big cat' was found... There is no significant threat to livestock from a 'big cat' in Bodmin Moor.\"\n\nLess than a week after the government report, a boy was walking by the River Fowey when he discovered a large cat skull. Measuring about long by wide, the skull was lacking its lower jaw but possessed two sharp, prominent canines that suggested that it might have been a leopard. The story hit the national press at about the same time of the official denial of alien big cat evidence on Bodmin Moor.\n\nThe skull was sent to the Natural History Museum in London for verification. They determined that it was a genuine skull from a young male leopard, but also found that the cat had not died in Britain and that the skull had been imported as part of a leopard-skin rug. The back of the skull was cleanly cut off in a way that is commonly used to mount the head on a rug. There was an egg case inside the skull that had been laid by a tropical cockroach that could not possibly be found in Britain. There were also cut marks on the skull indicating the flesh had been scraped off with a knife, and the skull had begun to decompose only after a recent immersion in water.\n\n\n"}
{"id": "4157", "url": "https://en.wikipedia.org/wiki?curid=4157", "title": "Brown University", "text": "Brown University\n\nBrown University is a private Ivy League research university in Providence, Rhode Island, United States. Founded in 1764 as the College in the English Colony of Rhode Island and Providence Plantations, Brown is the seventh-oldest institution of higher education in the United States and one of the nine Colonial Colleges chartered before the American Revolution.\n\nAt its foundation, Brown was the first college in the United States to accept students regardless of their religious affiliation. Its engineering program was established in 1847 and was the first in the Ivy League. It was one of the early doctoral-granting U.S. institutions in the late 19th century, adding master and doctoral studies in 1887. Brown's New Curriculum is sometimes referred to in education theory as the Brown Curriculum and was adopted by faculty vote in 1969 after a period of student lobbying. The New Curriculum eliminated mandatory \"general education\" distribution requirements, made students \"the architects of their own syllabus,\" and allowed them to take any course for a grade of satisfactory or unrecorded no-credit. In 1971, Brown's coordinate women's institution Pembroke College was fully merged into the university. Pembroke Campus now includes dormitories and classrooms used by all of Brown.\n\nUndergraduate admissions is very selective, with an acceptance rate of 8.3 percent for the class of 2021. The University comprises the College, the Graduate School, Alpert Medical School, the School of Engineering, the School of Public Health, and the School of Professional Studies (which includes the IE Brown Executive MBA program). Brown's international programs are organized through the Watson Institute for International and Public Affairs, and the university is academically affiliated with the Marine Biological Laboratory and the Rhode Island School of Design. The Brown/RISD Dual Degree Program, offered in conjunction with the Rhode Island School of Design, is a five-year course that awards degrees from both institutions.\n\nBrown's main campus is located in the College Hill Historic District in the city of Providence, the third largest city in New England. The University's neighborhood is a federally listed architectural district with a dense concentration of Colonial-era buildings. Benefit Street, on the western edge of the campus, contains \"one of the finest cohesive collections of restored seventeenth- and eighteenth-century architecture in the United States\".\n\nBrown's faculty and alumni include eight Nobel Prize laureates, five National Humanities Medalists, and ten National Medal of Science laureates. Other notable alumni include eight billionaire graduates, a U.S. Supreme Court Chief Justice, four U.S. Secretaries of State and other Cabinet officials, 54 members of the United States Congress, 55 Rhodes Scholars, 48 Marshall Scholars, 14 MacArthur Genius Fellows, 19 Pulitzer Prize winners, members of royal families, as well as leaders and founders of major companies.\n\nThe origin of Brown University can be dated to 1761, when three residents of Newport, Rhode Island drafted a petition to the General Assembly of the colony:\n\nYour Petitioners propose to open a literary institution or School for instructing young Gentlemen in the Languages, Mathematics, Geography & History, & such other branches of Knowledge as shall be desired. That for this End ... it will be necessary ... to erect a public Building or Buildings for the boarding of the youth & the Residence of the Professors.\n\nThe three petitioners were Ezra Stiles, pastor of Newport's Second Congregational Church and future president of Yale; William Ellery, Jr., future signer of the United States Declaration of Independence; and Josias Lyndon, future governor of the colony. Stiles and Ellery were co-authors of the Charter of the College two years later. The editor of Stiles's papers observes, \"This draft of a petition connects itself with other evidence of Dr. Stiles's project for a Collegiate Institution in Rhode Island, before the charter of what became Brown University.\"\n\nThere is further documentary evidence that Stiles was making plans for a college in 1762. On January 20, Chauncey Whittelsey, pastor of the First Church of New Haven, answered a letter from Stiles:\n\nThe week before last I sent you the Copy of Yale College Charter... Should you make any Progress in the Affair of a Colledge, I should be glad to hear of it; I heartily wish you Success therein.\n\nThe Philadelphia Association of Baptist Churches also had an eye on Rhode Island, home of the mother church of their denomination: the First Baptist Church in America, founded in Providence in 1638 by Roger Williams. The Baptists were as yet unrepresented among colonial colleges; the Congregationalists had Harvard and Yale, the Presbyterians had the College of New Jersey (later Princeton), and the Episcopalians had the College of William and Mary and King's College (later Columbia). Isaac Backus was the historian of the New England Baptists and an inaugural Trustee of Brown, writing in 1784. He described the October 1762 resolution taken at Philadelphia:\n\nThe Philadelphia Association obtained such an acquaintance with our affairs, as to bring them to an apprehension that it was practicable and expedient to erect a college in the Colony of Rhode-Island, under the chief direction of the Baptists; ... Mr. James Manning, who took his first degree in New-Jersey college in September, 1762, was esteemed a suitable leader in this important work.\n\nManning arrived at Newport in July 1763 and was introduced to Stiles, who agreed to write the Charter for the College. Stiles's first draft was read to the General Assembly in August 1763 and rejected by Baptist members who worried that the College Board of Fellows would under-represent the Baptists. A revised Charter written by Stiles and Ellery was adopted by the Assembly on March 3, 1764.\n\nIn September 1764, the inaugural meeting of the College Corporation was held at Newport. Governor Stephen Hopkins was chosen chancellor, former and future governor Samuel Ward was vice chancellor, John Tillinghast treasurer, and Thomas Eyres secretary. The Charter stipulated that the Board of Trustees be composed of 22 Baptists, five Quakers, five Episcopalians, and four Congregationalists. Of the 12 Fellows, eight should be Baptists—including the College president—\"and the rest indifferently of any or all Denominations.\"\n\nThe Charter was not the grant of King George III, as is sometimes supposed, but rather an Act of the colonial General Assembly. In two particulars, the Charter may be said to be a uniquely progressive document. First, other colleges had curricular strictures against opposing doctrines, while Brown's Charter asserted, \"Sectarian differences of opinions, shall not make any Part of the Public and Classical Instruction.\" Second, according to Brown University historian Walter Bronson, \"the instrument governing Brown University recognized more broadly and fundamentally than any other the principle of denominational cooperation.\" The oft-repeated statement is inaccurate that Brown's Charter alone prohibited a religious test for College membership; other college charters were also liberal in that particular.\n\nJames Manning was sworn in as the College's first president in 1765 and served until 1791. In 1770, the College moved from Warren, Rhode Island to the crest of College Hill overlooking Providence. Solomon Drowne, a freshman in the class of 1773, wrote in his diary on March 26, 1770:\n\nThis day the Committee for settling the spot for the College, met at the New-Brick School House, when it was determined it should be set on ye Hill opposite Mr. John Jenkes; up the Presbyterian Lane.\n\nPresbyterian Lane is the present College Street. The eight-acre site had been purchased in two parcels by the Corporation for £219, mainly from Moses Brown and John Brown, the parcels having \"formed a part of the original home lots of their ancestor, Chad Brown, and of George Rickard, who bought them from the Indians.\" University Hall was known as \"The College Edifice\" until 1823; it was modelled on Nassau Hall at the College of New Jersey. Its construction was managed by the firm of Nicholas Brown and Company, which spent £2,844 in the first year building the College Edifice and the adjacent President's House.\n\nNicholas Brown, his son Nicholas Brown, Jr. (class of 1786), John Brown, Joseph Brown, and Moses Brown were all instrumental in moving the College to Providence and securing its endowment. Joseph became a professor of natural philosophy at the College; John served as its treasurer from 1775 to 1796; and Nicholas Junior succeeded his uncle as treasurer from 1796 to 1825.\n\nOn September 8, 1803, the Corporation voted, \"That the donation of $5000 Dollars, if made to this College within one Year from the late Commencement, shall entitle the donor to name the College.\" That appeal was answered by College treasurer Nicholas Brown Junior in a letter dated September 6, 1804, and the Corporation honored its promise. \"In gratitude to Mr. Brown, the Corporation at the same meeting voted, 'That this College be called and known in all future time by the Name of Brown University'.\" Over the years, the benefactions of Nicholas Brown, Jr. totaled nearly $160,000, an enormous sum for that period, and included the buildings Hope College (1821–22) and Manning Hall (1834-35).\n\nIt is sometimes erroneously supposed that Brown University was named after John Brown, whose commercial activity included the transportation of African slaves. In fact, Brown University was named for Nicholas Brown, Jr.—philanthropist, founder of the Providence Athenaeum, co-founder of Butler Hospital, and an abolitionist. Nicholas Brown, Jr. became a financier of the movement under the guidance of his uncle Moses Brown, one of the leading abolitionists of his day.\n\nThe College library was moved out of Providence for safekeeping in the fall of 1776, with British vessels patrolling Narragansett Bay. On December 7, 1776, six thousand British and Hessian troops sailed into Newport harbor under the command of Sir Peter Parker. College president Manning said in a letter written after the war:\n\nThe royal Army landed on Rhode Island & took possession of the same: This brought their Camp in plain View from the College with the naked Eye; upon which the Country flew to Arms & marched for Providence, there, unprovided with Barracks they marched into the College & dispossessed the Students, about 40 in Number.\n\n\"In the claim for damages presented by the Corporation to the United States government,\" says the University historian, \"it is stated that the American troops used it for barracks and hospital from December 10, 1776, to April 20, 1780, and that the French troops used it for a hospital from June 26, 1780, to May 27, 1782.\" The French troops were those of the Comte de Rochambeau.\n\nIn 1850, Brown President Francis Wayland wrote: \"The various courses should be so arranged that, insofar as practicable, every student might study what he chose, all that he chose, and nothing but what he chose.\" The New Curriculum was adopted in 1969; it is a milestone in the University's history and is seen as the realization of Wayland's vision.\n\nThe curriculum was the result of a paper written by Ira Magaziner and Elliot Maxwell entitled \"Draft of a Working Paper for Education at Brown University.\" The paper came out of a year-long Group Independent Study Project (GISP) involving 80 students and 15 professors. The GISP was inspired by student-initiated experimental schools, especially San Francisco State College, and sought ways to \"put students at the center of their education\" and \"teach students how to think rather than just teaching facts.\"\n\nThe paper made concrete proposals for the new curriculum, including interdisciplinary freshman-year courses that would introduce \"modes of thought,\" with instruction from faculty brought together from different disciplines. The aim was to transform the traditional survey course—often experienced passively by first-year students—into a more engaging process, an investigation of the intellectual and philosophical connections between disciplines. A grading option of Satisfactory/No Credit was introduced to encourage students to try courses outside their grade-point comfort zone. In practice, this grading innovation of the New Curriculum has been its most successful component, sometimes misunderstood and mischaracterized but responsible for uncounted career-changing decisions in the decades since its adoption—studio art swapped for neuroscience, biology swapped for anthropology, mathematics swapped for playwriting (and Pulitzer Prizes).\nUniversity president Ray Heffner appointed the Special Committee on Curricular Philosophy in the spring of 1969, following student rallies in support of reform, and gave them the task of developing specific reforms. The resulting report was called the Maeder Report after its committee chairman and was presented to the faculty, which voted the New Curriculum into existence on May 7, 1969. Its key features included:\n\nThe Modes of Thought course was a key component in the original conception of the New Curriculum; it was discontinued early on, but all of the other elements are still in place. In 2006, the reintroduction of plus/minus grading was broached by persons concerned about grade inflation. The idea was rejected by the College Curriculum Council after canvassing alumni, faculty, and students, including the original authors of the Magaziner-Maxwell Report.\n\nBrown University's coat of arms is a white field divided into four sectors by a red cross; within each sector is an open book. Above the shield is a crest consisting of the upper half of a sun in splendor among the clouds atop a red and white torse.\nThe sun and clouds represent \"learning piercing the clouds of ignorance.\" The cross is believed to be a Saint George's Cross, and the open books represent learning.\n\nBrown is the largest institutional landowner in Providence, with properties on College Hill and in the Jewelry District. The College Hill campus was built contemporarily with the eighteenth- and nineteenth-century precincts that surround it, so that University buildings blend with the architectural fabric of the city. The only indicator of \"campus\" is a brick and wrought-iron fence on Prospect, George, and Waterman streets, enclosing the College Green and Front Green. The character of Brown's urban campus is, then, European organic rather than American landscaped.\n\nThe main campus, comprising 235 buildings and , is on College Hill in Providence's East Side. It is reached from downtown principally by three extremely steep streets—College, Waterman, and Angell—which run through the Benefit Street historic district and the campus of the Rhode Island School of Design. College Street, culminating with Van Wickle Gates at the top of the hill, is especially beautiful, and is the setting for the Convocation and Commencement processions.\n\nVan Wickle Gates\n\nThe Van Wickle Gates, dedicated on June 18, 1901, have a pair of smaller side gates that are open year-round, and a large central gate that is opened two days a year for Convocation and Commencement. At Convocation the gate opens inward to admit the procession of new students. At Commencement the gate opens outward for the procession of graduates. A Brown superstition is that students who walk through the central gate a second time prematurely will not graduate, although walking backwards is said to cancel the hex. Members of the Brown University Band famously flout the superstition by walking through the gate three times too many, as they annually play their role in the Commencement parade.\nThe core green spaces of the main campus are the Front (or \"Quiet\") Green, the College (or \"Main\") Green, and the Ruth J. Simmons Quadrangle (until 2012 called Lincoln Field). The old buildings on these three greens are the most photographed.\n\nAdjacent to this older campus are, to the south, academic buildings and residential quadrangles, including Wriston, Keeney, and Gregorian quadrangles; to the east, Sciences Park occupying two city blocks; to the north, connected to Simmons Quadrangle by The Walk, academic and residential precincts, including the life sciences complex and the Pembroke Campus; and to the west, on the slope of College Hill, academic buildings, including List Art Center and the Hay and Rockefeller libraries. Also on the slope of College Hill, contiguous with Brown, is the campus of the Rhode Island School of Design.\nJohn Hay Library\n\nThe John Hay Library is the second oldest library on campus. It was opened in 1910 and named for John Hay (class of 1858, private secretary to Abraham Lincoln and Secretary of State under two Presidents) at the request of his friend Andrew Carnegie, who contributed half of the $300,000 cost of the building. It is now the repository of the University's archives, rare books and manuscripts, and special collections. Noteworthy among the latter are the Anne S. K. Brown Military Collection (described as \"the foremost American collection of material devoted to the history and iconography of soldiers and soldiering\"), the Harris Collection of American Poetry and Plays (described as \"the largest and most comprehensive collection of its kind in any research library\"), the Lownes Collection of the History of Science (described as \"one of the three most important private collections of books of science in America\"), and (for popularity of requests) the papers of H.P. Lovecraft. The Hay Library is home to one of the broadest collections of incunabula (15th-century printed books) in the Americas, as well as such rarities as the manuscript of Orwell's \"Nineteen Eighty-Four\" and a Shakespeare First Folio. There are also three books bound in human skin.\nJohn Carter Brown Library\n\nThe John Carter Brown Library, founded in 1846, is administered separately from the University, but has been located on the Main Green of the campus since 1904. It is generally regarded as the world's leading collection of primary historical sources pertaining to the Americas before 1825. It houses a very large percentage of the titles published before that date about the discovery, settlement, history, and natural history of the New World. The \"JCB\", as it is known, published the 29-volume \"Bibliotheca Americana\", a principal bibliography in the field. Typical of its noteworthy holdings is the best preserved of the eleven surviving copies of the Bay Psalm Book the earliest extant book printed in British North America and the most expensive printed book in the world. There is also a very fine Shakespeare First Folio, added to the collection by John Carter Brown's widow (a Shakespeare enthusiast) on the grounds that it includes \"The Tempest\", a play set in the New World. The JCB holdings comprise more than 50,000 early titles and about 16,000 modern books, as well as prints, manuscripts, maps, and other items in the library's specialty.\n\nThe exhibition galleries of the Haffenreffer Museum of Anthropology, Brown's teaching museum, are located in Manning Hall on the campus's main green. Its one million artifacts, available for research and educational purposes, are located at its Collections Research Center in Bristol, RI. The museum's goal is to inspire creative and critical thinking about culture by fostering interdisciplinary understanding of the material world. It provides opportunities for faculty and students to work with collections and the public, teaching through objects and programs in classrooms and exhibitions. The museum sponsors lectures and events in all areas of anthropology, and also runs an extensive program of outreach to local schools.\n\nThe \"Walk\" connects Pembroke Campus to the main campus. It is a succession of green spaces extending from Ruth Simmons Quadrangle (Lincoln Field) in the south to the Pembroke College monument on Meeting Street in the north. It is bordered by departmental buildings and the Granoff Center for the Creative Arts. A focal point of The Walk will be the Maya Lin-designed water-circulating topographical sculpture of Narragansett Bay, to be installed in 2014 next to the Institute for the Study of Environment and Society.\n\nThe Women's College in Brown University, known as Pembroke College, was founded in October 1891. When it merged with Brown in 1971, the Pembroke Campus was absorbed into the Brown campus. The Pembroke campus is centered on a quadrangle that fronts on Meeting Street, where a garden and monument—with scale-model of the quadrangle in bronze—compose the formal entry to the campus. The Pembroke campus is among the most pleasing spaces at Brown, with noteworthy examples of Victorian and Georgian architecture. The west side of the quadrangle comprises Pembroke Hall (1897), Smith-Buonanno Hall (1907, formerly Pembroke Gymnasium), and Metcalf Hall (1919); the east side comprises Alumnae Hall (1927) and Miller Hall (1910); the quadrangle culminates on the north with Andrews Hall (1947) and its terrace and garden. Pembroke Hall, originally a classroom building and library, now houses the Cogut Center for the Humanities.\nEast Campus, centered on Hope and Charlesfield streets, was originally the site of Bryant University. In 1969, as Bryant was preparing to move to Smithfield, Rhode Island, Brown bought their Providence campus for $5 million. This expanded the Brown campus by and 26 buildings, included several historic houses, notably the Isaac Gifford Ladd house, built 1850 (now Brown's Orwig Music Library), and the Robert Taft House, built 1895 (now King House). The area was named East Campus in 1971.\n\nThayer Street runs through Brown's main campus, north to south, and is College Hill's reduced-scale counterpart to Harvard Square or Berkeley's Telegraph Avenue. Restaurants, cafes, bistros, tavernas, pubs, bookstores, second-hand shops, and the like abound. Tourists, people-watchers, buskers, and students from Providence's six colleges make the scene. Half a mile south of campus is Thayer Street's hipper cousin, Wickenden Street. More picturesque and with older architecture, it features galleries, pubs, specialty shops, artist-supply stores, and a regionally famous coffee shop that doubles as a film set (for Woody Allen and others).\n\nBrown Stadium, built in 1925 and home to the football team, is located approximately a mile to the northeast of the main campus. Marston Boathouse, the home of the crew teams, lies on the Blackstone/Seekonk River, to the southeast of campus. Brown's Warren Alpert Medical School is situated in the historic Jewelry District of Providence, near the medical campus of Brown's teaching hospitals, Rhode Island Hospital, Women and Infants Hospital, and Hasbro Children's Hospital. Other University research facilities in the Jewelry District include the Laboratories for Molecular Medicine.\n\nBrown's School of Public Health occupies a landmark modernist building overlooking Memorial Park on the Providence Riverwalk. Brown also owns the Mount Hope Grant in Bristol, Rhode Island, an important Native American and King Philip's War site. Brown's Haffenreffer Museum of Anthropology Collection Research Center, particularly strong in Native American items, is located in the Mount Hope Grant.\n\nBrown's current president Christina Hull Paxson took office in 2012. She had previously been dean of the Woodrow Wilson School at Princeton University and a past-chair of Princeton's economics department. In 2014 and 2015, Paxson presided over the year-long celebration of the 250th anniversary of Brown's founding. Her immediate predecessor as president was Ruth J. Simmons, the first African American president of an Ivy League institution. Simmons will remain at Brown as a professor of Comparative Literature and Africana Studies.\n\nFounded in 1764, the College is the oldest school of Brown University. About 6,400 undergraduate students are currently enrolled in the College, and 79 concentrations (majors) are offered. Completed concentrations of undergraduates by area are social sciences 42 percent, humanities 26 percent, life sciences 17 percent, and physical sciences 14 percent. The concentrations with the greatest number of students are Biology, History, and International Relations. Brown is one of the few schools in the United States with an undergraduate concentration (major) in Egyptology. Undergraduates can also design an independent concentration if the existing programs do not align with their curricular focus.\n\n35 percent of undergraduates pursue graduate or professional study immediately, 60 percent within 5 years, and 80 percent within 10 years. For the Class of 1998, 75 percent of all graduates have since enrolled in a graduate or professional degree program. The degrees acquired were doctoral 22 percent, master's 35 percent, medicine 28 percent, and law 14 percent.\n\nThe highest fields of employment for graduates of the College are business 36 percent, education 19 percent, health/medical 6 percent, arts 6 percent, government 6 percent, and communications/media 5 percent.\n\nThe language of the College Charter has been interpreted as discouraging the establishment professional schools. Brown and Princeton are the only Ivy League colleges with neither business school nor law school.\n\nBrown's near neighbor on College Hill is the Rhode Island School of Design (RISD), America's top-ranked art college. Brown and RISD students can cross-register at the two institutions, with Brown students permitted to take as many as four courses at RISD that count towards a Brown degree. The two institutions partner to provide various student-life services and the two student bodies compose a synergy in the College Hill cultural scene.\n\nThe Brown/RISD Dual Degree Program, among the most selective in the country, offered admission to 17 of the 512 applicants for the class entering in autumn 2015, an acceptance rate of 3.3 percent. It combines the complementary strengths of the two institutions, integrating studio art at RISD with the entire spectrum of Brown's departmental offerings. Students are admitted to the Dual Degree Program for a course lasting five years and culminating in both the Bachelor of Arts (A.B.) degree from Brown and the Bachelor of Fine Arts (B.F.A.) degree from RISD. Prospective students must apply to the two schools separately and be accepted by separate admissions committees. Their application must then be approved by a third Brown/RISD joint committee.\n\nAdmitted students spend the first year in residence at RISD completing its \"foundation course,\" and the second year in residence at Brown. Another year at each school ensues, with the fifth year spent according to the student's electives. Program participants are noted for their creative and original approach to cross-disciplinary opportunities, combining, for example, industrial design with engineering, or anatomical illustration with human biology, or philosophy with sculpture, or architecture with urban studies. An annual \"BRDD Exhibition\" is a well-publicized and heavily attended event, drawing interest and attendees from the wider world of industry, design, the media, and the fine arts.\n\nBrown's theatre and playwriting programs are among the best-regarded in the country. Since 2003 eight different Brown graduates have either won (four times) or been nominated for (six times) the Pulitzer Prize—including winners Lynn Nottage '86 (twice—2009, 2017), Ayad Akhtar '93, Nilo Cruz '94, and Quiara Alegría Hudes '04; and nominees Sarah Ruhl '97 (twice), Gina Gionfriddo '97 (twice), Stephen Karam '02, and Jordan Harrison '03. In \"American Theater\" magazine's 2009 ranking of the most-produced American plays, Brown graduates occupied four of the top five places—Peter Nachtrieb '97, Rachel Sheinkin '89, Sarah Ruhl '97, and Stephen Karam '02.\n\nThe undergraduate concentration (major) encompasses programs in theatre history, performance theory, playwriting, dramaturgy, acting, directing, dance, speech, and technical production. Applications for doctoral and master's degree programs are made through the University Graduate School. Master's degrees in acting and directing are pursued in conjunction with the Rep MFA program, which partners with one of the country's great regional theatres, Trinity Repertory Company, home of the last longstanding resident acting company in the country. Trinity Rep's present artistic director Curt Columbus succeeded Oskar Eustis in 2006, when Eustis was chosen to lead New York's Public Theater.\n\nThe many performance spaces available to Brown students include the Chace and Dowling theaters at Trinity Rep; the McCormack Family, Lee Strasberg, Rites and Reason, Ashamu Dance, Stuart, and Leeds theatres in University departments; the Upstairs Space and Downstairs Space belonging to the wholly student-run Production Workshop; and Alumnae Hall, used by Brown University Gilbert & Sullivan and by Brown Opera Productions. Production design courses utilize the John Street Studio of Eugene Lee, three-time Tony Award-winner.\n\nWriting at Brown—fiction, non-fiction, poetry, playwriting, screenwriting, electronic writing, mixed media, and the undergraduate writing proficiency requirement—is catered for by various centers and degree programs, and a faculty that has long included nationally and internationally known authors. The undergraduate concentration (major) in literary arts offers courses in fiction, poetry, screenwriting, literary hypermedia, and translation. Graduate programs include the fiction and poetry MFA writing programs in the literary arts department, and the MFA playwriting program in the theatre arts and performance studies department. The non-fiction writing program is offered in the English department. Screenwriting and cinema narrativity courses are offered in the departments of literary arts and modern culture and media. The undergraduate writing proficiency requirement is supported by the Writing Center.\n\nAlumni authors take their degrees across the spectrum of degree concentrations, but a gauge of the strength of writing at Brown is the number of major national writing prizes won. To note only winners since the year 2000: Pulitzer Prize for Fiction-winners Jeffrey Eugenides '82 (2003) and Marilynne Robinson '66 (2005); British Orange Prize-winners Marilynne Robinson '66 (2009) and Madeline Miller '00 (2012); Pulitzer Prize for Drama-winners Nilo Cruz '94 (2003), Lynn Nottage '86 (twice, 2009, 2017), Quiara Alegría Hudes '04 (2012), and Ayad Akhtar '93 (2013); Pulitzer Prize for Biography-winner David Kertzer '69 (2015); Pulitzer Prize for Journalism-winners James Risen '77 (twice, 2002, 2006), Mark Maremont '80 (twice, 2003, 2007), Gareth Cook '91 (2005), Peter Kovacs '77 (2006), Stephanie Grace '86 (2006), Mary Swerczek '98 (2006), Jane B. Spencer '99 (2006), Usha Lee McFarling '89 (2007), James Bandler '89 (2007), Amy Goldstein '75 (2009), and David Rohde '90 (twice, 1996, 2009), as well as Pulitzer Prize for Poetry-winner Peter Balakian PhD '80.\n\nTeaching of computer science began at Brown in 1956 when an IBM machine was installed and computing courses were offered through the departments of Economics and Applied Mathematics. In January 1958 an IBM650 was added, the only one of its type between Hartford and Boston. In 1960 Brown's first computer building, designed by Philip Johnson, was opened on George Street and an IBM7070 computer installed the next year. It was given full Departmental status in 1979. In 2009, IBM and Brown announced the installation of a supercomputer (by teraflops standards), the most powerful in the southeastern New England region.\nThe Hypertext Editing Systems, HES and FRESS, were invented in the 1960s at Brown by Andries van Dam, Ted Nelson, and Bob Wallace, with Nelson coining the word \"hypertext\". Van Dam's students were instrumental in the origin of the XML, XSLT, and related Web standards. Brown alumni who have distinguished themselves in the computer sciences and industry are listed in the Notable people section, below. They include a principal architect of the Classic Mac OS, a principal architect of the Intel 80386 microprocessor line, the Microsoft Windows 95 project chief, a CEO of Apple, the current head of the MIT Computer Science and Artificial Intelligence Laboratory, the inaugural chair of the Computing Community Consortium, and design chiefs at Pixar and Industrial Light & Magic, protegees of graphics guru Andries van Dam. The character \"Andy\" in the animated film \"Toy Story\" is taken to be an homage to Van Dam from his students employed at Pixar. Van Dam denies this, but a copy of his book (\"Computer Graphics: Principles and Practice\") appears on Andy's bookshelf in the film. Brown computer science graduate and \"Heroes\" actor Masi Oka '97, was an animator at Industrial Light & Magic.\n\nThe department today is home to The CAVE. This project is a virtual reality room used for everything from three-dimensional drawing classes to tours of the circulatory system for medical students. In 2000 students from Brown's Technology House converted the south face of the Sciences Library into a Tetris game, the first high-rise-building Tetris ever attempted. Code named La Bastille, the game used a personal computer running Linux, a radio-frequency video game controller, eleven circuit boards, a 12-story data network, and over 10,000 Christmas lights.\n\nThe Joukowsky Institute for Archaeology and the Ancient World pursues fieldwork and excavations, regional surveys, and academic study of the archaeology and art of the ancient Mediterranean, Egypt, and Western Asia from the Levant to the Caucasus. The Institute has a very active fieldwork profile, with faculty-led excavations and regional surveys presently in Petra, Jordan, in West-Central Turkey, at Abydos in Egypt, and in Sudan, Italy, Mexico, Guatemala, Montserrat in the West Indies, and Providence, Rhode Island.\n\nThe Institute's faculty includes cross-appointments from the departments of Egyptology, Assyriology, Classics, Anthropology, and History of Art and Architecture. Faculty research and publication areas include Greek and Roman art and architecture, landscape archaeology, urban and religious architecture of the Levant, Roman provincial studies, the Aegean Bronze Age, and the archaeology of the Caucasus. The Institute offers visiting teaching appointments and postdoctoral fellowships which have, in recent years, included Near Eastern Archaeology and Art, Classical Archaeology and Art, Islamic Archaeology and Art, and Archaeology and Media Studies.\n\nEgyptology and Assyriology\n\nFacing the Joukowsky Institute, across the Front Green, is the Department of Egyptology and Assyriology, formed in 2006 by the merger of Brown's renowned departments of Egyptology and History of Mathematics. It is one of only a handful of such departments in the United States. The curricular focus is on three principal areas: Egyptology (the study of the ancient languages, history, and culture of Egypt), Assyriology (the study of the ancient lands of present-day Iraq, Syria, and Turkey), and the history of the ancient exact sciences (astronomy, astrology, and mathematics). Many courses in the department are open to all Brown undergraduates without prerequisite, and include archaeology, languages, history, and Egyptian and Mesopotamian religions, literature, and science. Students concentrating (majoring) in the department choose a track of either Egyptology or Assyriology. Graduate level study comprises three tracks to the doctoral degree: Egyptology, Assyriology, or the History of the Exact Sciences in Antiquity.\n\nThe Watson Institute for International and Public Affairs is a center for the study of global issues and public affairs and is one of the leading institutes of its type in the country. It occupies an architecturally distinctive building designed by Uruguayan architect Rafael Viñoly. The Institute was initially endowed by Thomas Watson, Jr., Brown class of 1937, former Ambassador to the Soviet Union, and longtime president of IBM. Institute faculty includes, or formerly included, Italian prime minister and European Commission president Romano Prodi, Brazilian president Fernando Henrique Cardoso, Chilean president Ricardo Lagos Escobar, Mexican novelist and statesman Carlos Fuentes, Brazilian statesman and United Nations commission head Paulo Sérgio Pinheiro, Indian foreign minister and ambassador to the United States Nirupama Rao, American diplomat and Dayton Peace Accords author Richard Holbrooke (Brown '62), and Sergei Khrushchev, editor of the papers of his father Nikita Khrushchev, leader of the Soviet Union.\n\nThe Institute's curricular interest is organized into the principal themes of development, security, and governance—with further focuses on globalization, economic uncertainty, security threats, environmental degradation, and poverty. Three Brown undergraduate concentrations (majors) are hosted by the Watson Institute—Development Studies, International Relations, and Public Policy. Graduate programs offered at the Watson Institute include the Graduate Program in Development (Ph.D.) and the Public Policy Program (M.P.A). The Institute also offers Post Doctoral, professional development and global outreach programming. In support of these programs, the Institute houses various centers, including the Brazil Initiative, Brown-India Initiative, China Initiative, Middle East Studies center, The Center for Latin American and Caribbean Studies (CLACS) and the Taubman Center for Public Policy. In recent years, the most internationally cited product of the Watson Institute has been its Costs of War Project, first released in 2011 and continuously updated. The Project comprises a team of economists, anthropologists, political scientists, legal experts, and physicians, and seeks to calculate the economic costs, human casualties, and impact on civil liberties of the wars in Iraq, Afghanistan, and Pakistan since 2001.\n\nEstablished in 1847, Brown's engineering program is the oldest in the Ivy League and the third oldest civilian engineering program in the country, preceded only by Rensselaer Polytechnic Institute (1824) and Union College (1845). In 1916 the departments of electrical, mechanical, and civil engineering were merged into a Division of Engineering, and in 2010 the division was elevated to a School of Engineering.\n\nEngineering at Brown is especially interdisciplinary. The School is organized without the traditional departments or boundaries found at most schools, and follows a model of connectivity between disciplines—including biology, medicine, physics, chemistry, computer science, the humanities and the social sciences. The School practices an innovative clustering of faculties in which engineers team with non-engineers to bring a convergence of ideas.\n\nSince 2009 Brown has developed an Executive MBA program in conjunction with one of the leading Business Schools in Europe; IE Business School in Madrid. This relationship has since strengthened resulting in both institutions offering a dual degree program. In this partnership, Brown provides its traditional coursework while IE provides most of the business-related subjects making a differentiated alternative program to other Ivy League's EMBAs. The cohort typically consists of 25-30 EMBA candidates from some 20 countries.\nClasses are held in Providence, Madrid, Cape Town and Online.\n\nThe Pembroke Center for Teaching and Research on Women was established at Brown in 1981 by Joan Wallach Scott as a research center on gender. It was named for Pembroke College, the former women's coordinate college at Brown, and is affiliated with Brown's Sarah Doyle Women's Center. It supports the undergraduate concentration in Gender and Sexuality Studies, post-doctoral research fellowships, the annual Pembroke Seminar, and other academic programs. The Center also manages various collections, archives, and resources, including the Elizabeth Weed Feminist Theory Papers and the Christine Dunlap Farnham Archive.\n\nEstablished in 1887, the Graduate School has around 2,000 students studying over 50 disciplines. 20 different master's degrees are offered as well as Ph.D. degrees in over 40 subjects ranging from applied mathematics to public policy. Overall, admission to the Graduate School is most competitive with an acceptance rate of about 18 percent.\n\nThe University's medical program started in 1811, but the school was suspended by President Wayland in 1827 after the program's faculty declined to live on campus (a new requirement under Wayland). In 1975, the first M.D. degrees from the new Program in Medicine were awarded to a graduating class of 58 students. In 1991, the school was officially renamed the Brown University School of Medicine, then renamed once more to Brown Medical School in October 2000. In January 2007, Warren Alpert donated $100 million to Brown Medical School, in recognition of which its name was changed to the Warren Alpert Medical School of Brown University.\n\nIn 2014 \"U.S. News & World Report\" ranked Brown's medical school the 5th most selective in the country, with an acceptance rate of 2.9 percent.\n\"U.S. News\" ranks it 29th for research and 28th in primary care.\n\nThe medical school is known especially for its eight-year Program in Liberal Medical Education (PLME), inaugurated in 1984. One of the most selective and renowned programs of its type in the country, it offered admission to 90 of the 2,290 applicants for the class entering in autumn 2015, an acceptance rate of 3.9 percent. Since 1976, the Early Identification Program (EIP) has encouraged Rhode Island residents to pursue careers in medicine by recruiting sophomores from Providence College, Rhode Island College, the University of Rhode Island, and Tougaloo College. In 2004, the school once again began to accept applications from premedical students at other colleges and universities via AMCAS like most other medical schools. The medical school also offers combined degree programs leading to the M.D./Ph.D., M.D./M.P.H. and M.D./M.P.P. degrees.\n\nThe Marine Biological Laboratory (MBL) is an independent research institution established in 1882 at Woods Hole, Massachusetts. The laboratory is linked to 54 current or past Nobel Laureates who have been research or teaching faculty. Since 2005 the MBL and Brown have collaborated in a Ph.D. program in biological and environmental sciences that combines faculty at both institutions, including the faculties of the Ecosystems Center, the Bay Paul Center, the Program in Cellular Dynamics, and the Marine Resources Center.\n\nFor the undergraduate class of 2021, Brown received 32,724 applications, the largest applicant pool in the University's history, of whom 2,722 were accepted, for an acceptance rate of 8.3%, the lowest in university history. Additionally, for the academic year 2015-16 there were 1,834 transfer applicants, of whom 8.9% were accepted, with an SAT range of 2180-2330, ACT range of 31-34, and average college GPA of 3.85. In 2013 the Graduate School accepted 17 percent of 9,215 applicants. In 2014, \"U.S. News\" ranked Brown's Warren Alpert Medical School the 5th most selective in the country, with an acceptance rate of 2.9 percent.\n\nBrown admission policy is stipulated need-blind for all domestic applicants. Brown's financial aid basics website, in August 2014, stated: \"For families (including both parents) with a total income below $60,000, and assets less than $100,000, no parent contribution is calculated towards the Expected Family Contribution (EFC). For families with total income below $100,000, the loan component of the financial aid award is replaced with additional scholarship.\" In 2013-14, the program awarded need-based scholarships worth $95 million. The average need-based award for the class of 2017 was $43,427.\n\nBrown has committed to \"minimize its energy use, reduce negative environmental impacts and promote environmental stewardship.\" The Energy and Environmental Advisory Committee has developed a set of ambitious goals for the university to reduce its carbon emissions and eventually achieve carbon neutrality. The \"Brown is Green\" website collects information about Brown's progress toward greenhouse gas emissions reductions and related campus initiatives, such as student groups, courses, and research. Brown's grade of A-minus was the top one issued in the 2009 report of the Sustainable Endowments Institute (no A-grade was issued).\nBrown has a number of active environmental leadership groups on campus. These groups have begun a number of campus-wide environmental initiatives—including promoting the reduction of supply and demand of bottled water and investigating a composting program.\n\nBrown is a member of the Ivy League athletic conference, which is categorized as a Division I (top level) conference of the National Collegiate Athletic Association (NCAA). The Brown Bears are the third largest university sports program in the United States, sponsoring 38 varsity intercollegiate teams (Harvard sponsors 42 and Princeton 39). Brown's athletic program is one of the \"U.S. News & World Report\" top 20—the \"College Sports Honor Roll\"—based on breadth of program and athletes' graduation rates. Brown's newest varsity team is women's rugby, promoted from club-sport status in 2014.\n\nBrown women's rowing has won 7 national titles in the last 14 years. Brown men's rowing perennially finishes in the top 5 in the nation, most recently winning silver, bronze, and silver in the national championship races of 2012, 2013, and 2014. The men's and women's crews have also won championship trophies at the Henley Royal Regatta and the Henley Women's Regatta. Brown's men's soccer is consistently ranked in the top 20, and has won 18 Ivy League titles overall; recent soccer graduates play professionally in Major League Soccer and overseas. Brown football, under its most successful coach historically, Phil Estes, won Ivy League championships in 1999, 2005, and 2008. (Brown football's reemergence is credited to its 1976 Ivy League championship team, \"The Magnificent Andersons,\" so named for its coach, John Anderson.) High-profile alumni of the football program include Houston Texans head coach Bill O'Brien; former Penn State football coach Joe Paterno, Heisman Trophy namesake John W. Heisman, and Pollard Award namesake Fritz Pollard. The Men's Lacrosse team also has a long and storied history. Brown women's gymnastics won the Ivy League tournament in 2013 and 2014. Brown varsity equestrian has won the Ivy League championship several times. Brown also supports competitive intercollegiate club sports, including sailing and ultimate frisbee. The men's ultimate team, Brownian Motion, has twice won the national championship, in 2000 and 2005.\n\nThe first intercollegiate ice hockey game in America was played between Brown and Harvard on January 19, 1898. The first university rowing regatta larger than a dual-meet was held between Brown, Harvard, and Yale at Lake Quinsigamond in Massachusetts on July 26, 1859\n\nIn 2014, Brown University tied with the University of Connecticut for the highest number of reported rapes in the nation, with its \"total of reports of rape\" on their main campus standing at 43.\n\nThe weekend includes an annual spring concert festival which has featured numerous famous artists, including Bob Dylan, Childish Gambino, Bruce Springsteen, Bhudda, Chance the Rapper, Vampire Weekend, and Fetty Wap.\n\nAbout 12 percent of Brown students are in fraternities and sororities. There are 11 residential Greek houses: six fraternities (Beta Rho Pi, Delta Phi, Delta Tau, Phi Kappa Psi, Sigma Chi, and Theta Delta Chi; three sororities (Alpha Chi Omega, Kappa Alpha Theta, and Kappa Delta), one co-ed house (Zeta Delta Xi), and one co-ed literary society (Alpha Delta Phi). Phi Sigma Kappa fraternity was present on campus from 1906 to 1939, but was unable to reactivate after World War II due to wartime losses. All recognized Greek-letter organizations are located on campus in Wriston Quadrangle in university-owned housing. They are overseen by the Greek Council.\n\nAn alternative to Greek-letter organizations are the program houses organized by themes. As with Greek houses, the residents of program houses select their new members, usually at the start of the spring semester. Examples of program houses are St. Anthony Hall (located in King House), Buxton International House, the Machado French/Hispanic House, Technology House, Harambee (African culture) House, Social Action House and Interfaith House.\n\nCurrently, there are three student cooperative houses at Brown. Two of them, Watermyn and Finlandia on Waterman Street, are owned by the Brown Association for Cooperative Housing (BACH), a non-profit corporation owned by its members. The third co-op, West House, is located in a Brown-owned house on Brown Street. The three organizations run a vegetarian co-op for the larger community.\n\nAll students not in program housing enter a lottery for general housing. Students form groups and are assigned time slots during which they can pick among the remaining housing options.\n\nThe earliest societies at Brown were devoted to oration and debate. The Pronouncing Society is mentioned in the diary of Solomon Drowne, class of 1773, who was voted its president in 1771. It seems to have disappeared during the Revolutionary War. We next hear of the Misokosmian Society, founded in 1794 and renamed the Philermenian Society in 1798. This was effectively a secret society with membership limited to 45. It met fortnightly to hear speeches and debate and thrived until the Civil War; in 1821 its library held 1594 volumes. In 1799 a chapter of the Philandrian Society, also secret, was established at the College. In 1806 the United Brothers was formed as an egalitarian alternative to the Philermenian Society. \"These two great rivals,\" says the University historian, \"divided the student body between them for many years, surviving into the days of President Sears. A tincture of political controversy sharpened their rivalry, the older society inclining to the aristocratic Federals, the younger to the Republicans, the democrats of that day. ... The students continuing to increase in number, they outran the constitutional limits of both societies, and a third, the Franklin Society, was established in 1824; it never had the vitality of the other two, however, and died after ten years.\" Other nineteenth century clubs and societies, too numerous to treat here, are described in Bronson's history of the University.\nThe Cammarian Club—founded in 1893 and taking its name from the Latin for lobster, its members' favorite dinner food—was at first a semi-secret society which \"tapped\" 15 seniors each year. In 1915, self-perpetuating membership gave way to popular election by the student body, and thenceforward the Club served as the \"de facto\" undergraduate student government. In 1971, unaccountably, it voted the name Cammarian Club out of existence, thereby amputating its tradition and longevity. The successor and present-day organization is the generically-named Undergraduate Council of Students.\n\nSocietas Domi Pacificae, known colloquially as \"Pacifica House,\" is a present-day, self-described secret society, which nonetheless publishes a website and an email address. It claims a continuous line of descent from the Franklin Society of 1824, citing a supposed intermediary \"Franklin Society\" traceable in the nineteenth century. But the intermediary turns out to be, on closer inspection, the well-known Providence Franklin Society, a civic organization unconnected to Brown whose origins and activity are well-documented. It was founded in 1821 by merchants William Grinnell and Joseph Balch, Jr., and chartered by the General Assembly in January 1823. The \"Pacifica House\" account of this (conflated) Franklin Society cites published mentions of it in 1859, 1876, and 1883. But the first of these (Rhees 1859, see footnote \"infra\") is merely a sketch of the 1824 Brown organization; the second (Stockwell 1876) is a reference-book article on the Providence Franklin Society itself; and the third is the Providence Franklin Society's own publication, which the \"Pacifica House\" reference mis-ascribes to the \"Franklin Society,\" dropping the word \"Providence.\"\n\nThere are over 300 registered student organizations on campus with diverse interests. The Student Activities Fair, during the orientation program, provides first-year students the opportunity to become acquainted with the wide range of organizations. A sample of organizations includes:\nBrown University has several resource centers on campus. The centers often act as sources of support as well as safe spaces for students to explore certain aspects of their identity. Additionally, the centers often provide physical spaces for students to study and have meetings. Although most centers are identity-focused, some provide academic support as well.\n\nThe Brown Center for Students of Color (BCSC) is a space that provides support for students of color. Established in 1972 at the demand of student protests, the BCSC encourages students to engage in critical dialogue, develop leadership skills, and promote social justice. The center houses various programs for students to share their knowledge and engage in discussion. Programs include the Third World Transition Program, the Minority Peer Counselor Program, the Heritage Series, and other student-led initiatives. Additionally, the BCSC hopes to foster community among the students it serves by providing spaces for students to meet and study.\n\nThe Sarah Doyle Women's Center aims to provide a space for members of the Brown community to examine and explore issues surrounding gender. The center was named after one of the first women to attend Brown University, Sarah Doyle. The center emphasizes intersectionality in its conversations on gender, encouraging people to see gender as present and relevant in various aspects of life. The center hosts programs and workshops in order to facilitate dialogue and provide resources for students, faculty, and staff.\n\nOther centers include the LGBTQ+ Center, the First-Generation College and Low-Income Student (FLi) Center, and the Curricular Resource Center.\n\nLike many colleges, Brown mandates forced medical leaves for students expressing feelings of self-harm, depression, schizophrenia, or other forms of mental illness. Controversy arises within the college as to whether Brown follows the US Americans With Disabilities Act of 1990, with Brown's own newspaper, The Brown Daily Herald, criticizing the University's policy behind readmission from psychological medical leave. According to a 2010 article in The Brown Daily Herald, even Dean of Student Life & Chair of the Medical Leave Readmission Committee, Maria Suarez, admits that “I wish we had more resources to connect with students,” Suarez said. “The support is there, but it is student-initiated.” However, Suarez has been widely criticized within and outside the University as being non-compliant with American ADA laws, often denying students readmission for little to no reason. Moreover, the University leaves students with few guidelines on how to become readmitted; students report that very little communication is made with the University during a leave of absence, and that the University does not aid disabled students in their progress. One mentally disabled student alleges, after 5 semesters of mandatory leave, that: \"One of the administrators [Suarez], he claims, told him: \"You should consider yourself lucky because Brown's better than other schools. At least you're not getting kicked out of Brown.\"' The psychological leave readmission process takes place once every 6 months, and requires only a student's statement and a psychiatrist's or psychologist's statement of well-being, with no interview. Many students report that the process is not thorough enough to judge one's own progress in mental health, thereby violating ADA rights; the denial letters are often short and generic, with one sentence changed at most for multiple denials.\n\nIn 2014, National Science Foundation ranked Brown University 66th in the United States by research. For 2017, \"U.S. News & World Report\" ranks Brown University 85th globally.\n\nThe 2013 \"U.S. News & World Report\" rankings' peer assessment portion gives the school a score of 4.4, tied with University of Pennsylvania, Dartmouth, Northwestern, and University of Michigan.\n\nIn 2014, \"Forbes\" magazine ranked Brown seventh (between Caltech and Princeton University) on its list of \"America's Most Entrepreneurial Universities.\" The \"Forbes\" analysis looked at the ratio of \"alumni and students who have identified themselves as founders and business owners on LinkedIn\" and the total number of alumni and students.\n\nLinkedIn particularized the \"Forbes\" rankings, placing Brown third (between MIT and Princeton) among \"Best Undergraduate Universities for Software Developers at Startups.\" LinkedIn's methodology involved a career-path examination of \"millions of alumni profiles\" in its membership database.\n\nBrown ranked 7th in the country (between Princeton and Columbia) in a study of high school seniors' revealed preferences for matriculation conducted by economists at Harvard, Wharton, and Boston University, and published in 2005 by the National Bureau of Economic Research.\nThe 2008 Center for College Affordability and Productivity (CCAP) ranked Brown 5th in the country among national universities.\"\n\nBrown ranked 5th in the country in Newsweek/The Daily Beast's \"America's Brainiac Schools\"—based on the number of prestigious scholarships won (adjusted for student body size), including the Rhodes Scholarship, the Truman Scholarship, the Marshall Scholarship, the Gates Scholarship (since 2001), and the Fulbright scholarship (since 1993). Also factored in are standardized test scores, admissions rates, and students in the top 10 percent of their high school class.\n\nIn 2014, \"U.S. News\" ranked Brown's Warren Alpert Medical School the 5th most selective in the country, with an acceptance rate of 2.9 percent.\n\nIn the 2012 evaluation of MFA writing programs by \"Poets & Writers Magazine\", Brown was ranked 4th in the country, 3rd for selectivity, and 1st in the Ivy League.\n\nThe \"Forbes\" magazine annual ranking of \"America's Top Colleges 2016\"—which differs from the \"U.S. News\" by putting research universities and liberal arts colleges in a single sequence—ranked Brown 8th overall.\n\n\"U.S. News & World Report\" ranked Brown 14th in its 2017 edition. The 2013 edition had ranked Brown 4th for undergraduate teaching, tied with Yale.\n\nAs it had in 2007 and 2010, the 2011 Princeton Review email poll of college students ranked Brown 1st in the country for \"Happiest Students.\" In the 2016-17 year, Brown students received 30 Fulbright scholarships, the highest number of any institution in the United States.\n\nAlumni in politics include U.S. Secretary of State John Hay (1852), U.S. Secretary of State and Attorney General Richard Olney (1856), Chief Justice of the United States and U.S. Secretary of State Charles Evans Hughes (1881), Senator Maggie Hassan '80 of New Hampshire, Governor Jack Markell '82 of Delaware, Rhode Island Representative David Cicilline '83, and DNC Chair Tom Perez '83.\n\nProminent alums in business and finance include philanthropist John D. Rockefeller Jr. (1897), Chair of the Federal Reserve Janet Yellen '67, World Bank President Jim Yong Kim '82, Bank of America CEO Brian Moynihan '81, CNN founder and America's Cup yachtsman Ted Turner '60, IBM chairman and CEO Thomas Watson, Jr. '37, Apple Inc. CEO John Sculley '61, and magazine editor John F. Kennedy, Jr. '83.\n\nImportant figures in the history of education include the father of American public school education Horace Mann (1819), civil libertarian and Amherst College president Alexander Meiklejohn, first president of the University of South Carolina Jonathan Maxcy (1787), Bates College founder Oren B. Cheney (1836), University of Michigan president (1871–1909) James Burrill Angell (1849), University of California president (1899–1919) Benjamin Ide Wheeler (1875), and Morehouse College's first African-American president John Hope (1894).\n\nAlumni in the computer sciences and industry include architect of Intel 386, 486, and Pentium microprocessors John H. Crawford '75, and inventor of the first silicon transistor Gordon Kidd Teal '31.\n\nAlumni in the arts and media include actor Daveed Diggs '04, actress Emma Watson '14, NPR program host Ira Glass '82, singer-composer Mary Chapin Carpenter '81, \"\" humorist and Marx Brothers screenwriter S.J. Perelman '25, novelists Nathanael West '24, Jeffrey Eugenides '83, Edwidge Danticat (MFA '93), and Marilynne Robinson '66; actress Jo Beth Williams '70, composer and synthesizer pioneer Wendy Carlos '62, and journalist James Risen '77, political pundit Mara Liasson.\n\nOther notable alumni include \"Lafayette of the Greek Revolution\" and its historian Samuel Gridley Howe (1821) Governor of Wyoming Territory and Governor of Nebraska John Milton Thayer (1841), Governor of Rhode Island Augustus Bourn (1855), NASA head during first seven Apollo missions Thomas O. Paine '42, diplomat Richard Holbrooke '62, sportscaster Chris Berman '77, Houston Texans head coach Bill O'Brien '92, Penn State football coach Joe Paterno '50, Heisman Trophy namesake John W. Heisman '91, royals and nobles such as Prince Rahim Aga Khan, Prince Faisal bin Al Hussein, Princess Leila Pahlavi of Iran '92, Prince Nikolaos of Greece and Denmark, Prince Nikita Romanov, Princess Theodora of Greece and Denmark, Prince Jaime of Bourbon-Parma, Duke of San Jaime and Count of Bardi, Prince Ra'ad bin Zeid, Lady Gabriella Windsor, Prince Alexander von Fürstenberg, Countess Cosima von Bülow Pavoncelli, and her half-brother Prince Alexander-Georg von Auersperg.\n\nNobel Laureates Craig Mello '82 and Jerry White '87, Cooley-Tukey FFT algorithm co-originator John Wilder Tukey '36, biologist Stanley Falkow (PhD '59), and psychologist Aaron Beck '50.\n\nNotable past or current faculty have included Nobel Laureates Michael Kosterlitz, Lars Onsager, George Stigler, Vernon L. Smith, George Snell and Leon Cooper; Fields Medal winning mathematician David Mumford, Pulitzer Prize–winning historian Gordon S. Wood, Sakurai Prize winning physicist Gerald Guralnik, computer scientist Andries van Dam, engineer Daniel C. Drucker, sociologist Lester Frank Ward, former Prime Minister of Italy and former EU chief Romano Prodi, former President of Brazil Fernando Cardoso, former President of Chile Ricardo Lagos, writers Carlos Fuentes, Chinua Achebe, and Robert Coover, philosopher Martha Nussbaum, linguist Hans Kurath, political scientist James Morone and Senior Fellow Sergei Khrushchev.\n\nBrown's reputation as an institution with a free-spirited, iconoclastic student body is portrayed in fiction and popular culture. \"Family Guy\" character Brian Griffin is a Brown alumnus. \"The O.C.\"s main character Seth Cohen is denied acceptance to Brown while his girlfriend is accepted. In \"Gossip Girl\", New York socialite Serena vies with her friends for a spot at Brown, and \"The Simpsons\" character Lisa Simpson is told by Harvard’s president after she fails a test that she will be unable to attend Harvard, but he can “pass (her) file along to Brown.” Amy Gardner from \"The West Wing\" is a Brown alumna.\n\n\n\n"}
{"id": "4158", "url": "https://en.wikipedia.org/wiki?curid=4158", "title": "Bill Atkinson", "text": "Bill Atkinson\n\nBill Atkinson (born 1951) is an American computer engineer and photographer. Atkinson worked at Apple Computer from 1978 to 1990.\n\nAtkinson was the principal designer and developer of the graphical user interface (GUI) of the Apple Lisa and, later, one of the first thirty members of the original Apple Macintosh development team, and was the creator of the ground-breaking MacPaint application, which fulfilled the vision of using the computer as a creative tool. He also designed and implemented QuickDraw, the fundamental toolbox that the Lisa and Macintosh used for graphics. QuickDraw's performance was essential for the success of the Macintosh GUI. He also was one of the main designers of the Lisa and Macintosh user interfaces. Atkinson also conceived, designed and implemented HyperCard, the first popular hypermedia system. HyperCard put the power of computer programming and database design into the hands of nonprogrammers. In 1994, Atkinson received the EFF Pioneer Award for his contributions.\n\nHe received his undergraduate degree from the University of California, San Diego, where Apple Macintosh developer Jef Raskin was one of his professors. Atkinson continued his studies as a graduate student in neurochemistry at the University of Washington. Raskin invited Atkinson to visit him at Apple Computer; Steve Jobs persuaded him to join the company immediately as employee #51, and Atkinson never finished his PhD.\n\nAround 1990, General Magic's founding, with Bill Atkinson as one of the three cofounders, met the following press in \"Byte\" magazine:\nThe obstacles to General Magic's success may appear daunting, but General Magic is not your typical start-up company. Its partners include some of the biggest players in the worlds of computing, communications, and consumer electronics, and it's loaded with top-notch engineers who have been given a clean slate to reinvent traditional approaches to ubiquitous worldwide communications.\n\nIn 2007, Atkinson began working as an outside developer with Numenta, a startup working on computer intelligence. On his work there Atkinson said, \"what Numenta is doing is more fundamentally important to society than the personal computer and the rise of the Internet.\"\n\nCurrently, Atkinson has combined his passion for computer programming with his love of nature photography to create art images. He takes close-up photographs of stones that have been cut and polished. His works are highly regarded for their resemblance to miniature landscapes which are hidden within the stones. Atkinson’s 2004 book \"Within the Stone\" features a collection of his close-up photographs. The highly intricate and detailed images he creates are made possible by the accuracy and creative control of the digital printing process that he helped create.\n\nSome of Atkinson's noteworthy contributions to the field of computing include:\n\nAtkinson now works as a nature photographer. Actor Nelson Franklin portrayed him in the 2013 film, \"Jobs\".\n\n4. Nndb.com. 'Bill Atkinson'. N.p., 2015. Web. 15 Nov. 2015.\n5. BillAtkinson.com \"About The Artist.\" N.p., n.d. Web. 15. Nov. 2015.\n"}
{"id": "4160", "url": "https://en.wikipedia.org/wiki?curid=4160", "title": "Battle of Lostwithiel", "text": "Battle of Lostwithiel\n\nThe Battles of Lostwithiel or Lostwithiel Campaign, took place near Lostwithiel and Fowey in Cornwall during the First English Civil War in 1644. They resulted in victory for the Royalists commanded by King Charles over the Parliamentarians commanded by the Earl of Essex.\n\nAfter defeating the army of Sir William Waller at the Battle of Cropredy Bridge, King Charles marched west in pursuit of the Parliamentarian army of the Earl of Essex, who was invading the Royalist stronghold of Cornwall.\n\nEssex had been misled into believing that he could expect substantial support from the people of Cornwall. When he had reached Bodmin on 28 July, he found that there was no chance of supplies or recruits, and he also learned that the Royalist army was at Launceston, close to his rear. He withdrew to Lostwithiel, covering the port of Fowey. Essex had previously arranged to rendezvous at Fowey with the Parliamentarian fleet under the Earl of Warwick, but no ships appeared. Warwick was unable to leave Portsmouth because of westerly winds.\n\nCharles's army had been reinforced as it marched, and outnumbered that of Essex by nearly two to one. The first clashes took place on 2 August, but little action took place for several days, as the King waited for all his forces to arrive and Essex waited for the fleet.\n\nOn 13 August, the Royalists began to attack in earnest, occupying several outposts on the east bank of the River Fowey, making it even more difficult for help to reach Essex. A Parliamentarian attempt to send a relieving force under Lieutenant General Middleton was defeated at Bridgwater in Somerset.\n\nOn 21 August, the Royalists attacked Essex's positions north of Lostwithiel, capturing the ruins of Restormel Castle. Royalist cavalry threatened to cut the Parliamentarians off from Fowey. Essex realised that there was no hope of relief and ordered his cavalry to break out of the encirclement. Under Sir William Balfour, they broke through the Royalist lines on the night of 31 August, eventually reaching Plymouth thirty miles to the east.\n\nThe increasingly demoralised Parliamentarian infantry fell back towards Fowey in pouring rain. They were forced to abandon several guns which became bogged down in the muddy roads. On 1 September, the pursuing Royalists captured Castle Dore, another ruined fortification which the Parliamentarians were using to anchor their lines. Essex left Sir Philip Skippon, his Sergeant Major General of Foot, in command while he himself escaped to Plymouth in a fishing boat.\n\nOn 2 September, Skippon, having been told that his infantry were unable to break out as the cavalry had done, and having been offered generous terms by the King, surrendered 6,000 infantry and all his army's guns and train.\n\nThe disarmed soldiers marched eastward to Portsmouth in continuing bad weather, being continually robbed and threatened by local people. About 1,000 died of exposure and hunger, and 1,000 more deserted or fell sick.\n\nCharles meanwhile wheeled about and marched towards London.\n\nThis setback for Parliament in Cornwall, and the last major victory for the Royalists, was reversed by Sir Thomas Fairfax leading the New Model Army at or near Tresillian Bridge, close to Truro on 12 March 1645.\n\n\n"}
{"id": "4162", "url": "https://en.wikipedia.org/wiki?curid=4162", "title": "Beeb", "text": "Beeb\n\nThe nickname Beeb may refer to:\n\n\n"}
{"id": "4163", "url": "https://en.wikipedia.org/wiki?curid=4163", "title": "Bertrand Russell", "text": "Bertrand Russell\n\nBertrand Arthur William Russell, 3rd Earl Russell, (; 18 May 1872 – 2 February 1970) was a British philosopher, logician, mathematician, historian, writer, social critic, political activist and Nobel laureate. At various points in his life he considered himself a liberal, a socialist, and a pacifist, but he also admitted that he had \"never been any of these things, in any profound sense\". He was born in Monmouthshire into one of the most prominent aristocratic families in the United Kingdom.\n\nIn the early 20th century, Russell led the British \"revolt against idealism\". He is considered one of the founders of analytic philosophy along with his predecessor Gottlob Frege, colleague G. E. Moore, and protégé Ludwig Wittgenstein. He is widely held to be one of the 20th century's premier logicians. With A. N. Whitehead he wrote \"Principia Mathematica\", an attempt to create a logical basis for mathematics. His philosophical essay \"On Denoting\" has been considered a \"paradigm of philosophy\". His work has had a considerable influence on mathematics, logic, set theory, linguistics, artificial intelligence, cognitive science, computer science (see type theory and type system), and philosophy, especially the philosophy of language, epistemology, and metaphysics.\n\nRussell was a prominent anti-war activist; he championed anti-imperialism. Occasionally, he advocated preventive nuclear war, before the opportunity provided by the atomic monopoly had passed, and \"welcomed with enthusiasm\" world government. He went to prison for his pacifism during World War I. Later, he concluded war against Adolf Hitler was a necessary \"lesser of two evils\". He criticized Stalinist totalitarianism, attacked the involvement of the United States in the Vietnam War, and was an outspoken proponent of nuclear disarmament. In 1950 Russell was awarded the Nobel Prize in Literature \"in recognition of his varied and significant writings in which he champions humanitarian ideals and freedom of thought\".\n\nBertrand Russell was born on 18 May 1872, at Ravenscroft, Trellech, Monmouthshire, into an influential and liberal family of the British aristocracy. His parents, Viscount and Viscountess Amberley, were radical for their times. Lord Amberley consented to his wife's affair with their children's tutor, the biologist Douglas Spalding. Both were early advocates of birth control at a time when this was considered scandalous. Lord Amberley was an atheist and his atheism was evident when he asked the philosopher John Stuart Mill to act as Russell's secular godfather. Mill died the year after Russell's birth, but his writings had a great effect on Russell's life.\n\nHis paternal grandfather, the Earl Russell, had been asked twice by Queen Victoria to form a government, serving her as Prime Minister in the 1840s and 1860s.\nThe Russells had been prominent in England for several centuries before this, coming to power and the peerage with the rise of the Tudor dynasty (see: Duke of Bedford). They established themselves as one of the leading British Whig families, and participated in every great political event from the Dissolution of the Monasteries in 1536–40 to the Glorious Revolution in 1688–89 and the Great Reform Act in 1832.\n\nLady Amberley was the daughter of Lord and Lady Stanley of Alderley. Russell often feared the ridicule of his maternal grandmother, one of the campaigners for education of women.\n\nRussell had two siblings: brother Frank (nearly seven years older than Bertrand), and sister Rachel (four years older). In June 1874 Russell's mother died of diphtheria, followed shortly by Rachel's death. In January 1876, his father died of bronchitis following a long period of depression. Frank and Bertrand were placed in the care of their staunchly Victorian paternal grandparents, who lived at Pembroke Lodge in Richmond Park. His grandfather, former Prime Minister Earl Russell, died in 1878, and was remembered by Russell as a kindly old man in a wheelchair. His grandmother, the Countess Russell (née Lady Frances Elliot), was the dominant family figure for the rest of Russell's childhood and youth.\n\nThe countess was from a Scottish Presbyterian family, and successfully petitioned the Court of Chancery to set aside a provision in Amberley's will requiring the children to be raised as agnostics. Despite her religious conservatism, she held progressive views in other areas (accepting Darwinism and supporting Irish Home Rule), and her influence on Bertrand Russell's outlook on social justice and standing up for principle remained with him throughout his life. (One could challenge the view that Bertrand stood up for his principles, based on his own well-known quotation: \"I would never die for my beliefs because I might be wrong\".) Her favourite Bible verse, 'Thou shalt not follow a multitude to do evil' (Exodus 23:2), became his motto. The atmosphere at Pembroke Lodge was one of frequent prayer, emotional repression, and formality; Frank reacted to this with open rebellion, but the young Bertrand learned to hide his feelings.\n\nRussell's adolescence was very lonely, and he often contemplated suicide. He remarked in his autobiography that his keenest interests were in religion and mathematics, and that only his wish to know more mathematics kept him from suicide. He was educated at home by a series of tutors. When Russell was eleven years old, his brother Frank introduced him to the work of Euclid, which transformed his life.\n\nDuring these formative years he also discovered the works of Percy Bysshe Shelley. In his autobiography, he writes: \"I spent all my spare time reading him, and learning him by heart, knowing no one to whom I could speak of what I thought or felt, I used to reflect how wonderful it would have been to know Shelley, and to wonder whether I should meet any live human being with whom I should feel so much sympathy\". Russell claimed that beginning at age 15, he spent considerable time thinking about the validity of Christian religious dogma, which he found very unconvincing. At this age, he came to the conclusion that there is no free will and, two years later, that there is no life after death. Finally, at the age of 18, after reading Mill's \"Autobiography\", he abandoned the \"First Cause\" argument and became an atheist.\n\nRussell won a scholarship to read for the Mathematical Tripos at Trinity College, Cambridge, and commenced his studies there in 1890, taking as coach Robert Rumsey Webb. He became acquainted with the younger George Edward Moore and came under the influence of Alfred North Whitehead, who recommended him to the Cambridge Apostles. He quickly distinguished himself in mathematics and philosophy, graduating as seventh Wrangler in the former in 1893 and becoming a Fellow in the latter in 1895.\n\nRussell first met the American Quaker Alys Pearsall Smith when he was 17 years old. He became a friend of the Pearsall Smith family – they knew him primarily as “Lord John’s grandson” and enjoyed showing him off. He traveled with them to the continent; it was in their company that Russell visited the Paris Exhibition of 1889 and was able to climb the Eiffel Tower soon after it was completed.\n\nHe soon fell in love with the puritanical, high-minded Alys, who was a graduate of Bryn Mawr College near Philadelphia, and, contrary to his grandmother's wishes, married her on 13 December 1894. Their marriage began to fall apart in 1901 when it occurred to Russell, while he was cycling, that he no longer loved her. She asked him if he loved her and he replied that he did not. Russell also disliked Alys's mother, finding her controlling and cruel. It was to be a hollow shell of a marriage and they finally divorced in 1921, after a lengthy period of separation.\nDuring this period, Russell had passionate (and often simultaneous) affairs with a number of women, including Lady Ottoline Morrell and the actress Lady Constance Malleson. Some have suggested that at this point he had an affair with Vivienne Haigh-Wood, the English governess and writer, and first wife of T. S. Eliot.\n\nRussell began his published work in 1896 with \"German Social Democracy\", a study in politics that was an early indication of a lifelong interest in political and social theory. In 1896 he taught German social democracy at the London School of Economics. He was a member of the Coefficients dining club of social reformers set up in 1902 by the Fabian campaigners Sidney and Beatrice Webb.\n\nHe now started an intensive study of the foundations of mathematics at Trinity.\nIn 1898 he wrote \"An Essay on the Foundations of Geometry\" which discussed the Cayley–Klein metrics used for non-Euclidean geometry.\nHe attended the International Congress of Philosophy in Paris in 1900 where he met Giuseppe Peano and Alessandro Padoa. The Italians had responded to Georg Cantor, making a science of set theory; they gave Russell their literature including the Formulario mathematico. Russell was impressed by the precision of Peano's arguments at the Congress, read the literature upon returning to England, and came upon Russell's paradox. In 1903 he published \"The Principles of Mathematics\", a work on foundations of mathematics. It advanced a thesis of logicism, that mathematics and logic are one and the same.\n\nAt the age of 29, in February 1901, Russell underwent what he called a \"sort of mystic illumination\", after witnessing Whitehead's wife's acute suffering in an angina attack. \"I found myself filled with semi-mystical feelings about beauty ... and with a desire almost as profound as that of the Buddha to find some philosophy which should make human life endurable\", Russell would later recall. \"At the end of those five minutes, I had become a completely different person.\"\n\nIn 1905 he wrote the essay \"On Denoting\", which was published in the philosophical journal \"Mind\". Russell was elected a Fellow of the Royal Society (FRS) in 1908. The three-volume \"Principia Mathematica\", written with Whitehead, was published between 1910 and 1913. This, along with the earlier \"The Principles of Mathematics\", soon made Russell world-famous in his field.\n\nIn 1910 he became a lecturer in the University of Cambridge, where he was approached by the Austrian engineering student Ludwig Wittgenstein, who became his PhD student. Russell viewed Wittgenstein as a genius and a successor who would continue his work on logic. He spent hours dealing with Wittgenstein's various phobias and his frequent bouts of despair. This was often a drain on Russell's energy, but Russell continued to be fascinated by him and encouraged his academic development, including the publication of Wittgenstein's \"Tractatus Logico-Philosophicus\" in 1922. Russell delivered his lectures on Logical Atomism, his version of these ideas, in 1918, before the end of World War I. Wittgenstein was, at that time, serving in the Austrian Army and subsequently spent nine months in an Italian prisoner of war camp at the end of the conflict.\n\nDuring World War I, Russell was one of the few people to engage in active pacifist activities and in 1916, he was dismissed from Trinity College following his conviction under the Defence of the Realm Act 1914. Russell played a significant part in the \"Leeds Convention\" in June 1917, a historic event which saw well over a thousand \"anti-war socialists\" gather; many being delegates from the Independent Labour Party and the Socialist Party, united in their pacifist beliefs and advocating a peace settlement. The international press reported that Russell appeared with a number of Labour MPs, including Ramsay MacDonald and Philip Snowden, as well as former Liberal MP and anti-conscription campaigner, Professor Arnold Lupton. After the event, Russell told Lady Ottoline Morrell that, \"to my surprise, when I got up to speak, I was given the greatest ovation that was possible to give anybody\".\n\nThe Trinity incident resulted in Russell being fined £100, which he refused to pay in hope that he would be sent to prison, but his books were sold at auction to raise the money. The books were bought by friends; he later treasured his copy of the King James Bible that was stamped \"Confiscated by Cambridge Police\".\n\nA later conviction for publicly lecturing against inviting the US to enter the war on the United Kingdom's side resulted in six months' imprisonment in Brixton prison (see \"Bertrand Russell's views on society\") in 1918. He later of said of his imprisonment:\n\nRussell was reinstated to Trinity in 1919, resigned in 1920, was Tarner Lecturer 1926 and became a Fellow again in 1944 until 1949.\n\nIn 1924, Bertrand again gained press attention when attending a \"banquet\" in the House of Commons with well-known campaigners, including Arnold Lupton, who had been a Member of Parliament and had also endured imprisonment for \"passive resistance to military or naval service\".\n\nIn 1941, G. H. Hardy wrote a 61-page pamphlet titled \"Bertrand Russell and Trinity\" – published later as a book by Cambridge University Press with a foreword by C. D. Broad – in which he gave an authoritative account about Russell's 1916 dismissal from Trinity College, explaining that a reconciliation between the college and Russell had later taken place and gave details about Russell's personal life. Hardy writes that Russell's dismissal had created a scandal since the vast majority of the Fellows of the College opposed the decision. The ensuing pressure from the Fellows induced the Council to reinstate Russell. In January 1920, it was announced that Russell had accepted the reinstatement offer from Trinity and would begin lecturing from October. In July 1920, Russell applied for a one year leave of absence; this was approved. He spent the year giving lectures in China and Japan. In January 1921, it was announced by Trinity that Russell had resigned and his resignation had been accepted. This resignation, Hardy explains, was completely voluntary and was not the result of another altercation.\n\nThe reason for the resignation, according to Hardy, was that Russell was going through a tumultuous time in his personal life with a divorce and subsequent remarriage. Russell contemplated asking Trinity for another one-year leave of absence but decided against it, since this would have been an “unusual application” and the situation had the potential to snowball into another controversy. Although Russell did the right thing, in Hardy's opinion, the reputation of the College suffered due to Russell's resignation since the ‘world of learning’ knew about Russell's altercation with Trinity but not that the rift had healed. In 1925, Russell was asked by the Council of Trinity College to give the \"Tarner Lectures\" on the Philosophy of the Sciences; these would later be the basis for one of Russell's best received books according to Hardy: \"The Analysis of Matter\", published in 1927. In the preface to this pamphlet, Hardy wrote:\n\nIn August 1920, Russell travelled to Russia as part of an official delegation sent by the British government to investigate the effects of the Russian Revolution. He wrote a four-part series of articles, titled \"Soviet Russia1920\", for the US magazine \"The Nation\". He met Vladimir Lenin and had an hour-long conversation with him. In his autobiography, he mentions that he found Lenin disappointing, sensing an \"impish cruelty\" in him and comparing him to \"an opinionated professor\". He cruised down the Volga on a steamship. His experiences destroyed his previous tentative support for the revolution. He wrote a book \"The Practice and Theory of Bolshevism\" about his experiences on this trip, taken with a group of 24 others from the UK, all of whom came home thinking well of the régime, despite Russell's attempts to change their minds. For example, he told them that he heard shots fired in the middle of the night and was sure these were clandestine executions, but the others maintained that it was only cars backfiring.\nRussell's lover Dora Black, a British author, feminist and socialist campaigner, visited Russia independently at the same time; in contrast to his reaction, she was enthusiastic about the revolution.\n\nThe following autumn Russell, accompanied by Dora, visited Peking (as it was then known in the West) to lecture on philosophy for a year. He went with optimism and hope, seeing China as then being on a new path. Other scholars present in China at the time included John Dewey and Rabindranath Tagore, the Indian Nobel-laureate poet. Before leaving China, Russell became gravely ill with pneumonia, and incorrect reports of his death were published in the Japanese press. When the couple visited Japan on their return journey, Dora took on the role of spurning the local press by handing out notices reading \"Mr. Bertrand Russell, having died according to the Japanese press, is unable to give interviews to Japanese journalists\". Apparently they found this harsh and reacted resentfully.\n\nDora was six months pregnant when the couple returned to England on 26 August 1921. Russell arranged a hasty divorce from Alys, marrying Dora six days after the divorce was finalised, on 27 September 1921. Russell's children with Dora were John Conrad Russell, 4th Earl Russell, born on 16 November 1921, and Katharine Jane Russell (now Lady Katharine Tait), born on 29 December 1923. Russell supported his family during this time by writing popular books explaining matters of physics, ethics, and education to the layman.\n\nFrom 1922 to 1927 the Russells divided their time between London and Cornwall, spending summers in Porthcurno. In the 1922 and 1923 general elections Russell stood as a Labour Party candidate in the Chelsea constituency, but only on the basis that he knew he was extremely unlikely to be elected in such a safe Conservative seat, and he was not on either occasion.\n\nTogether with Dora, Russell founded the experimental Beacon Hill School in 1927. The school was run from a succession of different locations, including its original premises at the Russells' residence, Telegraph House, near Harting, West Sussex. On 8 July 1930 Dora gave birth to her third child Harriet Ruth. After he left the school in 1932, Dora continued it until 1943.\n\nOn a tour through the US in 1927 Russell met Barry Fox (later Barry Stevens) who became a well-known Gestalt therapist and writer in later years. Russell and Fox developed an intensive relationship. In Fox's words: \"... for three years we were very close.\" Fox sent her daughter Judith to Beacon Hill School for some time. From 1927 to 1932 Russell wrote 34 letters to Fox.\n\nUpon the death of his elder brother Frank, in 1931, Russell became the 3rd Earl Russell.\n\nRussell's marriage to Dora grew increasingly tenuous, and it reached a breaking point over her having two children with an American journalist, Griffin Barry. They separated in 1932 and finally divorced. On 18 January 1936, Russell married his third wife, an Oxford undergraduate named Patricia (\"Peter\") Spence, who had been his children's governess since 1930. Russell and Peter had one son, Conrad Sebastian Robert Russell, 5th Earl Russell, who became a prominent historian and one of the leading figures in the Liberal Democrat party.\n\nRussell returned to the London School of Economics to lecture on the science of power in 1937.\n\nDuring the 1930s, Russell became a close friend and collaborator of V. K. Krishna Menon, then secretary of the India League, the foremost lobby in the United Kingdom for Indian self-rule.\n\nRussell opposed rearmament against Nazi Germany. In 1937 he wrote in a personal letter: \"If the Germans succeed in sending an invading army to England we should do best to treat them as visitors, give them quarters and invite the commander and chief to dine with the prime minister.\" In 1940, he changed his view that avoiding a full-scale world war was more important than defeating Hitler. He concluded that Adolf Hitler taking over all of Europe would be a permanent threat to democracy. In 1943, he adopted a stance toward large-scale warfare: \"War was always a great evil, but in some particularly extreme circumstances, it may be the lesser of two evils.\"\n\nBefore World War II, Russell taught at the University of Chicago, later moving on to Los Angeles to lecture at the UCLA Department of Philosophy. He was appointed professor at the City College of New York (CCNY) in 1940, but after a public outcry the appointment was annulled by a court judgment that pronounced him \"morally unfit\" to teach at the college due to his opinions, especially those relating to sexual morality, detailed in \"Marriage and Morals\" (1929). The protest was started by the mother of a student who would not have been eligible for his graduate-level course in mathematical logic; many intellectuals, led by John Dewey, protested at his treatment. Albert Einstein's oft-quoted aphorism that \"great spirits have always encountered violent opposition from mediocre minds\" originated in his open letter, dated 19 March 1940, to Morris Raphael Cohen, a professor emeritus at CCNY, supporting Russell's appointment. Dewey and Horace M. Kallen edited a collection of articles on the CCNY affair in \"The Bertrand Russell Case\". He soon joined the Barnes Foundation, lecturing to a varied audience on the history of philosophy; these lectures formed the basis of \"A History of Western Philosophy\". His relationship with the eccentric Albert C. Barnes soon soured, and he returned to the UK in 1944 to rejoin the faculty of Trinity College.\n\nRussell participated in many broadcasts over the BBC, particularly \"The Brains Trust\" and the Third Programme, on various topical and philosophical subjects. By this time Russell was world-famous outside academic circles, frequently the subject or author of magazine and newspaper articles, and was called upon to offer opinions on a wide variety of subjects, even mundane ones. En route to one of his lectures in Trondheim, Russell was one of 24 survivors (among a total of 43 passengers) of an aeroplane crash in Hommelvik in October 1948. He said he owed his life to smoking since the people who drowned were in the non-smoking part of the plane. \"A History of Western Philosophy\" (1945) became a best-seller and provided Russell with a steady income for the remainder of his life.\n\nIn 1942 Russell argued in favour of a moderate socialism, capable of overcoming its metaphysical principles, in an inquiry on Dialectical Materialism, launched by the Austrian artist and philosopher Wolfgang Paalen in his journal \"DYN\", saying, \"I think the metaphysics of both Hegel and Marx plain nonsense – Marx's claim to be 'science' is no more justified than Mary Baker Eddy's. This does not mean that I am opposed to socialism.\"\nIn 1943, Russell expressed support for Zionism: \"I have come gradually to see that, in a dangerous and largely hostile world, it is essential to Jews to have some country which is theirs, some region where they are not suspected aliens, some state which embodies what is distinctive in their culture\".\n\nIn a speech in 1948, Russell said that if the USSR's aggression continued, it would be morally worse to go to war after the USSR possessed an atomic bomb than before it possessed one, because if the USSR had no bomb the West's victory would come more swiftly and with fewer casualties than if there were atom bombs on both sides. At that time, only the United States possessed an atomic bomb, and the USSR was pursuing an extremely aggressive policy towards the countries in Eastern Europe which were being absorbed into the Soviet Union's sphere of influence. Many understood Russell's comments to mean that Russell approved of a first strike in a war with the USSR, including Nigel Lawson, who was present when Russell spoke of such matters. Others, including Griffin, who obtained a transcript of the speech, have argued that he was merely explaining the usefulness of America's atomic arsenal in deterring the USSR from continuing its domination of Eastern Europe.\nHowever, just after the atomic bombs exploded over Hiroshima and Nagasaki, Russell wrote letters, and published articles in newspapers from 1945 to 1948, stating clearly that it was morally justified and better to go to war against the USSR using atomic bombs while the USA possessed them and before the USSR did. In September 1949, one week after the USSR tested its first A-bomb, but before this became known, Russell wrote that USSR would be unable to develop nuclear weapons because following Stalin's purges only science based on Marxist principles would be practiced in the Soviet Union. After it became known that the USSR carried out its nuclear bomb tests, Russell declared his position advocating for the total abolition of atomic weapons.\n\nIn 1948, Russell was invited by the BBC to deliver the inaugural Reith Lectures—what was to become an annual series of lectures, still broadcast by the BBC. His series of six broadcasts, titled \"Authority and the Individual\", explored themes such as the role of individual initiative in the development of a community and the role of state control in a progressive society. Russell continued to write about philosophy. He wrote a foreword to \"Words and Things\" by Ernest Gellner, which was highly critical of the later thought of Ludwig Wittgenstein and of ordinary language philosophy. Gilbert Ryle refused to have the book reviewed in the philosophical journal \"Mind\", which caused Russell to respond via \"The Times\". The result was a month-long correspondence in \"The Times\" between the supporters and detractors of ordinary language philosophy, which was only ended when the paper published an editorial critical of both sides but agreeing with the opponents of ordinary language philosophy.\n\nIn the King's Birthday Honours of 9 June 1949, Russell was awarded the Order of Merit, and the following year he was awarded the Nobel Prize in Literature. When he was given the Order of Merit, George VI was affable but slightly embarrassed at decorating a former jailbird, saying, \"You have sometimes behaved in a manner that would not do if generally adopted\". Russell merely smiled, but afterwards claimed that the reply \"That's right, just like your brother\" immediately came to mind. In 1952 Russell was divorced by Spence, with whom he had been very unhappy. Conrad, Russell's son by Spence, did not see his father between the time of the divorce and 1968 (at which time his decision to meet his father caused a permanent breach with his mother).\n\nRussell married his fourth wife, Edith Finch, soon after the divorce, on 15 December 1952. They had known each other since 1925, and Edith had taught English at Bryn Mawr College near Philadelphia, sharing a house for 20 years with Russell's old friend Lucy Donnelly. Edith remained with him until his death, and, by all accounts, their marriage was a happy, close, and loving one. Russell's eldest son John suffered from serious mental illness, which was the source of ongoing disputes between Russell and his former wife Dora.\n\nIn September 1961, at the age of 89, Russell was jailed for seven days in Brixton Prison for \"breach of peace\" after taking part in an anti-nuclear demonstration in London. The magistrate offered to exempt him from jail if he pledged himself to \"good behaviour\", to which Russell replied: \"No, I won't.\"\n\nIn 1962 Russell played a public role in the Cuban Missile Crisis: in an exchange of telegrams with Soviet leader Nikita Khrushchev, Khrushchev assured him that the Soviet government would not be reckless. Russell sent this telegram to President Kennedy:\nYOUR ACTION DESPERATE. THREAT TO HUMAN SURVIVAL. NO CONCEIVABLE JUSTIFICATION. CIVILIZED MAN CONDEMNS IT. WE WILL NOT HAVE MASS MURDER. ULTIMATUM MEANS WAR... END THIS MADNESS.\n\nAccording to historian Peter Knight, after JFK's assassination, Russell, \"prompted by the emerging work of the lawyer Mark Lane in the US ... rallied support from other noteworthy and left-leaning compatriots to form a Who Killed Kennedy Committee in June 1964, members of which included Michael Foot MP, Caroline Benn, the publisher Victor Gollancz, the writers John Arden and J. B. Priestley, and the Oxford history professor Hugh Trevor-Roper. Russell published a highly critical article weeks before the Warren Commission Report was published, setting forth \"16 Questions on the Assassination\" and equating the Oswald case with the Dreyfus affair of late 19th-century France, in which the state wrongly convicted an innocent man. Russell also criticised the American press for failing to heed any voices critical of the official version.\n\nRussell spent the 1950s and 1960s engaged in political causes primarily related to nuclear disarmament and opposing the Vietnam War. The 1955 Russell–Einstein Manifesto was a document calling for nuclear disarmament and was signed by eleven of the most prominent nuclear physicists and intellectuals of the time. In 1966–67, Russell worked with Jean-Paul Sartre and many other intellectual figures to form the Russell Vietnam War Crimes Tribunal to investigate the conduct of the United States in Vietnam. He wrote a great many letters to world leaders during this period.\n\nIn 1956, immediately before and during the Suez Crisis, Russell expressed his opposition to European imperialism in the Middle East. He viewed the crisis as another reminder of the pressing need for a more effective mechanism for international governance, and to restrict national sovereignty to places such as the Suez Canal area \"where general interest is involved\". At the same time the Suez Crisis was taking place, the world was also captivated by the Hungarian Revolution and the subsequent crushing of the revolt by intervening Soviet forces. Russell attracted criticism for speaking out fervently against the Suez war while ignoring Soviet repression in Hungary, to which he responded that he did not criticise the Soviets \"because there was no need. Most of the so-called Western World was fulminating\". Although he later feigned a lack of concern, at the time he was disgusted by the brutal Soviet response, and on 16 November 1956, he expressed approval for a declaration of support for Hungarian scholars which Michael Polanyi had cabled to the Soviet embassy in London twelve days previously, shortly after Soviet troops had already entered Budapest.\n\nIn November 1957 Russell wrote an article addressing US President Dwight D. Eisenhower and Soviet Premier Nikita Khrushchev, urging a summit to consider \"the conditions of co-existence\". Khrushchev responded that peace could indeed be served by such a meeting. In January 1958 Russell elaborated his views in \"The Observer\", proposing a cessation of all nuclear-weapons production, with the UK taking the first step by unilaterally suspending its own nuclear-weapons program if necessary, and with Germany \"freed from all alien armed forces and pledged to neutrality in any conflict between East and West\". US Secretary of State John Foster Dulles replied for Eisenhower. The exchange of letters was published as \"The Vital Letters of Russell, Khrushchev, and Dulles\".\n\nRussell was asked by \"The New Republic\", a liberal American magazine, to elaborate his views on world peace. He urged that all nuclear-weapons testing and constant flights by planes armed with nuclear weapons be halted immediately, and negotiations be opened for the destruction of all hydrogen bombs, with the number of conventional nuclear devices limited to ensure a balance of power. He proposed that Germany be reunified and accept the Oder-Neisse line as its border, and that a neutral zone be established in Central Europe, consisting at the minimum of Germany, Poland, Hungary, and Czechoslovakia, with each of these countries being free of foreign troops and influence, and prohibited from forming alliances with countries outside the zone. In the Middle East, Russell suggested that the West avoid opposing Arab nationalism, and proposed the creation of a United Nations peacekeeping force to guard Israel's frontiers to ensure that Israel was prevented from committing aggression and protected from it. He also suggested Western recognition of the People's Republic of China, and that it be admitted to the UN with a permanent seat on the UN Security Council.\n\nHe was in contact with Lionel Rogosin while the latter was filming his anti-war film \"Good Times, Wonderful Times\" in the 1960s. He became a hero to many of the youthful members of the New Left. In early 1963, in particular, Russell became increasingly vocal in his disapproval of the Vietnam War, and felt that the US government's policies there were near-genocidal. In 1963 he became the inaugural recipient of the Jerusalem Prize, an award for writers concerned with the freedom of the individual in society. In 1964 he was one of eleven world figures who issued an appeal to Israel and the Arab countries to accept an arms embargo and international supervision of nuclear plants and rocket weaponry. In October 1965 he tore up his Labour Party card because he suspected Harold Wilson's Labour government was going to send troops to support the United States in Vietnam.\n\nIn June 1955 Russell had leased Plas Penrhyn in Penrhyndeudraeth, Merionethshire, Wales and on 5 July of the following year it became his and Edith's principal residence.\nRussell published his three-volume autobiography in 1967, 1968, and 1969. Russell made a cameo appearance playing himself in the anti-war Hindi film \"Aman\" which was released in India in 1967. This was Russell's only appearance in a feature film.\n\nOn 23 November 1969 he wrote to \"The Times\" newspaper saying that the preparation for show trials in Czechoslovakia was \"highly alarming\". The same month, he appealed to Secretary General U Thant of the United Nations to support an international war crimes commission to investigate alleged torture and genocide by the United States in South Vietnam during the Vietnam War. The following month, he protested to Alexei Kosygin over the expulsion of Aleksandr Solzhenitsyn from the Soviet Union of Writers.\n\nOn 31 January 1970 Russell issued a statement condemning \"Israel's aggression in the Middle East\", and in particular, Israeli bombing raids being carried out deep in Egyptian territory as part of the War of Attrition. He called for an Israeli withdrawal to the pre-Six-Day War borders. This was Russell's final political statement or act. It was read out at the International Conference of Parliamentarians in Cairo on 3 February 1970, the day after his death.\n\nRussell died of influenza on 2 February 1970 at his home in Penrhyndeudraeth. His body was cremated in Colwyn Bay on 5 February 1970. In accordance with his will, there was no religious ceremony; his ashes were scattered over the Welsh mountains later that year. He left an estate valued at £69,423.\nIn 1980 a memorial to Russell was commissioned by a committee including the philosopher A. J. Ayer. It consists of a bust of Russell in Red Lion Square in London sculpted by Marcelle Quinton.\n\nRussell held throughout his life the following styles and honours:\n\nRussell is generally credited with being one of the founders of analytic philosophy. He was deeply impressed by Gottfried Leibniz (1646–1716), and wrote on every major area of philosophy except aesthetics. He was particularly prolific in the field of metaphysics, the logic and the philosophy of mathematics, the philosophy of language, ethics and epistemology. When Brand Blanshard asked Russell why he did not write on aesthetics, Russell replied that he did not know anything about it, \"but that is not a very good excuse, for my friends tell me it has not deterred me from writing on other subjects\".\n\nOn ethics, Russell considered himself a utilitarian.\n\nRussell described himself as an agnostic, \"speaking to a purely philosophical audience\", but as an atheist \"speaking popularly\", on the basis that he could not disprove the Christian God – similar to the way that he could not disprove the Olympic gods either. For most of his adult life, Russell maintained religion to be little more than superstition and, in spite of any positive effects, largely harmful to people. He believed that religion and the religious outlook serve to impede knowledge and foster fear and dependency, and to be responsible for much of our world's wars, oppression, and misery. He was a member of the Advisory Council of the British Humanist Association and President of Cardiff Humanists until his death.\n\nPolitical and social activism occupied much of Russell's time for most of his life. Russell remained politically active almost to the end of his life, writing to and exhorting world leaders and lending his name to various causes.\n\nRussell argued for a \"scientific society\", where war would be abolished, population growth would be limited, and prosperity would be shared. He suggested the establishment of a \"single supreme world government\" able to enforce peace, claiming that \"the only thing that will redeem mankind is co-operation\".\n\nRussell was an active supporter of the Homosexual Law Reform Society, being one of the signatories of A. E. Dyson's 1958 letter to \"The Times\" calling for a change in the law regarding male homosexual practices, which were partly legalised in 1967, when Russell was still alive.\n\nIn \"Reflections on My Eightieth Birthday\" (\"Postscript\" in his \"Autobiography\"), Russell wrote: \"I have lived in the pursuit of a vision, both personal and social. Personal: to care for what is noble, for what is beautiful, for what is gentle; to allow moments of insight to give wisdom at more mundane times. Social: to see in imagination the society that is to be created, where individuals grow freely, and where hate and greed and envy die because there is nothing to nourish them. These things I believe, and the world, for all its horrors, has left me unshaken\".\n\nBelow is a selected bibliography of Russell's books in English, sorted by year of first publication:\n\nRussell was the author of more than sixty books and over two thousand articles. Additionally, he wrote many pamphlets, introductions, and letters to the editor. One pamphlet titled, \"'I Appeal unto Caesar': The Case of the Conscientious Objectors\", ghostwritten for Margaret Hobhouse, the mother of imprisoned peace activist Stephen Hobhouse, allegedly helped secure the release from prison of hundreds of conscientious objectors.\n\nHis works can be found in anthologies and collections, including \"The Collected Papers of Bertrand Russell\", which McMaster University began publishing in 1983. By March 2017 this collection of his shorter and previously unpublished works included 18 volumes, and several more are in progress. A bibliography in three additional volumes catalogues his publications. The Russell Archives held by McMaster's William Ready Division of Archives and Research Collections possess over 40,000 of his letters.\n\n\n\n\n\n\n\n\n"}
{"id": "4165", "url": "https://en.wikipedia.org/wiki?curid=4165", "title": "Boeing 767", "text": "Boeing 767\n\nThe Boeing 767 is a mid- to large-size, mid- to long-range, wide-body twin-engine jet airliner built by Boeing Commercial Airplanes. It was Boeing's first wide-body twinjet and its first airliner with a two-crew glass cockpit. The aircraft has two turbofan engines, a conventional tail, and, for reduced aerodynamic drag, a supercritical wing design. Designed as a smaller wide-body airliner than earlier aircraft such as the 747, the 767 has seating capacity for 181 to 375 people, and a design range of , depending on variant. Development of the 767 occurred in tandem with a narrow-body twinjet, the 757, resulting in shared design features which allow pilots to obtain a common type rating to operate both aircraft.\n\nThe 767 is produced in three fuselage lengths. The original 767-200 entered service in 1982, followed by the 767-300 in 1986 and the 767-400ER, an extended-range (ER) variant, in 2000. The extended-range 767-200ER and 767-300ER models entered service in 1984 and 1988, respectively, while a production freighter version, the 767-300F, debuted in 1995. Conversion programs have modified passenger 767-200 and 767-300 series aircraft for cargo use, while military derivatives include the E-767 surveillance aircraft, the KC-767 and KC-46 aerial tankers, and VIP transports. Engines featured on the 767 include the General Electric CF6, Pratt & Whitney JT9D and PW4000, and Rolls-Royce RB211 turbofans.\n\nUnited Airlines first placed the 767 in commercial service in 1982. The aircraft was initially flown on domestic and transcontinental routes, during which it demonstrated the reliability of its twinjet design. In 1985, the 767 became the first twin-engined airliner to receive regulatory approval for extended overseas flights. The aircraft was then used to expand non-stop service on medium- to long-haul intercontinental routes. In 1986, Boeing initiated studies for a higher-capacity 767, ultimately leading to the development of the 777, a larger wide-body twinjet. In the 1990s, the 767 became the most frequently used airliner for transatlantic flights between North America and Europe.\n\nThe 767 is the first twinjet wide-body type to reach 1,000 aircraft delivered. As of July 2017, Boeing has received 1,204 orders for the 767 from 74 customers; 1,102 have been delivered. A total of 742 of these aircraft were in service in July 2016; the most popular variant is the 767-300ER, with 583 delivered; Delta Air Lines is the largest operator, with 91 aircraft. Competitors have included the Airbus A300, A310, and A330-200, while a successor, the 787 Dreamliner, entered service in October 2011. Despite this, the 767 still remains in production.\n\nIn 1970, Boeing's 747 became the first wide-body jetliner to enter service. The 747 was the first passenger jet wide enough to feature a twin-aisle cabin. Two years later, the manufacturer began a development study, code-named 7X7, for a new wide-body aircraft intended to replace the 707 and other early generation narrow-body jets. The aircraft would also provide twin-aisle seating, but in a smaller fuselage than the existing 747, McDonnell Douglas DC-10, and Lockheed L-1011 TriStar wide-bodies. To defray the high cost of development, Boeing signed risk-sharing agreements with Italian corporation Aeritalia and the Civil Transport Development Corporation (CTDC), a consortium of Japanese aerospace companies. This marked the manufacturer's first major international joint venture, and both Aeritalia and the CTDC received supply contracts in return for their early participation. The initial 7X7 was conceived as a short take-off and landing airliner intended for short-distance flights, but customers were unenthusiastic about the concept, leading to its redefinition as a mid-size, transcontinental-range airliner. At this stage the proposed aircraft featured two or three engines, with possible configurations including over-wing engines and a T-tail.\nBy 1976, a twinjet layout, similar to the one which had debuted on the Airbus A300, became the baseline configuration. The decision to use two engines reflected increased industry confidence in the reliability and economics of new-generation jet powerplants. While airline requirements for new wide-body aircraft remained ambiguous, the 7X7 was generally focused on mid-size, high-density markets. As such, it was intended to transport large numbers of passengers between major cities. Advancements in civil aerospace technology, including high-bypass-ratio turbofan engines, new flight deck systems, aerodynamic improvements, and lighter construction materials were to be applied to the 7X7. Many of these features were also included in a parallel development effort for a new mid-size narrow-body airliner, code-named 7N7, which would become the 757. Work on both proposals proceeded through the airline industry upturn in the late 1970s.\n\nIn January 1978, Boeing announced a major extension of its Everett factory—which was then dedicated to manufacturing the 747—to accommodate its new wide-body family. In February 1978, the new jetliner received the 767 model designation, and three variants were planned: a 767-100 with 190 seats, a 767-200 with 210 seats, and a trijet 767MR/LR version with 200 seats intended for intercontinental routes. The 767MR/LR was subsequently renamed 777 for differentiation purposes. The 767 was officially launched on July 14, 1978, when United Airlines ordered 30 of the 767-200 variant, followed by 50 more 767-200 orders from American Airlines and Delta Air Lines later that year. The 767-100 was ultimately not offered for sale, as its capacity was too close to the 757's seating, while the 777 trijet was eventually dropped in favor of standardizing around the twinjet configuration.\n\nIn the late 1970s, operating cost replaced capacity as the primary factor in airliner purchases. As a result, the 767's design process emphasized fuel efficiency from the outset. Boeing targeted a 20 to 30 percent cost saving over earlier aircraft, mainly through new engine and wing technology. As development progressed, engineers used computer-aided design for over a third of the 767's design drawings, and performed 26,000 hours of wind tunnel tests. Design work occurred concurrently with the 757 twinjet, leading Boeing to treat both as almost one program to reduce risk and cost. Both aircraft would ultimately receive shared design features, including avionics, flight management systems, instruments, and handling characteristics. Combined development costs were estimated at $3.5 to $4 billion.\nEarly 767 customers were given the choice of Pratt & Whitney JT9D or General Electric CF6 turbofans, marking the first time that Boeing had offered more than one engine option at the launch of a new airliner. Both jet engine models had a maximum output of of thrust. The engines were mounted approximately one-third the length of the wing from the fuselage, similar to previous wide-body trijets. The larger wings were designed using an aft-loaded shape which reduced aerodynamic drag and distributed lift more evenly across their surface span than any of the manufacturer's previous aircraft. The wings provided higher-altitude cruise performance, added fuel capacity, and expansion room for future stretched variants. The initial 767-200 was designed for sufficient range to fly across North America or across the northern Atlantic, and would be capable of operating routes up to .\n\nThe 767's fuselage width was set midway between that of the 707 and the 747 at . While it was narrower than previous wide-body designs, seven abreast seating with two aisles could be fitted, and the reduced width produced less aerodynamic drag. However, the fuselage was not wide enough to accommodate two standard LD3 wide-body unit load devices side-by-side. As a result, a smaller container, the LD2, was created specifically for the 767. Using a conventional tail design also allowed the rear fuselage to be tapered over a shorter section, providing for parallel aisles along the full length of the passenger cabin, and eliminating irregular seat rows toward the rear of the aircraft.\nThe 767 was the first Boeing wide-body to be designed with a two-crew digital glass cockpit. Cathode ray tube (CRT) color displays and new electronics replaced the role of the flight engineer by enabling the pilot and co-pilot to monitor aircraft systems directly. Despite the promise of reduced crew costs, United Airlines initially demanded a conventional three-person cockpit, citing concerns about the risks associated with introducing a new aircraft. The carrier maintained this position until July 1981, when a US presidential task force determined that a crew of two was safe for operating wide-body jets. A three-crew cockpit remained as an option and was fitted to the first production models. Ansett Australia ordered 767s with three-crew cockpits due to union demands; it was the only airline to operate 767s so configured. The 767's two-crew cockpit was also applied to the 757, allowing pilots to operate both aircraft after a short conversion course, and adding incentive for airlines to purchase both types. Although nominally similar in control design, flying the 767 feels different from the 757. The 757's controls are heavy, similar to the 727 and 747; the control yoke can be rotated to 90 degrees in each direction. The 767 has far lighter control feel in pitch and roll, and the control yoke has approximately 2/3 the rotation travel.\n\nTo produce the 767, Boeing formed a network of subcontractors which included domestic suppliers and international contributions from Italy's Aeritalia and Japan's CTDC. The wings and cabin floor were produced in-house, while Aeritalia provided control surfaces, Boeing Vertol made the leading edge for the wings, and Boeing Wichita produced the forward fuselage. The CTDC provided multiple assemblies through its constituent companies, namely Fuji Heavy Industries (wing fairings and gear doors), Kawasaki Heavy Industries (center fuselage), and Mitsubishi Heavy Industries (rear fuselage, doors, and tail). Components were integrated during final assembly at the Everett factory. For expedited production of wing spars, the main structural member of aircraft wings, the Everett factory received robotic machinery to automate the process of drilling holes and inserting fasteners. This method of wing construction expanded on techniques developed for the 747. Final assembly of the first aircraft began in July 1979.\nThe prototype aircraft, registered N767BA and equipped with JT9D turbofans, rolled out on August 4, 1981. By this time, the 767 program had accumulated 173 firm orders from 17 customers, including Air Canada, All Nippon Airways, Britannia Airways, Transbrasil, and Trans World Airlines (TWA). On September 26, 1981, the prototype took its maiden flight under the command of company test pilots Tommy Edmonds, Lew Wallick, and John Brit. The maiden flight was largely uneventful, save for the inability to retract the landing gear because of a hydraulic fluid leak. The prototype was used for subsequent flight tests.\n\nThe 10-month 767 flight test program utilized the first six aircraft built. The first four aircraft were equipped with JT9D engines, while the fifth and sixth were fitted with CF6 engines. The test fleet was largely used to evaluate avionics, flight systems, handling, and performance, while the sixth aircraft was used for route-proving flights. During testing, pilots described the 767 as generally easy to fly, with its maneuverability unencumbered by the bulkiness associated with larger wide-body jets. Following 1,600 hours of flight tests, the JT9D-powered 767-200 received certification from the US Federal Aviation Administration (FAA) and the UK Civil Aviation Authority (CAA) in July 1982. The first delivery occurred on August 19, 1982, to United Airlines. The CF6-powered 767-200 received certification in September 1982, followed by the first delivery to Delta Air Lines on October 25, 1982.\n\nThe 767 entered service with United Airlines on September 8, 1982. The aircraft's first commercial flight used a JT9D-powered 767-200 on the Chicago-to-Denver route. The CF6-powered 767-200 commenced service three months later with Delta Air Lines. Upon delivery, early 767s were mainly deployed on domestic routes, including US transcontinental services. American Airlines and TWA began flying the 767-200 in late 1982, while Air Canada, China Airlines, and El Al began operating the aircraft in 1983. The aircraft's introduction was relatively smooth, with few operational glitches and greater dispatch reliability than prior jetliners. In its first year, the 767 logged a 96.1 percent dispatch rate, which exceeded the industry average for new aircraft. Operators reported generally favorable ratings for the twinjet's sound levels, interior comfort, and economic performance. Resolved issues were minor and included the recalibration of a leading edge sensor to prevent false readings, the replacement of an evacuation slide latch, and the repair of a tailplane pivot to match production specifications.\nSeeking to capitalize on its new wide-body's potential for growth, Boeing offered an extended-range model, the 767-200ER, in its first year of service. Ethiopian Airlines placed the first order for the type in December 1982. Featuring increased gross weight and greater fuel capacity, the extended-range model could carry heavier payloads at distances up to , and was targeted at overseas customers. The 767-200ER entered service with El Al Airline on March 27, 1984. The type was mainly ordered by international airlines operating medium-traffic, long-distance flights.\n\nIn the mid-1980s, the 767 spearheaded the growth of twinjet flights across the northern Atlantic under extended-range twin-engine operational performance standards (ETOPS) regulations, the FAA's safety rules governing transoceanic flights by aircraft with two engines. Before the 767, overwater flight paths of twinjets could be no more than 90 minutes away from diversion airports. In May 1985, the FAA granted its first approval for 120-minute ETOPS flights to 767 operators, on an individual airline basis starting with TWA, provided that the operator met flight safety criteria. This allowed the aircraft to fly overseas routes at up to two hours' distance from land. The larger safety margins were permitted because of the improved reliability demonstrated by the twinjet and its turbofan engines. The FAA lengthened the ETOPS time to 180 minutes for CF6-powered 767s in 1989, making the type the first to be certified under the longer duration, and all available engines received approval by 1993. Regulatory approval spurred the expansion of transoceanic 767 flights and boosted the aircraft's sales.\n\nForecasting airline interest in larger-capacity models, Boeing announced the stretched 767-300 in 1983 and the extended-range 767-300ER in 1984. Both models offered a 20 percent passenger capacity increase, while the extended-range version was capable of operating flights up to . Japan Airlines placed the first order for the 767-300 in September 1983. Following its first flight on January 30, 1986, the type entered service with Japan Airlines on October 20, 1986. The 767-300ER completed its first flight on December 9, 1986, but it was not until March 1987 that the first firm order, from American Airlines, was placed. The type entered service with American Airlines on March 3, 1988. The 767-300 and 767-300ER gained popularity after entering service, and came to account for approximately two-thirds of all 767s sold.\n\nAfter the debut of the first stretched 767s, Boeing sought to address airline requests for greater capacity by proposing larger models, including a partial double-deck version informally named the \"Hunchback of Mukilteo\" (from a town near Boeing's Everett factory) with a 757 body section mounted over the aft main fuselage. In 1986, Boeing proposed the 767-X, a revised model with extended wings and a wider cabin, but received little interest. By 1988, the 767-X had evolved into an all-new twinjet, which revived the 777 designation. Until the 777's 1995 debut, the 767-300 and 767-300ER remained Boeing's second-largest wide-bodies behind the 747.\nBuoyed by a recovering global economy and ETOPS approval, 767 sales accelerated in the mid-to-late 1980s; 1989 was the most prolific year with 132 firm orders. By the early 1990s, the wide-body twinjet had become its manufacturer's annual best-selling aircraft, despite a slight decrease due to economic recession. During this period, the 767 became the most common airliner for transatlantic flights between North America and Europe. By the end of the decade, 767s crossed the Atlantic more frequently than all other aircraft types combined. The 767 also propelled the growth of point-to-point flights which bypassed major airline hubs in favor of direct routes. Taking advantage of the aircraft's lower operating costs and smaller capacity, operators added non-stop flights to secondary population centers, thereby eliminating the need for connecting flights. The increased number of cities receiving non-stop services caused a paradigm shift in the airline industry as point-to-point travel gained prominence at the expense of the traditional hub-and-spoke model.\n\nIn February 1990, the first 767 equipped with Rolls-Royce RB211 turbofans, a 767-300, was delivered to British Airways. Six months later, the carrier temporarily grounded its entire 767 fleet after discovering cracks in the engine pylons of several aircraft. The cracks were related to the extra weight of the RB211 engines, which are heavier than other 767 engines. During the grounding, interim repairs were conducted to alleviate stress on engine pylon components, and a parts redesign in 1991 prevented further cracks. Boeing also performed a structural reassessment, resulting in production changes and modifications to the engine pylons of all 767s in service.\nIn January 1993, following an order from UPS Airlines, Boeing launched a freighter variant, the 767-300F, which entered service with UPS on October 16, 1995. The 767-300F featured a main deck cargo hold, upgraded landing gear, and strengthened wing structure. In November 1993, the Japanese government launched the first 767 military derivative when it placed orders for the , an Airborne Early Warning and Control (AWACS) variant based on the 767-200ER. The first two , featuring extensive modifications to accommodate surveillance radar and other monitoring equipment, were delivered in 1998 to the Japan Self-Defense Forces.\n\nIn November 1995, after abandoning development of a smaller version of the 777, Boeing announced that it was revisiting studies for a larger 767. The proposed 767-400X, a second stretch of the aircraft, offered a 12 percent capacity increase versus the 767-300, and featured an upgraded flight deck, enhanced interior, and greater wingspan. The variant was specifically aimed at Delta Air Lines' pending replacement of its aging Lockheed L-1011 TriStars, and faced competition from the A330-200, a shortened derivative of the Airbus A330. In March 1997, Delta Air Lines launched the 767-400ER when it ordered the type to replace its L-1011 fleet. In October 1997, Continental Airlines also ordered the 767-400ER to replace its McDonnell Douglas DC-10 fleet. The type completed its first flight on October 9, 1999, and entered service with Continental Airlines on September 14, 2000.\n\nIn the early 2000s, cumulative 767 deliveries approached 900, but new sales declined during an airline industry downturn. In 2001, Boeing dropped plans for a longer-range model, the 767-400ERX, in favor of the proposed Sonic Cruiser, a new jetliner which aimed to fly 15 percent faster while having comparable fuel costs as the 767. The following year, Boeing announced the KC-767 Tanker Transport, a second military derivative of the 767-200ER. Launched with an order in October 2002 from the Italian Air Force, the KC-767 was intended for the dual role of refueling other aircraft and carrying cargo. The Japanese government became the second customer for the type in March 2003. In May 2003, the United States Air Force (USAF) announced its intent to lease KC-767s to replace its aging KC-135 tankers. The plan was suspended in March 2004 amid a conflict of interest scandal, resulting in multiple US government investigations and the departure of several Boeing officials, including Philip Condit, the company's chief executive officer, and chief financial officer Michael Sears. The first KC-767s were delivered in 2008 to the Japan Self-Defense Forces.\n\nIn late 2002, after airlines expressed reservations about its emphasis on speed over cost reduction, Boeing halted development of the Sonic Cruiser. The following year, the manufacturer announced the 7E7, a mid-size 767 successor made from composite materials which promised to be 20 percent more fuel efficient. The new jetliner was the first stage of a replacement aircraft initiative called the Boeing Yellowstone Project. Customers embraced the 7E7, later renamed 787 Dreamliner, and within two years it had become the fastest-selling airliner in the company's history. In 2005, Boeing opted to continue 767 production despite record Dreamliner sales, citing a need to provide customers waiting for the 787 with a more readily available option. Subsequently, the 767-300ER was offered to customers affected by 787 delays, including All Nippon Airways and Japan Airlines. Some aging 767s, exceeding 20 years in age, were also kept in service past planned retirement dates due to the delays. To extend the operational lives of older aircraft, airlines increased heavy maintenance procedures, including D-check teardowns and inspections for corrosion, a recurring issue on aging 767s. The first 787s entered service with All Nippon Airways in October 2011, 42 months behind schedule.\nIn 2007, the 767 received a production boost when UPS and DHL Aviation placed a combined 33 orders for the 767-300F. Renewed freighter interest led Boeing to consider enhanced versions of the 767-200 and 767-300F with increased gross weights, 767-400ER wing extensions, and 777 avionics. However, net orders for the 767 declined from 24 in 2008 to just three in 2010. During the same period, operators upgraded aircraft already in service; in 2008, the first 767-300ER retrofitted with blended winglets from Aviation Partners Incorporated debuted with American Airlines. The manufacturer-sanctioned winglets, at in height, improved fuel efficiency by an estimated 6.5 percent. Other carriers including All Nippon Airways and Delta Air Lines also ordered winglet kits.\n\nOn February 2, 2011, the 1,000th 767 rolled out, destined for All Nippon Airways. The aircraft was the 91st 767-300ER ordered by the Japanese carrier, and with its completion the 767 became the second wide-body airliner to reach the thousand-unit milestone after the 747. The 1,000th aircraft also marked the last model produced on the original 767 assembly line. Beginning with the 1,001st aircraft, production moved to another area in the Everett factory which occupied about half of the previous floor space. The new assembly line made room for 787 production and aimed to boost manufacturing efficiency by over twenty percent.\n\nAt the inauguration of its new assembly line, the 767's order backlog numbered approximately 50, only enough for production to last until 2013. Despite the reduced backlog, Boeing officials expressed optimism that additional orders would be forthcoming. On February 24, 2011, the USAF announced its selection of the KC-767 Advanced Tanker, an upgraded variant of the KC-767, for its KC-X fleet renewal program. The selection followed two rounds of tanker competition between Boeing and Airbus parent EADS, and came eight years after the USAF's original 2003 announcement of its plan to lease KC-767s. The tanker order encompassed 179 aircraft and was expected to sustain 767 production past 2013.\n\nIn December 2011, FedEx Express announced a 767-300F order for 27 aircraft to replace its DC-10 freighters, citing the USAF tanker order and Boeing's decision to continue production as contributing factors. FedEx Express agreed to buy an additional 19 of the −300F variant in June 2012. In June 2015, FedEx said it was accelerating retirements of planes both to reflect demand and to modernize its fleet, recording charges of $276 million. On July 21, 2015 FedEx announced an order for 50 767-300F with options on another 50, the largest order for the type. FedEx confirmed that it has firm orders for 106 of the freighters for delivery between 2018 and 2023.\n\nThe 767 is a low-wing cantilever monoplane with a conventional tail unit featuring a single fin and rudder. The wings are swept at 31.5 degrees and optimized for a cruising speed of Mach 0.8 (). Each wing features a supercritical cross-section and is equipped with six-panel leading edge slats, single- and double-slotted flaps, inboard and outboard ailerons, and six spoilers. The airframe further incorporates Carbon-fiber-reinforced polymer composite material wing surfaces, Kevlar fairings and access panels, plus improved aluminum alloys, which together reduce overall weight by versus preceding aircraft.\n\nTo distribute the aircraft's weight on the ground, the 767 has a retractable tricycle landing gear with four wheels on each main gear and two for the nose gear. The original wing and gear design accommodated the stretched 767-300 without major changes. The 767-400ER features a larger, more widely spaced main gear with 777 wheels, tires, and brakes. To prevent damage if the tail section contacts the runway surface during takeoff, 767-300 and 767-400ER models are fitted with a retractable tailskid. The 767 has left-side exit doors near the front and rear of the aircraft.\n\nIn addition to shared avionics and computer technology, the 767 uses the same auxiliary power unit, electric power systems, and hydraulic parts as the 757. A raised cockpit floor and the same forward cockpit windows result in similar pilot viewing angles. Related design and functionality allows 767 pilots to obtain a common type rating to operate the 757 and share the same seniority roster with pilots of either aircraft.\n\nThe original 767 flight deck uses six Rockwell Collins CRT screens to display Electronic flight instrument system (EFIS) and engine indication and crew alerting system (EICAS) information, allowing pilots to handle monitoring tasks previously performed by the flight engineer. The CRTs replace conventional electromechanical instruments found on earlier aircraft. An enhanced flight management system, improved over versions used on early 747s, automates navigation and other functions, while an automatic landing system facilitates CAT IIIb instrument landings in low visibility situations. The 767 became the first aircraft to receive CAT IIIb certification from the FAA for landings with minimum visibility in 1984. On the 767-400ER, the cockpit layout is simplified further with six Rockwell Collins liquid crystal display (LCD) screens, and adapted for similarities with the 777 and the Next Generation 737. To retain operational commonality, the LCD screens can be programmed to display information in the same manner as earlier 767s. In 2012, Boeing and Rockwell Collins launched a further 787-based cockpit upgrade for the 767, featuring three landscape-format LCD screens that can display two windows each.\n\nThe 767 is equipped with three redundant hydraulic systems for operation of control surfaces, landing gear, and utility actuation systems. Each engine powers a separate hydraulic system, and the third system uses electric pumps. A ram air turbine provides power for basic controls in the event of an emergency. An early form of fly-by-wire is employed for spoiler operation, utilizing electric signaling instead of traditional control cables. The fly-by-wire system reduces weight and allows independent operation of individual spoilers.\n\nThe 767 features a twin-aisle cabin with a typical configuration of six abreast in business class and seven across in economy. The standard seven abreast, 2–3–2 economy class layout places approximately 87 percent of all seats at a window or aisle. As a result, the aircraft can be largely occupied before center seats need to be filled, and each passenger is no more than one seat from the aisle. It is possible to configure the aircraft with extra seats for up to an eight abreast configuration, but this is less common.\n\nThe 767 interior introduced larger overhead bins and more lavatories per passenger than previous aircraft. The bins are wider to accommodate garment bags without folding, and strengthened for heavier carry-on items. A single, large galley is installed near the aft doors, allowing for more efficient meal service and simpler ground resupply. Passenger and service doors are an overhead plug type, which retract upwards, and commonly used doors can be equipped with an electric-assist system.\n\nIn 2000, a 777-style interior, known as the Boeing Signature Interior, debuted on the 767-400ER. Subsequently adopted for all new-build 767s, the Signature Interior features even larger overhead bins, indirect lighting, and sculpted, curved panels. The 767-400ER also received larger windows derived from the 777. Older 767s can be retrofitted with the Signature Interior. Some operators have adopted a simpler modification known as the Enhanced Interior, featuring curved ceiling panels and indirect lighting with minimal modification of cabin architecture, as well as aftermarket modifications such as the NuLook 767 package by Heath Tecna.\n\nThe 767 has been produced in three fuselage lengths. These debuted in progressively larger form as the 767-200, 767-300, and 767-400ER. Longer-range variants include the 767-200ER and 767-300ER, while cargo models include the 767-300F, a production freighter, and conversions of passenger 767-200 and 767-300 models.\n\nWhen referring to different variants, Boeing and airlines often collapse the model number (767) and the variant designator (e.g. –200 or –300) into a truncated form (e.g. \"762\" or \"763\"). Subsequent to the capacity number, designations may append the range identifier. The International Civil Aviation Organization (ICAO) aircraft type designator system uses a similar numbering scheme, but adds a preceding manufacturer letter; all variants based on the 767-200 and 767-300 are classified under the codes \"B762\" and \"B763\"; the 767-400ER receives the designation of \"B764.\"\n\nThe 767-200 was the original model and entered service with United Airlines in 1982. The type has been used primarily by mainline U.S. carriers for domestic routes between major hub centers such as Los Angeles to Washington. The 767-200 was the first aircraft to be used on transatlantic ETOPS flights, beginning with TWA on February 1, 1985 under 90-minute diversion rules. Deliveries for the variant totaled 128 aircraft. There were 44 passenger and freighter conversions of the model in commercial service as of July 2016. The type's competitors included the Airbus A300 and A310.\n\nThe 767-200 ceased production in the late 1980s, superseded by the extended-range 767-200ER. Some early 767-200s were subsequently upgraded to extended-range specification. In 1998, Boeing began offering 767-200 conversions to 767-200SF (Special Freighter) specification for cargo use, and Israel Aerospace Industries has been licensed to perform cargo conversions since 2005. The conversion process entails the installation of a side cargo door, strengthened main deck floor, and added freight monitoring and safety equipment. The 767-200SF was positioned as a replacement for Douglas DC-8 freighters.\n\nA commercial freighter version of the Boeing 767-200 with series 300 wings and an updated flightdeck was first flown on 29 December 2014. A military tanker variant of the Boeing 767-2C is being developed for the USAF as the KC-46. Boeing is building two aircraft as commercial freighters which will be used to obtain Federal Aviation Administration certification, a further two Boeing 767-2Cs will be modified as military tankers. , Boeing does not have customers for the freighter.\n\nThe 767-200ER was the first extended-range model and entered service with El Al in 1984. The type's increased range is due to an additional center fuel tank and a higher maximum takeoff weight (MTOW) of up to . The type was originally offered with the same engines as the 767-200, while more powerful Pratt & Whitney PW4000 and General Electric CF6 engines later became available. The 767-200ER was the first 767 to complete a non-stop transatlantic journey, and broke the flying distance record for a twinjet airliner on April 17, 1988 with an Air Mauritius flight from Halifax, Nova Scotia to Port Louis, Mauritius, covering . The 767-200ER has been acquired by international operators seeking smaller wide-body aircraft for long-haul routes such as New York to Beijing. Deliveries of the type totaled 121 with no unfilled orders. As of July 2016, 32 examples of passenger and freighter conversion versions were in airline service. The type's main competitors of the time included the Airbus A300-600R and the A310-300.\n\nThe 767-300, the first stretched version of the aircraft, entered service with Japan Airlines in 1986. The type features a fuselage extension over the 767-200, achieved by additional sections inserted before and after the wings, for an overall length of . Reflecting the growth potential built into the original 767 design, the wings, engines, and most systems were largely unchanged on the 767-300. An optional mid-cabin exit door is positioned ahead of the wings on the left, while more powerful Pratt & Whitney PW4000 and Rolls-Royce RB211 engines later became available. The 767-300's increased capacity has been used on high-density routes within Asia and Europe. Deliveries for the type totaled 104 aircraft with no unfilled orders remaining. As of July 2016, 54 of the variant were in airline service. The type's main competitor was the Airbus A300.\n\nThe 767-300ER, the extended-range version of the 767-300, entered service with American Airlines in 1988. The type's increased range was made possible by greater fuel tankage and a higher MTOW of . Design improvements allowed the available MTOW to increase to by 1993. Power is provided by Pratt & Whitney PW4000, General Electric CF6, or Rolls-Royce RB211 engines. Typical routes for the type include Los Angeles to Frankfurt. The combination of increased capacity and range offered by the 767-300ER has been particularly attractive to both new and existing 767 operators. It is the most successful version of the aircraft, with more orders placed than all other variants combined. As of July 2017, 767-300ER deliveries stand at 583 with no unfilled orders. There were 441 examples in service as of July 2016. The type's main competitor is the Airbus A330-200.\n\nThe 767-300F, the production freighter version of the 767-300ER, entered service with UPS Airlines in 1995. The 767-300F can hold up to 24 standard pallets on its main deck and up to 30 LD2 unit load devices on the lower deck, with a total cargo volume of . The freighter has a main deck cargo door and crew exit, while the lower deck features two port-side cargo doors and one starboard cargo door. A general market version with onboard freight-handling systems, refrigeration capability, and crew facilities was delivered to Asiana Airlines on August 23, 1996. As of July 2017, 767-300F deliveries stand at 128 with 64 unfilled orders. Airlines operated 134 examples of the freighter variant and freighter conversions in July 2016.\n\nIn June 2008, All Nippon Airways took delivery of the first 767-300BCF (Boeing Converted Freighter), a modified passenger-to-freighter model. The conversion work was performed in Singapore by ST Aerospace Services, the first supplier to offer a 767-300BCF program, and involved the addition of a main deck cargo door, strengthened main deck floor, and additional freight monitoring and safety equipment. Since then, Boeing, Israel Aerospace Industries, and Wagner Aeronautical have also offered passenger-to-freighter conversion programs for 767-300 series aircraft.\n\nThe 767-400ER, the first Boeing wide-body jet resulting from two fuselage stretches, entered service with Continental Airlines in 2000. The type features a stretch over the 767-300, for a total length of . The wingspan is also increased by through the addition of raked wingtips. Other differences include an updated cockpit, redesigned landing gear, and 777-style Signature Interior. Power is provided by uprated Pratt & Whitney PW4000 or General Electric CF6 engines.\n\nThe FAA granted approval for the 767-400ER to operate 180-minute ETOPS flights before it entered service. Because its fuel capacity was not increased over preceding models, the 767-400ER has a range of , less than previous extended-range 767s. This is roughly the distance from Shenzhen to Seattle. No 767-400 version was developed, while a longer-range version, the 767-400ERX, was offered for sale in 2000 before it was cancelled a year later, leaving the 767-400ER as the sole version of the largest 767. Boeing dropped the 767-400ER and the -200ER from its pricing list in 2014. A total of 37 aircraft were delivered to the variant's two airline customers, Continental Airlines (now merged with United Airlines) and Delta Air Lines, with no unfilled orders. All 37 examples of the -400ER were in service in July 2016. One additional example was produced as a military testbed, and later sold as a VIP transport. The type's closest competitor is the Airbus A330-200.\n\nVersions of the 767 serve in a number of military and government applications, with responsibilities ranging from airborne surveillance and refueling to cargo and VIP transport. Several military 767s have been derived from the 767-200ER, the longest-range version of the aircraft.\n\n\nBoeing offered the 767-400ERX, a longer-range version of the largest 767 model, in 2000. Introduced concurrently with the 747X, the type was to be powered by the 747X's engines, the Engine Alliance GP7000 and the Rolls-Royce Trent 600. An increased range of was specified. Kenya Airways provisionally ordered three 767-400ERXs to supplement its 767 fleet, but after Boeing cancelled the type's development in 2001, switched the order to the 777-200ER.\n\nThe Northrop Grumman E-10 MC2A was to be a 767-400ER-based replacement for the USAF's 707-based E-3 Sentry AWACS, Northrop Grumman E-8 Joint STARS, and RC-135 SIGINT aircraft. The E-10 MC2A would have included an all-new AWACS system, with a powerful active electronically scanned array (AESA) that was also capable of jamming enemy aircraft or missiles. One 767-400ER aircraft was produced as a testbed for systems integration, but the program was terminated in January 2009 and the prototype was sold to Bahrain as a VIP transport.\n\nIn July 2016, 742 aircraft were in airline service: 76 -200s, 629 -300 and 37 -400 with 77 -300 on order; the largest operators are Delta Air Lines (91), UPS Airlines (59 - largest cargo operator), United Airlines (51), American Airlines (40), Japan Airlines (40), All Nippon Airways (37).\n\nThe largest customers are Delta Air Lines with 117 orders, FedEx (108), All Nippon Airways (96), and United Airlines (82). Delta and United are the only customers of all -200, -300 and -400 passenger variants. In July 2015, FedEx placed a firm order for 50 Boeing 767 freighters with deliveries from 2018 to 2023.\n\n\n\nAs of May 2017, the Boeing 767 has been in 45 aviation occurrences, including 16 hull-loss accidents. Six fatal crashes, including three hijackings, have resulted in a total of 851 occupant fatalities. The airliner's first fatal crash, Lauda Air Flight 004, occurred near Bangkok on May 26, 1991, following the in-flight deployment of the left engine thrust reverser on a 767-300ER; none of the 223 aboard survived; as a result of this accident all 767 thrust reversers were deactivated until a redesign was implemented. Investigators determined that an electronically controlled valve, common to late-model Boeing aircraft, was to blame. A new locking device was installed on all affected jetliners, including 767s. On October 31, 1999, EgyptAir Flight 990, a 767-300ER, crashed off Nantucket Island, Massachusetts, in international waters killing all 217 people on board. The US National Transportation Safety Board (NTSB) determined the probable cause to be due to a deliberate action by the first officer; Egypt disputed this conclusion. On April 15, 2002, Air China Flight 129, a 767-200ER, crashed into a hill amid inclement weather while trying to land at Gimhae International Airport in Busan, South Korea. The crash resulted in the death of 129 of the 166 people on board, and the cause was attributed to pilot error.\nAn early 767 incident was survived by all on board. On July 23, 1983, Air Canada Flight 143, a 767-200, ran out of fuel in-flight and had to glide with both engines out for almost to an emergency landing at Gimli, Manitoba. The pilots used the aircraft's ram air turbine to power the hydraulic systems for aerodynamic control. There were no fatalities and only minor injuries. This aircraft was nicknamed \"Gimli Glider\" after its landing site. The aircraft, registered C-GAUN, continued flying for Air Canada until its retirement in January 2008.\n\nThe 767 has been involved in six hijackings, three resulting in loss of life, for a combined total of 282 occupant fatalities. On November 23, 1996, Ethiopian Airlines Flight 961, a 767-200ER, was hijacked and crash-landed in the Indian Ocean near the Comoros Islands after running out of fuel, killing 125 out of the 175 persons on board; survivors have been rare among instances of land-based aircraft ditching on water. Two 767s were involved in the September 11 attacks on the World Trade Center in 2001, resulting in the collapse of its two main towers. American Airlines Flight 11, a 767-200ER, crashed into the north tower, killing all 92 people on board, and United Airlines Flight 175, a 767-200, crashed into the south tower, with the death of all 65 on board. In addition, more than 2,600 people were killed in the towers or on the ground. A foiled 2001 shoe bomb plot involving an American Airlines 767-300ER resulted in passengers being required to remove their shoes for scanning at US security checkpoints.\n\nOn November 1, 2011, LOT Polish Airlines Flight 16, a 767-300ER, safely landed at Warsaw Frederic Chopin Airport in Warsaw, Poland after a mechanical failure of the landing gear forced an emergency landing with the landing gear up. There were no injuries, but the aircraft involved was damaged and subsequently written off. At the time of the incident, aviation analysts speculated that it may have been the first instance of a complete landing gear failure in the 767's service history. Subsequent investigation however determined that while a damaged hose had disabled the aircraft's primary landing gear extension system, an otherwise functional backup system was inoperative due to an accidentally deactivated circuit breaker.\n\nIn January 2014, the US Federal Aviation Administration issued a directive that ordered inspections of the elevators on more than 400 767s beginning in March 2014; the focus is on fasteners and other parts that can fail and cause the elevators to jam. The issue was first identified in 2000 and has been the subject of several Boeing service bulletins. The inspections and repairs are required to be completed within six years. The aircraft has also had multiple occurrences of \"uncommanded escape slide inflation\" during maintenance or operations, and during flight. In late 2015, the FAA issued a preliminary directive to address the issue.\n\nOn October 28, 2016, American Airlines Flight 383, a 767-300ER with 161 passengers and 9 crew members, aborted takeoff at Chicago O'Hare Airport following an uncontained failure of the right GE CF6-80C2 engine. The engine failure, which hurled fragments over a considerable distance, caused a fuel leak resulting in a fire under the right wing. Fire and smoke entered the cabin. All passengers and crew evacuated the aircraft, with 20 passengers and one flight attendant sustaining minor injuries using the evacuation slides. \nAs new 767s roll off the assembly line, older models have been retired and scrapped. One complete aircraft is known to have been retained for exhibition: N102DA, the first 767-200 to operate for Delta Air Lines and the twelfth example built. The exhibition aircraft, named \"The Spirit of Delta\" by the employees who helped purchase it in 1982, underwent restoration at the Delta Air Lines Air Transport Heritage Museum in Atlanta, Georgia. The restoration was completed in 2010.\n\n\n"}
{"id": "4166", "url": "https://en.wikipedia.org/wiki?curid=4166", "title": "Bill Walsh (American football coach)", "text": "Bill Walsh (American football coach)\n\nWilliam Ernest Walsh (November 30, 1931 – July 30, 2007) was an American football coach. He served as head coach of the San Francisco 49ers and the Stanford Cardinal football team, during which time he popularized the West Coast offense. After retiring from the 49ers, Walsh worked as a sports broadcaster for several years and then returned as head coach at Stanford for three seasons.\n\nWalsh went 102–63–1 with the 49ers, winning 10 of his 14 postseason games along with six division titles, three NFC Championship titles, and three Super Bowls. He was named NFL Coach of the Year in 1981 and 1984. In 1993, he was elected to the Pro Football Hall of Fame.\n\nBorn in Los Angeles, Walsh played running back in the San Francisco Bay Area for Hayward High School in Hayward.\nWalsh played quarterback at the College of San Mateo for two seasons. Both John Madden and Walsh played and coached at the College of San Mateo early in their careers. After playing at the College of San Mateo, Walsh transferred to San Jose State University, where he played tight end and defensive end. He also participated in intercollegiate boxing. Walsh graduated from San Jose State with a bachelor's degree in physical education in 1955.\n\nHe served under Bob Bronzan as a graduate assistant coach on the Spartans football coaching staff and graduated with a master's degree in physical education from San Jose State in 1959. His master's thesis was entitled \"Flank Formation Football -- Stress:: Defense\". Thesis 796.W228f.\n\nFollowing graduation, Walsh coached the football and swim teams at Washington High School in Fremont, California.\n\nWalsh was coaching in Fremont when he interviewed for an assistant coaching position with Marv Levy, who had just been hired as the head coach at the University of California, Berkeley.\n\"I was very impressed, individually, by his knowledge, by his intelligence, by his personality, and hired him,\" Levy said. Levy and Walsh, two future NFL Hall of Famers, would never produce a winning season at Cal.\n\nAfter coaching at Cal, Walsh did a stint at Stanford as an assistant coach, before beginning his pro coaching career.\n\nWalsh began his pro coaching career in 1966 as an assistant with the AFL's Oakland Raiders. As a Raider assistant, Walsh was trained in the vertical passing offense favored by Al Davis, putting Walsh in Davis's mentor Sid Gillman's coaching tree.\n\nIn 1967 Walsh was the head coach and general manager of the San Jose Apaches of the Continental Football League (CFL). Walsh led the Apaches to 2nd place in the Pacific Division. Prior to the start of the 1968 CFL season the Apaches ceased all football operations.\n\nIn 1968, Walsh moved to the AFL expansion Cincinnati Bengals, joining the staff of legendary coach Paul Brown. It was there that Walsh developed the philosophy now known as the \"West Coast Offense\", as a matter of necessity. Cincinnati's new quarterback, Virgil Carter, was known for his great mobility and accuracy but lacked a strong arm necessary to throw deep passes. Thus, Walsh modified the vertical passing scheme he had learned during his time with the Raiders, designing a horizontal passing system that relied on quick, short throws - often spreading the ball across the entire width of the field. The new offense was much better suited to Carter's physical abilities; he led the league in pass completion percentage in 1971.\n\nWalsh spent eight seasons as an assistant with the Bengals. Ken Anderson eventually replaced Carter as starting quarterback, and together with star wide receiver Isaac Curtis, produced a consistent, effective offensive attack. Initially, Walsh started out as the wide receivers coach from 1968 to 1970 before also coaching the quarterbacks from 1971 to 1975.\n\nWhen Brown retired as head coach following the 1975 season and appointed Bill \"Tiger\" Johnson as his successor, Walsh resigned and served as an assistant coach for Tommy Prothro with the San Diego Chargers in 1976. In a 2006 interview, Walsh claimed that during his tenure with the Bengals, Brown \"worked against my candidacy\" to be a head coach anywhere in the league. \"All the way through I had opportunities, and I never knew about them,\" Walsh said. \"And then when I left him, he called whoever he thought was necessary to keep me out of the NFL.\"\n\nIn 1977, Walsh was hired as the head coach at Stanford where he stayed for two seasons. His two Stanford teams were successful, posting a 9–3 record in 1977 with a win in the Sun Bowl, and 8–4 in 1978 with a win in the Bluebonnet Bowl. His notable players at Stanford included quarterbacks Guy Benjamin and Steve Dils, wide receivers James Lofton and Ken Margerum, linebacker Gordy Ceresino, in addition to running back Darrin Nelson. Walsh was the Pac-8 Conference Coach of the Year in 1977.\n\nIn 1979, Walsh was hired as head coach of the San Francisco 49ers by owner Edward J. DeBartolo, Jr. The long-suffering 49ers went 2–14 in 1978, the season before Walsh's arrival and repeated the same dismal record in his first season. But, Walsh got the entire organization to buy into his philosophy and vowed to turn around a miserable situation. He also drafted quarterback Joe Montana from Notre Dame in the third round. Despite their second consecutive 2-14 record, the 49ers were playing more competitive football.\n\nIn 1980, Steve DeBerg was the starting quarterback who got San Francisco off to a 3-0 start, but after a 59-14 blowout loss to Dallas in week 6, Walsh promoted Montana to starting QB. In a Sunday game, December 7 vs. the New Orleans Saints, Montana brought the 49ers back from a 35-7 halftime deficit to win 38-35 in overtime. The 49ers improved to 6–10, but more importantly, Walsh had them making great strides and they were getting better every week.\n\nIn 1981, four important wins were two wins each over the Los Angeles Rams and the Dallas Cowboys. The Rams were only one year removed from a Super Bowl appearance, and had dominated the series with the 49ers since 1967 winning 23, losing 3 and tying 1. San Francisco's two wins over the Rams in 1981 marked the shift of dominance in favor of the 49ers that lasted until 1998 with 30 wins (including 17 consecutively) against only 6 defeats. The 49ers blew out the Cowboys in week 6 of the regular season. On \"Monday Night Football\" that week, the win was not included in the halftime highlights. Walsh felt that this was because the Cowboys were scheduled to play the Rams the next week in a Sunday night game and that showing the highlights of the 49ers' win would potentially hurt the game's ratings. However, Walsh used this as a motivating factor for his team, who felt they were disrespected.\n\nThe 49ers faced the Cowboys again that same season in the NFC title game. The game was very close, and in the fourth quarter Walsh called a series of running plays as the 49ers marched down the field against the Cowboys' prevent defense, which had been expecting the 49ers to mainly pass. The 49ers came from behind to win the game on Dwight Clark's touchdown reception, known as The Catch, propelling Walsh to his first Super Bowl. Walsh would later write that the 49ers' two wins over the Rams showed a shift of power in their division, while the wins over the Cowboys showed a shift of power in the conference.\n\nSan Francisco won its first championship just two years after winning only two games. The 49ers won Super Bowl XVI defeating the Cincinnati Bengals 26-21 in Pontiac, Michigan. Under Walsh the team rose from the cellar to the top of the NFL in just two seasons.\n\nThe 49ers won Super Bowl championships in 1981, 1984 and 1988. Walsh served as 49ers head coach for ten years, and during his tenure he and his coaching staff perfected the style of play known popularly as the West Coast offense. Walsh was nicknamed \"The Genius\" for both his innovative play calling and design. Walsh would regularly script the first 10-15 offensive plays before the start of each game. In the ten years during which Walsh was the 49ers' head coach, San Francisco scored 3,714 points (24.4 per game), the most of any team in the league during that span.\n\nIn addition to Joe Montana, Walsh drafted Ronnie Lott, Charles Haley, and Jerry Rice. He also traded a 2nd and 4th round pick in the 1987 draft for Steve Young. His success with the 49ers was rewarded with his election to the Professional Football Hall of Fame in 1993. Montana, Lott, Haley, Rice and Young were also elected to the Hall of Fame.\n\nMany of Bill Walsh's assistant coaches went on to be head coaches themselves, including George Seifert, Mike Holmgren, Ray Rhodes, and Dennis Green. After Walsh's retirement from the 49ers, Seifert succeeded him as 49ers head coach, and guided San Francisco to victories in Super Bowl XXIV and Super Bowl XXIX. Holmgren won a Super Bowl with the Green Bay Packers, and made 3 Super Bowl appearances as a head coach: 2 with the Packers, and another with the Seattle Seahawks. These coaches in turn have their own disciples who have used Walsh's West Coast system, such as former Washington Redskins head coach Mike Shanahan and former Houston Texans head coach Gary Kubiak. Mike Shanahan was an offensive coordinator under George Seifert and went on to win Super Bowl XXXII and Super Bowl XXXIII during his time as head coach of the Denver Broncos. Kubiak was first a quarterback coach with the 49ers, then offensive coordinator for Shanahan with the Broncos. In 2015, he became head coach and led Denver to victory in Super Bowl 50. Dennis Green trained Tony Dungy, who won a Super Bowl with the Indianapolis Colts, and Brian Billick with his brother-in law and linebackers coach Mike Smith. Billick won a Super Bowl as head coach of the Baltimore Ravens. Mike Holmgren trained many of his assistants to become head coaches, including Jon Gruden and Andy Reid. Gruden won a Super Bowl with the Tampa Bay Buccaneers. Reid served as head coach of the Philadelphia Eagles from 1999-2012, and guided the Eagles to multiple winning seasons and numerous playoff appearances. In addition to this, Marc Trestman, former head coach of the Chicago Bears, served as Offensive Coordinator under Seifert in the 90's. Gruden himself would train Mike Tomlin, who led the Pittsburgh Steelers to their sixth Super Bowl championship, and Jim Harbaugh, whose 49ers would face his brother, John Harbaugh, whom Reid himself trained, and the Baltimore Ravens at Super Bowl XLVII, which marked the Ravens' second World Championship.\n\nBill Walsh was viewed as a strong advocate for African-American head coaches in the NFL and NCAA. Thus, the impact of Walsh also changed the NFL into an equal opportunity for African-American coaches. Along with Ray Rhodes and Dennis Green, Tyrone Willingham became the head coach at Stanford, then later Notre Dame and Washington. One of Mike Shanahan's assistants, Karl Dorrell went on to be the head coach at UCLA. Walsh directly helped propel Dennis Green into the NFL head coaching ranks by offering to take on the head coaching job at Stanford.\n\nMany former and current NFL head coaches trace their lineage back to Bill Walsh on his coaching tree, shown below. Walsh, in turn, belonged to the coaching tree of American Football League great and Hall of Fame coach Sid Gillman of the AFL's Los Angeles/San Diego Chargers and Hall of Fame coach Paul Brown.\nAfter leaving the coaching ranks immediately following his team's victory in Super Bowl XXIII, Walsh went to work as a broadcaster for NBC, teaming with Dick Enberg to form the lead broadcasting team, replacing Merlin Olsen.\n\nWalsh returned to Stanford as head coach in 1992, leading the Cardinal to a 10–3 record and a Pacific-10 Conference co-championship. Stanford finished the season with an upset victory over Penn State in the Blockbuster Bowl on January 1, 1993 and a #9 ranking in the final AP Poll. In 1994, after consecutive losing seasons, Walsh left Stanford and retired from coaching.\n\nIn 1996 Walsh returned to the 49ers as an administrative aide Walsh was the Vice President and General Manager for the 49ers from 1999 to 2001 and was a special consultant to the team for three years afterwards.\n\nIn 2004, Walsh was appointed as special assistant to the athletic director at Stanford. In 2005, after then-athletic director Ted Leland stepped down, Walsh was named interim athletic director. He also acted as a consultant for his alma mater San Jose State University in their search for an Athletic Director and Head Football Coach in 2005.\n\nWalsh was also the author of three books, a motivational speaker, and taught classes at the Stanford Graduate School of Business.\n\nWalsh was a Board Member for the Lott IMPACT Trophy, which is named after Pro Football Hall of Fame defensive back Ronnie Lott, and is awarded annually to college football's Defensive IMPACT Player of the Year. Walsh served as a keynote speaker at the award's banquet.\n\nBill Walsh died of leukemia on July 30, 2007 at his home in Woodside, California.\n\nFollowing Walsh's death, the playing field at the former Candlestick Park was renamed \"Bill Walsh Field\". Additionally, the regular San Jose State versus Stanford football game was renamed the \"Bill Walsh Legacy Game\".\n\nWalsh is survived by his wife Geri, his son Craig and his daughter Elizabeth. Walsh also lost a son, Steve, in 2002. Craig Walsh flipped the coin at Super Bowl XLII in Glendale, Arizona, accompanied by his sister, their mother and several ex-49ers.\n\n\n \n"}
{"id": "4168", "url": "https://en.wikipedia.org/wiki?curid=4168", "title": "Utility knife", "text": "Utility knife\n\nA utility knife is a knife used for general or utility purposes. The utility knife was originally a fixed blade knife with a cutting edge suitable for general work such as cutting hides and cordage, scraping hides, butchering animals, cleaning fish, and other tasks. Craft knives are tools mostly used for crafts. Today, the term \"utility knife\" also includes small folding or retractable-blade knives suited for use in the modern workplace or in the construction industry.\n\nThere is also a utility knife for kitchen use which is between a chef's knife and paring knife in size.\n\nThe fixed-blade utility knife was developed some 500,000 years ago, when human ancestors began to make knives made of stone. These knives were general-purpose tools, designed for cutting and shaping wooden implements, scraping hides, preparing food, and for other utilitarian purposes.\n\nBy the 19th century the fixed-blade utility knife had evolved into a steel-bladed outdoors field knife capable of butchering game, cutting wood, and preparing campfires and meals. With the invention of the backspring, pocket-size utility knives were introduced with folding blades and other folding tools designed to increase the utility of the overall design. The folding pocketknife and utility tool is typified by the \"Camper\" or \"Boy Scout\" pocketknife, the U.S. folding utility knife, the Swiss Army Knife, and by multi-tools fitted with knife blades. The development of stronger locking blade mechanisms for folding knives—as with the Spanish navaja, the Opinel, and the Buck 110 Folding Hunter—significantly increased the utility of such knives when employed for heavy-duty tasks such as preparing game or cutting through dense or tough materials.\n\nThe fixed or folding blade utility knife is popular for both indoor and outdoor use. One of the most popular types of workplace utility knife is the retractable or folding utility knife (also known as a \"Stanley knife\", \"box cutter\", \"X-Acto knife\", or by various other names). These types of utility knives are designed as multi-purpose cutting tools for use in a variety of trades and crafts. Designed to be lightweight and easy to carry and use, utility knives are commonly used in factories, warehouses, construction projects, and other situations where a tool is routinely needed to mark cut lines, trim plastic or wood materials, or to cut tape, cord, strapping, cardboard, or other packaging material.\n\nIn British, Australian and New Zealand English, along with Dutch and Austrian German, a utility knife frequently used in the construction industry is known as a \"Stanley knife\". This name is a genericised trademark named after Stanley Works, a manufacturer of such knives. In Israel and Switzerland, these knives are known as \"Japanese knives\". In Brazil they are known as \"estiletes\" or \"cortadores Olfa\" (the latter, being another genericised trademark). In Portugal and Canada they are also known as \"X-Acto\" (yet another genericised trademark). In India, the Philippines, France, Italy, Egypt, and Germany, they are simply called \"cutter\". In the Flemish region of Belgium it is called \"cuttermes(je)\" (cutter knife). In general Spanish, they are known as \"cortaplumas\" (penknife, when it comes to folding blades); in Spain, Mexico, and Costa Rica, they are colloquially known as \"cutters\"; in Argentina and Uruguay the segmented fixed-blade knives are known as \"Trinchetas\". In Turkey, they are known as \"maket bıçağı\" (which literally translates as \"model knife\").\n\nOther names for the tool are \"box cutter\" or \"boxcutter\", \"razor blade knife\", \"razor knife\", \"carpet knife\", \"pen knife\", \"stationery knife\", \"sheetrock knife\", or \"drywall knife\".\n\nUtility knives may use fixed, folding, or retractable or replaceable blades, and come in a wide variety of lengths and styles suited to the particular set of tasks they are designed to perform. Thus, an outdoors utility knife suited for camping or hunting might use a broad fixed blade, while a utility knife designed for the construction industry might feature a replaceable utility or razor blade for cutting packaging, cutting shingles, marking cut lines, or scraping paint.\n\nLarge fixed-blade utility knives are most often employed in an outdoors context, such as fishing, camping, or hunting. Outdoor utility knives typically feature sturdy blades from in length, with edge geometry designed to resist chipping and breakage.\n\nThe term \"utility knife\" may also refer to small fixed-blade knives used for crafts, model-making and other artisanal projects. These small knives feature light-duty blades best suited for cutting thin, lightweight materials. The small, thin blade and specialized handle permit cuts requiring a high degree of precision and control.\n\nThe largest construction or workplace utility knives typically feature retractable and replaceable blades, made of either die-cast metal or molded plastic. Some use standard razor blades, others specialized double-ended utility blades. The user can adjust how far the blade extends from the handle, so that, for example, the knife can be used to cut the tape sealing a package without damaging the contents of the package. When the blade becomes dull, it can be quickly reversed or switched for a new one. Spare or used blades are stored in the hollow handle of some models, and can be accessed by removing a screw and opening the handle. Other models feature a quick-change mechanism that allows replacing the blade without tools, as well as a flip-out blade storage tray. The blades for this type of utility knife come in both double- and single-ended versions, and are interchangeable with many, but not all, of the later copies. Specialized blades also exist for cutting string, linoleum, and other materials.\nAnother style is a snap-off utility knife that contains a long, segmented blade that slides out from it. As the endmost edge becomes dull, it can be broken off the remaining blade, exposing the next section, which is sharp and ready for use. The snapping is best accomplished with a blade snapper that is often built-in, or a pair of pliers, and the break occurs at the score lines, where the metal is thinnest. When all of the individual segments are used, the knife may be thrown away, or, more often, refilled with a replacement blade. This design was introduced by Japanese manufacturer Olfa Corporation in 1956 as the world's first snap-off blade and was inspired from analyzing the sharp cutting edge produced when glass is broken and how pieces of a chocolate bar break into segments.\nAnother utility knife often used for cutting open boxes consists of a simple sleeve around a rectangular handle into which single-edge utility blades can be inserted. The sleeve slides up and down on the handle, holding the blade in place during use and covering the blade when not in use. The blade holder may either retract or fold into the handle, much like a folding-blade pocketknife. The blade holder is designed to expose just enough edge to cut through one layer of corrugated fiberboard, to minimize chances of damaging contents of cardboard boxes.\n\nMost utility knives are not well suited to use as offensive weapons, with the exception of some outdoor-type utility knives employing longer blades. However, even small razor-blade type utility knives may sometimes find use as slashing weapons. The 9-11 commission report stated passengers in cell phone calls reported knives or \"box-cutters\" were used as weapons, (also Mace or a bomb), in hi-jacking airplanes in the September 11, 2001 terrorist attacks against the United States, though the exact design of the knives used is unknown. Two of the hijackers were known to have purchased Leatherman knives, which feature a 4\" locking blade which were not prohibited on U.S. flights at the time. Those knives were not found in the possessions the two hijackers left behind. Similar cutters, including paper cutters, have also been known to be used as a lethal weapon.\n\nSmall work-type utility knives have also been used to commit robbery and other crimes. In June 2004, a Japanese student was slashed to death with a segmented-type utility knife.\n\nIn the United Kingdom, the law was changed to raise the age limit for purchasing knives, including utility knives, from 16 to 18.\n\n"}
{"id": "4169", "url": "https://en.wikipedia.org/wiki?curid=4169", "title": "Bronze", "text": "Bronze\n\nBronze is an alloy consisting primarily of copper, commonly with about 12% tin and often with the addition of other metals (such as aluminium, manganese, nickel or zinc) and sometimes non-metals or metalloids such as arsenic, phosphorus or silicon. These additions produce a range of alloys that may be harder than copper alone, or have other useful properties, such as stiffness, ductility, or machinability.\n\nThe archeological period where bronze was the hardest metal in widespread use is known as the Bronze Age. In the ancient Near East this began with the rise of Sumer in the 4th millennium BC, with India and China starting to use bronze around the same time; everywhere it gradually spread across regions. The Bronze Age was followed by the Iron Age starting from about 1300 BC and reaching most of Eurasia by about 500 BC, though bronze continued to be much more widely used than it is in modern times.\n\nBecause historical pieces were often made of brasses (copper and zinc) and bronzes with different compositions, modern museum and scholarly descriptions of older objects increasingly use the more inclusive term \"copper alloy\" instead.\n\nThe word \"bronze\" (1730–40) is borrowed from French \"bronze\" (1511), itself borrowed from Italian \"bronzo\" \"bell metal, brass\" (13th century) (transcribed in Medieval Latin as \"bronzium\") from either,\n\nThe discovery of bronze enabled people to create metal objects which were harder and more durable than previously possible. Bronze tools, weapons, armor, and building materials such as decorative tiles were harder and more durable than their stone and copper (\"Chalcolithic\") predecessors. Initially, bronze was made out of copper and arsenic, forming arsenic bronze, or from naturally or artificially mixed ores of copper and arsenic, with the earliest artifacts so far known coming from the Iranian plateau in the 5th millennium BCE. It was only later that tin was used, becoming the major non-copper ingredient of bronze in the late 3rd millennium BC.\n\nTin bronze was superior to arsenic bronze in that the alloying process could be more easily controlled, and the resulting alloy was stronger and easier to cast. Also, unlike arsenic, metallic tin and fumes from tin refining are not toxic. The earliest tin-alloy bronze dates to 4500 BCE in a Vinča culture site in Pločnik (Serbia). Other early examples date to the late 4th millennium BC in Egypt, Susa (Iran) and some ancient sites in China, Luristan (Iran) and Mesopotamia (Iraq).\n\nOres of copper and the far rarer tin are not often found together (exceptions include one ancient site in Thailand and one in Iran), so serious bronze work has always involved trade. Tin sources and trade in ancient times had a major influence on the development of cultures. In Europe, a major source of tin was the British deposits of ore in Cornwall, which were traded as far as Phoenicia in the Eastern Mediterranean.\n\nIn many parts of the world, large hoards of bronze artefacts are found, suggesting that bronze also represented a store of value and an indicator of social status. In Europe, large hoards of bronze tools, typically socketed axes (illustrated above), are found, which mostly show no signs of wear. With Chinese ritual bronzes, which are documented in the inscriptions they carry and from other sources, the case is very clear. These were made in enormous quantities for elite burials, and also used by the living for ritual offerings.\n\nThough bronze is generally harder than wrought iron, with Vickers hardness of 60–258 vs. 30–80, the Bronze Age gave way to the Iron Age after a serious disruption of the tin trade: the population migrations of around 1200–1100 BC reduced the shipping of tin around the Mediterranean and from Britain, limiting supplies and raising prices. As the art of working in iron improved, iron became cheaper and improved in quality. As cultures advanced from hand-wrought iron to machine-forged iron (typically made with trip hammers powered by water), blacksmiths learned how to make steel. Steel is stronger than bronze and holds a sharper edge longer.\n\nBronze was still used during the Iron Age, and has continued in use for many purposes to the modern day.\n\nThere are many different bronze alloys, but typically modern bronze is 88% copper and 12% tin. Alpha bronze consists of the alpha solid solution of tin in copper. Alpha bronze alloys of 4–5% tin are used to make coins, springs, turbines and blades. Historical \"bronzes\" are highly variable in composition, as most metalworkers probably used whatever scrap was on hand; the metal of the 12th-century English Gloucester Candlestick is bronze containing a mixture of copper, zinc, tin, lead, nickel, iron, antimony, arsenic with an unusually large amount of silver – between 22.5% in the base and 5.76% in the pan below the candle. The proportions of this mixture suggests that the candlestick was made from a hoard of old coins. The Benin Bronzes are really brass, and the Romanesque Baptismal font at St Bartholomew's Church, Liège is described as both bronze and brass.\n\nIn the Bronze Age, two forms of bronze were commonly used: \"classic bronze\", about 10% tin, was used in casting; and \"mild bronze\", about 6% tin, was hammered from ingots to make sheets. Bladed weapons were mostly cast from classic bronze, while helmets and armor were hammered from mild bronze.\n\nCommercial bronze (90% copper and 10% zinc) and architectural bronze (57% copper, 3% lead, 40% zinc) are more properly regarded as brass alloys because they contain zinc as the main alloying ingredient. They are commonly used in architectural applications.\n\nBismuth bronze is a bronze alloy with a composition of 52% copper, 30% nickel, 12% zinc, 5% lead, and 1% bismuth. It is able to hold a good polish and so is sometimes used in light reflectors and mirrors.\n\nPlastic bronze is bronze containing a significant quantity of lead which makes for improved plasticity possibly used by the ancient Greeks in their ship construction.\n\nSilicon bronze has a composition of Si: 2.80–3.80%, Mn: 0.50–1.30%, Fe: 0.80% max., Zn: 1.50% max., Pb: 0.05% max., Cu: balance.\n\nOther bronze alloys include aluminium bronze, phosphor bronze, manganese bronze, bell metal, arsenical bronze, speculum metal and cymbal alloys.\n\nBronzes are typically very ductile alloys. By way of comparison, most bronzes are considerably less brittle than cast iron. Typically bronze only oxidizes superficially; once a copper oxide (eventually becoming copper carbonate) layer is formed, the underlying metal is protected from further corrosion. However, if copper chlorides are formed, a corrosion-mode called \"bronze disease\" will eventually completely destroy it. Copper-based alloys have lower melting points than steel or iron, and are more readily produced from their constituent metals. They are generally about 10 percent denser than steel, although alloys using aluminium or silicon may be slightly less dense. Bronze is a better conductor of heat and electricity than most steels. The cost of copper-base alloys is generally higher than that of steels but lower than that of nickel-base alloys.\n\nCopper and its alloys have a huge variety of uses that reflect their versatile physical, mechanical, and chemical properties. Some common examples are the high electrical conductivity of pure copper, the low-friction properties of bearing bronze (bronze which has a high lead content— 6–8%), the resonant qualities of bell bronze (20% tin, 80% copper), and the resistance to corrosion by sea water of several bronze alloys.\n\nThe melting point of bronze varies depending on the ratio of the alloy components and is about . Bronze is usually nonmagnetic, but certain alloys containing iron or nickel may have magnetic properties.\n\nBronze, or bronze-like alloys and mixtures, were used for coins over a longer period. Bronze was especially suitable for use in boat and ship fittings prior to the wide employment of stainless steel owing to its combination of toughness and resistance to salt water corrosion. Bronze is still commonly used in ship propellers and submerged bearings.\n\nIn the 20th century, silicon was introduced as the primary alloying element, creating an alloy with wide application in industry and the major form used in contemporary statuary. Sculptors may prefer silicon bronze because of the ready availability of silicon bronze brazing rod, which allows colour-matched repair of defects in castings. Aluminium is also used for the structural metal aluminium bronze.\n\nIt is also widely used for casting bronze sculptures. Many common bronze alloys have the unusual and very desirable property of expanding slightly just before they set, thus filling in the finest details of a mould. Bronze parts are tough and typically used for bearings, clips, electrical connectors and springs.\n\nBronze also has very low friction against dissimilar metals, making it important for cannons prior to modern tolerancing, where iron cannonballs would otherwise stick in the barrel. It is still widely used today for springs, bearings, bushings, automobile transmission pilot bearings, and similar fittings, and is particularly common in the bearings of small electric motors. Phosphor bronze is particularly suited to precision-grade bearings and springs. It is also used in guitar and piano strings.\n\nUnlike steel, bronze struck against a hard surface will not generate sparks, so it (along with beryllium copper) is used to make hammers, mallets, wrenches and other durable tools to be used in explosive atmospheres or in the presence of flammable vapors. Bronze is used to make bronze wool for woodworking applications where steel wool would discolour oak.\n\nVarious kinds of bronze are used in many different industrial applications.\n\nPhosphor bronze is used for ships' propellers, musical instruments, and electrical contacts. Bearings are often made of bronze for its friction properties. It can be filled with oil to make the proprietary Oilite and similar material for bearings. Aluminium bronze is very hard and is used for bearings and machine tool ways.\n\nThe Assyrian king Sennacherib (704–681 BC) claims to have been the first to cast monumental bronze statues (of up to 30 tonnes) using two-part moulds instead of the lost-wax method.\n\nBronze statues were regarded as the highest form of sculpture in Ancient Greek art, though survivals are few, as bronze was a valuable material in short supply in the Late Antique and medieval periods. Many of the most famous Greek bronze sculptures are known through Roman copies in marble, which were more likely to survive. \nIn India, bronze sculptures from the Kushana (Chausa hoard) and Gupta periods (Brahma from Mirpur-Khas, Akota Hoard, Sultanganj Buddha) and later periods (Hansi Hoard) have been found. Indian Hindu artisans from the period of the Chola empire in Tamil Nadu used bronze to create intricate statues via the lost wax casting method with ornate detailing depicting the deities of Hinduism. The art form survives to this day, with many silpis, craftsmen, working in the areas of Swamimalai and Chennai.\n\nIn antiquity other cultures also produced works of high art using bronze. For example: in Africa, the bronze heads of the Kingdom of Benin; in Europe, Grecian bronzes typically of figures from Greek mythology; in east Asia, Chinese ritual bronzes of the Shang and Zhou dynasty—more often ceremonial vessels but including some figurine examples. Bronze sculptures, although known for their longevity, still undergo microbial degradation; such as from certain species of yeasts.\n\nBronze continues into modern times as one of the materials of choice for monumental statuary.\n\nBefore it became possible to produce glass with acceptably flat surfaces, bronze was a standard material for mirrors. The reflecting surface was typically made slightly convex so that the whole face could be seen in a small mirror. Bronze was used for this purpose in many parts of the world, probably based on independent discoveries.\n\nBronze mirrors survive from the Egyptian Middle Kingdom (2040–1750 BCE). In Europe, the Etruscans were making bronze mirrors in the sixth century BCE, and Greek and Roman mirrors followed the same pattern. Although other materials such as speculum metal had come into use, bronze mirrors were still being made in Japan in the eighteenth century AD.\n\n \nBronze is the preferred metal for bells, in the form of a high tin bronze alloy known colloquially as bell metal, which is about 23% tin.\n\nNearly all professional cymbals are made from bronze, which gives a desirable balance of durability and timbre. Several types of bronze are used, commonly B20 bronze, which is roughly 20% tin, 80% copper, with traces of silver, or the tougher B8 bronze which is made from 8% tin and 92% copper. As the tin content in a bell or cymbal rises, the timbre drops.\n\nBronze is also used for the windings of steel and nylon strings of various stringed instruments such as the double bass, piano, harpsichord, and the guitar. Bronze strings are commonly reserved on pianoforte for the lower pitch tones, as they possess a superior sustain quality to that of high-tensile steel.\n\nBronzes of various metallurgical properties are widely used in struck idiophones around the world, notably bells, singing bowls, gongs, cymbals and other idiophones from Asia. Examples include Tibetan singing bowls, temple bells of many sizes and shapes, gongs, Javanese gamelan and other bronze musical instruments. The earliest bronze archeological finds in Indonesia date from 1–2 BCE, including flat plates probably suspended and struck by a wooden or bone mallet. Ancient bronze drums from Thailand and Vietnam date back 2,000 years. Bronze bells from Thailand and Cambodia date back to 3,600 BCE.\n\nSome companies are now making saxophones from phosphor bronze (3.5 to 10% tin and up to 1% phosphorus content). Bell bronze is used to make the tone rings of many professional model banjos. The tone ring is a heavy (usually 3 lbs.) folded or arched metal ring attached to a thick wood rim, over which a skin, or most often, a plastic membrane (or head) is stretched – it is the bell bronze that gives the banjo a crisp powerful lower register and clear, bell-like treble register-especially in bluegrass music.\n\nBronze has also been used in coins; most “copper” coins are actually bronze, with about 4 percent tin and 1 percent zinc.\n\nAs with coins, bronze has been used in the manufacture of various types of medals for centuries, and are known in contemporary times for being awarded for third place in sporting competitions and other events. The later usage was in part attributed to the choices of gold, silver and bronze to represent the first three Ages of Man in Greek mythology: the Golden Age, when men lived among the gods; the Silver age, where youth lasted a hundred years; and the Bronze Age, the era of heroes, and was first adopted at the 1904 Summer Olympics. At the 1896 event, silver was awarded to winners and bronze to runners-up, while at 1900 other prizes were given, not medals.\n\n"}
{"id": "4170", "url": "https://en.wikipedia.org/wiki?curid=4170", "title": "Benelux", "text": "Benelux\n\nThe Benelux Union (; ) is a politico-economic union of three neighbouring states in western Europe: Belgium, the Netherlands, and Luxembourg.\n\nThe name \"Benelux\" is formed from joining the first two or three letters of each country's name – Belgium Netherlands Luxembourg – and was first used to name the customs agreement that initiated the union (signed in 1944). It is now used more generally to refer to the geographic, economic and cultural grouping of the three countries.\n\nIn 1951, these countries joined West Germany, France, and Italy to form the European Coal and Steel Community, a predecessor of the European Economic Community (EEC) and today's European Union (EU).\n\nThe main institutions of the Union are the Committee of Ministers, the Benelux Parliament, the Council of the Union and the Secretariat-General, while the Benelux Office for Intellectual Property and the Benelux Court of Justice cover the same territory but are not part of the Union.\n\nThe Benelux General Secretariat is located in Brussels. It is the central administrative pillar of the Benelux Union. It handles the secretariat of the Committee of Ministers, the Council of Economic Union and the various committees and working parties.\n\nA Benelux Parliament (originally referred to as an \"Interparliamentary Consultative Council\") was created in 1955. This parliamentary assembly is composed of 21 members of the Dutch parliament, 21 members of the Belgian national and regional parliaments, and 7 members of the Luxembourg parliament.\n\nIn 1944, exiled representatives of the three countries signed the London Customs Convention, the treaty that established the Benelux Customs Union. Ratified in 1947, the treaty was in force from 1948 until it was superseded by the Benelux Economic Union. The treaty establishing the Benelux Economic Union (\"Benelux Economische Unie/Union Économique Benelux\") was signed on 3 February 1958 in The Hague and came into force on 1 November 1960 to promote the free movement of workers, capital, services, and goods in the region. Under the Treaty the Union implies the co-operation of economic, financial and social policies.\n\nThe Benelux Union involves an intergovernmental co-operation.\n\nThe unification of the law of the three Benelux countries is mainly achieved by regulations of its Committee of Ministers, that only bind the three states, but are not directly applicable in their internal legal orders. They only become legally valid after having been incorporated into national law, with the exception of Belgium. The Belgian Court of Cassation decided in 1971 that any self-executing treaties have priority over laws by the Belgian parliament.\n\nThe Treaty establishing the Benelux Union has provided the Committee of Ministers with the following legal instruments: decisions, conventions, recommendations and directives.\n\nThe Committee of Ministers can promulgate decisions in the fields for which it has competence - those fields are explicitly set down in the Union Treaty or the additional conventions. When the Committee of Ministers adopts a decision, it immediately becomes binding on the three governments. For a decision to be also applicable to the citizen, it must be transposed into national law.\n\nThe Union Treaty is not exhaustive. For this reason, Article 19 of the Treaty provides that the Committee of Ministers may conclude additional conventions. These therefore constitute extensions of the Union Treaty. They are submitted to the national parliaments for approval in keeping with the ratification procedure applied in each of the Member States. Thus there are a large number of Benelux conventions in a wide range of subject matters.\n\nIn 1965, the treaty establishing a Benelux Court of Justice was signed. It entered into force in 1974. The Court, composed of judges from the highest courts of the three States, has to guarantee the uniform interpretation of common legal rules. This international judicial institution is located in Brussels.\n\nThe Benelux is particularly active in the field of intellectual property. The three countries established a Benelux Trademarks Office and a Benelux Designs Office, both situated in The Hague. In 2005, they concluded a treaty establishing a \"Benelux Organisation for Intellectual Property\" which replaced both offices upon its entry into force on 1 September 2006. This Organisation is the official body for the registration of trademarks and designs in the Benelux. In addition, it offers the possibility to formally record the existence of ideas, concepts, designs, prototypes and the like.\n\nIn 2000, Belgium and the Netherlands jointly hosted the UEFA European Championship. In June 2007, representatives of the three countries announced they would bid, as a single political entity, for the 2018 FIFA World Cup.\n\nThe Treaty between the Benelux countries establishing the Benelux Economic Union was limited to a period of 50 years. During the following years, and even more so after the creation of the European Union, the Benelux cooperation focused on developing other fields of activity within a constantly changing international context.\n\nAt the end of the 50 years, the governments of the three Benelux countries decided to renew the agreement, taking into account the new aspects of the Benelux-cooperation – such as security – and the new federal government structure of Belgium. The original establishing treaty, set to expire in 2010, was replaced by a new legal framework (called the Treaty revising the Treaty establishing the Benelux Economic Union), which was signed on 17 June 2008.\n\nThe new treaty has no set time limit and the name of the \"Benelux Economic Union\" changed to \"Benelux Union\" to reflect the broad scope on the union. The main objectives of the treaty are the continuation and enlargement of the cooperation between the three member states within a larger European context. The renewed treaty explicitly foresees the possibility that the Benelux countries will cooperate with other European member States or with regional cooperation structures. The new Benelux cooperation focuses on three main topics: internal market and economic union, sustainability, justice and internal affairs. The number of structures in the renewed Treaty has been reduced and thus simplified. Five Benelux institutions remain: the Benelux Committee of Ministers, the Benelux Council, the Benelux Parliament, the Benelux Court of Justice, the Benelux Secretariat General. Beside these five institutions, the Benelux Organisation for Intellectual Property is also present in this Treaty.\n\n\n"}
{"id": "4171", "url": "https://en.wikipedia.org/wiki?curid=4171", "title": "Boston Herald", "text": "Boston Herald\n\nThe Boston Herald is an American daily newspaper whose primary market is Boston, Massachusetts and its surrounding area. It was founded in 1846 and is one of the oldest daily newspapers in the United States. It has been awarded eight Pulitzer Prizes in its history, including four for editorial writing and three for photography before it was converted to tabloid format in 1981. The \"Herald\" was named one of the \"10 Newspapers That 'Do It Right' in 2012 by \"Editor & Publisher\".\n\nThe \"Herald\" history can be traced back through two lineages, the \"Daily Advertiser\" and the old \"Boston Herald\", and two media moguls, William Randolph Hearst and Rupert Murdoch.\n\nThe original \"Boston Herald\" was founded in 1846 by a group of Boston printers jointly under the name of John A. French & Company. The paper was published as a single two-sided sheet, selling for one cent. Its first editor, William O. Eaton, just 22 years old, said \"The \"Herald\" will be independent in politics and religion; liberal, industrious, enterprising, critically concerned with literacy and dramatic matters, and diligent in its mission to report and analyze the news, local and global.\"\n\nIn 1847, the \"Boston Herald\" absorbed the Boston \"American Eagle\" and the Boston \"Daily Times\".\n\nIn October 1917, John H. Higgins, the publisher and treasurer of the Boston Herald bought out its next door neighbor \"The Boston Journal\" and created \"The Boston Herald and Boston Journal\"\n\nEven earlier than the \"Herald\", the weekly \"American Traveler\" was founded in 1825 as a bulletin for stagecoach listings.\n\nThe \"Boston Evening Traveler\" was founded in 1845. The \" Boston Evening Traveler\" was the successor to the weekly \"American Traveler\" and the semi-weekly \"Boston Traveler\". In 1912, the \"Herald\" acquired the \"Traveler\", continuing to publish both under their own names. For many years, the newspaper was controlled by many of the investors in United Shoe Machinery Co. After a newspaper strike in 1967, Herald-Traveler Corp. suspended the afternoon \"Traveler\" and absorbed the evening edition into the Herald to create the \"Boston Herald Traveler.\"\n\nThe \"Boston Daily Advertiser\" was established in 1813 in Boston by Nathan Hale. The paper grew to prominence throughout the 19th century, taking over other Boston area papers. In 1832 The Advertiser took over control of \"The Boston Patriot\", and then in 1840 it took over and absorbed \"The Boston Gazette\". The paper was purchased by William Randolph Hearst in 1917. In 1920 the \"Advertiser\" was merged with \"The Boston Record\", initially the combined newspaper was called the \"Boston Advertiser\" however when the combined newspaper became an illustrated tabloid in 1921 it was renamed \"The Boston American\". Hearst Corp. continued using the name \"Advertiser\" for its Sunday paper until the early 1970s.\n\nOn September 3, 1884, \"The Boston Evening Record\" was started by the \"Boston Advertiser\" as a campaign newspaper. The \"Record\" was so popular that it was made a permanent publication.\n\nIn 1904, William Randolph Hearst began publishing his own newspaper in Boston called \"The American\". Hearst ultimately ended up purchasing the \"Daily Advertiser\" in 1917. By 1938, the \"Daily Advertiser\" had changed to the \"Daily Record\", and \"The American\" had become the \"Sunday Advertiser\". A third paper owned by Hearst, called the \"Afternoon Record\", which had been renamed the \"Evening American\", merged in 1961 with the \"Daily Record\" to form the \"Record American\". The \"Sunday Advertiser\" and \"Record American\" would ultimately be merged in 1972 into \"The Boston Herald Traveler\" a line of newspapers that stretched back to the old \"Boston Herald\".\n\nIn 1946, Herald-Traveler Corporation acquired Boston radio station WHDH. Two years later, WHDH-FM was licensed, and on November 26, 1957, WHDH-TV made its début as an ABC affiliate on channel 5. In 1961, WHDH-TV's affiliation switched to CBS. Herald-Traveler Corp. operated for years under temporary authority from the Federal Communications Commission stemming from controversy over luncheon meetings the newspaper's chief executive had with an FCC commissioner during the original licensing process (Some Boston broadcast historians accuse the \"Boston Globe\" of being covertly behind the proceeding. The \"Herald Traveler\" was Republican in sympathies, and the \"Globe\" then had a firm policy of not endorsing political candidates.) The FCC ordered comparative hearings, and in 1969 a competing applicant, Boston Broadcasters, Inc. was granted a construction permit to replace WHDH-TV on channel 5. Herald-Traveler Corp. fought the decision in court—by this time, revenues from channel 5 were all but keeping the newspaper afloat—but its final appeal ran out in 1972, and on March 19 WHDH-TV was forced to surrender channel 5 to the new WCVB-TV.\n\nWithout a television station to subsidize the newspaper, the \"Herald Traveler\" was no longer able to remain in business, and the newspaper was sold to Hearst Corporation, which published the rival all-day newspaper, the \"Record American\". The two papers were merged to become an all-day paper called the \"Boston Herald Traveler and Record American\" in the morning and \"Record-American and Boston Herald Traveler\" in the afternoon. The first editions published under the new combined name were those of June 19, 1972. The afternoon edition was soon dropped and the unwieldy name shortened to \"Boston Herald American\", with the Sunday edition called the \"Sunday Herald Advertiser\". The \"Herald American\" was printed in broadsheet format, and failed to target a particular readership; where the \"Record American\" had been a typical city tabloid, the \"Herald Traveler\" was a Republican paper.\n\nThe \"Herald American\" converted to tabloid format in September 1981, but Hearst faced steep declines in circulation and advertising. The company announced it would close the \"Herald American\"—making Boston a one-newspaper town—on December 3, 1982. When the deadline came, Australian media baron Rupert Murdoch was negotiating to buy the paper and save it. He closed on the deal after 30 hours of talks with Hearst and newspaper unions—and five hours after Hearst had sent out notices to newsroom employees telling them they were terminated. The newspaper announced its own survival the next day with a full-page headline: \"You Bet We're Alive!\"\n\nMurdoch changed the paper's name back to the \"Boston Herald\". The \"Herald\" continued to grow, expanding its coverage and increasing its circulation until 2001, when nearly all newspapers fell victim to declining circulations and revenue.\n\nIn February 1994, Murdoch's News Corporation was forced to sell the paper, in order that its subsidiary Fox Television Stations could legally consummate its purchase of Fox affiliate WFXT (Channel 25) because Massachusetts Senator Ted Kennedy included language in an appropriations barring one company from owning a newspaper and television station in the same market. Patrick J. Purcell, who was the publisher of the \"Boston Herald\" and a former News Corporation executive, purchased the \"Herald\" and established it as an independent newspaper. Several years later, Purcell would give the \"Herald\" a suburban presence it never had by purchasing the money-losing Community Newspaper Company from Fidelity Investments. Although the companies merged under the banner of Herald Media, Inc., the suburban papers maintained their distinct editorial and marketing identity.\n\nAfter years of operating profits at Community Newspaper and losses at the \"Herald\", Purcell in 2006 sold the suburban chain to newspaper conglomerate Liberty Group Publishing of Illinois, which soon after changed its name to GateHouse Media. The deal, which also saw GateHouse acquiring \"The Patriot Ledger\" and \"The Enterprise\" respectively in south suburban Quincy and Brockton, netted $225 million for Purcell, who vowed to use the funds to clear the \"Herald\"<nowiki>'s</nowiki> debt and reinvest in the Paper.\n\nOn August 5, 2013, the \"Herald\" launched an internet radio station named Boston Herald Radio which includes radio shows by much of the Herald staff. The station's morning lineup is simulcast on 830 AM WCRN from 10 AM Eastern time to 12 noon Eastern time.\n\nThe \"Herald\" four Pulitzer Prizes for Editorial Writing, in 1924, 1927, 1949 and 1954, are among the most awarded to a single newspaper in the category. In 1957 Harry Trask was a young staff photographer at the \"Traveler\" when he was awarded a Pulitzer Prize for his photo sequence of the sinking of in July 1956. \"Herald\" photographer Stanley Forman received two Pulitzer Prizes consecutively in 1976 and 1977, the first for \"Fire Escape Collapse\", a dramatic shot of a young child falling in mid-air from her mother's arms on the upper stories of a burning apartment building to the waiting arms of firefighters below. The 1977 Pulitzer was awarded for \"The Soiling of Old Glory\", as Ted Landsmark, an African American civil rights lawyer, was charged at by a protester with an American flag during the Boston busing crisis. The 1978 Pulitzer for Feature photography for staff coverage of The Blizzard of 1978.\n\nIn 2006, the \"Herald\" won two SABEW awards from the Society of American Business Editors and Writers: one for its breaking news coverage of the takeover of the Boston-based Gillette Company by Procter & Gamble, and another for \"overall excellence.\"\n\n\nBorges has been named Massachusetts Sportswriter of the Year by the National Association of Sportswriters and Sports Broadcasters five times since 1999. He also holds the record for most first prizes and overall awards in the annual competition of the Professional Boxing Writers Association. He has also been awarded a half dozen writing awards in the Associated Press Sports Editors' annual competition and his work has been included in the annual anthology \"Best Sports Stories\" eight times. He has been awarded either a first or second prize 20 times in writing competitions held by the Professional Football Writers Association as well, including multiple awards in the same year three times. In 1995, he was the recipient of the Nat Fleischer Award for boxing journalism from the Boxing Writers Association of America. He is one of less than 25 boxing writers to ever receive that honor.\n\nBorges' hostile opinions have frequently earned him criticism. He has severely criticized Bill Belichick; some media figures, including Bill Simmons, have asserted that this is because Borges relied on former quarterback Drew Bledsoe, benched and traded by Belichick, as his primary source of Patriots information. Borges also wrote a controversial column asserting that Lance Armstrong is not an athlete.\n\nRon Borges also played a part in starting the long running feud between the Boston Globe and Boston sports talk radio station WEEI. In 1999, the \"Boston Globe\"'s executive sports editor banned \"Globe\" sportswriters from appearing on WEEI's afternoon 'The Big Show' after Borges appeared on it and allegedly used a racial slur to describe New York Yankees pitcher Hideki Irabu. Glenn Ordway, host of the show defended Borges stating that he was only trying to 'recall Yankees owner George Steinbrenner's infamous description of Irabu as a \"fat, pussy toad.\" Ordway claims he corrected Borges on the air and was surprised when the ban was announced. Two weeks later, Skwar banned Globe sportswriters from appearing on WEEI's morning Dennis and Callahan Show because of its perceived lowbrow humor. After this ban, WEEI retaliated by banning \"Globe\" sportswriters from all WEEI programs.\n\nIn June 2004, Borges was involved in a physical altercation with \"New York Times\" and MaxBoxing.com reporter Michael Katz at a press conference in Las Vegas. Reports state that Katz was in the process of interviewing boxing promoter Bob Arum when Borges interrupted to ask Arum a question. Katz objected to the interruption and allegedly accused Borges of \"being a shill for\" boxing promoter Don King. In a column earlier in the year Katz had called Borges \"a vomit-smelling sleaze\" and criticized Borges for \"writing about a fight without revealing he was being paid by King to provide television commentary\". Borges responded by striking Katz, who responded by striking at Borges with his cane. Katz was described as \"a short, fat man in his 60s who walks with a cane and wears a neck brace because of chronic back problems\". The fight between the two was broken up by Arum and his aide.\n\nOn March 4, 2007, Borges was caught in plagiarism allegations after an online reader on ESPN.com's New England Patriots message board revealed that there were extensive similarities between a March 4 article by Borges in the Boston Globe and a February 25 article written by sportswriter Mike Sando of the Tacoma News Tribune. On March 5, Borges was suspended for plagiarism by the Globe, without pay, and barred from broadcast appearances for two months.\n\n\nOn May 18, 2007, Less than two weeks after returning from his 2-month suspension for plagiarism, Borges announced his retirement from the \"Globe\". In October 2008, Borges resumed his role as a Boston sportswriter, this time with the \"Boston Herald\".\n\nThe Boston Herald Newspapers in Education (NIE) program provides teachers with classroom newspapers and educational materials designed to help students of all ages and abilities excel. This is made possible through donations from Herald readers and other sponsors. The \"Boston Herald\" is available in two formats: the print edition and the online e-Edition. The website can be found at http://bostonheraldnie.com/\n\nThe \"Boston Herald\" prices are: $1.50($2.00 Outside Greater Boston) daily, $2.00 Sunday.\n\n\n\n"}
{"id": "4173", "url": "https://en.wikipedia.org/wiki?curid=4173", "title": "Babe Ruth", "text": "Babe Ruth\n\nGeorge Herman \"Babe\" Ruth Jr. (February 6, 1895 – August 16, 1948) was an American professional baseball player whose career in Major League Baseball (MLB) spanned 22 seasons, from 1914 through 1935. Nicknamed \"The Bambino\" and \"The Sultan of Swat\", he began his MLB career as a stellar left-handed pitcher for the Boston Red Sox, but achieved his greatest fame as a slugging outfielder for the New York Yankees. Ruth established many MLB batting (and some pitching) records, including career home runs (714), runs batted in (RBIs) (2,213), bases on balls (2,062), slugging percentage (.6897), and on-base plus slugging (OPS) (1.164); the latter two still stand today. Ruth is regarded as one of the greatest sports heroes in American culture and is considered by many to be the greatest baseball player of all time. In , Ruth was elected into the Baseball Hall of Fame as one of its \"first five\" inaugural members.\n\nAt age seven, Ruth was sent to St. Mary's Industrial School for Boys, a reformatory where he learned life lessons and baseball skills from Brother Matthias Boutlier of the Christian Brothers, the school's disciplinarian and a capable baseball player. In 1914, Ruth was signed to play minor-league baseball for the Baltimore Orioles but was soon sold to the Red Sox. By 1916, he had built a reputation as an outstanding pitcher who sometimes hit long home runs, a feat unusual for any player in the pre-1920 dead-ball era. Although Ruth twice won 23 games in a season as a pitcher and was a member of three World Series championship teams with Boston, he wanted to play every day and was allowed to convert to an outfielder. With regular playing time, he broke the MLB single-season home run record in 1919.\n\nAfter that season, Red Sox owner Harry Frazee sold Ruth to the Yankees amid controversy. The trade fueled Boston's subsequent 86 year championship drought and popularized the \"Curse of the Bambino\" superstition. In his 15 years with New York, Ruth helped the Yankees win seven American League (AL) championships and four World Series championships. His big swing led to escalating home run totals that not only drew fans to the ballpark and boosted the sport's popularity but also helped usher in the live-ball era of baseball, in which it evolved from a low-scoring game of strategy to a sport where the home run was a major factor. As part of the Yankees' vaunted \"Murderer's Row\" lineup of 1927, Ruth hit 60 home runs, extending his MLB single-season record. He retired in 1935 after a short stint with the Boston Braves. During his career, Ruth led the AL in home runs during a season twelve times.\n\nRuth's legendary power and charismatic personality made him a larger-than-life figure in the Roaring Twenties. During his career, he was the target of intense press and public attention for his baseball exploits and off-field penchants for drinking and womanizing. His often reckless lifestyle was tempered by his willingness to do good by visiting children at hospitals and orphanages. After his retirement as a player, he was denied a managerial job in baseball, most likely due to poor behavior during parts of his playing career. In his final years, Ruth made many public appearances, especially in support of American efforts in World War II. In 1946, he became ill with cancer, and died two years later.\n\nGeorge Herman Ruth Jr. was born in 1895 at 216 Emory Street in Pigtown, a working-class section of Baltimore, Maryland, named for its meat-packing plants. Its population included recent immigrants from Ireland, Germany and Italy, and African Americans. Ruth's parents, George Herman Ruth Sr. (1871–1918) and Katherine Schamberger, were both of German American ancestry. According to the 1880 census, his parents were born in Maryland. His paternal grandparents were from Prussia and Hanover. Ruth Sr. had a series of jobs, including lightning rod salesman and streetcar operator, before becoming a counterman in a family-owned combination grocery and saloon on Frederick Street. George Ruth Jr. was born in the house of his maternal grandfather, Pius Schamberger, a German immigrant and trade unionist. Only one of young George's seven siblings, his younger sister Mamie, survived infancy.\n\nMany details of Ruth's childhood are unknown, including the date of his parents' marriage. Ruth spoke German in his childhood. When young George was a toddler, the family moved to 339 South Goodyear Street, not far from the rail yards; by the time the boy was 6, his father had a saloon with an upstairs apartment at 426 West Camden Street. Details are equally scanty about why young George was sent at the age of 7 to St. Mary's Industrial School for Boys, a reformatory and orphanage. As an adult, Babe Ruth suggested that not only had he been running the streets and rarely attending school, he was drinking beer when his father was not looking. Some accounts say that, after a violent incident at his father's saloon, the city authorities decided this environment was unsuitable for a small child. At St. Mary's, which George Jr. entered on June 13, 1902, he was recorded as \"incorrigible\"; he spent much of the next twelve years there.\nAlthough St. Mary's inmates received an education, students were also expected to learn work skills and help operate the school, particularly once the boys turned 12. Ruth became a shirtmaker, and was also proficient as a carpenter. He would adjust his own shirt collars, rather than having a tailor do so, even during his well-paid baseball career. The boys, aged 5 to 21, did most work around the facility, from cooking to shoemaking, and renovated St. Mary's in 1912. The food was simple, and the Xaverian Brothers who ran the school insisted on strict discipline; corporal punishment was common. Ruth's nickname there was \"Niggerlips\", as he had large facial features and was darker than most boys at the all-white reformatory.\n\nRuth was sometimes allowed to rejoin his family, or was placed at St. James's Home, a supervised residence with work in the community, but he was always returned to St. Mary's. He was rarely visited by his family; his mother died when he was 12 and by some accounts, he was permitted to leave St. Mary's only to attend the funeral. How Ruth came to play baseball there is uncertain: according to one account, his placement at St. Mary's was due in part to repeatedly breaking Baltimore's windows with long hits while playing street ball; by another, he was told to join a team on his first day at St. Mary's by the school's athletic director, Brother Herman, becoming a catcher even though left-handers rarely play that position. During his time there he also played third base and shortstop, again unusual for a left-hander, and was forced to wear mitts and gloves made for right-handers. He was encouraged in his pursuits by the school's Prefect of Discipline, Brother Matthias Boutlier, a native of Nova Scotia. A large man, Brother Matthias was greatly respected by the boys both for his strength and for his fairness. For the rest of his life, Ruth would praise Brother Matthias, and his running and hitting styles closely resembled his teacher's. Ruth stated, \"I think I was born as a hitter the first day I ever saw him hit a baseball.\" The older man became a mentor and role model to George; biographer Robert W. Creamer commented on the closeness between the two:\n\nThe school's influence remained with Ruth in other ways. He was a lifelong Catholic who would sometimes attend Mass after carousing all night, and he became a well-known member of the Knights of Columbus. He would visit orphanages, schools, and hospitals throughout his life, often avoiding publicity. He was generous to St. Mary's as he became famous and rich, donating money and his presence at fundraisers, and spending $5,000 to buy Brother Matthias a Cadillac in 1926—subsequently replacing it when it was destroyed in an accident. Nevertheless, his biographer Leigh Montville suggested that many of the off-the-field excesses of Ruth's career were driven by the deprivations of his time at St. Mary's.\n\nMost of the boys at St. Mary's played baseball in organized leagues at different levels of proficiency. Ruth later estimated that he played 200 games a year as he steadily climbed the ladder of success. Although he played all positions at one time or another (including infield positions generally reserved for right-handers), he gained stardom as a pitcher. According to Brother Matthias, Ruth was standing to one side laughing at the bumbling pitching efforts of fellow students, and Matthias told him to go in and see if he could do better. Ruth had become the best pitcher at St. Mary's, and when he was 18 in 1913, he was allowed to leave the premises to play weekend games on teams that were drawn from the community. He was mentioned in several newspaper articles, for both his pitching prowess and ability to hit long home runs.\n\nIn early 1914, Ruth signed a professional baseball contract with Jack Dunn, who owned and managed the minor-league Baltimore Orioles, an International League team. The circumstances of Ruth's signing are not known with certainty; historical fact is obscured by stories that cannot all be true. By some accounts, Dunn was urged to attend a game between an all-star team from St. Mary's and one from another Xaverian facility, Mount St. Mary's College. Some versions have Ruth running away before the eagerly awaited game, to return in time to be punished, and then pitching St. Mary's to victory as Dunn watched. Others have Washington Senators pitcher Joe Engel, a Mount St. Mary's graduate, pitching in an alumni game after watching a preliminary contest between the college's freshmen and a team from St. Mary's, including Ruth. Engel watched Ruth play, then told Dunn about him at a chance meeting in Washington. Ruth, in his autobiography, stated only that he worked out for Dunn for a half-hour, and was signed. According to biographer Kal Wagenheim, there were legal difficulties to be straightened out as Ruth was supposed to remain at the school until he turned 21.\nThe train journey to spring training in Fayetteville, North Carolina, in early March was likely Ruth's first outside the Baltimore area. The rookie ballplayer was the subject of various pranks by the veterans, who were probably also the source of his famous nickname. There are various accounts of how Ruth came to be called Babe, but most center on his being referred to as \"Dunnie's babe\" or a variant. \"Babe\" was at that time a common nickname in baseball, with perhaps the most famous to that point being Pittsburgh Pirates pitcher and 1909 World Series hero Babe Adams, who appeared younger than he was.\n\nRuth's first appearance as a professional ballplayer occurred in an inter-squad game on March 7, 1914. He played shortstop and pitched the last two innings of a 15–9 victory. In his second at-bat, Ruth hit a long home run to right field; the blast was locally reported to be longer than a legendary shot that was hit in Fayetteville by Jim Thorpe. Ruth made first appearance against a team in organized baseball in an exhibition game versus the major-league Philadelphia Phillies. Ruth pitched the middle three innings and gave up two runs in the fourth, but then settled down and pitched a scoreless fifth and sixth innings. In a game against the Phillies the following afternoon, Ruth entered during the sixth inning and did not allow a run the rest of the way. The Orioles scored seven runs in the bottom of the eighth inining to overcome a 6–0 deficit, and Ruth was the winning pitcher.\n\nOnce the regular season began, Ruth was a star pitcher who was also dangerous at the plate. The team performed well, yet received almost no attention from the Baltimore press. A third major league, the Federal League, had begun play, and the local franchise, the Baltimore Terrapins, restored that city to the major leagues for the first time since 1902. Few fans visited Oriole Park, where Ruth and his teammates labored in relative obscurity. Ruth may have been offered a bonus and a larger salary to jump to the Terrapins; when rumors to that effect swept Baltimore, giving Ruth the most publicity he had experienced to date, a Terrapins official denied it, stating it was their policy not to sign players under contract to Dunn.\n\nThe competition from the Terrapins caused Dunn to sustain large losses. Although by late June the Orioles were in first place, having won over two-thirds of their games, the paid attendance dropped as low as 150. Dunn explored a possible move by the Orioles to Richmond, Virginia, as well as the sale of a minority interest in the club. These possibilities fell through, leaving Dunn with little choice other than to sell his best players to major league teams to raise money. He offered Ruth to the reigning World Series champions, Connie Mack's Philadelphia Athletics, but Mack had his own financial problems. The Cincinnati Reds and New York Giants expressed interest in Ruth, but Dunn sold his contract, along with those of pitchers Ernie Shore and Ben Egan, to the Boston Red Sox of the American League (AL) on July 4. The sale price was announced as $25,000 but other reports lower the amount to half that, or possibly $8,500 plus the cancellation of a $3,000 loan. Ruth remained with the Orioles for several days while the Red Sox completed a road trip, and reported to the team in Boston on July 11.\n\nOn July 11, 1914, Ruth arrived in Boston with Egan and Shore. Ruth later told the story of how, that morning, he met Helen Woodford, the girl he would first marry. She was a 16-year-old waitress at Landers Coffee Shop, and Ruth related that she served him when he had breakfast there. Other stories, though, suggested that the meeting occurred on another day, and perhaps under other circumstances. Regardless of when he began to woo his first wife, he won his first game for the Red Sox that afternoon, 4–3, over the Cleveland Naps. He pitched to catcher Bill Carrigan, who was also the Red Sox manager. Shore was given a start by Carrigan the next day; he won that and his second start and thereafter was pitched regularly. Ruth lost his second start, and was thereafter little used. As a batter in his major-league debut, Ruth went 0-for-2 against left-hander Willie Mitchell, striking out in his first at bat, before being removed for a pinch hitter in the seventh inning. Ruth was not much noticed by the fans, as Bostonians watched the Red Sox's crosstown rivals, the Braves, begin a legendary comeback that would take them from last place on the Fourth of July to the 1914 World Series championship.\n\nEgan was traded to Cleveland after two weeks on the Boston roster. During his time as a Red Sox, he kept an eye on the inexperienced Ruth, much as Dunn had in Baltimore. When he was traded, no one took his place as supervisor. Ruth's new teammates considered him brash, and would have preferred him, as a rookie, to remain quiet and inconspicuous. When Ruth insisted on taking batting practice despite his being both a rookie who did not play regularly, and a pitcher, he arrived to find his bats sawn in half. His teammates nicknamed him \"the Big Baboon\", a name the swarthy Ruth, who had disliked the nickname \"Niggerlips\" at St. Mary's, detested. Ruth had received a raise on promotion to the major leagues, and quickly acquired tastes for fine food, liquor, and women, among other temptations.\n\nManager Carrigan allowed Ruth to pitch two exhibition games in mid-August. Although Ruth won both against minor-league competition, he was not restored to the pitching rotation. It is uncertain why Carrigan did not give Ruth additional opportunities to pitch. There are legends—filmed for the screen in \"The Babe Ruth Story\" (1948)—that the young pitcher had a habit of signaling his intent to throw a curveball by sticking out his tongue slightly, and that he was easy to hit until this changed. Creamer pointed out that it is common for inexperienced pitchers to display such habits, and the need to break Ruth of his would not constitute a reason to not use him at all. The biographer suggested that Carrigan was unwilling to use Ruth due to poor behavior by the rookie.\nOn July 30, 1914, Boston owner Joseph Lannin had purchased the minor-league Providence Grays, members of the International League. The Providence team had been owned by several people associated with the Detroit Tigers, including star hitter Ty Cobb, and as part of the transaction, a Providence pitcher was sent to the Tigers. To soothe Providence fans upset at losing a star, Lannin announced that the Red Sox would soon send a replacement to the Grays. This was intended to be Ruth, but his departure for Providence was delayed when Cincinnati Reds owner Garry Herrmann claimed him off waivers. After Lannin wrote to Herrmann explaining that the Red Sox wanted Ruth in Providence so he could develop as a player, and would not release him to a major league club, Herrmann allowed Ruth to be sent to the minors. Carrigan later stated that Ruth was not sent down to Providence to make him a better player, but to help the Grays win the International League pennant (league championship).\n\nRuth joined the Grays on August 18, 1914. After Dunn's deals, the Baltimore Orioles managed to hold on to first place until August 15, after which they continued to fade, leaving the pennant race between Providence and Rochester. Ruth was deeply impressed by Providence manager \"Wild Bill\" Donovan, previously a star pitcher with a 25–4 win–loss record for Detroit in 1907; in later years, he credited Donovan with teaching him much about pitching. Ruth was often called upon to pitch, in one stretch starting (and winning) four games in eight days. On September 5 at Maple Leaf Park in Toronto, Ruth pitched a one-hit 9–0 victory, and hit his first professional home run, his only one as a minor leaguer, off Ellis Johnson. Recalled to Boston after Providence finished the season in first place, he pitched and won a game for the Red Sox against the New York Yankees on October 2, getting his first major league hit, a double. Ruth finished the season with a record of 2–1 as a major leaguer and 23–8 in the International League (for Baltimore and Providence). Once the season concluded, Ruth married Helen in Ellicott City, Maryland. Creamer speculated that they did not marry in Baltimore, where the newlyweds boarded with George Ruth Sr., to avoid possible interference from those at St. Mary's—both bride and groom were not yet of age and Ruth remained on parole from that institution until his 21st birthday.\n\nIn March 1915, Ruth reported to his first major league spring training in Hot Springs, Arkansas. Despite a relatively successful first season, he was not slated to start regularly for the Red Sox, who already had two stellar left-handed pitchers: the established stars Dutch Leonard, who had broken the record for the lowest earned run average (ERA) in a single season; and Ray Collins, a 20-game winner in both 1913 and 1914. Ruth was ineffective in his first start, taking the loss in the third game of the season. Injuries and ineffective pitching by other Boston pitchers gave Ruth another chance, and after some good relief appearances, Carrigan allowed Ruth another start, and he won a rain-shortened seven inning game. Ten days later, the manager had him start against the New York Yankees at the Polo Grounds. Ruth took a 3–2 lead into the ninth, but lost the game 4–3 in 13 innings. Ruth, hitting ninth as was customary for pitchers, hit a massive home run into the upper deck in right field off of Jack Warhop. At the time, home runs were rare in baseball, and Ruth's majestic shot awed the crowd. The winning pitcher, Warhop, would in August 1915 conclude a major league career of eight seasons, undistinguished but for being the first major league pitcher to give up a home run to Babe Ruth.\nCarrigan was sufficiently impressed by Ruth's pitching to give him a spot in the starting rotation. Ruth finished the 1915 season 18–8 as a pitcher; as a hitter, he batted .315 and had four home runs. The Red Sox won the AL pennant, but with the pitching staff healthy, Ruth was not called upon to pitch in the 1915 World Series against the Philadelphia Phillies. Boston won in five games; Ruth was used as a pinch hitter in Game Five, but grounded out against Phillies ace Grover Cleveland Alexander. Despite his success as a pitcher, Ruth was acquiring a reputation for long home runs; at Sportsman's Park against the St. Louis Browns, a Ruth hit soared over Grand Avenue, breaking the window of a Chevrolet dealership.\n\nIn 1916, there was attention focused on Ruth for his pitching, as he engaged in repeated pitching duels with the ace of the Washington Senators, Walter Johnson. The two met five times during the season, with Ruth winning four and Johnson one (Ruth had a no decision in Johnson's victory). Two of Ruth's victories were by the score of 1–0, one in a 13-inning game. Of the 1–0 shutout decided without extra innings, AL President Ban Johnson stated, \"That was one of the best ball games I have ever seen.\" For the season, Ruth went 23–12, with a 1.75 ERA and nine shutouts, both of which led the league. Ruth's nine shutouts in 1916 set a league record for left-handers that would remain unmatched until Ron Guidry tied it in 1978. The Red Sox won the pennant and World Series again, this time defeating the Brooklyn Superbas (as the Dodgers were then known) in five games. Ruth started and won Game 2, 2–1, in 14 innings. Until another game of that length was played in 2005, this was the longest World Series game, and Ruth's pitching performance is still the longest postseason complete game victory.\n\nCarrigan retired as player and manager after 1916, returning to his native Maine to be a businessman. Ruth, who played under four managers who are in the National Baseball Hall of Fame, always maintained that Carrigan, who is not enshrined there, was the best skipper he ever played for. There were other changes in the Red Sox organization that offseason, as Lannin sold the team to a three-man group headed by New York theatrical promoter Harry Frazee. Jack Barry was hired by Frazee as manager.\n\nRuth went 24–13 with a 2.01 ERA and six shutouts in 1917, but the Sox finished in second place in the league, nine games behind the Chicago White Sox in the standings. On June 23 at Washington, Ruth made a memorable pitching start. When the home plate umpire 'Brick' Owens called the first four pitches as balls, Ruth threw a punch at him, and was ejected from the game and later suspended for ten days and fined $100. Ernie Shore was called in to relieve Ruth, and was allowed eight warm-up pitches. The runner who had reached base on the walk was caught stealing, and Shore retired all 26 batters he faced to win the game. Shore's feat was listed as a perfect game for many years; in 1991, Major League Baseball's (MLB) Committee on Statistical Accuracy caused it to be listed as a combined no-hitter. In 1917, Ruth was used little as a batter, other than his plate appearances while pitching, and hit .325 with two home runs.\nThe entry of the United States into World War I occurred at the start of the season, and overshadowed the sport. Conscription was introduced in September 1917, and most baseball players in the big leagues were of draft age. This included Barry, who was a player-manager, and who joined the Naval Reserve in an attempt to avoid the draft, only to be called up after the 1917 season. Frazee hired International League President Ed Barrow as Red Sox manager. Barrow had spent the previous 30 years in a variety of baseball jobs, though he never played the game professionally. With the major leagues shorthanded due to the war, Barrow had many holes in the Red Sox lineup to fill.\n\nRuth also noticed these vacancies in the lineup, and, dissatisfied in the role of a pitcher who appeared every four or five days, wanted to play every day at another position. Barrow tried Ruth at first base and in the outfield during the exhibition season, but as the team moved towards Boston and the season opener, restricted him to pitching. At the time, Ruth was possibly the best left-handed pitcher in baseball; allowing him to play another position was an experiment that could have backfired.\n\nInexperienced as a manager, Barrow had player Harry Hooper advise him on baseball game strategy. Hooper urged his manager to allow Ruth to play another position when he was not pitching, arguing to Barrow, who had invested in the club, that the crowds were larger on days when Ruth played, as they were attracted by his hitting. Barrow gave in early in May; Ruth promptly hit home runs in four consecutive games (one an exhibition), the last off of Walter Johnson. For the first time in his career (disregarding pinch-hitting appearances), Ruth was allowed a place in the batting order higher than ninth.\n\nAlthough Barrow predicted that Ruth would beg to return to pitching the first time he experienced a batting slump, that did not occur. Barrow used Ruth primarily as an outfielder in the war-shortened 1918 season. Ruth hit .300, with 11 home runs, enough to secure him a share of the major league home run title with Tillie Walker of the Philadelphia Athletics. He was still occasionally used as a pitcher, and had a 13–7 record with a 2.22 ERA.\n\nIn 1918, the Red Sox won their third pennant in four years and faced the Chicago Cubs in the World Series, which began on September 5, the earliest date in history. The season had been shortened because the government had ruled that baseball players who were eligible for the military would have to be inducted or work in critical war industries, such as armaments plants. Ruth pitched and won Game One for the Red Sox, a 1–0 shutout. Before Game Four, Ruth injured his left hand in a fight; he pitched anyway. He gave up seven hits and six walks, but was helped by outstanding fielding behind him and by his own batting efforts, as a fourth-inning triple by Ruth gave his team a 2–0 lead. The Cubs tied the game in the eighth inning, but the Red Sox scored to take a 3–2 again in the bottom of that inning. After Ruth gave up a hit and a walk to start the ninth inning, he was relieved on the mound by Joe Bush. To keep Ruth and his bat in the game, he was sent to play left field. Bush retired the side to give Ruth his second win of the Series, and the third and last World Series pitching victory of his career, against no defeats, in three pitching appearances. Ruth's effort gave his team a three-games-to-one lead, and two days later the Red Sox won their third Series in four years, four games to two. Before allowing the Cubs to score in Game Four, Ruth pitched consecutive scoreless innings, a record for the World Series that stood for more than 40 years until 1961, broken by Whitey Ford after Ruth's death. Ruth was prouder of that record than he was of any of his batting feats.\nWith the World Series over, Ruth gained exemption from the war draft by accepting a nominal position with a Pennsylvania steel mill. Many industrial establishments took pride in their baseball teams and sought to hire major leaguers. The end of the war in November set Ruth free to play baseball without such contrivances.\n\nDuring the 1919 season, Ruth was used as a pitcher in only 17 of his 130 games and compiled an 8–5 record. Barrow used him as a pitcher mostly in the early part of the season, when the Red Sox manager still had hopes of a second consecutive pennant. By late June, the Red Sox were clearly out of the race, and Barrow had no objection to Ruth concentrating on his hitting, if only because it drew people to the ballpark. Ruth had hit a home run against the Yankees on Opening Day, and another during a month-long batting slump that soon followed. Relieved of his pitching duties, Ruth began an unprecedented spell of slugging home runs, which gave him widespread public and press attention. Even his failures were seen as majestic—one sportswriter noted, \"When Ruth misses a swipe at the ball, the stands quiver\".\n\nTwo home runs by Ruth on July 5, and one in each of two consecutive games a week later, raised his season total to 11, tying his career best from 1918. The first record to fall was the AL single-season mark of 16, set by Ralph \"Socks\" Seybold in 1902. Ruth matched that on July 29, then pulled ahead toward the major league record of 24, set by Buck Freeman in 1899. Ruth reached this on September 8, by which time, writers had discovered that Ned Williamson of the 1884 Chicago White Stockings had hit 27—though in a ballpark where the distance to right field was only . On September 20, \"Babe Ruth Day\" at Fenway Park, Ruth won the game with a home run in the bottom of the ninth inning, tying Williamson. He broke the record four days later against the Yankees at the Polo Grounds, and hit one more against the Senators to finish with 29. The home run at Washington made Ruth the first major league player to hit a home run at all eight ballparks in his league. In spite of Ruth's hitting heroics, the Red Sox finished sixth, games behind the league champion White Sox.\n\nAs an out-of-towner from New York City, Frazee had been regarded with suspicion by Boston's sportswriters and baseball fans when he bought the team. He won them over with success on the field and a willingness to build the Red Sox by purchasing or trading for players. He offered the Senators $60,000 for Walter Johnson, but Washington owner Clark Griffith was unwilling. Even so, Frazee was successful in bringing other players to Boston, especially as replacements for players in the military. This willingness to spend for players helped the Red Sox secure the 1918 title. The 1919 season saw record-breaking attendance, and Ruth's home runs for Boston made him a national sensation. In March 1919 Ruth was reported as having accepted a three-year contract for a total of $27,000, after protracted negotiations. Nevertheless, on December 26, 1919, Frazee sold Ruth's contract to the New York Yankees.\nNot all of the circumstances concerning the sale are known, but brewer and former congressman Jacob Ruppert, the New York team's principal owner, reportedly asked Yankee manager Miller Huggins what the team needed to be successful. \"Get Ruth from Boston\", Huggins supposedly replied, noting that Frazee was perennially in need of money to finance his theatrical productions. In any event, there was precedent for the Ruth transaction: when Boston pitcher Carl Mays left the Red Sox in a 1919 dispute, Frazee had settled the matter by selling Mays to the Yankees, though over the opposition of AL President Johnson.\n\nAccording to one of Ruth's biographers, Jim Reisler, \"why Frazee needed cash in 1919—and large infusions of it quickly—is still, more than 80 years later, a bit of a mystery\". The often-told story is that Frazee needed money to finance the musical \"No, No, Nanette\", which was a Broadway hit and brought Frazee financial security. That play did not open until 1925, however, by which time Frazee had sold the Red Sox. Still, the story may be true in essence: \"No, No, Nanette\" was based on a Frazee-produced play, \"My Lady Friends\", which opened in 1919.\n\nThere were other financial pressures on Frazee, despite his team's success. Ruth, fully aware of baseball's popularity and his role in it, wanted to renegotiate his contract, signed before the 1919 season for $10,000 per year through 1921. He demanded that his salary be doubled, or he would sit out the season and cash in on his popularity through other ventures. Ruth's salary demands were causing other players to ask for more money. Additionally, Frazee still owed Lannin as much as $125,000 from the purchase of the club.\n\nAlthough Ruppert and his co-owner, Colonel Tillinghast Huston, were both wealthy, and had aggressively purchased and traded for players in 1918 and 1919 to build a winning team, Ruppert faced losses in his brewing interests as Prohibition was implemented, and if their team left the Polo Grounds, where the Yankees were the tenants of the New York Giants, building a stadium in New York would be expensive. Nevertheless, when Frazee, who moved in the same social circles as Huston, hinted to the colonel that Ruth was available for the right price, the Yankees owners quickly pursued the purchase.\n\nFrazee sold the rights to Babe Ruth for $100,000, the largest sum ever paid for a baseball player. The deal also involved a $350,000 loan from Ruppert to Frazee, secured by a mortgage on Fenway Park. Once it was agreed, Frazee informed Barrow, who, stunned, told the owner that he was getting the worse end of the bargain. Cynics have suggested that Barrow may have played a larger role in the Ruth sale, as less than a year after, he became the Yankee general manager, and in the following years made a number of purchases of Red Sox players from Frazee. The $100,000 price included $25,000 in cash, and notes for the same amount due November 1 in 1920, 1921, and 1922; Ruppert and Huston assisted Frazee in selling the notes to banks for immediate cash.\n\nThe transaction was contingent on Ruth signing a new contract, which was quickly accomplished—Ruth agreed to fulfill the remaining two years on his contract, but was given a $20,000 bonus, payable over two seasons. The deal was announced on January 6, 1920. Reaction in Boston was mixed: some fans were embittered at the loss of Ruth; others conceded that the slugger had become difficult to deal with. \"The New York Times\" suggested presciently, \"The short right field wall at the Polo Grounds should prove an easy target for Ruth next season and, playing seventy-seven games at home, it would not be surprising if Ruth surpassed his home run record of twenty-nine circuit clouts next Summer.\" According to Reisler, \"The Yankees had pulled off the sports steal of the century.\"\n\nAccording to Marty Appel in his history of the Yankees, the transaction, \"changed the fortunes of two high-profile franchises for decades\". The Red Sox, winners of five of the first sixteen World Series, those played between 1903 and 1919, would not win another pennant until 1946, or another World Series until 2004, a drought attributed in baseball superstition to Frazee's sale of Ruth and sometimes dubbed the \"Curse of the Bambino\". The Yankees, on the other hand, had not won the AL championship prior to their acquisition of Ruth. They won seven AL pennants and four World Series with Ruth, and lead baseball with 40 pennants and 27 World Series titles in their history.\n\nWhen Ruth signed with the Yankees, his transition from a pitcher to a power-hitting outfielder became complete. His fifteen-season Yankee career consisted of over 2,000 games, and Ruth broke many batting records while making only five widely scattered appearances on the mound, winning all of them.\n\nAt the end of April 1920, the Yankees were 4–7, with the Red Sox leading the league with a 10–2 mark. Ruth had done little, having injured himself swinging the bat. Both situations began to change on May 1, when Ruth hit a tape measure home that sent the ball completely out of the Polo Grounds, a feat believed to have been previously accomplished only by Shoeless Joe Jackson. The Yankees won, 6–0, taking three out of four from the Red Sox. Ruth hit his second home run on May 2, and by the end of the month had set a major league record for home runs in a month with 11, and promptly broke it with 13 in June. Fans responded with record attendance: on May 16, Ruth and the Yankees drew 38,600 to the Polo Grounds, a record for the ballpark, and 15,000 fans were turned away. Large crowds jammed stadiums to see Ruth play when the Yankees were on the road.\nThe home runs kept coming; Ruth tied his own record of 29 on July 15, and broke it with home runs in both games of a doubleheader four days later. By the end of July, he had 37, but his pace slackened somewhat after that. Nevertheless, on September 4, he both tied and broke the organized baseball record for home runs in a season, snapping Perry Werden's 1895 mark of 44 in the minor Western League. The Yankees played well as a team, battling for the league lead early in the summer, but slumped in August in the AL pennant battle with Chicago and Cleveland. The championship was won by Cleveland, surging ahead after the Black Sox Scandal broke on September 28 and led to the suspension of many of the team's top players, including Joe Jackson. The Yankees finished third, but drew 1.2 million fans to the Polo Grounds, the first time a team had drawn a seven figure attendance. The rest of the league sold 600,000 more tickets, many fans there to see Ruth, who led the league with 54 home runs, 158 runs, and 137 runs batted in (RBIs).\n\nIn 1920 and afterwards, Ruth was aided in his power hitting by the fact that A.J. Reach Company—the maker of baseballs used in the major leagues—was using a more efficient machine to wind the yarn found within the baseball. The new baseballs went into play in 1920 and ushered the start of the live-ball era; the number of home runs increased by 184 over the previous year across the major leagues. Baseball statistician Bill James pointed out that while Ruth was likely aided by the change in the baseball, there were other factors at work, including the gradual abolition of the spitball (accelerated after the death of Ray Chapman, struck by a pitched ball thrown by Mays in August 1920) and the more frequent use of new baseballs (also a response to Chapman's death). Nevertheless, James theorized that Ruth's 1920 explosion might have happened in 1919, had a full season of 154 games been played rather than 140, had Ruth refrained from pitching 133 innings that season, and if he were playing at any other home field but Fenway Park, where he hit only 9 of 29 home runs.\nYankees business manager Harry Sparrow had died early in the 1920 season; to replace him, Ruppert and Huston hired Barrow. Ruppert and Barrow quickly made a deal with Frazee for New York to acquire some of the players who would be mainstays of the early Yankee pennant-winning teams, including catcher Wally Schang and pitcher Waite Hoyt. The 21-year-old Hoyt became close to Ruth:\n\nRuth hit home runs early and often in the 1921 season, during which he broke Roger Connor's mark for home runs in a career, 138. Each of the almost 600 home runs Ruth hit in his career after that extended his own record. After a slow start, the Yankees were soon locked in a tight pennant race with Cleveland, winners of the 1920 World Series. On September 15, Ruth hit his 55th home run, shattering his year-old single season record. In late September, the Yankees visited Cleveland and won three out of four games, giving them the upper hand in the race, and clinched their first pennant a few days later. Ruth finished the regular season with 59 home runs, batting .378 and with a slugging percentage of .846.\n\nThe Yankees had high expectations when they met the New York Giants in the 1921 World Series, and the Yankees won the first two games with Ruth in the lineup. However, Ruth badly scraped his elbow during Game 2, sliding into third base (he had walked and stolen both second and third bases). After the game, he was told by the team physician not to play the rest of the series. Despite this advice, he did play in the next three games, and pinch-hit in Game Eight of the best-of-nine series, but the Yankees lost, five games to three. Ruth hit .316, drove in five runs and hit his first World Series home run.\nAfter the Series, Ruth and teammates Bob Meusel and Bill Piercy participated in a barnstorming tour in the Northeast. A rule then in force prohibited World Series participants from playing in exhibition games during the offseason, the purpose being to prevent Series participants from replicating the Series and undermining its value. Baseball Commissioner Kenesaw Mountain Landis suspended the trio until May 20, 1922, and fined them their 1921 World Series checks. In August 1922, the rule was changed to allow limited barnstorming for World Series participants, with Landis's permission required.\n\nOn March 6, 1922, Ruth signed a new contract for three years at $52,000 a year. This was the largest sum ever paid to a ballplayer up to that point, and it represented 40% of the team's player payroll. Despite his suspension, Ruth was named the Yankees' new on-field captain prior to the 1922 season. During the suspension, he worked out with the team in the morning, and played exhibition games with the Yankees on their off days. He and Meusel returned on May 20, to a sellout crowd at the Polo Grounds, but Ruth batted 0-for-4, and was booed. On May 25, he was thrown out of the game for throwing dust in umpire George Hildebrand's face, then climbed into the stands to confront a heckler. Ban Johnson ordered him fined, suspended, and stripped of his captaincy. In his shortened season, Ruth appeared in 110 games, batted .315, with 35 home runs, and drove in 99 runs, but the 1922 season was a disappointment in comparison to his two previous dominating years. Despite Ruth's off-year, the Yankees managed to win the pennant and faced the New York Giants in the World Series for the second consecutive year. In the Series, Giants manager John McGraw instructed his pitchers to throw him nothing but curveballs, and Ruth never adjusted. Ruth had just two hits in seventeen at bats, and the Yankees lost to the Giants for the second straight year, by 4–0 (with one tie game). Sportswriter Joe Vila called him, \"an exploded phenomenon\".\n\nAfter the season, Ruth was a guest at an Elks Club banquet, set up by Ruth's agent with Yankee team support. There, each speaker, concluding with future New York mayor Jimmy Walker, censured him for his poor behavior. An emotional Ruth promised reform, and, to the surprise of many, followed through. When he reported to spring training, he was in his best shape as a Yankee, weighing only .\n\nThe Yankees' status as tenants of the Giants at the Polo Grounds had become increasingly uneasy, and in 1922, Giants owner Charles Stoneham stated that the Yankees' lease, expiring after that season, would not be renewed. Ruppert and Huston had long contemplated a new stadium, and had taken an option on property at 161st Street and River Avenue in the Bronx. Yankee Stadium was completed in time for the home opener on April 18, 1923, at which the Babe hit the first home run in what was quickly dubbed \"the House that Ruth Built\". The ballpark was designed with Ruth in mind: although the venue's left-field fence was further from home plate than at the Polo Grounds, Yankee Stadium's right-field fence was closer, making home runs easier to hit for left-handed batters. To spare Ruth's eyes, right field–his defensive position–was not pointed into the afternoon sun, as was traditional; left fielder Meusel was soon suffering headaches from squinting toward home plate.\n\nDuring the 1923 season, The Yankees were never seriously challenged and won the AL pennant by 17 games. Ruth finished the season with a career-high .393 batting average and major-league leading 41 home runs (tied with Cy Williams). Ruth hit a career high 45 doubles in 1923, and he reached base 379 times, then a major league record. For the third straight year, the Yankees faced the Giants in the World Series, which Ruth dominated. He batted .368, walked eight times, scored eight runs, hit three home runs and slugged 1.000 during the series, as the Yankees won their first World Series championship, four games to two.\n\nIn 1924, the Yankees were favored to become the first team to win four consecutive pennants. Plagued by injuries, they found themselves in a battle with the Senators. Although the Yankees won 18 of 22 at one point in September, the Senators beat out the Yankees by two games. Ruth hit .378, winning his only AL batting title, with a league-leading 46 home runs.\n\nRuth had kept up his efforts to stay in shape in 1923 and 1924, but by early 1925 weighed nearly . His annual visit to Hot Springs, Arkansas, where he exercised and took saunas early in the year, did him no good as he spent much of the time carousing in the resort town. He became ill while there, and suffered relapses during spring training. Ruth collapsed in Asheville, North Carolina, as the team journeyed north. He was put on a train for New York, where he was briefly hospitalized. A rumor circulated that he had died, prompting British newspapers to print a premature obituary. In New York, Ruth collapsed again and was found unconscious in his hotel bathroom. He was taken to a hospital where he suffered multiple convulsions. After sportswriter W. O. McGeehan wrote that Ruth's illness was due to binging on hot dogs and soda pop before a game, it became known as \"the bellyache heard 'round the world\". However, the exact cause of his ailment has never been confirmed and remains a mystery. Glenn Stout, in his history of the Yankees, notes that the Ruth legend is \"still one of the most sheltered in sports\"; he suggests that alcohol was at the root of Ruth's illness, pointing to the fact that Ruth remained six weeks at St. Vincent's Hospital but was allowed to leave, under supervision, for workouts with the team for part of that time. He concludes that the hospitalization was behavior-related. Playing just 98 games, Ruth had his worst season as a Yankee; he finished with a .290 average and 25 home runs. The Yankees finished next to last in the AL with a 69–85 record, their last season with a losing record until 1965.\n\nRuth spent part of the offseason of 1925–26 working out at Artie McGovern's gym, getting back into shape. Barrow and Huggins had rebuilt the team, surrounding the veteran core with good young players like Tony Lazzeri and Lou Gehrig. But New York was not expected to win the pennant.\n\nBabe Ruth returned to his normal production during 1926, batting .372 with 47 home runs and 146 RBIs. The Yankees built a ten-game lead by mid-June, and coasted to win the pennant by three games. The St. Louis Cardinals had won the National League with the lowest winning percentage for a pennant winner to that point (.578) and the Yankees were expected to win the World Series easily. Although the Yankees won the opener in New York, St. Louis took Games Two and Three. In Game Four, Ruth hit three home runs, the first time this had been done in a World Series game, to lead the Yankees to victory; in the fifth game Ruth caught a ball as he crashed into the fence, described by baseball writers as a defensive gem. New York took that game, but Grover Cleveland Alexander won Game Six for St. Louis to tie the Series at three games each, then got very drunk. He was nevertheless inserted into Game Seven in the seventh inning and shut down the Yankees to win the game, 3–2, and win the Series. Ruth had hit his fourth home run of the Series earlier in the game, and was the only Yankee to reach base off Alexander, walking in the ninth inning before being caught stealing to end the game. Although Ruth's attempt to steal second is often deemed a baserunning blunder, Creamer pointed out that the Yankees' chances of tying the game would have been greatly improved with a runner in scoring position.\nThe 1926 Series was also known for Ruth's promise to Johnny Sylvester, a hospitalized 11-year-old, that he would hit a home run on his behalf. Sylvester had been injured in a fall from a horse, and a friend of Sylvester's father gave the boy two autographed baseballs signed by Yankees and Cardinals, and relayed a promise from Ruth, who did not know the boy, to hit a home run for him. After the Series, Ruth visited the boy in the hospital. When the matter became public, the press greatly inflated it, and by some accounts, Ruth saved a dying boy's life by visiting him, emotionally promising to hit a home run, and doing so.\n\nThe 1927 New York Yankees team is considered one of the greatest squads that ever took the field. Known as Murderer's Row because of the power of its lineup, the team won a then-AL-record 110 games, and took the AL pennant by 19 games, clinching first place on Labor Day. With little suspense as to the pennant race, the nation's attention turned to Ruth's pursuit of his own single-season home run record of 59. He was not alone in this chase: Gehrig proved to be a slugger capable of challenging Ruth for his home run crown, tying Ruth with 24 home runs late in June. Through July and August, they were never separated by more than two home runs. Gehrig took the lead, 45–44, in the first game of a doubleheader at Fenway Park early in September; Ruth responded with two of his own to take the lead, as it proved permanently—Gehrig finished with 47. Even so, as of September 6, Ruth was still several games off his 1921 pace, and going into the final series against the Senators, had only 57. He hit two in the first game of the series, including one off of Paul Hopkins, facing his first major league batter, to tie the record. The following day, September 30, he broke it with his 60th homer, in the eighth inning off Tom Zachary to break a 2–2 tie. \"Sixty! Let's see some son of a bitch try to top that one\", Ruth exulted after the game. In addition to his career-high 60 home runs, Ruth batted .356, drove in 164 runs and slugged .772. In the 1927 World Series, the Yankees swept the Pittsburgh Pirates in four games; the National Leaguers were disheartened after watching the Yankees take batting practice before Game One, with ball after ball leaving Forbes Field. According to Appel, \"The 1927 New York Yankees. Even today, the words inspire awe ... all baseball success is measured against the '27 team.\"\nThe following season started off well for the Yankees, who led the league in the early going. But the Yankees were plagued by injuries, erratic pitching and inconsistent play. The Philadelphia Athletics, rebuilding after some lean years, erased the Yankees' big lead and even took over first place briefly in early September. The Yankees, however, regained first place when they beat the Athletics three out of four games in a pivotal series at Yankee Stadium later that month, and clinched the pennant in the final weekend of the season. Ruth's play in 1928 mirrored his team's performance. He got off to a hot start and on August 1, he had 42 home runs. This put him ahead of his 60 home run pace from the previous season. He then slumped for the latter part of the season, and he hit just twelve home runs in the last two months. Ruth's batting average also fell to .323, well below his career average. Nevertheless, he ended the season with 54 home runs. The Yankees swept the favored Cardinals in four games in the World Series, with Ruth batting .625 and hitting three home runs in Game Four, including one off Alexander.\n\nBefore the 1929 season, Ruppert, who had bought out Huston in 1923, announced that the Yankees would wear uniform numbers to allow fans at cavernous Yankee Stadium to tell one player from another. The Cardinals and Indians had each experimented with uniform numbers; the Yankees were the first to use them on both home and away uniforms. As Ruth batted third, he was given number 3. According to a long-standing baseball legend, the Yankees adopted their now-iconic pinstriped uniforms in hopes of making Ruth look slimmer. In truth, though, they had been wearing pinstripes since Ruppert bought the team in 1915.\n\nAlthough the Yankees started well, the Athletics soon proved they were the better team in 1929, splitting two series with the Yankees in the first month of the season, then taking advantage of a Yankee losing streak in mid-May to gain first place. Although Ruth performed well, the Yankees were not able to catch the Athletics—Connie Mack had built another great team. Tragedy struck the Yankees late in the year as manager Huggins died of erysipelas, a bacterial skin infection, on September 25, only ten days after he had last led the team. Despite past differences, Ruth praised Huggins and described him as a \"great guy\". The Yankees finished second, 18 games behind the Athletics. Ruth hit .345 during the season, with 46 home runs and 154 RBIs.\n\nOn October 17, the Yankees hired Bob Shawkey as manager, their fourth choice. Ruth had politicked for the job of player-manager, but was never seriously considered by Ruppert and Barrow; Stout deems this the first hint Ruth would have no future with the Yankees once he was done as a player. Shawkey, a former Yankees player and teammate of Ruth, would prove unable to command the slugger's respect.\n\nOn January 7, 1930, salary negotiations between the Yankees and Ruth quickly broke down. Having just concluded a three-year contract at an annual salary of $70,000, Ruth promptly rejected both the Yankees' initial proposal of $70,000 for one year and their 'final' offer of two years at seventy-five—the latter figure equalling the annual salary of then US President Herbert Hoover; instead, Ruth demanded at least $85,000 and three years. When asked why he thought he was \"worth more than the President of the United States,\" Ruth responded: \"Say, if I hadn't been sick last summer, I'd have broken hell out of that home run record! Besides, the President gets a four-year contract. I'm only asking for three.\" Exactly two months later, a compromise was reached, with Ruth settling for two years at an unprecedented $80,000 per year.\n\nIn 1930, Ruth hit .359 with 49 home runs (his best in his years after 1928) and 153 RBIs, and pitched his first game in nine years, a complete game victory. Nevertheless, the Athletics won their second consecutive pennant and World Series, as the Yankees finished in third place, sixteen games back. At the end of the season, Shawkey was fired and replaced with Cubs manager Joe McCarthy, though Ruth again unsuccessfully sought the job.\n\nMcCarthy was a disciplinarian, but chose not to interfere with Ruth, and the slugger for his part did not seek conflict with the manager. The team improved in 1931, but was no match for the Athletics, who won 107 games, games in front of the Yankees. Ruth, for his part, hit .373, with 46 home runs and 163 RBIs. He had 31 doubles, his most since 1924. In the 1932 season, the Yankees went 107–47 and won the pennant. Ruth's effectiveness had decreased somewhat, but he still hit .341 with 41 home runs and 137 RBIs. Nevertheless, he twice was sidelined due to injury during the season.\n\nThe Yankees faced the Cubs, McCarthy's former team, in the 1932 World Series. There was bad blood between the two teams as the Yankees resented the Cubs only awarding half a World Series share to Mark Koenig, a former Yankee. The games at Yankee Stadium had not been sellouts; both were won by the home team, with Ruth collecting two singles, but scoring four runs as he was walked four times by the Cubs pitchers. In Chicago, Ruth was resentful at the hostile crowds that met the Yankees's train and jeered them at the hotel. The crowd for Game Three included New York Governor Franklin D. Roosevelt, the Democratic candidate for president, who sat with Chicago Mayor Anton Cermak. Many in the crowd threw lemons at Ruth, a sign of derision, and others (as well as the Cubs themselves) shouted abuse at Ruth and other Yankees. They were briefly silenced when Ruth hit a three-run home run off Charlie Root in the first inning, but soon revived, and the Cubs tied the score at 4–4 in the fourth inning. When Ruth came to the plate in the top of the fifth, the Chicago crowd and players, led by pitcher Guy Bush, were screaming insults at Ruth. With the count at two balls and one strike, Ruth gestured, possibly in the direction of center field, and after the next pitch (a strike), may have pointed there with one hand. Ruth hit the fifth pitch over the center field fence; estimates were that it traveled nearly . Whether or not Ruth intended to indicate where he planned to (and did) hit the ball, the incident has gone down in legend as Babe Ruth's called shot. The Yankees won Game Three, and the following day clinched the Series with another victory. During that game, Bush hit Ruth on the arm with a pitch, causing words to be exchanged and provoking a game-winning Yankee rally.\n\nRuth remained productive in 1933, as he batted .301, with 34 home runs, 103 RBIs, and a league-leading 114 walks, as the Yankees finished in second place, seven games behind the Senators. Athletics manager Connie Mack selected him to play right field in the first Major League Baseball All-Star Game, held on July 6, 1933, at Comiskey Park in Chicago. He hit the first home run in the All-Star Game's history, a two-run blast against Bill Hallahan during the third inning, which helped the AL win the game 4–2. During the final game of the 1933 season, as a publicity stunt organized by his team, Ruth was called upon and pitched a complete game victory against the Red Sox, his final appearance as a pitcher. Despite unremarkable pitching numbers, Ruth had a 5–0 record in five games for the Yankees, raising his career totals to 94–46.\n\nIn 1934, Ruth played in his last full season. By this time, years of high living were starting to catch up with him. His conditioning had deteriorated to the point that he could no longer field or run. He accepted a pay cut from Ruppert to $35,000, but was still the highest-paid player in the major leagues. He could still handle a bat, recording a .288 batting average with 22 home runs, statistics Reisler described as \"merely mortal\". Ruth was selected to the AL All-Star team for the second consecutive year, even though he was in the twilight of his career. During the game, New York Giants pitcher Carl Hubbell struck out Ruth and four other future Hall-of-Famers consecutively. The Yankees finished second again, seven games behind the Tigers.\n\nAlthough Ruth knew he was nearly finished as a player, he desired to remain in baseball as a manager. He was often spoken of as a possible candidate as managerial jobs opened up, but in 1932, when he was mentioned as a contender for the Red Sox position, Ruth stated that he was not yet ready to leave the field. There were rumors that Ruth was a likely candidate each time when the Cleveland Indians, Cincinnati Reds, and Detroit Tigers were looking for a manager, but nothing came of them.\n\nJust before the 1934 season, Ruppert offered to make Ruth the manager of the Yankees' top minor-league team, the Newark Bears, but he was talked out of it by his wife, Claire, and his business manager, Christy Walsh. Shortly afterward, Tigers owner Frank Navin made a proposal to Ruppert and Barrow—if the Yankees traded Ruth to Detroit, Navin would name Ruth player-manager. Navin believed Ruth would not only bring a winning attitude to a team that had not finished higher than third since 1923, but would also revive the Tigers' sagging attendance figures. Navin asked Ruth to come to Detroit for an interview. However, Ruth balked, since Walsh had already arranged for him to take part in a celebrity golf tournament in Hawaii. Ruth and Navin negotiated over the phone while Ruth was in Hawaii, but those talks foundered when Navin refused to give Ruth a portion of the Tigers' box office proceeds.\n\nEarly in the 1934 season, Ruth began openly campaigning to become manager of the Yankees. However, the Yankee job was never a serious possibility. Ruppert always supported McCarthy, who would remain in his position for another 12 seasons. Ruth and McCarthy's relationship had been lukewarm at best, and Ruth's managerial ambitions further chilled their relations. By the end of the season, Ruth hinted that he would retire unless Ruppert named him manager of the Yankees. For his part, Ruppert wanted his slugger to leave the team without drama and hard feelings when the time came.\n\nDuring the 1934–35 offseason, Ruth circled the world with his wife, including a barnstorming tour of the Far East. At his final stop in the United Kingdom before returning home, Ruth was introduced to cricket by Australian player Alan Fairfax, and after having little luck in a cricketer's stance, stood as a baseball batter and launched some massive shots around the field, destroying the bat in the process. Although Fairfax regretted that he could not have the time to make Ruth a cricket player, Ruth had lost any interest in such a career upon learning that the best batsmen made only about $40 per week.\n\nAlso during the offseason, Ruppert had been sounding out the other clubs in hopes of finding one that would be willing to take Ruth as a manager and/or a player. However, the only serious offer came from Athletics owner-manager Connie Mack, who gave some thought to stepping down as manager in favor of Ruth. However, Mack later dropped the idea, saying that Ruth's wife would be running the team in a month if Ruth ever took over.\n\nWhile the barnstorming tour was under way, Ruppert began negotiating with Boston Braves owner Judge Emil Fuchs, who wanted Ruth as a gate attraction. Although the Braves had enjoyed modest recent success, finishing fourth in the National League in both 1933 and 1934, the team performed poorly at the box office. Unable to afford the rent at Braves Field, Fuchs had considered holding dog races there when the Braves were not at home, only to be turned down by Landis. After a series of phone calls, letters, and meetings, the Yankees traded Ruth to the Braves on February 26, 1935. Ruppert had stated that he would not release Ruth to go to another team as a full-time player. For this reason, it was announced that Ruth would become a team vice president and would be consulted on all club transactions, in addition to playing. He was also made assistant manager to Braves skipper Bill McKechnie. In a long letter to Ruth a few days before the press conference, Fuchs promised Ruth a share in the Braves' profits, with the possibility of becoming co-owner of the team. Fuchs also raised the possibility of Ruth succeeding McKechnie as manager, perhaps as early as 1936. Ruppert called the deal \"the greatest opportunity Ruth ever had\".\n\nThere was considerable attention as Ruth reported for spring training. He did not hit his first home run of the spring until after the team had left Florida, and was beginning the road north in Savannah. He hit two in an exhibition against the Bears. Amid much press attention, Ruth played his first home game in Boston in over 16 years. Before an opening-day crowd of over 25,000, including five of New England's six state governors, Ruth accounted for all of the Braves' runs in a 4–2 defeat of the New York Giants, hitting a two-run home run, singling to drive in a third run and later in the inning scoring the fourth. Although age and weight had slowed him, he made a running catch in left field that sportswriters deemed the defensive highlight of the game.\n\nRuth had two hits in the second game of the season, but it quickly went downhill both for him and the Braves from there. The season soon settled down to a routine of Ruth performing poorly on the few occasions he even played at all, and the Braves losing most games. As April passed into May, Ruth's deterioration became even more pronounced. While he remained productive at the plate early on, he could do little else. His condition had deteriorated to the point that he could barely trot around the bases. His fielding had become so poor that three Braves pitchers told McKechnie that they would not take the mound if he was in the lineup. Before long, Ruth stopped hitting as well. He grew increasingly annoyed that McKechnie ignored most of his advice. For his part, McKechnie later said that Ruth's huge salary and refusal to stay with the team while on the road made it nearly impossible to enforce discipline.\n\nRuth soon realized that Fuchs had deceived him, and had no intention of making him manager or giving him any significant off-field duties. He later stated that his only duties as vice president consisted of making public appearances and autographing tickets. Ruth also found out that far from giving him a share of the profits, Fuchs wanted him to invest some of \"his\" money in the team in a last-ditch effort to improve its balance sheet. As it turned out, both Fuchs and Ruppert had known all along that Ruth's non-playing positions were meaningless.\n\nBy the end of the first month of the season, Ruth concluded he was finished even as a part-time player. As early as May 12, he asked Fuchs to let him retire. Ultimately, Fuchs persuaded Ruth to remain at least until after the Memorial Day doubleheader in Philadelphia. In the interim was a western road trip, at which the rival teams had scheduled days to honor him. In Chicago and St. Louis, Ruth performed poorly, and his batting average sank to .155, with only three home runs. In the first two games in Pittsburgh, Ruth had only one hit, though a long fly caught by Paul Waner probably would have been a home run in any other ballpark besides Forbes Field.\n\nRuth played in the third game of the Pittsburgh series on May 25, 1935, and added one more tale to his playing legend. Ruth went 4-for-4, including three home runs, though the Braves lost the game 11–7. The last two were off Ruth's old Cubs nemesis, Guy Bush. The final home run, both of the game and of Ruth's career, sailed over the upper deck in right field and out of the ballpark, the first time anyone had hit a fair ball completely out of Forbes Field. Ruth was urged to make this his last game, but he had given his word to Fuchs and played in Cincinnati and Philadelphia. The first game of the doubleheader in Philadelphia—the Braves lost both—was his final major league appearance. On June 2, after an argument with Fuchs, Ruth retired. He finished 1935 with a .181 average—easily his worst as a full-time position player—and the final six of his 714 home runs. The Braves, 10–27 when Ruth left, finished 38–115, at .248 the worst winning percentage in modern National League history. Insolvent like his team, Fuchs gave up control of the Braves before the end of the season; the National League took over the franchise at the end of the year.\n\nAlthough Fuchs had given Ruth his unconditional release, no major league team expressed an interest in hiring him in any capacity. Ruth still hoped to be hired as a manager if he could not play anymore, but only one managerial position, Cleveland, became available between Ruth's retirement and the end of the 1937 season. Asked if he had considered Ruth for the job, Indians owner Alva Bradley replied negatively.\n\nThe writer Creamer believed Ruth was unfairly treated in never being given an opportunity to manage a major league club. The author believed there was not necessarily a relationship between personal conduct and managerial success, noting that McGraw, Billy Martin, and Bobby Valentine were winners despite character flaws. Team owners and general managers assessed Ruth's flamboyant personal habits as a reason to exclude him from a managerial job; Barrow said of him, \"How can he manage other men when he can't even manage himself?\"\n\nRuth played much golf and in a few exhibition baseball games, demonstrating a continuing ability to draw large crowds. This appeal contributed to the Dodgers hiring him as first base coach in 1938. But Brooklyn general manager Larry MacPhail made it clear when Ruth was hired that he would not be considered for the manager's job if, as expected, Burleigh Grimes retired at the end of the season. Although much was said about what Ruth could teach the younger players, in practice, his duties were to appear on the field in uniform and encourage base runners—he was not called upon to relay signs. He got along well with everyone except team captain Leo Durocher, who was hired as Grimes' replacement at season's end. Ruth returned to retirement, never again to work in baseball.\n\nOn July 4, 1939, Ruth spoke on Lou Gehrig Appreciation Day at Yankee Stadium as members of the 1927 Yankees and a sellout crowd turned out to honor the first baseman, forced into premature retirement by ALS disease, which would kill him in two years. The next week, Ruth went to Cooperstown, New York, for the formal opening of the Baseball Hall of Fame. Three years earlier he was one of the first five players elected to it. As radio broadcasts of baseball became popular, Ruth sought a job in that field, arguing that his celebrity and knowledge of baseball would assure large audiences, but he received no offers. During World War II, he made many personal appearances to advance the war effort, including his last appearance as a player at Yankee Stadium, in a 1943 exhibition for the Army–Navy Relief Fund. He hit a long fly ball off Walter Johnson; the blast left the field, curving foul, but Ruth circled the bases anyway. In 1946, he made a final effort to gain a job in baseball, contacting new Yankees boss MacPhail, but was sent a rejection letter.\n\nRuth met Helen Woodford (1897–1929), by some accounts, in a coffee shop in Boston where she was a waitress, and they were married on October 17, 1914; he was 19 and she was 17. Although Ruth later claimed to have been married in Elkton, Maryland, records show that they were married in Ellicott City at St. Paul's Catholic Church. They adopted a daughter, Dorothy (1921–1989), in 1921. Ruth and Helen separated around 1925, reportedly due to his repeated infidelities. Their last public appearance together came during the 1926 World Series. Helen died in January 1929 at age 31 in a house fire in Watertown, Massachusetts, in a house owned by Edward Kinder, a dentist with whom she had been living as \"Mrs. Kinder\". In her book, \"My Dad, the Babe\", Dorothy claimed that she was Ruth's biological child by a mistress named Juanita Jennings. She died in 1989.\n\nOn April 17, 1929, only three months after the death of his first wife, Ruth married actress and model Claire Merritt Hodgson (1897–1976) and adopted her daughter Julia; he was 34 and she was 31. It was the second and final marriage for both parties. By one account, Julia and Dorothy were, through no fault of their own, the reason for the seven-year rift in Ruth's relationship with teammate Lou Gehrig. Sometime in 1932, Gehrig's mother, during a conversation which she assumed was private, remarked, \"It's a shame [Claire] doesn't dress Dorothy as nicely as she dresses her own daughter.\" When the comment inevitably got back to Ruth, he angrily told Gehrig to tell his mother to mind her own business. Gehrig in turn took offense at what he perceived as Ruth's comment about his mother. The two men reportedly never spoke off the field until they reconciled at Yankee Stadium on Lou Gehrig Appreciation Day in 1939.\n\nAlthough Ruth was married through most of his baseball career, when Colonel Huston asked him to tone down his lifestyle, the player said, \"I'll promise to go easier on drinking and to get to bed earlier, but not for you, fifty thousand dollars, or two-hundred and fifty thousand dollars will I give up women. They're too much fun.\"\n\nAs early as the war years, doctors had cautioned Ruth to take better care of his health, and he grudgingly followed their advice, limiting his drinking and not going on a proposed trip to support the troops in the South Pacific. In 1946, Ruth began experiencing severe pain over his left eye, and had difficulty swallowing. In November 1946, he entered French Hospital in New York for tests, which revealed that Ruth had an inoperable malignant tumor at the base of his skull and in his neck. It was a lesion known as nasopharyngeal carcinoma, or \"lymphoepithelioma.\" His name and fame gave him access to experimental treatments, and he was one of the first cancer patients to receive both drugs and radiation treatment simultaneously. He was discharged from the hospital in February, having lost , and went to Florida to recuperate. He returned to New York and Yankee Stadium after the season started. The new commissioner, Happy Chandler (Judge Landis had died in 1944), proclaimed April 27, 1947, Babe Ruth Day around the major leagues, with the most significant observance to be at Yankee Stadium. A number of teammates and others spoke in honor of Ruth, who briefly addressed the crowd of almost 60,000.\n\nAround this time, developments in chemotherapy offered some hope for Ruth. The doctors had not told Ruth that he had cancer because of his family's fear that he might do himself harm. They treated him with teropterin, a folic acid derivative; he may have been the first human subject. Ruth showed dramatic improvement during the summer of 1947, so much so that his case was presented by his doctors at a scientific meeting, without using his name. He was able to travel around the country, doing promotional work for the Ford Motor Company on American Legion Baseball. He appeared again at another day in his honor at Yankee Stadium in September, but was not well enough to pitch in an old-timers game as he had hoped.\n\nThe improvement was only a temporary remission, and by late 1947, Ruth was unable to help with the writing of his autobiography, \"The Babe Ruth Story\", which was almost entirely ghostwritten. In and out of the hospital in Manhattan, he left for Florida in February 1948, doing what activities he could. After six weeks he returned to New York to appear at a book-signing party. He also traveled to California to witness the filming of the book.\n\nOn June 5, 1948, a \"gaunt and hollowed out\" Ruth visited Yale University to donate a manuscript of \"The Babe Ruth Story\" to its library. On June 13, Ruth visited Yankee Stadium for the final time in his life, appearing at the 25th anniversary celebrations of \"The House that Ruth Built\". By this time he had lost much weight and had difficulty walking. Introduced along with his surviving teammates from 1923, Ruth used a bat as a cane. Nat Fein's photo of Ruth taken from behind, standing near home plate and facing \"Ruthville\" (right field) became one of baseball's most famous and widely circulated photographs, and won the Pulitzer Prize.\n\nRuth made one final trip on behalf of American Legion Baseball, then entered Memorial Hospital, where he would die. He was never told he had cancer, but before his death, had surmised it. He was able to leave the hospital for a few short trips, including a final visit to Baltimore. On July 26, 1948, Ruth left the hospital to attend the premiere of the film \"The Babe Ruth Story\". Shortly thereafter, Ruth returned to the hospital for the final time. He was barely able to speak. Ruth's condition gradually grew worse; only a few visitors were allowed to see him, one of whom was National League president and future Commissioner of Baseball Ford Frick. \"Ruth was so thin it was unbelievable. He had been such a big man and his arms were just skinny little bones, and his face was so haggard\", Frick said years later.\n\nThousands of New Yorkers, including many children, stood vigil outside the hospital in Ruth's final days. On August 16, 1948, at 8:01 p.m., Ruth died in his sleep at the age of 53. His open casket was placed on display in the rotunda of Yankee Stadium, where it remained for two days; 77,000 people filed past to pay him tribute. His funeral Mass took place at St. Patrick's Cathedral; a crowd estimated at 75,000 waited outside. Ruth was buried on a hillside in Section 25 at the Gate of Heaven Cemetery in Hawthorne, New York. An epitaph by Cardinal Spellman appears on his headstone. His second wife, Claire Merritt Ruth, would be interred with him 28 years later in 1976.\n\nOn April 19, 1949, the Yankees unveiled a granite monument in Ruth's honor in center field of Yankee Stadium. The monument was located in the field of play next to a flagpole and similar tributes to Huggins and Gehrig until the stadium was remodeled from 1974 to 1975, which resulted in the outfield fences moving inward and enclosing the monuments from the playing field. This area was known thereafter as Monument Park. Yankee Stadium, \"the House that Ruth Built\", was replaced after the 2008 season with a new Yankee Stadium across the street from the old one; Monument Park was subsequently moved to the new venue behind the center field fence. Ruth's uniform number 3 has been retired by the Yankees, and he is one of five Yankees players or managers to have a granite monument within the stadium.\n\nThe Babe Ruth Birthplace Museum is located at 216 Emory Street, a Baltimore row house where Ruth was born, and three blocks west of Oriole Park at Camden Yards, where the AL's Baltimore Orioles play. The property was restored and opened to the public in 1973 by the non-profit Babe Ruth Birthplace Foundation, Inc. Ruth's widow, Claire, his two daughters, Dorothy and Julia, and his sister, Mamie, helped select and install exhibits for the museum.\n\nRuth was the first baseball star to be the subject of overwhelming adulation by the public. Baseball had been known for star players such as Ty Cobb and \"Shoeless Joe\" Jackson, but both men had uneasy relations with fans, in Cobb's case sometimes marked by violence. Ruth's biographers agreed that he benefited from the timing of his ascension to \"Home Run King\", with a country hit hard by both the war and the 1918 flu pandemic longing for something to help put these traumas behind it. He also resonated in a country which felt, in the aftermath of the war, that it took second place to no one. Montville argues that as a larger-than-life figure capable of unprecedented athletic feats in the nation's largest city, Ruth became an icon of the significant social changes which marked the early 1920s. Glenn Stout notes in his history of the Yankees, \"Ruth was New York incarnate—uncouth and raw, flamboyant and flashy, oversized, out of scale, and absolutely unstoppable\".\n\nRuth became such a symbol of the United States during his lifetime that during World War II, Japanese soldiers yelled in English, \"To hell with Babe Ruth\", to anger American soldiers. Ruth replied that he hoped that \"every Jap that mention[ed] my name gets shot\". Creamer recorded that \"Babe Ruth transcended sport, moved far beyond the artificial limits of baselines and outfield fences and sports pages\". Wagenheim stated, \"He appealed to a deeply rooted American yearning for the definitive climax: clean, quick, unarguable.\" According to Glenn Stout, \"Ruth's home runs were exalted, uplifting experience that meant more to fans than any runs they were responsible for. A Babe Ruth home run was an event unto itself, one that meant anything was possible.\"\n\nRuth's penchant for hitting home runs altered how baseball is played. Prior to 1920, home runs were unusual, and managers tried to win games by getting a runner on base and bringing him around to score through such means as the stolen base, the bunt, and the hit and run. Advocates of what was dubbed \"inside baseball\", such as Giants manager McGraw, disliked the home run, considering it a blot on the purity of the game. According to sportswriter W. A. Phelon after the 1920 season, Ruth's breakout performance that season and the response in excitement and attendance, \"settled, for all time to come, that the American public is nuttier over the Home Run than the Clever Fielding or the Hitless Pitching. Viva el Home Run and two times viva Babe Ruth, exponent of the home run, and overshadowing star.\" Bill James noted, \"When the owners discovered that the fans \"liked\" to see home runs, and when the foundations of the games were simultaneously imperiled by disgrace [in the Black Sox Scandal], then there was no turning back.\" While a few, such as McGraw and Cobb, decried the passing of the old-style play, teams quickly began to seek and develop sluggers.\n\nAccording to contemporary sportswriter Grantland Rice, only two sports figures of the 1920s approached Ruth in popularity—boxer Jack Dempsey and racehorse Man o' War. One of the factors that contributed to Ruth's broad appeal was the uncertainty about his family and early life. Ruth appeared to exemplify the American success story, that even an uneducated, unsophisticated youth, without any family wealth or connections, can do something better than anyone else in the world. Montville noted that \"the fog [surrounding his childhood] will make him forever accessible, universal. He will be the patron saint of American possibility.\" Similarly, the fact that Ruth played in the pre-television era, when a relatively small portion of his fans had the opportunity to see him play allowed his legend to grow through word of mouth and the hyperbole of sports reporters. Reisler noted that recent sluggers who surpassed Ruth's 60-home run mark, such as Mark McGwire and Barry Bonds, generated much less excitement than when Ruth repeatedly broke the single-season home run record in the 1920s. Ruth dominated a relatively small sports world, while Americans of the present era have many sports available to watch.\n\nCreamer termed Ruth \"a unique figure in the social history of the United States\". Ruth has even entered the language: a dominant figure in a field, whether within or outside sports, is often referred to as \"the Babe Ruth\" of that field. Similarly, \"Ruthian\" has come to mean in sports, \"colossal, dramatic, prodigious, magnificent; with great power.\"\n\nIn 2006, Montville noted that more books have been written about Ruth than any other member of the Baseball Hall of Fame. At least five of these books (including Creamer's and Wagenheim's) were written in 1973 and 1974. The books were timed to capitalize on the increase in public interest in Ruth as Henry Aaron approached his career home run mark, which he broke on April 8, 1974. As he approached Ruth's record, Aaron stated, \"I can't remember a day this year or last when I did not hear the name of Babe Ruth.\"\n\nMontville suggested that Ruth is probably even more popular today than he was when his career home run record was broken by Aaron. The long ball era that Ruth started continues in baseball, to the delight of the fans. Owners build ballparks to encourage home runs, which are featured on \"SportsCenter\" and \"Baseball Tonight\" each evening during the season. The questions of performance-enhancing drug use, which dogged later home run hitters such as McGwire and Bonds, do nothing to diminish Ruth's reputation; his overindulgences with beer and hot dogs seem part of a simpler time.\n\nRuth has been named the greatest baseball player of all time in various surveys and rankings. In 1998, \"The Sporting News\" ranked him number one on the list of \"Baseball's 100 Greatest Players\". In 1999, baseball fans named Ruth to the Major League Baseball All-Century Team. He was named baseball's Greatest Player Ever in a ballot commemorating the 100th anniversary of professional baseball, in 1969. The Associated Press reported in 1993 that Muhammad Ali was tied with Babe Ruth as the most recognized athletes in America. In a 1999 ESPN poll, he was ranked as the second-greatest U.S. athlete of the century, behind Michael Jordan. In 1983, the United States Postal Service honored Ruth with the issuance of a twenty-cent stamp.\n\nSeveral of the most expensive items of sports memorabilia and baseball memorabilia ever sold at auction are associated with Ruth. As of November 2016, the most expensive piece of sports memorabilia ever sold is Ruth's 1920 Yankees jersey, which sold for $4,415,658 in 2012. The bat with which he hit the first home run at Yankee Stadium is in \"The Guinness Book of World Records\" as the most expensive baseball bat sold at auction, having fetched $1,265,000 on December 2, 2004. A hat of Ruth's from the 1934 season set a record for a baseball cap when David Wells sold it at auction for $537,278 in 2012. In 2017, Charlie Sheen sold Ruth's 1927 World Series ring for $2,093,927 at auction. It easily broke the record for a championship ring previously set when Julius Erving's 1974 ABA championship ring sold for $460,741 in 2011.\nOne long-term survivor of the craze over Ruth may be the Baby Ruth candy bar. The original company to market the confectionery, the Curtis Candy Company, maintained that the bar was named after Ruth Cleveland, daughter of former president Grover Cleveland. She died in 1904 and the bar was first marketed in 1921, at the height of the craze over the slugger. The slugger later sought to market candy bearing his name; he was refused a trademark because of the Baby Ruth bar. Corporate files from 1921 are no longer extant; the brand has changed hands several times and is now owned by the Nestlé company. The Ruth estate licensed his likeness for use in an advertising campaign for Baby Ruth in 1995. Due to a marketing arrangement, in 2005, the Baby Ruth bar became the official candy bar of Major League Baseball.\n\nMontville noted the continuing relevance of Babe Ruth in American culture, more than three-quarters of a century after he last swung a bat in a major league game:\n\n\n\n\n\n\n \n\n \n"}
{"id": "4177", "url": "https://en.wikipedia.org/wiki?curid=4177", "title": "Barge", "text": "Barge\n\nA barge is a flat-bottomed boat, built mainly for river and canal transport of heavy goods. Some barges are not self-propelled and must be towed or pushed by towboats. Canal barges, towed by draft animals on an adjacent towpath, contended with the railway in the early Industrial Revolution, but were outcompeted in the carriage of high-value items due to the higher speed, falling costs and route flexibility of railways.\n\n\"Barge\" is attested from 1300, from Old French \"barge\", from Vulgar Latin \"barga\". The word originally could refer to any small boat; the modern meaning arose around 1480. \"Bark\" \"small ship\" is attested from 1420, from Old French \"barque\", from Vulgar Latin \"barca\" (400 AD). The more precise meaning \"three-masted ship\" arose in the 17th century, and often takes the French spelling for disambiguation. Both are probably derived from the Latin \"barica\", from , from \"bāri\" \"small boat\", hieroglyphic Egyptian and similar \"ba-y-r\" for \"basket-shaped boat\". By extension, the term \"embark\" literally means to board the kind of boat called a \"barque\".\n\nThe long pole used to maneuver or propel a barge has given rise to the saying \"I wouldn't touch that [subject/thing] with a barge pole.\"\n\nOn the British canal system, the term 'barge' is used to describe a boat wider than a narrowboat, and the people who move barges are often known as lightermen. In the United States, deckhands perform the labor and are supervised by a leadman or the mate. The captain and pilot steer the towboat, which pushes one or more barges held together with rigging, collectively called 'the tow'. The crew live aboard the towboat as it travels along the inland river system or the intracoastal waterways. These towboats travel between ports and are also called line-haul boats.\n\nPoles are used on barges to fend off the barge as it nears other vessels or a wharf. These are often called 'pike poles'.\n\nBarges are used today for low-value bulk items, as the cost of hauling goods by barge is very low. Barges are also used for very heavy or bulky items; a typical American barge measures , and can carry up to about of cargo. The most common European barge measures and can carry up to about .\n\nAs an example, on June 26, 2006, a catalytic cracking unit reactor was shipped by barge from the Tulsa Port of Catoosa in Oklahoma to a refinery in Pascagoula, Mississippi. Extremely large objects are normally shipped in sections and assembled onsite, but shipping an assembled unit reduced costs and avoided reliance on construction labor at the delivery site (which in this case was still recovering from Hurricane Katrina). Of the reactor's journey, only about were traveled overland, from the final port to the refinery.\n\nSelf-propelled barges may be used as such when traveling downstream or upstream in placid waters; they are operated as an unpowered barge, with the assistance of a tugboat, when traveling upstream in faster waters. Canal barges are usually made for the particular canal in which they will operate.\n\nMany barges, primarily Dutch barges, which were originally designed for carrying cargo along the canals of Europe, are no longer large enough to compete in this industry with larger newer vessels. Many of these barges have been renovated and are now used as luxury hotel barges carrying holidaymakers along the same canals on which they once carried grain or coal.\n\nIn primitive regions today and in all pre-development (lacking highways or railways) regions worldwide in times before industrial development and highways, barges were the predominant and most efficient means of inland transportation in many regions. This holds true today, for many areas of the world.\n\nIn such pre-industrialized, or poorly developed infrastructure regions, many barges are purpose-designed to be powered on waterways by long slender poles — thereby becoming known on American waterways as poleboats as the extensive west of North America was settled using the vast tributary river systems of the Mississippi drainage basin. Poleboats use muscle power of \"walkers\" along the sides of the craft pushing a pole against the streambed, canal or lake bottom to move the vessel where desired. In settling the American west it was generally faster to navigate downriver from Brownsville, Pennsylvania, to the Ohio River confluence with the Mississippi and then pole upriver against the current to St. Louis than to travel overland on the rare primitive dirt roads for many decades after the American Revolution.\n\nOnce the New York Central and Pennsylvania Railroads reached Chicago, that time dynamic changed, and American poleboats became less common, relegated to smaller rivers and more remote streams. On the Mississippi riverine system today, including that of other sheltered waterways, industrial barge trafficking in bulk raw materials such as coal, coke, timber, iron ore and other minerals is extremely common; in the developed world using huge cargo barges that connect in groups and trains-of-barges in ways that allow cargo volumes and weights considerably greater than those used by pioneers of modern barge systems and methods in the Victorian era.\nSuch barges need to be towed by tugboats or pushed by towboats. Canal barges, towed by draft animals on a waterway adjacent towpath were of fundamental importance in the early Industrial Revolution, whose major early engineering projects were efforts to build viaducts, aqueducts and especially canals to fuel and feed raw materials to nascent factories in the early industrial takeoff (18th century) and take their goods to ports and cities for distribution.\n\nThe barge and canal system contended favourably with the railways in the early Industrial Revolution before around the 1850s–1860s — for example, the Erie Canal in New York state is credited by economic historians with giving the growth boost needed for New York City to eclipse Philadelphia as America's largest port and city — but such canal systems with their locks, need for maintenance and dredging, pumps and sanitary issues were eventually outcompeted in the carriage of high-value items by the railways due to the higher speed, falling costs and route flexibility of rail transport. Barge and canal systems were nonetheless of great, perhaps even primary, economic importance until after the First World War in Europe, particularly in the more developed nations of the Low Countries, France, Germany, Poland and especially Great Britain which more or less made the system characteristically its own.\n\nNowadays, custom built special purpose equipment called modular barges which is extensively used in survey, mapping, laying and burial of Sub Sea optic fibre cables worldwide and other support services. its a cost effective tool and is able to carry out tasks with much ease and precision.\n\n"}
{"id": "4178", "url": "https://en.wikipedia.org/wiki?curid=4178", "title": "Bill Schelter", "text": "Bill Schelter\n\nWilliam Frederick Schelter (1947 – July 30, 2001) was a professor of mathematics at The University of Texas at Austin and a Lisp developer and programmer. Schelter is credited with the development of the GNU Common Lisp (GCL) implementation of Common Lisp and the GPL'd version of the computer algebra system Macsyma called Maxima. Schelter authored Austin Kyoto Common Lisp (AKCL) under contract with IBM. AKCL formed the foundation for Axiom, another computer algebra system. AKCL eventually became GNU Common Lisp. He is also credited with the first port of the GNU C compiler to the Intel 386 architecture, used in the original implementation of the Linux kernel.\n\nSchelter obtained his Ph.D. at McGill University in 1972. His mathematical specialties were noncommutative ring theory and computational algebra and its applications, including automated theorem proving in geometry.\n\nIn the summer of 2001, age 54, he died suddenly of a heart attack while traveling in Russia.\n\n\n"}
{"id": "4179", "url": "https://en.wikipedia.org/wiki?curid=4179", "title": "British English", "text": "British English\n\nBritish English is the English language as spoken and written in the United Kingdom. Variations exist in formal, written English in the United Kingdom. For example, the adjective \"wee\" is almost exclusively used in parts of Scotland and Ireland, and occasionally Yorkshire, whereas \"little\" is predominant elsewhere. Nevertheless, there is a meaningful degree of uniformity in written English within the United Kingdom, and this could be described by the term British English. The forms of spoken English, however, vary considerably more than in most other areas of the world where English is spoken, so a uniform concept of British English is more difficult to apply to the spoken language. According to Tom McArthur in the \"Oxford Guide to World English\", British English shares \"all the ambiguities and tensions in the word 'British' and as a result can be used and interpreted in two ways, more broadly or more narrowly, within a range of blurring and ambiguity\".\n\nWhen distinguished from American English, the term \"British English\" is sometimes used broadly as a synonym for the various varieties of English spoken in some member states of the Commonwealth of Nations.\n\nEnglish is a West Germanic language that originated from the Anglo-Frisian dialects brought to Britain by Germanic settlers from various parts of what is now northwest Germany and the northern Netherlands. The resident population at this time was generally speaking Common Brittonic—the insular variety of continental Celtic, which was influenced by the Roman occupation. This group of languages (Welsh, Cornish, Cumbric) cohabited alongside English into the modern period, but due to their remoteness from the Germanic languages, influence on English was notably limited. However, the degree of influence remains debated, and it has recently been argued that its grammatical influence accounts for the substantial innovations noted between English and the other West Germanic languages. Initially, Old English was a diverse group of dialects, reflecting the varied origins of the Anglo-Saxon Kingdoms of England. One of these dialects, Late West Saxon, eventually came to dominate. The original Old English language was then influenced by two waves of invasion: the first was by speakers of the Scandinavian branch of the Germanic family, who conquered and colonised parts of Britain in the 8th and 9th centuries; the second was the Normans in the 11th century, who spoke Old Norman and ultimately developed an English variety of this called Anglo-Norman. These two invasions caused English to become \"mixed\" to some degree (though it was never a truly mixed language in the strictest sense of the word; mixed languages arise from the cohabitation of speakers of different languages, who develop a hybrid tongue for basic communication).\n\nThe more idiomatic, concrete and descriptive English is, the more it is from Anglo-Saxon origins. The more intellectual and abstract English is, the more it contains Latin and French influences e.g. swine (like the Germanic schwein) is the animal in the field bred by the occupied Anglo-Saxons and pork (like the French porc) is the animal at the table eaten by the occupying Normans\n\nCohabitation with the Scandinavians resulted in a significant grammatical simplification and lexical enrichment of the Anglo-Frisian core of English; the later Norman occupation led to the grafting onto that Germanic core of a more elaborate layer of words from the Romance branch of the European languages. This Norman influence entered English largely through the courts and government. Thus, English developed into a \"borrowing\" language of great flexibility and with a huge vocabulary.\n\nDialects and accents vary amongst the four countries of the United Kingdom, as well as within the countries themselves.\n\nThe major divisions are normally classified as English English (or English as spoken in England, which encompasses Southern English dialects, West Country dialects, East and West Midlands English dialects and Northern English dialects), Ulster English in Northern Ireland, Welsh English (not to be confused with the Welsh language), and Scottish English (not to be confused with the Scots language). The various British dialects also differ in the words that they have borrowed from other languages. Around the middle of the 15th century, there were points where within the 5 major dialects there were almost 500 ways to spell the same the word \"though\".\n\nFollowing its last major survey of English Dialects (1949–1950), the University of Leeds has started work on a new project. In May 2007 the Arts and Humanities Research Council awarded a grant to Leeds to study British regional dialects.\n\nThe team are sifting through a large collection of examples of regional slang words and phrases turned up by the \"Voices project\" run by the BBC, in which they invited the public to send in examples of English still spoken throughout the country. The BBC Voices project also collected hundreds of news articles about how the British speak English from swearing through to items on language schools. This information will also be collated and analysed by Johnson's team both for content and for where it was reported. \"Perhaps the most remarkable finding in the Voices study is that the English language is as diverse as ever, despite our increased mobility and constant exposure to other accents and dialects through TV and radio\". When discussing the award of the grant in 2007, Leeds University stated:\nMost people in Britain speak with a regional accent or dialect. However about 2% of Britons speak with an accent called Received Pronunciation (also called as \"the Queen's English\", \"Oxford English\" and \"BBC English\"), that is essentially region-less. It derives from a mixture of the Midland and Southern dialects spoken in London in the early modern period. It is frequently used as a model for teaching English to foreign learners.\n\nIn the South East there are significantly different accents; the Cockney accent spoken by some East Londoners is strikingly different from Received Pronunciation (RP). The Cockney rhyming slang can be (and was initially intended to be) difficult for outsiders to understand, although the extent of its use is often somewhat exaggerated.\n\nEstuary English has been gaining prominence in recent decades: it has some features of RP and some of Cockney. In London itself, the broad local accent is still changing, partly influenced by Caribbean speech. Immigrants to the UK in recent decades have brought many more languages to the country. Surveys started in 1979 by the Inner London Education Authority discovered over 100 languages being spoken domestically by the families of the inner city's schoolchildren. As a result, Londoners speak with a mixture of accents, depending on ethnicity, neighbourhood, class, age, upbringing, and sundry other factors.\n\nSince the mass internal immigration to Northamptonshire in the 1940s and its position between several major accent regions, it has become a source of various accent developments. In Northampton the older accent has been influenced by overspill Londoners. There is an accent known locally as the Kettering accent, which is a transitional accent between the East Midlands and East Anglian. It is the last southern midland accent to use the broad \"a\" in words like \"bath\"/\"grass\" (i.e. barth/grarss). Conversely \"crass\"/\"plastic\" use a slender \"a\". A few miles northwest in Leicestershire the slender \"a\" becomes more widespread generally. In the town of Corby, five miles (8 km) north, one can find Corbyite, which unlike the Kettering accent, is largely influenced by the West Scottish accent.\n\nIn addition, most British people can to some degree temporarily \"swing\" their accent towards a more neutral form of English at will, to reduce difficulty where very different accents are involved, or when speaking to foreigners.\n\nPhonological features characteristic of British English revolve around the pronunciation of the letter R, as well as the dental plosive T and some diphthongs specific to this dialect.\n\nIn a number of forms of spoken British English, it is common for the phoneme to be realised as a glottal stop when it is in the intervocalic position, in a process called T-glottalisation. Once regarded as a Cockney feature, it has become much more widespread. It is still stigmatised when used in words like \"later\", but becoming very widespread at the end of words such as \"not\" (as in no interested). Other consonants subject to this usage in Cockney English are \"p\", as in paer and \"k\" as in baer.\n\nIn most areas of Britain outside Scotland, the consonant R is not pronounced if not followed by a vowel, lengthening the preceding vowel instead. This phenomenon is known as non-rhoticity. \nIn these same areas, a tendency exists to insert an R between a word ending in a vowel and a next word beginning with a vowel. This is called the intrusive R. This could be understood as a merger, in that words that once ended in an R and words that didn't are no longer treated differently.\n\nBritish dialects differ on the extent of diphthongisation of long vowels, with southern varieties extensively turning them into diphthongs, and with northern dialects normally preserving many of them. As a comparison, North American varieties could be said to be in-between.\n\nLong vowels /i:/ and /u:/ are diphthongised to [ɪi] and [ʊu] respectively (or, more technically, [ʏʉ], with a raised tongue), so that ee and oo in feed and food are pronounced with a movement. The diphthong [oʊ] is also pronounced with a greater movement, normally [əʊ], [əʉ] or [əɨ].\n\nLong vowels /i:/ and /u:/ are usually preserved, and in several areas also /o:/ and /e:/, as in go and say (unlike other varieties of English, that change them to [oʊ] and [eɪ] respectively). Some areas go as far as not diphthongising medieval /i:/ and /u:/, that give rise to modern /aɪ/ and /aʊ/; that is, for example, in the traditional accent of Manchester, 'out' will sound as 'oot', and in parts of Scotland, 'my' will be pronounced as 'me'.\n\nA tendency to drop grammatical number in collective nouns, stronger in British English than in North American English, exists. This is namely treating them, that were once grammatically singular, as grammatically plural, that is: the perceived natural number prevails. This applies especially to nouns of institutions and groups made of many people.\n\nThe noun 'police', for example, undergoes this treatment:\n\nA football team can be treated likewise:\n\nSome dialects of British English, unlike American English, uses negative concords, also known as double negatives. Rather than changing a word or using a positive, words like nobody, not, nothing, and never would be used in the same sentence. While this does not occur in Standard English, it does occur in non-standard dialects. The double negation follows the idea of two different morphemes, one that causes the double negation, and one that is used for the point or the verb.\n\nAs with English around the world, the English language as used in the United Kingdom is governed by convention rather than formal code: there is no body equivalent to the Académie française or the Real Academia Española. Dictionaries (for example, Oxford English Dictionary, Longman Dictionary of Contemporary English, Chambers Dictionary, Collins Dictionary) record usage rather than attempting to prescribe it. In addition, vocabulary and usage change with time: words are freely borrowed from other languages and other strains of English, and neologisms are frequent.\n\nFor historical reasons dating back to the rise of London in the 9th century, the form of language spoken in London and the East Midlands became standard English within the Court, and ultimately became the basis for generally accepted use in the law, government, literature and education in Britain. The standardization of British English is thought to be from both dialect leveling and a thought of social superiority. Speaking in the Standard dialect created class distinctions; those who did not speak the standard English would be considered of a lesser class or social status and often discounted or considered of a low intelligence. Another contribution to the standardization of British English was the introduction of the printing press to England in the mid-15th century. In doing so, William Caxton enabled a common language and spelling to be dispersed among the entirety of England at a much faster rate.\n\nSamuel Johnson's A Dictionary of the English Language (1755) was a large step in the English-language spelling reform, where the purification of language focused on standardizing both speech and spelling. By the early 20th century, British authors have produced numerous books intended as guides to English grammar and usage, a few of which have achieved sufficient acclaim to have remained in print for long periods and to have been reissued in new editions after some decades. These include, most notably of all, Fowler's Modern English Usage and The Complete Plain Words by Sir Ernest Gowers.\n\nDetailed guidance on many aspects of writing British English for publication is included in style guides issued by various publishers including The Times newspaper, the Oxford University Press and the Cambridge University Press. The Oxford University Press guidelines were originally drafted as a single broadsheet page by Horace Henry Hart, and were at the time (1893) the first guide of their type in English; they were gradually expanded and eventually published, first as Hart's Rules, and in 2002 as part of \"The Oxford Manual of Style\". Comparable in authority and stature to The Chicago Manual of Style for published American English, the Oxford Manual is a fairly exhaustive standard for published British English that writers can turn to in the absence of specific guidance from their publishing house.\n\n\nCitations\n\n\n"}
